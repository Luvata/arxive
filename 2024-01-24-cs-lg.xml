<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.05259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.01135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.05410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.01636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.04033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.12305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.05612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.00144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.14358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.15269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.05481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10586" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.10895">
<title>AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10895</link>
<description rdf:parseType="Literal">&lt;p&gt;Supply chain risk assessment (SCRA) has witnessed a profound evolution
through the integration of artificial intelligence (AI) and machine learning
(ML) techniques, revolutionizing predictive capabilities and risk mitigation
strategies. The significance of this evolution stems from the critical role of
robust risk management strategies in ensuring operational resilience and
continuity within modern supply chains. Previous reviews have outlined
established methodologies but have overlooked emerging AI/ML techniques,
leaving a notable research gap in understanding their practical implications
within SCRA. This paper conducts a systematic literature review combined with a
comprehensive bibliometric analysis. We meticulously examined 1,717 papers and
derived key insights from a select group of 48 articles published between 2014
and 2023. The review fills this research gap by addressing pivotal research
questions, and exploring existing AI/ML techniques, methodologies, findings,
and future trajectories, thereby providing a more encompassing view of the
evolving landscape of SCRA. Our study unveils the transformative impact of
AI/ML models, such as Random Forest, XGBoost, and hybrids, in substantially
enhancing precision within SCRA. It underscores adaptable post-COVID
strategies, advocating for resilient contingency plans and aligning with
evolving risk landscapes. Significantly, this review surpasses previous
examinations by accentuating emerging AI/ML techniques and their practical
implications within SCRA. Furthermore, it highlights the contributions through
a comprehensive bibliometric analysis, revealing publication trends,
influential authors, and highly cited articles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1&quot;&gt;Md Abrar Jahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naife_S/0/1/0/all/0/1&quot;&gt;Saleh Akram Naife&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Anik Kumar Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mridha_M/0/1/0/all/0/1&quot;&gt;M. F. Mridha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10903">
<title>Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock. (arXiv:2401.10903v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2401.10903</link>
<description rdf:parseType="Literal">&lt;p&gt;This document presents a stock market analysis conducted on a dataset
consisting of 750 instances and 16 attributes donated in 2014-10-23. The
analysis includes an exploratory data analysis (EDA) section, feature
engineering, data preparation, model selection, and insights from the analysis.
The Fama French 3-factor model is also utilized in the analysis. The results of
the analysis are presented, with linear regression being the best-performing
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dengxin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10921">
<title>Push- and Pull-based Effective Communication in Cyber-Physical Systems. (arXiv:2401.10921v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.10921</link>
<description rdf:parseType="Literal">&lt;p&gt;In Cyber Physical Systems (CPSs), two groups of actors interact toward the
maximization of system performance: the sensors, observing and disseminating
the system state, and the actuators, performing physical decisions based on the
received information. While it is generally assumed that sensors periodically
transmit updates, returning the feedback signal only when necessary, and
consequently adapting the physical decisions to the communication policy, can
significantly improve the efficiency of the system. In particular, the choice
between push-based communication, in which updates are initiated autonomously
by the sensors, and pull-based communication, in which they are requested by
the actuators, is a key design step. In this work, we propose an analytical
model for optimizing push- and pull-based communication in CPSs, observing that
the policy optimality coincides with Value of Information (VoI) maximization.
Our results also highlight that, despite providing a better optimal solution,
implementable push-based communication strategies may underperform even in
relatively simple scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Talli_P/0/1/0/all/0/1&quot;&gt;Pietro Talli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mason_F/0/1/0/all/0/1&quot;&gt;Federico Mason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chiariotti_F/0/1/0/all/0/1&quot;&gt;Federico Chiariotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zanella_A/0/1/0/all/0/1&quot;&gt;Andrea Zanella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10927">
<title>Debiasing and a local analysis for population clustering using semidefinite programming. (arXiv:2401.10927v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.10927</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the problem of partitioning a small data sample of
size $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular,
we analyze computational efficient algorithms proposed by the same author, to
partition data into two groups approximately according to their population of
origin given a small sample. This work is motivated by the application of
clustering individuals according to their population of origin using $p$
markers, when the divergence between any two of the populations is small. We
build upon the semidefinite relaxation of an integer quadratic program that is
formulated essentially as finding the maximum cut on a graph, where edge
weights in the cut represent dissimilarity scores between two nodes based on
their $p$ features. Here we use $\Delta^2 :=p \gamma$ to denote the $\ell_2^2$
distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$
$\in$ $\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between
$n, p, \gamma$ in the sense that partial recovery (success rate $&amp;lt; 100\%$) is
feasible once the signal to noise ratio $s^2 := \min\{np \gamma^2, \Delta^2\}$
is lower bounded by a constant. Importantly, we prove that the
misclassification error decays exponentially with respect to the SNR $s^2$.
This result was introduced earlier without a full proof. We therefore present
the full proof in the present work. Finally, for balanced partitions, we
consider a variant of the SDP1, and show that the new estimator has a superb
debiasing property. This is novel to the best of our knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuheng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10931">
<title>Forecasting Cryptocurrency Staking Rewards. (arXiv:2401.10931v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2401.10931</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores a relatively unexplored area of predicting
cryptocurrency staking rewards, offering potential insights to researchers and
investors. We investigate two predictive methodologies: a) a straightforward
sliding-window average, and b) linear regression models predicated on
historical data. The findings reveal that ETH staking rewards can be forecasted
with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day
look-aheads respectively, using a 7-day sliding-window average approach.
Additionally, we discern diverse prediction accuracies across various
cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is
identified as superior to the moving-window average for perdicting in the short
term for XTZ and ATOM. The results underscore the generally stable and
predictable nature of staking rewards for most assets, with MATIC presenting a
noteworthy exception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sauren Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Katharaki_A/0/1/0/all/0/1&quot;&gt;Apoorva Hathi Katharaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Krishnamachari_B/0/1/0/all/0/1&quot;&gt;Bhaskar Krishnamachari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rajarshi Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10938">
<title>Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10938</link>
<description rdf:parseType="Literal">&lt;p&gt;EXplainable AI has received significant attention in recent years. Machine
learning models often operate as black boxes, lacking explainability and
transparency while supporting decision-making processes. Local post-hoc
explainability queries attempt to answer why individual inputs are classified
in a certain way by a given model. While there has been important work on
counterfactual explanations, less attention has been devoted to semifactual
ones. In this paper, we focus on local post-hoc explainability queries within
the semifactual `even-if&apos; thinking and their computational complexity among
different classes of models, and show that both linear and tree-based models
are strictly more interpretable than neural networks. After this, we introduce
a preference-based framework that enables users to personalize explanations
based on their preferences, both in the case of semifactuals and
counterfactuals, enhancing interpretability and user-centricity. Finally, we
explore the complexity of several interpretability problems in the proposed
preference-based framework and provide algorithms for polynomial cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfano_G/0/1/0/all/0/1&quot;&gt;Gianvincenzo Alfano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greco_S/0/1/0/all/0/1&quot;&gt;Sergio Greco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandaglio_D/0/1/0/all/0/1&quot;&gt;Domenico Mandaglio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisi_F/0/1/0/all/0/1&quot;&gt;Francesco Parisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahbazian_R/0/1/0/all/0/1&quot;&gt;Reza Shahbazian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trubitsyna_I/0/1/0/all/0/1&quot;&gt;Irina Trubitsyna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10940">
<title>RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.10940</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of information proliferation, discerning the credibility of news
content poses an ever-growing challenge. This paper introduces RELIANCE, a
pioneering ensemble learning system designed for robust information and fake
news credibility evaluation. Comprising five diverse base models, including
Support Vector Machine (SVM), naive Bayes, logistic regression, random forest,
and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs
an innovative approach to integrate their strengths, harnessing the collective
intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the
superiority of RELIANCE over individual models, indicating its efficacy in
distinguishing between credible and non-credible information sources. RELIANCE,
also surpasses baseline models in information and news credibility assessment,
establishing itself as an effective solution for evaluating the reliability of
information sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1&quot;&gt;Majid Ramezani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammad_Shahi_H/0/1/0/all/0/1&quot;&gt;Hamed Mohammad-Shahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daliry_M/0/1/0/all/0/1&quot;&gt;Mahshid Daliry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1&quot;&gt;Soroor Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asghari_A/0/1/0/all/0/1&quot;&gt;Amir-Hosein Asghari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10941">
<title>Crowd-PrefRL: Preference-Based Reward Learning from Crowds. (arXiv:2401.10941v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.10941</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference-based reinforcement learning (RL) provides a framework to train
agents using human feedback through pairwise preferences over pairs of
behaviors, enabling agents to learn desired behaviors when it is difficult to
specify a numerical reward function. While this paradigm leverages human
feedback, it currently treats the feedback as given by a single human user.
Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of
users) in a robust manner remains a challenge, and the problem of training RL
agents using feedback from multiple human users remains understudied. In this
work, we introduce Crowd-PrefRL, a framework for performing preference-based RL
leveraging feedback from crowds. This work demonstrates the viability of
learning reward functions from preference feedback provided by crowds of
unknown expertise and reliability. Crowd-PrefRL not only robustly aggregates
the crowd preference feedback, but also estimates the reliability of each user
within the crowd using only the (noisy) crowdsourced preference comparisons.
Most importantly, we show that agents trained with Crowd-PrefRL outperform
agents trained with majority-vote preferences or preferences from any
individual user in most cases, especially when the spread of user error rates
among the crowd is large. Results further suggest that our method can identify
minority viewpoints within the crowd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhan_D/0/1/0/all/0/1&quot;&gt;David Chhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novoseller_E/0/1/0/all/0/1&quot;&gt;Ellen Novoseller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawhern_V/0/1/0/all/0/1&quot;&gt;Vernon J. Lawhern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10942">
<title>Machine Unlearning for Recommendation Systems: An Insight. (arXiv:2401.10942v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.10942</link>
<description rdf:parseType="Literal">&lt;p&gt;This review explores machine unlearning (MUL) in recommendation systems,
addressing adaptability, personalization, privacy, and bias challenges. Unlike
traditional models, MUL dynamically adjusts system knowledge based on shifts in
user preferences and ethical considerations. The paper critically examines
MUL&apos;s basics, real-world applications, and challenges like algorithmic
transparency. It sifts through literature, offering insights into how MUL could
transform recommendations, discussing user trust, and suggesting paths for
future research in responsible and user-focused artificial intelligence (AI).
The document guides researchers through challenges involving the trade-off
between personalization and privacy, encouraging contributions to meet
practical demands for targeted data removal. Emphasizing MUL&apos;s role in secure
and adaptive machine learning, the paper proposes ways to push its boundaries.
The novelty of this paper lies in its exploration of the limitations of the
methods, which highlights exciting prospects for advancing the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachdeva_B/0/1/0/all/0/1&quot;&gt;Bhavika Sachdeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathee_H/0/1/0/all/0/1&quot;&gt;Harshita Rathee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sristi/0/1/0/all/0/1&quot;&gt;Sristi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Arun Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wydmanski_W/0/1/0/all/0/1&quot;&gt;Witold Wydma&amp;#x144;ski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10945">
<title>Automatic dimensionality reduction of Twin-in-the-Loop Observers. (arXiv:2401.10945v1 [cs.SY])</title>
<link>http://arxiv.org/abs/2401.10945</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art vehicle dynamics estimation techniques usually share one
common drawback: each variable to estimate is computed with an independent,
simplified filtering module. These modules run in parallel and need to be
calibrated separately. To solve this issue, a unified Twin-in-the-Loop (TiL)
Observer architecture has recently been proposed: the classical simplified
control-oriented vehicle model in the estimators is replaced by a full-fledged
vehicle simulator, or digital twin (DT). The states of the DT are corrected in
real time with a linear time invariant output error law. Since the simulator is
a black-box, no explicit analytical formulation is available, hence classical
filter tuning techniques cannot be used. Due to this reason, Bayesian
Optimization will be used to solve a data-driven optimization problem to tune
the filter. Due to the complexity of the DT, the optimization problem is
high-dimensional. This paper aims to find a procedure to tune the
high-complexity observer by lowering its dimensionality. In particular, in this
work we will analyze both a supervised and an unsupervised learning approach.
The strategies have been validated for speed and yaw-rate estimation on
real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delcaro_G/0/1/0/all/0/1&quot;&gt;Giacomo Delcaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dettu_F/0/1/0/all/0/1&quot;&gt;Federico Dett&amp;#xf9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Formentin_S/0/1/0/all/0/1&quot;&gt;Simone Formentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savaresi_S/0/1/0/all/0/1&quot;&gt;Sergio Matteo Savaresi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10949">
<title>The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning. (arXiv:2401.10949v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.10949</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the integration of optimal transport (OT) theory with
multi-agent reinforcement learning (MARL). This integration uses OT to handle
distributions and transportation problems to enhance the efficiency,
coordination, and adaptability of MARL. There are five key areas where OT can
impact MARL: (1) policy alignment, where OT&apos;s Wasserstein metric is used to
align divergent agent strategies towards unified goals; (2) distributed
resource management, employing OT to optimize resource allocation among agents;
(3) addressing non-stationarity, using OT to adapt to dynamic environmental
shifts; (4) scalable multi-agent learning, harnessing OT for decomposing
large-scale learning objectives into manageable tasks; and (5) enhancing energy
efficiency, applying OT principles to develop sustainable MARL systems. This
paper articulates how the synergy between OT and MARL can address scalability
issues, optimize resource distribution, align agent policies in cooperative
environments, and ensure adaptability in dynamically changing conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baheri_A/0/1/0/all/0/1&quot;&gt;Ali Baheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_a/0/1/0/all/0/1&quot;&gt;and Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10962">
<title>One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10962</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual fine-tuning has garnered significant attention with the rise of
pre-trained vision models. The current prevailing method, full fine-tuning,
suffers from the issue of knowledge forgetting as it focuses solely on fitting
the downstream training set. In this paper, we propose a novel weight
rollback-based fine-tuning method called OLOR (One step Learning, One step
Review). OLOR combines fine-tuning with optimizers, incorporating a weight
rollback term into the weight update term at each step. This ensures
consistency in the weight range of upstream and downstream models, effectively
mitigating knowledge forgetting and enhancing fine-tuning performance. In
addition, a layer-wise penalty is presented to employ penalty decay and the
diversified decay rate to adjust the weight rollback levels of layers for
adapting varying downstream tasks. Through extensive experiments on various
tasks such as image classification, object detection, semantic segmentation,
and instance segmentation, we demonstrate the general applicability and
state-of-the-art performance of our proposed OLOR. Code is available at
https://github.com/rainbow-xiao/OLOR-AAAI-2024.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiankun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xuesong Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10967">
<title>HOSC: A Periodic Activation Function for Preserving Sharp Features in Implicit Neural Representations. (arXiv:2401.10967v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.10967</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently proposed methods for implicitly representing signals such as images,
scenes, or geometries using coordinate-based neural network architectures often
do not leverage the choice of activation functions, or do so only to a limited
extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC),
a novel activation function with a controllable sharpness parameter. Unlike any
previous activations, HOSC has been specifically designed to better capture
sudden changes in the input signal, and hence sharp or acute features of the
underlying data, as well as smooth low-frequency transitions. Due to its
simplicity and modularity, HOSC offers a plug-and-play functionality that can
be easily incorporated into any existing method employing a neural network as a
way of implicitly representing a signal. We benchmark HOSC against other
popular activations in an array of general tasks, empirically showing an
improvement in the quality of obtained representations, provide the
mathematical motivation behind the efficacy of HOSC, and discuss its
limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serrano_D/0/1/0/all/0/1&quot;&gt;Danzel Serrano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szymkowiak_J/0/1/0/all/0/1&quot;&gt;Jakub Szymkowiak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musialski_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Musialski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10972">
<title>Clustering Molecular Energy Landscapes by Adaptive Network Embedding. (arXiv:2401.10972v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.10972</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to efficiently explore the chemical space of all possible small
molecules, a common approach is to compress the dimension of the system to
facilitate downstream machine learning tasks. Towards this end, we present a
data driven approach for clustering potential energy landscapes of molecular
structures by applying recently developed Network Embedding techniques, to
obtain latent variables defined through the embedding function. To scale up the
method, we also incorporate an entropy sensitive adaptive scheme for
hierarchical sampling of the energy landscape, based on Metadynamics and
Transition Path Theory. By taking into account the kinetic information implied
by a system&apos;s energy landscape, we are able to interpret dynamical node-node
relationships in reduced dimensions. We demonstrate the framework through
Lennard-Jones (LJ) clusters and a human DNA sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mercurio_P/0/1/0/all/0/1&quot;&gt;Paula Mercurio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Di Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10973">
<title>T2MAC: Targeted and Trusted Multi-Agent Communication through Selective Engagement and Evidence-Driven Integration. (arXiv:2401.10973v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.10973</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication stands as a potent mechanism to harmonize the behaviors of
multiple agents. However, existing works primarily concentrate on broadcast
communication, which not only lacks practicality, but also leads to information
redundancy. This surplus, one-fits-all information could adversely impact the
communication efficiency. Furthermore, existing works often resort to basic
mechanisms to integrate observed and received information, impairing the
learning process. To tackle these difficulties, we propose Targeted and Trusted
Multi-Agent Communication (T2MAC), a straightforward yet effective method that
enables agents to learn selective engagement and evidence-driven integration.
With T2MAC, agents have the capability to craft individualized messages,
pinpoint ideal communication windows, and engage with reliable partners,
thereby refining communication efficiency. Following the reception of messages,
the agents integrate information observed and received from different sources
at an evidence level. This process enables agents to collectively use evidence
garnered from multiple perspectives, fostering trusted and cooperative
behaviors. We evaluate our method on a diverse set of cooperative multi-agent
tasks, with varying difficulties, involving different scales and ranging from
Hallway, MPE to SMAC. The experiments indicate that the proposed model not only
surpasses the state-of-the-art methods in terms of cooperative performance and
communication efficiency, but also exhibits impressive generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chuxiong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1&quot;&gt;Zehua Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiabao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10989">
<title>Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.10989</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational families with full-rank covariance approximations are known not
to work well in black-box variational inference (BBVI), both empirically and
theoretically. In fact, recent computational complexity results for BBVI have
established that full-rank variational families scale poorly with the
dimensionality of the problem compared to e.g. mean field families. This is
particularly critical to hierarchical Bayesian models with local variables;
their dimensionality increases with the size of the datasets. Consequently, one
gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on
the dataset size $N$. In this paper, we explore a theoretical middle ground
between mean-field variational families and full-rank families: structured
variational families. We rigorously prove that certain scale matrix structures
can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better
scaling with respect to $N$. We empirically verify our theoretical results on
large-scale hierarchical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Joohwan Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyurae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Chang Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Jacob R. Gardner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11016">
<title>Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models. (arXiv:2401.11016v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11016</link>
<description rdf:parseType="Literal">&lt;p&gt;A common theory of choice posits that individuals make choices in a two-step
process, first selecting some subset of the alternatives to consider before
making a selection from the resulting consideration set. However, inferring
unobserved consideration sets (or item consideration probabilities) in this
&quot;consider then choose&quot; setting poses significant challenges, because even
simple models of consideration with strong independence assumptions are not
identifiable, even if item utilities are known. We consider a natural extension
of consider-then-choose models to a top-$k$ ranking setting, where we assume
rankings are constructed according to a Plackett-Luce model after sampling a
consideration set. While item consideration probabilities remain non-identified
in this setting, we prove that knowledge of item utilities allows us to infer
bounds on the relative sizes of consideration probabilities. Additionally,
given a condition on the expected consideration set size, we derive absolute
upper and lower bounds on item consideration probabilities. We also provide
algorithms to tighten those bounds on consideration probabilities by
propagating inferred constraints. Thus, we show that we can learn useful
information about consideration probabilities despite not being able to
identify them precisely. We demonstrate our methods on a ranking dataset from a
psychology experiment with two different ranking tasks (one with fixed
consideration sets and one with unknown consideration sets). This combination
of data allows us to estimate utilities and then learn about unknown
consideration probabilities using our bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aoki_Sherwood_B/0/1/0/all/0/1&quot;&gt;Ben Aoki-Sherwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bregou_C/0/1/0/all/0/1&quot;&gt;Catherine Bregou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liben_Nowell_D/0/1/0/all/0/1&quot;&gt;David Liben-Nowell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomlinson_K/0/1/0/all/0/1&quot;&gt;Kiran Tomlinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Thomas Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11017">
<title>Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition. (arXiv:2401.11017v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.11017</link>
<description rdf:parseType="Literal">&lt;p&gt;Speaker embeddings carry valuable emotion-related information, which makes
them a promising resource for enhancing speech emotion recognition (SER),
especially with limited labeled data. Traditionally, it has been assumed that
emotion information is indirectly embedded within speaker embeddings, leading
to their under-utilization. Our study reveals a direct and useful link between
emotion and state-of-the-art speaker embeddings in the form of intra-speaker
clusters. By conducting a thorough clustering analysis, we demonstrate that
emotion information can be readily extracted from speaker embeddings. In order
to leverage this information, we introduce a novel contrastive pretraining
approach applied to emotion-unlabeled data for speech emotion recognition. The
proposed approach involves the sampling of positive and the negative examples
based on the intra-speaker clusters of speaker embeddings. The proposed
strategy, which leverages extensive emotion-unlabeled data, leads to a
significant improvement in SER performance, whether employed as a standalone
pretraining task or integrated into a multi-task pretraining setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ulgen_I/0/1/0/all/0/1&quot;&gt;Ismail Rasim Ulgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zongyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Busso_C/0/1/0/all/0/1&quot;&gt;Carlos Busso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1&quot;&gt;Berrak Sisman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11018">
<title>Communication Efficient and Provable Federated Unlearning. (arXiv:2401.11018v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11018</link>
<description rdf:parseType="Literal">&lt;p&gt;We study federated unlearning, a novel problem to eliminate the impact of
specific clients or data points on the global model learned via federated
learning (FL). This problem is driven by the right to be forgotten and the
privacy challenges in FL. We introduce a new framework for exact federated
unlearning that meets two essential criteria: \textit{communication efficiency}
and \textit{exact unlearning provability}. To our knowledge, this is the first
work to tackle both aspects coherently. We start by giving a rigorous
definition of \textit{exact} federated unlearning, which guarantees that the
unlearned model is statistically indistinguishable from the one trained without
the deleted data. We then pinpoint the key property that enables fast exact
federated unlearning: total variation (TV) stability, which measures the
sensitivity of the model parameters to slight changes in the dataset.
Leveraging this insight, we develop a TV-stable FL algorithm called
\texttt{FATS}, which modifies the classical
\texttt{\underline{F}ed\underline{A}vg} algorithm for \underline{T}V
\underline{S}tability and employs local SGD with periodic averaging to lower
the communication round. We also design efficient unlearning algorithms for
\texttt{FATS} under two settings: client-level and sample-level unlearning. We
provide theoretical guarantees for our learning and unlearning algorithms,
proving that they achieve exact federated unlearning with reasonable
convergence rates for both the original and unlearned models. We empirically
validate our framework on 6 benchmark datasets, and show its superiority over
state-of-the-art methods in terms of accuracy, communication cost, computation
cost, and unlearning efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Youming Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng-Long Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Miao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongxiao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiuzhen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11030">
<title>Exploring Highly Quantised Neural Networks for Intrusion Detection in Automotive CAN. (arXiv:2401.11030v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.11030</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicles today comprise intelligent systems like connected autonomous driving
and advanced driving assistance systems (ADAS) to enhance the driving
experience, which is enabled through increased connectivity to infrastructure
and fusion of information from different sensing modes. However, the rising
connectivity coupled with the legacy network architecture within vehicles can
be exploited for launching active and passive attacks on critical vehicle
systems and directly affecting the safety of passengers. Machine learning-based
intrusion detection models have been shown to successfully detect multiple
targeted attack vectors in recent literature, whose deployments are enabled
through quantised neural networks targeting low-power platforms. Multiple
models are often required to simultaneously detect multiple attack vectors,
increasing the area, (resource) cost, and energy consumption. In this paper, we
present a case for utilising custom-quantised MLP&apos;s (CQMLP) as a multi-class
classification model, capable of detecting multiple attacks from the benign
flow of controller area network (CAN) messages. The specific quantisation and
neural architecture are determined through a joint design space exploration,
resulting in our choice of the 2-bit precision and the n-layer MLP. Our 2-bit
version is trained using Brevitas and optimised as a dataflow hardware model
through the FINN toolflow from AMD/Xilinx, targeting an XCZU7EV device. We show
that the 2-bit CQMLP model, when integrated as the IDS, can detect malicious
attack messages (DoS, fuzzing, and spoofing attack) with a very high accuracy
of 99.9%, on par with the state-of-the-art methods in the literature.
Furthermore, the dataflow model can perform line rate detection at a latency of
0.11 ms from message reception while consuming 0.23 mJ/inference, making it
ideally suited for integration with an ECU in critical CAN networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_S/0/1/0/all/0/1&quot;&gt;Shashwat Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanker_S/0/1/0/all/0/1&quot;&gt;Shreejith Shanker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11037">
<title>Equivariant Graph Neural Operator for Modeling 3D Dynamics. (arXiv:2401.11037v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11037</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the complex three-dimensional (3D) dynamics of relational systems is
an important problem in the natural sciences, with applications ranging from
molecular simulations to particle mechanics. Machine learning methods have
achieved good success by learning graph neural networks to model spatial
interactions. However, these approaches do not faithfully capture temporal
correlations since they only model next-step predictions. In this work, we
propose Equivariant Graph Neural Operator (EGNO), a novel and principled method
that directly models dynamics as trajectories instead of just next-step
prediction. Different from existing methods, EGNO explicitly learns the
temporal evolution of 3D dynamics where we formulate the dynamics as a function
over time and learn neural operators to approximate it. To capture the temporal
correlations while keeping the intrinsic SE(3)-equivariance, we develop
equivariant temporal convolutions parameterized in the Fourier space and build
EGNO by stacking the Fourier layers over equivariant networks. EGNO is the
first operator learning framework that is capable of modeling solution dynamics
functions over time while retaining 3D equivariance. Comprehensive experiments
in multiple domains, including particle simulations, human motion capture, and
molecular dynamics, demonstrate the significantly superior performance of EGNO
against existing methods, thanks to the equivariant temporal modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiaqi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1&quot;&gt;Aaron Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1&quot;&gt;Jean Kossaifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1&quot;&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1&quot;&gt;Kamyar Azizzadenesheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11044">
<title>The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making. (arXiv:2401.11044v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11044</link>
<description rdf:parseType="Literal">&lt;p&gt;The applicability of widely adopted machine learning (ML) methods to
classification is circumscribed by the imperatives of explicability and
uncertainty, particularly evident in domains such as healthcare, behavioural
sciences, and finances, wherein accountability assumes priority. Recently,
Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the
ability to perform classification in such domains, by developing a data
abstraction protocol using a ROC curve-based method. This paper focuses on
column-wise data transformations called abstractions, which are crucial for
SaNDA&apos;s classification process and explores alternative abstractions protocols,
such as constant binning and quantiles. The best-performing methods have been
compared against Random Forest as a baseline for explainable methods. The
results suggests that SaNDA can be a viable substitute for Random Forest when
data is incomplete, even with minimal missing values. It consistently maintains
high accuracy even when half of the dataset is missing, unlike Random Forest
which experiences a significant decline in accuracy under similar conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capala_K/0/1/0/all/0/1&quot;&gt;Karol Capa&amp;#x142;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tworek_P/0/1/0/all/0/1&quot;&gt;Paulina Tworek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sousa_J/0/1/0/all/0/1&quot;&gt;Jose Sousa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11074">
<title>On The Temporal Domain of Differential Equation Inspired Graph Neural Networks. (arXiv:2401.11074v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11074</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling
complex relationships in graph-structured data. A recent innovation in this
field is the family of Differential Equation-Inspired Graph Neural Networks
(DE-GNNs), which leverage principles from continuous dynamical systems to model
information flow on graphs with built-in properties such as feature smoothing
or preservation. However, existing DE-GNNs rely on first or second-order
temporal dependencies. In this paper, we propose a neural extension to those
pre-defined temporal dependencies. We show that our model, called TDE-GNN, can
capture a wide range of temporal dynamics that go beyond typical first or
second-order methods, and provide use cases where existing temporal models are
challenged. We demonstrate the benefit of learning the temporal dependencies
using our method rather than using pre-defined temporal dynamics on several
graph benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1&quot;&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1&quot;&gt;Eldad Haber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1&quot;&gt;Eran Treister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11081">
<title>Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions. (arXiv:2401.11081v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11081</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the rise of privacy concerns, in many practical applications the
training data is aggregated before being shared with the learner, in order to
protect privacy of users&apos; sensitive responses. In an aggregate learning
framework, the dataset is grouped into bags of samples, where each bag is
available only with an aggregate response, providing a summary of individuals&apos;
responses in that bag. In this paper, we study two natural loss functions for
learning from aggregate responses: bag-level loss and the instance-level loss.
In the former, the model is learnt by minimizing a loss between aggregate
responses and aggregate model predictions, while in the latter the model aims
to fit individual predictions to the aggregate responses. In this work, we show
that the instance-level loss can be perceived as a regularized form of the
bag-level loss. This observation lets us compare the two approaches with
respect to bias and variance of the resulting estimators, and introduce a novel
interpolating estimator which combines the two approaches. For linear
regression tasks, we provide a precise characterization of the risk of the
interpolating estimator in an asymptotic regime where the size of the training
set grows in proportion to the features dimension. Our analysis allows us to
theoretically understand the effect of different factors, such as bag size on
the model prediction risk. In addition, we propose a mechanism for
differentially private learning from aggregate responses and derive the optimal
bag size in terms of prediction risk-privacy trade-off. We also carry out
thorough experiments to corroborate our theory and show the efficacy of the
interpolating estimator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javanmard_A/0/1/0/all/0/1&quot;&gt;Adel Javanmard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1&quot;&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badanidiyuru_A/0/1/0/all/0/1&quot;&gt;Ashwinkumar Badanidiyuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1&quot;&gt;Gang Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11098">
<title>Neural auto-designer for enhanced quantum kernels. (arXiv:2401.11098v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2401.11098</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum kernels hold great promise for offering computational advantages over
classical learners, with the effectiveness of these kernels closely tied to the
design of the quantum feature map. However, the challenge of designing
effective quantum feature maps for real-world datasets, particularly in the
absence of sufficient prior information, remains a significant obstacle. In
this study, we present a data-driven approach that automates the design of
problem-specific quantum feature maps. Our approach leverages feature-selection
techniques to handle high-dimensional data on near-term quantum machines with
limited qubits, and incorporates a deep neural predictor to efficiently
evaluate the performance of various candidate quantum kernels. Through
extensive numerical simulations on different datasets, we demonstrate the
superiority of our proposal over prior methods, especially for the capability
of eliminating the kernel concentration issue and identifying the feature map
with prediction advantages. Our work not only unlocks the potential of quantum
kernels for enhancing real-world tasks but also highlights the substantial role
of deep learning in advancing quantum machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lei_C/0/1/0/all/0/1&quot;&gt;Cong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Mi_P/0/1/0/all/0/1&quot;&gt;Peng Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11103">
<title>Efficient Data Shapley for Weighted Nearest Neighbor Algorithms. (arXiv:2401.11103v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2401.11103</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to address an open problem in data valuation literature
concerning the efficient computation of Data Shapley for weighted $K$ nearest
neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label
KNN with discretized weights as the utility function, we reframe the
computation of WKNN-Shapley into a counting problem and introduce a
quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the
best result from existing literature. We develop a deterministic approximation
algorithm that further improves computational efficiency while maintaining the
key fairness properties of the Shapley value. Through extensive experiments, we
demonstrate WKNN-Shapley&apos;s computational efficiency and its superior
performance in discerning data quality compared to its unweighted counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiachen T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1&quot;&gt;Prateek Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Ruoxi Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11105">
<title>Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study. (arXiv:2401.11105v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.11105</link>
<description rdf:parseType="Literal">&lt;p&gt;Collecting relevant and high-quality data is integral to the development of
effective Software Vulnerability (SV) prediction models. Most of the current SV
datasets rely on SV-fixing commits to extract vulnerable functions and lines.
However, none of these datasets have considered latent SVs existing between the
introduction and fix of the collected SVs. There is also little known about the
usefulness of these latent SVs for SV prediction. To bridge these gaps, we
conduct a large-scale study on the latent vulnerable functions in two commonly
used SV datasets and their utilization for function-level and line-level SV
predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more
than 100k latent vulnerable functions in the studied datasets. We find that
these latent functions can increase the number of SVs by 4x on average and
correct up to 5k mislabeled functions, yet they have a noise level of around
6%. Despite the noise, we show that the state-of-the-art SV prediction model
can significantly benefit from such latent SVs. The improvements are up to
24.5% in the performance (F1-Score) of function-level SV predictions and up to
67% in the effectiveness of localizing vulnerable lines. Overall, our study
presents the first promising step toward the use of latent SVs to improve the
quality of SV datasets and enhance the performance of SV prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Triet H. M. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoning Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1&quot;&gt;M. Ali Babar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11110">
<title>VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE. (arXiv:2401.11110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.11110</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised video object learning seeks to decompose video scenes into
structural object representations without any supervision from depth, optical
flow, or segmentation. We present VONet, an innovative approach that is
inspired by MONet. While utilizing a U-Net architecture, VONet employs an
efficient and effective parallel attention inference process, generating
attention masks for all slots simultaneously. Additionally, to enhance the
temporal consistency of each mask across consecutive video frames, VONet
develops an object-wise sequential VAE framework. The integration of these
innovative encoder-side techniques, in conjunction with an expressive
transformer-based decoder, establishes VONet as the leading unsupervised method
for object learning across five MOVI datasets, encompassing videos of diverse
complexities. Code is available at https://github.com/hnyu/vonet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haonan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11113">
<title>SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11113</link>
<description rdf:parseType="Literal">&lt;p&gt;Sleep behavior significantly impacts health and acts as an indicator of
physical and mental well-being. Monitoring and predicting sleep behavior with
ubiquitous sensors may therefore assist in both sleep management and tracking
of related health conditions. While sleep behavior depends on, and is reflected
in the physiology of a person, it is also impacted by external factors such as
digital media usage, social network contagion, and the surrounding weather. In
this work, we propose SPAND (Sleep Prediction Architecture using Network
Dynamics), a system that exploits social contagion in sleep behavior through
graph networks and integrates it with physiological and phone data extracted
from ubiquitous mobile and wearable devices for predicting next-day sleep
labels about sleep duration. Our architecture overcomes the limitations of
large-scale graphs containing connections irrelevant to sleep behavior by
devising an attention mechanism. The extensive experimental evaluation
highlights the improvement provided by incorporating social networks in the
model. Additionally, we conduct robustness analysis to demonstrate the system&apos;s
performance in real-life conditions. The outcomes affirm the stability of SPAND
against perturbations in input data. Further analyses emphasize the
significance of network topology in prediction performance revealing that users
with higher eigenvalue centrality are more vulnerable to data perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalid_M/0/1/0/all/0/1&quot;&gt;Maryam Khalid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klerman_E/0/1/0/all/0/1&quot;&gt;Elizabeth B. Klerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mchill_A/0/1/0/all/0/1&quot;&gt;Andrew W. Mchill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_A/0/1/0/all/0/1&quot;&gt;Andrew J. K. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1&quot;&gt;Akane Sano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11118">
<title>Meta Reinforcement Learning for Strategic IoT Deployments Coverage in Disaster-Response UAV Swarms. (arXiv:2401.11118v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11118</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the
attention of researchers in academia and industry for their potential use in
critical emergency applications, such as providing wireless services to ground
users and collecting data from areas affected by disasters, due to their
advantages in terms of maneuverability and movement flexibility. The UAVs&apos;
limited resources, energy budget, and strict mission completion time have posed
challenges in adopting UAVs for these applications. Our system model considers
a UAV swarm that navigates an area collecting data from ground IoT devices
focusing on providing better service for strategic locations and allowing UAVs
to join and leave the swarm (e.g., for recharging) in a dynamic way. In this
work, we introduce an optimization model with the aim of minimizing the total
energy consumption and provide the optimal path planning of UAVs under the
constraints of minimum completion time and transmit power. The formulated
optimization is NP-hard making it not applicable for real-time decision making.
Therefore, we introduce a light-weight meta-reinforcement learning solution
that can also cope with sudden changes in the environment through fast
convergence. We conduct extensive simulations and compare our approach to three
state-of-the-art learning models. Our simulation results prove that our
introduced approach is better than the three state-of-the-art algorithms in
providing coverage to strategic locations with fast convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1&quot;&gt;Marwan Dhuheir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1&quot;&gt;Aiman Erbad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1&quot;&gt;Ala Al-Fuqaha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11124">
<title>EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions. (arXiv:2401.11124v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.11124</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask learning (MTL) has gained prominence for its ability to jointly
predict multiple tasks, achieving better per-task performance while using fewer
per-task model parameters than single-task learning. More recently,
decoder-focused architectures have considerably improved multitask performance
by refining task predictions using the features of other related tasks.
However, most of these refinement methods fail to simultaneously capture local
and global task-specific representations, as well as cross-task patterns in a
parameter-efficient manner. In this paper, we introduce the Efficient Multitask
Affinity Learning Network (EMA-Net), which is a lightweight framework that
enhances the task refinement capabilities of multitask networks. EMA-Net
adeptly captures local, global, and cross-task interactions using our novel
Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in
its ability to manipulate task affinity matrices in a manner that is optimally
suited to apply parameter-efficient grouped convolutions without worrying about
information loss. Our results show that we achieve state-of-the-art MTL
performance for CNN-based decoder-focused models while using substantially
fewer model parameters. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinodinos_D/0/1/0/all/0/1&quot;&gt;Dimitrios Sinodinos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11126">
<title>CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive Attackers for Security Applications. (arXiv:2401.11126v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.11126</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensemble defenses, are widely employed in various security-related
applications to enhance model performance and robustness. The widespread
adoption of these techniques also raises many questions: Are general ensembles
defenses guaranteed to be more robust than individuals? Will stronger adaptive
attacks defeat existing ensemble defense strategies as the cybersecurity arms
race progresses? Can ensemble defenses achieve adversarial robustness to
different types of attacks simultaneously and resist the continually adjusted
adaptive attacks? Unfortunately, these critical questions remain unresolved as
there are no platforms for comprehensive evaluation of ensemble adversarial
attacks and defenses in the cybersecurity domain. In this paper, we propose a
general Cybersecurity Adversarial Robustness Evaluation (CARE) platform aiming
to bridge this gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hangsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jinsong Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11130">
<title>Identification and Estimation of Conditional Average Partial Causal Effects via Instrumental Variable. (arXiv:2401.11130v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11130</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been considerable recent interest in estimating heterogeneous
causal effects. In this paper, we introduce conditional average partial causal
effects (CAPCE) to reveal the heterogeneity of causal effects with continuous
treatment. We provide conditions for identifying CAPCE in an instrumental
variable setting. We develop three families of CAPCE estimators: sieve,
parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze
their statistical properties. We illustrate the proposed CAPCE estimators on
synthetic and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawakami_Y/0/1/0/all/0/1&quot;&gt;Yuta Kawakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuroki_M/0/1/0/all/0/1&quot;&gt;Manabu Kuroki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jin Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11143">
<title>Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11143</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM&apos;s
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannides_G/0/1/0/all/0/1&quot;&gt;Georgios Ioannides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elkins_A/0/1/0/all/0/1&quot;&gt;Aaron Elkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11145">
<title>Document Set Expansion with Positive-Unlabeled Learning: A Density Estimation-based Approach. (arXiv:2401.11145v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11145</link>
<description rdf:parseType="Literal">&lt;p&gt;Document set expansion aims to identify relevant documents from a large
collection based on a small set of documents that are on a fine-grained topic.
Previous work shows that PU learning is a promising method for this task.
However, some serious issues remain unresolved, i.e. typical challenges that PU
methods suffer such as unknown class prior and imbalanced data, and the need
for transductive experimental settings. In this paper, we propose a novel PU
learning framework based on density estimation, called puDE, that can handle
the above issues. The advantage of puDE is that it neither constrained to the
SCAR assumption and nor require any class prior knowledge. We demonstrate the
effectiveness of the proposed method using a series of real-world datasets and
conclude that our method is a better alternative for the DSE task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiuyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuanjie Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yushan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1&quot;&gt;Mark Stevenson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11174">
<title>Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.11174</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach enables a multi-level comprehension
of surgical activities, encompassing long-term tasks such as surgical phases
and steps recognition and short-term tasks including surgical instrument
segmentation and atomic visual actions detection. To exploit our proposed
benchmark, we introduce the Transformers for Actions, Phases, Steps, and
Instrument Segmentation (TAPIS) model, a general architecture that combines a
global video feature extractor with localized region proposals from an
instrument segmentation model to tackle the multi-granularity of our benchmark.
Through extensive experimentation, we demonstrate the impact of including
segmentation annotations in short-term recognition tasks, highlight the varying
granularity requirements of each task, and establish TAPIS&apos;s superiority over
previously proposed baselines and conventional CNN-based models. Additionally,
we validate the robustness of our method across multiple public benchmarks,
confirming the reliability and applicability of our dataset. This work
represents a significant step forward in Endoscopic Vision, offering a novel
and comprehensive framework for future research towards a holistic
understanding of surgical procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Alejandra P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1&quot;&gt;Isabela Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Aparicio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dessevres_E/0/1/0/all/0/1&quot;&gt;Eug&amp;#xe9;nie Dessevres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pena_S/0/1/0/all/0/1&quot;&gt;Sebasti&amp;#xe1;n Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santander_J/0/1/0/all/0/1&quot;&gt;Jessica Santander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1&quot;&gt;Juan Ignacio Caicedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11176">
<title>Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\&apos;er-Rao Bound. (arXiv:2401.11176v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2401.11176</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern radar systems, precise target localization using azimuth and
velocity estimation is paramount. Traditional unbiased estimation methods have
leveraged gradient descent algorithms to reach the theoretical limits of the
Cram\&apos;er Rao Bound (CRB) for the error of the parameter estimates. In this
study, we present a data-driven neural network approach that outperforms these
traditional techniques, demonstrating improved accuracies in target azimuth and
velocity estimation. Using a representative simulated scenario, we show that
our proposed neural network model consistently achieves improved parameter
estimates due to its inherently biased nature, yielding a diminished mean
squared error (MSE). Our findings underscore the potential of employing deep
learning methods in radar systems, paving the way for more accurate
localization in cluttered and dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Venkatasubramanian_S/0/1/0/all/0/1&quot;&gt;Shyam Venkatasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gogineni_S/0/1/0/all/0/1&quot;&gt;Sandeep Gogineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bosung Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pezeshki_A/0/1/0/all/0/1&quot;&gt;Ali Pezeshki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rangaswamy_M/0/1/0/all/0/1&quot;&gt;Muralidhar Rangaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tarokh_V/0/1/0/all/0/1&quot;&gt;Vahid Tarokh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11188">
<title>Fast and Exact Enumeration of Deep Networks Partitions Regions. (arXiv:2401.11188v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11188</link>
<description rdf:parseType="Literal">&lt;p&gt;One fruitful formulation of Deep Networks (DNs) enabling their theoretical
study and providing practical guidelines to practitioners relies on Piecewise
Affine Splines. In that realm, a DN&apos;s input-mapping is expressed as per-region
affine mapping where those regions are implicitly determined by the model&apos;s
architecture and form a partition of their input space. That partition -- which
is involved in all the results spanned from this line of research -- has so far
only been computed on $2/3$-dimensional slices of the DN&apos;s input space or
estimated by random sampling. In this paper, we provide the first parallel
algorithm that does exact enumeration of the DN&apos;s partition regions. The
proposed algorithm enables one to finally assess the closeness of the commonly
employed approximations methods, e.g. based on random sampling of the DN input
space. One of our key finding is that if one is only interested in regions with
``large&apos;&apos; volume, then uniform sampling of the space is highly efficient, but
that if one is also interested in discovering the ``small&apos;&apos; regions of the
partition, then uniform sampling is exponentially costly with the DN&apos;s input
space dimension. On the other hand, our proposed method has complexity scaling
linearly with input dimension and the number of regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1&quot;&gt;Yann LeCun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11196">
<title>Machine learning based state observer for discrete time systems evolving on Lie groups. (arXiv:2401.11196v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.11196</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a machine learning based observer for systems evolving on
manifolds is designed such that the state of the observer is restricted to the
Lie group on which the system evolves. Conventional techniques involving
machine learning based observers on systems evolving on Lie groups involve
designing charts for the Lie group, training a machine learning based observer
for each chart, and switching between the trained models based on the state of
the system. We propose a novel deep learning based technique whose predictions
are restricted to a measure 0 subset of Euclidean space without using charts.
Using this network, we design an observer ensuring that the state of the
observer is restricted to the Lie group, and predicting the state using only
one trained algorithm. The deep learning network predicts an ``error term&apos;&apos; on
the Lie algebra of the Lie group, uses the map from the Lie algebra to the
group, and uses the group action and the present state to estimate the state at
the next epoch. This model being purely data driven does not require the model
of the system. The proposed algorithm provides a novel framework for
constraining the output of machine learning networks to a measure 0 subset of a
Euclidean space without chart specific training and without requiring
switching. We show the validity of this method using Monte Carlo simulations
performed of the rigid body rotation and translation system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shanbhag_S/0/1/0/all/0/1&quot;&gt;Soham Shanbhag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Dong Eui Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11199">
<title>Projected Belief Networks With Discriminative Alignment for Acoustic Event Classification: Rivaling State of the Art CNNs. (arXiv:2401.11199v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11199</link>
<description rdf:parseType="Literal">&lt;p&gt;The projected belief network (PBN) is a generative stochastic network with
tractable likelihood function based on a feed-forward neural network (FFNN).
The generative function operates by &quot;backing up&quot; through the FFNN. The PBN is
two networks in one, a FFNN that operates in the forward direction, and a
generative network that operates in the backward direction. Both networks
co-exist based on the same parameter set, have their own cost functions, and
can be separately or jointly trained. The PBN therefore has the potential to
possess the best qualities of both discriminative and generative classifiers.
To realize this potential, a separate PBN is trained on each class, maximizing
the generative likelihood function for the given class, while minimizing the
discriminative cost for the FFNN against &quot;all other classes&quot;. This technique,
called discriminative alignment (PBN-DA), aligns the contours of the likelihood
function to the decision boundaries and attains vastly improved classification
performance, rivaling that of state of the art discriminative networks. The
method may be further improved using a hidden Markov model (HMM) as a component
of the PBN, called PBN-DA-HMM. This paper provides a comprehensive treatment of
PBN, PBN-DA, and PBN-DA-HMM. In addition, the results of two new classification
experiments are provided. The first experiment uses air-acoustic events, and
the second uses underwater acoustic data consisting of marine mammal calls. In
both experiments, PBN-DA-HMM attains comparable or better performance as a
state of the art CNN, and attain a factor of two error reduction when combined
with the CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baggenstoss_P/0/1/0/all/0/1&quot;&gt;Paul M. Baggenstoss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilkinghoff_K/0/1/0/all/0/1&quot;&gt;Kevin Wilkinghoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govaers_F/0/1/0/all/0/1&quot;&gt;Felix Govaers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurth_F/0/1/0/all/0/1&quot;&gt;Frank Kurth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11202">
<title>PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11202</link>
<description rdf:parseType="Literal">&lt;p&gt;Training of modern large neural networks (NN) requires a combination of
parallelization strategies encompassing data, model, or optimizer sharding.
When strategies increase in complexity, it becomes necessary for partitioning
tools to be 1) expressive, allowing the composition of simpler strategies, and
2) predictable to estimate performance analytically. We present PartIR, our
design for a NN partitioning system. PartIR is focused on an incremental
approach to rewriting and is hardware-and-runtime agnostic. We present a simple
but powerful API for composing sharding strategies and a simulator to validate
them. The process is driven by high-level programmer-issued partitioning
tactics, which can be both manual and automatic. Importantly, the tactics are
specified separately from the model code, making them easy to change. We
evaluate PartIR on several different models to demonstrate its predictability,
expressibility, and ability to reach peak performance..
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabed_S/0/1/0/all/0/1&quot;&gt;Sami Alabed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrzaszcz_B/0/1/0/all/0/1&quot;&gt;Bart Chrzaszcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_J/0/1/0/all/0/1&quot;&gt;Juliana Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewe_D/0/1/0/all/0/1&quot;&gt;Dominik Grewe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maclaurin_D/0/1/0/all/0/1&quot;&gt;Dougal Maclaurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_J/0/1/0/all/0/1&quot;&gt;James Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natan_T/0/1/0/all/0/1&quot;&gt;Tom Natan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norman_T/0/1/0/all/0/1&quot;&gt;Tamara Norman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paszke_A/0/1/0/all/0/1&quot;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rink_N/0/1/0/all/0/1&quot;&gt;Norman A. Rink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaarschmidt_M/0/1/0/all/0/1&quot;&gt;Michael Schaarschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitdikov_T/0/1/0/all/0/1&quot;&gt;Timur Sitdikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swietlik_A/0/1/0/all/0/1&quot;&gt;Agnieszka Swietlik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wee_J/0/1/0/all/0/1&quot;&gt;Joel Wee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11215">
<title>Selecting Walk Schemes for Database Embedding. (arXiv:2401.11215v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11215</link>
<description rdf:parseType="Literal">&lt;p&gt;Machinery for data analysis often requires a numeric representation of the
input. Towards that, a common practice is to embed components of structured
data into a high-dimensional vector space. We study the embedding of the tuples
of a relational database, where existing techniques are often based on
optimization tasks over a collection of random walks from the database. The
focus of this paper is on the recent FoRWaRD algorithm that is designed for
dynamic databases, where walks are sampled by following foreign keys between
tuples. Importantly, different walks have different schemas, or &quot;walk schemes&quot;,
that are derived by listing the relations and attributes along the walk. Also
importantly, different walk schemes describe relationships of different natures
in the database. We show that by focusing on a few informative walk schemes, we
can obtain tuple embedding significantly faster, while retaining the quality.
We define the problem of scheme selection for tuple embedding, devise several
approaches and strategies for scheme selection, and conduct a thorough
empirical study of the performance over a collection of downstream tasks. Our
results confirm that with effective strategies for scheme selection, we can
obtain high-quality embeddings considerably (e.g., three times) faster,
preserve the extensibility to newly inserted tuples, and even achieve an
increase in the precision of some tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lubarsky_Y/0/1/0/all/0/1&quot;&gt;Yuval Lev Lubarsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonshoff_J/0/1/0/all/0/1&quot;&gt;Jan T&amp;#xf6;nshoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1&quot;&gt;Martin Grohe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimelfeld_B/0/1/0/all/0/1&quot;&gt;Benny Kimelfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11217">
<title>A Hybrid Approach of Transfer Learning and Physics-Informed Modeling: Improving Dissolved Oxygen Concentration Prediction in an Industrial Wastewater Treatment Plant. (arXiv:2401.11217v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11217</link>
<description rdf:parseType="Literal">&lt;p&gt;Constructing first principles models is a challenging task for nonlinear and
complex systems such as a wastewater treatment unit. In recent years,
data-driven models are widely used to overcome the complexity. However, they
often suffer from issues such as missing, low quality or noisy data. Transfer
learning is a solution for this issue where knowledge from another task is
transferred to target one to increase the prediction performance. In this work,
the objective is increasing the prediction performance of an industrial
wastewater treatment plant by transferring the knowledge of (i) an open-source
simulation model that captures the underlying physics of the process, albeit
with dissimilarities to the target plant, (ii) another industrial plant
characterized by noisy and limited data but located in the same refinery, and
(iii) the model in (ii) and making the objective function of the training
problem physics informed where the physics information derived from the
open-source model in (ii). The results have shown that test and validation
performance are improved up to 27% and 59%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koksal_E/0/1/0/all/0/1&quot;&gt;Ece S. Koksal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aydin_E/0/1/0/all/0/1&quot;&gt;Erdal Aydin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11235">
<title>TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly Detection with Inexact Supervision. (arXiv:2401.11235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11235</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series anomaly detection (TSAD) plays a vital role in various domains
such as healthcare, networks, and industry. Considering labels are crucial for
detection but difficult to obtain, we turn to TSAD with inexact supervision:
only series-level labels are provided during the training phase, while
point-level anomalies are predicted during the testing phase. Previous works
follow a traditional multi-instance learning (MIL) approach, which focuses on
encouraging high anomaly scores at individual time steps. However, time series
anomalies are not only limited to individual point anomalies, they can also be
collective anomalies, typically exhibiting abnormal patterns over subsequences.
To address the challenge of collective anomalies, in this paper, we propose a
tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to
divide the entire series into multiple nodes, where nodes at different levels
represent subsequences with different lengths. Then, the subsequence features
are extracted to determine the presence of collective anomalies. Finally, we
calculate point-level anomaly scores by aggregating features from nodes at
different levels. Experiments conducted on seven public datasets and eight
baselines demonstrate that TreeMIL achieves an average 32.3% improvement in F1-
score compared to previous state-of-the-art methods. The code is available at
https://github.com/fly-orange/TreeMIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shibo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shizhong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11237">
<title>Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11237</link>
<description rdf:parseType="Literal">&lt;p&gt;Some reinforcement learning (RL) algorithms can stitch pieces of experience
to solve a task never seen before during training. This oft-sought property is
one of the few ways in which RL methods based on dynamic-programming differ
from RL methods based on supervised-learning (SL). Yet, certain RL methods
based on off-the-shelf SL algorithms achieve excellent results without an
explicit mechanism for stitching; it remains unclear whether those methods
forgo this important stitching property. This paper studies this question for
the problems of achieving a target goal state and achieving a target return
value. Our main result is to show that the stitching property corresponds to a
form of combinatorial generalization: after training on a distribution of
(state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen
together in the training data. Our analysis shows that this sort of
generalization is different from i.i.d. generalization. This connection between
stitching and generalisation reveals why we should not expect SL-based RL
methods to perform stitching, even in the limit of large datasets and models.
Based on this analysis, we construct new datasets to explicitly test for this
property, revealing that SL-based methods lack this stitching property and
hence fail to perform combinatorial generalization. Nonetheless, the connection
between stitching and combinatorial generalisation also suggests a simple
remedy for improving generalisation in SL: data augmentation. We propose a
temporal data augmentation and demonstrate that adding it to SL-based methods
enables them to successfully complete tasks not seen together during training.
On a high level, this connection illustrates the importance of combinatorial
generalization for data efficiency in time-series data beyond tasks beyond RL,
like audio, video, or text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghugare_R/0/1/0/all/0/1&quot;&gt;Raj Ghugare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1&quot;&gt;Glen Berseth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1&quot;&gt;Benjamin Eysenbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11250">
<title>AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking. (arXiv:2401.11250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11250</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of feature selection in general machine learning (ML)
context, which is one of the most critical subjects in the field. Although,
there exist many feature selection methods, however, these methods face
challenges such as scalability, managing high-dimensional data, dealing with
correlated features, adapting to variable feature importance, and integrating
domain knowledge. To this end, we introduce the ``Adaptive Feature Selection
with Binary Masking&quot; (AFS-BM) which remedies these problems. AFS-BM achieves
this by joint optimization for simultaneous feature selection and model
training. In particular, we do the joint optimization and binary masking to
continuously adapt the set of features and model parameters during the training
process. This approach leads to significant improvements in model accuracy and
a reduction in computational requirements. We provide an extensive set of
experiments where we compare AFS-BM with the established feature selection
methods using well-known datasets from real-life competitions. Our results show
that AFS-BM makes significant improvement in terms of accuracy and requires
significantly less computational complexity. This is due to AFS-BM&apos;s ability to
dynamically adjust to the changing importance of features during the training
process, which an important contribution to the field. We openly share our code
for the replicability of our results and to facilitate further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turali_M/0/1/0/all/0/1&quot;&gt;Mehmet Y. Turali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorasdagi_M/0/1/0/all/0/1&quot;&gt;Mehmet E. Lorasdagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koc_A/0/1/0/all/0/1&quot;&gt;Ali T. Koc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozat_S/0/1/0/all/0/1&quot;&gt;Suleyman S. Kozat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11252">
<title>Automated Fusion of Multimodal Electronic Health Records for Better Medical Predictions. (arXiv:2401.11252v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11252</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread adoption of Electronic Health Record (EHR) systems in
healthcare institutes has generated vast amounts of medical data, offering
significant opportunities for improving healthcare services through deep
learning techniques. However, the complex and diverse modalities and feature
structures in real-world EHR data pose great challenges for deep learning model
design. To address the multi-modality challenge in EHR data, current approaches
primarily rely on hand-crafted model architectures based on intuition and
empirical experiences, leading to sub-optimal model architectures and limited
performance. Therefore, to automate the process of model design for mining EHR
data, we propose a novel neural architecture search (NAS) framework named
AutoFM, which can automatically search for the optimal model architectures for
encoding diverse input modalities and fusion strategies. We conduct thorough
experiments on real-world multi-modal EHR data and prediction tasks, and the
results demonstrate that our framework not only achieves significant
performance improvement over existing state-of-the-art methods but also
discovers meaningful network architectures effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Suhan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fenglong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11261">
<title>Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11261</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models (DMs) are a type of generative model that has a huge impact
on image synthesis and beyond. They achieve state-of-the-art generation results
in various generative tasks. A great diversity of conditioning inputs, such as
text or bounding boxes, are accessible to control the generation. In this work,
we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as
feature conditioning to guide the denoising process. Based on set theory, we
provide a comprehensive theoretical analysis that shows that conditional latent
distribution based on features and classes is significantly different, so that
conditional latent distribution on features produces fewer defect generations
than conditioning on classes. Two diffusion models conditioned on the Gaussian
mixture model are trained separately for comparison. Experiments support our
findings. A novel gradient function called the negative Gaussian mixture
gradient (NGMG) is proposed and applied in diffusion model training with an
additional classifier. Training stability has improved. We also theoretically
prove that NGMG shares the same benefit as the Earth Mover distance
(Wasserstein) as a more sensible cost function when learning distributions
supported by low-dimensional manifolds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weiguo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1&quot;&gt;Deng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinqiao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jirong Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Gangnan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11271">
<title>DACR: Distribution-Augmented Contrastive Reconstruction for Time-Series Anomaly Detection. (arXiv:2401.11271v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.11271</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection in time-series data is crucial for identifying faults,
failures, threats, and outliers across a range of applications. Recently, deep
learning techniques have been applied to this topic, but they often struggle in
real-world scenarios that are complex and highly dynamic, e.g., the normal data
may consist of multiple distributions, and various types of anomalies may
differ from the normal data to different degrees. In this work, to tackle these
challenges, we propose Distribution-Augmented Contrastive Reconstruction
(DACR). DACR generates extra data disjoint from the normal data distribution to
compress the normal data&apos;s representation space, and enhances the feature
extractor through contrastive learning to better capture the intrinsic
semantics from time-series data. Furthermore, DACR employs an attention
mechanism to model the semantic dependencies among multivariate time-series
features, thereby achieving more robust reconstruction for anomaly detection.
Extensive experiments conducted on nine benchmark datasets in various anomaly
detection scenarios demonstrate the effectiveness of DACR in achieving new
state-of-the-art time-series anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shichao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinyu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.05259">
<title>Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series. (arXiv:2006.05259v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2006.05259</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging the symmetries inherent to specific data domains for the
construction of equivariant neural networks has lead to remarkable improvements
in terms of data efficiency and generalization. However, most existing research
focuses on symmetries arising from planar and volumetric data, leaving a
crucial data source largely underexplored: time-series. In this work, we fill
this gap by leveraging the symmetries inherent to time-series for the
construction of equivariant neural network. We identify two core symmetries:
*scale and translation*, and construct scale-translation equivariant neural
networks for time-series learning. Intriguingly, we find that scale-translation
equivariant mappings share strong resemblance with the wavelet transform.
Inspired by this resemblance, we term our networks Wavelet Networks, and show
that they perform nested non-linear wavelet-like time-frequency transforms.
Empirical results show that Wavelet Networks outperform conventional CNNs on
raw waveforms, and match strongly engineered spectrogram techniques across
several tasks and time-series types, including audio, environmental sounds, and
electrical signals. Our code is publicly available at
https://github.com/dwromero/wavelet_networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1&quot;&gt;David W. Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1&quot;&gt;Erik J. Bekkers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1&quot;&gt;Jakub M. Tomczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1&quot;&gt;Mark Hoogendoorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.01135">
<title>MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.01135</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a dynamic assortment selection problem where a seller has a fixed
inventory of $N$ substitutable products and faces an unknown demand that
arrives sequentially over $T$ periods. In each period, the seller needs to
decide on the assortment of products (of cardinality at most $K$) to offer to
the customers. The customer&apos;s response follows an unknown multinomial logit
model (MNL) with parameters $v$. The goal of the seller is to maximize the
total expected revenue given the fixed initial inventory of $N$ products. We
give a policy that achieves a regret of $\tilde O\Big(K \sqrt{KN
T}\Big(\sqrt{v_{\text{max}}} + \frac{1}{q_{\text{min}}}\text{OPT}\Big)\Big)$,
where $v_{\text{max}}\leq 1$ is the maximum utility for any product and
$q_{\text{min}}$ the minimum inventory level, under a mild assumption on the
model parameters. In particular, our policy achieves a near-optimal $\tilde
O(\sqrt{T})$ regret in a large-inventory setting.
&lt;/p&gt;
&lt;p&gt;Our policy builds upon the UCB-based approach for MNL-bandit without
inventory constraints in [1] and addresses the inventory constraints through an
exponentially sized LP for which we present a tractable approximation while
keeping the $\tilde O(\sqrt{T})$ regret bound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aznag_A/0/1/0/all/0/1&quot;&gt;Abdellah Aznag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1&quot;&gt;Vineet Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perivier_N/0/1/0/all/0/1&quot;&gt;Noemie Perivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.05410">
<title>DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly Detection. (arXiv:2106.05410v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.05410</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder&apos;s reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hojjati_H/0/1/0/all/0/1&quot;&gt;Hadi Hojjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.01636">
<title>Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2109.01636</link>
<description rdf:parseType="Literal">&lt;p&gt;With the fast development of Deep Learning techniques, Named Entity
Recognition (NER) is becoming more and more important in the information
extraction task. The greatest difficulty that the NER task faces is to keep the
detectability even when types of NE and documents are unfamiliar. Realizing
that the specificity information may contain potential meanings of a word and
generate semantic-related features for word embedding, we develop a
distribution-aware word embedding and implement three different methods to make
use of the distribution information in a NER framework. And the result shows
that the performance of NER will be improved if the word specificity is
incorporated into existing NER methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.04033">
<title>New Versions of Gradient Temporal Difference Learning. (arXiv:2109.04033v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.04033</link>
<description rdf:parseType="Literal">&lt;p&gt;Sutton, Szepesv\&apos;{a}ri and Maei introduced the first gradient
temporal-difference (GTD) learning algorithms compatible with both linear
function approximation and off-policy training. The goal of this paper is (a)
to propose some variants of GTDs with extensive comparative analysis and (b) to
establish new theoretical analysis frameworks for the GTDs. These variants are
based on convex-concave saddle-point interpretations of GTDs, which effectively
unify all the GTDs into a single framework, and provide simple stability
analysis based on recent results on primal-dual gradient dynamics. Finally,
numerical comparative analysis is given to evaluate these approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Han-Dong Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jihoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_O/0/1/0/all/0/1&quot;&gt;Okyong Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.12305">
<title>Thundernna: a white box adversarial attack. (arXiv:2111.12305v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.12305</link>
<description rdf:parseType="Literal">&lt;p&gt;The existing work shows that the neural network trained by naive
gradient-based optimization method is prone to adversarial attacks, adds small
malicious on the ordinary input is enough to make the neural network wrong. At
the same time, the attack against a neural network is the key to improving its
robustness. The training against adversarial examples can make neural networks
resist some kinds of adversarial attacks. At the same time, the adversarial
attack against a neural network can also reveal some characteristics of the
neural network, a complex high-dimensional non-linear function, as discussed in
previous work.
&lt;/p&gt;
&lt;p&gt;In This project, we develop a first-order method to attack the neural
network. Compare with other first-order attacks, our method has a much higher
success rate. Furthermore, it is much faster than second-order attacks and
multi-steps first-order attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1&quot;&gt;Linfeng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidi_S/0/1/0/all/0/1&quot;&gt;Shayan Mohajer Hamidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.05612">
<title>High-dimensional Inference and FDR Control for Simulated Markov Random Fields. (arXiv:2202.05612v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2202.05612</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying important features linked to a response variable is a fundamental
task in various scientific domains. This article explores statistical inference
for simulated Markov random fields in high-dimensional settings. We introduce a
methodology based on Markov Chain Monte Carlo Maximum Likelihood Estimation
(MCMC-MLE) with Elastic-net regularization. Under mild conditions on the MCMC
method, our penalized MCMC-MLE method achieves $\ell_{1}$-consistency. We
propose a decorrelated score test, establishing both its asymptotic normality
and that of a one-step estimator, along with the associated confidence
interval. Furthermore, we construct two false discovery rate control procedures
via the asymptotic behaviors for both p-values and e-values. Comprehensive
numerical simulations confirm the theoretical validity of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Haoyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lei_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yixin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huiming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.00144">
<title>The Concordance Index decomposition: A measure for a deeper understanding of survival prediction models. (arXiv:2203.00144v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.00144</link>
<description rdf:parseType="Literal">&lt;p&gt;The Concordance Index (C-index) is a commonly used metric in Survival
Analysis for evaluating the performance of a prediction model. In this paper,
we propose a decomposition of the C-index into a weighted harmonic mean of two
quantities: one for ranking observed events versus other observed events, and
the other for ranking observed events versus censored cases. This decomposition
enables a finer-grained analysis of the relative strengths and weaknesses
between different survival prediction methods. The usefulness of this
decomposition is demonstrated through benchmark comparisons against classical
models and state-of-the-art methods, together with the new variational
generative neural-network-based method (SurVED) proposed in this paper. The
performance of the models is assessed using four publicly available datasets
with varying levels of censoring. Using the C-index decomposition and synthetic
censoring, the analysis shows that deep learning models utilize the observed
events more effectively than other models. This allows them to keep a stable
C-index in different censoring levels. In contrast to such deep learning
methods, classical machine learning models deteriorate when the censoring level
decreases due to their inability to improve on ranking the events versus other
events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabdallah_A/0/1/0/all/0/1&quot;&gt;Abdallah Alabdallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohlsson_M/0/1/0/all/0/1&quot;&gt;Mattias Ohlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pashami_S/0/1/0/all/0/1&quot;&gt;Sepideh Pashami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rognvaldsson_T/0/1/0/all/0/1&quot;&gt;Thorsteinn R&amp;#xf6;gnvaldsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07526">
<title>Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity. (arXiv:2204.07526v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07526</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor PCA is a stylized statistical inference problem introduced by
Montanari and Richard to study the computational difficulty of estimating an
unknown parameter from higher-order moment tensors. Unlike its matrix
counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a
sample size regime where the problem is information-theoretically solvable but
conjectured to be computationally hard. This paper derives computational lower
bounds on the run-time of memory bounded algorithms for Tensor PCA using
communication complexity. These lower bounds specify a trade-off among the
number of passes through the data sample, the sample size, and the memory
required by any algorithm that successfully solves Tensor PCA. While the lower
bounds do not rule out polynomial-time algorithms, they do imply that many
commonly-used algorithms, such as gradient descent and power method, must have
a higher iteration count when the sample size is not large enough. Similar
lower bounds are obtained for Non-Gaussian Component Analysis, a family of
statistical estimation problems in which low-order moment tensors carry no
information about the unknown parameter. Finally, stronger lower bounds are
obtained for an asymmetric variant of Tensor PCA and related statistical
estimation problems. These results explain why many estimators for these
problems use a memory state that is significantly larger than the effective
dimensionality of the parameter of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dudeja_R/0/1/0/all/0/1&quot;&gt;Rishabh Dudeja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;Daniel Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.05173">
<title>Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) plays a crucial role in various domains, including
cybersecurity, finance, and healthcare, by identifying patterns or events that
deviate from normal behaviour. In recent years, significant progress has been
made in this field due to the remarkable growth of deep learning models.
Notably, the advent of self-supervised learning has sparked the development of
novel AD algorithms that outperform the existing state-of-the-art approaches by
a considerable margin. This paper aims to provide a comprehensive review of the
current methodologies in self-supervised anomaly detection. We present
technical details of the standard methods and discuss their strengths and
drawbacks. We also compare the performance of these models against each other
and other state-of-the-art anomaly detection models. Finally, the paper
concludes with a discussion of future directions for self-supervised anomaly
detection, including the development of more effective and efficient algorithms
and the integration of these techniques with other related fields, such as
multi-modal learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hojjati_H/0/1/0/all/0/1&quot;&gt;Hadi Hojjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Thi Kieu Khanh Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11359">
<title>Towards Size-Independent Generalization Bounds for Deep Operator Nets. (arXiv:2205.11359v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11359</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times machine learning methods have made significant advances in
becoming a useful tool for analyzing physical systems. A particularly active
area in this theme has been &quot;physics-informed machine learning&quot; which focuses
on using neural nets for numerically solving differential equations. In this
work, we aim to advance the theory of measuring out-of-sample error while
training DeepONets -- which is among the most versatile ways to solve PDE
systems in one-shot.
&lt;/p&gt;
&lt;p&gt;Firstly, for a class of DeepONets, we prove a bound on their Rademacher
complexity which does not explicitly scale with the width of the nets involved.
Secondly, we use this to show how the Huber loss can be chosen so that for
these DeepONet classes generalization error bounds can be obtained that have no
explicit dependence on the size of the nets. We note that our theoretical
results apply to any PDE being targeted to be solved by DeepONets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalani_P/0/1/0/all/0/1&quot;&gt;Pulkit Gopalani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmakar_S/0/1/0/all/0/1&quot;&gt;Sayar Karmakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Dibyakanti Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Anirbit Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14540">
<title>SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14540</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, self-supervised Masked Autoencoders (MAE) have attracted
unprecedented attention for their impressive representation learning ability.
However, the pretext task, Masked Image Modeling (MIM), reconstructs the
missing local patches, lacking the global understanding of the image. This
paper extends MAE to a fully supervised setting by adding a supervised
classification branch, thereby enabling MAE to learn global features from
golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a
visible subset of image patches for classification, unlike the standard
supervised pre-training where all image patches are used. Through experiments,
we demonstrate that SupMAE is not only more training efficient but it also
learns more robust and transferable features. Specifically, SupMAE achieves
comparable performance with MAE using only 30% of compute when evaluated on
ImageNet with the ViT-B/16 model. SupMAE&apos;s robustness on ImageNet variants and
transfer learning performance outperforms MAE and standard supervised
pre-training counterparts. Codes are available at
https://github.com/enyac-group/supmae.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Feng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marculescu_D/0/1/0/all/0/1&quot;&gt;Diana Marculescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10078">
<title>The Manifold Scattering Transform for High-Dimensional Point Cloud Data. (arXiv:2206.10078v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10078</link>
<description rdf:parseType="Literal">&lt;p&gt;The manifold scattering transform is a deep feature extractor for data
defined on a Riemannian manifold. It is one of the first examples of extending
convolutional neural network-like operators to general manifolds. The initial
work on this model focused primarily on its theoretical stability and
invariance properties but did not provide methods for its numerical
implementation except in the case of two-dimensional surfaces with predefined
meshes. In this work, we present practical schemes, based on the theory of
diffusion maps, for implementing the manifold scattering transform to datasets
arising in naturalistic systems, such as single cell genetics, where the data
is a high-dimensional point cloud modeled as lying on a low-dimensional
manifold. We show that our methods are effective for signal classification and
manifold classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chew_J/0/1/0/all/0/1&quot;&gt;Joyce Chew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steach_H/0/1/0/all/0/1&quot;&gt;Holly R. Steach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanath_S/0/1/0/all/0/1&quot;&gt;Siddharth Viswanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hau-Tieng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirn_M/0/1/0/all/0/1&quot;&gt;Matthew Hirn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1&quot;&gt;Smita Krishnaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1&quot;&gt;Michael Perlmutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.14358">
<title>Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2206.14358</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding public discourse on emergency use of unproven therapeutics is
crucial for monitoring safe use and combating misinformation. We developed a
natural language processing-based pipeline to comprehend public perceptions of
and stances on coronavirus disease 2019 (COVID-19)-related drugs on Twitter
over time. This retrospective study included 609,189 US-based tweets from
January 29, 2020, to November 30, 2021, about four drugs that garnered
significant public attention during the COVID-19 pandemic: (1)
Hydroxychloroquine and Ivermectin, therapies with anecdotal evidence; and (2)
Molnupiravir and Remdesivir, FDA-approved treatments for eligible patients.
Time-trend analysis was employed to understand popularity trends and related
events. Content and demographic analyses were conducted to explore potential
rationales behind people&apos;s stances on each drug. Time-trend analysis indicated
that Hydroxychloroquine and Ivermectin were discussed more than Molnupiravir
and Remdesivir, particularly during COVID-19 surges. Hydroxychloroquine and
Ivermectin discussions were highly politicized, related to conspiracy theories,
hearsay, and celebrity influences. The distribution of stances between the two
major US political parties was significantly different (P &amp;lt; .001); Republicans
were more likely to support Hydroxychloroquine (55%) and Ivermectin (30%) than
Democrats. People with healthcare backgrounds tended to oppose
Hydroxychloroquine (7%) more than the general population, while the general
population was more likely to support Ivermectin (14%). Our study found that
social media users have varying perceptions and stances on off-label versus
FDA-authorized drug use at different stages of COVID-19. This indicates that
health systems, regulatory agencies, and policymakers should design tailored
strategies to monitor and reduce misinformation to promote safe drug use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yining Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shixu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plasek_J/0/1/0/all/0/1&quot;&gt;Joseph M. Plasek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bates_D/0/1/0/all/0/1&quot;&gt;David W. Bates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Li Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.15269">
<title>Deep Reinforcement Learning with Swin Transformers. (arXiv:2206.15269v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.15269</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers are neural network models that utilize multiple layers of
self-attention heads and have exhibited enormous potential in natural language
processing tasks. Meanwhile, there have been efforts to adapt transformers to
visual tasks of machine learning, including Vision Transformers and Swin
Transformers. Although some researchers use Vision Transformers for
reinforcement learning tasks, their experiments remain at a small scale due to
the high computational cost. This article presents the first online
reinforcement learning scheme that is based on Swin Transformers: Swin DQN. In
contrast to existing research, our novel approach demonstrate the superior
performance with experiments on 49 games in the Arcade Learning Environment.
The results show that our approach achieves significantly higher maximal
evaluation scores than the baseline method in 45 of all the 49 games (92%), and
higher mean evaluation scores than the baseline method in 40 of all the 49
games (82%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Li Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1&quot;&gt;Anis Yazidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelstad_P/0/1/0/all/0/1&quot;&gt;Paal Engelstad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04957">
<title>Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04957</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating agents that can achieve zero-shot coordination (ZSC) with unseen
partners is a new challenge in cooperative multi-agent reinforcement learning
(MARL). Recently, some studies have made progress in ZSC by exposing the agents
to diverse partners during the training process. They usually involve self-play
when training the partners, implicitly assuming that the tasks are homogeneous.
However, many real-world tasks are heterogeneous, and hence previous methods
may be inefficient. In this paper, we study the heterogeneous ZSC problem for
the first time and propose a general method based on coevolution, which
coevolves two populations of agents and partners through three sub-processes:
pairing, updating and selection. Experimental results on various heterogeneous
tasks highlight the necessity of considering the heterogeneous setting and
demonstrate that our proposed method is a promising solution for heterogeneous
ZSC tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1&quot;&gt;Ke Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yutong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Cong Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lei Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haobo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.05481">
<title>High-Frequency Space Diffusion Models for Accelerated MRI. (arXiv:2208.05481v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.05481</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models with continuous stochastic differential equations (SDEs)
have shown superior performances in image generation. It can serve as a deep
generative prior to solving the inverse problem in magnetic resonance (MR)
reconstruction. However, low-frequency regions of $k$-space data are typically
fully sampled in fast MR imaging, while existing diffusion models are performed
throughout the entire image or $k$-space, inevitably introducing uncertainty in
the reconstruction of low-frequency regions. Additionally, existing diffusion
models often demand substantial iterations to converge, resulting in
time-consuming reconstructions. To address these challenges, we propose a novel
SDE tailored specifically for MR reconstruction with the diffusion process in
high-frequency space (referred to as HFS-SDE). This approach ensures
determinism in the fully sampled low-frequency regions and accelerates the
sampling procedure of reverse diffusion. Experiments conducted on the publicly
available fastMRI dataset demonstrate that the proposed HFS-SDE method
outperforms traditional parallel imaging methods, supervised deep learning, and
existing diffusion models in terms of reconstruction accuracy and stability.
The fast convergence properties are also confirmed through theoretical and
experimental validation. Our code and weights are available at
https://github.com/Aboriginer/HFS-SDE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chentao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhuo-Xu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaonan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Taijin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hairong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Dong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanjie Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00044">
<title>Task Formulation Matters When Learning Continually: A Case Study in Visual Question Answering. (arXiv:2210.00044v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00044</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning aims to train a model incrementally on a sequence of tasks
without forgetting previous knowledge. Although continual learning has been
widely studied in computer vision, its application to Vision+Language tasks is
not that straightforward, as settings can be parameterized in multiple ways
according to their input modalities. In this paper, we present a detailed study
of how different settings affect performance for Visual Question Answering. We
first propose three plausible task formulations and demonstrate their impact on
the performance of continual learning algorithms. We break down several factors
of task similarity, showing that performance and sensitivity to task order
highly depend on the shift of the output distribution. We also investigate the
potential of pretrained models and compare the robustness of transformer models
with different visual embeddings. Finally, we provide an analysis interpreting
model representations and their impact on forgetting. Our results highlight the
importance of stabilizing visual representations in deeper layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikandrou_M/0/1/0/all/0/1&quot;&gt;Mavina Nikandrou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1&quot;&gt;Alessandro Suglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1&quot;&gt;Ioannis Konstas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1&quot;&gt;Verena Rieser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00108">
<title>ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (arXiv:2210.00108v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00108</link>
<description rdf:parseType="Literal">&lt;p&gt;Early backdoor attacks against machine learning set off an arms race in
attack and defence development. Defences have since appeared demonstrating some
ability to detect backdoors in models or even remove them. These defences work
by inspecting the training data, the model, or the integrity of the training
procedure. In this work, we show that backdoors can be added during
compilation, circumventing any safeguards in the data preparation and model
training stages. The attacker can not only insert existing weight-based
backdoors during compilation, but also a new class of weight-independent
backdoors, such as ImpNet. These backdoors are impossible to detect during the
training or data preparation processes, because they are not yet present. Next,
we demonstrate that some backdoors, including ImpNet, can only be reliably
detected at the stage where they are inserted and removing them anywhere else
presents a significant challenge. We conclude that ML model security requires
assurance of provenance along the entire technical pipeline, including the
data, model architecture, compiler, and hardware specification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_T/0/1/0/all/0/1&quot;&gt;Tim Clifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1&quot;&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiren Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1&quot;&gt;Ross Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullins_R/0/1/0/all/0/1&quot;&gt;Robert Mullins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09745">
<title>Transfer learning with affine model transformation. (arXiv:2210.09745v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09745</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised transfer learning has received considerable attention due to its
potential to boost the predictive power of machine learning in scenarios where
data are scarce. Generally, a given set of source models and a dataset from a
target domain are used to adapt the pre-trained models to a target domain by
statistically learning domain shift and domain-specific factors. While such
procedurally and intuitively plausible methods have achieved great success in a
wide range of real-world applications, the lack of a theoretical basis hinders
further methodological development. This paper presents a general class of
transfer learning regression called affine model transfer, following the
principle of expected-square loss minimization. It is shown that the affine
model transfer broadly encompasses various existing methods, including the most
common procedure based on neural feature extractors. Furthermore, the current
paper clarifies theoretical properties of the affine model transfer such as
generalization error and excess risk. Through several case studies, we
demonstrate the practical benefits of modeling and estimating inter-domain
commonality and domain-specific factors separately with the affine-type
transfer models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Minami_S/0/1/0/all/0/1&quot;&gt;Shunya Minami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fukumizu_K/0/1/0/all/0/1&quot;&gt;Kenji Fukumizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hayashi_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yoshida_R/0/1/0/all/0/1&quot;&gt;Ryo Yoshida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08594">
<title>Orthogonal Polynomials Approximation Algorithm (OPAA):a functional analytic approach to estimating probability densities. (arXiv:2211.08594v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08594</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the new Orthogonal Polynomials Approximation Algorithm (OPAA), a
parallelizable algorithm that estimates probability distributions using
functional analytic approach: first, it finds a smooth functional estimate of
the probability distribution, whether it is normalized or not; second, the
algorithm provides an estimate of the normalizing weight; and third, the
algorithm proposes a new computation scheme to compute such estimates.
&lt;/p&gt;
&lt;p&gt;A core component of OPAA is a special transform of the square root of the
joint distribution into a special functional space of our construct. Through
this transform, the evidence is equated with the $L^2$ norm of the transformed
function, squared. Hence, the evidence can be estimated by the sum of squares
of the transform coefficients. Computations can be parallelized and completed
in one pass.
&lt;/p&gt;
&lt;p&gt;OPAA can be applied broadly to the estimation of probability density
functions. In Bayesian problems, it can be applied to estimating the
normalizing weight of the posterior, which is also known as the evidence,
serving as an alternative to existing optimization-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bialokozowicz_L/0/1/0/all/0/1&quot;&gt;Lilian W. Bialokozowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09634">
<title>On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation. (arXiv:2211.09634v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09634</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the sample complexity of bounded two-layer neural networks
using different activation functions.
&lt;/p&gt;
&lt;p&gt;In particular, we consider the class
&lt;/p&gt;
&lt;p&gt;$$ \mathcal{H} = \left\{\textbf{x}\mapsto \langle \textbf{v}, \sigma \circ
W\textbf{b} + \textbf{b} \rangle : \textbf{b}\in\mathbb{R}^d, W \in
\mathbb{R}^{\mathcal{T}\times d}, \textbf{v} \in
\mathbb{R}^{\mathcal{T}}\right\} $$
&lt;/p&gt;
&lt;p&gt;where the spectral norm of $W$ and $\textbf{v}$ is bounded by $O(1)$, the
Frobenius norm of $W$ is bounded from its initialization by $R &amp;gt; 0$, and
$\sigma$ is a Lipschitz activation function.
&lt;/p&gt;
&lt;p&gt;We prove that if $\sigma$ is element-wise, then the sample complexity of
$\mathcal{H}$ has only logarithmic dependency in width and that this complexity
is tight, up to logarithmic factors.
&lt;/p&gt;
&lt;p&gt;We further show that the element-wise property of $\sigma$ is essential for a
logarithmic dependency bound in width, in the sense that there exist
non-element-wise activation functions whose sample complexity is linear in
width, for widths that can be up to exponential in the input dimension.
&lt;/p&gt;
&lt;p&gt;For the upper bound, we use the recent approach for norm-based bounds named
Approximate Description Length (ADL) by &lt;a href=&quot;/abs/1910.05697&quot;&gt;arXiv:1910.05697&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;We further develop new techniques and tools for this approach that will
hopefully inspire future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1&quot;&gt;Amit Daniely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granot_E/0/1/0/all/0/1&quot;&gt;Elad Granot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00325">
<title>HashVFL: Defending Against Data Reconstruction Attacks in Vertical Federated Learning. (arXiv:2212.00325v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00325</link>
<description rdf:parseType="Literal">&lt;p&gt;Vertical Federated Learning (VFL) is a trending collaborative machine
learning model training solution. Existing industrial frameworks employ secure
multi-party computation techniques such as homomorphic encryption to ensure
data security and privacy. Despite these efforts, studies have revealed that
data leakage remains a risk in VFL due to the correlations between intermediate
representations and raw data. Neural networks can accurately capture these
correlations, allowing an adversary to reconstruct the data. This emphasizes
the need for continued research into securing VFL systems.
&lt;/p&gt;
&lt;p&gt;Our work shows that hashing is a promising solution to counter data
reconstruction attacks. The one-way nature of hashing makes it difficult for an
adversary to recover data from hash codes. However, implementing hashing in VFL
presents new challenges, including vanishing gradients and information loss. To
address these issues, we propose HashVFL, which integrates hashing and
simultaneously achieves learnability, bit balance, and consistency.
&lt;/p&gt;
&lt;p&gt;Experimental results indicate that HashVFL effectively maintains task
performance while defending against data reconstruction attacks. It also brings
additional benefits in reducing the degree of label leakage, mitigating
adversarial attacks, and detecting abnormal inputs. We hope our work will
inspire further research into the potential applications of HashVFL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_P/0/1/0/all/0/1&quot;&gt;Pengyu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shouling Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01168">
<title>Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning. (arXiv:2212.01168v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01168</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning for physics have focused on discovering
shared representations of target systems by incorporating physics priors or
inductive biases into neural networks. While effective, these methods are
limited to the system domain, where the type of system remains consistent and
thus cannot ensure the adaptation to new, or unseen physical systems governed
by different laws. For instance, a neural network trained on a mass-spring
system cannot guarantee accurate predictions for the behavior of a two-body
system or any other system with different physical laws. In this work, we take
a significant leap forward by targeting cross domain generalization within the
field of Hamiltonian dynamics. We model our system with a graph neural network
and employ a meta learning algorithm to enable the model to gain experience
over a distribution of tasks and make it adapt to new physics. Our approach
aims to learn a unified Hamiltonian representation that is generalizable across
multiple system domains, thereby overcoming the limitations of system-specific
models. Our results demonstrate that the meta-trained model not only adapts
effectively to new systems but also captures a generalized Hamiltonian
representation that is consistent across different physical domains. Overall,
through the use of meta learning, we offer a framework that achieves cross
domain generalization, providing a step towards a unified model for
understanding a wide array of dynamical systems via deep learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yeongwoo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Hawoong Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03369">
<title>Exploring Randomly Wired Neural Networks for Climate Model Emulation. (arXiv:2212.03369v4 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03369</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploring the climate impacts of various anthropogenic emissions scenarios is
key to making informed decisions for climate change mitigation and adaptation.
State-of-the-art Earth system models can provide detailed insight into these
impacts, but have a large associated computational cost on a per-scenario
basis. This large computational burden has driven recent interest in developing
cheap machine learning models for the task of climate model emulation. In this
manuscript, we explore the efficacy of randomly wired neural networks for this
task. We describe how they can be constructed and compare them to their
standard feedforward counterparts using the ClimateBench dataset. Specifically,
we replace the serially connected dense layers in multilayer perceptrons,
convolutional neural networks, and convolutional long short-term memory
networks with randomly wired dense layers and assess the impact on model
performance for models with 1 million and 10 million parameters. We find that
models with less complex architectures see the greatest performance improvement
with the addition of random wiring (up to 30.4% for multilayer perceptrons).
Furthermore, out of 24 different model architecture, parameter count, and
prediction task combinations, only one saw a statistically significant
performance deficit in randomly wired networks compared to their standard
counterparts, with 14 cases showing statistically significant improvement. We
also find no significant difference in prediction speed between networks with
standard feedforward dense layers and those with randomly wired layers. These
findings indicate that randomly wired neural networks may be suitable direct
replacements for traditional dense layers in many standard models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yik_W/0/1/0/all/0/1&quot;&gt;William Yik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Silva_S/0/1/0/all/0/1&quot;&gt;Sam J. Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Geiss_A/0/1/0/all/0/1&quot;&gt;Andrew Geiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Watson_Parris_D/0/1/0/all/0/1&quot;&gt;Duncan Watson-Parris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06726">
<title>Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration. (arXiv:2302.06726v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06726</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and study Swap Agnostic Learning. The problem can be phrased as
a game between a predictor and an adversary: first, the predictor selects a
hypothesis $h$; then, the adversary plays in response, and for each level set
of the predictor $\{x \in \mathcal{X} : h(x) = v\}$ selects a (different)
loss-minimizing hypothesis $c_v \in \mathcal{C}$; the predictor wins if $h$
competes with the adaptive adversary&apos;s loss. Despite the strength of the
adversary, we demonstrate the feasibility Swap Agnostic Learning for any convex
loss.
&lt;/p&gt;
&lt;p&gt;Somewhat surprisingly, the result follows through an investigation into the
connections between Omniprediction and Multicalibration. Omniprediction is a
new notion of optimality for predictors that strengthtens classical notions
such as agnostic learning. It asks for loss minimization guarantees (relative
to a hypothesis class) that apply not just for a specific loss function, but
for any loss belonging to a rich family of losses. A recent line of work shows
that omniprediction is implied by multicalibration and related multi-group
fairness notions. This unexpected connection raises the question: is
multi-group fairness necessary for omniprediction?
&lt;/p&gt;
&lt;p&gt;Our work gives the first affirmative answer to this question. We establish an
equivalence between swap variants of omniprediction and multicalibration and
swap agnostic learning. Further, swap multicalibration is essentially
equivalent to the standard notion of multicalibration, so existing learning
algorithms can be used to achieve any of the three notions. Building on this
characterization, we paint a complete picture of the relationship between
different variants of multi-group fairness, omniprediction, and Outcome
Indistinguishability. This inquiry reveals a unified notion of OI that captures
all existing notions of omniprediction and multicalibration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalan_P/0/1/0/all/0/1&quot;&gt;Parikshit Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Michael P. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1&quot;&gt;Omer Reingold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11337">
<title>Bayesian Matrix Decomposition and Applications. (arXiv:2302.11337v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11337</link>
<description rdf:parseType="Literal">&lt;p&gt;The sole aim of this book is to give a self-contained introduction to
concepts and mathematical tools in Bayesian matrix decomposition in order to
seamlessly introduce matrix decomposition techniques and their applications in
subsequent sections. However, we clearly realize our inability to cover all the
useful and interesting results concerning Bayesian matrix decomposition and
given the paucity of scope to present this discussion, e.g., the separated
analysis of variational inference for conducting the optimization. We refer the
reader to literature in the field of Bayesian analysis for a more detailed
introduction to the related fields.
&lt;/p&gt;
&lt;p&gt;This book is primarily a summary of purpose, significance of important
Bayesian matrix decomposition methods, e.g., real-valued decomposition,
nonnegative matrix factorization, Bayesian interpolative decomposition, and the
origin and complexity of the methods which shed light on their applications.
The mathematical prerequisite is a first course in statistics and linear
algebra. Other than this modest background, the development is self-contained,
with rigorous proof provided throughout.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jun Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05479">
<title>Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05479</link>
<description rdf:parseType="Literal">&lt;p&gt;A compelling use case of offline reinforcement learning (RL) is to obtain a
policy initialization from existing datasets followed by fast online
fine-tuning with limited interaction. However, existing offline RL methods tend
to behave poorly during fine-tuning. In this paper, we devise an approach for
learning an effective initialization from offline data that also enables fast
online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL),
accomplishes this by learning a conservative value function initialization that
underestimates the value of the learned policy from offline data, while also
being calibrated, in the sense that the learned Q-values are at a reasonable
scale. We refer to this property as calibration, and define it formally as
providing a lower bound on the true value function of the learned policy and an
upper bound on the value of some other (suboptimal) reference policy, which may
simply be the behavior policy. We show that offline RL algorithms that learn
such calibrated value functions lead to effective online fine-tuning, enabling
us to take the benefits of offline initializations in online fine-tuning. In
practice, Cal-QL can be implemented on top of the conservative Q learning (CQL)
for offline RL within a one-line code change. Empirically, Cal-QL outperforms
state-of-the-art methods on 9/11 fine-tuning benchmark tasks that we study in
this paper. Code and video are available at https://nakamotoo.github.io/Cal-QL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamoto_M/0/1/0/all/0/1&quot;&gt;Mitsuhiko Nakamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Anikait Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mark_M/0/1/0/all/0/1&quot;&gt;Max Sobol Mark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07287">
<title>Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm. (arXiv:2303.07287v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07287</link>
<description rdf:parseType="Literal">&lt;p&gt;In non-asymptotic learning, variance-type parameters of sub-Gaussian
distributions are of paramount importance. However, directly estimating these
parameters using the empirical moment generating function (MGF) is infeasible.
To address this, we suggest using the sub-Gaussian intrinsic moment norm
[Buldygin and Kozachenko (2000), Theorem 1.3] achieved by maximizing a sequence
of normalized moments. Significantly, the suggested norm can not only
reconstruct the exponential moment bounds of MGFs but also provide tighter
sub-Gaussian concentration inequalities. In practice, we provide an intuitive
method for assessing whether data with a finite sample size is sub-Gaussian,
utilizing the sub-Gaussian plot. The intrinsic moment norm can be robustly
estimated via a simple plug-in approach. Our theoretical findings are also
applicable to reinforcement learning, including the multi-armed bandit
scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Haoyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11249">
<title>What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11249</link>
<description rdf:parseType="Literal">&lt;p&gt;The question of what makes a data distribution suitable for deep learning is
a fundamental open problem. Focusing on locally connected neural networks (a
prevalent family of architectures that includes convolutional and recurrent
neural networks as well as local self-attention models), we address this
problem by adopting theoretical tools from quantum physics. Our main
theoretical result states that a certain locally connected neural network is
capable of accurate prediction over a data distribution if and only if the data
distribution admits low quantum entanglement under certain canonical partitions
of features. As a practical application of this result, we derive a
preprocessing method for enhancing the suitability of a data distribution to
locally connected neural networks. Experiments with widespread models over
various datasets demonstrate our findings. We hope that our use of quantum
entanglement will encourage further adoption of tools from physics for formally
reasoning about the relation between deep learning and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_Y/0/1/0/all/0/1&quot;&gt;Yotam Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vega_N/0/1/0/all/0/1&quot;&gt;Nimrod De La Vega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razin_N/0/1/0/all/0/1&quot;&gt;Noam Razin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Nadav Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00418">
<title>An Empirical Study of Using Large Language Models for Unit Test Generation. (arXiv:2305.00418v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00418</link>
<description rdf:parseType="Literal">&lt;p&gt;A code generation model generates code by taking a prompt from a code
comment, existing code, or a combination of both. Although code generation
models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is
unclear whether they can successfully be used for unit test generation without
fine-tuning for a strongly typed language like Java. To fill this gap, we
investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can
generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to
investigate the effect of context generation on the unit test generation
process. We evaluated the models based on compilation rates, test correctness,
test coverage, and test smells. We found that the Codex model achieved above
80% coverage for the HumanEval dataset, but no model had more than 2% coverage
for the EvoSuite SF110 benchmark. The generated tests also suffered from test
smells, such as Duplicated Asserts and Empty Tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiq_M/0/1/0/all/0/1&quot;&gt;Mohammed Latif Siddiq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Joanna C. S. Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanvir_R/0/1/0/all/0/1&quot;&gt;Ridwanul Hasan Tanvir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulfat_N/0/1/0/all/0/1&quot;&gt;Noshin Ulfat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rifat_F/0/1/0/all/0/1&quot;&gt;Fahmid Al Rifat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopes_V/0/1/0/all/0/1&quot;&gt;Vinicius Carvalho Lopes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04073">
<title>Explaining RL Decisions with Trajectories. (arXiv:2305.04073v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04073</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanation is a key component for the adoption of reinforcement learning
(RL) in many real-world decision-making problems. In the literature, the
explanation is often provided by saliency attribution to the features of the RL
agent&apos;s state. In this work, we propose a complementary approach to these
explanations, particularly for offline RL, where we attribute the policy
decisions of a trained RL agent to the trajectories encountered by it during
training. To do so, we encode trajectories in offline training data
individually as well as collectively (encoding a set of trajectories). We then
attribute policy decisions to a set of trajectories in this encoded space by
estimating the sensitivity of the decision with respect to that set. Further,
we demonstrate the effectiveness of the proposed approach in terms of quality
of attributions as well as practical scalability in diverse environments that
involve both discrete and continuous state and action spaces such as
grid-worlds, video games (Atari) and continuous control (MuJoCo). We also
conduct a human study on a simple navigation task to observe how their
understanding of the task compares with data attributed for a trained RL
policy. Keywords -- Explainable AI, Verifiability of AI Decisions, Explainable
RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1&quot;&gt;Shripad Vilasrao Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1&quot;&gt;Arpan Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1&quot;&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theocharous_G/0/1/0/all/0/1&quot;&gt;Georgios Theocharous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_J/0/1/0/all/0/1&quot;&gt;Jayakumar Subramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08404">
<title>Theoretical Analysis of Inductive Biases in Deep Convolutional Networks. (arXiv:2305.08404v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08404</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide a theoretical analysis of the inductive biases in
convolutional neural networks (CNNs). We start by examining the universality of
CNNs, i.e., the ability to approximate any continuous functions. We prove that
a depth of $\mathcal{O}(\log d)$ suffices for deep CNNs to achieve this
universality, where $d$ in the input dimension. Additionally, we establish that
learning sparse functions with CNNs requires only
$\widetilde{\mathcal{O}}(\log^2d)$ samples, indicating that deep CNNs can
efficiently capture {\em long-range} sparse correlations. These results are
made possible through a novel combination of the multichanneling and
downsampling when increasing the network depth. We also delve into the distinct
roles of weight sharing and locality in CNNs. To this end, we compare the
performance of CNNs, locally-connected networks (LCNs), and fully-connected
networks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs
without weight sharing. On the one hand, we prove that LCNs require
${\Omega}(d)$ samples while CNNs need only $\widetilde{\mathcal{O}}(\log^2d)$
samples, highlighting the critical role of weight sharing. On the other hand,
we prove that FCNs require $\Omega(d^2)$ samples, whereas LCNs need only
$\widetilde{\mathcal{O}}(d)$ samples, underscoring the importance of locality.
These provable separations quantify the difference between the two biases, and
the major observation behind our proof is that weight sharing and locality
break different symmetries in the learning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12997">
<title>Evaluating Privacy Leakage in Split Learning. (arXiv:2305.12997v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12997</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy-Preserving machine learning (PPML) can help us train and deploy
models that utilize private information. In particular, on-device machine
learning allows us to avoid sharing raw data with a third-party server during
inference. On-device models are typically less accurate when compared to their
server counterparts due to the fact that (1) they typically only rely on a
small set of on-device features and (2) they need to be small enough to run
efficiently on end-user devices. Split Learning (SL) is a promising approach
that can overcome these limitations. In SL, a large machine learning model is
divided into two parts, with the bigger part residing on the server side and a
smaller part executing on-device, aiming to incorporate the private features.
However, end-to-end training of such models requires exchanging gradients at
the cut layer, which might encode private features or labels. In this paper, we
provide insights into potential privacy risks associated with SL. Furthermore,
we also investigate the effectiveness of various mitigation strategies. Our
results indicate that the gradients significantly improve the attackers&apos;
effectiveness in all tested datasets reaching almost perfect reconstruction
accuracy for some features. However, a small amount of differential privacy
(DP) can effectively mitigate this risk without causing significant training
degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xinchi Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1&quot;&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melis_L/0/1/0/all/0/1&quot;&gt;Luca Melis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1&quot;&gt;Alex Sablayrolles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1&quot;&gt;Pierre Stock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14189">
<title>Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14189</link>
<description rdf:parseType="Literal">&lt;p&gt;Using a vocabulary that is shared across languages is common practice in
Multilingual Neural Machine Translation (MNMT). In addition to its simple
design, shared tokens play an important role in positive knowledge transfer,
assuming that shared tokens refer to similar meanings across languages.
However, when word overlap is small, especially due to different writing
systems, transfer is inhibited. In this paper, we define word-level information
transfer pathways via word equivalence classes and rely on graph networks to
fuse word embeddings across languages. Our experiments demonstrate the
advantages of our approach: 1) embeddings of words with similar meanings are
better aligned across languages, 2) our method achieves consistent BLEU
improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less
than 1.0\% additional trainable parameters are required with a limited increase
in computational costs, while inference time remains identical to the baseline.
We release the codebase to the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1&quot;&gt;Christof Monz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15586">
<title>Manifold Diffusion Fields. (arXiv:2305.15586v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15586</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Manifold Diffusion Fields (MDF), an approach that unlocks learning
of diffusion models of data in general non-Euclidean geometries. Leveraging
insights from spectral geometry analysis, we define an intrinsic coordinate
system on the manifold via the eigen-functions of the Laplace-Beltrami
Operator. MDF represents functions using an explicit parametrization formed by
a set of multiple input-output pairs. Our approach allows to sample continuous
functions on manifolds and is invariant with respect to rigid and isometric
transformations of the manifold. In addition, we show that MDF generalizes to
the case where the training set contains functions on different manifolds.
Empirical results on multiple datasets and manifolds including challenging
scientific problems like weather prediction or molecular conformation show that
MDF can capture distributions of such functions with better diversity and
fidelity than previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhag_A/0/1/0/all/0/1&quot;&gt;Ahmed A. Elhag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1&quot;&gt;Joshua M. Susskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1&quot;&gt;Miguel Angel Bautista&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16326">
<title>Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16326</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical literature is growing rapidly, making it challenging to curate and
extract knowledge manually. Biomedical natural language processing (BioNLP)
techniques that can automatically extract information from biomedical
literature help alleviate this burden. Recently, large Language Models (LLMs),
such as GPT-3 and GPT-4, have gained significant attention for their impressive
performance. However, their effectiveness in BioNLP tasks and impact on method
development and downstream users remain understudied. This pilot study (1)
establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and
one-shot settings in eight BioNLP datasets across four applications: named
entity recognition, relation extraction, multi-label document classification,
and semantic similarity and reasoning, (2) examines the errors produced by the
LLMs and categorized the errors into three types: missingness, inconsistencies,
and unwanted artificial content, and (3) provides suggestions for using LLMs in
BioNLP applications. We make the datasets, baselines, and results publicly
available to the community via
https://github.com/qingyu-qc/gpt_bionlp_benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jingcheng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keloth_V/0/1/0/all/0/1&quot;&gt;Vipina Kuttichi Keloth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xueqing Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1&quot;&gt;Kalpana Raja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hua Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16789">
<title>Modulate Your Spectrum in Self-Supervised Learning. (arXiv:2305.16789v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16789</link>
<description rdf:parseType="Literal">&lt;p&gt;Whitening loss offers a theoretical guarantee against feature collapse in
self-supervised learning (SSL) with joint embedding architectures. Typically,
it involves a hard whitening approach, transforming the embedding and applying
loss to the whitened output. In this work, we introduce Spectral Transformation
(ST), a framework to modulate the spectrum of embedding and to seek for
functions beyond whitening that can avoid dimensional collapse. We show that
whitening is a special instance of ST by definition, and our empirical
investigations unveil other ST instances capable of preventing collapse.
Additionally, we propose a novel ST instance named IterNorm with trace loss
(INTL). Theoretical analysis confirms INTL&apos;s efficacy in preventing collapse
and modulating the spectrum of embedding toward equal-eigenvalues during
optimization. Our experiments on ImageNet classification and COCO object
detection demonstrate INTL&apos;s potential in learning superior representations.
The code is available at https://github.com/winci-ai/INTL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1&quot;&gt;Xi Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yunhao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tengwei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16943">
<title>DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models. (arXiv:2305.16943v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16943</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing NAS methods suffer from either an excessive amount of time for
repetitive sampling and training of many task-irrelevant architectures. To
tackle such limitations of existing NAS methods, we propose a paradigm shift
from NAS to a novel conditional Neural Architecture Generation (NAG) framework
based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the
neural architectures as directed graphs and propose a graph diffusion model for
generating them. Moreover, with the guidance of parameterized predictors,
DiffusionNAG can flexibly generate task-optimal architectures with the desired
properties for diverse tasks, by sampling from a region that is more likely to
satisfy the properties. This conditional NAG scheme is significantly more
efficient than previous NAS schemes which sample the architectures and filter
them using the property predictors. We validate the effectiveness of
DiffusionNAG through extensive experiments in two predictor-based NAS
scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS.
DiffusionNAG achieves superior performance with speedups of up to 20 times when
compared to the baselines on Transferable NAS benchmarks. Furthermore, when
integrated into a BO-based algorithm, DiffusionNAG outperforms existing
BO-based NAS approaches, particularly in the large MobileNetV3 search space on
the ImageNet 1K dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Sohyun An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hayeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1&quot;&gt;Jaehyeong Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seanie Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17028">
<title>Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17028</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep probabilistic time series forecasting has gained significant attention
due to its superior performance in nonlinear approximation and its ability to
provide valuable uncertainty quantification for decision-making tasks. However,
many existing models oversimplify the problem by assuming that the error
process is time-independent, thereby overlooking the serial correlation in the
error process. To overcome this limitation, we propose an innovative training
method that incorporates error autocorrelation to further enhance the accuracy
of probabilistic forecasting. Our method involves constructing a mini-batch as
a collection of $D$ consecutive time series segments for model training and
explicitly learning a time-varying covariance matrix over each mini-batch that
encodes the error correlation among adjacent time steps. The learned covariance
matrix can be used to improve prediction accuracy and enhance uncertainty
quantification. We evaluate our method on two different neural forecasting
models and multiple public datasets, and the experimental results confirm the
effectiveness of the proposed approach in enhancing the performance of both
models across a wide range of datasets, yielding notable improvements in
predictive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_V/0/1/0/all/0/1&quot;&gt;Vincent Zhihao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seongjin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lijun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18394">
<title>On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18394</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational regularization is commonly used to solve linear inverse problems,
and involves augmenting a data fidelity by a regularizer. The regularizer is
used to promote a priori information and is weighted by a regularization
parameter. Selection of an appropriate regularization parameter is critical,
with various choices leading to very different reconstructions. Classical
strategies used to determine a suitable parameter value include the discrepancy
principle and the L-curve criterion, and in recent years a supervised machine
learning approach called bilevel learning has been employed. Bilevel learning
is a powerful framework to determine optimal parameters and involves solving a
nested optimization problem. While previous strategies enjoy various
theoretical results, the well-posedness of bilevel learning in this setting is
still an open question. In particular, a necessary property is positivity of
the determined regularization parameter. In this work, we provide a new
condition that better characterizes positivity of optimal regularization
parameters than the existing theory. Numerical results verify and explore this
new condition for both small and high-dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ehrhardt_M/0/1/0/all/0/1&quot;&gt;Matthias J. Ehrhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gazzola_S/0/1/0/all/0/1&quot;&gt;Silvia Gazzola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scott_S/0/1/0/all/0/1&quot;&gt;Sebastian J. Scott&lt;/a&gt; (Department of Mathematical Sciences, University of Bath, Bath, UK)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19604">
<title>Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19604</link>
<description rdf:parseType="Literal">&lt;p&gt;Medication recommendation is a fundamental yet crucial branch of healthcare,
which provides opportunities to support clinical physicians with more accurate
medication prescriptions for patients with complex health conditions. Learning
from electronic health records (EHR) to recommend medications is the most
common way in previous studies. However, most of them neglect incorporating
domain knowledge according to the clinical manifestations in the EHR of the
patient. To address these issues, we propose a novel \textbf{D}omain
\textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate
domain knowledge with observable clinical manifestations of the patient, which
is the first dynamic domain knowledge informed framework toward medication
recommendation. In particular, we first design a knowledge-driven encoder to
capture the domain information and then develop a data-driven encoder to
integrate domain knowledge into the observable EHR. To endow the model with the
capability of temporal decision, we design an explicit medication encoder for
learning the longitudinal dependence of the patient. Extensive experiments on
three publicly available datasets verify the superiority of our method. The
code will be public upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sicen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xianbing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01589">
<title>Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01589</link>
<description rdf:parseType="Literal">&lt;p&gt;Interatomic potentials learned using machine learning methods have been
successfully applied to atomistic simulations. However, accurate models require
large training datasets, while generating reference calculations is
computationally demanding. To bypass this difficulty, we propose a transfer
learning algorithm that leverages the ability of graph neural networks (GNNs)
to represent chemical environments together with kernel mean embeddings. We
extract a feature map from GNNs pre-trained on the OC20 dataset and use it to
learn the potential energy surface from system-specific datasets of catalytic
processes. Our method is further enhanced by incorporating into the kernel the
chemical species information, resulting in improved performance and
interpretability. We test our approach on a series of realistic datasets of
increasing complexity, showing excellent generalization and transferability
performance, and improving on methods that rely on GNNs or ridge regression
alone, as well as similar fine-tuning approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falk_J/0/1/0/all/0/1&quot;&gt;John Falk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonati_L/0/1/0/all/0/1&quot;&gt;Luigi Bonati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novelli_P/0/1/0/all/0/1&quot;&gt;Pietro Novelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parrinello_M/0/1/0/all/0/1&quot;&gt;Michele Parrinello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01631">
<title>Bi-level Contrastive Learning for Knowledge-Enhanced Molecule Representations. (arXiv:2306.01631v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01631</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecule representation learning is crucial for various downstream
applications, such as understanding and predicting molecular properties and
side effects. In this paper, we propose a novel method called GODE, which takes
into account the two-level structure of individual molecules. We recognize that
molecules have an intrinsic graph structure as well as being a node in a larger
molecule knowledge graph. GODE integrates graph representations of individual
molecules with multidomain biochemical data from knowledge graphs. By
pre-training two graph neural networks (GNNs) on different graph structures,
combined with contrastive learning, GODE fuses molecular structures with their
corresponding knowledge graph substructures. This fusion results in a more
robust and informative representation, which enhances molecular property
prediction by harnessing both chemical and biological information. When
fine-tuned across 11 chemical property tasks, our model outperforms existing
benchmarks, registering an average ROC-AUC uplift of 13.8% for classification
tasks and an average RMSE/MAE enhancement of 35.1% for regression tasks.
Impressively, it surpasses the current leading model in molecule property
predictions with average advancements of 2.1% in classification and 6.4% in
regression tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Pengcheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Cao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tianfan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02869">
<title>Data-Driven Regret Balancing for Online Model Selection in Bandits. (arXiv:2306.02869v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider model selection for sequential decision making in stochastic
environments with bandit feedback, where a meta-learner has at its disposal a
pool of base learners, and decides on the fly which action to take based on the
policies recommended by each base learner. Model selection is performed by
regret balancing but, unlike the recent literature on this subject, we do not
assume any prior knowledge about the base learners like candidate regret
guarantees; instead, we uncover these quantities in a data-driven manner. The
meta-learner is therefore able to leverage the realized regret incurred by each
base learner for the learning environment at hand (as opposed to the expected
regret), and single out the best such regret. We design two model selection
algorithms operating with this more ambitious notion of regret and, besides
proving model selection guarantees via regret balancing, we experimentally
demonstrate the compelling practical benefits of dealing with actual regrets
instead of candidate regret bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1&quot;&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1&quot;&gt;Christoph Dann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1&quot;&gt;Claudio Gentile&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05023">
<title>Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05023</link>
<description rdf:parseType="Literal">&lt;p&gt;The posterior collapse phenomenon in variational autoencoder (VAE), where the
variational posterior distribution closely matches the prior distribution, can
hinder the quality of the learned latent variables. As a consequence of
posterior collapse, the latent variables extracted by the encoder in VAE
preserve less information from the input data and thus fail to produce
meaningful representations as input to the reconstruction process in the
decoder. While this phenomenon has been an actively addressed topic related to
VAE performance, the theory for posterior collapse remains underdeveloped,
especially beyond the standard VAE. In this work, we advance the theoretical
understanding of posterior collapse to two important and prevalent yet less
studied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via
a non-trivial theoretical analysis of linear conditional VAE and hierarchical
VAE with two levels of latent, we prove that the cause of posterior collapses
in these models includes the correlation between the input and output of the
conditional VAE and the effect of learnable encoder variance in the
hierarchical VAE. We empirically validate our theoretical findings for linear
conditional and hierarchical VAE and demonstrate that these results are also
predictive for non-linear cases with extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dang_H/0/1/0/all/0/1&quot;&gt;Hien Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tho Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ho_N/0/1/0/all/0/1&quot;&gt;Nhat Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06064">
<title>Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06064</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving NP-hard/complete combinatorial problems with neural networks is a
challenging research area that aims to surpass classical approximate
algorithms. The long-term objective is to outperform hand-designed heuristics
for NP-hard/complete problems by learning to generate superior solutions solely
from training data. Current neural-based methods for solving CO problems often
overlook the inherent &quot;algorithmic&quot; nature of the problems. In contrast,
heuristics designed for CO problems, e.g. TSP, frequently leverage
well-established algorithms, such as those for finding the minimum spanning
tree. In this paper, we propose leveraging recent advancements in neural
algorithmic reasoning to improve the learning of CO problems. Specifically, we
suggest pre-training our neural model on relevant algorithms before training it
on CO instances. Our results demonstrate that by using this learning setup, we
achieve superior performance compared to non-algorithmically informed deep
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1&quot;&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Numeroso_D/0/1/0/all/0/1&quot;&gt;Danilo Numeroso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1&quot;&gt;Davide Bacciu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06071">
<title>Adversarial Attack On Yolov5 For Traffic And Road Sign Detection. (arXiv:2306.06071v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06071</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper implements and investigates popular adversarial attacks on the
YOLOv5 Object Detection algorithm. The paper explores the vulnerability of the
YOLOv5 to adversarial attacks in the context of traffic and road sign
detection. The paper investigates the impact of different types of attacks,
including the Limited memory Broyden Fletcher Goldfarb Shanno (L-BFGS), the
Fast Gradient Sign Method (FGSM) attack, the Carlini and Wagner (C&amp;amp;W) attack,
the Basic Iterative Method (BIM) attack, the Projected Gradient Descent (PGD)
attack, One Pixel Attack, and the Universal Adversarial Perturbations attack on
the accuracy of YOLOv5 in detecting traffic and road signs. The results show
that YOLOv5 is susceptible to these attacks, with misclassification rates
increasing as the magnitude of the perturbations increases. We also explain the
results using saliency maps. The findings of this paper have important
implications for the safety and reliability of object detection algorithms used
in traffic and transportation systems, highlighting the need for more robust
and secure models to ensure their effectiveness in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanyam Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09136">
<title>Finite-Time Logarithmic Bayes Regret Upper Bounds. (arXiv:2306.09136v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09136</link>
<description rdf:parseType="Literal">&lt;p&gt;We derive the first finite-time logarithmic Bayes regret upper bounds for
Bayesian bandits. In a multi-armed bandit, we obtain $O(c_\Delta \log n)$ and
$O(c_h \log^2 n)$ upper bounds for an upper confidence bound algorithm, where
$c_h$ and $c_\Delta$ are constants depending on the prior distribution and the
gaps of bandit instances sampled from it, respectively. The latter bound
asymptotically matches the lower bound of Lai (1987). Our proofs are a major
technical departure from prior works, while being simple and general. To show
the generality of our techniques, we apply them to linear bandits. Our results
provide insights on the value of prior in the Bayesian setting, both in the
objective and as a side information given to the learner. They significantly
improve upon existing $\tilde{O}(\sqrt{n})$ bounds, which have become standard
in the literature despite the logarithmic lower bound of Lai (1987).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atsidakou_A/0/1/0/all/0/1&quot;&gt;Alexia Atsidakou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katariya_S/0/1/0/all/0/1&quot;&gt;Sumeet Katariya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caramanis_C/0/1/0/all/0/1&quot;&gt;Constantine Caramanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1&quot;&gt;Sujay Sanghavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16761">
<title>Moreau Envelope Based Difference-of-weakly-Convex Reformulation and Algorithm for Bilevel Programs. (arXiv:2306.16761v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16761</link>
<description rdf:parseType="Literal">&lt;p&gt;Bilevel programming has emerged as a valuable tool for hyperparameter
selection, a central concern in machine learning. In a recent study by Ye et
al. (2023), a value function-based difference of convex algorithm was
introduced to address bilevel programs. This approach proves particularly
powerful when dealing with scenarios where the lower-level problem exhibits
convexity in both the upper-level and lower-level variables. Examples of such
scenarios include support vector machines and $\ell_1$ and $\ell_2$ regularized
regression. In this paper, we significantly expand the range of applications,
now requiring convexity only in the lower-level variables of the lower-level
program. We present an innovative single-level difference of weakly convex
reformulation based on the Moreau envelope of the lower-level problem. We
further develop a sequentially convergent Inexact Proximal Difference of Weakly
Convex Algorithm (iP-DwCA). To evaluate the effectiveness of the proposed
iP-DwCA, we conduct numerical experiments focused on tuning hyperparameters for
kernel support vector machines on simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lucy L. Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jane J. Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haian Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shangzhi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17778">
<title>Look, Remember and Reason: Grounded reasoning in videos with language models. (arXiv:2306.17778v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17778</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal language models (LM) have recently shown promising performance in
high-level reasoning tasks on videos. However, existing methods still fall
short in tasks like causal or compositional spatiotemporal reasoning over
actions, in which model predictions need to be grounded in fine-grained
low-level details, such as object motions and object interactions. In this
work, we propose training an LM end-to-end on low-level surrogate tasks,
including object detection, re-identification, and tracking, to endow the model
with the required low-level visual capabilities. We show that a two-stream
video encoder with spatiotemporal attention is effective at capturing the
required static and motion-based cues in the video. By leveraging the LM&apos;s
ability to perform the low-level surrogate tasks, we can cast reasoning in
videos as the three-step process of Look, Remember, Reason wherein visual
information is extracted using low-level visual skills step-by-step and then
integrated to arrive at a final answer. We demonstrate the effectiveness of our
framework on diverse visual reasoning tasks from the ACRE, CATER,
Something-Else and STAR datasets. Our approach is trainable end-to-end and
surpasses state-of-the-art task-specific methods across these tasks by a large
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1&quot;&gt;Apratim Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panchal_S/0/1/0/all/0/1&quot;&gt;Sunny Panchal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Mingu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1&quot;&gt;Reza Pourreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1&quot;&gt;Pulkit Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1&quot;&gt;Roland Memisevic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04417">
<title>Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04417</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has garnered considerable attention due to its
privacy-preserving feature. Nonetheless, the lack of freedom in managing user
data can lead to group fairness issues, where models are biased towards
sensitive factors such as race or gender. To tackle this issue, this paper
proposes a novel algorithm, fair federated averaging with augmented Lagrangian
method (FFALM), designed explicitly to address group fairness issues in FL.
Specifically, we impose a fairness constraint on the training objective and
solve the minimax reformulation of the constrained optimization problem. Then,
we derive the theoretical upper bound for the convergence rate of FFALM. The
effectiveness of FFALM in improving fairness is shown empirically on CelebA and
UTKFace datasets in the presence of severe statistical heterogeneity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunda_G/0/1/0/all/0/1&quot;&gt;Gerry Windiarto Mohamad Dunda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shenghui Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06564">
<title>Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06564</link>
<description rdf:parseType="Literal">&lt;p&gt;Prescriptive process monitoring methods seek to optimize the performance of
business processes by triggering interventions at runtime, thereby increasing
the probability of positive case outcomes. These interventions are triggered
according to an intervention policy. Reinforcement learning has been put
forward as an approach to learning intervention policies through trial and
error. Existing approaches in this space assume that the number of resources
available to perform interventions in a process is unlimited, an unrealistic
assumption in practice. This paper argues that, in the presence of resource
constraints, a key dilemma in the field of prescriptive process monitoring is
to trigger interventions based not only on predictions of their necessity,
timeliness, or effect but also on the uncertainty of these predictions and the
level of resource utilization. Indeed, committing scarce resources to an
intervention when the necessity or effects of this intervention are highly
uncertain may intuitively lead to suboptimal intervention effects. Accordingly,
the paper proposes a reinforcement learning approach for prescriptive process
monitoring that leverages conformal prediction techniques to consider the
uncertainty of the predictions upon which an intervention decision is based. An
evaluation using real-life datasets demonstrates that explicitly modeling
uncertainty using conformal predictions helps reinforcement learning agents
converge towards policies with higher net intervention gain
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoush_M/0/1/0/all/0/1&quot;&gt;Mahmoud Shoush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1&quot;&gt;Marlon Dumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08097">
<title>EasyTPP: Towards Open Benchmarking Temporal Point Processes. (arXiv:2307.08097v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08097</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous-time event sequences play a vital role in real-world domains such
as healthcare, finance, online shopping, social networks, and so on. To model
such data, temporal point processes (TPPs) have emerged as the most natural and
competitive models, making a significant impact in both academic and
application communities. Despite the emergence of many powerful models in
recent years, there hasn&apos;t been a central benchmark for these models and future
research endeavors. This lack of standardization impedes researchers and
practitioners from comparing methods and reproducing results, potentially
slowing down progress in this field. In this paper, we present EasyTPP, the
first central repository of research assets (e.g., data, models, evaluation
programs, documentations) in the area of event sequence modeling. Our EasyTPP
makes several unique contributions to this area: a unified interface of using
existing datasets and adding new datasets; a wide range of evaluation programs
that are easy to use and extend as well as facilitate reproducible research;
implementations of popular neural TPPs, together with a rich library of modules
by composing which one could quickly build complex models. All the data and
implementation can be found at
\href{https://github.com/ant-research/EasyTemporalPointProcess}{\textcolor{blue}{Github
repository}}. We will actively maintain this benchmark and welcome
contributions from other researchers and practitioners. Our benchmark will help
promote reproducible research in this field, thus accelerating research
progress as well as making more significant real-world impacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1&quot;&gt;Siqiao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaoming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1&quot;&gt;Hongyan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Caigao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;James Y. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1&quot;&gt;Qingsong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hongyuan Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10266">
<title>A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10266</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) have emerged as an effective approach to tackling
real-world problems. However, like human-written software, DNNs can have bugs
and can be attacked. To address this, research has explored a wide-range of
algorithmic approaches to verify DNN behavior. In this work, we introduce
NeuralSAT, a new verification approach that adapts the widely-used DPLL(T)
algorithm used in modern SMT solvers. A key feature of SMT solvers is the use
of conflict clause learning and search restart to scale verification. Unlike
prior DNN verification approaches, NeuralSAT combines an abstraction-based
deductive theory solver with clause learning and an evaluation clearly
demonstrates the benefits of the approach on a set of challenging verification
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_H/0/1/0/all/0/1&quot;&gt;Hai Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;ThanhVu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwyer_M/0/1/0/all/0/1&quot;&gt;Matthew Dwyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13158">
<title>Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13158</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep reinforcement learning solution for optimizing
multi-UAV cell-association decisions and their moving velocity on a 3D aerial
highway. The objective is to enhance transportation and communication
performance, including collision avoidance, connectivity, and handovers. The
problem is formulated as a Markov decision process (MDP) with UAVs&apos; states
defined by velocities and communication data rates. We propose a neural
architecture with a shared decision module and multiple network branches, each
dedicated to a specific action dimension in a 2D transportation-communication
space. This design efficiently handles the multi-dimensional action space,
allowing independence for individual action dimensions. We introduce two
models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep
Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show
a significant improvement of 18.32% compared to existing benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zijiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaafar_W/0/1/0/all/0/1&quot;&gt;Wael Jaafar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selim_B/0/1/0/all/0/1&quot;&gt;Bassant Selim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabassum_H/0/1/0/all/0/1&quot;&gt;Hina Tabassum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13716">
<title>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13716</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional federated learning uses the number of samples to calculate the
weights of each client model and uses this fixed weight value to fusion the
global model. However, in practical scenarios, each client&apos;s device and data
heterogeneity leads to differences in the quality of each client&apos;s model. Thus
the contribution to the global model is not wholly determined by the sample
size. In addition, if clients intentionally upload low-quality or malicious
models, using these models for aggregation will lead to a severe decrease in
global model accuracy. Traditional federated learning algorithms do not address
these issues. To solve this probelm, we propose FedDRL, a model fusion approach
using reinforcement learning based on a two staged approach. In the first
stage, Our method could filter out malicious models and selects trusted client
models to participate in the model fusion. In the second stage, the FedDRL
algorithm adaptively adjusts the weights of the trusted client models and
aggregates the optimal global model. We also define five model fusion scenarios
and compare our method with two baseline algorithms in those scenarios. The
experimental results show that our algorithm has higher reliability than other
algorithms while maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Leiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Cihao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Sibo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yuming Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chee Wei Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04620">
<title>Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04620</link>
<description rdf:parseType="Literal">&lt;p&gt;We study online multiclass classification under bandit feedback. We extend
the results of Daniely and Helbertal [2013] by showing that the finiteness of
the Bandit Littlestone dimension is necessary and sufficient for bandit online
learnability even when the label space is unbounded. Moreover, we show that,
unlike the full-information setting, sequential uniform convergence is
necessary but not sufficient for bandit online learnability. Our result
complements the recent work by Hanneke, Moran, Raman, Subedi, and Tewari [2023]
who show that the Littlestone dimension characterizes online multiclass
learnability in the full-information setting even when the label space is
unbounded.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1&quot;&gt;Ananth Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1&quot;&gt;Vinod Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subedi_U/0/1/0/all/0/1&quot;&gt;Unique Subedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehalel_I/0/1/0/all/0/1&quot;&gt;Idan Mehalel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09543">
<title>Latent State Models of Training Dynamics. (arXiv:2308.09543v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09543</link>
<description rdf:parseType="Literal">&lt;p&gt;The impact of randomness on model training is poorly understood. How do
differences in data order and initialization actually manifest in the model,
such that some training runs outperform others or converge faster? Furthermore,
how can we interpret the resulting training dynamics and the phase transitions
that characterize different trajectories? To understand the effect of
randomness on the dynamics and outcomes of neural network training, we train
models multiple times with different random seeds and compute a variety of
metrics throughout training, such as the $L_2$ norm, mean, and variance of the
neural network&apos;s weights. We then fit a hidden Markov model (HMM) over the
resulting sequences of metrics. The HMM represents training as a stochastic
process of transitions between latent states, providing an intuitive overview
of significant changes during training. Using our method, we produce a
low-dimensional, discrete representation of training dynamics on grokking
tasks, image classification, and masked language modeling. We use the HMM
representation to study phase transitions and identify latent &quot;detour&quot; states
that slow down convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Michael Y. Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Angelica Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1&quot;&gt;Naomi Saphra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09647">
<title>Robust Uncertainty Quantification Using Conformalised Monte Carlo Prediction. (arXiv:2308.09647v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09647</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying deep learning models in safety-critical applications remains a very
challenging task, mandating the provision of assurances for the dependable
operation of these models. Uncertainty quantification (UQ) methods estimate the
model&apos;s confidence per prediction, informing decision-making by considering the
effect of randomness and model misspecification. Despite the advances of
state-of-the-art UQ methods, they are computationally expensive or produce
conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ
method that combines a new adaptive Monte Carlo (MC) dropout method with
conformal prediction (CP). MC-CP adaptively modulates the traditional MC
dropout at runtime to save memory and computation resources, enabling
predictions to be consumed by CP, yielding robust prediction sets/intervals.
Throughout comprehensive experiments, we show that MC-CP delivers significant
improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in
classification and regression benchmarks. MC-CP can be easily added to existing
models, making its deployment simple.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethell_D/0/1/0/all/0/1&quot;&gt;Daniel Bethell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerasimou_S/0/1/0/all/0/1&quot;&gt;Simos Gerasimou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calinescu_R/0/1/0/all/0/1&quot;&gt;Radu Calinescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11737">
<title>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape. (arXiv:2308.11737v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11737</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately estimating the 3D pose and shape is an essential step towards
understanding animal behavior, and can potentially benefit many downstream
applications, such as wildlife conservation. However, research in this area is
held back by the lack of a comprehensive and diverse dataset with high-quality
3D pose and shape annotations. In this paper, we propose Animal3D, the first
comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D
consists of 3379 images collected from 40 mammal species, high-quality
annotations of 26 keypoints, and importantly the pose and shape parameters of
the SMAL model. All annotations were labeled and checked manually in a
multi-stage process to ensure highest quality results. Based on the Animal3D
dataset, we benchmark representative shape and pose estimation models at: (1)
supervised learning from only the Animal3D data, (2) synthetic to real transfer
from synthetically generated images, and (3) fine-tuning human pose and shape
estimation models. Our experimental results demonstrate that predicting the 3D
shape and pose of animals across species remains a very challenging task,
despite significant advances in human pose estimation. Our results further
demonstrate that synthetic pre-training is a viable strategy to boost the model
performance. Overall, Animal3D opens new directions for facilitating future
research in animal 3D pose and shape estimation, and is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiacong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiawei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wufei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jesslen_A/0/1/0/all/0/1&quot;&gt;Artur Jesslen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1&quot;&gt;Pengliang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qixin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiehua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaoding Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaushik_P/0/1/0/all/0/1&quot;&gt;Prakhar Kaushik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yushan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yawen Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1&quot;&gt;Adam Kortylewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13507">
<title>Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code. (arXiv:2308.13507v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13507</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have significantly improved the ability to
perform tasks in the field of code generation. However, there is still a gap
between LLMs being capable coders and being top-tier software engineers. Based
on the observation that toplevel software engineers often ask clarifying
questions to reduce ambiguity in both requirements and coding solutions, I
argue that the same should be applied to LLMs for code generation tasks. By
asking probing questions in various topics before generating the final code,
the challenges of programming with LLMs, such as unclear intent specification,
lack of computational thinking, and undesired code quality, may be alleviated.
This, in turn, increases confidence in the generated code. In this work, I
explore how to leverage better communication skills to achieve greater
confidence in generated code. I propose a communication-centered process that
uses an LLM-generated communicator to identify issues with high ambiguity or
low confidence in problem descriptions and generated code. I then ask
clarifying questions to obtain responses from users for refining the code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie JW Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13894">
<title>FwdLLM: Efficient FedLLM using Forward Gradient. (arXiv:2308.13894v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13894</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are transforming the landscape of mobile
intelligence. Federated Learning (FL), a method to preserve user data privacy,
is often employed in fine-tuning LLMs to downstream mobile tasks, an approach
known as FedLLM. Though recent efforts have addressed the network issue induced
by the vast model size, they have not practically mitigated vital challenges
concerning integration with mobile devices, such as significant memory
consumption and sluggish model convergence.
&lt;/p&gt;
&lt;p&gt;In response to these challenges, this work introduces FwdLLM, an innovative
FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM
to employ backpropagation (BP)-free training methods, requiring devices only to
execute ``perturbed inferences&apos;&apos;. Consequently, FwdLLM delivers way better
memory efficiency and time efficiency (expedited by mobile NPUs and an expanded
array of participant devices). FwdLLM centers around three key designs: (1) it
combines BP-free training with parameter-efficient training methods, an
essential way to scale the approach to the LLM era; (2) it systematically and
adaptively allocates computational loads across devices, striking a careful
balance between convergence speed and accuracy; (3) it discriminatively samples
perturbed predictions that are more valuable to model convergence.
Comprehensive experiments with five LLMs and three NLP tasks illustrate
FwdLLM&apos;s significant advantages over conventional methods, including up to
three orders of magnitude faster convergence and a 14.6x reduction in memory
footprint. Uniquely, FwdLLM paves the way for federated learning of
billion-parameter LLMs such as LLaMA on COTS mobile devices -- a feat
previously unattained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Dongqi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yaozong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangguang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00733">
<title>TExplain: Explaining Learned Visual Features via Pre-trained (Frozen) Language Models. (arXiv:2309.00733v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00733</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting the learned features of vision models has posed a longstanding
challenge in the field of machine learning. To address this issue, we propose a
novel method that leverages the capabilities of language models to interpret
the learned features of pre-trained image classifiers. Our method, called
TExplain, tackles this task by training a neural network to establish a
connection between the feature space of image classifiers and language models.
Then, during inference, our approach generates a vast number of sentences to
explain the features learned by the classifier for a given image. These
sentences are then used to extract the most frequent words, providing a
comprehensive understanding of the learned features and patterns within the
classifier. Our method, for the first time, utilizes these frequent words
corresponding to a visual representation to provide insights into the
decision-making process of the independently trained classifier, enabling the
detection of spurious correlations, biases, and a deeper comprehension of its
behavior. To validate the effectiveness of our approach, we conduct experiments
on diverse datasets, including ImageNet-9L and Waterbirds. The results
demonstrate the potential of our method to enhance the interpretability and
robustness of image classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1&quot;&gt;Saeid Asgari Taghanaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khani_A/0/1/0/all/0/1&quot;&gt;Aliasghar Khani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1&quot;&gt;Amir Khasahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1&quot;&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Karl D.D. Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Mahdavi-Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01115">
<title>Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province. (arXiv:2309.01115v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01115</link>
<description rdf:parseType="Literal">&lt;p&gt;This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan
industries using matrix normalization. DBSCAN clustering identified 16 feature
classes to objectively group industries. Penalized regression models were then
applied for their advantages in overfitting control, high-dimensional data
processing, and feature selection - well-suited for the complex energy data.
Results showed the second cluster around coal had highest emissions due to
production needs. Emissions from gasoline-focused and coke-focused clusters
were also significant. Based on this, emission reduction suggestions included
clean coal technologies, transportation management, coal-electricity
replacement in steel, and industry standardization. The research introduced
unsupervised learning to objectively select factors and aimed to explore new
emission reduction avenues. In summary, the study identified industry
groupings, assessed emissions drivers, and proposed scientific reduction
strategies to better inform decision-making using algorithms like DBSCAN and
penalized regression models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yonghang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03202">
<title>Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v2 [q-fin.TR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03202</link>
<description rdf:parseType="Literal">&lt;p&gt;This work seeks to answer key research questions regarding the viability of
reinforcement learning over the S&amp;amp;P 500 index. The on-policy techniques of
Value Iteration (VI) and State-action-reward-state-action (SARSA) are
implemented along with the off-policy technique of Q-Learning. The models are
trained and tested on a dataset comprising multiple years of stock market data
from 2000-2023. The analysis presents the results and findings from training
and testing the models using two different time periods: one including the
COVID-19 pandemic years and one excluding them. The results indicate that
including market data from the COVID-19 period in the training dataset leads to
superior performance compared to the baseline strategies. During testing, the
on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the
influence of bias-variance tradeoff and the generalization capabilities of
simpler policies. However, it is noted that the performance of Q-learning may
vary depending on the stability of future market conditions. Future work is
suggested, including experiments with updated Q-learning policies during
testing and trading diverse individual stocks. Additionally, the exploration of
alternative economic indicators for training the models is proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Khare_I/0/1/0/all/0/1&quot;&gt;Ishan S. Khare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Martheswaran_T/0/1/0/all/0/1&quot;&gt;Tarun K. Martheswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dassanaike_Perera_A/0/1/0/all/0/1&quot;&gt;Akshana Dassanaike-Perera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05173">
<title>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer&apos;s quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving substantial memory and time costs compared to vanilla
PT and its variants, without changing trainable parameter sizes. Through
extensive experiments on 23 natural language processing (NLP) and
vision-language (VL) tasks, we demonstrate that DePT outperforms
state-of-the-art PEFT approaches, including the full fine-tuning baseline, in
some scenarios. Additionally, we empirically show that DEPT grows more
efficient as the model size increases. Our further study reveals that DePT
integrates seamlessly with parameter-efficient transfer learning in the
few-shot learning setting and highlights its adaptability to various model
architectures and sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10688">
<title>On the different regimes of Stochastic Gradient Descent. (arXiv:2309.10688v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10688</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern deep networks are trained with stochastic gradient descent (SGD) whose
key hyperparameters are the number of data considered at each step or batch
size $B$, and the step size or learning rate $\eta$. For small $B$ and large
$\eta$, SGD corresponds to a stochastic evolution of the parameters, whose
noise amplitude is governed by the `temperature&apos; $T\equiv \eta/B$. Yet this
description is observed to break down for sufficiently large batches $B\geq
B^*$, or simplifies to gradient descent (GD) when the temperature is
sufficiently small. Understanding where these cross-overs take place remains a
central challenge. Here, we resolve these questions for a teacher-student
perceptron classification model and show empirically that our key predictions
still apply to deep networks. Specifically, we obtain a phase diagram in the
$B$-$\eta$ plane that separates three dynamical phases: \textit{(i)} a
noise-dominated SGD governed by temperature, \textit{(ii)} a
large-first-step-dominated SGD and \textit{(iii)} GD. These different phases
also correspond to different regimes of generalization error. Remarkably, our
analysis reveals that the batch size $B^*$ separating regimes \textit{(i)} and
\textit{(ii)} scale with the size $P$ of the training set, with an exponent
that characterizes the hardness of the classification problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sclocchi_A/0/1/0/all/0/1&quot;&gt;Antonio Sclocchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1&quot;&gt;Matthieu Wyart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11680">
<title>Federated Learning with Neural Graphical Models. (arXiv:2309.11680v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11680</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) addresses the need to create models based on
proprietary data in such a way that multiple clients retain exclusive control
over their data, while all benefit from improved model accuracy due to pooled
resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic
Graphical models that utilize the expressive power of neural networks to learn
complex non-linear dependencies between the input features. They learn to
capture the underlying data distribution and have efficient algorithms for
inference and sampling. We develop a FL framework which maintains a global NGM
model that learns the averaged information from the local NGM models while
keeping the training data within the client&apos;s environment. Our design, FedNGMs,
avoids the pitfalls and shortcomings of neuron matching frameworks like
Federated Matched Averaging that suffers from model parameter explosion. Our
global model size remains constant throughout the process. In the cases where
clients have local variables that are not part of the combined global
distribution, we propose a `Stitching&apos; algorithm, which personalizes the global
NGM models by merging the additional variables using the client&apos;s data. FedNGM
is robust to data heterogeneity, large number of participants, and limited
communication bandwidth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chajewska_U/0/1/0/all/0/1&quot;&gt;Urszula Chajewska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_H/0/1/0/all/0/1&quot;&gt;Harsh Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12701">
<title>Decision Tree Search as a Markov Decision Problem. (arXiv:2309.12701v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12701</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding an optimal decision tree for a supervised learning task is a
challenging combinatorial problem to solve at scale. It was recently proposed
to frame the problem as a Markov Decision Problem (MDP) and use deep
reinforcement learning to tackle scaling. Unfortunately, these methods are not
competitive with the current branch-and-bound state-of-the-art. We propose
instead to scale the resolution of such MDPs using an information-theoretic
tests generating function that heuristically, and dynamically for every state,
limits the set of admissible test actions to a few good candidates. As a
solver, we show empirically that our algorithm is at the very least competitive
with branch-and-bound alternatives. As a machine learning tool, a key advantage
of our approach is to solve for multiple complexity-performance trade-offs at
virtually no additional cost. With such a set of solutions, a user can then
select the tree that generalizes best and which has the interpretability level
that best suits their needs, which no current branch-and-bound method allows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_H/0/1/0/all/0/1&quot;&gt;Hector Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akrour_R/0/1/0/all/0/1&quot;&gt;Riad Akrour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1&quot;&gt;Philippe Preux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13365">
<title>Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13365</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability of AI models allows for user safety checks to build trust in
such AIs. In particular, Decision Trees (DTs) provide a global look at the
learned model and transparently reveal which features of the input are critical
for making a decision. However, interpretability is hindered if the DT is too
large. To learn compact trees, a recent Reinforcement Learning (RL) framework
has been proposed to explore the space of DTs using deep RL. This framework
augments a decision problem (e.g. a supervised classification task) with
additional actions that gather information about the features of an otherwise
hidden input. By appropriately penalizing these actions, the agent learns to
optimally trade-off size and performance of DTs. In practice, a reactive policy
for a partially observable Markov decision process (MDP) needs to be learned,
which is still an open problem. We show in this paper that deep RL can fail
even on simple toy tasks of this class. However, when the underlying decision
problem is a supervised classification task, we show that finding the optimal
tree can be cast as a fully observable Markov decision problem and be solved
efficiently, giving rise to a new family of algorithms for learning DTs that go
beyond the classical greedy maximization ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_H/0/1/0/all/0/1&quot;&gt;Hector Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akrour_R/0/1/0/all/0/1&quot;&gt;Riad Akrour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preux_P/0/1/0/all/0/1&quot;&gt;Philippe Preux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15462">
<title>DTC: Deep Tracking Control. (arXiv:2309.15462v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15462</link>
<description rdf:parseType="Literal">&lt;p&gt;Legged locomotion is a complex control problem that requires both accuracy
and robustness to cope with real-world challenges. Legged systems have
traditionally been controlled using trajectory optimization with inverse
dynamics. Such hierarchical model-based methods are appealing due to intuitive
cost function tuning, accurate planning, generalization, and most importantly,
the insightful understanding gained from more than one decade of extensive
research. However, model mismatch and violation of assumptions are common
sources of faulty operation. Simulation-based reinforcement learning, on the
other hand, results in locomotion policies with unprecedented robustness and
recovery skills. Yet, all learning algorithms struggle with sparse rewards
emerging from environments where valid footholds are rare, such as gaps or
stepping stones. In this work, we propose a hybrid control architecture that
combines the advantages of both worlds to simultaneously achieve greater
robustness, foot-placement accuracy, and terrain generalization. Our approach
utilizes a model-based planner to roll out a reference motion during training.
A deep neural network policy is trained in simulation, aiming to track the
optimized footholds. We evaluate the accuracy of our locomotion pipeline on
sparse terrains, where pure data-driven methods are prone to fail. Furthermore,
we demonstrate superior robustness in the presence of slippery or deformable
ground when compared to model-based counterparts. Finally, we show that our
proposed tracking controller generalizes across different trajectory
optimization methods not seen during training. In conclusion, our work unites
the predictive capabilities and optimality guarantees of online planning with
the inherent robustness attributed to offline learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenelten_F/0/1/0/all/0/1&quot;&gt;Fabian Jenelten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junzhe He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farshidian_F/0/1/0/all/0/1&quot;&gt;Farbod Farshidian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marco Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16034">
<title>Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization. (arXiv:2309.16034v2 [cs.ET] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16034</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in nanotechnology and material science are paving the way toward
nanoscale devices that combine sensing, computing, data and energy storage, and
wireless communication. In precision medicine, these nanodevices show promise
for disease diagnostics, treatment, and monitoring from within the patients&apos;
bloodstreams. Assigning the location of a sensed biological event with the
event itself, which is the main proposition of flow-guided in-body nanoscale
localization, would be immensely beneficial from the perspective of precision
medicine. The nanoscale nature of the nanodevices and the challenging
environment that the bloodstream represents, result in current flow-guided
localization approaches being constrained in their communication and
energy-related capabilities. The communication and energy constraints of the
nanodevices result in different features of raw data for flow-guided
localization, in turn affecting its performance. An analytical modeling of the
effects of imperfect communication and constrained energy causing intermittent
operation of the nanodevices on the raw data produced by the nanodevices would
be beneficial. Hence, we propose an analytical model of raw data for
flow-guided localization, where the raw data is modeled as a function of
communication and energy-related capabilities of the nanodevice. We evaluate
the model by comparing its output with the one obtained through the utilization
of a simulator for objective evaluation of flow-guided localization, featuring
comparably higher level of realism. Our results across a number of scenarios
and heterogeneous performance metrics indicate high similarity between the
model and simulator-generated raw datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascual_G/0/1/0/all/0/1&quot;&gt;Guillem Pascual&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemic_F/0/1/0/all/0/1&quot;&gt;Filip Lemic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delgado_C/0/1/0/all/0/1&quot;&gt;Carmen Delgado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_Perez_X/0/1/0/all/0/1&quot;&gt;Xavier Costa-Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16952">
<title>Leveraging Optimization for Adaptive Attacks on Image Watermarks. (arXiv:2309.16952v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16952</link>
<description rdf:parseType="Literal">&lt;p&gt;Untrustworthy users can misuse image generators to synthesize high-quality
deepfakes and engage in unethical activities. Watermarking deters misuse by
marking generated content with a hidden message, enabling its detection using a
secret watermarking key. A core security property of watermarking is
robustness, which states that an attacker can only evade detection by
substantially degrading image quality. Assessing robustness requires designing
an adaptive attack for the specific watermarking algorithm. When evaluating
watermarking algorithms and their (adaptive) attacks, it is challenging to
determine whether an adaptive attack is optimal, i.e., the best possible
attack. We solve this problem by defining an objective function and then
approach adaptive attacks as an optimization problem. The core idea of our
adaptive attacks is to replicate secret watermarking keys locally by creating
surrogate keys that are differentiable and can be used to optimize the attack&apos;s
parameters. We demonstrate for Stable Diffusion models that such an attacker
can break all five surveyed watermarking methods at no visible degradation in
image quality. Optimizing our attacks is efficient and requires less than 1 GPU
hour to reduce the detection accuracy to 6.3% or less. Our findings emphasize
the need for more rigorous robustness testing against adaptive, learnable
attackers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukas_N/0/1/0/all/0/1&quot;&gt;Nils Lukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaa_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Diaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenaux_L/0/1/0/all/0/1&quot;&gt;Lucas Fenaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1&quot;&gt;Florian Kerschbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01361">
<title>GenSim: Generating Robotic Simulation Tasks via Large Language Models. (arXiv:2310.01361v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01361</link>
<description rdf:parseType="Literal">&lt;p&gt;Collecting large amounts of real-world interaction data to train general
robotic policies is often prohibitively expensive, thus motivating the use of
simulation data. However, existing methods for data generation have generally
focused on scene-level diversity (e.g., object instances and poses) rather than
task-level diversity, due to the human effort required to come up with and
verify novel tasks. This has made it challenging for policies trained on
simulation data to demonstrate significant task-level generalization. In this
paper, we propose to automatically generate rich simulation environments and
expert demonstrations by exploiting a large language models&apos; (LLM) grounding
and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed
generation, wherein a target task is given to the LLM and the LLM proposes a
task curriculum to solve the target task, and exploratory generation, wherein
the LLM bootstraps from previous tasks and iteratively proposes novel tasks
that would be helpful in solving more complex tasks. We use GPT4 to expand the
existing benchmark by ten times to over 100 tasks, on which we conduct
supervised finetuning and evaluate several LLMs including finetuned GPTs and
Code Llama on code generation for robotic simulation tasks. Furthermore, we
observe that LLMs-generated simulation programs can enhance task-level
generalization significantly when used for multitask policy training. We
further find that with minimal sim-to-real adaptation, the multitask policies
pretrained on GPT4-generated simulation tasks exhibit stronger transfer to
unseen long-horizon tasks in the real world and outperform baselines by 25%.
See the project website (https://liruiw.github.io/gensim) for code, demos, and
videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lirui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1&quot;&gt;Yiyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhecheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1&quot;&gt;Mohit Shridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1&quot;&gt;Chen Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bailin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huazhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02255">
<title>MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. (arXiv:2310.02255v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02255</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tony Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1&quot;&gt;Michel Galley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03298">
<title>A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03298</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate
modeling and design optimization by incorporating data from various
low-fidelity (LF) models. While most existing MF methods assume a fixed
dataset, adaptive sampling methods that dynamically allocate resources among
fidelity models can achieve higher efficiency in the exploring and exploiting
the design space. However, most existing MF methods rely on the hierarchical
assumption of fidelity levels or fail to capture the intercorrelation between
multiple fidelity levels and utilize it to quantify the value of the future
samples and navigate the adaptive sampling. To address this hurdle, we propose
a framework hinged on a latent embedding for different fidelity models and the
associated pre-posterior analysis to explicitly utilize their correlation for
adaptive sampling. In this framework, each infill sampling iteration includes
two steps: We first identify the location of interest with the greatest
potential improvement using the high-fidelity (HF) model, then we search for
the next sample across all fidelity levels that maximize the improvement per
unit cost at the location identified in the first step. This is made possible
by a single Latent Variable Gaussian Process (LVGP) model that maps different
fidelity models into an interpretable latent space to capture their
correlations without assuming hierarchical fidelity levels. The LVGP enables us
to assess how LF sampling candidates will affect HF response with pre-posterior
analysis and determine the next sample with the best benefit-to-cost ratio.
Through test cases, we demonstrate that the proposed method outperforms the
benchmark methods in both MF global fitting (GF) and Bayesian Optimization (BO)
problems in convergence rate and robustness. Moreover, the method offers the
flexibility to switch between GF and BO by simply changing the acquisition
function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Ping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Comlek_Y/0/1/0/all/0/1&quot;&gt;Yigitcan Comlek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06333">
<title>Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06333</link>
<description rdf:parseType="Literal">&lt;p&gt;We establish finite-sample guarantees for efficient proper learning of
bounded-degree polytrees, a rich class of high-dimensional probability
distributions and a subclass of Bayesian networks, a widely-studied type of
graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample
guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees.
We extend their results by providing an efficient algorithm which learns
$d$-polytrees in polynomial time and sample complexity for any bounded $d$ when
the underlying undirected graph (skeleton) is known. We complement our
algorithm with an information-theoretic sample complexity lower bound, showing
that the dependence on the dimension and target accuracy parameters are nearly
tight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_D/0/1/0/all/0/1&quot;&gt;Davin Choo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Joy Qiping Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1&quot;&gt;Arnab Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canonne_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment L. Canonne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09126">
<title>Physics-guided Noise Neural Proxy for Practical Low-light Raw Image Denoising. (arXiv:2310.09126v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09126</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the mainstream practice for training low-light raw image denoising
methods has shifted towards employing synthetic data. Noise modeling, which
focuses on characterizing the noise distribution of real-world sensors,
profoundly influences the effectiveness and practicality of synthetic data.
Currently, physics-based noise modeling struggles to characterize the entire
real noise distribution, while learning-based noise modeling impractically
depends on paired real data. In this paper, we propose a novel strategy:
learning the noise model from dark frames instead of paired real data, to break
down the data dependency. Based on this strategy, we introduce an efficient
physics-guided noise neural proxy (PNNP) to approximate the real-world sensor
noise model. Specifically, we integrate physical priors into neural proxies and
introduce three efficient techniques: physics-guided noise decoupling (PND),
physics-guided proxy model (PPM), and differentiable distribution loss (DDL).
PND decouples the dark frame into different components and handles different
levels of noise flexibly, which reduces the complexity of noise modeling. PPM
incorporates physical priors to constrain the generated noise, which promotes
the accuracy of noise modeling. DDL provides explicit and reliable supervision
for noise distribution, which promotes the precision of noise modeling. PNNP
exhibits powerful potential in characterizing the real noise distribution.
Extensive experiments on public datasets demonstrate superior performance in
practical low-light raw image denoising. The code will be available at
\url{https://github.com/fenghansen/PNNP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hansen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12817">
<title>2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12817</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Multimodal Interlaced Transformer (MIT) that jointly considers
2D and 3D data for weakly supervised point cloud segmentation. Research studies
have shown that 2D and 3D features are complementary for point cloud
segmentation. However, existing methods require extra 2D annotations to achieve
2D-3D information fusion. Considering the high annotation cost of point clouds,
effective 2D and 3D feature fusion based on weakly supervised learning is in
great demand. To this end, we propose a transformer model with two encoders and
one decoder for weakly supervised point cloud segmentation using only
scene-level class tags. Specifically, the two encoders compute the
self-attended features for 3D point clouds and 2D multi-view images,
respectively. The decoder implements interlaced 2D-3D cross-attention and
carries out implicit 2D and 3D feature fusion. We alternately switch the roles
of queries and key-value pairs in the decoder layers. It turns out that the 2D
and 3D features are iteratively enriched by each other. Experiments show that
it performs favorably against existing weakly supervised point cloud
segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The
project page will be available at https://jimmy15923.github.io/mit_web/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yung-Yu Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Yu Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17168">
<title>Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17168</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we address the problem of learning and backtesting inventory
control policies in the presence of general arrival dynamics -- which we term
as a quantity-over-time arrivals model (QOT). We also allow for order
quantities to be modified as a post-processing step to meet vendor constraints
such as order minimum and batch size constraints -- a common practice in real
supply chains. To the best of our knowledge this is the first work to handle
either arbitrary arrival dynamics or an arbitrary downstream post-processing of
order quantities. Building upon recent work (Madeka et al., 2022) we similarly
formulate the periodic review inventory control problem as an exogenous
decision process, where most of the state is outside the control of the agent.
Madeka et al., 2022 show how to construct a simulator that replays historic
data to solve this class of problem. In our case, we incorporate a deep
generative model for the arrivals process as part of the history replay. By
formulating the problem as an exogenous decision process, we can apply results
from Madeka et al., 2022 to obtain a reduction to supervised learning. Via
simulation studies we show that this approach yields statistically significant
improvements in profitability over production baselines. Using data from a
real-world A/B test, we show that Gen-QOT generalizes well to off-policy data
and that the resulting buying policy outperforms traditional inventory
management systems in real world settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andaz_S/0/1/0/all/0/1&quot;&gt;Sohrab Andaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenach_C/0/1/0/all/0/1&quot;&gt;Carson Eisenach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1&quot;&gt;Dhruv Madeka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torkkola_K/0/1/0/all/0/1&quot;&gt;Kari Torkkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Randy Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dean Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17544">
<title>Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17544</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel ensemble approach for feature selection based on
hierarchical stacking for non-stationarity and/or a limited number of samples
with a large number of features. Our approach exploits the co-dependency
between features using a hierarchical structure. Initially, a machine learning
model is trained using a subset of features, and then the output of the model
is updated using other algorithms in a hierarchical manner with the remaining
features to minimize the target loss. This hierarchical structure allows for
flexible depth and feature selection. By exploiting feature co-dependency
hierarchically, our proposed approach overcomes the limitations of traditional
feature selection methods and feature importance scores. The effectiveness of
the approach is demonstrated on synthetic and well-known real-life datasets,
providing significant scalable and stable performance improvements compared to
the traditional methods and the state-of-the-art approaches. We also provide
the source code of our approach to facilitate further research and
replicability of our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumay_A/0/1/0/all/0/1&quot;&gt;Aysin Tumay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aydin_M/0/1/0/all/0/1&quot;&gt;Mustafa E. Aydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koc_A/0/1/0/all/0/1&quot;&gt;Ali T. Koc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozat_S/0/1/0/all/0/1&quot;&gt;Suleyman S. Kozat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19491">
<title>Generator Identification for Linear SDEs with Additive and Multiplicative Noise. (arXiv:2310.19491v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19491</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present conditions for identifying the generator of a
linear stochastic differential equation (SDE) from the distribution of its
solution process with a given fixed initial state. These identifiability
conditions are crucial in causal inference using linear SDEs as they enable the
identification of the post-intervention distributions from its observational
distribution. Specifically, we derive a sufficient and necessary condition for
identifying the generator of linear SDEs with additive noise, as well as a
sufficient condition for identifying the generator of linear SDEs with
multiplicative noise. We show that the conditions derived for both types of
SDEs are generic. Moreover, we offer geometric interpretations of the derived
identifiability conditions to enhance their understanding. To validate our
theoretical results, we perform a series of simulations, which support and
substantiate the established findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xi Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Biwei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Mingming Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00931">
<title>Learning Defect Prediction from Unrealistic Data. (arXiv:2311.00931v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00931</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained models of code, such as CodeBERT and CodeT5, have become popular
choices for code understanding and generation tasks. Such models tend to be
large and require commensurate volumes of training data, which are rarely
available for downstream tasks. Instead, it has become popular to train models
with far larger but less realistic datasets, such as functions with
artificially injected bugs. Models trained on such data, however, tend to only
perform well on similar data, while underperforming on real world programs. In
this paper, we conjecture that this discrepancy stems from the presence of
distracting samples that steer the model away from the real-world task
distribution. To investigate this conjecture, we propose an approach for
identifying the subsets of these large yet unrealistic datasets that are most
similar to examples in real-world datasets based on their learned
representations. Our approach extracts high-dimensional embeddings of both
real-world and artificial programs using a neural model and scores artificial
samples based on their distance to the nearest real-world sample. We show that
training on only the nearest, representationally most similar samples while
discarding samples that are not at all similar in representations yields
consistent improvements across two popular pretrained models of code on two
code understanding tasks. Our results are promising, in that they show that
training models on a representative subset of an unrealistic dataset can help
us harness the power of large-scale synthetic data generation while preserving
downstream task performance. Finally, we highlight the limitations of applying
AI models for predicting vulnerabilities and bugs in real-world applications
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alrashedy_K/0/1/0/all/0/1&quot;&gt;Kamel Alrashedy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1&quot;&gt;Vincent J. Hellendoorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orso_A/0/1/0/all/0/1&quot;&gt;Alessandro Orso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02332">
<title>Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02332</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also highlights the need for
principled assessments and practical implementation of such models, bringing
attention to the dynamics between decision support systems and healthcare
providers and personnel. Despite advancements, challenges such as data biases
and the scarcity of &quot;big data&quot; in many biomedical domains persist. We conclude
with a discussion on principled innovation and collaborative efforts to further
the mission of seamless integration of multimodal ML models into biomedical
practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1&quot;&gt;Elisa Warner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joonsang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;William Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1&quot;&gt;Tanveer Syeda-Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1&quot;&gt;Charles Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1&quot;&gt;Olivier Gevaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Arvind Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03242">
<title>Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures. (arXiv:2311.03242v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03242</link>
<description rdf:parseType="Literal">&lt;p&gt;We sample from a given target distribution by constructing a neural network
which maps samples from a simple reference, e.g. the standard normal
distribution, to samples from the target. To that end, we propose using a
neural network architecture inspired by the Langevin Monte Carlo (LMC)
algorithm. Based on LMC perturbation results, we show approximation rates of
the proposed architecture for smooth, log-concave target distributions measured
in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of
sub-Gaussianity of the intermediate measures of the perturbed LMC process. In
particular, we derive bounds on the growth of the intermediate variance proxies
under different assumptions on the perturbations. Moreover, we propose an
architecture similar to deep residual neural networks and derive expressivity
results for approximating the sample to target distribution map.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miranda_C/0/1/0/all/0/1&quot;&gt;Charles Miranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutte_J/0/1/0/all/0/1&quot;&gt;Janina Sch&amp;#xfc;tte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_D/0/1/0/all/0/1&quot;&gt;David Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eigel_M/0/1/0/all/0/1&quot;&gt;Martin Eigel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05587">
<title>Bayesian Methods for Media Mix Modelling with shape and funnel effects. (arXiv:2311.05587v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05587</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, significant progress in generative AI has highlighted the
important role of physics-inspired models that utilize advanced mathematical
concepts based on fundamental physics principles to enhance artificial
intelligence capabilities. Among these models, those based on diffusion
equations have greatly improved image quality. This study aims to explore the
potential uses of Maxwell-Boltzmann equation, which forms the basis of the
kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix
Modelling (MMM) applications. We propose incorporating these equations into
Hierarchical Bayesian models to analyse consumer behaviour in the context of
advertising. These equation sets excel in accurately describing the random
dynamics in complex systems like social interactions and consumer-advertising
interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marin_J/0/1/0/all/0/1&quot;&gt;Javier Marin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06101">
<title>In-Context Learning for MIMO Equalization Using Transformer-Based Sequence Models. (arXiv:2311.06101v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06101</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained sequence models, such as transformer-based architectures,
have been recently shown to have the capacity to carry out in-context learning
(ICL). In ICL, a decision on a new input is made via a direct mapping of the
input and of a few examples from the given task, serving as the task&apos;s context,
to the output variable. No explicit updates of the model parameters are needed
to tailor the decision to a new task. Pre-training, which amounts to a form of
meta-learning, is based on the observation of examples from several related
tasks. Prior work has shown ICL capabilities for linear regression. In this
study, we leverage ICL to address the inverse problem of multiple-input and
multiple-output (MIMO) equalization based on a context given by pilot symbols.
A task is defined by the unknown fading channel and by the signal-to-noise
ratio (SNR) level, which may be known. To highlight the practical potential of
the approach, we allow the presence of quantization of the received signals. We
demonstrate via numerical results that transformer-based ICL has a threshold
behavior, whereby, as the number of pre-training tasks grows, the performance
switches from that of a minimum mean squared error (MMSE) equalizer with a
prior determined by the pre-trained tasks to that of an MMSE equalizer with the
true data-generating prior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zecchin_M/0/1/0/all/0/1&quot;&gt;Matteo Zecchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06558">
<title>Convolve and Conquer: Data Comparison with Wiener Filters. (arXiv:2311.06558v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06558</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative evaluations of differences and/or similarities between data
samples define and shape optimisation problems associated with learning data
distributions. Current methods to compare data often suffer from limitations in
capturing such distributions or lack desirable mathematical properties for
optimisation (e.g. smoothness, differentiability, or convexity). In this paper,
we introduce a new method to measure (dis)similarities between paired samples
inspired by Wiener-filter theory. The convolutional nature of Wiener filters
allows us to comprehensively compare data samples in a globally correlated way.
We validate our approach in four machine learning applications: data
compression, medical imaging imputation, translated classification, and
non-parametric generative modelling. Our results demonstrate increased
resolution in reconstructed images with better perceptual quality and higher
data fidelity, as well as robustness against translations, compared to
conventional mean-squared-error analogue implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1&quot;&gt;Deborah Pelacani Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strong_G/0/1/0/all/0/1&quot;&gt;George Strong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bates_O/0/1/0/all/0/1&quot;&gt;Oscar Bates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cueto_C/0/1/0/all/0/1&quot;&gt;Carlos Cueto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiashun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guasch_L/0/1/0/all/0/1&quot;&gt;Lluis Guasch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09018">
<title>On the Foundation of Distributionally Robust Reinforcement Learning. (arXiv:2311.09018v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09018</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the need for a robust policy in the face of environment shifts
between training and the deployment, we contribute to the theoretical
foundation of distributionally robust reinforcement learning (DRRL). This is
accomplished through a comprehensive modeling framework centered around
distributionally robust Markov decision processes (DRMDPs). This framework
obliges the decision maker to choose an optimal policy under the worst-case
distributional shift orchestrated by an adversary. By unifying and extending
existing formulations, we rigorously construct DRMDPs that embraces various
modeling attributes for both the decision maker and the adversary. These
attributes include adaptability granularity, exploring history-dependent,
Markov, and Markov time-homogeneous decision maker and adversary dynamics.
Additionally, we delve into the flexibility of shifts induced by the adversary,
examining SA and S-rectangularity. Within this DRMDP framework, we investigate
conditions for the existence or absence of the dynamic programming principle
(DPP). From an algorithmic standpoint, the existence of DPP holds significant
implications, as the vast majority of existing data and computationally
efficiency RL algorithms are reliant on the DPP. To study its existence, we
comprehensively examine combinations of controller and adversary attributes,
providing streamlined proofs grounded in a unified methodology. We also offer
counterexamples for settings in which a DPP with full generality is absent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1&quot;&gt;Nian Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1&quot;&gt;Jose Blanchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09127">
<title>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts. (arXiv:2311.09127v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09127</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing work on jailbreak Multimodal Large Language Models (MLLMs) has
focused primarily on adversarial examples in model inputs, with less attention
to vulnerabilities, especially in model API. To fill the research gap, we carry
out the following work: 1) We discover a system prompt leakage vulnerability in
GPT-4V. Through carefully designed dialogue, we successfully extract the
internal system prompts of GPT-4V. This finding indicates potential exploitable
security risks in MLLMs; 2) Based on the acquired system prompts, we propose a
novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via
System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim
to search for potential jailbreak prompts leveraging stolen system prompts.
Furthermore, in pursuit of better performance, we also add human modification
based on GPT-4&apos;s analysis, which further improves the attack success rate to
98.7\%; 3) We evaluated the effect of modifying system prompts to defend
against jailbreaking attacks. Results show that appropriately designed system
prompts can significantly reduce jailbreak success rates. Overall, our work
provides new insights into enhancing MLLM security, demonstrating the important
role of system prompts in jailbreaking. This finding could be leveraged to
greatly facilitate jailbreak success rates while also holding the potential for
defending against jailbreaks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuanwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11990">
<title>Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN. (arXiv:2311.11990v2 [cond-mat.mtrl-sci] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11990</link>
<description rdf:parseType="Literal">&lt;p&gt;Using the atomic cluster expansion (ACE) framework, we develop a machine
learning interatomic potential for fast and accurately modelling the phonon
transport properties of wurtzite aluminum nitride. The predictive power of the
ACE potential against density functional theory (DFT) is demonstrated across a
broad range of properties of w-AlN, including ground-state lattice parameters,
specific heat capacity, coefficients of thermal expansion, bulk modulus, and
harmonic phonon dispersions. Validation of lattice thermal conductivity is
further carried out by comparing the ACE-predicted values to the DFT
calculations and experiments, exhibiting the overall capability of our ACE
potential in sufficiently describing anharmonic phonon interactions. As a
practical application, we perform a lattice dynamics analysis using the
potential to unravel the effects of biaxial strains on thermal conductivity and
phonon properties of w-AlN, which is identified as a significant tuning factor
for near-junction thermal design of w-AlN-based electronics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan-Bin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bing-Yang Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14212">
<title>Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14212</link>
<description rdf:parseType="Literal">&lt;p&gt;When training data are collected from human annotators, the design of the
annotation instrument, the instructions given to annotators, the
characteristics of the annotators, and their interactions can impact training
data. This study demonstrates that design choices made when creating an
annotation instrument also impact the models trained on the resulting
annotations. We introduce the term annotation sensitivity to refer to the
impact of annotation data collection methods on the annotations themselves and
on downstream model performance and predictions. We collect annotations of hate
speech and offensive language in five experimental conditions of an annotation
instrument, randomly assigning annotators to conditions. We then fine-tune BERT
models on each of the five resulting datasets and evaluate model performance on
a holdout portion of each condition. We find considerable differences between
the conditions for 1) the share of hate speech/offensive language annotations,
2) model performance, 3) model predictions, and 4) model learning curves. Our
results emphasize the crucial role played by the annotation instrument which
has received little attention in the machine learning literature. We call for
additional research into how and why the instrument impacts the annotations to
inform the development of best practices in instrument design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kern_C/0/1/0/all/0/1&quot;&gt;Christoph Kern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Eckman_S/0/1/0/all/0/1&quot;&gt;Stephanie Eckman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Beck_J/0/1/0/all/0/1&quot;&gt;Jacob Beck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chew_R/0/1/0/all/0/1&quot;&gt;Rob Chew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Bolei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kreuter_F/0/1/0/all/0/1&quot;&gt;Frauke Kreuter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16141">
<title>Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16141</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs) have gained considerable attention due to the
energy-efficient and multiplication-free characteristics. The continuous growth
in scale of deep SNNs poses challenges for model deployment. Network pruning
reduces hardware resource requirements of model deployment by compressing the
network scale. However, existing SNN pruning methods cause high pruning costs
and performance loss because the pruning iterations amplify the training
difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in
neuroscience, we propose a regeneration mechanism based on the neuron
criticality for SNN pruning to enhance feature extraction and accelerate the
pruning process. Firstly, we propose a low-cost metric for the criticality in
SNNs. Then, we re-rank the pruned structures after pruning and regenerate those
with higher criticality to obtain the critical network. Our method achieves
higher performance than the current state-of-the-art (SOTA) method with up to
95.26% reduction of pruning cost. Moreover, we investigate the underlying
mechanism of our method and find that it efficiently selects potential
structures and learns the consistent feature representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Boxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1&quot;&gt;Haihang You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00157">
<title>Universal Backdoor Attacks. (arXiv:2312.00157v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00157</link>
<description rdf:parseType="Literal">&lt;p&gt;Web-scraped datasets are vulnerable to data poisoning, which can be used for
backdooring deep image classifiers during training. Since training on large
datasets is expensive, a model is trained once and re-used many times. Unlike
adversarial examples, backdoor attacks often target specific classes rather
than any class learned by the model. One might expect that targeting many
classes through a naive composition of attacks vastly increases the number of
poison samples. We show this is not necessarily true and more efficient,
universal data poisoning attacks exist that allow controlling
misclassifications from any source class into any target class with a small
increase in poison samples. Our idea is to generate triggers with salient
characteristics that the model can learn. The triggers we craft exploit a
phenomenon we call inter-class poison transferability, where learning a trigger
from one class makes the model more vulnerable to learning triggers for other
classes. We demonstrate the effectiveness and robustness of our universal
backdoor attacks by controlling models with up to 6,000 classes while poisoning
only 0.15% of the training dataset. Our source code is available at
https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_B/0/1/0/all/0/1&quot;&gt;Benjamin Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukas_N/0/1/0/all/0/1&quot;&gt;Nils Lukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerschbaum_F/0/1/0/all/0/1&quot;&gt;Florian Kerschbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02063">
<title>The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet Transits. (arXiv:2312.02063v2 [astro-ph.EP] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02063</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents GPFC, a novel Graphics Processing Unit (GPU) Phase
Folding and Convolutional Neural Network (CNN) system to detect exoplanets
using the transit method. We devise a fast folding algorithm parallelized on a
GPU to amplify low signal-to-noise ratio transit signals, allowing a search at
high precision and speed. A CNN trained on two million synthetic light curves
reports a score indicating the likelihood of a planetary signal at each period.
While the GPFC method has broad applicability across period ranges, this
research specifically focuses on detecting ultra-short-period planets with
orbital periods less than one day. GPFC improves on speed by three orders of
magnitude over the predominant Box-fitting Least Squares (BLS) method. Our
simulation results show GPFC achieves $97%$ training accuracy, higher true
positive rate at the same false positive rate of detection, and higher
precision at the same recall rate when compared to BLS. GPFC recovers $100\%$
of known ultra-short-period planets in $\textit{Kepler}$ light curves from a
blind search. These results highlight the promise of GPFC as an alternative
approach to the traditional BLS algorithm for finding new transiting exoplanets
in data taken with $\textit{Kepler}$ and other space transit missions such as
K2, TESS and future PLATO and Earth 2.0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaitlyn Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jian Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Kevin Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kevin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yinan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02277">
<title>ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02277</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits a class of convex Finite-Sum Coupled Compositional
Stochastic Optimization (cFCCO) problems with many applications, including
group distributionally robust optimization (GDRO), learning with imbalanced
data, reinforcement learning, and learning to rank. To better solve these
problems, we introduce an efficient single-loop primal-dual block-coordinate
proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate
stochastic mirror ascent updates for the dual variable and stochastic proximal
gradient descent updates for the primal variable. We establish the convergence
rates of ALEXR in both convex and strongly convex cases under smoothness and
non-smoothness conditions of involved functions, which not only improve the
best rates in previous works on smooth cFCCO problems but also expand the realm
of cFCCO for solving more challenging non-smooth problems such as the dual form
of GDRO. Finally, we present lower complexity bounds to demonstrate that the
convergence rates of ALEXR are optimal among first-order block-coordinate
stochastic algorithms for the considered class of cFCCO problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bokun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02471">
<title>Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks. (arXiv:2312.02471v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02471</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational offloading has become an enabling component for edge
intelligence in mobile and smart devices. Existing offloading schemes mainly
focus on mobile devices and servers, while ignoring the potential network
congestion caused by tasks from multiple mobile devices, especially in wireless
multi-hop networks. To fill this gap, we propose a low-overhead,
congestion-aware distributed task offloading scheme by augmenting a distributed
greedy framework with graph-based machine learning. In simulated wireless
multi-hop networks with 20-110 nodes and a resource allocation scheme based on
shortest path routing and contention-based link scheduling, our approach is
demonstrated to be effective in reducing congestion or unstable queues under
the context-agnostic baseline, while improving the execution latency over local
computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perazzone_J/0/1/0/all/0/1&quot;&gt;Jake Perazzone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1&quot;&gt;Gunjan Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1&quot;&gt;Santiago Segarra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03311">
<title>On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03311</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel methods are a popular class of nonlinear predictive models in machine
learning. Scalable algorithms for learning kernel models need to be iterative
in nature, but convergence can be slow due to poor conditioning. Spectral
preconditioning is an important tool to speed-up the convergence of such
iterative algorithms for training kernel models. However computing and storing
a spectral preconditioner can be expensive which can lead to large
computational and storage overheads, precluding the application of kernel
methods to problems with large datasets. A Nystrom approximation of the
spectral preconditioner is often cheaper to compute and store, and has
demonstrated success in practical applications. In this paper we analyze the
trade-offs of using such an approximated preconditioner. Specifically, we show
that a sample of logarithmic size (as a function of the size of the dataset)
enables the Nystrom-based approximated preconditioner to accelerate gradient
descent nearly as well as the exact preconditioner, while also reducing the
computational and storage overheads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abedsoltan_A/0/1/0/all/0/1&quot;&gt;Amirhesam Abedsoltan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pandit_P/0/1/0/all/0/1&quot;&gt;Parthe Pandit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rademacher_L/0/1/0/all/0/1&quot;&gt;Luis Rademacher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05134">
<title>Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-distribution learning (MDL), which seeks to learn a shared model that
minimizes the worst-case risk across $k$ distinct data distributions, has
emerged as a unified framework in response to the evolving demand for
robustness, fairness, multi-group collaboration, etc. Achieving data-efficient
MDL necessitates adaptive sampling, also called on-demand sampling, throughout
the learning process. However, there exist substantial gaps between the
state-of-the-art upper and lower bounds on the optimal sample complexity.
Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we
propose a novel algorithm that yields an $varepsilon$-optimal randomized
hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$
(modulo some logarithmic factor), matching the best-known lower bound. Our
algorithmic ideas and theory have been further extended to accommodate
Rademacher classes. The proposed algorithms are oracle-efficient, which access
the hypothesis class solely through an empirical risk minimization oracle.
Additionally, we establish the necessity of randomization, unveiling a large
sample size barrier when only deterministic hypotheses are permitted. These
findings successfully resolve three open problems presented in COLT 2023 (i.e.,
Awasthi et al., (2023, Problem 1, 3 and 4)).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wenhao Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07178">
<title>Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07178</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications in Reinforcement Learning (RL) usually have noise or
stochasticity present in the environment. Beyond their impact on learning,
these uncertainties lead the exact same policy to perform differently, i.e.
yield different return, from one roll-out to another. Common evaluation
procedures in RL summarise the consequent return distributions using solely the
expected return, which does not account for the spread of the distribution. Our
work defines this spread as the policy reproducibility: the ability of a policy
to obtain similar performance when rolled out many times, a crucial property in
some real-world applications. We highlight that existing procedures that only
use the expected return are limited on two fronts: first an infinite number of
return distributions with a wide range of performance-reproducibility
trade-offs can have the same expected return, limiting its effectiveness when
used for comparing policies; second, the expected return metric does not leave
any room for practitioners to choose the best trade-off value for considered
applications. In this work, we address these limitations by recommending the
use of Lower Confidence Bound, a metric taken from Bayesian optimisation that
provides the user with a preference parameter to choose a desired
performance-reproducibility trade-off. We also formalise and quantify policy
reproducibility, and demonstrate the benefit of our metrics using extensive
experiments of popular RL algorithms on common uncertain RL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flageat_M/0/1/0/all/0/1&quot;&gt;Manon Flageat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07930">
<title>Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07930</link>
<description rdf:parseType="Literal">&lt;p&gt;We study statistical watermarking by formulating it as a hypothesis testing
problem, a general framework which subsumes all previous statistical
watermarking methods. Key to our formulation is a coupling of the output tokens
and the rejection region, realized by pseudo-random generators in practice,
that allows non-trivial trade-off between the Type I error and Type II error.
We characterize the Uniformly Most Powerful (UMP) watermark in the general
hypothesis testing setting and the minimax Type II error in the model-agnostic
setting. In the common scenario where the output is a sequence of $n$ tokens,
we establish nearly matching upper and lower bounds on the number of i.i.d.
tokens required to guarantee small Type I and Type II errors. Our rate of
$\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$
highlights potentials for improvement from the rate of $h^{-2}$ in the previous
works. Moreover, we formulate the robust watermarking problem where users are
allowed to perform a class of perturbations on the generated texts, and
characterize the optimal type II error of robust UMP tests via a linear
programming problem. To the best of our knowledge, this is the first systematic
statistical treatment on the watermarking problem with near-optimal rates in
the i.i.d. setting, which might be of interest for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Baihe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanlin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10305">
<title>Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10305</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech signals are inherently complex as they encompass both global acoustic
characteristics and local semantic information. However, in the task of target
speech extraction, certain elements of global and local semantic information in
the reference speech, which are irrelevant to speaker identity, can lead to
speaker confusion within the speech extraction network. To overcome this
challenge, we propose a self-supervised disentangled representation learning
method. Our approach tackles this issue through a two-phase process, utilizing
a reference speech encoding network and a global information disentanglement
network to gradually disentangle the speaker identity information from other
irrelevant factors. We exclusively employ the disentangled speaker identity
information to guide the speech extraction network. Moreover, we introduce the
adaptive modulation Transformer to ensure that the acoustic representation of
the mixed signal remains undisturbed by the speaker embeddings. This component
incorporates speaker embeddings as conditional information, facilitating
natural and efficient guidance for the speech extraction network. Experimental
results substantiate the effectiveness of our meticulously crafted approach,
showcasing a substantial reduction in the likelihood of speaker confusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Z/0/1/0/all/0/1&quot;&gt;Zhaoxi Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sining Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11413">
<title>DeRDaVa: Deletion-Robust Data Valuation for Machine Learning. (arXiv:2312.11413v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11413</link>
<description rdf:parseType="Literal">&lt;p&gt;Data valuation is concerned with determining a fair valuation of data from
data sources to compensate them or to identify training examples that are the
most or least useful for predictions. With the rising interest in personal data
ownership and data protection regulations, model owners will likely have to
fulfil more data deletion requests. This raises issues that have not been
addressed by existing works: Are the data valuation scores still fair with
deletions? Must the scores be expensively recomputed? The answer is no. To
avoid recomputations, we propose using our data valuation framework DeRDaVa
upfront for valuing each data source&apos;s contribution to preserving robust model
performance after anticipated data deletions. DeRDaVa can be efficiently
approximated and will assign higher values to data that are more useful or less
likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to
risk-averse/seeking model owners who are concerned with the worst/best-cases
model utility. We also empirically demonstrate the practicality of our
solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xiao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1&quot;&gt;Rachael Hwee Ling Sim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jue Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1&quot;&gt;Bryan Kian Hsiang Low&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11532">
<title>Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation. (arXiv:2312.11532v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11532</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach for topic modeling utilizing latent
codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely
encapsulating the rich information of the pre-trained embeddings such as the
pre-trained language model. From the novel interpretation of the latent
codebooks and embeddings as conceptual bag-of-words, we propose a new
generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates
the original documents related to the respective latent codebook. The TVQ-VAE
can visualize the topics with various generative distributions including the
traditional BoW distribution and the autoregressive image generation. Our
experimental results on document analysis and image generation demonstrate that
TVQ-VAE effectively captures the topic context which reveals the underlying
structures of the dataset and supports flexible forms of document generation.
Official implementation of the proposed TVQ-VAE is available at
https://github.com/clovaai/TVQ-VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;YoungJoon Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongwon Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11835">
<title>Provably Convergent Federated Trilevel Learning. (arXiv:2312.11835v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11835</link>
<description rdf:parseType="Literal">&lt;p&gt;Trilevel learning, also called trilevel optimization (TLO), has been
recognized as a powerful modelling tool for hierarchical decision process and
widely applied in many machine learning applications, such as robust neural
architecture search, hyperparameter optimization, and domain adaptation.
Tackling TLO problems has presented a great challenge due to their nested
decision-making structure. In addition, existing works on TLO face the
following key challenges: 1) they all focus on the non-distributed setting,
which may lead to privacy breach; 2) they do not offer any non-asymptotic
convergence analysis which characterizes how fast an algorithm converges. To
address the aforementioned challenges, this paper proposes an asynchronous
federated trilevel optimization method to solve TLO problems. The proposed
method utilizes $\mu$-cuts to construct a hyper-polyhedral approximation for
the TLO problem and solve it in an asynchronous manner. We demonstrate that the
proposed $\mu$-cuts are applicable to not only convex functions but also a wide
range of non-convex functions that meet the $\mu$-weakly convex assumption.
Furthermore, we theoretically analyze the non-asymptotic convergence rate for
the proposed method by showing its iteration complexity to obtain
$\epsilon$-stationary point is upper bounded by
$\mathcal{O}(\frac{1}{\epsilon^2})$. Extensive experiments on real-world
datasets have been conducted to elucidate the superiority of the proposed
method, e.g., it has a faster convergence rate with a maximum acceleration of
approximately 80$\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tiancheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_C/0/1/0/all/0/1&quot;&gt;Chengtao Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianwei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11976">
<title>When Model Meets New Normals: Test-time Adaptation for Unsupervised Time-series Anomaly Detection. (arXiv:2312.11976v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11976</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-series anomaly detection deals with the problem of detecting anomalous
timesteps by learning normality from the sequence of observations. However, the
concept of normality evolves over time, leading to a &quot;new normal problem&quot;,
where the distribution of normality can be changed due to the distribution
shifts between training and test data. This paper highlights the prevalence of
the new normal problem in unsupervised time-series anomaly detection studies.
To tackle this issue, we propose a simple yet effective test-time adaptation
strategy based on trend estimation and a self-supervised approach to learning
new normalities during inference. Extensive experiments on real-world
benchmarks demonstrate that incorporating the proposed strategy into the
anomaly detector consistently improves the model&apos;s performance compared to the
baselines, leading to robustness to the distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sunghyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13118">
<title>LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate. (arXiv:2312.13118v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13118</link>
<description rdf:parseType="Literal">&lt;p&gt;The transferability of adversarial examples is of central importance to
transfer-based black-box adversarial attacks. Previous works for generating
transferable adversarial examples focus on attacking \emph{given} pretrained
surrogate models while the connections between surrogate models and adversarial
trasferability have been overlooked. In this paper, we propose {\em Lipschitz
Regularized Surrogate} (LRS) for transfer-based black-box attacks, a novel
approach that transforms surrogate models towards favorable adversarial
transferability. Using such transformed surrogate models, any existing
transfer-based black-box attack can run without any change, yet achieving much
better performance. Specifically, we impose Lipschitz regularization on the
loss landscape of surrogate models to enable a smoother and more controlled
optimization process for generating more transferable adversarial examples. In
addition, this paper also sheds light on the connection between the inner
properties of surrogate models and adversarial transferability, where three
factors are identified: smaller local Lipschitz constant, smoother loss
landscape, and stronger adversarial robustness. We evaluate our proposed LRS
approach by attacking state-of-the-art standard deep neural networks and
defense models. The results demonstrate significant improvement on the attack
success rates and transferability. Our code is available at
https://github.com/TrustAIoT/LRS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1&quot;&gt;Donald C. Wunsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13141">
<title>Augment on Manifold: Mixup Regularization with UMAP. (arXiv:2312.13141v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13141</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation techniques play an important role in enhancing the
performance of deep learning models. Despite their proven benefits in computer
vision tasks, their application in the other domains remains limited. This
paper proposes a Mixup regularization scheme, referred to as UMAP Mixup,
designed for ``on-manifold&quot; automated data augmentation for deep learning
predictive models. The proposed approach ensures that the Mixup operations
result in synthesized samples that lie on the data manifold of the features and
labels by utilizing a dimensionality reduction technique known as uniform
manifold approximation and projection. Evaluations across diverse regression
tasks show that UMAP Mixup is competitive with or outperforms other Mixup
variants, show promise for its potential as an effective tool for enhancing the
generalization performance of deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Laham_Y/0/1/0/all/0/1&quot;&gt;Yousef El-Laham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fons_E/0/1/0/all/0/1&quot;&gt;Elizabeth Fons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daudert_D/0/1/0/all/0/1&quot;&gt;Dillon Daudert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1&quot;&gt;Svitlana Vyetrenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13152">
<title>Neural Stochastic Differential Equations with Change Points: A Generative Adversarial Approach. (arXiv:2312.13152v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13152</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic differential equations (SDEs) have been widely used to model real
world random phenomena. Existing works mainly focus on the case where the time
series is modeled by a single SDE, which might be restrictive for modeling time
series with distributional shift. In this work, we propose a change point
detection algorithm for time series modeled as neural SDEs. Given a time series
dataset, the proposed method jointly learns the unknown change points and the
parameters of distinct neural SDE models corresponding to each change point.
Specifically, the SDEs are learned under the framework of generative
adversarial networks (GANs) and the change points are detected based on the
output of the GAN discriminator in a forward pass. At each step of the proposed
algorithm, the change points and the SDE model parameters are updated in an
alternating fashion. Numerical results on both synthetic and real datasets are
provided to validate the performance of our algorithm in comparison to
classical change point detection benchmarks, standard GAN-based neural SDEs,
and other state-of-the-art deep generative models for time series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhongchang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Laham_Y/0/1/0/all/0/1&quot;&gt;Yousef El-Laham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1&quot;&gt;Svitlana Vyetrenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16113">
<title>Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction. (arXiv:2312.16113v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16113</link>
<description rdf:parseType="Literal">&lt;p&gt;Since artificial intelligence has seen tremendous recent successes in many
areas, it has sparked great interest in its potential for trustworthy and
interpretable risk prediction. However, most models lack causal reasoning and
struggle with class imbalance, leading to poor precision and recall. To address
this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to
transform original feature values into causal feature attributions for the
specific risk prediction task. The causal feature attribution helps describe
how much contribution the value of this feature can make to the risk prediction
result. After the causal feature distillation, a deep neural network is applied
to produce trustworthy prediction results with causal interpretability and high
precision/recall. We evaluate the performance of our TDCFD method on several
synthetic and real datasets, and the results demonstrate its superiority over
the state-of-the-art methods regarding precision, recall, interpretability, and
causality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mengxuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1&quot;&gt;Qing Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16313">
<title>Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16313</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning datasets may contain multiple cues that explain the
training set equally well, i.e., learning any of them would lead to the correct
predictions on the training data. However, many of them can be spurious, i.e.,
lose their predictive power under a distribution shift and consequently fail to
generalize to out-of-distribution (OOD) data. Recently developed
&quot;diversification&quot; methods (Lee et al., 2023; Pagliardini et al., 2023) approach
this problem by finding multiple diverse hypotheses that rely on different
features. This paper aims to study this class of methods and identify the key
components contributing to their OOD generalization abilities.
&lt;/p&gt;
&lt;p&gt;We show that (1) diversification methods are highly sensitive to the
distribution of the unlabeled data used for diversification and can
underperform significantly when away from a method-specific sweet spot. (2)
Diversification alone is insufficient for OOD generalization. The choice of the
used learning algorithm, e.g., the model&apos;s architecture and pretraining, is
crucial. In standard experiments (classification on Waterbirds and Office-Home
datasets), using the second-best choice leads to an up to 20\% absolute drop in
accuracy. (3) The optimal choice of learning algorithm depends on the unlabeled
data and vice versa i.e. they are co-dependent. (4) Finally, we show that, in
practice, the above pitfalls cannot be alleviated by increasing the number of
diverse hypotheses, the major feature of diversification methods.
&lt;/p&gt;
&lt;p&gt;These findings provide a clearer understanding of the critical design factors
influencing the OOD generalization abilities of diversification methods. They
can guide practitioners in how to use the existing methods best and guide
researchers in developing new, better ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benoit_H/0/1/0/all/0/1&quot;&gt;Harold Benoit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liangze Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1&quot;&gt;Andrei Atanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_O/0/1/0/all/0/1&quot;&gt;O&amp;#x11f;uzhan Fatih Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1&quot;&gt;Mattia Rigotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1&quot;&gt;Amir Zamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00313">
<title>Matching of Users and Creators in Two-Sided Markets with Departures. (arXiv:2401.00313v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00313</link>
<description rdf:parseType="Literal">&lt;p&gt;Many online platforms of today, including social media sites, are two-sided
markets bridging content creators and users. Most of the existing literature on
platform recommendation algorithms largely focuses on user preferences and
decisions, and does not simultaneously address creator incentives. We propose a
model of content recommendation that explicitly focuses on the dynamics of
user-content matching, with the novel property that both users and creators may
leave the platform permanently if they do not experience sufficient engagement.
In our model, each player decides to participate at each time step based on
utilities derived from the current match: users based on alignment of the
recommended content with their preferences, and creators based on their
audience size. We show that a user-centric greedy algorithm that does not
consider creator departures can result in arbitrarily poor total engagement,
relative to an algorithm that maximizes total engagement while accounting for
two-sided departures. Moreover, in stark contrast to the case where only users
or only creators leave the platform, we prove that with two-sided departures,
approximating maximum total engagement within any constant factor is NP-hard.
We present two practical algorithms, one with performance guarantees under mild
assumptions on user preferences, and another that tends to outperform
algorithms that ignore two-sided departures in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huttenlocher_D/0/1/0/all/0/1&quot;&gt;Daniel Huttenlocher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hannah Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Liang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1&quot;&gt;Asuman Ozdaglar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siderius_J/0/1/0/all/0/1&quot;&gt;James Siderius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00334">
<title>Explainability-Driven Leaf Disease Classification using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00334</link>
<description rdf:parseType="Literal">&lt;p&gt;This work focuses on plant leaf disease classification and explores three
crucial aspects: adversarial training, model explainability, and model
compression. The models&apos; robustness against adversarial attacks is enhanced
through adversarial training, ensuring accurate classification even in the
presence of threats. Leveraging explainability techniques, we gain insights
into the model&apos;s decision-making process, improving trust and transparency.
Additionally, we explore model compression techniques to optimize computational
efficiency while maintaining classification performance. Through our
experiments, we determine that on a benchmark dataset, the robustness can be
the price of the classification accuracy with performance reductions of 3%-20%
for regular tests and gains of 50%-70% for adversarial attack tests. We also
demonstrate that a student model can be 15-25 times more computationally
efficient for a slight performance reduction, distilling the knowledge of more
complex models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echim_S/0/1/0/all/0/1&quot;&gt;Sebastian-Vasile Echim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taiatu_I/0/1/0/all/0/1&quot;&gt;Iulian-Marius T&amp;#x103;iatu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1&quot;&gt;Dumitru-Clementin Cercel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1&quot;&gt;Florin Pop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01084">
<title>Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01084</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural policy gradient (NPG) and its variants are widely-used policy search
methods in reinforcement learning. Inspired by prior work, a new NPG variant
coined NPG-HM is developed in this paper, which utilizes the Hessian-aided
momentum technique for variance reduction, while the sub-problem is solved via
the stochastic gradient descent method. It is shown that NPG-HM can achieve the
global last iterate $\epsilon$-optimality with a sample complexity of
$\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy
gradient type methods under the generic Fisher non-degenerate policy
parameterizations. The convergence analysis is built upon a relaxed weak
gradient dominance property tailored for NPG under the compatible function
approximation framework, as well as a neat way to decompose the error when
handling the sub-problem. Moreover, numerical experiments on Mujoco-based
environments demonstrate the superior performance of NPG-HM over other
state-of-the-art policy gradient methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Ke Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinchi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01841">
<title>Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01841</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely&apos;&apos; to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,&apos;&apos; we disintegrate the aleatoric and epistemic uncertainty in the
agent&apos;s updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03197">
<title>Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03197</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential decision-making under uncertainty is present in many important
problems. Two popular approaches for tackling such problems are reinforcement
learning and online search (e.g., Monte Carlo tree search). While the former
learns a policy by interacting with the environment (typically done before
execution), the latter uses a generative model of the environment to sample
promising action trajectories at decision time. Decision-making is particularly
challenging in non-stationary environments, where the environment in which an
agent operates can change over time. Both approaches have shortcomings in such
settings -- on the one hand, policies learned before execution become stale
when the environment changes and relearning takes both time and computational
effort. Online search, on the other hand, can return sub-optimal actions when
there are limitations on allowed runtime. In this paper, we introduce
\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines
action-value estimates from an out-of-date policy with an online search using
an up-to-date model of the environment. We prove theoretical results showing
conditions under which PA-MCTS selects the one-step optimal action and also
bound the error accrued while following PA-MCTS as a policy. We compare and
contrast our approach with AlphaZero, another hybrid planning approach, and
Deep Q Learning on several OpenAI Gym environments. Through extensive
experiments, we show that under non-stationary settings with limited time
constraints, PA-MCTS outperforms these baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pettet_A/0/1/0/all/0/1&quot;&gt;Ava Pettet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wray_K/0/1/0/all/0/1&quot;&gt;Kyle Wray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baier_H/0/1/0/all/0/1&quot;&gt;Hendrik Baier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszka_A/0/1/0/all/0/1&quot;&gt;Aron Laszka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03506">
<title>DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03506</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce DiarizationLM, a framework to leverage large
language models (LLM) to post-process the outputs from a speaker diarization
system. Various goals can be achieved with the proposed framework, such as
improving the readability of the diarized transcript, or reducing the word
diarization error rate (WDER). In this framework, the outputs of the automatic
speech recognition (ASR) and speaker diarization systems are represented as a
compact textual format, which is included in the prompt to an optionally
finetuned LLM. The outputs of the LLM can be used as the refined diarization
results with the desired enhancement. As a post-processing step, this framework
can be easily applied to any off-the-shelf ASR and speaker diarization systems
without retraining existing components. Our experiments show that a finetuned
PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone
conversation dataset, and rel. 44.9% on the Callhome English dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guanlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clark_E/0/1/0/all/0/1&quot;&gt;Evan Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hank Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06144">
<title>DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06144</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion generative models have achieved remarkable success in generating
images with a fixed resolution. However, existing models have limited ability
to generalize to different resolutions when training data at those resolutions
are not available. Leveraging techniques from operator learning, we present a
novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the
score operator by combining both spatial and spectral information at multiple
resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1)
simultaneously training on multiple resolutions improves FID over training at
any single fixed resolution; 2) DFU generalizes beyond its training
resolutions, allowing for coherent, high-fidelity generation at
higher-resolutions with the same model, i.e. zero-shot super-resolution
image-generation; 3) we propose a fine-tuning strategy to further enhance the
zero-shot super-resolution image-generation capability of our model, leading to
a FID of 11.3 at 1.66 times the maximum training resolution on FFHQ, which no
other method can come close to achieving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1&quot;&gt;Alex Havrilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_K/0/1/0/all/0/1&quot;&gt;Kevin Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenjing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Molei Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07364">
<title>PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws. (arXiv:2401.07364v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07364</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we build a single large model for a wide range of PDE-related scientific
learning tasks? Can this model generalize to new PDEs, even of new forms,
without any fine-tuning? In-context operator learning and the corresponding
model In-Context Operator Networks (ICON) represent an initial exploration of
these questions. The capability of ICON regarding the first question has been
demonstrated previously. In this paper, we present a detailed methodology for
solving PDE problems with ICON, and show how a single ICON model can make
forward and reverse predictions for different equations with different strides,
provided with appropriately designed data prompts. We show the positive
evidence to the second question, i.e., ICON can generalize well to some PDEs
with new forms without any fine-tuning. This is exemplified through a study on
1D scalar nonlinear conservation laws, a family of PDEs with temporal
evolution. We also show how to broaden the range of problems that an ICON model
can address, by transforming functions and equations to ICON&apos;s capability
scope. We believe that the progress in this paper is a significant step towards
the goal of training a foundation model for PDE-related tasks under the
in-context operator learning framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley J. Osher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07656">
<title>Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07656</link>
<description rdf:parseType="Literal">&lt;p&gt;Strategies for partially observable Markov decision processes (POMDP)
typically require memory. One way to represent this memory is via automata. We
present a method to learn an automaton representation of a strategy using a
modification of the L*-algorithm. Compared to the tabular representation of a
strategy, the resulting automaton is dramatically smaller and thus also more
explainable. Moreover, in the learning process, our heuristics may even improve
the strategy&apos;s performance. In contrast to approaches that synthesize an
automaton directly from the POMDP thereby solving it, our approach is
incomparably more scalable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bork_A/0/1/0/all/0/1&quot;&gt;Alexander Bork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Debraj Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_K/0/1/0/all/0/1&quot;&gt;Kush Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1&quot;&gt;Jan Kretinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohr_S/0/1/0/all/0/1&quot;&gt;Stefanie Mohr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07991">
<title>Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes. (arXiv:2401.07991v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07991</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) could be deceived by generating
human-imperceptible perturbations of clean samples. Therefore, enhancing the
robustness of DNNs against adversarial attacks is a crucial task. In this
paper, we aim to train robust DNNs by limiting the set of outputs reachable via
a norm-bounded perturbation added to a clean sample. We refer to this set as
adversarial polytope, and each clean sample has a respective adversarial
polytope. Indeed, if the respective polytopes for all the samples are compact
such that they do not intersect the decision boundaries of the DNN, then the
DNN is robust against adversarial samples. Hence, the inner-working of our
algorithm is based on learning \textbf{c}onfined \textbf{a}dversarial
\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we
demonstrate the effectiveness of CAP over existing adversarial robustness
methods in improving the robustness of models against state-of-the-art attacks
including AutoAttack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidi_S/0/1/0/all/0/1&quot;&gt;Shayan Mohajer Hamidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1&quot;&gt;Linfeng Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08573">
<title>Benchmarking the Robustness of Image Watermarks. (arXiv:2401.08573v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08573</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the weaknesses of image watermarking techniques. We
present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel
benchmark for assessing watermark robustness, overcoming the limitations of
current evaluation methods.WAVES integrates detection and identification tasks,
and establishes a standardized evaluation protocol comprised of a diverse range
of stress tests. The attacks in WAVES range from traditional image distortions
to advanced and novel variations of diffusive, and adversarial attacks. Our
evaluation examines two pivotal dimensions: the degree of image quality
degradation and the efficacy of watermark detection after attacks. We develop a
series of Performance vs. Quality 2D plots, varying over several prominent
image similarity metrics, which are then aggregated in a heuristically novel
manner to paint an overall picture of watermark robustness and attack potency.
Our comprehensive evaluation reveals previously undetected vulnerabilities of
several modern watermarking algorithms. We envision WAVES as a toolkit for the
future development of robust watermarking systems. The project is available at
https://wavesbench.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mucong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbani_T/0/1/0/all/0/1&quot;&gt;Tahseen Rabbani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Aakriti Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Chenghao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abdirisak Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08738">
<title>Machine Learning-Based Analysis of Ebola Virus&apos; Impact on Gene Expression in Nonhuman Primates. (arXiv:2401.08738v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08738</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces the Supervised Magnitude-Altitude Scoring (SMAS)
methodology, a machine learning-based approach, for analyzing gene expression
data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV).
We utilize a comprehensive dataset of NanoString gene expression profiles from
Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen
interaction analysis. SMAS effectively combines gene selection based on
statistical significance and expression changes, employing linear classifiers
such as logistic regression to accurately differentiate between RT-qPCR
positive and negative NHP samples. A key finding of our research is the
identification of IFI6 and IFI27 as critical biomarkers, demonstrating
exceptional predictive performance with 100% accuracy and Area Under the Curve
(AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6
and IFI27, genes, including MX1, OAS1, and ISG15, were significantly
upregulated, highlighting their essential roles in the immune response to EBOV.
Our results underscore the efficacy of the SMAS method in revealing complex
genetic interactions and response mechanisms during EBOV infection. This
research provides valuable insights into EBOV pathogenesis and aids in
developing more precise diagnostic tools and therapeutic strategies to address
EBOV infection in particular and viral infection in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rezapour_M/0/1/0/all/0/1&quot;&gt;Mostafa Rezapour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Niazi_M/0/1/0/all/0/1&quot;&gt;Muhammad Khalid Khan Niazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Narayanan_A/0/1/0/all/0/1&quot;&gt;Aarthi Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gurcan_M/0/1/0/all/0/1&quot;&gt;Metin Nafi Gurcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08865">
<title>The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08865</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates discrepancies in how neural networks learn from
different imaging domains, which are commonly overlooked when adopting computer
vision techniques from the domain of natural images to other specialized
domains such as medical images. Recent works have found that the generalization
error of a trained network typically increases with the intrinsic dimension
($d_{data}$) of its training set. Yet, the steepness of this relationship
varies significantly between medical (radiological) and natural imaging
domains, with no existing theoretical explanation. We address this gap in
knowledge by establishing and empirically validating a generalization scaling
law with respect to $d_{data}$, and propose that the substantial scaling
discrepancy between the two considered domains may be at least partially
attributed to the higher intrinsic &quot;label sharpness&quot; ($K_F$) of medical imaging
datasets, a metric which we propose. Next, we demonstrate an additional benefit
of measuring the label sharpness of a training set: it is negatively correlated
with the trained model&apos;s adversarial robustness, which notably leads to models
for medical images having a substantially higher vulnerability to adversarial
attack. Finally, we extend our $d_{data}$ formalism to the related metric of
learned representation intrinsic dimension ($d_{repr}$), derive a
generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$
serves as an upper bound for $d_{repr}$. Our theoretical results are supported
by thorough experiments with six models and eleven natural and medical imaging
datasets over a range of training set sizes. Our findings offer insights into
the influence of intrinsic dataset properties on generalization, representation
learning, and robustness in deep neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1&quot;&gt;Nicholas Konz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1&quot;&gt;Maciej A. Mazurowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08947">
<title>AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URL Detection. (arXiv:2401.08947v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08947</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating reliance on revolutionary online web services has introduced
heightened security risks, with persistent challenges posed by phishing despite
extensive security measures. Traditional phishing systems, reliant on machine
learning and manual features, struggle with evolving tactics. Recent advances
in deep learning offer promising avenues for tackling novel phishing challenges
and malicious URLs. This paper introduces a two-phase stack generalized model
named AntiPhishStack, designed to detect phishing sites. The model leverages
the learning of URLs and character-level TF-IDF features symmetrically,
enhancing its ability to combat emerging phishing threats. In Phase I, features
are trained on a base machine learning classifier, employing K-fold
cross-validation for robust mean prediction. Phase II employs a two-layered
stacked-based LSTM network with five adaptive optimizers for dynamic
compilation, ensuring premier prediction on these features. Additionally, the
symmetrical predictions from both phases are optimized and integrated to train
a meta-XGBoost classifier, contributing to a final robust prediction. The
significance of this work lies in advancing phishing detection with
AntiPhishStack, operating without prior phishing-specific feature knowledge.
Experimental validation on two benchmark datasets, comprising benign and
phishing or malicious URLs, demonstrates the model&apos;s exceptional performance,
achieving a notable 96.04% accuracy compared to existing studies. This research
adds value to the ongoing discourse on symmetry and asymmetry in information
security and provides a forward-thinking solution for enhancing network
security in the face of evolving cyber threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aslam_S/0/1/0/all/0/1&quot;&gt;Saba Aslam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aslam_H/0/1/0/all/0/1&quot;&gt;Hafsa Aslam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzoor_A/0/1/0/all/0/1&quot;&gt;Arslan Manzoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_C/0/1/0/all/0/1&quot;&gt;Chen Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1&quot;&gt;Abdur Rasool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09003">
<title>Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09003</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent progress in improving the mathematical reasoning ability of
large language models(LLMs), solving competition-level math problems without
the use of external tools remains challenging for open-source LLMs. In this
work, we introduce the MMIQC dataset, a mixture of processed web data and
synthetic question-response pairs, to equip base models with better
mathematical reasoning skills. In different model sizes, the models fine-tuned
on MMIQC consistently outperform their counterparts by a clear margin on MATH
test set. Notably, DeepSeek-67B-MMIQC achieves a 41.0% accuracy, 4.2% higher
than the previous open-source SOTA. Our experiments also show that a large part
of the improvement can be attributed to our novel augmentation method
IQC(Iterative Question Composing), where we iteratively ask an LLM to compose
new questions from the given seed problems and do rejection sampling from
another LLM. MMIQC has now been released on
https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoxiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Andrew Chi-Chih Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09031">
<title>Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09031</link>
<description rdf:parseType="Literal">&lt;p&gt;Data attribution methods trace model behavior back to its training dataset,
offering an effective approach to better understand &apos;&apos;black-box&apos;&apos; neural
networks. While prior research has established quantifiable links between model
output and training data in diverse settings, interpreting diffusion model
outputs in relation to training samples remains underexplored. In particular,
diffusion models operate over a sequence of timesteps instead of instantaneous
input-output relationships in previous contexts, posing a significant challenge
to extend existing frameworks to diffusion models directly. Notably, we present
Diffusion-TracIn that incorporates this temporal dynamics and observe that
samples&apos; loss gradient norms are highly dependent on timestep. This trend leads
to a prominent bias in influence estimation, and is particularly noticeable for
samples trained on large-norm-inducing timesteps, causing them to be generally
influential. To mitigate this effect, we introduce Diffusion-ReTrac as a
re-normalized adaptation that enables the retrieval of training samples more
targeted to the test sample of interest, facilitating a localized measurement
of influence and considerably more intuitive visualization. We demonstrate the
efficacy of our approach through various evaluation metrics and auxiliary
tasks, reducing the amount of generally influential samples to $\frac{1}{3}$ of
its original quantity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_A/0/1/0/all/0/1&quot;&gt;Andrew Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cho-Jui Hsieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09074">
<title>Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09074</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the extent to which Large Language Models (LLMs) can simulate
the execution of computer code and algorithms. We begin by looking at straight
line programs, and show that current LLMs demonstrate poor performance even
with such simple programs -- performance rapidly degrades with the length of
code. We then investigate the ability of LLMs to simulate programs that contain
critical paths and redundant instructions. We also go beyond straight line
program simulation with sorting algorithms and nested loops, and we show the
computational complexity of a routine directly affects the ability of an LLM to
simulate its execution. We observe that LLMs execute instructions sequentially
and with a low error margin only for short programs or standard procedures.
LLMs&apos; code simulation is in tension with their pattern recognition and
memorisation capabilities: on tasks where memorisation is detrimental, we
propose a novel prompting method to simulate code execution line by line.
Empirically, our new Chain of Simulation (CoSm) method improves on the standard
Chain of Thought prompting approach by avoiding the pitfalls of memorisation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1&quot;&gt;Emanuele La Malfa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1&quot;&gt;Christoph Weinhuber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_O/0/1/0/all/0/1&quot;&gt;Orazio Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fangru Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1&quot;&gt;Anthony Cohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1&quot;&gt;Nigel Shadbolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1&quot;&gt;Michael Wooldridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09333">
<title>Machines Do See Color: A Guideline to Classify Different Forms of Racist Discourse in Large Corpora. (arXiv:2401.09333v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09333</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods to identify and classify racist language in text rely on
small-n qualitative approaches or large-n approaches focusing exclusively on
overt forms of racist discourse. This article provides a step-by-step
generalizable guideline to identify and classify different forms of racist
discourse in large corpora. In our approach, we start by conceptualizing racism
and its different manifestations. We then contextualize these racist
manifestations to the time and place of interest, which allows researchers to
identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a
cross-lingual model for supervised text classification with a cutting-edge
contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our
pretrained model, outperform other state-of-the-art approaches in classifying
racism in large corpora. We illustrate our approach using a corpus of tweets
relating to the Ecuadorian ind\&apos;igena community between 2018 and 2021.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordillo_D/0/1/0/all/0/1&quot;&gt;Diana Davila Gordillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timoneda_J/0/1/0/all/0/1&quot;&gt;Joan Timoneda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_S/0/1/0/all/0/1&quot;&gt;Sebastian Vallejo Vera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09451">
<title>Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09451</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of deducing three-dimensional molecular configurations from their
two-dimensional graph representations holds paramount importance in the fields
of computational chemistry and pharmaceutical development. The rapid
advancement of machine learning, particularly within the domain of deep
generative networks, has revolutionized the precision of predictive modeling in
this context. Traditional approaches often adopt a two-step strategy: initially
estimating interatomic distances and subsequently refining the spatial
molecular structure by solving a distance geometry problem. However, this
sequential approach occasionally falls short in accurately capturing the
intricacies of local atomic arrangements, thereby compromising the fidelity of
the resulting structural models. Addressing these limitations, this research
introduces a cutting-edge generative framework named \method{}. This framework
is grounded in the principles of diffusion observed in classical
non-equilibrium thermodynamics. \method{} views atoms as discrete entities and
excels in guiding the reversal of diffusion, transforming a distribution of
stochastic noise back into coherent molecular structures through a process akin
to a Markov chain. This transformation commences with the initial
representation of a molecular graph in an abstract latent space, culminating in
the realization of three-dimensional structures via a sophisticated bilevel
optimization scheme meticulously tailored to meet the specific requirements of
the task. One of the formidable challenges in this modeling endeavor involves
preserving roto-translational invariance to ensure that the generated molecular
conformations adhere to the laws of physics. Extensive experimental evaluations
confirm the efficacy of the proposed \method{} in comparison to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bobin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruoxue Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09671">
<title>Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09671</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain translation (UDT) aims to find functions that convert
samples from one domain (e.g., sketches) to another domain (e.g., photos)
without changing the high-level semantic meaning (also referred to as
``content&apos;&apos;). The translation functions are often sought by probability
distribution matching of the transformed source domain and target domain.
CycleGAN stands as arguably the most representative approach among this line of
work. However, it was noticed in the literature that CycleGAN and variants
could fail to identify the desired translation functions and produce
content-misaligned translations. This limitation arises due to the presence of
multiple translation functions -- referred to as ``measure-preserving
automorphism&quot; (MPA) -- in the solution space of the learning criteria. Despite
awareness of such identifiability issues, solutions have remained elusive. This
study delves into the core identifiability inquiry and introduces an MPA
elimination theory. Our analysis shows that MPA is unlikely to exist, if
multiple pairs of diverse cross-domain conditional distributions are matched by
the learning function. Our theory leads to a UDT learner using distribution
matching over auxiliary variable-induced subsets of the domains -- other than
over the entire data domains as in the classical approaches. The proposed
framework is the first to rigorously establish translation identifiability
under reasonable UDT settings, to our best knowledge. Experiments corroborate
with our theoretical claims.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1&quot;&gt;Sagar Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09793">
<title>PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection. (arXiv:2401.09793v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09793</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection stands as a crucial aspect of time series analysis, aiming
to identify abnormal events in time series samples. The central challenge of
this task lies in effectively learning the representations of normal and
abnormal patterns in a label-lacking scenario. Previous research mostly relied
on reconstruction-based approaches, restricting the representational abilities
of the models. In addition, most of the current deep learning-based methods are
not lightweight enough, which prompts us to design a more efficient framework
for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale
patch-based MLP-Mixer architecture that leverages contrastive learning for
representational extraction and anomaly detection. Specifically, PatchAD is
composed of four distinct MLP Mixers, exclusively utilizing the MLP
architecture for high efficiency and lightweight architecture. Additionally, we
also innovatively crafted a dual project constraint module to mitigate
potential model degradation. Comprehensive experiments demonstrate that PatchAD
achieves state-of-the-art results across multiple real-world multivariate time
series datasets. Our code is publicly
available.\footnote{\url{https://github.com/EmorZz1G/PatchAD}}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhijie Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weizheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09953">
<title>Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09953</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have become the preferred tool to process graph
data, with their efficacy being boosted through graph data augmentation
techniques. Despite the evolution of augmentation methods, issues like graph
property distortions and restricted structural changes persist. This leads to
the question: Is it possible to develop more property-conserving and
structure-sensitive augmentation methods? Through a spectral lens, we
investigate the interplay between graph properties, their augmentation, and
their spectral behavior, and found that keeping the low-frequency eigenvalues
unchanged can preserve the critical properties at a large scale when generating
augmented graphs. These observations inform our introduction of the Dual-Prism
(DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly
retains essential graph properties while diversifying augmented graphs.
Extensive experiments validate the efficiency of our approach, providing a new
and promising direction for graph data augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yutong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Runpeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bresson_X/0/1/0/all/0/1&quot;&gt;Xavier Bresson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10107">
<title>Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10107</link>
<description rdf:parseType="Literal">&lt;p&gt;Study Objectives: Polysomnography (PSG) currently serves as the benchmark for
evaluating sleep disorders. Its discomfort, impracticality for home-use, and
introduction of bias in sleep quality assessment necessitate the exploration of
less invasive, cost-effective, and portable alternatives. One promising
contender is the in-ear-EEG sensor, which offers advantages in terms of
comfort, fixed electrode positions, resistance to electromagnetic interference,
and user-friendliness. This study aims to establish a methodology to assess the
similarity between the in-ear-EEG signal and standard PSG.
&lt;/p&gt;
&lt;p&gt;Methods: We assess the agreement between the PSG and in-ear-EEG derived
hypnograms. We extract features in the time- and frequency- domain from PSG and
in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers
and the in-ear-EEG-scorers were in agreement. We introduce a methodology to
quantify the similarity between PSG derivations and the single-channel
in-ear-EEG. The approach relies on a comparison of distributions of selected
features -- extracted for each sleep stage and subject on both PSG and the
in-ear-EEG signals -- via a Jensen-Shannon Divergence Feature-based Similarity
Index (JSD-FSI).
&lt;/p&gt;
&lt;p&gt;Results: We found a high intra-scorer variability, mainly due to the
uncertainty the scorers had in evaluating the in-ear-EEG signals. We show that
the similarity between PSG and in-ear-EEG signals is high (JSD-FSI: 0.61 +/-
0.06 in awake, 0.60 +/- 0.07 in NREM and 0.51 +/- 0.08 in REM), and in line
with the similarity values computed independently on standard
PSG-channel-combinations.
&lt;/p&gt;
&lt;p&gt;Conclusions: In-ear-EEG is a valuable solution for home-based sleep
monitoring, however further studies with a larger and more heterogeneous
dataset are needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Palo_G/0/1/0/all/0/1&quot;&gt;Gianpaolo Palo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fiorillo_L/0/1/0/all/0/1&quot;&gt;Luigi Fiorillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Monachino_G/0/1/0/all/0/1&quot;&gt;Giuliana Monachino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bechny_M/0/1/0/all/0/1&quot;&gt;Michal Bechny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Melnykowycz_M/0/1/0/all/0/1&quot;&gt;Mark Melnykowycz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tzovara_A/0/1/0/all/0/1&quot;&gt;Athina Tzovara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Agostini_V/0/1/0/all/0/1&quot;&gt;Valentina Agostini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Faraci_F/0/1/0/all/0/1&quot;&gt;Francesca Dalia Faraci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10155">
<title>A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10155</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time and accurate traffic flow prediction is the foundation for ensuring
the efficient operation of intelligent transportation systems.In existing
traffic flow prediction methods based on graph neural networks (GNNs),
pre-defined graphs were usually used to describe the spatial correlations of
different traffic nodes in urban road networks. However, the ability of
pre-defined graphs used to describe spatial correlation was limited by prior
knowledge and graph generation methods. Although time-varying graphs based on
data-driven learning can partially overcome the drawbacks of pre-defined
graphs, the learning ability of existing adaptive graphs was limited. For
example, time-varying graphs cannot adequately capture the inherent spatial
correlations in traffic flow data.In order to solve these problems, we have
proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Ben Ao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1&quot;&gt;Bao-Lin Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10189">
<title>Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10189</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huimin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10305">
<title>Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning Approach. (arXiv:2401.10305v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10305</link>
<description rdf:parseType="Literal">&lt;p&gt;This study provides evidence that personality can be reliably predicted from
activity data collected through mobile phone sensors. Employing a set of well
informed indicators calculable from accelerometer records and movement
patterns, we were able to predict users&apos; personality up to a 0.78 F1 score on a
two class problem. Given the fast growing number of data collected from mobile
phones, our novel personality indicators open the door to exciting avenues for
future research in social sciences. Our results reveal distinct behavioral
patterns that proved to be differentially predictive of big five personality
traits. They potentially enable cost effective, questionnaire free
investigation of personality related questions at an unprecedented scale. We
show how a combination of rich behavioral data obtained with smartphone sensing
and the use of machine learning techniques can help to advance personality
research and can inform both practitioners and researchers about the different
behavioral patterns of personality. These findings have practical implications
for organizations harnessing mobile sensor data for personality assessment,
guiding the refinement of more precise and efficient prediction models in the
future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sze_W/0/1/0/all/0/1&quot;&gt;Wun Yung Shaney Sze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Herrero_M/0/1/0/all/0/1&quot;&gt;Maryglen Pearl Herrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garriga_R/0/1/0/all/0/1&quot;&gt;Roger Garriga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10337">
<title>Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10337</link>
<description rdf:parseType="Literal">&lt;p&gt;Tactics, Techniques and Procedures (TTPs) represent sophisticated attack
patterns in the cybersecurity domain, described encyclopedically in textual
knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP
mapping, is an important and challenging task. Conventional learning approaches
often target the problem in the classical multi-class or multilabel
classification setting. This setting hinders the learning ability of the model
due to a large number of classes (i.e., TTPs), the inevitable skewness of the
label distribution and the complex hierarchical structure of the label space.
We formulate the problem in a different learning paradigm, where the assignment
of a text to a TTP label is decided by the direct semantic similarity between
the two, thus reducing the complexity of competing solely over the large
labeling space. To that end, we propose a neural matching architecture with an
effective sampling-based learn-to-compare mechanism, facilitating the learning
process of the matching model despite constrained resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srndic_N/0/1/0/all/0/1&quot;&gt;Nedim Srndic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neth_A/0/1/0/all/0/1&quot;&gt;Alexander Neth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10371">
<title>Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning. (arXiv:2401.10371v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10371</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning has raised significant interest with the adoption of laws
ensuring the ``right to be forgotten&apos;&apos;. Researchers have provided a
probabilistic notion of approximate unlearning under a similar definition of
Differential Privacy (DP), where privacy is defined as statistical
indistinguishability to retraining from scratch. We propose Langevin
unlearning, an unlearning framework based on noisy gradient descent with
privacy guarantees for approximate unlearning problems. Langevin unlearning
unifies the DP learning process and the privacy-certified unlearning process
with many algorithmic benefits. These include approximate certified unlearning
for non-convex problems, complexity saving compared to retraining, sequential
and batch unlearning for multiple unlearning requests. We verify the
practicality of Langevin unlearning by studying its privacy-utility-complexity
trade-off via experiments on benchmark datasets, and also demonstrate its
superiority against gradient-decent-plus-output-perturbation based approximate
unlearning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1&quot;&gt;Eli Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10393">
<title>Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10393</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks often suffer from catastrophic interference (CI): performance
on previously learned tasks drops off significantly when learning a new task.
This contrasts strongly with humans, who can sequentially learn new tasks
without appreciably forgetting previous tasks. Prior work has explored various
techniques for mitigating CI such as regularization, rehearsal, generative
replay, and distillation methods. The current work takes a different approach,
one guided by cognitive science research showing that in naturalistic
environments, the probability of encountering a task decreases as a power-law
of the time since it was last performed. We argue that a realistic evaluation
of techniques for the mitigation of CI should be performed in simulated
naturalistic learning environments. Thus, we evaluate the extent of mitigation
of CI when training simple rehearsal-based methods in power-law environments
similar to the ones humans face. Our work explores this novel rehearsal-based
approach for a domain-incremental task: learning permutations in the MNIST
task. We compare our rehearsal environment with other baselines to show its
efficacy in promoting continual learning. Additionally, we investigate whether
this environment shows forward facilitation, i.e., faster learning of later
tasks. Next, we explore the robustness of our learning environment to the
number of tasks, model size, and amount of data rehearsed after each task.
Notably, our results show that the performance is comparable or superior to
that of models trained using popular regularization methods and also to
rehearsals in non-power-law environments. The benefits of this training
paradigm include simplicity and the lack of a need for extra neural circuitry.
In addition, because our method is orthogonal to other methods, future research
can combine training in power-law environments with other continual learning
mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1&quot;&gt;Atith Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marupudi_V/0/1/0/all/0/1&quot;&gt;Vijay Marupudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10451">
<title>Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10451</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving large-scale capacity expansion problems (CEPs) is central to
cost-effective decarbonization of regional-scale energy systems. To ensure the
intended outcomes of CEPs, modeling uncertainty due to weather-dependent
variable renewable energy (VRE) supply and energy demand becomes crucially
important. However, the resulting stochastic optimization models are often less
computationally tractable than their deterministic counterparts. Here, we
propose a learning-assisted approximate solution method to tractably solve
two-stage stochastic CEPs. Our method identifies low-cost planning decisions by
constructing and solving a sequence of tractable temporally aggregated
surrogate problems. We adopt a Bayesian optimization approach to searching the
space of time series aggregation hyperparameters and compute approximate
solutions that minimize costs on a validation set of supply-demand projections.
Importantly, we evaluate solved planning outcomes on a held-out set of test
projections. We apply our approach to generation and transmission expansion
planning for a joint power-gas system spanning New England. We show that our
approach yields an estimated cost savings of up to 3.8% in comparison to
benchmark time series aggregation approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brenner_A/0/1/0/all/0/1&quot;&gt;Aron Brenner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khorramfar_R/0/1/0/all/0/1&quot;&gt;Rahman Khorramfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mallapragada_D/0/1/0/all/0/1&quot;&gt;Dharik Mallapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Amin_S/0/1/0/all/0/1&quot;&gt;Saurabh Amin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10765">
<title>Starlit: Privacy-Preserving Federated Learning to Enhance Financial Fraud Detection. (arXiv:2401.10765v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10765</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a data-minimization approach enabling
collaborative model training across diverse clients with local data, avoiding
direct data exchange. However, state-of-the-art FL solutions to identify
fraudulent financial transactions exhibit a subset of the following
limitations. They (1) lack a formal security definition and proof, (2) assume
prior freezing of suspicious customers&apos; accounts by financial institutions
(limiting the solutions&apos; adoption), (3) scale poorly, involving either $O(n^2)$
computationally expensive modular exponentiation (where $n$ is the total number
of financial institutions) or highly inefficient fully homomorphic encryption,
(4) assume the parties have already completed the identity alignment phase,
hence excluding it from the implementation, performance evaluation, and
security analysis, and (5) struggle to resist clients&apos; dropouts. This work
introduces Starlit, a novel scalable privacy-preserving FL mechanism that
overcomes these limitations. It has various applications, such as enhancing
financial fraud detection, mitigating terrorism, and enhancing digital health.
We implemented Starlit and conducted a thorough performance analysis using
synthetic data from a key player in global financial transactions. The
evaluation indicates Starlit&apos;s scalability, efficiency, and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abadi_A/0/1/0/all/0/1&quot;&gt;Aydin Abadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doyle_B/0/1/0/all/0/1&quot;&gt;Bradley Doyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gini_F/0/1/0/all/0/1&quot;&gt;Francesco Gini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guinamard_K/0/1/0/all/0/1&quot;&gt;Kieron Guinamard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murakonda_S/0/1/0/all/0/1&quot;&gt;Sasi Kumar Murakonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liddell_J/0/1/0/all/0/1&quot;&gt;Jack Liddell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mellor_P/0/1/0/all/0/1&quot;&gt;Paul Mellor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murdoch_S/0/1/0/all/0/1&quot;&gt;Steven J. Murdoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseri_M/0/1/0/all/0/1&quot;&gt;Mohammad Naseri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Page_H/0/1/0/all/0/1&quot;&gt;Hector Page&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorakopoulos_G/0/1/0/all/0/1&quot;&gt;George Theodorakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_S/0/1/0/all/0/1&quot;&gt;Suzanne Weller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02015">
<title>Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.02015</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Ling Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shenda Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhilin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zheming Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06344">
<title>Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.06344</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting crowded intents and trajectories is crucial in varouls real-world
applications, including service robots and autonomous vehicles. Understanding
environmental dynamics is challenging, not only due to the complexities of
modeling pair-wise spatial and temporal interactions but also the diverse
influence of group-wise interactions. To decode the comprehensive pair-wise and
group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a
Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory
prediction. In Hyper-STTN, crowded group-wise correlations are constructed
using a set of multi-scale hypergraphs with varying group sizes, captured
through random-walk robability-based hypergraph spectral convolution.
Additionally, a spatial-temporal transformer is adapted to capture pedestrians&apos;
pair-wise latent interactions in spatial-temporal dimensions. These
heterogeneous group-wise and pair-wise are then fused and aligned though a
multimodal transformer network. Hyper-STTN outperformes other state-of-the-art
baselines and ablation models on 5 real-world pedestrian motion datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weizheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Le Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Baijian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guohua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1&quot;&gt;Byung-Cheol Min&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10586">
<title>PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.10586</link>
<description rdf:parseType="Literal">&lt;p&gt;Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model&apos;s architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Ping Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingchuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingfu Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>