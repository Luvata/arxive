<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.08717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.07333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.09035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10541" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.12168">
<title>RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets. (arXiv:2310.12168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12168</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the field of machine learning has undergone a transition from
model-centric to data-centric. The advancements in diverse learning tasks have
been propelled by the accumulation of more extensive datasets, subsequently
facilitating the training of larger models on these datasets. However, these
datasets remain relatively under-explored. To this end, we introduce a
pioneering approach known as RK-core, to empower gaining a deeper understanding
of the intricate hierarchical structure within datasets. Across several
benchmark datasets, we find that samples with low coreness values appear less
representative of their respective categories, and conversely, those with high
coreness values exhibit greater representativeness. Correspondingly, samples
with high coreness values make a more substantial contribution to the
performance in comparison to those with low coreness values. Building upon
this, we further employ RK-core to analyze the hierarchical structure of
samples with different coreset selection methods. Remarkably, we find that a
high-quality coreset should exhibit hierarchical diversity instead of solely
opting for representative samples. The code is available at
https://github.com/yaolu-zjut/Kcore.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yutian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiaqi Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zuohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1&quot;&gt;Qi Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12189">
<title>Mesh Represented Recycle Learning for 3D Hand Pose and Mesh Estimation. (arXiv:2310.12189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12189</link>
<description rdf:parseType="Literal">&lt;p&gt;In general, hand pose estimation aims to improve the robustness of model
performance in the real-world scenes. However, it is difficult to enhance the
robustness since existing datasets are obtained in restricted environments to
annotate 3D information. Although neural networks quantitatively achieve a high
estimation accuracy, unsatisfied results can be observed in visual quality.
This discrepancy between quantitative results and their visual qualities
remains an open issue in the hand pose representation. To this end, we propose
a mesh represented recycle learning strategy for 3D hand pose and mesh
estimation which reinforces synthesized hand mesh representation in a training
phase. To be specific, a hand pose and mesh estimation model first predicts
parametric 3D hand annotations (i.e., 3D keypoint positions and vertices for
hand mesh) with real-world hand images in the training phase. Second, synthetic
hand images are generated with self-estimated hand mesh representations. After
that, the synthetic hand images are fed into the same model again. Thus, the
proposed learning strategy simultaneously improves quantitative results and
visual qualities by reinforcing synthetic mesh representation. To encourage
consistency between original model output and its recycled one, we propose
self-correlation loss which maximizes the accuracy and reliability of our
learning strategy. Consequently, the model effectively conducts self-refinement
on hand pose estimation by learning mesh representation from its own output. To
demonstrate the effectiveness of our learning strategy, we provide extensive
experiments on FreiHAND dataset. Notably, our learning strategy improves the
performance on hand pose and mesh estimation without any extra computational
burden during the inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bosang Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jonghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyotae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lanying Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jeongwon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1&quot;&gt;Dowoo Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jungpyo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1&quot;&gt;Wonhyeok Im&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;KyungMin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12190">
<title>DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors. (arXiv:2310.12190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12190</link>
<description rdf:parseType="Literal">&lt;p&gt;Enhancing a still image with motion offers more engaged visual experience.
Traditional image animation techniques mainly focus on animating natural scenes
with random dynamics, such as clouds and fluid, and thus limits their
applicability to generic visual contents. To overcome this limitation, we
explore the synthesis of dynamic content for open-domain images, converting
them into animated videos. The key idea is to utilize the motion prior of
text-to-video diffusion models by incorporating the image into the generative
process as guidance. Given an image, we first project it into a text-aligned
rich image embedding space using a learnable image encoding network, which
facilitates the video model to digest the image content compatibly. However,
some visual details still struggle to be preserved in the resulting videos. To
supplement more precise image information, we further feed the full image to
the diffusion model by concatenating it with the initial noises. Experimental
results reveal that our proposed method produces visually convincing animated
videos, exhibiting both natural motions and high fidelity to the input image.
Comparative evaluation demonstrates the notable superiority of our approach
over existing competitors. The source code will be released upon publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1&quot;&gt;Tien-Tsin Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12243">
<title>REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes. (arXiv:2310.12243v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning models, such as those used in an autonomous vehicle are
vulnerable to adversarial attacks where an attacker could place an adversarial
object in the environment, leading to mis-classification. Generating these
adversarial objects in the digital space has been extensively studied, however
successfully transferring these attacks from the digital realm to the physical
realm has proven challenging when controlling for real-world environmental
factors. In response to these limitations, we introduce REVAMP, an easy-to-use
Python library that is the first-of-its-kind tool for creating attack scenarios
with arbitrary objects and simulating realistic environmental factors,
lighting, reflection, and refraction. REVAMP enables researchers and
practitioners to swiftly explore various scenarios within the digital realm by
offering a wide range of configurable options for designing experiments and
using differentiable rendering to reproduce physically plausible adversarial
objects. We will demonstrate and invite the audience to try REVAMP to produce
an adversarial texture on a chosen object while having control over various
scene parameters. The audience will choose a scene, an object to attack, the
desired attack class, and the number of camera positions to use. Then, in real
time, we show how this altered texture causes the chosen object to be
mis-classified, showcasing the potential of REVAMP in real-world scenarios.
REVAMP is open-source and available at https://github.com/poloclub/revamp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hull_M/0/1/0/all/0/1&quot;&gt;Matthew Hull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12262">
<title>Improving SCGAN&apos;s Similarity Constraint and Learning a Better Disentangled Representation. (arXiv:2310.12262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12262</link>
<description rdf:parseType="Literal">&lt;p&gt;SCGAN adds a similarity constraint between generated images and conditions as
a regularization term on generative adversarial networks. Similarity constraint
works as a tutor to instruct the generator network to comprehend the difference
of representations based on conditions. We understand how SCGAN works on a
deeper level. This understanding makes us realize that the similarity
constraint functions like the contrastive loss function. We believe that a
model with high understanding and intelligence measures the similarity between
images based on their structure and high level features, just like humans do.
Two major changes we applied to SCGAN in order to make a modified model are
using SSIM to measure similarity between images and applying contrastive loss
principles to the similarity constraint. The modified model performs better
using FID and FactorVAE metrics. The modified model also has better
generalisability compared to other models. Keywords Generative Adversarial
Nets, Unsupervised Learning, Disentangled Representation Learning, Contrastive
Disentanglement, SSIM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdanpanah_I/0/1/0/all/0/1&quot;&gt;Iman Yazdanpanah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12274">
<title>An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12274</link>
<description rdf:parseType="Literal">&lt;p&gt;Textural Inversion, a prompt learning method, learns a singular embedding for
a new &quot;word&quot; to represent image style and appearance, allowing it to be
integrated into natural language sentences to generate novel synthesised
images. However, identifying and integrating multiple object-level concepts
within one scene poses significant challenges even when embeddings for
individual concepts are attainable. This is further confirmed by our empirical
tests. To address this challenge, we introduce a framework for Multi-Concept
Prompt Learning (MCPL), where multiple new &quot;words&quot; are simultaneously learned
from a single sentence-image pair. To enhance the accuracy of word-concept
correlation, we propose three regularisation techniques: Attention Masking
(AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss
(PromptCL) to separate the embeddings of different concepts; and Bind adjective
(Bind adj.) to associate new &quot;words&quot; with known words. We evaluate via image
generation, editing, and attention visualisation with diverse images. Extensive
quantitative comparisons demonstrate that our method can learn more
semantically disentangled concepts with enhanced word-concept correlation.
Additionally, we introduce a novel dataset and evaluation protocol tailored for
this new task of learning object-level concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saseendran_A/0/1/0/all/0/1&quot;&gt;Amrutha Saseendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diethe_T/0/1/0/all/0/1&quot;&gt;Tom Diethe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teare_P/0/1/0/all/0/1&quot;&gt;Philip Teare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12296">
<title>Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability. (arXiv:2310.12296v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12296</link>
<description rdf:parseType="Literal">&lt;p&gt;Video segmentation encompasses a wide range of categories of problem
formulation, e.g., object, scene, actor-action and multimodal video
segmentation, for delineating task-specific scene components with pixel-level
masks. Recently, approaches in this research area shifted from concentrating on
ConvNet-based to transformer-based models. In addition, various
interpretability approaches have appeared for transformer models and video
temporal dynamics, motivated by the growing interest in basic scientific
understanding, model diagnostics and societal implications of real-world
deployment. Previous surveys mainly focused on ConvNet models on a subset of
video segmentation tasks or transformers for classification tasks. Moreover,
component-wise discussion of transformer-based video segmentation models has
not yet received due focus. In addition, previous reviews of interpretability
methods focused on transformers for classification, while analysis of video
temporal dynamics modelling capabilities of video models received less
attention. In this survey, we address the above with a thorough discussion of
various categories of video segmentation, a component-wise discussion of the
state-of-the-art transformer-based models, and a review of related
interpretability methods. We first present an introduction to the different
video segmentation task categories, their objectives, specific challenges and
benchmark datasets. Next, we provide a component-wise review of recent
transformer-based models and document the state of the art on different video
segmentation tasks. Subsequently, we discuss post-hoc and ante-hoc
interpretability methods for transformer models and interpretability methods
for understanding the role of the temporal dimension in video models. Finally,
we conclude our discussion with future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_R/0/1/0/all/0/1&quot;&gt;Rezaul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1&quot;&gt;Richard P. Wildes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12334">
<title>Improving Representation Learning for Histopathologic Images with Cluster Constraints. (arXiv:2310.12334v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12334</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in whole-slide image (WSI) scanners and computational
capabilities have significantly propelled the application of artificial
intelligence in histopathology slide analysis. While these strides are
promising, current supervised learning approaches for WSI analysis come with
the challenge of exhaustively labeling high-resolution slides - a process that
is both labor-intensive and time-consuming. In contrast, self-supervised
learning (SSL) pretraining strategies are emerging as a viable alternative,
given that they don&apos;t rely on explicit data annotations. These SSL strategies
are quickly bridging the performance disparity with their supervised
counterparts. In this context, we introduce an SSL framework. This framework
aims for transferable representation learning and semantically meaningful
clustering by synergizing invariance loss and clustering loss in WSI analysis.
Notably, our approach outperforms common SSL methods in downstream
classification and clustering tasks, as evidenced by tests on the Camelyon16
and a pancreatic cancer dataset. The code and additional details are accessible
at: https://github.com/wwyi1828/CluSiam.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weiyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chongyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DiPalma_J/0/1/0/all/0/1&quot;&gt;Joseph DiPalma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1&quot;&gt;Soroush Vosoughi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1&quot;&gt;Saeed Hassanpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12344">
<title>LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following. (arXiv:2310.12344v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12344</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end Transformers have demonstrated an impressive success rate for
Embodied Instruction Following when the environment has been seen in training.
However, they tend to struggle when deployed in an unseen environment. This
lack of generalizability is due to the agent&apos;s insensitivity to subtle changes
in natural language instructions. To mitigate this issue, we propose explicitly
aligning the agent&apos;s hidden states with the instructions via contrastive
learning. Nevertheless, the semantic gap between high-level language
instructions and the agent&apos;s low-level action space remains an obstacle.
Therefore, we further introduce a novel concept of meta-actions to bridge the
gap. Meta-actions are ubiquitous action patterns that can be parsed from the
original action sequence. These patterns represent higher-level semantics that
are intuitively aligned closer to the instructions. When meta-actions are
applied as additional training signals, the agent generalizes better to unseen
environments. Compared to a strong multi-modal Transformer baseline, we achieve
a significant 4.5% absolute gain in success rate in unseen environments of
ALFRED Embodied Instruction Following. Additional analysis shows that the
contrastive objective and meta-actions are complementary in achieving the best
results, and the resulting agent better aligns its states with corresponding
instructions, making it more suitable for real-world embodied agents. The code
is available at: https://github.com/joeyy5588/LACMA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Fu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12345">
<title>ClusT3: Information Invariant Test-Time Training. (arXiv:2310.12345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12345</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning models have shown remarkable performance in a broad range of
vision tasks. However, they are often vulnerable against domain shifts at
test-time. Test-time training (TTT) methods have been developed in an attempt
to mitigate these vulnerabilities, where a secondary task is solved at training
time simultaneously with the main task, to be later used as an self-supervised
proxy task at test-time. In this work, we propose a novel unsupervised TTT
technique based on the maximization of Mutual Information between multi-scale
feature maps and a discrete latent representation, which can be integrated to
the standard training as an auxiliary clustering task. Experimental results
demonstrate competitive classification performance on different popular
test-time adaptation benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakim_G/0/1/0/all/0/1&quot;&gt;Gustavo A. Vargas Hakim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osowiechi_D/0/1/0/all/0/1&quot;&gt;David Osowiechi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noori_M/0/1/0/all/0/1&quot;&gt;Mehrdad Noori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheraghalikhani_M/0/1/0/all/0/1&quot;&gt;Milad Cheraghalikhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1&quot;&gt;Christian Desrosiers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12393">
<title>Deep Learning Techniques for Video Instance Segmentation: A Survey. (arXiv:2310.12393v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12393</link>
<description rdf:parseType="Literal">&lt;p&gt;Video instance segmentation, also known as multi-object tracking and
segmentation, is an emerging computer vision research area introduced in 2019,
aiming at detecting, segmenting, and tracking instances in videos
simultaneously. By tackling the video instance segmentation tasks through
effective analysis and utilization of visual information in videos, a range of
computer vision-enabled applications (e.g., human action recognition, medical
image processing, autonomous vehicle navigation, surveillance, etc) can be
implemented. As deep-learning techniques take a dominant role in various
computer vision areas, a plethora of deep-learning-based video instance
segmentation schemes have been proposed. This survey offers a multifaceted view
of deep-learning schemes for video instance segmentation, covering various
architectural paradigms, along with comparisons of functional performance,
model complexity, and computational overheads. In addition to the common
architectural designs, auxiliary techniques for improving the performance of
deep-learning models for video instance segmentation are compiled and
discussed. Finally, we discuss a range of major challenges and directions for
further investigations to help advance this promising research field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenhao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chang-Tsun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yongjian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1&quot;&gt;Chee Peng Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creighton_D/0/1/0/all/0/1&quot;&gt;Douglas Creighton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12405">
<title>LoMAE: Low-level Vision Masked Autoencoders for Low-dose CT Denoising. (arXiv:2310.12405v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12405</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-dose computed tomography (LDCT) offers reduced X-ray radiation exposure
but at the cost of compromised image quality, characterized by increased noise
and artifacts. Recently, transformer models emerged as a promising avenue to
enhance LDCT image quality. However, the success of such models relies on a
large amount of paired noisy and clean images, which are often scarce in
clinical settings. In the fields of computer vision and natural language
processing, masked autoencoders (MAE) have been recognized as an effective
label-free self-pretraining method for transformers, due to their exceptional
feature representation ability. However, the original pretraining and
fine-tuning design fails to work in low-level vision tasks like denoising. In
response to this challenge, we redesign the classical encoder-decoder learning
model and facilitate a simple yet effective low-level vision MAE, referred to
as LoMAE, tailored to address the LDCT denoising problem. Moreover, we
introduce an MAE-GradCAM method to shed light on the latent learning mechanisms
of the MAE/LoMAE. Additionally, we explore the LoMAE&apos;s robustness and
generability across a variety of noise levels. Experiments results show that
the proposed LoMAE can enhance the transformer&apos;s denoising performance and
greatly relieve the dependence on the ground truth clean data. It also
demonstrates remarkable robustness and generalizability over a spectrum of
noise levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dayang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongshun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shuo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Li Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morovati_B/0/1/0/all/0/1&quot;&gt;Bahareh Morovati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hengyong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12430">
<title>DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond. (arXiv:2310.12430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12430</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we introduce DocXChain, a powerful open-source toolchain for
document parsing, which is designed and developed to automatically convert the
rich information embodied in unstructured documents, such as text, tables and
charts, into structured representations that are readable and manipulable by
machines. Specifically, basic capabilities, including text detection, text
recognition, table structure recognition and layout analysis, are provided.
Upon these basic capabilities, we also build a set of fully functional
pipelines for document parsing, i.e., general text reading, table parsing, and
document structurization, to drive various applications related to documents in
real-world scenarios. Moreover, DocXChain is concise, modularized and flexible,
such that it can be readily integrated with existing tools, libraries or models
(such as LangChain and ChatGPT), to construct more powerful systems that can
accomplish more complicated and challenging tasks. The code of DocXChain is
publicly available
at:~\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Applications/DocXChain}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Cong Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12431">
<title>Segment Anything Meets Universal Adversarial Perturbation. (arXiv:2310.12431v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12431</link>
<description rdf:parseType="Literal">&lt;p&gt;As Segment Anything Model (SAM) becomes a popular foundation model in
computer vision, its adversarial robustness has become a concern that cannot be
ignored. This works investigates whether it is possible to attack SAM with
image-agnostic Universal Adversarial Perturbation (UAP). In other words, we
seek a single perturbation that can fool the SAM to predict invalid masks for
most (if not all) images. We demonstrate convetional image-centric attack
framework is effective for image-independent attacks but fails for universal
adversarial attack. To this end, we propose a novel perturbation-centric
framework that results in a UAP generation method based on self-supervised
contrastive learning (CL), where the UAP is set to the anchor sample and the
positive sample is augmented from the UAP. The representations of negative
samples are obtained from the image encoder in advance and saved in a memory
bank. The effectiveness of our proposed CL-based UAP generation method is
validated by both quantitative and qualitative results. On top of the ablation
study to understand various components in our proposed method, we shed light on
the roles of positive and negative samples in making the generated UAP
effective for attacking SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongshen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12452">
<title>Not Just Learning from Others but Relying on Yourself: A New Perspective on Few-Shot Segmentation in Remote Sensing. (arXiv:2310.12452v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12452</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot segmentation (FSS) is proposed to segment unknown class targets with
just a few annotated samples. Most current FSS methods follow the paradigm of
mining the semantics from the support images to guide the query image
segmentation. However, such a pattern of `learning from others&apos; struggles to
handle the extreme intra-class variation, preventing FSS from being directly
generalized to remote sensing scenes. To bridge the gap of intra-class
variance, we develop a Dual-Mining network named DMNet for cross-image mining
and self-mining, meaning that it no longer focuses solely on support images but
pays more attention to the query image itself. Specifically, we propose a
Class-public Region Mining (CPRM) module to effectively suppress irrelevant
feature pollution by capturing the common semantics between the support-query
image pair. The Class-specific Region Mining (CSRM) module is then proposed to
continuously mine the class-specific semantics of the query image itself in a
`filtering&apos; and `purifying&apos; manner. In addition, to prevent the co-existence of
multiple classes in remote sensing scenes from exacerbating the collapse of FSS
generalization, we also propose a new Known-class Meta Suppressor (KMS) module
to suppress the activation of known-class objects in the sample. Extensive
experiments on the iSAID and LoveDA remote sensing datasets have demonstrated
that our method sets the state-of-the-art with a minimum number of model
parameters. Significantly, our model with the backbone of Resnet-50 achieves
the mIoU of 49.58% and 51.34% on iSAID under 1-shot and 5-shot settings,
outperforming the state-of-the-art method by 1.8% and 1.12%, respectively. The
code is publicly available at https://github.com/HanboBizl/DMNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_H/0/1/0/all/0/1&quot;&gt;Hanbo Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yingchao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_W/0/1/0/all/0/1&quot;&gt;Wenhui Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xian Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12464">
<title>Lidar Panoptic Segmentation and Tracking without Bells and Whistles. (arXiv:2310.12464v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12464</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art lidar panoptic segmentation (LPS) methods follow bottom-up
segmentation-centric fashion wherein they build upon semantic segmentation
networks by utilizing clustering to obtain object instances. In this paper, we
re-think this approach and propose a surprisingly simple yet effective
detection-centric network for both LPS and tracking. Our network is modular by
design and optimized for all aspects of both the panoptic segmentation and
tracking task. One of the core components of our network is the object instance
detection branch, which we train using point-level (modal) annotations, as
available in segmentation-centric datasets. In the absence of amodal (cuboid)
annotations, we regress modal centroids and object extent using
trajectory-level supervision that provides information about object size, which
cannot be inferred from single scans due to occlusions and the sparse nature of
the lidar data. We obtain fine-grained instance segments by learning to
associate lidar points with detected centroids. We evaluate our method on
several 3D/4D LPS benchmarks and observe that our model establishes a new
state-of-the-art among open-sourced models, outperforming recent query-based
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwalla_A/0/1/0/all/0/1&quot;&gt;Abhinav Agarwalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuhua Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziglar_J/0/1/0/all/0/1&quot;&gt;Jason Ziglar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferroni_F/0/1/0/all/0/1&quot;&gt;Francesco Ferroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1&quot;&gt;Laura Leal-Taix&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1&quot;&gt;James Hays&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1&quot;&gt;Aljo&amp;#x161;a O&amp;#x161;ep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12465">
<title>WeedCLR: Weed Contrastive Learning through Visual Representations with Class-Optimized Loss in Long-Tailed Datasets. (arXiv:2310.12465v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12465</link>
<description rdf:parseType="Literal">&lt;p&gt;Image classification is a crucial task in modern weed management and crop
intervention technologies. However, the limited size, diversity, and balance of
existing weed datasets hinder the development of deep learning models for
generalizable weed identification. In addition, the expensive labelling
requirements of mainstream fully-supervised weed classifiers make them cost-
and time-prohibitive to deploy widely, for new weed species, and in
site-specific weed management. This paper proposes a novel method for Weed
Contrastive Learning through visual Representations (WeedCLR), that uses
class-optimized loss with Von Neumann Entropy of deep representation for weed
classification in long-tailed datasets. WeedCLR leverages self-supervised
learning to learn rich and robust visual features without any labels and
applies a class-optimized loss function to address the class imbalance problem
in long-tailed datasets. WeedCLR is evaluated on two public weed datasets:
CottonWeedID15, containing 15 weed species, and DeepWeeds, containing 8 weed
species. WeedCLR achieves an average accuracy improvement of 4.3\% on
CottonWeedID15 and 5.6\% on DeepWeeds over previous methods. It also
demonstrates better generalization ability and robustness to different
environmental conditions than existing methods without the need for expensive
and time-consuming human annotations. These significant improvements make
WeedCLR an effective tool for weed classification in long-tailed datasets and
allows for more rapid and widespread deployment of site-specific weed
management and crop intervention technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1&quot;&gt;Alzayat Saleh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olsen_A/0/1/0/all/0/1&quot;&gt;Alex Olsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wood_J/0/1/0/all/0/1&quot;&gt;Jake Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Philippa_B/0/1/0/all/0/1&quot;&gt;Bronson Philippa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1&quot;&gt;Mostafa Rahimi Azghadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12470">
<title>RecolorCloud: A Point Cloud Tool for Recoloring, Segmentation, and Conversion. (arXiv:2310.12470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12470</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are a 3D space representation of an environment that was
recorded with a high precision laser scanner. These scanners can suffer from
environmental interference such as surface shading, texturing, and reflections.
Because of this, point clouds may be contaminated with fake or incorrect
colors. Current open source or proprietary tools offer limited or no access to
correcting these visual errors automatically.
&lt;/p&gt;
&lt;p&gt;RecolorCloud is a tool developed to resolve these color conflicts by
utilizing automated color recoloring. We offer the ability to deleting or
recoloring outlier points automatically with users only needing to specify
bounding box regions to effect colors. Results show a vast improvement of the
photo-realistic quality of large point clouds. Additionally, users can quickly
recolor a point cloud with set semantic segmentation colors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_E/0/1/0/all/0/1&quot;&gt;Esteban Segarra Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMahan_R/0/1/0/all/0/1&quot;&gt;Ryan P. McMahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12474">
<title>Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping. (arXiv:2310.12474v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12474</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution 3D object generation remains a challenging task primarily due
to the limited availability of comprehensive annotated training data. Recent
advancements have aimed to overcome this constraint by harnessing image
generative models, pretrained on extensive curated web datasets, using
knowledge transfer techniques like Score Distillation Sampling (SDS).
Efficiently addressing the requirements of high-resolution rendering often
necessitates the adoption of latent representation-based models, such as the
Latent Diffusion Model (LDM). In this framework, a significant challenge
arises: To compute gradients for individual image pixels, it is necessary to
backpropagate gradients from the designated latent space through the frozen
components of the image model, such as the VAE encoder used within LDM.
However, this gradient propagation pathway has never been optimized, remaining
uncontrolled during training. We find that the unregulated gradients adversely
affect the 3D model&apos;s capacity in acquiring texture-related information from
the image generative model, leading to poor quality appearance synthesis. To
address this overarching challenge, we propose an innovative operation termed
Pixel-wise Gradient Clipping (PGC) designed for seamless integration into
existing 3D generative models, thereby enhancing their synthesis quality.
Specifically, we control the magnitude of stochastic gradients by clipping the
pixel-wise gradients efficiently, while preserving crucial texture-related
gradient directions. Despite this simplicity and minimal extra cost, extensive
experiments demonstrate the efficacy of our PGC in enhancing the performance of
existing 3D generative models for high-resolution object rendering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiachen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12509">
<title>Machine Learning for Leaf Disease Classification: Data, Techniques and Applications. (arXiv:2310.12509v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12509</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing demand for sustainable development brings a series of information
technologies to help agriculture production. Especially, the emergence of
machine learning applications, a branch of artificial intelligence, has shown
multiple breakthroughs which can enhance and revolutionize plant pathology
approaches. In recent years, machine learning has been adopted for leaf disease
classification in both academic research and industrial applications.
Therefore, it is enormously beneficial for researchers, engineers, managers,
and entrepreneurs to have a comprehensive view about the recent development of
machine learning technologies and applications for leaf disease detection. This
study will provide a survey in different aspects of the topic including data,
techniques, and applications. The paper will start with publicly available
datasets. After that, we summarize common machine learning techniques,
including traditional (shallow) learning, deep learning, and augmented
learning. Finally, we discuss related applications. This paper would provide
useful resources for future study and application of machine learning for smart
agriculture in general and leaf disease classification in particular.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jianping Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1&quot;&gt;Son N. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawyer_S/0/1/0/all/0/1&quot;&gt;Samantha Sawyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Saurabh Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12520">
<title>Lost in Translation: When GPT-4V(ision) Can&apos;t See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond. (arXiv:2310.12520v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.12520</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in multimodal techniques open exciting possibilities for
models excelling in diverse tasks involving text, audio, and image processing.
Models like GPT-4V, blending computer vision and language modeling, excel in
complex text and image tasks. Numerous prior research endeavors have diligently
examined the performance of these Vision Large Language Models (VLLMs) across
tasks like object detection, image captioning and others. However, these
analyses often focus on evaluating the performance of each modality in
isolation, lacking insights into their cross-modal interactions. Specifically,
questions concerning whether these vision-language models execute vision and
language tasks consistently or independently have remained unanswered. In this
study, we draw inspiration from recent investigations into multilingualism and
conduct a comprehensive analysis of model&apos;s cross-modal interactions. We
introduce a systematic framework that quantifies the capability disparities
between different modalities in the multi-modal setting and provide a set of
datasets designed for these evaluations. Our findings reveal that models like
GPT-4V tend to perform consistently modalities when the tasks are relatively
simple. However, the trustworthiness of results derived from the vision
modality diminishes as the tasks become more challenging. Expanding on our
findings, we introduce &quot;Vision Description Prompting,&quot; a method that
effectively improves performance in challenging vision-related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Senyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1&quot;&gt;Ning Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12547">
<title>PGA: Personalizing Grasping Agents with Single Human-Robot Interaction. (arXiv:2310.12547v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that
ground and grasp objects based on natural language instructions. While robots
capable of recognizing personal objects like &quot;my wallet&quot; can interact more
naturally with non-expert users, current LCRG systems primarily limit robots to
understanding only generic expressions. To this end, we introduce a task
scenario GraspMine with a novel dataset that aims to locate and grasp personal
objects given personal indicators via learning from a single human-robot
interaction. To address GraspMine, we propose Personalized Grasping Agent
(PGA), that learns personal objects by propagating user-given information
through a Reminiscence-a collection of raw images from the user&apos;s environment.
Specifically, PGA acquires personal object information by a user presenting a
personal object with its associated indicator, followed by PGA inspecting the
object by rotating it. Based on the acquired information, PGA pseudo-labels
objects in the Reminiscence by our proposed label propagation algorithm.
Harnessing the information acquired from the interactions and the
pseudo-labeled objects in the Reminiscence, PGA adapts the object grounding
model to grasp personal objects. Experiments on GraspMine show that PGA
significantly outperforms baseline methods both in offline and online settings,
signifying its effectiveness and personalization applicability on real-world
scenarios. Finally, qualitative analysis shows the effectiveness of PGA through
a detailed investigation of results in each phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Gi-Cheon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaein Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seoyun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Minjoon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12553">
<title>Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12553</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of explanations for the predictions of complex machine learning
predictors is often measured using insertion and deletion metrics, which assess
the faithfulness of the explanations, i.e., how correctly the explanations
reflect the predictor&apos;s behavior. To improve the faithfulness, we propose
insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which
optimizes differentiable predictors to improve both insertion and deletion
scores of the explanations while keeping their predictive accuracy. Since the
original insertion and deletion metrics are indifferentiable with respect to
the explanations and directly unavailable for gradient-based optimization, we
extend the metrics to be differentiable and use them to formalize insertion and
deletion metric-based regularizers. The experimental results on image and
tabular datasets show that the deep neural networks-based predictors fine-tuned
using ID-ExpO enable popular post-hoc explainers to produce more faithful and
easy-to-interpret explanations while keeping high predictive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshikawa_u/0/1/0/all/0/1&quot;&gt;uya Yoshikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1&quot;&gt;Tomoharu Iwata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12562">
<title>Click on Mask: A Labor-efficient Annotation Framework with Level Set for Infrared Small Target Detection. (arXiv:2310.12562v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12562</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared Small Target Detection is a challenging task to separate small
targets from infrared clutter background. Recently, deep learning paradigms
have achieved promising results. However, these data-driven methods need plenty
of manual annotation. Due to the small size of infrared targets, manual
annotation consumes more resources and restricts the development of this field.
This letter proposed a labor-efficient and cursory annotation framework with
level set, which obtains a high-quality pseudo mask with only one cursory
click. A variational level set formulation with an expectation difference
energy functional is designed, in which the zero level contour is intrinsically
maintained during the level set evolution. It solves the issue that zero level
contour disappearing due to small target size and excessive regularization.
Experiments on the NUAA-SIRST and IRSTD-1k datasets reveal that our approach
achieves superior performance. Code is available at
https://github.com/Li-Haoqing/COM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinfu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runshi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12570">
<title>DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12570</link>
<description rdf:parseType="Literal">&lt;p&gt;Great progress has been made in automatic medical image segmentation due to
powerful deep representation learning. The influence of transformer has led to
research into its variants, and large-scale replacement of traditional CNN
modules. However, such trend often overlooks the intrinsic feature extraction
capabilities of the transformer and potential refinements to both the model and
the transformer module through minor adjustments. This study proposes a novel
deep medical image segmentation framework, called DA-TransUNet, aiming to
introduce the Transformer and dual attention block into the encoder and decoder
of the traditional U-shaped architecture. Unlike prior transformer-based
solutions, our DA-TransUNet utilizes attention mechanism of transformer and
multifaceted feature extraction of DA-Block, which can efficiently combine
global, local, and multi-scale features to enhance medical image segmentation.
Meanwhile, experimental results show that a dual attention block is added
before the Transformer layer to facilitate feature extraction in the U-net
structure. Furthermore, incorporating dual attention blocks in skip connections
can enhance feature transfer to the decoder, thereby improving image
segmentation performance. Experimental results across various benchmark of
medical image segmentation reveal that DA-TransUNet significantly outperforms
the state-of-the-art methods. The codes and parameters of our model will be
publicly available at https://github.com/SUN-1024/DA-TransUnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guanqun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yizhi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1&quot;&gt;Weikun Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zichang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianhua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Racharak_T/0/1/0/all/0/1&quot;&gt;Teeradaj Racharak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Le-Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Junyi Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12574">
<title>A reproducible 3D convolutional neural network with dual attention module (3D-DAM) for Alzheimer&apos;s disease classification. (arXiv:2310.12574v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12574</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease is one of the most common types of neurodegenerative
disease, characterized by the accumulation of amyloid-beta plaque and tau
tangles. Recently, deep learning approaches have shown promise in Alzheimer&apos;s
disease diagnosis. In this study, we propose a reproducible model that utilizes
a 3D convolutional neural network with a dual attention module for Alzheimer&apos;s
disease classification. We trained the model in the ADNI database and verified
the generalizability of our method in two independent datasets (AIBL and
OASIS1). Our method achieved state-of-the-art classification performance, with
an accuracy of 91.94% for MCI progression classification and 96.30% for
Alzheimer&apos;s disease classification on the ADNI dataset. Furthermore, the model
demonstrated good generalizability, achieving an accuracy of 86.37% on the AIBL
dataset and 83.42% on the OASIS1 dataset. These results indicate that our
proposed approach has competitive performance and generalizability when
compared to recent studies in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoang_G/0/1/0/all/0/1&quot;&gt;Gia Minh Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Youngjoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jae Gwan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12583">
<title>Diverse Diffusion: Enhancing Image Diversity in Text-to-Image Generation. (arXiv:2310.12583v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12583</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent diffusion models excel at producing high-quality images from text.
Yet, concerns appear about the lack of diversity in the generated imagery. To
tackle this, we introduce Diverse Diffusion, a method for boosting image
diversity beyond gender and ethnicity, spanning into richer realms, including
color diversity.Diverse Diffusion is a general unsupervised technique that can
be applied to existing text-to-image models. Our approach focuses on finding
vectors in the Stable Diffusion latent space that are distant from each other.
We generate multiple vectors in the latent space until we find a set of vectors
that meets the desired distance requirements and the required batch size.To
evaluate the effectiveness of our diversity methods, we conduct experiments
examining various characteristics, including color diversity, LPIPS metric, and
ethnicity/gender representation in images featuring humans.The results of our
experiments emphasize the significance of diversity in generating realistic and
varied images, offering valuable insights for improving text-to-image models.
Through the enhancement of image diversity, our approach contributes to the
creation of more inclusive and representative AI-generated art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zameshina_M/0/1/0/all/0/1&quot;&gt;Mariia Zameshina&lt;/a&gt; (LIGM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teytaud_O/0/1/0/all/0/1&quot;&gt;Olivier Teytaud&lt;/a&gt; (TAU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1&quot;&gt;Laurent Najman&lt;/a&gt; (LIGM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12590">
<title>PrivacyGAN: robust generative image privacy. (arXiv:2310.12590v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12590</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical techniques for protecting facial image privacy typically fall into
two categories: data-poisoning methods, exemplified by Fawkes, which introduce
subtle perturbations to images, or anonymization methods that generate images
resembling the original only in several characteristics, such as gender,
ethnicity, or facial expression.In this study, we introduce a novel approach,
PrivacyGAN, that uses the power of image generation techniques, such as VQGAN
and StyleGAN, to safeguard privacy while maintaining image usability,
particularly for social media applications. Drawing inspiration from Fawkes,
our method entails shifting the original image within the embedding space
towards a decoy image.We evaluate our approach using privacy metrics on
traditional and novel facial image datasets. Additionally, we propose new
criteria for evaluating the robustness of privacy-protection methods against
unknown image recognition techniques, and we demonstrate that our approach is
effective even in unknown embedding transfer scenarios. We also provide a human
evaluation that further proves that the modified image preserves its utility as
it remains recognisable as an image of the same person by friends and family.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zameshina_M/0/1/0/all/0/1&quot;&gt;Mariia Zameshina&lt;/a&gt; (LIGM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Careil_M/0/1/0/all/0/1&quot;&gt;Marlene Careil&lt;/a&gt; (MM, IDS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teytaud_O/0/1/0/all/0/1&quot;&gt;Olivier Teytaud&lt;/a&gt; (LRI, TANC), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1&quot;&gt;Laurent Najman&lt;/a&gt; (LIGM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12600">
<title>FUSC: Fetal Ultrasound Semantic Clustering of Second Trimester Scans Using Deep Self-supervised Learning. (arXiv:2310.12600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12600</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound is the primary imaging modality in clinical practice during
pregnancy. More than 140M fetuses are born yearly, resulting in numerous scans.
The availability of a large volume of fetal ultrasound scans presents the
opportunity to train robust machine learning models. However, the abundance of
scans also has its challenges, as manual labeling of each image is needed for
supervised methods. Labeling is typically labor-intensive and requires
expertise to annotate the images accurately. This study presents an
unsupervised approach for automatically clustering ultrasound images into a
large range of fetal views, reducing or eliminating the need for manual
labeling. Our Fetal Ultrasound Semantic Clustering (FUSC) method is developed
using a large dataset of 88,063 images and further evaluated on an additional
unseen dataset of 8,187 images achieving over 92% clustering purity. The result
of our investigation hold the potential to significantly impact the field of
fetal ultrasound imaging and pave the way for more advanced automated labeling
solutions. Finally, we make the code and the experimental setup publicly
available to help advance the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alasmawi_H/0/1/0/all/0/1&quot;&gt;Hussain Alasmawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bricker_L/0/1/0/all/0/1&quot;&gt;Leanne Bricker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1&quot;&gt;Mohammad Yaqub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12616">
<title>Cross-attention Spatio-temporal Context Transformer for Semantic Segmentation of Historical Maps. (arXiv:2310.12616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12616</link>
<description rdf:parseType="Literal">&lt;p&gt;Historical maps provide useful spatio-temporal information on the Earth&apos;s
surface before modern earth observation techniques came into being. To extract
information from maps, neural networks, which gain wide popularity in recent
years, have replaced hand-crafted map processing methods and tedious manual
labor. However, aleatoric uncertainty, known as data-dependent uncertainty,
inherent in the drawing/scanning/fading defects of the original map sheets and
inadequate contexts when cropping maps into small tiles considering the memory
limits of the training process, challenges the model to make correct
predictions. As aleatoric uncertainty cannot be reduced even with more training
data collected, we argue that complementary spatio-temporal contexts can be
helpful. To achieve this, we propose a U-Net-based network that fuses
spatio-temporal features with cross-attention transformers (U-SpaTem),
aggregating information at a larger spatial range as well as through a temporal
sequence of images. Our model achieves a better performance than other
state-or-art models that use either temporal or spatial contexts. Compared with
pure vision transformers, our model is more lightweight and effective. To the
best of our knowledge, leveraging both spatial and temporal contexts have been
rarely explored before in the segmentation task. Even though our application is
on segmenting historical maps, we believe that the method can be transferred
into other fields with similar problems like temporal sequences of satellite
images. Our code is freely accessible at
https://github.com/chenyizi086/wu.2023.sigspatial.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sidi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yizi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurni_L/0/1/0/all/0/1&quot;&gt;Lorenz Hurni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12630">
<title>Heart Disease Detection using Vision-Based Transformer Models from ECG Images. (arXiv:2310.12630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12630</link>
<description rdf:parseType="Literal">&lt;p&gt;Heart disease, also known as cardiovascular disease, is a prevalent and
critical medical condition characterized by the impairment of the heart and
blood vessels, leading to various complications such as coronary artery
disease, heart failure, and myocardial infarction. The timely and accurate
detection of heart disease is of paramount importance in clinical practice.
Early identification of individuals at risk enables proactive interventions,
preventive measures, and personalized treatment strategies to mitigate the
progression of the disease and reduce adverse outcomes. In recent years, the
field of heart disease detection has witnessed notable advancements due to the
integration of sophisticated technologies and computational approaches. These
include machine learning algorithms, data mining techniques, and predictive
modeling frameworks that leverage vast amounts of clinical and physiological
data to improve diagnostic accuracy and risk stratification. In this work, we
propose to detect heart disease from ECG images using cutting-edge
technologies, namely vision transformer models. These models are Google-Vit,
Microsoft-Beit, and Swin-Tiny. To the best of our knowledge, this is the
initial endeavor concentrating on the detection of heart diseases through
image-based ECG data by employing cuttingedge technologies namely, transformer
models. To demonstrate the contribution of the proposed framework, the
performance of vision transformer models are compared with state-of-the-art
studies. Experiment results show that the proposed framework exhibits
remarkable classification results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1&quot;&gt;Zeynep Hilal Kilimci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yalcin_M/0/1/0/all/0/1&quot;&gt;Mustafa Yalcin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucukmanisa_A/0/1/0/all/0/1&quot;&gt;Ayhan Kucukmanisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Amit Kumar Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12646">
<title>TRUSTED: The Paired 3D Transabdominal Ultrasound and CT Human Data for Kidney Segmentation and Registration Research. (arXiv:2310.12646v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12646</link>
<description rdf:parseType="Literal">&lt;p&gt;Inter-modal image registration (IMIR) and image segmentation with abdominal
Ultrasound (US) data has many important clinical applications, including
image-guided surgery, automatic organ measurement and robotic navigation.
However, research is severely limited by the lack of public datasets. We
propose TRUSTED (the Tridimensional Renal Ultra Sound TomodEnsitometrie
Dataset), comprising paired transabdominal 3DUS and CT kidney images from 48
human patients (96 kidneys), including segmentation, and anatomical landmark
annotations by two experienced radiographers. Inter-rater segmentation
agreement was over 94 (Dice score), and gold-standard segmentations were
generated using the STAPLE algorithm. Seven anatomical landmarks were
annotated, important for IMIR systems development and evaluation. To validate
the dataset&apos;s utility, 5 competitive Deep Learning models for automatic kidney
segmentation were benchmarked, yielding average DICE scores from 83.2% to 89.1%
for CT, and 61.9% to 79.4% for US images. Three IMIR methods were benchmarked,
and Coherent Point Drift performed best with an average Target Registration
Error of 4.53mm. The TRUSTED dataset may be used freely researchers to develop
and validate new segmentation and IMIR methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ndzimbong_W/0/1/0/all/0/1&quot;&gt;William Ndzimbong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fourniol_C/0/1/0/all/0/1&quot;&gt;Cyril Fourniol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Themyr_L/0/1/0/all/0/1&quot;&gt;Loic Themyr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Keeza_Y/0/1/0/all/0/1&quot;&gt;Yvonne Keeza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sauer_B/0/1/0/all/0/1&quot;&gt;Beniot Sauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piechaud_P/0/1/0/all/0/1&quot;&gt;Pierre-Thierry Piechaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mejean_A/0/1/0/all/0/1&quot;&gt;Arnaud Mejean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marescaux_J/0/1/0/all/0/1&quot;&gt;Jacques Marescaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+George_D/0/1/0/all/0/1&quot;&gt;Daniel George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hostettler_A/0/1/0/all/0/1&quot;&gt;Alexandre Hostettler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Collins_T/0/1/0/all/0/1&quot;&gt;Toby Collins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12677">
<title>Weakly Supervised Learning for Breast Cancer Prediction on Mammograms in Realistic Settings. (arXiv:2310.12677v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12677</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic methods for early detection of breast cancer on mammography can
significantly decrease mortality. Broad uptake of those methods in hospitals is
currently hindered because the methods have too many constraints. They assume
annotations available for single images or even regions-of-interest (ROIs), and
a fixed number of images per patient. Both assumptions do not hold in a general
hospital setting. Relaxing those assumptions results in a weakly supervised
learning setting, where labels are available per case, but not for individual
images or ROIs. Not all images taken for a patient contain malignant regions
and the malignant ROIs cover only a tiny part of an image, whereas most image
regions represent benign tissue. In this work, we investigate a two-level
multi-instance learning (MIL) approach for case-level breast cancer prediction
on two public datasets (1.6k and 5k cases) and an in-house dataset of 21k
cases. Observing that breast cancer is usually only present in one side, while
images of both breasts are taken as a precaution, we propose a domain-specific
MIL pooling variant. We show that two-level MIL can be applied in realistic
clinical settings where only case labels, and a variable number of images per
patient are available. Data in realistic settings scales with continuous
patient intake, while manual annotation efforts do not. Hence, research should
focus in particular on unsupervised ROI extraction, in order to improve breast
cancer prediction for all patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1&quot;&gt;Shreyasi Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotterer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Schl&amp;#xf6;tterer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geerdink_J/0/1/0/all/0/1&quot;&gt;Jeroen Geerdink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijlbrief_O/0/1/0/all/0/1&quot;&gt;Onno Dirk Vijlbrief&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1&quot;&gt;Maurice van Keulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12678">
<title>TapMo: Shape-aware Motion Generation of Skeleton-free Characters. (arXiv:2310.12678v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2310.12678</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous motion generation methods are limited to the pre-rigged 3D human
model, hindering their applications in the animation of various non-rigged
characters. In this work, we present TapMo, a Text-driven Animation Pipeline
for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The
pivotal innovation in TapMo is its use of shape deformation-aware features as a
condition to guide the diffusion model, thereby enabling the generation of
mesh-specific motions for various characters. Specifically, TapMo comprises two
main components - Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh
Handle Predictor predicts the skinning weights and clusters mesh vertices into
adaptive handles for deformation control, which eliminates the need for
traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion
with mesh-specific adaptations. This module employs text-guided motions and
mesh features extracted during the first stage, preserving the geometric
integrity of the animations by accounting for the character&apos;s shape and
deformation. Trained in a weakly-supervised manner, TapMo can accommodate a
multitude of non-human meshes, both with and without associated text motions.
We demonstrate the effectiveness and generalizability of TapMo through rigorous
qualitative and quantitative experiments. Our results reveal that TapMo
consistently outperforms existing auto-animation methods, delivering
superior-quality animations for both seen or unseen heterogeneous 3D
characters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaoli Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhigang Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xiaohang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12692">
<title>Representation Learning via Consistent Assignment of Views over Random Partitions. (arXiv:2310.12692v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12692</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Consistent Assignment of Views over Random Partitions (CARP), a
self-supervised clustering method for representation learning of visual
features. CARP learns prototypes in an end-to-end online fashion using gradient
descent without additional non-differentiable modules to solve the cluster
assignment problem. CARP optimizes a new pretext task based on random
partitions of prototypes that regularizes the model and enforces consistency
between views&apos; assignments. Additionally, our method improves training
stability and prevents collapsed solutions in joint-embedding training. Through
an extensive evaluation, we demonstrate that CARP&apos;s representations are
suitable for learning downstream tasks. We evaluate CARP&apos;s representations
capabilities in 17 datasets across many standard protocols, including linear
evaluation, few-shot classification, k-NN, k-means, image retrieval, and copy
detection. We compare CARP performance to 11 existing self-supervised methods.
We extensively ablate our method and demonstrate that our proposed random
partition pretext task improves the quality of the learned representations by
devising multiple random classification tasks. In transfer learning tasks, CARP
achieves the best performance on average against many SSL methods trained for a
longer time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_T/0/1/0/all/0/1&quot;&gt;Thalles Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1&quot;&gt;Ad&amp;#xed;n Ram&amp;#xed;rez Rivera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12705">
<title>Exploiting Low-confidence Pseudo-labels for Source-free Object Detection. (arXiv:2310.12705v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12705</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free object detection (SFOD) aims to adapt a source-trained detector
to an unlabeled target domain without access to the labeled source data.
Current SFOD methods utilize a threshold-based pseudo-label approach in the
adaptation phase, which is typically limited to high-confidence pseudo-labels
and results in a loss of information. To address this issue, we propose a new
approach to take full advantage of pseudo-labels by introducing high and low
confidence thresholds. Specifically, the pseudo-labels with confidence scores
above the high threshold are used conventionally, while those between the low
and high thresholds are exploited using the Low-confidence Pseudo-labels
Utilization (LPU) module. The LPU module consists of Proposal Soft Training
(PST) and Local Spatial Contrastive Learning (LSCL). PST generates soft labels
of proposals for soft training, which can mitigate the label mismatch problem.
LSCL exploits the local spatial relationship of proposals to improve the
model&apos;s ability to differentiate between spatially adjacent proposals, thereby
optimizing representational features further. Combining the two components
overcomes the challenges faced by traditional methods in utilizing
low-confidence pseudo-labels. Extensive experiments on five cross-domain object
detection benchmarks demonstrate that our proposed method outperforms the
previous SFOD methods, achieving state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12707">
<title>Recoverable Privacy-Preserving Image Classification through Noise-like Adversarial Examples. (arXiv:2310.12707v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12707</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing prevalence of cloud computing platforms, ensuring data
privacy during the cloud-based image related services such as classification
has become crucial. In this study, we propose a novel privacypreserving image
classification scheme that enables the direct application of classifiers
trained in the plaintext domain to classify encrypted images, without the need
of retraining a dedicated classifier. Moreover, encrypted images can be
decrypted back into their original form with high fidelity (recoverable) using
a secret key. Specifically, our proposed scheme involves utilizing a feature
extractor and an encoder to mask the plaintext image through a newly designed
Noise-like Adversarial Example (NAE). Such an NAE not only introduces a
noise-like visual appearance to the encrypted image but also compels the target
classifier to predict the ciphertext as the same label as the original
plaintext image. At the decoding phase, we adopt a Symmetric Residual Learning
(SRL) framework for restoring the plaintext image with minimal degradation.
Extensive experiments demonstrate that 1) the classification accuracy of the
classifier trained in the plaintext domain remains the same in both the
ciphertext and plaintext domains; 2) the encrypted images can be recovered into
their original form with an average PSNR of up to 51+ dB for the SVHN dataset
and 48+ dB for the VGGFace2 dataset; 3) our system exhibits satisfactory
generalization capability on the encryption, decryption and classification
tasks across datasets that are different from the training one; and 4) a
high-level of security is achieved against three potential threat models. The
code is available at https://github.com/csjunjun/RIC.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinyu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weiwei Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12708">
<title>Generating Robust Adversarial Examples against Online Social Networks (OSNs). (arXiv:2310.12708v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2310.12708</link>
<description rdf:parseType="Literal">&lt;p&gt;Online Social Networks (OSNs) have blossomed into prevailing transmission
channels for images in the modern era. Adversarial examples (AEs) deliberately
designed to mislead deep neural networks (DNNs) are found to be fragile against
the inevitable lossy operations conducted by OSNs. As a result, the AEs would
lose their attack capabilities after being transmitted over OSNs. In this work,
we aim to design a new framework for generating robust AEs that can survive the
OSN transmission; namely, the AEs before and after the OSN transmission both
possess strong attack capabilities. To this end, we first propose a
differentiable network termed SImulated OSN (SIO) to simulate the various
operations conducted by an OSN. Specifically, the SIO network consists of two
modules: 1) a differentiable JPEG layer for approximating the ubiquitous JPEG
compression and 2) an encoder-decoder subnetwork for mimicking the remaining
operations. Based upon the SIO network, we then formulate an optimization
framework to generate robust AEs by enforcing model outputs with and without
passing through the SIO to be both misled. Extensive experiments conducted over
Facebook, WeChat and QQ demonstrate that our attack methods produce more robust
AEs than existing approaches, especially under small distortion constraints;
the performance gain in terms of Attack Success Rate (ASR) could be more than
60%. Furthermore, we build a public dataset containing more than 10,000 pairs
of AEs processed by Facebook, WeChat or QQ, facilitating future research in the
robust AEs generation. The dataset and code are available at
https://github.com/csjunjun/RobustOSNAttack.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haiwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weiwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinyu Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12724">
<title>Query-aware Long Video Localization and Relation Discrimination for Deep Video Understanding. (arXiv:2310.12724v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12724</link>
<description rdf:parseType="Literal">&lt;p&gt;The surge in video and social media content underscores the need for a deeper
understanding of multimedia data. Most of the existing mature video
understanding techniques perform well with short formats and content that
requires only shallow understanding, but do not perform well with long format
videos that require deep understanding and reasoning. Deep Video Understanding
(DVU) Challenge aims to push the boundaries of multimodal extraction, fusion,
and analytics to address the problem of holistically analyzing long videos and
extract useful knowledge to solve different types of queries. This paper
introduces a query-aware method for long video localization and relation
discrimination, leveraging an imagelanguage pretrained model. This model
adeptly selects frames pertinent to queries, obviating the need for a complete
movie-level knowledge graph. Our approach achieved first and fourth positions
for two groups of movie-level queries. Sufficient experiments and final
rankings demonstrate its effectiveness and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuting Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bin Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12733">
<title>Multiscale Motion-Aware and Spatial-Temporal-Channel Contextual Coding Network for Learned Video Compression. (arXiv:2310.12733v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12733</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learned video compression has achieved exciting performance.
Following the traditional hybrid prediction coding framework, most learned
methods generally adopt the motion estimation motion compensation (MEMC) method
to remove inter-frame redundancy. However, inaccurate motion vector (MV)
usually lead to the distortion of reconstructed frame. In addition, most
approaches ignore the spatial and channel redundancy. To solve above problems,
we propose a motion-aware and spatial-temporal-channel contextual coding based
video compression network (MASTC-VC), which learns the latent representation
and uses variational autoencoders (VAEs) to capture the characteristics of
intra-frame pixels and inter-frame motion. Specifically, we design a multiscale
motion-aware module (MS-MAM) to estimate spatial-temporal-channel consistent
motion vector by utilizing the multiscale motion prediction information in a
coarse-to-fine way. On the top of it, we further propose a
spatial-temporal-channel contextual module (STCCM), which explores the
correlation of latent representation to reduce the bit consumption from
spatial, temporal and channel aspects respectively. Comprehensive experiments
show that our proposed MASTC-VC is surprior to previous state-of-the-art (SOTA)
methods on three public benchmark datasets. More specifically, our method
brings average 10.15\% BD-rate savings against H.265/HEVC (HM-16.20) in PSNR
metric and average 23.93\% BD-rate savings against H.266/VVC (VTM-13.2) in
MS-SSIM metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Bin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huashan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12736">
<title>ExtSwap: Leveraging Extended Latent Mapper for Generating High Quality Face Swapping. (arXiv:2310.12736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12736</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel face swapping method using the progressively growing
structure of a pre-trained StyleGAN. Previous methods use different encoder
decoder structures, embedding integration networks to produce high-quality
results, but their quality suffers from entangled representation. We
disentangle semantics by deriving identity and attribute features separately.
By learning to map the concatenated features into the extended latent space, we
leverage the state-of-the-art quality and its rich semantic extended latent
space. Extensive experiments suggest that the proposed method successfully
disentangles identity and attribute features and outperforms many
state-of-the-art face swapping methods, both qualitatively and quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+PN_A/0/1/0/all/0/1&quot;&gt;Aravinda Reddy PN&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;K.Sreenivasa Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1&quot;&gt;Raghavendra Ramachandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+mitra_P/0/1/0/all/0/1&quot;&gt;Pabitra mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12755">
<title>Minimalist and High-Performance Semantic Segmentation with Plain Vision Transformers. (arXiv:2310.12755v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12755</link>
<description rdf:parseType="Literal">&lt;p&gt;In the wake of Masked Image Modeling (MIM), a diverse range of plain,
non-hierarchical Vision Transformer (ViT) models have been pre-trained with
extensive datasets, offering new paradigms and significant potential for
semantic segmentation. Current state-of-the-art systems incorporate numerous
inductive biases and employ cumbersome decoders. Building upon the original
motivations of plain ViTs, which are simplicity and generality, we explore
high-performance `minimalist&apos; systems to this end. Our primary purpose is to
provide simple and efficient baselines for practical semantic segmentation with
plain ViTs. Specifically, we first explore the feasibility and methodology for
achieving high-performance semantic segmentation using the last feature map. As
a result, we introduce the PlainSeg, a model comprising only three 3$\times$3
convolutions in addition to the transformer layers (either encoder or decoder).
In this process, we offer insights into two underlying principles: (i)
high-resolution features are crucial to high performance in spite of employing
simple up-sampling techniques and (ii) the slim transformer decoder requires a
much larger learning rate than the wide transformer decoder. On this basis, we
further present the PlainSeg-Hier, which allows for the utilization of
hierarchical features. Extensive experiments on four popular benchmarks
demonstrate the high performance and efficiency of our methods. They can also
serve as powerful tools for assessing the transfer ability of base models in
semantic segmentation. Code is available at
\url{https://github.com/ydhongHIT/PlainSeg}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yuanduo Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Huihui Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12769">
<title>Mixing Histopathology Prototypes into Robust Slide-Level Representations for Cancer Subtyping. (arXiv:2310.12769v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12769</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole-slide image analysis via the means of computational pathology often
relies on processing tessellated gigapixel images with only slide-level labels
available. Applying multiple instance learning-based methods or transformer
models is computationally expensive as, for each image, all instances have to
be processed simultaneously. The MLP-Mixer is an under-explored alternative
model to common vision transformers, especially for large-scale datasets. Due
to the lack of a self-attention mechanism, they have linear computational
complexity to the number of input patches but achieve comparable performance on
natural image datasets. We propose a combination of feature embedding and
clustering to preprocess the full whole-slide image into a reduced prototype
representation which can then serve as input to a suitable MLP-Mixer
architecture. Our experiments on two public benchmarks and one inhouse
malignant lymphoma dataset show comparable performance to current
state-of-the-art methods, while achieving lower training costs in terms of
computational time and memory load. Code is publicly available at
https://github.com/butkej/ProtoMixer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butke_J/0/1/0/all/0/1&quot;&gt;Joshua Butke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_N/0/1/0/all/0/1&quot;&gt;Noriaki Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyoshi_H/0/1/0/all/0/1&quot;&gt;Hiroaki Miyoshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohshima_K/0/1/0/all/0/1&quot;&gt;Koichi Ohshima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakuma_J/0/1/0/all/0/1&quot;&gt;Jun Sakuma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12787">
<title>DT/MARS-CycleGAN: Improved Object Detection for MARS Phenotyping Robot. (arXiv:2310.12787v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12787</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic crop phenotyping has emerged as a key technology to assess crops&apos;
morphological and physiological traits at scale. These phenotypical
measurements are essential for developing new crop varieties with the aim of
increasing productivity and dealing with environmental challenges such as
climate change. However, developing and deploying crop phenotyping robots face
many challenges such as complex and variable crop shapes that complicate
robotic object detection, dynamic and unstructured environments that baffle
robotic control, and real-time computing and managing big data that challenge
robotic hardware/software. This work specifically tackles the first challenge
by proposing a novel Digital-Twin(DT)MARS-CycleGAN model for image augmentation
to improve our Modular Agricultural Robotic System (MARS)&apos;s crop object
detection from complex and variable backgrounds. Our core idea is that in
addition to the cycle consistency losses in the CycleGAN model, we designed and
enforced a new DT-MARS loss in the deep learning model to penalize the
inconsistency between real crop images captured by MARS and synthesized images
sensed by DT MARS. Therefore, the generated synthesized crop images closely
mimic real images in terms of realism, and they are employed to fine-tune
object detectors such as YOLOv8. Extensive experiments demonstrated that our
new DT/MARS-CycleGAN framework significantly boosts our MARS&apos; crop object/row
detector&apos;s performance, contributing to the field of robotic crop phenotyping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;David Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengkun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changying Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12790">
<title>Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection. (arXiv:2310.12790v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12790</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly
detection area - aims at utilizing a few samples of anomaly classes seen during
training to detect unseen anomalies (i.e., samples from open-set anomaly
classes), while effectively identifying the seen anomalies. Benefiting from the
prior knowledge illustrated by the seen anomalies, current OSAD methods can
often largely reduce false positive errors. However, these methods treat the
anomaly examples as from a homogeneous distribution, rendering them less
effective in generalizing to unseen anomalies that can be drawn from any
distribution. In this paper, we propose to learn heterogeneous anomaly
distributions using the limited anomaly examples to address this issue. To this
end, we introduce a novel approach, namely Anomaly Heterogeneity Learning
(AHL), that simulates a diverse set of heterogeneous (seen and unseen) anomaly
distributions and then utilizes them to learn a unified heterogeneous
abnormality model. Further, AHL is a generic framework that existing OSAD
models can plug and play for enhancing their abnormality modeling. Extensive
experiments on nine real-world anomaly detection datasets show that AHL can 1)
substantially enhance different state-of-the-art (SOTA) OSAD models in
detecting both seen and unseen anomalies, achieving new SOTA performance on a
large set of datasets, and 2) effectively generalize to unseen anomalies in new
target domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiawen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Choubo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12793">
<title>OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.12793</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing works have made great progress in improving adversarial robustness,
but typically test their method only on data from the same distribution as the
training data, i.e. in-distribution (ID) testing. As a result, it is unclear
how such robustness generalizes under input distribution shifts, i.e.
out-of-distribution (OOD) testing. This is a concerning omission as such
distribution shifts are unavoidable when methods are deployed in the wild. To
address this issue we propose a benchmark named OODRobustBench to
comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts
(i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts
(i.e., unforeseen adversarial threat models). OODRobustBench is used to assess
706 robust models using 60.7K adversarial evaluations. This large-scale
analysis shows that: 1) adversarial robustness suffers from a severe OOD
generalization issue; 2) ID robustness correlates strongly with OOD robustness,
in a positive linear way, under many distribution shifts. The latter enables
the prediction of OOD robustness from ID robustness. Based on this, we are able
to predict the upper limit of OOD robustness for existing robust training
schemes. The results suggest that achieving OOD robustness requires designing
novel methods beyond the conventional ones. Last, we discover that extra data,
data augmentation, advanced model architectures and particular regularization
approaches can improve OOD robustness. Noticeably, the discovered training
schemes, compared to the baseline, exhibit dramatically higher robustness under
threat shift while keeping high ID robustness, demonstrating new promising
solutions for robustness against both multi-attack and unforeseen attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1&quot;&gt;Chawin Sitawarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1&quot;&gt;Michael Spratling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12817">
<title>2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12817</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a Multimodal Interlaced Transformer (MIT) that jointly considers
2D and 3D data for weakly supervised point cloud segmentation. Research studies
have shown that 2D and 3D features are complementary for point cloud
segmentation. However, existing methods require extra 2D annotations to achieve
2D-3D information fusion. Considering the high annotation cost of point clouds,
effective 2D and 3D feature fusion based on weakly supervised learning is in
great demand. To this end, we propose a transformer model with two encoders and
one decoder for weakly supervised point cloud segmentation using only
scene-level class tags. Specifically, the two encoders compute the
self-attended features for 3D point clouds and 2D multi-view images,
respectively. The decoder implements interlaced 2D-3D cross-attention and
carries out implicit 2D and 3D feature fusion. We alternately switch the roles
of queries and key-value pairs in the decoder layers. It turns out that the 2D
and 3D features are iteratively enriched by each other. Experiments show that
it performs favorably against existing weakly supervised point cloud
segmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The
project page will be available at https://jimmy15923.github.io/mit_web/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yung-Yu Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Yu Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12848">
<title>Neural Degradation Representation Learning for All-In-One Image Restoration. (arXiv:2310.12848v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12848</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods have demonstrated effective performance on a single
degradation type. In practical applications, however, the degradation is often
unknown, and the mismatch between the model and the degradation will result in
a severe performance drop. In this paper, we propose an all-in-one image
restoration network that tackles multiple degradations. Due to the
heterogeneous nature of different types of degradations, it is difficult to
process multiple degradations in a single network. To this end, we propose to
learn a neural degradation representation (NDR) that captures the underlying
characteristics of various degradations. The learned NDR decomposes different
types of degradations adaptively, similar to a neural dictionary that
represents basic degradation components. Subsequently, we develop a degradation
query module and a degradation injection module to effectively recognize and
utilize the specific degradation based on NDR, enabling the all-in-one
restoration ability for multiple degradations. Moreover, we propose a
bidirectional optimization strategy to effectively drive NDR to learn the
degradation representation by optimizing the degradation and restoration
processes alternately. Comprehensive experiments on representative types of
degradations (including noise, haze, rain, and downsampling) demonstrate the
effectiveness and generalization capability of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1&quot;&gt;Mingde Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruikang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yuanshen Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12866">
<title>Predicting Ovarian Cancer Treatment Response in Histopathology using Hierarchical Vision Transformers and Multiple Instance Learning. (arXiv:2310.12866v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12866</link>
<description rdf:parseType="Literal">&lt;p&gt;For many patients, current ovarian cancer treatments offer limited clinical
benefit. For some therapies, it is not possible to predict patients&apos; responses,
potentially exposing them to the adverse effects of treatment without any
therapeutic benefit. As part of the automated prediction of treatment
effectiveness in ovarian cancer using histopathological images (ATEC23)
challenge, we evaluated the effectiveness of deep learning to predict whether a
course of treatment including the antiangiogenic drug bevacizumab could
contribute to remission or prevent disease progression for at least 6 months in
a set of 282 histopathology whole slide images (WSIs) from 78 ovarian cancer
patients. Our approach used a pretrained Hierarchical Image Pyramid Transformer
(HIPT) to extract region-level features and an attention-based multiple
instance learning (ABMIL) model to aggregate features and classify whole
slides. The optimal HIPT-ABMIL model had an internal balanced accuracy of 60.2%
+- 2.9% and an AUC of 0.646 +- 0.033. Histopathology-specific model pretraining
was found to be beneficial to classification performance, though hierarchical
transformers were not, with a ResNet feature extractor achieving similar
performance. Due to the dataset being small and highly heterogeneous,
performance was variable across 5-fold cross-validation folds, and there were
some extreme differences between validation and test set performance within
folds. The model did not generalise well to tissue microarrays, with accuracy
worse than random chance. It is not yet clear whether ovarian cancer WSIs
contain information that can be used to accurately predict treatment response,
with further validation using larger, higher-quality datasets required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Breen_J/0/1/0/all/0/1&quot;&gt;Jack Breen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Allen_K/0/1/0/all/0/1&quot;&gt;Katie Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zucker_K/0/1/0/all/0/1&quot;&gt;Kieran Zucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hall_G/0/1/0/all/0/1&quot;&gt;Geoff Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1&quot;&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Orsi_N/0/1/0/all/0/1&quot;&gt;Nicolas M. Orsi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12868">
<title>EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model. (arXiv:2310.12868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12868</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale, big-variant, and high-quality data are crucial for developing
robust and successful deep-learning models for medical applications since they
potentially enable better generalization performance and avoid overfitting.
However, the scarcity of high-quality labeled data always presents significant
challenges. This paper proposes a novel approach to address this challenge by
developing controllable diffusion models for medical image synthesis, called
EMIT-Diff. We leverage recent diffusion probabilistic models to generate
realistic and diverse synthetic medical image data that preserve the essential
characteristics of the original medical images by incorporating edge
information of objects to guide the synthesis process. In our approach, we
ensure that the synthesized samples adhere to medically relevant constraints
and preserve the underlying structure of imaging data. Due to the random
sampling process by the diffusion model, we can generate an arbitrary number of
synthetic images with diverse appearances. To validate the effectiveness of our
proposed method, we conduct an extensive set of medical image segmentation
experiments on multiple datasets, including Ultrasound breast (+13.87%), CT
spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements
over the baseline segmentation methods. For the first time, to our best
knowledge, the promising results demonstrate the effectiveness of our EMIT-Diff
for medical image segmentation tasks and show the feasibility of introducing a
first-ever text-guided diffusion model for general medical image segmentation
tasks. With carefully designed ablation experiments, we investigate the
influence of various data augmentation ratios, hyper-parameter settings, patch
size for generating random merging mask settings, and combined influence with
different network architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lanhong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keles_E/0/1/0/all/0/1&quot;&gt;Elif Keles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medetalibeyoglu_A/0/1/0/all/0/1&quot;&gt;Alpay Medetalibeyoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12877">
<title>Perceptual Assessment and Optimization of High Dynamic Range Image Rendering. (arXiv:2310.12877v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.12877</link>
<description rdf:parseType="Literal">&lt;p&gt;High dynamic range (HDR) imaging has gained increasing popularity for its
ability to faithfully reproduce the luminance levels in natural scenes.
Accordingly, HDR image quality assessment (IQA) is crucial but has been
superficially treated. The majority of existing IQA models are developed for
and calibrated against low dynamic range (LDR) images, which have been shown to
be poorly correlated with human perception of HDR image quality. In this work,
we propose a family of HDR IQA models by transferring the recent advances in
LDR IQA. The key step in our approach is to specify a simple inverse display
model that decomposes an HDR image to a set of LDR images with different
exposures, which will be assessed by existing LDR quality models. The local
quality scores of each exposure are then aggregated with the help of a simple
well-exposedness measure into a global quality score for each exposure, which
will be further weighted across exposures to obtain the overall quality score.
When assessing LDR images, the proposed HDR quality models reduce gracefully to
the original LDR ones with the same performance. Experiments on four
human-rated HDR image datasets demonstrate that our HDR quality models are
consistently better than existing IQA methods, including the HDR-VDP family.
Moreover, we demonstrate their strengths in perceptual optimization of HDR
novel view synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_P/0/1/0/all/0/1&quot;&gt;Peibei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mantiuk_R/0/1/0/all/0/1&quot;&gt;Rafal K. Mantiuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kede Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12904">
<title>Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey. (arXiv:2310.12904v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.12904</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent enthusiasm for open-world vision systems show the high interest of
the community to perform perception tasks outside of the closed-vocabulary
benchmark setups which have been so popular until now. Being able to discover
objects in images/videos without knowing in advance what objects populate the
dataset is an exciting prospect. But how to find objects without knowing
anything about them? Recent works show that it is possible to perform
class-agnostic unsupervised object localization by exploiting self-supervised
pre-trained features. We propose here a survey of unsupervised object
localization methods that discover objects in images without requiring any
manual annotation in the era of self-supervised ViTs. We gather links of
discussed methods in the repository
https://github.com/valeoai/Awesome-Unsupervised-Object-Localization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1&quot;&gt;Oriane Sim&amp;#xe9;oni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;loi Zablocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1&quot;&gt;Spyros Gidaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1&quot;&gt;Gilles Puy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.08717">
<title>Relational Self-Supervised Learning. (arXiv:2203.08717v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.08717</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduce a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. To boost the performance, we
argue that weak augmentations matter to represent a more reliable relation, and
leverage momentum strategy for practical efficiency. The designed asymmetric
predictor head and an InfoNCE warm-up strategy enhance the robustness to
hyper-parameters and benefit the resulting performance. Experimental results
show that our proposed ReSSL substantially outperforms the state-of-the-art
methods across different network architectures, including various lightweight
networks (\eg, EfficientNet and MobileNet).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07439">
<title>INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks, but
they suffer from quality degradation due to the lack of freedom as activations
and weights are constrained to the binary values. To compensate for the
accuracy drop, we propose a novel BNN design called Binary Neural Network with
INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold
dynamically in an input-dependent or instance-aware manner. According to our
observation, higher-order statistics can be a representative metric to estimate
the characteristics of the input distribution. INSTA-BNN is designed to adjust
the threshold dynamically considering various information, including
higher-order statistics, but it is also optimized judiciously to realize
minimal overhead on a real device. Our extensive study shows that INSTA-BNN
outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task
with comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy on
ResNet-18 and MobileNetV1 based models, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Changhun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungjun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jae-Joon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07162">
<title>Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07162</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel meta-learning approach for 6D pose estimation on unknown
objects. In contrast to ``instance-level&quot; and ``category-level&quot; pose estimation
methods, our algorithm learns object representation in a category-agnostic way,
which endows it with strong generalization capabilities across object
categories. Specifically, we employ a neural process-based meta-learning
approach to train an encoder to capture texture and geometry of an object in a
latent representation, based on very few RGB-D images and ground-truth
keypoints. The latent representation is then used by a simultaneously
meta-trained decoder to predict the 6D pose of the object in new images.
Furthermore, we propose a novel geometry-aware decoder for the keypoint
prediction using a Graph Neural Network (GNN), which explicitly takes geometric
constraints specific to each object into consideration. To evaluate our
algorithm, extensive experiments are conducted on the \linemod dataset, and on
our new fully-annotated synthetic datasets generated from Multiple Categories
in Multiple Scenes (MCMS). Experimental results demonstrate that our model
performs well on unseen objects with very different shapes and appearances.
Remarkably, our model also shows robust performance on occluded scenes although
trained fully on data without occlusion. To our knowledge, this is the first
work exploring \textbf{cross-category level} 6D pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yumeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1&quot;&gt;Ning Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08517">
<title>ECTLO: Effective Continuous-time Odometry Using Range Image for LiDAR with Small FoV. (arXiv:2206.08517v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08517</link>
<description rdf:parseType="Literal">&lt;p&gt;Prism-based LiDARs are more compact and cheaper than the conventional
mechanical multi-line spinning LiDARs, which have become increasingly popular
in robotics, recently. However, there are several challenges for these new
LiDAR sensors, including small field of view, severe motion distortions, and
irregular patterns, which hinder them from being widely used in LiDAR odometry,
practically. To tackle these problems, we present an effective continuous-time
LiDAR odometry (ECTLO) method for the Risley-prism-based LiDARs with
non-repetitive scanning patterns. A single range image covering historical
points in LiDAR&apos;s small FoV is adopted for efficient map representation. To
account for the noisy data from occlusions after map updating, a filter-based
point-to-plane Gaussian Mixture Model is used for robust registration.
Moreover, a LiDAR-only continuous-time motion model is employed to relieve the
inevitable distortions. Extensive experiments have been conducted on various
testbeds using the prism-based LiDARs with different scanning patterns, whose
promising results demonstrate the efficacy of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jianke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.07333">
<title>Rain regime segmentation of Sentinel-1 observation learning from NEXRAD collocations with Convolution Neural Networks. (arXiv:2207.07333v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.07333</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing of rainfall events is critical for both operational and
scientific needs, including for example weather forecasting, extreme flood
mitigation, water cycle monitoring, etc. Ground-based weather radars, such as
NOAA&apos;s Next-Generation Radar (NEXRAD), provide reflectivity and precipitation
estimates of rainfall events. However, their observation range is limited to a
few hundred kilometers, prompting the exploration of other remote sensing
methods, particularly over the open ocean, that represents large areas not
covered by land-based radars. Here we propose a deep learning approach to
deliver a three-class segmentation of SAR observations in terms of rainfall
regimes. SAR satellites deliver very high resolution observations with a global
coverage. This seems particularly appealing to inform fine-scale rain-related
patterns, such as those associated with convective cells with characteristic
scales of a few kilometers. We demonstrate that a convolutional neural network
trained on a collocated Sentinel-1/NEXRAD dataset clearly outperforms
state-of-the-art filtering schemes such as the Koch&apos;s filters. Our results
indicate high performance in segmenting precipitation regimes, delineated by
thresholds at 24.7, 31.5, and 38.8 dBZ. Compared to current methods that rely
on Koch&apos;s filters to draw binary rainfall maps, these multi-threshold
learning-based models can provide rainfall estimation. They may be of interest
in improving high-resolution SAR-derived wind fields, which are degraded by
rainfall, and provide an additional tool for the study of rain cells.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colin_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Colin&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandeo_P/0/1/0/all/0/1&quot;&gt;Pierre Tandeo&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peureux_C/0/1/0/all/0/1&quot;&gt;Charles Peureux&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husson_R/0/1/0/all/0/1&quot;&gt;Romain Husson&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longepe_N/0/1/0/all/0/1&quot;&gt;Nicolas Long&amp;#xe9;p&amp;#xe9;&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt; (1) ((1) IMT Atlantique, Lab-STICC, UMR CNRS, France, (2) Collecte Localisation Satellites, Brest, France, (3) Phi-lab Explore Office, ESRIN, European Space Agency (ESA), Frascati, Italy)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.09035">
<title>Fairness in Face Presentation Attack Detection. (arXiv:2209.09035v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.09035</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition (FR) algorithms have been proven to exhibit discriminatory
behaviors against certain demographic and non-demographic groups, raising
ethical and legal concerns regarding their deployment in real-world scenarios.
Despite the growing number of fairness studies in FR, the fairness of face
presentation attack detection (PAD) has been overlooked, mainly due to the lack
of appropriately annotated data. To avoid and mitigate the potential negative
impact of such behavior, it is essential to assess the fairness in face PAD and
develop fair PAD models. To enable fairness analysis in face PAD, we present a
Combined Attribute Annotated PAD Dataset (CAAD-PAD), offering seven
human-annotated attribute labels. Then, we comprehensively analyze the fairness
of PAD and its relation to the nature of the training data and the Operational
Decision Threshold Assignment (ODTA) through a set of face PAD solutions.
Additionally, we propose a novel metric, the Accuracy Balanced Fairness (ABF),
that jointly represents both the PAD fairness and the absolute PAD performance.
The experimental results pointed out that female and faces with occluding
features (e.g. eyeglasses, beard, etc.) are relatively less protected than male
and non-occlusion groups by all PAD solutions. To alleviate this observed
unfairness, we propose a plug-and-play data augmentation method, FairSWAP, to
disrupt the identity/semantic information and encourage models to mine the
attack clues. The extensive experimental results indicate that FairSWAP leads
to better-performing and fairer face PADs in 10 out of 12 investigated cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meiling Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wufei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1&quot;&gt;Arjan Kuijper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1&quot;&gt;Vitomir Struc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11388">
<title>Physics-informed Deep Diffusion MRI Reconstruction: Break Training Data Bottleneck in Artificial Intelligence. (arXiv:2210.11388v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11388</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion magnetic resonance imaging (MRI) is the only imaging modality for
non-invasive movement detection of in vivo water molecules, with significant
clinical and research applications. Diffusion MRI (DWI) acquired by multi-shot
techniques can achieve higher resolution, better signal-to-noise ratio, and
lower geometric distortion than single-shot, but suffers from inter-shot
motion-induced artifacts. These artifacts cannot be removed prospectively,
leading to the absence of artifact-free training labels. Thus, the potential of
deep learning in multi-shot DWI reconstruction remains largely untapped. To
break the training data bottleneck, here, we propose a Physics-Informed Deep
DWI reconstruction method (PIDD) to synthesize high-quality paired training
data by leveraging the physical diffusion model (magnitude synthesis) and
inter-shot motion-induced phase model (motion phase synthesis). The network is
trained only once with 100,000 synthetic samples, achieving encouraging results
on multiple realistic in vivo data reconstructions. Advantages over
conventional methods include: (a) Better motion artifact suppression and
reconstruction stability; (b) Outstanding generalization to multi-scenario
reconstructions, including multi-resolution, multi-b-value,
multi-undersampling, multi-vendor, and multi-center; (c) Excellent clinical
adaptability to patients with verifications by seven experienced doctors
(p&amp;lt;0.001). In conclusion, PIDD presents a novel deep learning framework by
exploiting the power of MRI physics, providing a cost-effective and explainable
way to break the data bottleneck in deep learning medical imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuncheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mingyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruan_D/0/1/0/all/0/1&quot;&gt;Dan Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yiping Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yirong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Boyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Ran Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiazheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liuhong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kang_T/0/1/0/all/0/1&quot;&gt;Taishan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianzhong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gong_T/0/1/0/all/0/1&quot;&gt;Tao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fei_G/0/1/0/all/0/1&quot;&gt;Guoqiang Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meijin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meiyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05258">
<title>Image augmentation with conformal mappings for a convolutional neural network. (arXiv:2212.05258v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05258</link>
<description rdf:parseType="Literal">&lt;p&gt;For augmentation of the square-shaped image data of a convolutional neural
network (CNN), we introduce a new method, in which the original images are
mapped onto a disk with a conformal mapping, rotated around the center of this
disk and mapped under such a M\&quot;obius transformation that preserves the disk,
and then mapped back onto their original square shape. This process does not
result the loss of information caused by removing areas from near the edges of
the original images unlike the typical transformations used in the data
augmentation for a CNN. We offer here the formulas of all the mappings needed
together with detailed instructions how to write a code for transforming the
images. The new method is also tested with simulated data and, according the
results, using this method to augment the training data of 10 images into 40
images decreases the amount of the error in the predictions by a CNN for a test
set of 160 images in a statistically significant way (p-value=0.0360).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rainio_O/0/1/0/all/0/1&quot;&gt;Oona Rainio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasser_M/0/1/0/all/0/1&quot;&gt;Mohamed M.S. Nasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuorinen_M/0/1/0/all/0/1&quot;&gt;Matti Vuorinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klen_R/0/1/0/all/0/1&quot;&gt;Riku Kl&amp;#xe9;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08731">
<title>Multi-person 3D pose estimation from unlabelled data. (arXiv:2212.08731v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08731</link>
<description rdf:parseType="Literal">&lt;p&gt;Its numerous applications make multi-human 3D pose estimation a remarkably
impactful area of research. Nevertheless, assuming a multiple-view system
composed of several regular RGB cameras, 3D multi-pose estimation presents
several challenges. First of all, each person must be uniquely identified in
the different views to separate the 2D information provided by the cameras.
Secondly, the 3D pose estimation process from the multi-view 2D information of
each person must be robust against noise and potential occlusions in the
scenario. In this work, we address these two challenges with the help of deep
learning. Specifically, we present a model based on Graph Neural Networks
capable of predicting the cross-view correspondence of the people in the
scenario along with a Multilayer Perceptron that takes the 2D points to yield
the 3D poses of each person. These two models are trained in a self-supervised
manner, thus avoiding the need for large datasets with 3D annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Criado_D/0/1/0/all/0/1&quot;&gt;Daniel Rodriguez-Criado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachiller_P/0/1/0/all/0/1&quot;&gt;Pilar Bachiller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogiatzis_G/0/1/0/all/0/1&quot;&gt;George Vogiatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manso_L/0/1/0/all/0/1&quot;&gt;Luis J. Manso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09077">
<title>Unleash the Potential of Image Branch for Cross-modal 3D Object Detection. (arXiv:2301.09077v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09077</link>
<description rdf:parseType="Literal">&lt;p&gt;To achieve reliable and precise scene understanding, autonomous vehicles
typically incorporate multiple sensing modalities to capitalize on their
complementary attributes. However, existing cross-modal 3D detectors do not
fully utilize the image domain information to address the bottleneck issues of
the LiDAR-based detectors. This paper presents a new cross-modal 3D object
detector, namely UPIDet, which aims to unleash the potential of the image
branch from two aspects. First, UPIDet introduces a new 2D auxiliary task
called normalized local coordinate map estimation. This approach enables the
learning of local spatial-aware features from the image modality to supplement
sparse point clouds. Second, we discover that the representational capability
of the point cloud backbone can be enhanced through the gradients
backpropagated from the training objectives of the image branch, utilizing a
succinct and effective point-to-pixel module. Extensive experiments and
ablation studies validate the effectiveness of our method. Notably, we achieved
the top rank in the highly competitive cyclist class of the KITTI benchmark at
the time of submission. The source code is available at
https://github.com/Eaphan/UPIDet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qijian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_G/0/1/0/all/0/1&quot;&gt;Guoliang Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01328">
<title>IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01328</link>
<description rdf:parseType="Literal">&lt;p&gt;If you ask a human to describe an image, they might do so in a thousand
different ways. Traditionally, image captioning models are trained to generate
a single &quot;best&quot; (most like a reference) image caption. Unfortunately, doing so
encourages captions that are &quot;informationally impoverished,&quot; and focus on only
a subset of the possible details, while ignoring other potentially useful
information in the scene. In this work, we introduce a simple, yet novel,
method: &quot;Image Captioning by Committee Consensus&quot; (IC3), designed to generate a
single caption that captures high-level details from several annotator
viewpoints. Humans rate captions produced by IC3 at least as helpful as
baseline SOTA models more than two thirds of the time, and IC3 can improve the
performance of SOTA automated recall systems by up to 84%, outperforming single
human-generated reference captions, and indicating significant improvements
over SOTA approaches for visual description. Code is available at
https://davidmchan.github.io/caption-by-committee/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1&quot;&gt;David M. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_A/0/1/0/all/0/1&quot;&gt;Austin Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayanarasimhan_S/0/1/0/all/0/1&quot;&gt;Sudheendra Vijayanarasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1&quot;&gt;David A. Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1&quot;&gt;John Canny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11084">
<title>Test-Time Distribution Normalization for Contrastively Learned Vision-language Models. (arXiv:2302.11084v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11084</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in the field of vision-language contrastive learning have made it
possible for many downstream applications to be carried out efficiently and
accurately by simply taking the dot product between image and text
representations. One of the most representative approaches proposed recently
known as CLIP has garnered widespread adoption due to its effectiveness. CLIP
is trained with an InfoNCE loss that takes into account both positive and
negative samples to help learn a much more robust representation space. This
paper reveals that the common downstream practice of taking a dot product is
only a zeroth-order approximation of the optimization goal, resulting in a loss
of information during test-time. Intuitively, since the model has been
optimized based on the InfoNCE loss, test-time procedures should also be in
alignment. The question lies in how one can retrieve any semblance of negative
samples information during inference in a computationally efficient way. To
this end, we propose Distribution Normalization (DN), where we approximate the
mean representation of a batch of test samples and use such a mean to represent
what would be analogous to negative samples in the InfoNCE loss. DN requires no
retraining or fine-tuning and can be effortlessly applied during inference.
Extensive experiments on a wide variety of downstream tasks exhibit a clear
advantage of DN over the dot product on top of other existing test-time
augmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Juntao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zabih_R/0/1/0/all/0/1&quot;&gt;Ramin Zabih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09200">
<title>Reduction of rain-induced errors for wind speed estimation on SAR observations using convolutional neural networks. (arXiv:2303.09200v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09200</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic Aperture Radar is known to be able to provide high-resolution
estimates of surface wind speed. These estimates usually rely on a Geophysical
Model Function (GMF) that has difficulties accounting for non-wind processes
such as rain events. Convolutional neural network, on the other hand, have the
capacity to use contextual information and have demonstrated their ability to
delimit rainfall areas. By carefully building a large dataset of SAR
observations from the Copernicus Sentinel-1 mission, collocated with both GMF
and atmospheric model wind speeds as well as rainfall estimates, we were able
to train a wind speed estimator with reduced errors under rain. Collocations
with in-situ wind speed measurements from buoys show a root mean square error
that is reduced by 27% (resp. 45%) under rainfall estimated at more than 1 mm/h
(resp. 3 mm/h). These results demonstrate the capacity of deep learning models
to correct rain-related errors in SAR products.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colin_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Colin&lt;/a&gt; (1, 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandeo_P/0/1/0/all/0/1&quot;&gt;Pierre Tandeo&lt;/a&gt; (1, 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peureux_C/0/1/0/all/0/1&quot;&gt;Charles Peureux&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husson_R/0/1/0/all/0/1&quot;&gt;Romain Husson&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt; (1, 3) ((1) IMT Atlantique, Lab-STICC, UMR CNRS 6285, F-29238, France, (2) Collecte Localisation Satellites, Brest, France, (3) Odyssey, Inria/IMT, France)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13121">
<title>DetOFA: Efficient Training of Once-for-All Networks for Object Detection Using Path Filter. (arXiv:2303.13121v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13121</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the challenge of training a large supernet for the object
detection task, using a relatively small amount of training data. Specifically,
we propose an efficient supernet-based neural architecture search (NAS) method
that uses search space pruning. The search space defined by the supernet is
pruned by removing candidate models that are predicted to perform poorly. To
effectively remove the candidates over a wide range of resource constraints, we
particularly design a performance predictor for supernet, called path filter,
which is conditioned by resource constraints and can accurately predict the
relative performance of the models that satisfy similar resource constraints.
Hence, supernet training is more focused on the best-performing candidates. Our
path filter handles prediction for paths with different resource budgets.
Compared to once-for-all, our proposed method reduces the computational cost of
the optimal network architecture by 30% and 63%, while yielding better
accuracy-floating point operations Pareto front (0.85 and 0.45 points of
improvement on average precision for Pascal VOC and COCO, respectively).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakuma_Y/0/1/0/all/0/1&quot;&gt;Yuiko Sakuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1&quot;&gt;Masato Ishii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1&quot;&gt;Takuya Narihira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12526">
<title>Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12526</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are powerful, but they require a lot of time and data to
train. We propose Patch Diffusion, a generic patch-wise training framework, to
significantly reduce the training time costs while improving data efficiency,
which thus helps democratize diffusion model training to broader users. At the
core of our innovations is a new conditional score function at the patch level,
where the patch location in the original image is included as additional
coordinate channels, while the patch size is randomized and diversified
throughout training to encode the cross-region dependency at multiple scales.
Sampling with our method is as easy as in the original diffusion model. Through
Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while
maintaining comparable or better generation quality. Patch Diffusion meanwhile
improves the performance of diffusion models trained on relatively small
datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve
outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on
CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on
ImageNet-256$\times$256. We share our code and pre-trained models at
https://github.com/Zhendong-Wang/Patch-Diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pengcheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weizhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01115">
<title>In-Context Learning Unlocked for Diffusion Models. (arXiv:2305.01115v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01115</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Prompt Diffusion, a framework for enabling in-context learning in
diffusion-based generative models. Given a pair of task-specific example
images, such as depth from/to image and scribble from/to image, and a text
guidance, our model automatically understands the underlying task and performs
the same task on a new query image following the text guidance. To achieve
this, we propose a vision-language prompt that can model a wide range of
vision-language tasks and a diffusion model that takes it as input. The
diffusion model is trained jointly over six different tasks using these
prompts. The resulting Prompt Diffusion model is the first diffusion-based
vision-language foundation model capable of in-context learning. It
demonstrates high-quality in-context generation on the trained tasks and
generalizes effectively to new, unseen vision tasks with their respective
prompts. Our model also shows compelling text-guided image editing results. Our
framework aims to facilitate research into in-context learning for computer
vision. We share our code and pre-trained models at
https://github.com/Zhendong-Wang/Prompt-Diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yadong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yelong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pengcheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weizhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04226">
<title>Design, Implementation and Evaluation of an External Pose-Tracking System for Underwater Cameras. (arXiv:2305.04226v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04226</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to advance underwater computer vision and robotics from lab
environments and clear water scenarios to the deep dark ocean or murky coastal
waters, representative benchmarks and realistic datasets with ground truth
information are required. In particular, determining the camera pose is
essential for many underwater robotic or photogrammetric applications and known
ground truth is mandatory to evaluate the performance of e.g., simultaneous
localization and mapping approaches in such extreme environments. This paper
presents the conception, calibration and implementation of an external
reference system for determining the underwater camera pose in real-time. The
approach, based on an HTC Vive tracking system in air, calculates the
underwater camera pose by fusing the poses of two controllers tracked above the
water surface of a tank. It is shown that the mean deviation of this approach
to an optical marker based reference in air is less than 3 mm and 0.3 deg.
Finally, the usability of the system for underwater applications is
demonstrated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkel_B/0/1/0/all/0/1&quot;&gt;Birger Winkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakath_D/0/1/0/all/0/1&quot;&gt;David Nakath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woelk_F/0/1/0/all/0/1&quot;&gt;Felix Woelk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1&quot;&gt;Kevin K&amp;#xf6;ser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14281">
<title>Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining. (arXiv:2305.14281v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14281</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in vision-and-language pretraining has investigated supervised
signals from object detection data to learn better, fine-grained multimodal
representations. In this work, we take a step further and explore how we can
tap into supervision from small-scale visual relation data. In particular, we
propose two pretraining approaches to contextualise visual entities in a
multimodal setup. With verbalised scene graphs, we transform visual relation
triplets into structured captions, and treat them as additional image
descriptions. With masked relation prediction, we further encourage relating
entities from image regions with visually masked contexts. When applied to
strong baselines pretrained on large amounts of Web data, zero-shot evaluations
on both coarse-grained and fine-grained tasks show the efficacy of our methods
in learning multimodal representations from weakly-supervised relations data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1&quot;&gt;Emanuele Bugliarello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1&quot;&gt;Aida Nematzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1&quot;&gt;Lisa Anne Hendricks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14381">
<title>Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14381</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Contrastive Representation learning aims to encode different
modalities into a semantically aligned shared space. This paradigm shows
remarkable generalization ability on numerous downstream tasks across various
modalities. However, the reliance on massive high-quality data pairs limits its
further development on more modalities. This paper proposes a novel
training-efficient method for learning MCR without paired data called
Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given
two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project
them to a new space and use the data from the overlapping modality B to
aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A,
B) and (B, C) are already aligned within each MCR, the connection learned by
overlapping modality can also be transferred to non-overlapping modality pair
(A, C). To unleash the potential of C-MCR, we further introduce a
semantic-enhanced inter- and intra-MCR connection method. We first enhance the
semantic consistency and completion of embeddings across different modalities
for more robust alignment. Then we utilize the inter-MCR alignment to establish
the connection, and employ the intra-MCR alignment to better maintain the
connection for inputs from non-overlapping modalities. To demonstrate the
effectiveness of C-MCR, we connect CLIP and CLAP via texts to derive
audio-visual representations, and integrate CLIP and ULIP via images for
3D-language representations. Remarkably, without using any paired data, C-MCR
for audio-visual achieves state-of-the-art performance on audio-image
retrieval, audio-visual source localization, and counterfactual audio-image
recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced
zero-shot 3D point cloud classification accuracy on ModelNet40.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xize Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiageng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1&quot;&gt;Aoxiong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16986">
<title>NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16986</link>
<description rdf:parseType="Literal">&lt;p&gt;Trained with an unprecedented scale of data, large language models (LLMs)
like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities
from model scaling. Such a trend underscored the potential of training LLMs
with unlimited language data, advancing the development of a universal embodied
agent. In this work, we introduce the NavGPT, a purely LLM-based
instruction-following navigation agent, to reveal the reasoning capability of
GPT models in complex embodied scenes by performing zero-shot sequential action
prediction for vision-and-language navigation (VLN). At each step, NavGPT takes
the textual descriptions of visual observations, navigation history, and future
explorable directions as inputs to reason the agent&apos;s current status, and makes
the decision to approach the target. Through comprehensive experiments, we
demonstrate NavGPT can explicitly perform high-level planning for navigation,
including decomposing instruction into sub-goal, integrating commonsense
knowledge relevant to navigation task resolution, identifying landmarks from
observed scenes, tracking navigation progress, and adapting to exceptions with
plan adjustment. Furthermore, we show that LLMs is capable of generating
high-quality navigational instructions from observations and actions along a
path, as well as drawing accurate top-down metric trajectory given the agent&apos;s
navigation history. Despite the performance of using NavGPT to zero-shot R2R
tasks still falling short of trained models, we suggest adapting multi-modality
inputs for LLMs to use as visual navigation agents and applying the explicit
reasoning of LLMs to benefit learning-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Gengze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17931">
<title>Monocular 2D Camera-based Proximity Monitoring for Human-Machine Collision Warning on Construction Sites. (arXiv:2305.17931v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17931</link>
<description rdf:parseType="Literal">&lt;p&gt;Accident of struck-by machines is one of the leading causes of casualties on
construction sites. Monitoring workers&apos; proximities to avoid human-machine
collisions has aroused great concern in construction safety management.
Existing methods are either too laborious and costly to apply extensively, or
lacking spatial perception for accurate monitoring. Therefore, this study
proposes a novel framework for proximity monitoring using only an ordinary 2D
camera to realize real-time human-machine collision warning, which is designed
to integrate a monocular 3D object detection model to perceive spatial
information from 2D images and a post-processing classification module to
identify the proximity as four predefined categories: Dangerous, Potentially
Dangerous, Concerned, and Safe. A virtual dataset containing 22000 images with
3D annotations is constructed and publicly released to facilitate the system
development and evaluation. Experimental results show that the trained 3D
object detection model achieves 75% loose AP within 20 meters. Besides, the
implemented system is real-time and camera carrier-independent, achieving an F1
of roughly 0.8 within 50 meters under specified settings for machines of
different sizes. This study preliminarily reveals the potential and feasibility
of proximity monitoring using only a 2D camera, providing a new promising and
economical way for early warning of human-machine collisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yuexiong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaowei Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04542">
<title>On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04542</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are generative models, which gradually add and remove noise
to learn the underlying distribution of training data for data generation. The
components of diffusion models have gained significant attention with many
design choices proposed. Existing reviews have primarily focused on
higher-level solutions, thereby covering less on the design fundamentals of
components. This study seeks to address this gap by providing a comprehensive
and coherent review on component-wise design choices in diffusion models.
Specifically, we organize this review according to their three key components,
namely the forward process, the reverse process, and the sampling procedure.
This allows us to provide a fine-grained perspective of diffusion models,
benefiting future studies in the analysis of individual components, the
applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koulieris_G/0/1/0/all/0/1&quot;&gt;George Alex Koulieris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P. H. Shum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06599">
<title>Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing. (arXiv:2306.06599v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06599</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing regression models tend to fall short in both accuracy and
uncertainty estimation when the label distribution is imbalanced. In this
paper, we propose a probabilistic deep learning model, dubbed variational
imbalanced regression (VIR), which not only performs well in imbalanced
regression but naturally produces reasonable uncertainty estimation as a
byproduct. Different from typical variational autoencoders assuming I.I.D.
representations (a data point&apos;s representation is not directly affected by
other data points), our VIR borrows data with similar regression labels to
compute the latent representation&apos;s variational distribution; furthermore,
different from deterministic regression models producing point estimates, VIR
predicts the entire normal-inverse-gamma distributions and modulates the
associated conjugate distributions to impose probabilistic reweighting on the
imbalanced data, thereby providing better uncertainty estimation. Experiments
in several real-world datasets show that our VIR can outperform
state-of-the-art imbalanced regression models in terms of both accuracy and
uncertainty estimation. Code will soon be available at
https://github.com/Wang-ML-Lab/variational-imbalanced-regression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08141">
<title>ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08141</link>
<description rdf:parseType="Literal">&lt;p&gt;As generative AI becomes more prevalent, it is important to study how human
users interact with such models. In this work, we investigate how people use
text-to-image models to generate desired target images. To study this
interaction, we created ArtWhisperer, an online game where users are given a
target image and are tasked with iteratively finding a prompt that creates a
similar-looking image as the target. Through this game, we recorded over 50,000
human-AI interactions; each interaction corresponds to one text prompt created
by a user and the corresponding generated image. The majority of these are
repeated interactions where a user iterates to find the best prompt for their
target image, making this a unique sequential dataset for studying human-AI
collaborations. In an initial analysis of this dataset, we identify several
characteristics of prompt interactions and user strategies. People submit
diverse prompts and are able to discover a variety of text descriptions that
generate similar images. Interestingly, prompt diversity does not decrease as
users find better prompts. We further propose a new metric to quantify the
steerability of AI using our dataset. We define steerability as the expected
number of interactions required to adequately complete a task. We estimate this
value by fitting a Markov chain for each target task and calculating the
expected time to reach an adequate score in the Markov chain. We quantify and
compare AI steerability across different types of target images and two
different models, finding that images of cities and natural world images are
more steerable than artistic and fantasy images. These findings provide
insights into human-AI interaction behavior, present a concrete method of
assessing AI steerability, and demonstrate the general utility of the
ArtWhisperer dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1&quot;&gt;Kailas Vodrahalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13856">
<title>Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification. (arXiv:2306.13856v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13856</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel language-driven ordering alignment method for ordinal
classification. The labels in ordinal classification contain additional
ordering relations, making them prone to overfitting when relying solely on
training data. Recent developments in pre-trained vision-language models
inspire us to leverage the rich ordinal priors in human language by converting
the original task into a vision-language alignment task. Consequently, we
propose L2RCLIP, which fully utilizes the language priors from two
perspectives. First, we introduce a complementary prompt tuning technique
called RankFormer, designed to enhance the ordering relation of original rank
prompts. It employs token-level attention with residual-style prompt blending
in the word embedding space. Second, to further incorporate language priors, we
revisit the approximate bound optimization of vanilla cross-entropy loss and
restructure it within the cross-modal embedding space. Consequently, we propose
a cross-modal ordinal pairwise loss to refine the CLIP feature space, where
texts and images maintain both semantic alignment and ordering alignment.
Extensive experiments on three ordinal classification tasks, including facial
age estimation, historical color image (HCI) classification, and aesthetic
assessment demonstrate its promising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peipei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chunshui Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03512">
<title>Transfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03512</link>
<description rdf:parseType="Literal">&lt;p&gt;When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sech_G/0/1/0/all/0/1&quot;&gt;Gregory Sech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleni_P/0/1/0/all/0/1&quot;&gt;Paolo Soleni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaart_W/0/1/0/all/0/1&quot;&gt;Wouter B. Verschoof-van der Vaart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokalj_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;iga Kokalj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traviglia_A/0/1/0/all/0/1&quot;&gt;Arianna Traviglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05033">
<title>Towards Anytime Optical Flow Estimation with Event Cameras. (arXiv:2307.05033v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05033</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow estimation is a fundamental task in the field of autonomous
driving. Event cameras are capable of responding to log-brightness changes in
microseconds. Its characteristic of producing responses only to the changing
region is particularly suitable for optical flow estimation. In contrast to the
super low-latency response speed of event cameras, existing datasets collected
via event cameras, however, only provide limited frame rate optical flow ground
truth, (e.g., at 10Hz), greatly restricting the potential of event-driven
optical flow. To address this challenge, we put forward a high-frame-rate,
low-latency event representation Unified Voxel Grid, sequentially fed into the
network bin by bin. We then propose EVA-Flow, an EVent-based Anytime Flow
estimation network to produce high-frame-rate event optical flow with only
low-frame-rate optical flow ground truth for supervision. The key component of
our EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module,
which predicts temporally dense optical flow and enhances the accuracy via
spatial-temporal motion refinement. The time-dense feature warping utilized in
the SMR module provides implicit supervision for the intermediate optical flow.
Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for the
unsupervised evaluation of intermediate optical flow in the absence of ground
truth. This is, to the best of our knowledge, the first work focusing on
anytime optical flow estimation via event cameras. A comprehensive variety of
experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow
achieves competitive performance, super-low-latency (5ms), fastest inference
(9.2ms), time-dense motion estimation (200Hz), and strong generalization. Our
code will be available at https://github.com/Yaozhuwa/EVA-Flow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yaozu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yining Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06528">
<title>Optimised Least Squares Approach for Accurate Polygon and Ellipse Fitting. (arXiv:2307.06528v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06528</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a generalised least squares based method for fitting
polygons and ellipses to data points. The method is based on a trigonometric
fitness function that approximates a unit shape accurately, making it
applicable to various geometric shapes with minimal fitting parameters.
Furthermore, the proposed method does not require any constraints and can
handle incomplete data. The method is validated on synthetic and real-world
data sets and compared with the existing methods in the literature for polygon
and ellipse fitting. The test results show that the method achieves high
accuracy and outperforms the referenced methods in terms of root-mean-square
error, especially for noise-free data. The proposed method is a powerful tool
for shape fitting in computer vision and geometry processing applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1&quot;&gt;Yiming Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07362">
<title>A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07362</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers&apos;
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingquan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qingqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qianqian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09020">
<title>Face-PAST: Facial Pose Awareness and Style Transfer Networks. (arXiv:2307.09020v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09020</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial style transfer has been quite popular among researchers due to the
rise of emerging technologies such as eXtended Reality (XR), Metaverse, and
Non-Fungible Tokens (NFTs). Furthermore, StyleGAN methods along with
transfer-learning strategies have reduced the problem of limited data to some
extent. However, most of the StyleGAN methods overfit the styles while adding
artifacts to facial images. In this paper, we propose a facial pose awareness
and style transfer (Face-PAST) network that preserves facial details and
structures while generating high-quality stylized images. Dual StyleGAN
inspires our work, but in contrast, our work uses a pre-trained style
generation network in an external style pass with a residual modulation block
instead of a transform coding block. Furthermore, we use the gated mapping unit
and facial structure, identity, and segmentation losses to preserve the facial
structure and details. This enables us to train the network with a very limited
amount of data while generating high-quality stylized images. Our training
process adapts curriculum learning strategy to perform efficient and flexible
style mixing in the generative space. We perform extensive experiments to show
the superiority of Face-PAST in comparison to existing state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1&quot;&gt;Sunder Ali Khowaja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nkenyereye_L/0/1/0/all/0/1&quot;&gt;Lewis Nkenyereye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jiseok Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ik Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dev_K/0/1/0/all/0/1&quot;&gt;Kapal Dev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12964">
<title>Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment. (arXiv:2307.12964v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12964</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-video retrieval systems have recently made significant progress by
utilizing pre-trained models trained on large-scale image-text pairs. However,
most of the latest methods primarily focus on the video modality while
disregarding the audio signal for this task. Nevertheless, a recent advancement
by ECLIPSE has improved long-range text-to-video retrieval by developing an
audiovisual video representation. Nonetheless, the objective of the
text-to-video retrieval task is to capture the complementary audio and video
information that is pertinent to the text query rather than simply achieving
better audio and video alignment. To address this issue, we introduce TEFAL, a
TExt-conditioned Feature ALignment method that produces both audio and video
representations conditioned on the text query. Instead of using only an
audiovisual attention block, which could suppress the audio information
relevant to the text query, our approach employs two independent cross-modal
attention blocks that enable the text to attend to the audio and video
representations separately. Our proposed method&apos;s efficacy is demonstrated on
four benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, and
Charades, and achieves better than state-of-the-art performance consistently
across the four datasets. This is attributed to the additional
text-query-conditioned audio representation and the complementary information
it adds to the text-query-conditioned video representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahimi_S/0/1/0/all/0/1&quot;&gt;Sarah Ibrahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaohang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pichao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Amanmeet Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanan_A/0/1/0/all/0/1&quot;&gt;Ashutosh Sanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_M/0/1/0/all/0/1&quot;&gt;Mohamed Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02463">
<title>Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&amp;3D Medical Data. (arXiv:2308.02463v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02463</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM.We consider the construction of foundational models from
the perspectives of dataset construction, model design, and thorough
evaluation. Our contribution can be concluded as follows: (i), we construct a
large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D
medical scans with high-quality text descriptions or reports across various
data formats, modalities, and tasks, covering over 5000 distinct diseases. To
the best of our knowledge, this is the first large-scale, high-quality, medical
visual-language dataset, with both 2D and 3D scans; (ii ), we propose an
architecture that enables visually conditioned generative pre-training, i.e.,
allowing for integration of text input with 2D or 3D medical scans, and
generate responses for diverse radiologic tasks. The model was initially
pre-trained on MedMD and subsequently fine-tuned on the domain-specific
dataset, which is a radiologic cleaned version of MedMD, containing 3M
radiologic visual-language pairs, termed as RadMD; (iii), we propose a new
evaluation benchmark, RadBench, that comprises five tasks, including modality
recognition, disease diagnosis, visual question answering, report generation
and rationale diagnosis, aiming to comprehensively assess the capability of
foundation models in handling practical clinical problems. We conduct both
automatic and human evaluation on RadBench, in both cases, RadFM significantly
outperforms existing multi-modal foundation models. The codes, data, and model
checkpoint will all be made publicly available to promote further research and
development in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10491">
<title>DCPT: Darkness Clue-Prompted Tracking in Nighttime UAVs. (arXiv:2309.10491v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10491</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing nighttime unmanned aerial vehicle (UAV) trackers follow an
&quot;Enhance-then-Track&quot; architecture - first using a light enhancer to brighten
the nighttime video, then employing a daytime tracker to locate the object.
This separate enhancement and tracking fails to build an end-to-end trainable
vision system. To address this, we propose a novel architecture called Darkness
Clue-Prompted Tracking (DCPT) that achieves robust UAV tracking at night by
efficiently learning to generate darkness clue prompts. Without a separate
enhancer, DCPT directly encodes anti-dark capabilities into prompts using a
darkness clue prompter (DCP). Specifically, DCP iteratively learns emphasizing
and undermining projections for darkness clues. It then injects these learned
visual prompts into a daytime tracker with fixed parameters across transformer
layers. Moreover, a gated feature aggregation mechanism enables adaptive fusion
between prompts and between prompts and the base model. Extensive experiments
show state-of-the-art performance for DCPT on multiple dark scenario
benchmarks. The unified end-to-end learning of enhancement and tracking in DCPT
enables a more trainable system. The darkness clue prompting efficiently
injects anti-dark knowledge without extra modules. Code is available at
https://github.com/bearyi26/DCPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiawen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huayi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shihao Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16128">
<title>Joint Correcting and Refinement for Balanced Low-Light Image Enhancement. (arXiv:2309.16128v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16128</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement tasks demand an appropriate balance among
brightness, color, and illumination. While existing methods often focus on one
aspect of the image without considering how to pay attention to this balance,
which will cause problems of color distortion and overexposure etc. This
seriously affects both human visual perception and the performance of
high-level visual models. In this work, a novel synergistic structure is
proposed which can balance brightness, color, and illumination more
effectively. Specifically, the proposed method, so-called Joint Correcting and
Refinement Network (JCRNet), which mainly consists of three stages to balance
brightness, color, and illumination of enhancement. Stage 1: we utilize a basic
encoder-decoder and local supervision mechanism to extract local information
and more comprehensive details for enhancement. Stage 2: cross-stage feature
transmission and spatial feature transformation further facilitate color
correction and feature refinement. Stage 3: we employ a dynamic illumination
adjustment approach to embed residuals between predicted and ground truth
images into the model, adaptively adjusting illumination balance. Extensive
experiments demonstrate that the proposed method exhibits comprehensive
performance advantages over 21 state-of-the-art methods on 9 benchmark
datasets. Furthermore, a more persuasive experiment has been conducted to
validate our approach the effectiveness in downstream visual tasks (e.g.,
saliency detection). Compared to several enhancement models, the proposed
method effectively improves the segmentation results and quantitative metrics
of saliency detection. The source code will be available at
https://github.com/woshiyll/JCRNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Nana Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yahong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01506">
<title>Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code. (arXiv:2310.01506v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01506</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-guided diffusion models have revolutionized image generation and
editing, offering exceptional realism and diversity. Specifically, in the
context of diffusion-based editing, where a source image is edited according to
a target prompt, the process commences by acquiring a noisy latent vector
corresponding to the source image via the diffusion model. This vector is
subsequently fed into separate source and target diffusion branches for
editing. The accuracy of this inversion process significantly impacts the final
editing outcome, influencing both essential content preservation of the source
image and edit fidelity according to the target prompt. Prior inversion
techniques aimed at finding a unified solution in both the source and target
diffusion branches. However, our theoretical and empirical analyses reveal that
disentangling these branches leads to a distinct separation of responsibilities
for preserving essential content and ensuring edit fidelity. Building on this
insight, we introduce &quot;Direct Inversion,&quot; a novel technique achieving optimal
performance of both branches with just three lines of code. To assess image
editing performance, we present PIE-Bench, an editing benchmark with 700 images
showcasing diverse scenes and editing types, accompanied by versatile
annotations and comprehensive evaluation metrics. Compared to state-of-the-art
optimization-based inversion techniques, our solution not only yields superior
performance across 8 editing methods but also achieves nearly an order of
speed-up.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_X/0/1/0/all/0/1&quot;&gt;Xuan Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaoteng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01680">
<title>Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation. (arXiv:2310.01680v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01680</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretraining CNN models (i.e., UNet) through self-supervision has become a
powerful approach to facilitate medical image segmentation under low annotation
regimes. Recent contrastive learning methods encourage similar global
representations when the same image undergoes different transformations, or
enforce invariance across different image/patch features that are intrinsically
correlated. However, CNN-extracted global and local features are limited in
capturing long-range spatial dependencies that are essential in biological
anatomy. To this end, we present a keypoint-augmented fusion layer that
extracts representations preserving both short- and long-range self-attention.
In particular, we augment the CNN feature map at multiple scales by
incorporating an additional input that learns long-range spatial self-attention
among localized keypoint features. Further, we introduce both global and local
self-supervised pretraining for the framework. At the global scale, we obtain
global representations from both the bottleneck of the UNet, and by aggregating
multiscale keypoint features. These global features are subsequently
regularized through image-level contrastive objectives. At the local scale, we
define a distance-based criterion to first establish correspondences among
keypoints and encourage similarity between their features. Through extensive
experiments on both MRI and CT segmentation tasks, we demonstrate the
architectural advantages of our proposed method in comparison to both CNN and
Transformer-based UNets, when all architectures are trained with randomly
initialized weights. With our proposed pretraining strategy, our method further
outperforms existing SSL methods by producing more robust self-attention and
achieving state-of-the-art segmentation results. The code is available at
https://github.com/zshyang/kaf.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhangsihao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1&quot;&gt;Kaize Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1&quot;&gt;Guido Gerig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yalin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01799">
<title>SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models. (arXiv:2310.01799v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01799</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently gained popularity for accelerated MRI
reconstruction due to their high sample quality. They can effectively serve as
rich data priors while incorporating the forward model flexibly at inference
time, and they have been shown to be more robust than unrolled methods under
distribution shifts. However, diffusion models require careful tuning of
inference hyperparameters on a validation set and are still sensitive to
distribution shifts during testing. To address these challenges, we introduce
SURE-based MRI Reconstruction with Diffusion models (SMRD), a method that
performs test-time hyperparameter tuning to enhance robustness during testing.
SMRD uses Stein&apos;s Unbiased Risk Estimator (SURE) to estimate the mean squared
error of the reconstruction during testing. SURE is then used to automatically
tune the inference hyperparameters and to set an early stopping criterion
without the need for validation tuning. To the best of our knowledge, SMRD is
the first to incorporate SURE into the sampling stage of diffusion models for
automatic hyperparameter selection. SMRD outperforms diffusion model baselines
on various measurement noise levels, acceleration factors, and anatomies,
achieving a PSNR improvement of up to 6 dB under measurement noise. The code is
publicly available at https://github.com/NVlabs/SMRD .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozturkler_B/0/1/0/all/0/1&quot;&gt;Batu Ozturkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eckart_B/0/1/0/all/0/1&quot;&gt;Benjamin Eckart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mardani_M/0/1/0/all/0/1&quot;&gt;Morteza Mardani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03559">
<title>MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images. (arXiv:2310.03559v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03559</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an innovative methodology for producing high-quality 3D
lung CT images guided by textual information. While diffusion-based generative
models are increasingly used in medical imaging, current state-of-the-art
approaches are limited to low-resolution outputs and underutilize radiology
reports&apos; abundant information. The radiology reports can enhance the generation
process by providing additional guidance and offering fine-grained control over
the synthesis of images. Nevertheless, expanding text-guided generation to
high-resolution 3D images poses significant memory and anatomical
detail-preserving challenges. Addressing the memory issue, we introduce a
hierarchical scheme that uses a modified UNet architecture. We start by
synthesizing low-resolution images conditioned on the text, serving as a
foundation for subsequent generators for complete volumetric data. To ensure
the anatomical plausibility of the generated samples, we provide further
guidance by generating vascular, airway, and lobular segmentation masks in
conjunction with the CT images. The model demonstrates the capability to use
textual input and segmentation tasks to generate synthesized images. The
results of comparative assessments indicate that our approach exhibits superior
performance compared to the most advanced models based on GAN and diffusion
techniques, especially in accurately retaining crucial anatomical features such
as fissure lines, airways, and vascular structures. This innovation introduces
novel possibilities. This study focuses on two main objectives: (1) the
development of a method for creating images based on textual prompts and
anatomical components, and (2) the capability to generate new images
conditioning on anatomical elements. The advancements in image generation can
be applied to enhance numerous downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Li Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Visweswaran_S/0/1/0/all/0/1&quot;&gt;Shyam Visweswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05483">
<title>Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with Sparse Views. (arXiv:2310.05483v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05483</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for 3D scene and object
reconstruction from sparse multi-view images. Different from previous methods
that leverage extra information such as depth or generalizable features across
scenes, our approach leverages the scene properties embedded in the multi-view
inputs to create precise pseudo-labels for optimization without any prior
training. Specifically, we introduce a geometry-guided approach that improves
surface reconstruction accuracy from sparse views by leveraging spherical
harmonics to predict the novel radiance while holistically considering all
color observations for a point in the scene. Also, our pipeline exploits proxy
geometry and correctly handles the occlusion in generating the pseudo-labels of
radiance, which previous image-warping methods fail to avoid. Our method,
dubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender
datasets without requiring prior training, demonstrating its effectiveness in
addressing the problem of sparse view reconstruction. Our pipeline is flexible
and can be integrated into other implicit neural reconstruction methods for
sparse views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiawei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuming Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06196">
<title>DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised Transformers for Weakly Supervised Object Localization. (arXiv:2310.06196v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06196</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised vision transformers (SSTs) have shown great potential to
yield rich localization maps that highlight different objects in an image.
However, these maps remain class-agnostic since the model is unsupervised. They
often tend to decompose the image into multiple maps containing different
objects while being unable to distinguish the object of interest from
background noise objects. In this paper, Discriminative Pseudo-label Sampling
(DiPS) is introduced to leverage these class-agnostic maps for
weakly-supervised object localization (WSOL), where only image-class labels are
available. Given multiple attention maps, DiPS relies on a pre-trained
classifier to identify the most discriminative regions of each attention map.
This ensures that the selected ROIs cover the correct image object while
discarding the background ones, and, as such, provides a rich pool of diverse
and discriminative proposals to cover different parts of the object.
Subsequently, these proposals are used as pseudo-labels to train our new
transformer-based WSOL model designed to perform classification and
localization tasks. Unlike standard WSOL methods, DiPS optimizes performance in
both tasks by using a transformer encoder and a dedicated output head for each
task, each trained using dedicated loss functions. To avoid overfitting a
single proposal and promote better object coverage, a single proposal is
randomly selected among the top ones for a training image at each training
step. Experimental results on the challenging CUB, ILSVRC, OpenImages, and
TelDrone datasets indicate that our architecture, in combination with our
transformer-based proposals, can yield better localization performance than
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murtaza_S/0/1/0/all/0/1&quot;&gt;Shakeeb Murtaza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1&quot;&gt;Soufiane Belharbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarraf_A/0/1/0/all/0/1&quot;&gt;Aydin Sarraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08854">
<title>Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08854</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern detection transformers (DETRs) use a set of object queries to predict
a list of bounding boxes, sort them by their classification confidence scores,
and select the top-ranked predictions as the final detection results for the
given input image. A highly performant object detector requires accurate
ranking for the bounding box predictions. For DETR-based detectors, the
top-ranked bounding boxes suffer from less accurate localization quality due to
the misalignment between classification scores and localization accuracy, thus
impeding the construction of high-quality detectors. In this work, we introduce
a simple and highly performant DETR-based object detector by proposing a series
of rank-oriented designs, combinedly called Rank-DETR. Our key contributions
include: (i) a rank-oriented architecture design that can prompt positive
predictions and suppress the negative ones to ensure lower false positive
rates, as well as (ii) a rank-oriented loss function and matching cost design
that prioritizes predictions of more accurate localization accuracy during
ranking to boost the AP under high IoU thresholds. We apply our method to
improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong
COCO object detection results when using different backbones such as
ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our
approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yifan Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weicong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yiduo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yukang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10404">
<title>LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation. (arXiv:2310.10404v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10404</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-Supervised Scene Graph Generation (WSSGG) research has recently
emerged as an alternative to the fully-supervised approach that heavily relies
on costly annotations. In this regard, studies on WSSGG have utilized image
captions to obtain unlocalized triplets while primarily focusing on grounding
the unlocalized triplets over image regions. However, they have overlooked the
two issues involved in the triplet formation process from the captions: 1)
Semantic over-simplification issue arises when extracting triplets from
captions, where fine-grained predicates in captions are undesirably converted
into coarse-grained predicates, resulting in a long-tailed predicate
distribution, and 2) Low-density scene graph issue arises when aligning the
triplets in the caption with entity/predicate classes of interest, where many
triplets are discarded and not used in training, leading to insufficient
supervision. To tackle the two issues, we propose a new approach, i.e., Large
Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two
issues by leveraging the LLM&apos;s in-depth understanding of language and reasoning
ability during the extraction of triplets from captions and alignment of
entity/predicate classes with target data. To further engage the LLM in these
processes, we adopt the idea of Chain-of-Thought and the in-context few-shot
learning strategy. To validate the effectiveness of LLM4SGG, we conduct
extensive experiments on Visual Genome and GQA datasets, showing significant
improvements in both Recall@K and mean Recall@K compared to the
state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is
data-efficient, enabling effective model training with a small amount of
training images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kibum Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1&quot;&gt;Jaehyeong Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+In_Y/0/1/0/all/0/1&quot;&gt;Yeonjun In&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10765">
<title>BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10765</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid progress has been made in instruction-learning for image editing with
natural-language instruction, as exemplified by InstructPix2Pix. In
biomedicine, such methods can be applied to counterfactual image generation,
which helps differentiate causal structure from spurious correlation and
facilitate robust image interpretation for disease progression modeling.
However, generic image-editing models are ill-suited for the biomedical domain,
and counterfactual biomedical image generation is largely underexplored. In
this paper, we present BiomedJourney, a novel method for counterfactual
biomedical image generation by instruction-learning from multimodal patient
journeys. Given a patient with two biomedical images taken at different time
points, we use GPT-4 to process the corresponding imaging reports and generate
a natural language description of disease progression. The resulting triples
(prior image, progression description, new image) are then used to train a
latent diffusion model for counterfactual biomedical image generation. Given
the relative scarcity of image time series data, we introduce a two-stage
curriculum that first pretrains the denoising network using the much more
abundant single image-report pairs (with dummy prior image), and then continues
training using the counterfactual triples. Experiments using the standard
MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive
battery of tests on counterfactual medical image generation, BiomedJourney
substantially outperforms prior state-of-the-art methods in instruction image
editing and medical image generation such as InstructPix2Pix and RoentGen. To
facilitate future study in counterfactual medical generation, we plan to
release our instruction-learning code and pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1&quot;&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1&quot;&gt;Hoifung Poon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11040">
<title>Co-Learning Semantic-aware Unsupervised Segmentation for Pathological Image Registration. (arXiv:2310.11040v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11040</link>
<description rdf:parseType="Literal">&lt;p&gt;The registration of pathological images plays an important role in medical
applications. Despite its significance, most researchers in this field
primarily focus on the registration of normal tissue into normal tissue. The
negative impact of focal tissue, such as the loss of spatial correspondence
information and the abnormal distortion of tissue, are rarely considered. In
this paper, we propose GIRNet, a novel unsupervised approach for pathological
image registration by incorporating segmentation and inpainting through the
principles of Generation, Inpainting, and Registration (GIR). The registration,
segmentation, and inpainting modules are trained simultaneously in a
co-learning manner so that the segmentation of the focal area and the
registration of inpainted pairs can improve collaboratively. Overall, the
registration of pathological images is achieved in a completely unsupervised
learning framework. Experimental results on multiple datasets, including
Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of
our proposed method. Our results show that our method can accurately achieve
the registration of pathological images and identify lesions even in
challenging imaging modalities. Our unsupervised approach offers a promising
solution for the efficient and cost-effective registration of pathological
images. Our code is available at
https://github.com/brain-intelligence-lab/GIRNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shi Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11092">
<title>DORec: Decomposed Object Reconstruction Utilizing 2D Self-Supervised Features. (arXiv:2310.11092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11092</link>
<description rdf:parseType="Literal">&lt;p&gt;Decomposing a target object from a complex background while reconstructing is
challenging. Most approaches acquire the perception for object instances
through the use of manual labels, but the annotation procedure is costly. The
recent advancements in 2D self-supervised learning have brought new prospects
to object-aware representation, yet it remains unclear how to leverage such
noisy 2D features for clean decomposition. In this paper, we propose a
Decomposed Object Reconstruction (DORec) network based on neural implicit
representations. Our key idea is to transfer 2D self-supervised features into
masks of two levels of granularity to supervise the decomposition, including a
binary mask to indicate the foreground regions and a K-cluster mask to indicate
the semantically similar regions. These two masks are complementary to each
other and lead to robust decomposition. Experimental results show the
superiority of DORec in segmenting and reconstructing the foreground object on
various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Sihui Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yiyi Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11595">
<title>WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks. (arXiv:2310.11595v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11595</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the popularity of Artificial Intelligence (AI) technology, numerous
backdoor attacks are designed by adversaries to mislead deep neural network
predictions by manipulating training samples and training processes. Although
backdoor attacks are effective in various real scenarios, they still suffer
from the problems of both low fidelity of poisoned samples and non-negligible
transfer in latent space, which make them easily detectable by existing
backdoor detection algorithms. To overcome the weakness, this paper proposes a
novel frequency-based backdoor attack method named WaveAttack, which obtains
image high-frequency features through Discrete Wavelet Transform (DWT) to
generate backdoor triggers. Furthermore, we introduce an asymmetric frequency
obfuscation method, which can add an adaptive residual in the training and
inference stage to improve the impact of triggers and further enhance the
effectiveness of WaveAttack. Comprehensive experimental results show that
WaveAttack not only achieves higher stealthiness and effectiveness, but also
outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of
images by up to 28.27\% improvement in PSNR, 1.61\% improvement in SSIM, and
70.59\% reduction in IS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zhihao Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xian Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingsong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12060">
<title>Robust Class-Conditional Distribution Alignment for Partial Domain Adaptation. (arXiv:2310.12060v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12060</link>
<description rdf:parseType="Literal">&lt;p&gt;Unwanted samples from private source categories in the learning objective of
a partial domain adaptation setup can lead to negative transfer and reduce
classification performance. Existing methods, such as re-weighting or
aggregating target predictions, are vulnerable to this issue, especially during
initial training stages, and do not adequately address class-level feature
alignment. Our proposed approach seeks to overcome these limitations by delving
deeper than just the first-order moments to derive distinct and compact
categorical distributions. We employ objectives that optimize the intra and
inter-class distributions in a domain-invariant fashion and design a robust
pseudo-labeling for efficient target supervision. Our approach incorporates a
complement entropy objective module to reduce classification uncertainty and
flatten incorrect category predictions. The experimental findings and ablation
analysis of the proposed modules demonstrate the superior performance of our
proposed model compared to benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhuri_S/0/1/0/all/0/1&quot;&gt;Sandipan Choudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1&quot;&gt;Arunabha Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10541">
<title>Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2310.10541</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a large and state-of-the-art machine learning model typically
necessitates the use of large-scale datasets, which, in turn, makes the
training and parameter-tuning process expensive and time-consuming. Some
researchers opt to distil information from real-world datasets into tiny and
compact synthetic datasets while maintaining their ability to train a
well-performing model, hence proposing a data-efficient method known as Dataset
Distillation (DD). Despite recent progress in this field, existing methods
still underperform and cannot effectively replace large datasets. In this
paper, unlike previous methods that focus solely on improving the efficacy of
student distillation, we are the first to recognize the important interplay
between expert and student. We argue the significant impact of expert
smoothness when employing more potent expert trajectories in subsequent dataset
distillation. Based on this, we introduce the integration of clipping loss and
gradient penalty to regulate the rate of parameter changes in expert
trajectories. Furthermore, in response to the sensitivity exhibited towards
randomly initialized variables during distillation, we propose representative
initialization for synthetic dataset and balanced inner-loop loss. Finally, we
present two enhancement strategies, namely intermediate matching loss and
weight perturbation, to mitigate the potential occurrence of cumulative errors.
We conduct extensive experiments on datasets of different scales, sizes, and
resolutions. The results demonstrate that the proposed method significantly
outperforms prior methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiyuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenzhuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>