<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1908.04628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1910.09143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.05421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.14432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.14219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16375" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.07720">
<title>Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning. (arXiv:2310.07720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07720</link>
<description rdf:parseType="Literal">&lt;p&gt;Activation functions (AFs) are crucial components of deep neural networks
(DNNs), having a significant impact on their performance. An activation
function in a DNN is typically a smooth, nonlinear function that transforms an
input signal into an output signal for the subsequent layer. In this paper, we
propose the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function
designed to combine the strengths of both the Tanh and Leaky ReLU (LReLU)
activation functions. PLTanh is differentiable at all points and addresses the
&apos;dying ReLU&apos; problem by ensuring a non-zero gradient for negative inputs,
consistent with the behavior of LReLU. By integrating the unique advantages of
these two diverse activation functions, PLTanh facilitates the learning of more
intricate nonlinear relationships within the network. This paper presents an
empirical evaluation of PLTanh against established activation functions, namely
ReLU, LReLU, and ALReLU utilizing five diverse datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastromichalakis_S/0/1/0/all/0/1&quot;&gt;Stamatis Mastromichalakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07724">
<title>Visual Forecasting as a Mid-level Representation for Avoidance. (arXiv:2310.07724v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07724</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of navigation in environments with dynamic objects continues to
be a central issue in the study of autonomous agents. While predictive methods
hold promise, their reliance on precise state information makes them less
practical for real-world implementation. This study presents visual forecasting
as an innovative alternative. By introducing intuitive visual cues, this
approach projects the future trajectories of dynamic objects to improve agent
perception and enable anticipatory actions. Our research explores two distinct
strategies for conveying predictive information through visual forecasting: (1)
sequences of bounding boxes, and (2) augmented paths. To validate the proposed
visual forecasting strategies, we initiate evaluations in simulated
environments using the Unity engine and then extend these evaluations to
real-world scenarios to assess both practicality and effectiveness. The results
confirm the viability of visual forecasting as a promising solution for
navigation and obstacle avoidance in dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hsuan-Kung Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1&quot;&gt;Tsung-Chih Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting-Ru Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chun-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jou-Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07725">
<title>Extreme Image Transformations Facilitate Robust Latent Object Representations. (arXiv:2310.07725v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07725</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks can affect the object recognition capabilities of
machines in wild. These can often result from spurious correlations between
input and class labels, and are prone to memorization in large networks. While
networks are expected to do automated feature selection, it is not effective at
the scale of the object. Humans, however, are able to select the minimum set of
features required to form a robust representation of an object. In this work,
we show that finetuning any pretrained off-the-shelf network with Extreme Image
Transformations (EIT) not only helps in learning a robust latent
representation, it also improves the performance of these networks against
common adversarial attacks of various intensities. Our EIT trained networks
show strong activations in the object regions even when tested with more
intense noise, showing promising generalizations across different kinds of
adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1&quot;&gt;Girik Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowder_D/0/1/0/all/0/1&quot;&gt;Dakarai Crowder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mingolla_E/0/1/0/all/0/1&quot;&gt;Ennio Mingolla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07736">
<title>Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2310.07736</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models and specialized table embedding models have recently
demonstrated strong performance on many tasks over tabular data. Researchers
and practitioners are keen to leverage these models in many new application
contexts; but limited understanding of the strengths and weaknesses of these
models, and the table representations they generate, makes the process of
finding a suitable model for a given task reliant on trial and error. There is
an urgent need to gain a comprehensive understanding of these models to
minimize inefficiency and failures in downstream usage.
&lt;/p&gt;
&lt;p&gt;To address this need, we propose Observatory, a formal framework to
systematically analyze embedding representations of relational tables.
Motivated both by invariants of the relational data model and by statistical
considerations regarding data distributions, we define eight primitive
properties, and corresponding measures to quantitatively characterize table
embeddings for these properties. Based on these properties, we define an
extensible framework to evaluate language and table embedding models. We
collect and synthesize a suite of datasets and use Observatory to analyze seven
such models. Our analysis provides insights into the strengths and weaknesses
of learned representations over tables. We find, for example, that some models
are sensitive to table structure such as column order, that functional
dependencies are rarely reflected in embeddings, and that specialized table
embedding models have relatively lower sample fidelity. Such insights help
researchers and practitioners better anticipate model behaviors and select
appropriate models for their downstream tasks, while guiding researchers in the
development of new models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1&quot;&gt;Tianji Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hulsebos_M/0/1/0/all/0/1&quot;&gt;Madelon Hulsebos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhenjie Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1&quot;&gt;Paul Groth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagadish_H/0/1/0/all/0/1&quot;&gt;H. V. Jagadish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07740">
<title>Spiral-Elliptical automated galaxy morphology classification from telescope images. (arXiv:2310.07740v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2310.07740</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of galaxy morphologies is an important step in the
investigation of theories of hierarchical structure formation. While human
expert visual classification remains quite effective and accurate, it cannot
keep up with the massive influx of data from emerging sky surveys. A variety of
approaches have been proposed to classify large numbers of galaxies; these
approaches include crowdsourced visual classification, and automated and
computational methods, such as machine learning methods based on designed
morphology statistics and deep learning. In this work, we develop two novel
galaxy morphology statistics, descent average and descent variance, which can
be efficiently extracted from telescope galaxy images. We further propose
simplified versions of the existing image statistics concentration, asymmetry,
and clumpiness, which have been widely used in the literature of galaxy
morphologies. We utilize the galaxy image data from the Sloan Digital Sky
Survey to demonstrate the effective performance of our proposed image
statistics at accurately detecting spiral and elliptical galaxies when used as
features of a random forest classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Baumstark_M/0/1/0/all/0/1&quot;&gt;Matthew J. Baumstark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vinci_G/0/1/0/all/0/1&quot;&gt;Giuseppe Vinci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07745">
<title>Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey. (arXiv:2310.07745v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07745</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid increase in the number of cyber-attacks in recent years raises the
need for principled methods for defending networks against malicious actors.
Deep reinforcement learning (DRL) has emerged as a promising approach for
mitigating these attacks. However, while DRL has shown much potential for
cyber-defence, numerous challenges must be overcome before DRL can be applied
to autonomous cyber-operations (ACO) at scale. Principled methods are required
for environments that confront learners with very high-dimensional state
spaces, large multi-discrete action spaces, and adversarial learning. Recent
works have reported success in solving these problems individually. There have
also been impressive engineering efforts towards solving all three for
real-time strategy games. However, applying DRL to the full ACO problem remains
an open challenge. Here, we survey the relevant DRL literature and
conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the
domain properties that define the ACO problem; ii.) A comprehensive evaluation
of the extent to which domains used for benchmarking DRL approaches are
comparable to ACO; iii.) An overview of state-of-the-art approaches for scaling
DRL to domains that confront learners with the curse of dimensionality, and;
iv.) A survey and critique of current methods for limiting the exploitability
of agents within adversarial settings from the perspective of ACO. We conclude
with open research questions that we hope will motivate future directions for
researchers and practitioners working on ACO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmer_G/0/1/0/all/0/1&quot;&gt;Gregory Palmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parry_C/0/1/0/all/0/1&quot;&gt;Chris Parry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrold_D/0/1/0/all/0/1&quot;&gt;Daniel J.B. Harrold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1&quot;&gt;Chris Willis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07747">
<title>Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07747</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning transparent, interpretable controllers with offline data in
decision-making systems is an essential area of research due to its potential
to reduce the risk of applications in real-world systems. However, in
responsibility-sensitive settings such as healthcare, decision accountability
is of paramount importance, yet has not been adequately addressed by the
literature. This paper introduces the Accountable Offline Controller (AOC) that
employs the offline dataset as the Decision Corpus and performs accountable
control based on a tailored selection of examples, referred to as the Corpus
Subset. ABC operates effectively in low-data scenarios, can be extended to the
strictly offline imitation setting, and displays qualities of both conservation
and adaptability. We assess ABC&apos;s performance in both simulated and real-world
healthcare scenarios, emphasizing its capability to manage offline control
tasks with high levels of performance while maintaining accountability.
&lt;/p&gt;
&lt;p&gt;Keywords: Interpretable Reinforcement Learning, Explainable Reinforcement
Learning, Reinforcement Learning Transparency, Offline Reinforcement Learning,
Batched Control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1&quot;&gt;Alihan H&amp;#xfc;y&amp;#xfc;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1&quot;&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07756">
<title>Self-supervised Representation Learning From Random Data Projectors. (arXiv:2310.07756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07756</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised representation learning~(SSRL) has advanced considerably by
exploiting the transformation invariance assumption under artificially designed
data augmentations. While augmentation-based SSRL algorithms push the
boundaries of performance in computer vision and natural language processing,
they are often not directly applicable to other data modalities, and can
conflict with application-specific data augmentation constraints. This paper
presents an SSRL approach that can be applied to any data modality and network
architecture because it does not rely on augmentations or masking.
Specifically, we show that high-quality data representations can be learned by
reconstructing random data projections. We evaluate the proposed approach on a
wide range of representation learning tasks that span diverse modalities and
real-world applications. We show that it outperforms multiple state-of-the-art
SSRL baselines. Due to its wide applicability and strong empirical results, we
argue that learning from randomness is a fruitful research direction worthy of
attention and further study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yi Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tongzi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cresswell_J/0/1/0/all/0/1&quot;&gt;Jesse C. Cresswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Ga Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_G/0/1/0/all/0/1&quot;&gt;George Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Shi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1&quot;&gt;Maksims Volkovs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07765">
<title>Feature Learning and Generalization in Deep Networks with Orthogonal Weights. (arXiv:2310.07765v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07765</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully-connected deep neural networks with weights initialized from
independent Gaussian distributions can be tuned to criticality, which prevents
the exponential growth or decay of signals propagating through the network.
However, such networks still exhibit fluctuations that grow linearly with the
depth of the network, which may impair the training of networks with width
comparable to depth. We show analytically that rectangular networks with tanh
activations and weights initialized from the ensemble of orthogonal matrices
have corresponding preactivation fluctuations which are independent of depth,
to leading order in inverse width. Moreover, we demonstrate numerically that,
at initialization, all correlators involving the neural tangent kernel (NTK)
and its descendants at leading order in inverse width -- which govern the
evolution of observables during training -- saturate at a depth of $\sim 20$,
rather than growing without bound as in the case of Gaussian initializations.
We speculate that this structure preserves finite-width feature learning while
reducing overall noise, thus improving both generalization and training speed.
We provide some experimental justification by relating empirical measurements
of the NTK to the superior performance of deep nonlinear orthogonal networks
trained under full-batch gradient descent on the MNIST and CIFAR-10
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Day_H/0/1/0/all/0/1&quot;&gt;Hannah Day&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_Y/0/1/0/all/0/1&quot;&gt;Yonatan Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Daniel A. Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07780">
<title>Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches. (arXiv:2310.07780v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07780</link>
<description rdf:parseType="Literal">&lt;p&gt;Randomized smoothing has recently attracted attentions in the field of
adversarial robustness to provide provable robustness guarantees on smoothed
neural network classifiers. However, existing works show that vanilla
randomized smoothing usually does not provide good robustness performance and
often requires (re)training techniques on the base classifier in order to boost
the robustness of the resulting smoothed classifier. In this work, we propose
two cost-effective approaches to boost the robustness of randomized smoothing
while preserving its clean performance. The first approach introduces a new
robust training method AdvMacerwhich combines adversarial training and
robustness certification maximization for randomized smoothing. We show that
AdvMacer can improve the robustness performance of randomized smoothing
classifiers compared to SOTA baselines, while being 3x faster to train than
MACER baseline. The second approach introduces a post-processing method EsbRS
which greatly improves the robustness certificate based on building model
ensembles. We explore different aspects of model ensembles that has not been
studied by prior works and propose a novel design methodology to further
improve robustness of the ensemble based on our theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Linbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Trong Nghia Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Lam M. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1&quot;&gt;Tsui-Wei Weng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07786">
<title>Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07786</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world applications of contextual bandits often exhibit non-stationarity
due to seasonality, serendipity, and evolving social trends. While a number of
non-stationary contextual bandit learning algorithms have been proposed in the
literature, they excessively explore due to a lack of prioritization for
information of enduring value, or are designed in ways that do not scale in
modern applications with high-dimensional user-specific features and large
action set, or both. In this paper, we introduce a novel non-stationary
contextual bandit algorithm that addresses these concerns. It combines a
scalable, deep-neural-network-based architecture with a carefully designed
exploration mechanism that strategically prioritizes collecting information
with the most lasting value in a non-stationary environment. Through empirical
evaluations on two real-world recommendation datasets, which exhibit pronounced
non-stationarity, we demonstrate that our approach significantly outperforms
the state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yueyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_X/0/1/0/all/0/1&quot;&gt;Xu Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07787">
<title>Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data. (arXiv:2310.07787v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07787</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses predictive performance and processes undertaken on
flight pricing data utilizing r2(r-square) and RMSE that leverages a large
dataset, originally from Expedia.com, consisting of approximately 20 million
records or 4.68 gigabytes. The project aims to determine the best models usable
in the real world to predict airline ticket fares for non-stop flights across
the US. Therefore, good generalization capability and optimized processing
times are important measures for the model.
&lt;/p&gt;
&lt;p&gt;We will discover key business insights utilizing feature importance and
discuss the process and tools used for our analysis. Four regression machine
learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision
Tree, and Factorization Machines utilizing Cross Validator and Training
Validator functions for assessing performance and generalization capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_P/0/1/0/all/0/1&quot;&gt;Philip Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thant_P/0/1/0/all/0/1&quot;&gt;Phue Thant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Pratiksha Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antaliya_R/0/1/0/all/0/1&quot;&gt;Ruta Antaliya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1&quot;&gt;Jongwook Woo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07793">
<title>GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07793</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Ruotong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xu Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07794">
<title>CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving. (arXiv:2310.07794v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07794</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarking is a common method for evaluating trajectory prediction models
for autonomous driving. Existing benchmarks rely on datasets, which are biased
towards more common scenarios, such as cruising, and distance-based metrics
that are computed by averaging over all scenarios. Following such a regiment
provides a little insight into the properties of the models both in terms of
how well they can handle different scenarios and how admissible and diverse
their outputs are. There exist a number of complementary metrics designed to
measure the admissibility and diversity of trajectories, however, they suffer
from biases, such as length of trajectories.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a new benChmarking paRadIgm for evaluaTing
trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a
method for extracting driving scenarios at varying levels of specificity
according to the structure of the roads, models&apos; performance, and data
properties for fine-grained ranking of prediction models; 2) A set of new
bias-free metrics for measuring diversity, by incorporating the characteristics
of a given scenario, and admissibility, by considering the structure of roads
and kinematic compliancy, motivated by real-world driving constraints. 3) Using
the proposed benchmark, we conduct extensive experimentation on a
representative set of the prediction models using the large scale Argoverse
dataset. We show that the proposed benchmark can produce a more accurate
ranking of the models and serve as a means of characterizing their behavior. We
further present ablation studies to highlight contributions of different
elements that are used to compute the proposed metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourkeshavarz_M/0/1/0/all/0/1&quot;&gt;Mozhgan Pourkeshavarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1&quot;&gt;Amir Rasouli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07799">
<title>A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07799</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the limited information about emerging diseases, symptoms are hard to
be noticed and recognized, so that the window for clinical intervention could
be ignored. An effective prognostic model is expected to assist doctors in
making right diagnosis and designing personalized treatment plan, so to
promptly prevent unfavorable outcomes. However, in the early stage of a
disease, limited data collection and clinical experiences, plus the concern out
of privacy and ethics, may result in restricted data availability for
reference, to the extent that even data labels are difficult to mark correctly.
In addition, Electronic Medical Record (EMR) data of different diseases or of
different sources of the same disease can prove to be having serious
cross-dataset feature misalignment problems, greatly mutilating the efficiency
of deep learning models. This article introduces a transfer learning method to
build a transition model from source dataset to target dataset. By way of
constraining the distribution shift of features generated in disparate domains,
domain-invariant features that are exclusively relative to downstream tasks are
captured, so to cultivate a unified domain-invariant encoder across various
task domains to achieve better feature representation. Experimental results of
several target tasks demonstrate that our proposed model outperforms competing
baseline methods and has higher rate of training convergence, especially in
dealing with limited data amount. A multitude of experiences have proven the
efficacy of our method to provide more accurate predictions concerning newly
emergent pandemics and other diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaohe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liantao Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07800">
<title>Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07800</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention mechanisms have exhibited promising potential in enhancing learning
models by identifying salient portions of input data. This is particularly
valuable in scenarios where limited training samples are accessible due to
challenges in data collection and labeling. Drawing inspiration from human
recognition processes, we posit that an AI baseline&apos;s performance could be more
accurate and dependable if it is exposed to essential segments of raw data
rather than the entire input dataset, akin to human perception. However, the
task of selecting these informative data segments, referred to as hard
attention finding, presents a formidable challenge. In situations with few
training samples, existing studies struggle to locate such informative regions
due to the large number of training parameters that cannot be effectively
learned from the available limited samples. In this study, we introduce a novel
and practical framework for achieving explainable hard attention finding,
specifically tailored for few-shot learning scenarios, called FewXAT. Our
approach employs deep reinforcement learning to implement the concept of hard
attention, directly impacting raw input data and thus rendering the process
interpretable for human understanding. Through extensive experimentation across
various benchmark datasets, we demonstrate the efficacy of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikpour_B/0/1/0/all/0/1&quot;&gt;Bahareh Nikpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07805">
<title>Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07805</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models (DMs) represent state-of-the-art generative models for
continuous inputs. DMs work by constructing a Stochastic Differential Equation
(SDE) in the input space (ie, position space), and using a neural network to
reverse it. In this work, we introduce a novel generative modeling framework
grounded in \textbf{phase space dynamics}, where a phase space is defined as
{an augmented space encompassing both position and velocity.} Leveraging
insights from Stochastic Optimal Control, we construct a path measure in the
phase space that enables efficient sampling. {In contrast to DMs, our framework
demonstrates the capability to generate realistic data points at an early stage
of dynamics propagation.} This early prediction sets the stage for efficient
data generation by leveraging additional velocity information along the
trajectory. On standard image generation benchmarks, our model yields favorable
performance over baselines in the regime of small Number of Function
Evaluations (NFEs). Furthermore, our approach rivals the performance of
diffusion models equipped with efficient sampling techniques, underscoring its
potential as a new tool generative modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianrong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_L/0/1/0/all/0/1&quot;&gt;Laurent Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1&quot;&gt;Evangelos A. Theodorou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1&quot;&gt;Josh Susskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shuangfei Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07807">
<title>FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning. (arXiv:2310.07807v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07807</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a decentralized machine learning approach where
independent learners process data privately. Its goal is to create a robust and
accurate model by aggregating and retraining local models over multiple rounds.
However, FL faces challenges regarding data heterogeneity and model aggregation
effectiveness. In order to simulate real-world data, researchers use methods
for data partitioning that transform a dataset designated for centralized
learning into a group of sub-datasets suitable for distributed machine learning
with different data heterogeneity. In this paper, we study the currently
popular data partitioning techniques and visualize their main disadvantages:
the lack of precision in the data diversity, which leads to unreliable
heterogeneity indexes, and the inability to incrementally challenge the FL
algorithms. To resolve this problem, we propose a method that leverages entropy
and symmetry to construct &apos;the most challenging&apos; and controllable data
distributions with gradual difficulty. We introduce a metric to measure data
heterogeneity among the learning agents and a transformation technique that
divides any dataset into splits with precise data diversity. Through a
comparative study, we demonstrate the superiority of our method over existing
FL data partitioning approaches, showcasing its potential to challenge model
aggregation algorithms. Experimental results indicate that our approach
gradually challenges the FL strategies, and the models trained on FedSym
distributions are more distinct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyamousavi_E/0/1/0/all/0/1&quot;&gt;Ensiye Kiyamousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraychev_B/0/1/0/all/0/1&quot;&gt;Boris Kraychev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1&quot;&gt;Ivan Koychev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07811">
<title>Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore. (arXiv:2310.07811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07811</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider online reinforcement learning (RL) in episodic Markov decision
processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is
assumed that the action-values of all policies can be expressed as linear
functions of state-action features. This class is known to be more general than
linear MDPs, where the transition kernel and the reward function are assumed to
be linear functions of the feature vectors. As our first contribution, we show
that the difference between the two classes is the presence of states in
linearly $q^\pi$-realizable MDPs where for any policy, all the actions have
approximately equal values, and skipping over these states by following an
arbitrarily fixed policy in those states transforms the problem to a linear
MDP. Based on this observation, we derive a novel (computationally inefficient)
learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously
learns what states should be skipped over and runs another learning algorithm
on the linear MDP hidden in the problem. The method returns an
$\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions
with the MDP, where $H$ is the time horizon and $d$ is the dimension of the
feature vectors, giving the first polynomial-sample-complexity online RL
algorithm for this setting. The results are proved for the misspecified case,
where the sample complexity is shown to degrade gracefully with the
misspecification error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisz_G/0/1/0/all/0/1&quot;&gt;Gell&amp;#xe9;rt Weisz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Gy&amp;#xf6;rgy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07814">
<title>Explorable Mesh Deformation Subspaces from Unstructured Generative Models. (arXiv:2310.07814v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2310.07814</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploring variations of 3D shapes is a time-consuming process in traditional
3D modeling tools. Deep generative models of 3D shapes often feature continuous
latent spaces that can, in principle, be used to explore potential variations
starting from a set of input shapes. In practice, doing so can be problematic:
latent spaces are high dimensional and hard to visualize, contain shapes that
are not relevant to the input shapes, and linear paths through them often lead
to sub-optimal shape transitions. Furthermore, one would ideally be able to
explore variations in the original high-quality meshes used to train the
generative model, not its lower-quality output geometry. In this paper, we
present a method to explore variations among a given set of landmark shapes by
constructing a mapping from an easily-navigable 2D exploration space to a
subspace of a pre-trained generative model. We first describe how to find a
mapping that spans the set of input landmark shapes and exhibits smooth
variations between them. We then show how to turn the variations in this
subspace into deformation fields, to transfer those variations to high-quality
meshes for the landmark shapes. Our results show that our method can produce
visually-pleasing and easily-navigable 2D exploration spaces for several
different shape categories, especially as compared to prior work on learning
deformation spaces for 3D shapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maesumi_A/0/1/0/all/0/1&quot;&gt;Arman Maesumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1&quot;&gt;Paul Guerrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1&quot;&gt;Vladimir G. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1&quot;&gt;Matthew Fisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1&quot;&gt;Noam Aigerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1&quot;&gt;Daniel Ritchie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07815">
<title>Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.07815</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic identifier (ID) is an important concept in information retrieval
that aims to preserve the semantics of objects such as documents and items
inside their IDs. Previous studies typically adopt a two-stage pipeline to
learn semantic IDs by first procuring embeddings using off-the-shelf text
encoders and then deriving IDs based on the embeddings. However, each step
introduces potential information loss and there is usually an inherent mismatch
between the distribution of embeddings within the latent space produced by text
encoders and the anticipated distribution required for semantic indexing.
Nevertheless, it is non-trivial to design a method that can learn the
document&apos;s semantic representations and its hierarchical structure
simultaneously, given that semantic IDs are discrete and sequentially
structured, and the semantic supervision is deficient. In this paper, we
introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a
generative language model. We tackle the challenge of sequential discrete ID by
introducing a semantic indexer capable of generating neural sequential discrete
representations with progressive training and contrastive learning. In response
to the semantic supervision deficiency, we propose to train the model with a
self-supervised document reconstruction objective. The learned semantic indexer
can facilitate various downstream tasks, such as recommendation and retrieval.
We conduct experiments on three tasks including recommendation, product search,
and document retrieval on five datasets from various domains, where LMINDEXER
outperforms competitive baselines significantly and consistently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bowen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hansi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiusi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianxin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruirui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hanqing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Suhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xianfeng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07819">
<title>Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;A common approach to explain NLP models, is to use importance measures that
express which tokens are important for a prediction. Unfortunately, such
explanations are often wrong despite being persuasive. Therefore, it is
essential to measure their faithfulness. One such metric is if tokens are truly
important, then masking them should result in worse model performance. However,
token masking introduces out-of-distribution issues and existing solutions are
computationally expensive and employ proxy-models. Furthermore, other metrics
are very limited in scope. In this work, we propose an inherently faithfulness
measurable model that addresses these challenges. This is achieved by using a
novel fine-tuning method that incorporates masking, such that masking tokens
become in-distribution by design. This differs from existing approaches, which
are completely model-agnostic but are inapplicable in practice. We demonstrate
the generality of our approach by applying it to various tasks and validate it
using statistical in-distribution tests. Additionally, because masking is
in-distribution, importance measures which themselves use masking become more
faithful, thus our model becomes more explainable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1&quot;&gt;Andreas Madsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1&quot;&gt;Sarath Chandar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07820">
<title>Large Language Models Are Zero-Shot Time Series Forecasters. (arXiv:2310.07820v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07820</link>
<description rdf:parseType="Literal">&lt;p&gt;By encoding time series as a string of numerical digits, we can frame time
series forecasting as next-token prediction in text. Developing this approach,
we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can
surprisingly zero-shot extrapolate time series at a level comparable to or
exceeding the performance of purpose-built time series models trained on the
downstream tasks. To facilitate this performance, we propose procedures for
effectively tokenizing time series data and converting discrete distributions
over tokens into highly flexible densities over continuous values. We argue the
success of LLMs for time series stems from their ability to naturally represent
multimodal distributions, in conjunction with biases for simplicity, and
repetition, which align with the salient features in many time series, such as
repeated seasonal trends. We also show how LLMs can naturally handle missing
data without imputation through non-numerical text, accommodate textual side
information, and answer questions to help explain predictions. While we find
that increasing model size generally improves performance on time series, we
show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,
and poor uncertainty calibration, which is likely the result of alignment
interventions such as RLHF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruver_N/0/1/0/all/0/1&quot;&gt;Nate Gruver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1&quot;&gt;Marc Finzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shikai Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07830">
<title>Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07830</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data&apos;s potential, ensuring
optimal model performance in diverse applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_S/0/1/0/all/0/1&quot;&gt;Sia Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_M/0/1/0/all/0/1&quot;&gt;Marwan Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07831">
<title>When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07831</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning rate schedules used in practice bear little resemblance to those
recommended by theory. We close much of this theory/practice gap, and as a
consequence are able to derive new problem-adaptive learning rate schedules.
Our key technical contribution is a refined analysis of learning rate schedules
for a wide class of optimization algorithms (including SGD). In contrast to
most prior works that study the convergence of the average iterate, we study
the last iterate, which is what most people use in practice. When considering
only worst-case analysis, our theory predicts that the best choice is the
linear decay schedule: a popular choice in practice that sets the stepsize
proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the
total number of steps. To go beyond this worst-case analysis, we use the
observed gradient norms to derive schedules refined for any particular task.
These refined schedules exhibit learning rate warm-up and rapid learning rate
annealing near the end of training. Ours is the first systematic approach to
automatically yield both of these properties. We perform the most comprehensive
evaluation of learning rate schedules to date, evaluating across 10 diverse
deep learning problems, a series of LLMs, and a suite of logistic regression
problems. We validate that overall, the linear-decay schedule matches or
outperforms all commonly used default schedules including cosine annealing, and
that our schedule refinement method gives further improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1&quot;&gt;Aaron Defazio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1&quot;&gt;Ashok Cutkosky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1&quot;&gt;Harsh Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07837">
<title>Measuring Feature Sparsity in Language Models. (arXiv:2310.07837v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07837</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have proposed that activations in language models can be
modelled as sparse linear combinations of vectors corresponding to features of
input text. Under this assumption, these works aimed to reconstruct feature
directions using sparse coding. We develop metrics to assess the success of
these sparse coding techniques and test the validity of the linearity and
sparsity assumptions. We show our metrics can predict the level of sparsity on
synthetic sparse linear activations, and can distinguish between sparse linear
data and several other distributions. We use our metrics to measure levels of
sparsity in several language models. We find evidence that language model
activations can be accurately modelled by sparse linear combinations of
features, significantly more so than control datasets. We also show that model
activations appear to be sparsest in the first and final layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1&quot;&gt;Mingyang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Lucas Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benton_J/0/1/0/all/0/1&quot;&gt;Joe Benton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07838">
<title>Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07838</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingyue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07852">
<title>On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.07852</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of model selection in a high-dimensional sparse
linear regression model under the differential privacy framework. In
particular, we consider the problem of differentially private best subset
selection and study its utility guarantee. We adopt the well-known exponential
mechanism for selecting the best model, and under a certain margin condition,
we establish its strong model recovery property. However, the exponential
search space of the exponential mechanism poses a serious computational
bottleneck. To overcome this challenge, we propose a Metropolis-Hastings
algorithm for the sampling step and establish its polynomial mixing time to its
stationary distribution in the problem parameters $n,p$, and $s$. Furthermore,
we also establish approximate differential privacy for the final estimates of
the Metropolis-Hastings random walk using its mixing property. Finally, we also
perform some illustrative simulations that echo the theoretical findings of our
main results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Saptarshi Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07855">
<title>CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping. (arXiv:2310.07855v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07855</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging nearest neighbor retrieval for self-supervised representation
learning has proven beneficial with object-centric images. However, this
approach faces limitations when applied to scene-centric datasets, where
multiple objects within an image are only implicitly captured in the global
representation. Such global bootstrapping can lead to undesirable entanglement
of object representations. Furthermore, even object-centric datasets stand to
benefit from a finer-grained bootstrapping approach. In response to these
challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method
tailored to enhance dense visual representation learning. By employing
object-level nearest neighbor bootstrapping throughout the training, CrIBo
emerges as a notably strong and adequate candidate for in-context learning,
leveraging nearest neighbor retrieval at test time. CrIBo shows
state-of-the-art performance on the latter task while being highly competitive
in more standard downstream segmentation tasks. Our code and pretrained models
will be publicly available upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebailly_T/0/1/0/all/0/1&quot;&gt;Tim Lebailly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stegmuller_T/0/1/0/all/0/1&quot;&gt;Thomas Stegm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1&quot;&gt;Behzad Bozorgtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Thiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07858">
<title>QArchSearch: A Scalable Quantum Architecture Search Package. (arXiv:2310.07858v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2310.07858</link>
<description rdf:parseType="Literal">&lt;p&gt;The current era of quantum computing has yielded several algorithms that
promise high computational efficiency. While the algorithms are sound in theory
and can provide potentially exponential speedup, there is little guidance on
how to design proper quantum circuits to realize the appropriate unitary
transformation to be applied to the input quantum state. In this paper, we
present \texttt{QArchSearch}, an AI based quantum architecture search package
with the \texttt{QTensor} library as a backend that provides a principled and
automated approach to finding the best model given a task and input quantum
state. We show that the search package is able to efficiently scale the search
to large quantum circuits and enables the exploration of more complex models
for different quantum applications. \texttt{QArchSearch} runs at scale and high
efficiency on high-performance computing systems using a two-level
parallelization scheme on both CPUs and GPUs, which has been demonstrated on
the Polaris supercomputer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kulshrestha_A/0/1/0/all/0/1&quot;&gt;Ankit Kulshrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lykov_D/0/1/0/all/0/1&quot;&gt;Danylo Lykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Safro_I/0/1/0/all/0/1&quot;&gt;Ilya Safro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Alexeev_Y/0/1/0/all/0/1&quot;&gt;Yuri Alexeev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07874">
<title>Refined Mechanism Design for Approximately Structured Priors via Active Regression. (arXiv:2310.07874v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2310.07874</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of a revenue-maximizing seller with a large number of
items $m$ for sale to $n$ strategic bidders, whose valuations are drawn
independently from high-dimensional, unknown prior distributions. It is
well-known that optimal and even approximately-optimal mechanisms for this
setting are notoriously difficult to characterize or compute, and, even when
they can be found, are often rife with various counter-intuitive properties. In
this paper, following a model introduced recently by Cai and
Daskalakis~\cite{cai2022recommender}, we consider the case that bidders&apos; prior
distributions can be well-approximated by a topic model. We design an active
learning component, responsible for interacting with the bidders and outputting
low-dimensional approximations of their types, and a mechanism design
component, responsible for robustifying mechanisms for the low-dimensional
model to work for the approximate types of the former component. On the active
learning front, we cast our problem in the framework of Randomized Linear
Algebra (RLA) for regression problems, allowing us to import several
breakthrough results from that line of research, and adapt them to our setting.
On the mechanism design front, we remove many restrictive assumptions of prior
work on the type of access needed to the underlying distributions and the
associated mechanisms. To the best of our knowledge, our work is the first to
formulate connections between mechanism design, and RLA for active learning of
regression problems, opening the door for further applications of randomized
linear algebra primitives to mechanism design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutsikas_C/0/1/0/all/0/1&quot;&gt;Christos Boutsikas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drineas_P/0/1/0/all/0/1&quot;&gt;Petros Drineas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mertzanidis_M/0/1/0/all/0/1&quot;&gt;Marios Mertzanidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psomas_A/0/1/0/all/0/1&quot;&gt;Alexandros Psomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1&quot;&gt;Paritosh Verma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07875">
<title>TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07875</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present &quot;TabLib&apos;&apos;, a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eggert_G/0/1/0/all/0/1&quot;&gt;Gus Eggert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_K/0/1/0/all/0/1&quot;&gt;Kevin Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biven_M/0/1/0/all/0/1&quot;&gt;Mike Biven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waugh_J/0/1/0/all/0/1&quot;&gt;Justin Waugh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07881">
<title>DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks. (arXiv:2310.07881v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2310.07881</link>
<description rdf:parseType="Literal">&lt;p&gt;Content Delivery Networks carry the majority of Internet traffic, and the
increasing demand for video content as a major IP traffic across the Internet
highlights the importance of caching and prefetching optimization algorithms.
Prefetching aims to make data available in the cache before the requester
places its request to reduce access time and improve the Quality of Experience
on the user side. Prefetching is well investigated in operating systems,
compiler instructions, in-memory cache, local storage systems, high-speed
networks, and cloud systems. Traditional prefetching techniques are well
adapted to a particular access pattern, but fail to adapt to sudden variations
or randomization in workloads. This paper explores the use of reinforcement
learning to tackle the changes in user access patterns and automatically adapt
over time. To this end, we propose, DeePref, a Deep Reinforcement Learning
agent for online video content prefetching in Content Delivery Networks.
DeePref is a prefetcher implemented on edge networks and is agnostic to
hardware design, operating systems, and applications. Our results show that
DeePref DRQN, using a real-world dataset, achieves a 17% increase in
prefetching accuracy and a 28% increase in prefetching coverage on average
compared to baseline approaches that use video content popularity as a building
block to statically or dynamically make prefetching decisions. We also study
the possibility of transfer learning of statistical models from one edge
network into another, where unseen user requests from unknown distribution are
observed. In terms of transfer learning, the increase in prefetching accuracy
and prefetching coverage are [$30%$, $10%$], respectively. Our source code will
be available on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkassab_N/0/1/0/all/0/1&quot;&gt;Nawras Alkassab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chin-Tser Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botran_T/0/1/0/all/0/1&quot;&gt;Tania Lorido Botran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07882">
<title>The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research. (arXiv:2310.07882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07882</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the practical relevance of explainable
artificial intelligence (XAI) with a special focus on the producing industries
and relate them to the current state of academic XAI research. Our findings are
based on an extensive series of interviews regarding the role and applicability
of XAI along the Machine Learning (ML) lifecycle in current industrial practice
and its expected relevance in the future. The interviews were conducted among a
great variety of roles and key stakeholders from different industry sectors. On
top of that, we outline the state of XAI research by providing a concise review
of the relevant literature. This enables us to provide an encompassing overview
covering the opinions of the surveyed persons as well as the current state of
academic research. By comparing our interview results with the current research
approaches we reveal several discrepancies. While a multitude of different XAI
approaches exists, most of them are centered around the model evaluation phase
and data scientists. Their versatile capabilities for other stages are
currently either not sufficiently explored or not popular among practitioners.
In line with existing work, our findings also confirm that more efforts are
needed to enable also non-expert users&apos; interpretation and understanding of
opaque AI models with existing methods and frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decker_T/0/1/0/all/0/1&quot;&gt;Thomas Decker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_R/0/1/0/all/0/1&quot;&gt;Ralf Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koebler_A/0/1/0/all/0/1&quot;&gt;Alexander Koebler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebacher_M/0/1/0/all/0/1&quot;&gt;Michael Lebacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnitzer_R/0/1/0/all/0/1&quot;&gt;Ronald Schnitzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1&quot;&gt;Stefan H. Weber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07885">
<title>Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives. (arXiv:2310.07885v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07885</link>
<description rdf:parseType="Literal">&lt;p&gt;The collective behavior of a network with heterogeneous, resource-limited
information processing units (e.g., group of fish, flock of birds, or network
of neurons) demonstrates high self-organization and complexity. These emergent
properties arise from simple interaction rules where certain individuals can
exhibit leadership-like behavior and influence the collective activity of the
group. Motivated by the intricacy of these collectives, we propose a neural
network (NN) architecture inspired by the rules observed in nature&apos;s collective
ensembles. This NN structure contains workers that encompass one or more
information processing units (e.g., neurons, filters, layers, or blocks of
layers). Workers are either leaders or followers, and we train a
leader-follower neural network (LFNN) by leveraging local error signals and
optionally incorporating backpropagation (BP) and global loss. We investigate
worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs
trained with local error signals achieve significantly lower error rates than
previous BP-free algorithms on MNIST and CIFAR-10 and even surpass BP-enabled
baselines. In the case of ImageNet, our LFNN-l demonstrates superior
scalability and outperforms previous BP-free algorithms by a significant
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chenzhong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Mingxi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiongye Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazarian_S/0/1/0/all/0/1&quot;&gt;Shahin Nazarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irimia_A/0/1/0/all/0/1&quot;&gt;Andrei Irimia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1&quot;&gt;Paul Bogdan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07891">
<title>A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.07891</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature learning is thought to be one of the fundamental reasons for the
success of deep neural networks. It is rigorously known that in two-layer
fully-connected neural networks under certain conditions, one step of gradient
descent on the first layer followed by ridge regression on the second layer can
lead to feature learning; characterized by the appearance of a separated
rank-one component -- spike -- in the spectrum of the feature matrix. However,
with a constant gradient descent step size, this spike only carries information
from the linear component of the target function and therefore learning
non-linear components is impossible. We show that with a learning rate that
grows with the sample size, such training in fact introduces multiple rank-one
components, each corresponding to a specific polynomial feature. We further
prove that the limiting large-dimensional and large sample training and test
errors of the updated neural networks are fully characterized by these spikes.
By precisely analyzing the improvement in the loss, we demonstrate that these
non-linear features can enhance learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moniri_B/0/1/0/all/0/1&quot;&gt;Behrad Moniri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dobriban_E/0/1/0/all/0/1&quot;&gt;Edgar Dobriban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07892">
<title>ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control. (arXiv:2310.07892v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07892</link>
<description rdf:parseType="Literal">&lt;p&gt;Station keeping is an essential maneuver for Autonomous Surface Vehicles
(ASVs), mainly when used in confined spaces, to carry out surveys that require
the ASV to keep its position or in collaboration with other vehicles where the
relative position has an impact over the mission. However, this maneuver can
become challenging for classic feedback controllers due to the need for an
accurate model of the ASV dynamics and the environmental disturbances. This
work proposes a Model Predictive Controller using Neural Network Simulation
Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV
under wind disturbances. The performance of the proposed scheme under wind
disturbances is tested and compared against other controllers in simulation,
using the Robotics Operating System (ROS) and the multipurpose simulation
environment Gazebo. A set of six tests were conducted by combining two wind
speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and
180$^\circ$). The simulation results clearly show the advantage of the
NNSEM-MPC over the following methods: backstepping controller, sliding mode
controller, simplified dynamics MPC (SD-MPC), neural ordinary differential
equation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed
NNSEM-MPC approach performs better than the rest in 4 out of the 6 test
conditions, and it is the second best in the 2 remaining test cases, reducing
the mean position and heading error by at least 31\% and 46\% respectively
across all the test cases. In terms of execution speed, the proposed NNSEM-MPC
is at least 36\% faster than the rest of the MPC controllers. The field
experiments on two different ASV platforms showed that ASVs can effectively
keep the station utilizing the proposed method, with a position error as low as
$1.68$ m and a heading error as low as $6.14^{\circ}$ within time windows of at
least $150$s.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavez_Galaviz_J/0/1/0/all/0/1&quot;&gt;Jalil Chavez-Galaviz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1&quot;&gt;Ajinkya Chaudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoudian_N/0/1/0/all/0/1&quot;&gt;Nina Mahmoudian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07894">
<title>Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07894</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models suffer from slow sample generation at inference time.
Therefore, developing a principled framework for fast deterministic/stochastic
sampling for a broader class of diffusion models is a promising direction. We
propose two complementary frameworks for accelerating sample generation in
pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate
integrators generalize DDIM, mapping the reverse diffusion dynamics to a more
amenable space for sampling. In contrast, splitting-based integrators, commonly
used in molecular dynamics, reduce the numerical simulation error by cleverly
alternating between numerical updates involving the data and auxiliary
variables. After extensively studying these methods empirically and
theoretically, we present a hybrid method that leads to the best-reported
performance for diffusion models in augmented spaces. Applied to Phase Space
Langevin Diffusion [Pandey &amp;amp; Mandt, 2023] on CIFAR-10, our deterministic and
stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network
function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing
baselines, respectively. Our code and model checkpoints will be made publicly
available at \url{https://github.com/mandt-lab/PSLD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1&quot;&gt;Kushagra Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Maja Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07895">
<title>Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs. (arXiv:2310.07895v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07895</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a method to efficiently classify the gastroenterologic
section of images derived from Video Capsule Endoscopy (VCE) studies by
exploring the combination of a Convolutional Neural Network (CNN) for
classification with the time-series analysis properties of a Hidden Markov
Model (HMM). It is demonstrated that successive time-series analysis identifies
and corrects errors in the CNN output. Our approach achieves an accuracy of
$98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for
precise localization within the gastrointestinal (GI) tract while requiring
only approximately 1M parameters and thus, provides a method suitable for low
power devices
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_J/0/1/0/all/0/1&quot;&gt;Julia Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerum_C/0/1/0/all/0/1&quot;&gt;Christoph Gerum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiber_M/0/1/0/all/0/1&quot;&gt;Moritz Reiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nick_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Nick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bringmann_O/0/1/0/all/0/1&quot;&gt;Oliver Bringmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07896">
<title>NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration. (arXiv:2310.07896v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07896</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic learning for navigation in unfamiliar environments needs to provide
policies for both task-oriented navigation (i.e., reaching a goal that the
robot has located), and task-agnostic exploration (i.e., searching for a goal
in a novel setting). Typically, these roles are handled by separate models, for
example by using subgoal proposals, planning, or separate navigation
strategies. In this paper, we describe how we can train a single unified
diffusion policy to handle both goal-directed navigation and goal-agnostic
exploration, with the latter providing the ability to search novel
environments, and the former providing the ability to reach a user-specified
goal once it has been located. We show that this unified policy results in
better overall performance when navigating to visually indicated goals in novel
environments, as compared to approaches that use subgoal proposals from
generative models, or prior methods based on latent variable models. We
instantiate our method by using a large-scale Transformer-based policy trained
on data from multiple ground robots, with a diffusion model decoder to flexibly
handle both goal-conditioned and goal-agnostic navigation. Our experiments,
conducted on a real-world mobile robot platform, show effective navigation in
unseen environments in comparison with five alternative methods, and
demonstrate significant improvements in performance and lower collision rates,
despite utilizing smaller models than state-of-the-art approaches. For more
videos, code, and pre-trained model checkpoints, see
https://general-navigation-models.github.io/nomad/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Ajay Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Dhruv Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glossop_C/0/1/0/all/0/1&quot;&gt;Catherine Glossop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07902">
<title>Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07902</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of robotics, numerous downstream robotics tasks leverage machine
learning methods for processing, modeling, or synthesizing data. Often, this
data comprises variables that inherently carry geometric constraints, such as
the unit-norm condition of quaternions representing rigid-body orientations or
the positive definiteness of stiffness and manipulability ellipsoids. Handling
such geometric constraints effectively requires the incorporation of tools from
differential geometry into the formulation of machine learning methods. In this
context, Riemannian manifolds emerge as a powerful mathematical framework to
handle such geometric constraints. Nevertheless, their recent adoption in robot
learning has been largely characterized by a mathematically-flawed
simplification, hereinafter referred to as the ``single tangent space fallacy&quot;.
This approach involves merely projecting the data of interest onto a single
tangent (Euclidean) space, over which an off-the-shelf learning algorithm is
applied. This paper provides a theoretical elucidation of various
misconceptions surrounding this approach and offers experimental evidence of
its shortcomings. Finally, it presents valuable insights to promote best
practices when employing Riemannian geometry within robot learning
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaquier_N/0/1/0/all/0/1&quot;&gt;No&amp;#xe9;mie Jaquier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozo_L/0/1/0/all/0/1&quot;&gt;Leonel Rozo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asfour_T/0/1/0/all/0/1&quot;&gt;Tamim Asfour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07917">
<title>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07917</link>
<description rdf:parseType="Literal">&lt;p&gt;For over two decades, detecting rare events has been a challenging task among
researchers in the data mining and machine learning domain. Real-life problems
inspire researchers to navigate and further improve data processing and
algorithmic approaches to achieve effective and computationally efficient
methods for imbalanced learning. In this paper, we have collected and reviewed
258 peer-reviewed papers from archival journals and conference papers in an
attempt to provide an in-depth review of various approaches in imbalanced
learning from technical and application perspectives. This work aims to provide
a structured review of methods used to address the problem of imbalanced data
in various domains and create a general guideline for researchers in academia
or industry who want to dive into the broad field of machine learning using
large-scale imbalanced data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jafarigol_E/0/1/0/all/0/1&quot;&gt;Elaheh Jafarigol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1&quot;&gt;Theodore Trafalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07918">
<title>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07918</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable policy learning seeks to estimate intelligible decision
policies from observed actions; however, existing models fall short by forcing
a tradeoff between accuracy and interpretability. This tradeoff limits
data-driven interpretations of human decision-making process. e.g. to audit
medical decisions for biases and suboptimal practices, we require models of
decision processes which provide concise descriptions of complex behaviors.
Fundamentally, existing approaches are burdened by this tradeoff because they
represent the underlying decision process as a universal policy, when in fact
human decisions are dynamic and can change drastically with contextual
information. Thus, we propose Contextualized Policy Recovery (CPR), which
re-frames the problem of modeling complex decision processes as a multi-task
learning problem in which complex decision policies are comprised of
context-specific policies. CPR models each context-specific policy as a linear
observation-to-action mapping, and generates new decision models
$\textit{on-demand}$ as contexts are updated with new observations. CPR is
compatible with fully offline and partially observable decision environments,
and can be tailored to incorporate any recurrent black-box model or
interpretable decision model. We assess CPR through studies on simulated and
real data, achieving state-of-the-art performance on the canonical tasks of
predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs.
previous SOTA) and predicting MRI prescription for Alzheimer&apos;s patients
($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive
performance, CPR closes the accuracy gap between interpretable and black-box
methods for policy learning, allowing high-resolution exploration and analysis
of context-specific decision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuschel_J/0/1/0/all/0/1&quot;&gt;Jannik Deuschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellington_C/0/1/0/all/0/1&quot;&gt;Caleb N. Ellington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1&quot;&gt;Benjamin J. Lengerich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yingtao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friederich_P/0/1/0/all/0/1&quot;&gt;Pascal Friederich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07923">
<title>The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07923</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent theoretical work has identified surprisingly simple reasoning
problems, such as checking if two nodes in a graph are connected or simulating
finite-state machines, that are provably unsolvable by standard transformers
that answer immediately after reading their input. However, in practice,
transformers&apos; reasoning can be improved by allowing them to use a &quot;chain of
thought&quot; or &quot;scratchpad&quot;, i.e., generate and condition on a sequence of
intermediate tokens before answering. Motivated by this, we ask: Does such
intermediate generation fundamentally extend the computational power of a
decoder-only transformer? We show that the answer is yes, but the amount of
increase depends crucially on the amount of intermediate generation. For
instance, we find that transformer decoders with a logarithmic number of
decoding steps (w.r.t. the input length) push the limits of standard
transformers only slightly, while a linear number of decoding steps adds a
clear new ability (under standard complexity conjectures): recognizing all
regular languages. Our results also imply that linear steps keep transformer
decoders within context-sensitive languages, and polynomial steps make them
recognize exactly the class of polynomial-time solvable problems -- the first
exact characterization of a type of transformers in terms of standard
complexity classes. Together, our results provide a nuanced framework for
understanding how the length of a transformer&apos;s chain of thought or scratchpad
impacts its reasoning power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1&quot;&gt;William Merrill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1&quot;&gt;Ashish Sabharwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07925">
<title>First-Order Dynamic Optimization for Streaming Convex Costs. (arXiv:2310.07925v1 [math.OC])</title>
<link>http://arxiv.org/abs/2310.07925</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a set of novel optimization algorithms for solving a
class of convex optimization problems with time-varying streaming cost
function. We develop an approach to track the optimal solution with a bounded
error. Unlike the existing results, our algorithm is executed only by using the
first-order derivatives of the cost function which makes it computationally
efficient for optimization with time-varying cost function. We compare our
algorithms to the gradient descent algorithm and show why gradient descent is
not an effective solution for optimization problems with time-varying cost.
Several examples including solving a model predictive control problem cast as a
convex optimization problem with a streaming time-varying cost function
demonstrate our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rostami_M/0/1/0/all/0/1&quot;&gt;M. Rostami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Moradian_H/0/1/0/all/0/1&quot;&gt;H. Moradian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kia_S/0/1/0/all/0/1&quot;&gt;S. S. Kia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07927">
<title>Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables. (arXiv:2310.07927v1 [cond-mat.stat-mech])</title>
<link>http://arxiv.org/abs/2310.07927</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a graph neural network-based learning approach
using an autoencoder setup to derive low-dimensional variables from features
observed in experimental crystal structures. These variables are then biased in
enhanced sampling to observe state-to-state transitions and reliable
thermodynamic weights. Our approach uses simple convolution and pooling
methods. To verify the effectiveness of our protocol, we examined the
nucleation of various allotropes and polymorphs of iron and glycine from their
molten states. Our graph latent variables when biased in well-tempered
metadynamics consistently show transitions between states and achieve accurate
free energy calculations in agreement with experiments, both of which are
indicators of dependable sampling. This underscores the strength and promise of
our graph neural net variables for improved sampling. The protocol shown here
should be applicable for other systems and with other sampling methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Ziyue Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Tiwary_P/0/1/0/all/0/1&quot;&gt;Pratyush Tiwary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07931">
<title>D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07931</link>
<description rdf:parseType="Literal">&lt;p&gt;Analytical theories suggest that higher-quality data can lead to lower test
errors in models trained on a fixed data budget. Moreover, a model can be
trained on a lower compute budget without compromising performance if a dataset
can be stripped of its redundancies. Coreset selection (or data pruning) seeks
to select a subset of the training data so as to maximize the performance of
models trained on this subset, also referred to as coreset. There are two
dominant approaches: (1) geometry-based data selection for maximizing data
diversity in the coreset, and (2) functions that assign difficulty scores to
samples based on training dynamics. Optimizing for data diversity leads to a
coreset that is biased towards easier samples, whereas, selection by difficulty
ranking omits easy samples that are necessary for the training of deep learning
models. This demonstrates that data diversity and importance scores are two
complementary factors that need to be jointly considered during coreset
selection. We represent a dataset as an undirected graph and propose a novel
pruning algorithm, D2 Pruning, that uses forward and reverse message passing
over this dataset graph for coreset selection. D2 Pruning updates the
difficulty scores of each example by incorporating the difficulty of its
neighboring examples in the dataset graph. Then, these updated difficulty
scores direct a graph-based sampling method to select a coreset that
encapsulates both diverse and difficult regions of the dataset space. We
evaluate supervised and self-supervised versions of our method on various
vision and language datasets. Results show that D2 Pruning improves coreset
selection over previous state-of-the-art methods for up to 70% pruning rates.
Additionally, we find that using D2 Pruning for filtering large multimodal
datasets leads to increased diversity in the dataset and improved
generalization of pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1&quot;&gt;Adyasha Maharana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Prateek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07940">
<title>Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07940</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have long touted a vision of the future enabled by a
proliferation of internet-of-things devices, including smart sensors, homes,
and cities. Increasingly, embedding intelligence in such devices involves the
use of deep neural networks. However, their storage and processing requirements
make them prohibitive for cheap, off-the-shelf platforms. Overcoming those
requirements is necessary for enabling widely-applicable smart devices. While
many ways of making models smaller and more efficient have been developed,
there is a lack of understanding of which ones are best suited for particular
scenarios. More importantly for edge platforms, those choices cannot be
analyzed in isolation from cost and user experience. In this work, we
holistically explore how quantization, model scaling, and multi-modality
interact with system components such as memory, sensors, and processors. We
perform this hardware/software co-design from the cost, latency, and
user-experience perspective, and develop a set of guidelines for optimal system
design and model deployment for the most cost-constrained platforms. We
demonstrate our approach using an end-to-end, on-device, biometric user
authentication system using a $20 ESP-EYE board.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Ravit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romaszkan_W/0/1/0/all/0/1&quot;&gt;Wojciech Romaszkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feiqian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Puneet Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07958">
<title>Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2310.07958</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning vulnerability detection has shown promising results in recent
years. However, an important challenge that still blocks it from being very
useful in practice is that the model is not robust under perturbation and it
cannot generalize well over the out-of-distribution (OOD) data, e.g., applying
a trained model to unseen projects in real world. We hypothesize that this is
because the model learned non-robust features, e.g., variable names, that have
spurious correlations with labels. When the perturbed and OOD datasets no
longer have the same spurious features, the model prediction fails. To address
the challenge, in this paper, we introduced causality into deep learning
vulnerability detection. Our approach CausalVul consists of two phases. First,
we designed novel perturbations to discover spurious features that the model
may use to make predictions. Second, we applied the causal learning algorithms,
specifically, do-calculus, on top of existing deep learning models to
systematically remove the use of spurious features and thus promote causal
based prediction. Our results show that CausalVul consistently improved the
model accuracy, robustness and OOD performance for all the state-of-the-art
models and datasets we experimented. To the best of our knowledge, this is the
first work that introduces do calculus based causal learning to software
engineering models and shows it&apos;s indeed useful for improving the model
accuracy, robustness and generalization. Our replication package is located at
https://figshare.com/s/0ffda320dcb96c249ef2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Mahbubur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceka_I/0/1/0/all/0/1&quot;&gt;Ira Ceka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chengzhi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Saikat Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1&quot;&gt;Baishakhi Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1&quot;&gt;Wei Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07969">
<title>CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity. (arXiv:2310.07969v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07969</link>
<description rdf:parseType="Literal">&lt;p&gt;A major obstacle when attempting to train a machine learning system to
evaluate facial clefts is the scarcity of large datasets of high-quality,
ethics board-approved patient images. In response, we have built a deep
learning-based cleft lip generator designed to produce an almost unlimited
number of artificial images exhibiting high-fidelity facsimiles of cleft lip
with wide variation. We undertook a transfer learning protocol testing
different versions of StyleGAN-ADA (a generative adversarial network image
generator incorporating adaptive data augmentation (ADA)) as the base model.
Training images depicting a variety of cleft deformities were pre-processed to
adjust for rotation, scaling, color adjustment and background blurring. The ADA
modification of the primary algorithm permitted construction of our new
generative model while requiring input of a relatively small number of training
images. Adversarial training was carried out using 514 unique frontal
photographs of cleft-affected faces to adapt a pre-trained model based on
70,000 normal faces. The Frechet Inception Distance (FID) was used to measure
the similarity of the newly generated facial images to the cleft training
dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of
Severity Histograms (DISH) measures were also used to assess the performance of
the image generator that we dub CleftGAN. We found that StyleGAN3 with
translation invariance (StyleGAN3-t) performed optimally as a base model.
Generated images achieved a low FID reflecting a close similarity to our
training input dataset of genuine cleft images. Low PPL and DISH measures
reflected a smooth and semantically valid interpolation of images through the
transfer learning process and a similar distribution of severity in the
training and generated images, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayajneh_A/0/1/0/all/0/1&quot;&gt;Abdullah Hayajneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serpedin_E/0/1/0/all/0/1&quot;&gt;Erchin Serpedin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaqfeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Shaqfeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_G/0/1/0/all/0/1&quot;&gt;Graeme Glass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stotland_M/0/1/0/all/0/1&quot;&gt;Mitchell A. Stotland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07970">
<title>Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach. (arXiv:2310.07970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07970</link>
<description rdf:parseType="Literal">&lt;p&gt;Surrogate Optimization (SO) algorithms have shown promise for optimizing
expensive black-box functions. However, their performance is heavily influenced
by hyperparameters related to sampling and surrogate fitting, which poses a
challenge to their widespread adoption. We investigate the impact of
hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive
Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm,
but a generic self-adjusting SO algorithm that dynamically tunes its own
hyperparameters while concurrently optimizing the primary objective function,
without requiring additional evaluations. The aim is to improve the
accessibility, effectiveness, and convergence speed of SO algorithms for
practitioners. Our approach identifies and modifies the most influential
hyperparameters specific to each problem and SO approach, reducing the need for
manual tuning without significantly increasing the computational burden.
Experimental results demonstrate the effectiveness of HASSO in enhancing the
performance of various SO algorithms across different global optimization test
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nezami_N/0/1/0/all/0/1&quot;&gt;Nazanin Nezami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anahideh_H/0/1/0/all/0/1&quot;&gt;Hadis Anahideh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07972">
<title>Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07972</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models enable conditional generation and density modeling
of complex relationships like images and text. However, the nature of the
learned relationships is opaque making it difficult to understand precisely
what relationships between words and parts of an image are captured, or to
predict the effect of an intervention. We illuminate the fine-grained
relationships learned by diffusion models by noticing a precise relationship
between diffusion and information decomposition. Exact expressions for mutual
information and conditional mutual information can be written in terms of the
denoising model. Furthermore, pointwise estimates can be easily estimated as
well, allowing us to ask questions about the relationships between specific
images and captions. Decomposing information even further to understand which
variables in a high-dimensional space carry information is a long-standing
problem. For diffusion models, we show that a natural non-negative
decomposition of mutual information emerges, allowing us to quantify
informative relationships between words and pixels in an image. We exploit
these new relations to measure the compositional understanding of diffusion
models, to do unsupervised localization of objects in images, and to measure
effects when selectively editing images through prompt interventions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xianghao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1&quot;&gt;Ollie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1&quot;&gt;Dani Yogatama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07979">
<title>Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks. (arXiv:2310.07979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07979</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) approaches are increasingly being used to accelerate
combinatorial optimization (CO) problems. We look specifically at the Set Cover
Problem (SCP) and propose Graph-SCP, a graph neural network method that can
augment existing optimization solvers by learning to identify a much smaller
sub-problem that contains the solution space. We evaluate the performance of
Graph-SCP on synthetic weighted and unweighted SCP instances with diverse
problem characteristics and complexities, and on instances from the OR Library,
a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size
by 30-70% and achieves run time speedups up to~25x when compared to commercial
solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve
upon it or even achieve 100% optimality. This is in contrast to fast greedy
solutions that significantly compromise solution quality to achieve guaranteed
polynomial run time. Graph-SCP can generalize to larger problem sizes and can
be used with other conventional or ML-augmented CO solvers to lead to potential
additional run time improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafi_Z/0/1/0/all/0/1&quot;&gt;Zohair Shafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_B/0/1/0/all/0/1&quot;&gt;Benjamin A. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1&quot;&gt;Tina Eliassi-Rad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caceres_R/0/1/0/all/0/1&quot;&gt;Rajmonda S. Caceres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07980">
<title>GRASP: Accelerating Shortest Path Attacks via Graph Attention. (arXiv:2310.07980v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07980</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning (ML) have shown promise in aiding and
accelerating classical combinatorial optimization algorithms. ML-based speed
ups that aim to learn in an end to end manner (i.e., directly output the
solution) tend to trade off run time with solution quality. Therefore,
solutions that are able to accelerate existing solvers while maintaining their
performance guarantees, are of great interest. We consider an APX-hard problem,
where an adversary aims to attack shortest paths in a graph by removing the
minimum number of edges. We propose the GRASP algorithm: Graph Attention
Accelerated Shortest Path Attack, an ML aided optimization algorithm that
achieves run times up to 10x faster, while maintaining the quality of solution
generated. GRASP uses a graph attention network to identify a smaller subgraph
containing the combinatorial solution, thus effectively reducing the input
problem size. Additionally, we demonstrate how careful representation of the
input graph, including node features that correlate well with the optimization
task, can highlight important structure in the optimization solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_Z/0/1/0/all/0/1&quot;&gt;Zohair Shafi. Benjamin A. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1&quot;&gt;Ayan Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1&quot;&gt;Tina Eliassi-Rad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caceres_R/0/1/0/all/0/1&quot;&gt;Rajmonda S. Caceres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07981">
<title>Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach. (arXiv:2310.07981v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07981</link>
<description rdf:parseType="Literal">&lt;p&gt;A flow control system is a critical concept for increasing the production
capacity of manufacturing systems. To solve the scheduling optimization problem
related to the flow control with the aim of improving productivity, existing
methods depend on a heuristic design by domain human experts. Therefore, the
methods require correction, monitoring, and verification by using real
equipment. As system designs increase in complexity, the monitoring time
increases, which decreases the probability of arriving at the optimal design.
As an alternative approach to the heuristic design of flow control systems, the
use of deep reinforcement learning to solve the scheduling optimization problem
has been considered. Although the existing research on reinforcement learning
has yielded excellent performance in some areas, the applicability of the
results to actual FAB such as display and semiconductor manufacturing processes
is not evident so far. To this end, we propose a method to implement a physical
simulation environment and devise a feasible flow control system design using a
transfer robot in display manufacturing through reinforcement learning. We
present a model and parameter setting to build a virtual environment for
different display transfer robots, and training methods of reinforcement
learning on the environment to obtain an optimal scheduling of glass flow
control systems. Its feasibility was verified by using different types of
robots used in the actual process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hwajong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seong-Woo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07983">
<title>RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07983</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed optimization methods with random communication skips are gaining
increasing attention due to their proven benefits in accelerating communication
complexity. Nevertheless, existing research mainly focuses on centralized
communication protocols for strongly convex deterministic settings. In this
work, we provide a decentralized optimization method called RandCom, which
incorporates probabilistic local updates. We analyze the performance of RandCom
in stochastic non-convex, convex, and strongly convex settings and demonstrate
its ability to asymptotically reduce communication overhead by the probability
of communication. Additionally, we prove that RandCom achieves linear speedup
as the number of nodes increases. In stochastic strongly convex settings, we
further prove that RandCom can achieve linear speedup with network-independent
stepsizes. Moreover, we apply RandCom to federated learning and provide
positive results concerning the potential for achieving linear speedup and the
suitability of the probabilistic local update approach for non-convex settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Luyao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alghunaim_S/0/1/0/all/0/1&quot;&gt;Sulaiman A. Alghunaim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Condat_L/0/1/0/all/0/1&quot;&gt;Laurent Condat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jinde Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07985">
<title>Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization. (arXiv:2310.07985v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07985</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural combinatorial optimization (NCO) is a promising learning-based
approach for solving challenging combinatorial optimization problems without
specialized algorithm design by experts. However, most constructive NCO methods
cannot solve problems with large-scale instance sizes, which significantly
diminishes their usefulness for real-world applications. In this work, we
propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong
generalization ability to address this critical issue. The LEHD model can learn
to dynamically capture the relationships between all available nodes of varying
sizes, which is beneficial for model generalization to problems of various
scales. Moreover, we develop a data-efficient training scheme and a flexible
solution construction mechanism for the proposed LEHD model. By training on
small-scale problem instances, the LEHD model can generate nearly optimal
solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle
Routing Problem (CVRP) with up to 1000 nodes, and also generalizes well to
solve real-world TSPLib and CVRPLib problems. These results confirm our
proposed LEHD model can significantly improve the state-of-the-art performance
for constructive NCO. The code is available at
https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingfu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenkun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07987">
<title>Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications. (arXiv:2310.07987v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2310.07987</link>
<description rdf:parseType="Literal">&lt;p&gt;This letter proposes a novel relaying framework, semantic-forward (SF), for
cooperative communications towards the sixth-generation (6G) wireless networks.
The SF relay extracts and transmits the semantic features, which reduces
forwarding payload, and also improves the network robustness against intra-link
errors. Based on the theoretical basis for cooperative communications with side
information and the turbo principle, we design a joint source-channel coding
algorithm to iteratively exchange the extrinsic information for enhancing the
decoding gains at the destination. Surprisingly, simulation results indicate
that even in bad channel conditions, SF relaying can still effectively improve
the recovered information quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wensheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuna Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lixin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsumoto_T/0/1/0/all/0/1&quot;&gt;Tad Matsumoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07990">
<title>Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2310.07990</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Missing data is a common challenge in mass spectrometry-based
metabolomics, which can lead to biased and incomplete analyses. The integration
of whole-genome sequencing (WGS) data with metabolomics data has emerged as a
promising approach to enhance the accuracy of data imputation in metabolomics
studies. Method: In this study, we propose a novel method that leverages the
information from WGS data and reference metabolites to impute unknown
metabolites. Our approach utilizes a multi-view variational autoencoder to
jointly model the burden score, polygenetic risk score (PGS), and linkage
disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature
extraction and missing metabolomics data imputation. By learning the latent
representations of both omics data, our method can effectively impute missing
metabolomics values based on genomic information. Results: We evaluate the
performance of our method on empirical metabolomics datasets with missing
values and demonstrate its superiority compared to conventional imputation
techniques. Using 35 template metabolites derived burden scores, PGS and
LD-pruned SNPs, the proposed methods achieved r2-scores &amp;gt; 0.01 for 71.55% of
metabolites. Conclusion: The integration of WGS data in metabolomics imputation
not only improves data completeness but also enhances downstream analyses,
paving the way for more comprehensive and accurate investigations of metabolic
pathways and disease associations. Our findings offer valuable insights into
the potential benefits of utilizing WGS data for metabolomics data imputation
and underscore the importance of leveraging multi-modal data integration in
precision medicine research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kuan-Jui Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xuewei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sha_Q/0/1/0/all/0/1&quot;&gt;Qiuying Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhe Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tian Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiu_C/0/1/0/all/0/1&quot;&gt;Chuan Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lan Juan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lindong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Weihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hong-Wen Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07996">
<title>Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07996</link>
<description rdf:parseType="Literal">&lt;p&gt;This work identifies a simple pre-training mechanism that leads to
representations exhibiting better continual and transfer learning. This
mechanism -- the repeated resetting of weights in the last layer, which we
nickname &quot;zapping&quot; -- was originally designed for a meta-continual-learning
procedure, yet we show it is surprisingly applicable in many settings beyond
both meta-learning and continual learning. In our experiments, we wish to
transfer a pre-trained image classifier to a new set of classes, in a few
shots. We show that our zapping procedure results in improved transfer accuracy
and/or more rapid adaptation in both standard fine-tuning and continual
learning settings, while being simple to implement and computationally
efficient. In many cases, we achieve performance on par with state of the art
meta-learning without needing the expensive higher-order gradients, by using a
combination of zapping and sequential learning. An intuitive explanation for
the effectiveness of this zapping procedure is that representations trained
with repeated zapping learn features that are capable of rapidly adapting to
newly initialized classifiers. Such an approach may be considered a
computationally cheaper type of, or alternative to, meta-learning rapidly
adaptable features with higher-order gradients. This adds to recent work on the
usefulness of resetting neural network parameters during training, and invites
further investigation of this mechanism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frati_L/0/1/0/all/0/1&quot;&gt;Lapo Frati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traft_N/0/1/0/all/0/1&quot;&gt;Neil Traft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1&quot;&gt;Nick Cheney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07999">
<title>LEMON: Lossless model expansion. (arXiv:2310.07999v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07999</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling of deep neural networks, especially Transformers, is pivotal for
their surging performance and has further led to the emergence of sophisticated
reasoning capabilities in foundation models. Such scaling generally requires
training large models from scratch with random initialization, failing to
leverage the knowledge acquired by their smaller counterparts, which are
already resource-intensive to obtain. To tackle this inefficiency, we present
$\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a
recipe to initialize scaled models using the weights of their smaller but
pre-trained counterparts. This is followed by model training with an optimized
learning rate scheduler tailored explicitly for the scaled models,
substantially reducing the training time compared to training from scratch.
Notably, LEMON is versatile, ensuring compatibility with various network
structures, including models like Vision Transformers and BERT. Our empirical
results demonstrate that LEMON reduces computational costs by 56.7% for Vision
Transformers and 33.2% for BERT when compared to training from scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yite Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jiahao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hanlin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haibin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruoyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08012">
<title>AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE. (arXiv:2310.08012v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08012</link>
<description rdf:parseType="Literal">&lt;p&gt;Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS
involves polynomial approximation of unsupported non-linear activation
functions. However, existing approaches have three main limitations: 1)
Inflexibility: The polynomial approximation and associated homomorphic
evaluation architecture are customized manually for each CNN architecture and
do not generalize to other networks. 2) Suboptimal Approximation: Each
activation function is approximated instead of the function represented by the
CNN. 3) Restricted Design: Either high-degree or low-degree polynomial
approximations are used. The former retains high accuracy but slows down
inference due to bootstrapping operations, while the latter accelerates
ciphertext inference but compromises accuracy. To address these limitations, we
present AutoFHE, which automatically adapts standard CNNs for secure inference
under RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial
activation functions, which are optimized jointly with the homomorphic
evaluation architecture in terms of the placement of bootstrapping operations.
The problem is modeled within a multi-objective optimization framework to
maximize accuracy and minimize the number of bootstrapping operations. AutoFHE
can be applied flexibly on any CNN architecture, and it provides diverse
solutions that span the trade-off between accuracy and latency. Experimental
evaluation over RNS-CKKS encrypted CIFAR datasets shows that AutoFHE
accelerates secure inference by $1.32\times$ to $1.8\times$ compared to methods
employing high-degree polynomials. It also improves accuracy by up to 2.56%
compared to methods using low-degree polynomials. Lastly, AutoFHE accelerates
inference and improves accuracy by $103\times$ and 3.46%, respectively,
compared to CNNs under TFHE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ao_W/0/1/0/all/0/1&quot;&gt;Wei Ao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08015">
<title>Why Train More? Effective and Efficient Membership Inference via Memorization. (arXiv:2310.08015v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08015</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership Inference Attacks (MIAs) aim to identify specific data samples
within the private training dataset of machine learning models, leading to
serious privacy violations and other sophisticated threats. Many practical
black-box MIAs require query access to the data distribution (the same
distribution where the private data is drawn) to train shadow models. By doing
so, the adversary obtains models trained &quot;with&quot; or &quot;without&quot; samples drawn from
the distribution, and analyzes the characteristics of the samples under
consideration. The adversary is often required to train more than hundreds of
shadow models to extract the signals needed for MIAs; this becomes the
computational overhead of MIAs. In this paper, we propose that by strategically
choosing the samples, MI adversaries can maximize their attack success while
minimizing the number of shadow models. First, our motivational experiments
suggest memorization as the key property explaining disparate sample
vulnerability to MIAs. We formalize this through a theoretical bound that
connects MI advantage with memorization. Second, we show sample complexity
bounds that connect the number of shadow models needed for MIAs with
memorization. Lastly, we confirm our theoretical arguments with comprehensive
experiments; by utilizing samples with high memorization scores, the adversary
can (a) significantly improve its efficacy regardless of the MIA used, and (b)
reduce the number of shadow models by nearly two orders of magnitude compared
to state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jihye Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1&quot;&gt;Shruti Tople&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1&quot;&gt;Varun Chandrasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1&quot;&gt;Somesh Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08019">
<title>Robust 1-bit Compressed Sensing with Iterative Hard Thresholding. (arXiv:2310.08019v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2310.08019</link>
<description rdf:parseType="Literal">&lt;p&gt;In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector
$x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of
linear measurements that are quantized to just their signs, i.e., from
measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this
paper, we study a noisy version where a fraction of the measurements can be
flipped, potentially by an adversary. In particular, we analyze the Binary
Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a
properly defined loss function used for 1-bit compressed sensing, in this noisy
setting. It is known from recent results that, with
$\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an
estimate within $\epsilon$ error. This result is optimal and universal, meaning
one set of measurements work for all sparse vectors. In this paper, we show
that BIHT also provides better results than all known methods for the noisy
setting. We show that when up to $\tau$-fraction of the sign measurements are
incorrect (adversarial error), with the same number of measurements as before,
BIHT agnostically provides an estimate of $x$ within an
$\tilde{O}(\epsilon+\tau)$ error, maintaining the universality of measurements.
This establishes stability of iterative hard thresholding in the presence of
measurement error. To obtain the result, we use the restricted approximate
invertibility of Gaussian matrices, as well as a tight analysis of the
high-dimensional geometry of the adversarially corrupted measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsumoto_N/0/1/0/all/0/1&quot;&gt;Namiko Matsumoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1&quot;&gt;Arya Mazumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08031">
<title>Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08031</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing interest in machine learning problems over graphs with additional
node information such as texts, images, or labels has popularized methods that
require the costly operation of processing the entire graph. Yet, little effort
has been made to the development of fast local methods (i.e. without accessing
the entire graph) that extract useful information from such data. To that end,
we propose a study of local graph clustering using noisy node labels as a proxy
for additional node information. In this setting, nodes receive initial binary
labels based on cluster affiliation: 1 if they belong to the target cluster and
0 otherwise. Subsequently, a fraction of these labels is flipped. We
investigate the benefits of incorporating noisy labels for local graph
clustering. By constructing a weighted graph with such labels, we study the
performance of graph diffusion-based local clustering method on both the
original and the weighted graphs. From a theoretical perspective, we consider
recovering an unknown target cluster with a single seed node in a random graph
with independent noisy node labels. We provide sufficient conditions on the
label noise under which, with high probability, using diffusion in the weighted
graph yields a more accurate recovery of the target cluster. This approach
proves more effective than using the given labels alone or using diffusion in
the label-free original graph. Empirically, we show that reliable node labels
can be obtained with just a few samples from an attributed graph. Moreover,
utilizing these labels via diffusion in the weighted graph leads to
significantly better local clustering performance across several real-world
datasets, improving F1 scores by up to 13%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luca_A/0/1/0/all/0/1&quot;&gt;Artur Back de Luca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1&quot;&gt;Kimon Fountoulakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shenghao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08036">
<title>ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification. (arXiv:2310.08036v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2310.08036</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research works have proposed machine learning models for classifying
IoT devices connected to a network. However, there is still a practical
challenge of not having all devices (and hence their traffic) available during
the training of a model. This essentially means, during the operational phase,
we need to classify new devices not seen during the training phase. To address
this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based
on self-attention for classifying both seen and unseen devices. ZEST consists
of i) a self-attention based network feature extractor, termed SANE, for
extracting latent space representations of IoT traffic, ii) a generative model
that trains a decoder using latent features to generate pseudo data, and iii) a
supervised model that is trained on the generated pseudo data for classifying
devices. We carry out extensive experiments on real IoT traffic data; our
experiments demonstrate i) ZEST achieves significant improvement (in terms of
accuracy) over the baselines; ii) ZEST is able to better extract meaningful
representations than LSTM which has been commonly used for modeling network
traffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Binghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gysel_P/0/1/0/all/0/1&quot;&gt;Philipp Gysel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divakaran_D/0/1/0/all/0/1&quot;&gt;Dinil Mon Divakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1&quot;&gt;Mohan Gurusamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08038">
<title>Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08038</link>
<description rdf:parseType="Literal">&lt;p&gt;In continual learning, the learner learns multiple tasks in sequence, with
data being acquired only once for each task. Catastrophic forgetting is a major
challenge to continual learning. To reduce forgetting, some existing
rehearsal-based methods use episodic memory to replay samples of previous
tasks. However, in the process of knowledge integration when learning a new
task, this strategy also suffers from catastrophic forgetting due to an
imbalance between old and new knowledge. To address this problem, we propose a
novel replay strategy called Manifold Expansion Replay (MaER). We argue that
expanding the implicit manifold of the knowledge representation in the episodic
memory helps to improve the robustness and expressiveness of the model. To this
end, we propose a greedy strategy to keep increasing the diameter of the
implicit manifold represented by the knowledge in the buffer during memory
management. In addition, we introduce Wasserstein distance instead of cross
entropy as distillation loss to preserve previous knowledge. With extensive
experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show
that the proposed method significantly improves the accuracy in continual
learning setup, outperforming the state of the arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zihao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yufei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xian Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08039">
<title>Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.08039</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial systems such as recommender systems and online advertising, have
been widely equipped with multi-stage architectures, which are divided into
several cascaded modules, including matching, pre-ranking, ranking and
re-ranking. As a critical bridge between matching and ranking, existing
pre-ranking approaches mainly endure sample selection bias (SSB) problem owing
to ignoring the entire-chain data dependence, resulting in sub-optimal
performances. In this paper, we rethink pre-ranking system from the perspective
of the entire sample space, and propose Entire-chain Cross-domain Models (ECM),
which leverage samples from the whole cascaded stages to effectively alleviate
SSB problem. Besides, we design a fine-grained neural structure named ECMM to
further improve the pre-ranking accuracy. Specifically, we propose a
cross-domain multi-tower neural network to comprehensively predict for each
stage result, and introduce the sub-networking routing strategy with $L0$
regularization to reduce computational costs. Evaluations on real-world
large-scale traffic logs demonstrate that our pre-ranking models outperform
SOTA methods while time consumption is maintained within an acceptable level,
which achieves better trade-off between efficiency and effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jinbo Song&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruoran Huang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyang Wang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingming Chen&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yafei Yao&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chaosheng Fan&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Changping Peng&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhangang Lin&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinghe Hu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jingping Shao&lt;/a&gt; (1) ((1) Marketing and Commercialization Center, JD.com)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08040">
<title>SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution Detection. (arXiv:2310.08040v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08040</link>
<description rdf:parseType="Literal">&lt;p&gt;Current techniques for Out-of-Distribution (OoD) detection predominantly rely
on quantifying predictive uncertainty and incorporating model regularization
during the training phase, using either real or synthetic OoD samples. However,
methods that utilize real OoD samples lack exploration and are prone to overfit
the OoD samples at hand. Whereas synthetic samples are often generated based on
features extracted from training data, rendering them less effective when the
training and OoD data are highly overlapped in the feature space. In this work,
we propose a Wasserstein-score-based generative adversarial training scheme to
enhance OoD detection accuracy, which, for the first time, performs data
augmentation and exploration simultaneously under the supervision of limited
OoD samples. Specifically, the generator explores OoD spaces and generates
synthetic OoD samples using feedback from the discriminator, while the
discriminator exploits both the observed and synthesized samples for OoD
detection using a predefined Wasserstein score. We provide theoretical
guarantees that the optimal solutions of our generative scheme are
statistically achievable through adversarial training in empirical settings. We
then demonstrate that the proposed method outperforms state-of-the-art
techniques on various computer vision datasets and exhibits superior
generalizability to unseen OoD data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenbo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nouiehed_M/0/1/0/all/0/1&quot;&gt;Maher Nouiehed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1&quot;&gt;Raed Al Kontar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Judy Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08041">
<title>QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08041</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) excel in NLP, but their demands hinder their
widespread deployment. While Quantization-Aware Training (QAT) offers a
solution, its extensive training costs make Post-Training Quantization (PTQ) a
more practical approach for LLMs. In existing studies, activation outliers in
particular channels are identified as the bottleneck to PTQ accuracy. They
propose to transform the magnitudes from activations to weights, which however
offers limited alleviation or suffers from unstable gradients, resulting in a
severe performance drop at low-bitwidth. In this paper, we propose QLLM, an
accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM
introduces an adaptive channel reassembly technique that reallocates the
magnitude of outliers to other channels, thereby mitigating their impact on the
quantization range. This is achieved by channel disassembly and channel
assembly, which first breaks down the outlier channels into several
sub-channels to ensure a more balanced distribution of activation magnitudes.
Then similar channels are merged to maintain the original channel number for
efficiency. Additionally, an adaptive strategy is designed to autonomously
determine the optimal number of sub-channels for channel disassembly. To
further compensate for the performance loss caused by quantization, we propose
an efficient tuning method that only learns a small number of low-rank weights
while freezing the pre-trained quantized model. After training, these low-rank
parameters can be fused into the frozen weights without affecting inference.
Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate
quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B
within 10 hours on a single A100-80G GPU, outperforming the previous
state-of-the-art method by 7.89% on the average accuracy across five zero-shot
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Ruihao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiuying Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08049">
<title>Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08049</link>
<description rdf:parseType="Literal">&lt;p&gt;What is the relationship between model architecture and the ability to
perform in-context learning? In this empirical study, we take the first steps
towards answering this question. In particular, we evaluate fifteen model
architectures across a suite of synthetic in-context learning tasks. The
selected architectures represent a broad range of paradigms, including
recurrent and convolution-based neural networks, transformers, and emerging
attention alternatives. We discover that all considered architectures can
perform in-context learning under certain conditions. However, contemporary
architectures are found to be the best performing, especially as task
complexity grows. Additionally, our follow-up experiments delve into various
factors that influence in-context learning. We observe varied sensitivities
among architectures with respect to hyperparameter settings. Our study of
training dynamics reveals that certain architectures exhibit a smooth,
progressive learning trajectory, while others demonstrate periods of stagnation
followed by abrupt mastery of the task. Finally, and somewhat surprisingly, we
find that several emerging attention alternatives are more robust in-context
learners than transformers; since such approaches have constant-sized memory
footprints at inference time, this result opens the future possibility of
scaling up in-context learning to vastly larger numbers of in-context examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ivan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1&quot;&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08051">
<title>LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08051</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-Computer Interfaces (BCIs) are a groundbreaking technology for
interacting with external devices using brain signals. Despite advancements,
electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like
amplitude and phase variability, and complex spatial correlations, with a need
for smaller model size and faster inference. This study introduces the LGL-BCI
framework, employing a Geometric Deep Learning Framework for EEG processing in
non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD)
Manifold space. LGL-BCI offers robust EEG data representation and captures
spatial correlations. We propose an EEG channel selection solution via a
feature decomposition algorithm to reduce SPD matrix dimensionality, with a
lossless transformation boosting inference speed. Extensive experiments show
LGL-BCI&apos;s superior accuracy and efficiency compared to current solutions,
highlighting geometric deep learning&apos;s potential in MI-BCI applications. The
efficiency, assessed on two public EEG datasets and two real-world EEG devices,
significantly outperforms the state-of-the-art solution in accuracy ($82.54\%$
versus $62.22\%$) with fewer parameters (64.9M compared to 183.7M).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jianchao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1&quot;&gt;Quan Z. Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08056">
<title>Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08056</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from Label Proportions (LLP) is a learning problem where only
aggregate level labels are available for groups of instances, called bags,
during training, and the aim is to get the best performance at the
instance-level on the test data. This setting arises in domains like
advertising and medicine due to privacy considerations. We propose a novel
algorithmic framework for this problem that iteratively performs two main
steps. For the first step (Pseudo Labeling) in every iteration, we define a
Gibbs distribution over binary instance labels that incorporates a) covariate
information through the constraint that instances with similar covariates
should have similar labels and b) the bag level aggregated label. We then use
Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo
labels. In the second step (Embedding Refinement), we use the pseudo labels to
provide supervision for a learner that yields a better embedding. Further, we
iterate on the two steps again by using the second step&apos;s embeddings as new
covariates for the next iteration. In the final iteration, a classifier is
trained using the pseudo labels. Our algorithm displays strong gains against
several SOTA baselines (up to 15%) for the LLP Binary Classification problem on
various dataset types - tabular and Image. We achieve these improvements with
minimal computational overhead above standard supervised learning due to Belief
Propagation, for large bag sizes, even for a million samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreyas Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1&quot;&gt;Navodita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sareen_S/0/1/0/all/0/1&quot;&gt;Shubhi Sareen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1&quot;&gt;Aravindan Raghuveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08061">
<title>ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking. (arXiv:2310.08061v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2310.08061</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the docking between proteins and ligands is a crucial and
challenging task for drug discovery. However, traditional docking methods
mainly rely on scoring functions, and deep learning-based docking approaches
usually neglect the 3D spatial information of proteins and ligands, as well as
the graph-level features of ligands, which limits their performance. To address
these limitations, we propose an equivariant transformer neural network for
protein-ligand docking pose prediction. Our approach involves the fusion of
ligand graph-level features by feature processing, followed by the learning of
ligand and protein representations using our proposed TAMformer module.
Additionally, we employ an iterative optimization approach based on the
predicted distance matrix to generate refined ligand poses. The experimental
results on real datasets show that our model can achieve state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yi_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xu Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yatao Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ou_Yang_L/0/1/0/all/0/1&quot;&gt;Le Ou-Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peilin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08069">
<title>Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2310.08069</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, contrastive learning has become a key component in fine-tuning code
search models for software development efficiency and effectiveness. It pulls
together positive code snippets while pushing negative samples away given
search queries. Among contrastive learning, InfoNCE is the most widely used
loss function due to its better performance. However, the following problems in
negative samples of InfoNCE may deteriorate its representation learning: 1) The
existence of false negative samples in large code corpora due to duplications.
2). The failure to explicitly differentiate between the potential relevance of
negative samples. As an example, a bubble sorting algorithm example is less
``negative&apos;&apos; than a file saving function for the quick sorting algorithm query.
In this paper, we tackle the above problems by proposing a simple yet effective
Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss
function, we apply three methods to estimate the weights of negative pairs and
show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.
Theoretically, we analyze the effects of Soft-InfoNCE on controlling the
distribution of learnt code representations and on deducing a more precise
mutual information estimation. We furthermore discuss the superiority of
proposed loss functions with other design alternatives. Extensive experiments
demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods
under state-of-the-art code search models on a large-scale public dataset
consisting of six programming languages. Source code is available at
\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haochen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1&quot;&gt;Chunyan Miao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08070">
<title>Tight Time-Space Lower Bounds for Constant-Pass Learning. (arXiv:2310.08070v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08070</link>
<description rdf:parseType="Literal">&lt;p&gt;In his breakthrough paper, Raz showed that any parity learning algorithm
requires either quadratic memory or an exponential number of samples [FOCS&apos;16,
JACM&apos;19]. A line of work that followed extended this result to a large class of
learning problems. Until recently, all these results considered learning in the
streaming model, where each sample is drawn independently, and the learner is
allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC&apos;19]
considered a stronger model, allowing multiple passes over the stream. In the
$2$-pass model, they showed that learning parities of size $n$ requires either
a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}}$ samples. (Their result
also generalizes to other learning problems.)
&lt;/p&gt;
&lt;p&gt;In this work, for any constant $q$, we prove tight memory-sample lower bounds
for any parity learning algorithm that makes $q$ passes over the stream of
samples. We show that such a learner requires either $\Omega(n^{2})$ memory
size or at least $2^{\Omega(n)}$ samples. Beyond establishing a tight lower
bound, this is the first non-trivial lower bound for $q$-pass learning for any
$q\ge 3$. Similar to prior work, our results extend to any learning problem
with many nearly-orthogonal concepts.
&lt;/p&gt;
&lt;p&gt;We complement the lower bound with an upper bound, showing that parity
learning with $q$ passes can be done efficiently with $O(n^2/\log q)$ memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xin Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tal_A/0/1/0/all/0/1&quot;&gt;Avishay Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongxun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junzhao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08071">
<title>Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation. (arXiv:2310.08071v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08071</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the great progress of unsupervised domain adaptation (UDA) with the
deep neural networks, current UDA models are opaque and cannot provide
promising explanations, limiting their applications in the scenarios that
require safe and controllable model decisions. At present, a surge of work
focuses on designing deep interpretable methods with adequate data annotations
and only a few methods consider the distributional shift problem. Most existing
interpretable UDA methods are post-hoc ones, which cannot facilitate the model
learning process for performance enhancement. In this paper, we propose an
inherently interpretable method, named Transferable Conceptual Prototype
Learning (TCPL), which could simultaneously interpret and improve the processes
of knowledge transfer and decision-making in UDA. To achieve this goal, we
design a hierarchically prototypical module that transfers categorical basic
concepts from the source domain to the target domain and learns domain-shared
prototypes for explaining the underlying reasoning process. With the learned
transferable prototypes, a self-predictive consistent pseudo-label strategy
that fuses confidence, predictions, and prototype information, is designed for
selecting suitable target samples for pseudo annotations and gradually
narrowing down the domain gap. Comprehensive experiments show that the proposed
method can not only provide effective and intuitive explanations but also
outperform previous state-of-the-arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinhong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08073">
<title>Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks. (arXiv:2310.08073v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08073</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network pruning has shown to be an effective technique for reducing
the network size, trading desirable properties like generalization and
robustness to adversarial attacks for higher sparsity. Recent work has claimed
that adversarial pruning methods can produce sparse networks while also
preserving robustness to adversarial examples. In this work, we first
re-evaluate three state-of-the-art adversarial pruning methods, showing that
their robustness was indeed overestimated. We then compare pruned and dense
versions of the same models, discovering that samples on thin ice, i.e., closer
to the unpruned model&apos;s decision boundary, are typically misclassified after
pruning. We conclude by discussing how this intuition may lead to designing
more effective adversarial pruning methods in future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piras_G/0/1/0/all/0/1&quot;&gt;Giorgio Piras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1&quot;&gt;Maura Pintor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1&quot;&gt;Ambra Demontis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1&quot;&gt;Battista Biggio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08078">
<title>To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer. (arXiv:2310.08078v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;Choosing an appropriate tokenization scheme is often a bottleneck in
low-resource cross-lingual transfer. To understand the downstream implications
of text representation choices, we perform a comparative analysis on language
models having diverse text representation modalities including 2
segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model
(\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we
propose a scoring Language Quotient (LQ) metric capable of providing a weighted
representation of both zero-shot and few-shot evaluation combined. Utilizing
this metric, we perform experiments comprising 19 source languages and 133
target languages on three tasks (POS tagging, Dependency parsing, and NER). Our
analysis reveals that image-based models excel in cross-lingual transfer when
languages are closely related and share visually similar scripts. However, for
tasks biased toward word meaning (POS, NER), segmentation-based models prove to
be superior. Furthermore, in dependency parsing tasks where word relationships
play a crucial role, models with their character-level focus, outperform
others. Finally, we propose a recommendation scheme based on our findings to
guide model selection according to task and language requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Mushfiqur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakib_F/0/1/0/all/0/1&quot;&gt;Fardin Ahsan Sakib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1&quot;&gt;Fahim Faisal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1&quot;&gt;Antonios Anastasopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08087">
<title>A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification. (arXiv:2310.08087v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.08087</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) methods adopt efficient communication technologies to
distribute machine learning tasks across edge devices, reducing the overhead in
terms of data storage and computational complexity compared to centralized
solutions. Rather than moving large data volumes from producers (sensors,
machines) to energy-hungry data centers, raising environmental concerns due to
resource demands, FL provides an alternative solution to mitigate the energy
demands of several learning tasks while enabling new Artificial Intelligence of
Things (AIoT) applications. This paper proposes a framework for real-time
monitoring of the energy and carbon footprint impacts of FL systems. The carbon
tracking tool is evaluated for consensus (fully decentralized) and classical FL
policies. For the first time, we present a quantitative evaluation of different
computationally and communication efficient FL methods from the perspectives of
energy consumption and carbon equivalent emissions, suggesting also general
guidelines for energy-efficient design. Results indicate that consensus-driven
FL implementations should be preferred for limiting carbon emissions when the
energy efficiency of the communication is low (i.e., &amp;lt; 25 Kbit/Joule). Besides,
quantization and sparsification operations are shown to strike a balance
between learning performances and energy consumption, leading to sustainable FL
designs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barbieri_L/0/1/0/all/0/1&quot;&gt;Luca Barbieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Savazzi_S/0/1/0/all/0/1&quot;&gt;Stefano Savazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kianoush_S/0/1/0/all/0/1&quot;&gt;Sanaz Kianoush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nicoli_M/0/1/0/all/0/1&quot;&gt;Monica Nicoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Serio_L/0/1/0/all/0/1&quot;&gt;Luigi Serio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08088">
<title>Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach. (arXiv:2310.08088v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08088</link>
<description rdf:parseType="Literal">&lt;p&gt;In many cases, a machine learning model must learn to correctly predict a few
data points with particular values of interest in a broader range of data where
many target values are zero. Zero-inflated data can be found in diverse
scenarios, such as lumpy and intermittent demands, power consumption for home
appliances being turned on and off, impurities measurement in distillation
processes, and even airport shuttle demand prediction. The presence of zeroes
affects the models&apos; learning and may result in poor performance. Furthermore,
zeroes also distort the metrics used to compute the model&apos;s prediction quality.
This paper showcases two real-world use cases (home appliances classification
and airport shuttle demand prediction) where a hierarchical model applied in
the context of zero-inflated data leads to excellent results. In particular,
for home appliances classification, the weighted average of Precision, Recall,
F1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively.
Furthermore, it is estimated that the proposed approach is also four times more
energy efficient than the SOTA approach against which it was compared to.
Two-fold models performed best in all cases when predicting airport shuttle
demand, and the difference against other models has been proven to be
statistically significant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozanec_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#x17e;e M. Ro&amp;#x17e;anec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petelin_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#x161;per Petelin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertalanic_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; Bertalani&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerar_G/0/1/0/all/0/1&quot;&gt;Gregor Cerar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gucek_M/0/1/0/all/0/1&quot;&gt;Marko Gu&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papa_G/0/1/0/all/0/1&quot;&gt;Gregor Papa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1&quot;&gt;Dunja Mladeni&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08091">
<title>Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08091</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal difference learning (TD) is a foundational concept in reinforcement
learning (RL), aimed at efficiently assessing a policy&apos;s value function.
TD($\lambda$), a potent variant, incorporates a memory trace to distribute the
prediction error into the historical context. However, this approach often
neglects the significance of historical states and the relative importance of
propagating the TD error, influenced by challenges such as visitation imbalance
or outcome noise. To address this, we propose a novel TD algorithm named
discerning TD learning (DTD), which allows flexible emphasis
functions$-$predetermined or adapted during training$-$to allocate efforts
effectively across states. We establish the convergence properties of our
method within a specific class of emphasis functions and showcase its promising
potential for adaptation to deep RL contexts. Empirical results underscore that
employing a judicious emphasis function not only improves value estimation but
also expedites learning across diverse scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianfei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08096">
<title>ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets. (arXiv:2310.08096v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08096</link>
<description rdf:parseType="Literal">&lt;p&gt;Public and private actors struggle to assess the vast amounts of information
about sustainability commitments made by various institutions. To address this
problem, we create a novel tool for automatically detecting corporate,
national, and regional net zero and reduction targets in three steps. First, we
introduce an expert-annotated data set with 3.5K text samples. Second, we train
and release ClimateBERT-NetZero, a natural language classifier to detect
whether a text contains a net zero or reduction target. Third, we showcase its
analysis potential with two use cases: We first demonstrate how
ClimateBERT-NetZero can be combined with conventional question-answering (Q&amp;amp;A)
models to analyze the ambitions displayed in net zero and reduction targets.
Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call
transcripts and outline how communication patterns evolve over time. Our
experiments demonstrate promising pathways for extracting and analyzing net
zero and emission reduction targets at scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schimanski_T/0/1/0/all/0/1&quot;&gt;Tobias Schimanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1&quot;&gt;Julia Bingler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyslop_C/0/1/0/all/0/1&quot;&gt;Camilla Hyslop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1&quot;&gt;Mathias Kraus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1&quot;&gt;Markus Leippold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08100">
<title>Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08100</link>
<description rdf:parseType="Literal">&lt;p&gt;Future sequence represents the outcome after executing the action into the
environment. When driven by the information-theoretic concept of mutual
information, it seeks maximally informative consequences. Explicit outcomes may
vary across state, return, or trajectory serving different purposes such as
credit assignment or imitation learning. However, the inherent nature of
incorporating intrinsic motivation with reward maximization is often neglected.
In this work, we propose a variational approach to jointly learn the necessary
quantity for estimating the mutual information and the dynamics model,
providing a general framework for incorporating different forms of outcomes of
interest. Integrated into a policy iteration scheme, our approach guarantees
convergence to the optimal policy. While we mainly focus on theoretical
analysis, our approach opens the possibilities of leveraging intrinsic control
with model learning to enhance sample efficiency and incorporate uncertainty of
the environment into decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianfei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08109">
<title>Overview of Physics-Informed Machine Learning Inversion of Geophysical Data. (arXiv:2310.08109v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2310.08109</link>
<description rdf:parseType="Literal">&lt;p&gt;We review four types of algorithms for physics-informed machine learning
(PIML) inversion of geophysical data. The unifying equation is given by the
joint objective function $\epsilon$:
&lt;/p&gt;
&lt;p&gt;\begin{eqnarray} \epsilon^{||-PIML}&amp;amp;=&amp;amp;\lambda_1 \overbrace{||{\bf
W}^{ML}({\bf H}_{{\bf w}} {\bf d}^{obs}-{\bf m})||^2}^{NN} + \lambda_2
\overbrace{{||{\bf W}^{FWI}({\bf L} {\bf m}-{\bf d}^{obs})||^2}}^{FWI} ~+
\nonumber\\ \nonumber\\ &amp;amp;&amp;amp; + ~~Regularizer, \label{PIML.eq120}
\end{eqnarray}where the optimal model ${\bf m}^*$ and weights $\bf w^*$
minimize $\epsilon$. Here, The matrix weights are given by the boldface symbol
$\bf W$, and full waveform inversion (FWI) is typically computed using a
finite-difference solution of the wave equation, where $\bf L$ represents the
forward modeling operation of the wave equation as a function of the model $\bf
m$. Also, a fully-connected neural network (NN) is used to compute the model
${\bf H_w}{\bf d}^{obs} \approx \bf m$ from the observed input data ${\bf
d}^{obs}$. The selection of weights $\lambda_i$ and the NN operations determine
one of four different PIML algorithms.
&lt;/p&gt;
&lt;p&gt;PIML offers potential advantages over standard FWI through its enhanced
ability to avoid local minima and the option to locally train the inversion
operator, minimizing the requirement for extensive training data for global
applicability. However, the effectiveness of PIML relies on the similarity
between the test and trained data. Nevertheless, a possible strategy to
overcome this limitation involves initial pretraining of a PIML architecture
with data from a broader region, followed by fine-tuning for specific data-a
method reminiscent of the way large language models are pretrained and adapted
for various tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Schuster_G/0/1/0/all/0/1&quot;&gt;Gerard T. Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shihang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08122">
<title>Core-sets for Fair and Diverse Data Summarization. (arXiv:2310.08122v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2310.08122</link>
<description rdf:parseType="Literal">&lt;p&gt;We study core-set construction algorithms for the task of Diversity
Maximization under fairness/partition constraint. Given a set of points $P$ in
a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the
goal of this problem is to pick $k_i$ points from each group $i$ such that the
overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider
two natural diversity measures: sum-of-pairwise distances and
sum-of-nearest-neighbor distances, and show improved core-set construction
algorithms with respect to these measures. More precisely, we show the first
constant factor core-set w.r.t. sum-of-pairwise distances whose size is
independent of the size of the dataset and the aspect ratio. Second, we show
the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we
run several experiments showing the effectiveness of our core-set approach. In
particular, we apply constrained diversity maximization to summarize a set of
timed messages that takes into account the messages&apos; recency. Specifically, the
summary should include more recent messages compared to older ones. This is a
real task in one of the largest communication platforms, affecting the
experience of hundreds of millions daily active users. By utilizing our
core-set method for this task, we achieve a 100x speed-up while losing the
diversity by only a few percent. Moreover, our approach allows us to improve
the space usage of the algorithm in the streaming setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahabadi_S/0/1/0/all/0/1&quot;&gt;Sepideh Mahabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trajanovski_S/0/1/0/all/0/1&quot;&gt;Stojan Trajanovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08137">
<title>Counterfactual Explanations for Time Series Forecasting. (arXiv:2310.08137v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08137</link>
<description rdf:parseType="Literal">&lt;p&gt;Among recent developments in time series forecasting methods, deep
forecasting models have gained popularity as they can utilize hidden feature
patterns in time series to improve forecasting performance. Nevertheless, the
majority of current deep forecasting models are opaque, hence making it
challenging to interpret the results. While counterfactual explanations have
been extensively employed as a post-hoc approach for explaining classification
models, their application to forecasting models still remains underexplored. In
this paper, we formulate the novel problem of counterfactual generation for
time series forecasting, and propose an algorithm, called ForecastCF, that
solves the problem by applying gradient-based perturbations to the original
time series. ForecastCF guides the perturbations by applying constraints to the
forecasted values to obtain desired prediction outcomes. We experimentally
evaluate ForecastCF using four state-of-the-art deep model architectures and
compare to two baselines. Our results show that ForecastCF outperforms the
baseline in terms of counterfactual validity and data manifold closeness.
Overall, our findings suggest that ForecastCF can generate meaningful and
relevant counterfactual explanations for various forecasting tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miliou_I/0/1/0/all/0/1&quot;&gt;Ioanna Miliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samsten_I/0/1/0/all/0/1&quot;&gt;Isak Samsten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papapetrou_P/0/1/0/all/0/1&quot;&gt;Panagiotis Papapetrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08138">
<title>Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction. (arXiv:2310.08138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08138</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic flow prediction is one of the most fundamental tasks of intelligent
transportation systems. The complex and dynamic spatial-temporal dependencies
make the traffic flow prediction quite challenging. Although existing
spatial-temporal graph neural networks hold prominent, they often encounter
challenges such as (1) ignoring the fixed graph that limits the predictive
performance of the model, (2) insufficiently capturing complex spatial-temporal
dependencies simultaneously, and (3) lacking attention to spatial-temporal
information at different time lengths. In this paper, we propose a Multi-Scale
Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN,
which consists of two different recurrent neural networks: the single-step gate
recurrent unit and the multi-step gate recurrent unit to fully capture the
complex spatial-temporal information in the traffic data under different time
windows. Moreover, we propose a spatial-temporal synchronous attention
mechanism that integrates adaptive position graph convolutions into the
self-attention mechanism to achieve synchronous capture of spatial-temporal
dependencies. We conducted extensive experiments on four real traffic datasets
and demonstrated that our model achieves the best prediction accuracy with
non-trivial margins compared to all the twenty baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chunjiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Detian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08148">
<title>Open-Set Knowledge-Based Visual Question Answering with Inference Paths. (arXiv:2310.08148v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08148</link>
<description rdf:parseType="Literal">&lt;p&gt;Given an image and an associated textual question, the purpose of
Knowledge-Based Visual Question Answering (KB-VQA) is to provide a correct
answer to the question with the aid of external knowledge bases. Prior KB-VQA
models are usually formulated as a retriever-classifier framework, where a
pre-trained retriever extracts textual or visual information from knowledge
graphs and then makes a prediction among the candidates. Despite promising
progress, there are two drawbacks with existing models. Firstly, modeling
question-answering as multi-class classification limits the answer space to a
preset corpus and lacks the ability of flexible reasoning. Secondly, the
classifier merely consider &quot;what is the answer&quot; without &quot;how to get the
answer&quot;, which cannot ground the answer to explicit reasoning paths. In this
paper, we confront the challenge of \emph{explainable open-set} KB-VQA, where
the system is required to answer questions with entities at wild and retain an
explainable reasoning path. To resolve the aforementioned issues, we propose a
new retriever-ranker paradigm of KB-VQA, Graph pATH rankER (GATHER for
brevity). Specifically, it contains graph constructing, pruning, and path-level
ranking, which not only retrieves accurate answers but also provides inference
paths that explain the reasoning process. To comprehensively evaluate our
model, we reformulate the benchmark dataset OK-VQA with manually corrected
entity-level annotations and release it as ConceptVQA. Extensive experiments on
real-world questions demonstrate that our framework is not only able to perform
open-set question answering across the whole knowledge base but provide
explicit reasoning path.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1&quot;&gt;Jingru Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xinzhe Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08150">
<title>On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks. (arXiv:2310.08150v1 [math.ST])</title>
<link>http://arxiv.org/abs/2310.08150</link>
<description rdf:parseType="Literal">&lt;p&gt;Maximum-type statistics of certain functions of the sample covariance matrix
of high-dimensional vector time series are studied to statistically confirm or
reject the null hypothesis that a data set has been collected under normal
conditions. The approach generalizes the case of the maximal deviation of the
sample autocovariances function from its assumed values. Within a linear time
series framework it is shown that Gumbel-type extreme value asymptotics holds
true. As applications we discuss long-only mimimal-variance portfolio
optimization and subportfolio analysis with respect to idiosyncratic risks, ETF
index tracking by sparse tracking portfolios, convolutional deep learners for
image analysis and the analysis of array-of-sensors data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Steland_A/0/1/0/all/0/1&quot;&gt;Ansgar Steland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08164">
<title>Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08164</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) aligned to human preferences via reinforcement
learning from human feedback (RLHF) underpin many commercial applications.
However, how RLHF impacts LLM internals remains opaque. We propose a novel
method to interpret learned reward functions in RLHF-tuned LLMs using sparse
autoencoders. Our approach trains autoencoder sets on activations from a base
LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we
identify unique features that reflect the accuracy of the learned reward model.
To quantify this, we construct a scenario where the tuned LLM learns
token-reward mappings to maximize reward. This is the first application of
sparse autoencoders for interpreting learned rewards and broadly inspecting
reward learning in LLMs. Our method provides an abstract approximation of
reward integrity. This presents a promising technique for ensuring alignment
between specified objectives and model behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_L/0/1/0/all/0/1&quot;&gt;Luke Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdullah_A/0/1/0/all/0/1&quot;&gt;Amir Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_L/0/1/0/all/0/1&quot;&gt;Luna Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arike_R/0/1/0/all/0/1&quot;&gt;Rauno Arike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08165">
<title>COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images. (arXiv:2310.08165v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.08165</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate and efficient diagnosis of COVID-19 is of paramount importance,
particularly in the context of large-scale medical imaging datasets. In this
preprint paper, we propose a novel approach for COVID-19 diagnosis using CT
images that leverages the power of Swin Transformer models, state-of-the-art
solutions in computer vision tasks. Our method includes a systematic approach
for patient-level predictions, where individual CT slices are classified as
COVID-19 or non-COVID, and the patient&apos;s overall diagnosis is determined
through majority voting. The application of the Swin Transformer in this
context results in patient-level predictions that demonstrate exceptional
diagnostic accuracy. In terms of evaluation metrics, our approach consistently
outperforms the baseline, as well as numerous competing methods, showcasing its
effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model
exceeds the baseline and offers a robust solution for accurate diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1&quot;&gt;Kenan Morani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08176">
<title>Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08176</link>
<description rdf:parseType="Literal">&lt;p&gt;This work analyzes Graph Neural Networks, a generalization of Fully-Connected
Deep Neural Nets on Graph structured data, when their width, that is the number
of nodes in each fullyconnected layer is increasing to infinity. Infinite Width
Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels,
both Machine Learning Frameworks with long traditions and extensive theoretical
foundations. Gaussian Processes and Kernels have much less hyperparameters then
Neural Networks and can be used for uncertainty estimation, making them more
user friendly for applications. This works extends the increasing amount of
research connecting Gaussian Processes and Kernels to Neural Networks. The
Kernel and Gaussian Process closed forms are derived for a variety of
architectures, namely the standard Graph Neural Network, the Graph Neural
Network with Skip-Concatenate Connections and the Graph Attention Neural
Network. All architectures are evaluated on a variety of datasets on the task
of transductive Node Regression and Classification. Additionally, a Spectral
Sparsification method known as Effective Resistance is used to improve runtime
and memory requirements. Extending the setting to inductive graph learning
tasks (Graph Regression/ Classification) is straightforward and is briefly
discussed in 3.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobanoglu_Y/0/1/0/all/0/1&quot;&gt;Yunus Cobanoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08177">
<title>Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization. (arXiv:2310.08177v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08177</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the adversarial robustness of machine learning models using
gradient-based attacks is challenging. In this work, we show that
hyperparameter optimization can improve fast minimum-norm attacks by automating
the selection of the loss function, the optimizer and the step-size scheduler,
along with the corresponding hyperparameters. Our extensive evaluation
involving several robust models demonstrates the improved efficacy of fast
minimum-norm attacks when hyper-up with hyperparameter optimization. We release
our open-source code at https://github.com/pralab/HO-FMN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Floris_G/0/1/0/all/0/1&quot;&gt;Giuseppe Floris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mura_R/0/1/0/all/0/1&quot;&gt;Raffaele Mura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scionis_L/0/1/0/all/0/1&quot;&gt;Luca Scionis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piras_G/0/1/0/all/0/1&quot;&gt;Giorgio Piras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1&quot;&gt;Maura Pintor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1&quot;&gt;Ambra Demontis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1&quot;&gt;Battista Biggio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08182">
<title>XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation. (arXiv:2310.08182v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08182</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of standardized robustness metrics and the widespread reliance on
numerous unrelated benchmark datasets for testing have created a gap between
academically validated robust models and their often problematic practical
adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark
dataset with over 200K images and 15,600 manual semantic annotations. Covering
12 categories from ImageNet to represent objects commonly encountered in
practical life and simulating six diverse scenarios, including overexposure,
blurring, color changing, etc., we further propose a novel robustness criterion
that extends beyond model generation ability assessment. This benchmark
dataset, along with related code, is available at
https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners
can leverage this resource to evaluate the robustness of their visual models
under challenging conditions and ultimately benefit from the demands of
practical computer vision systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shengzhao Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamnoedboon_P/0/1/0/all/0/1&quot;&gt;Porawit Kamnoedboon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;WeiWei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08184">
<title>Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08184</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models (FM) have demonstrated remarkable performance across a wide
range of tasks (especially in the fields of natural language processing and
computer vision), primarily attributed to their ability to comprehend
instructions and access extensive, high-quality data. This not only showcases
their current effectiveness but also sets a promising trajectory towards the
development of artificial general intelligence. Unfortunately, due to multiple
constraints, the raw data of the model used for large model training are often
inaccessible, so the use of end-to-end models for downstream tasks has become a
new research trend, which we call Learn From Model (LFM) in this article. LFM
focuses on the research, modification, and design of FM based on the model
interface, so as to better understand the model structure and weights (in a
black box environment), and to generalize the model to downstream tasks. The
study of LFM techniques can be broadly categorized into five major areas: model
tuning, model distillation, model reuse, meta learning and model editing. Each
category encompasses a repertoire of methods and strategies that aim to enhance
the capabilities and performance of FM. This paper gives a comprehensive review
of the current methods based on FM from the perspective of LFM, in order to
help readers better understand the current research status and ideas. To
conclude, we summarize the survey by highlighting several critical areas for
future exploration and addressing open issues that require further attention
from the research community. The relevant papers we investigated in this
article can be accessed at
&amp;lt;https://github.com/ruthless-man/Awesome-Learn-from-Model&amp;gt;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hongling Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1&quot;&gt;Anke Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08198">
<title>Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics. (arXiv:2310.08198v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08198</link>
<description rdf:parseType="Literal">&lt;p&gt;Model identification of battery dynamics is a central problem in energy
research; many energy management systems and design processes rely on accurate
battery models for efficiency optimization. The standard methodology for
battery modelling is traditional design of experiments (DoE), where the battery
dynamics are excited with many different current profiles and the measured
outputs are used to estimate the system dynamics. However, although it is
possible to obtain useful models with the traditional approach, the process is
time consuming and expensive because of the need to sweep many different
current-profile configurations. In the present work, a novel DoE approach is
developed based on deep reinforcement learning, which alters the configuration
of the experiments on the fly based on the statistics of past experiments.
Instead of sticking to a library of predefined current profiles, the proposed
approach modifies the current profiles dynamically by updating the output space
covered by past measurements, hence only the current profiles that are
informative for future experiments are applied. Simulations and real
experiments are used to show that the proposed approach gives models that are
as accurate as those obtained with traditional DoE but by using 85\% less
resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budan_G/0/1/0/all/0/1&quot;&gt;Gokhan Budan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damiani_F/0/1/0/all/0/1&quot;&gt;Francesca Damiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurtulus_C/0/1/0/all/0/1&quot;&gt;Can Kurtulus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1&quot;&gt;N. Kemal Ure&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08204">
<title>Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08204</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a lifelong audio-video masked autoencoder that continually learns
the multimodal representations from a video stream containing audio-video
pairs, while its distribution continually shifts over time. Specifically, we
propose two novel ideas to tackle the problem: (1) Localized Alignment: We
introduce a small trainable multimodal encoder that predicts the audio and
video tokens that are well-aligned with each other. This allows the model to
learn only the highly correlated audiovisual patches with accurate multimodal
relationships. (2) Forget-robust multimodal patch selection: We compare the
relative importance of each audio-video patch between the current and past data
pair to mitigate unintended drift of the previously learned audio-video
representations. Our proposed method, FLAVA (Forget-robust Localized
Audio-Video Alignment), therefore, captures the complex relationships between
the audio and video modalities during training on a sequence of pre-training
tasks while alleviating the forgetting of learned audiovisual correlations. Our
experiments validate that FLAVA outperforms the state-of-the-art continual
learning methods on several benchmark datasets under continual audio-video
representation learning scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Wonjae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yunji Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08209">
<title>Conformal inference for regression on Riemannian Manifolds. (arXiv:2310.08209v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.08209</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression on manifolds, and, more broadly, statistics on manifolds, has
garnered significant importance in recent years due to the vast number of
applications for this type of data. Circular data is a classic example, but so
is data in the space of covariance matrices, data on the Grassmannian manifold
obtained as a result of principal component analysis, among many others. In
this work we investigate prediction sets for regression scenarios when the
response variable, denoted by $Y$, resides in a manifold, and the covariable,
denoted by X, lies in Euclidean space. This extends the concepts delineated in
[Lei and Wasserman, 2014] to this novel context. Aligning with traditional
principles in conformal inference, these prediction sets are distribution-free,
indicating that no specific assumptions are imposed on the joint distribution
of $(X, Y)$, and they maintain a non-parametric character. We prove the
asymptotic almost sure convergence of the empirical version of these regions on
the manifold to their population counterparts. The efficiency of this method is
shown through a comprehensive simulation study and an analysis involving
real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cholaquidis_A/0/1/0/all/0/1&quot;&gt;Alejandro Cholaquidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gamboa_F/0/1/0/all/0/1&quot;&gt;Fabrice Gamboa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreno_L/0/1/0/all/0/1&quot;&gt;Leonardo Moreno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08215">
<title>Trustworthy Machine Learning. (arXiv:2310.08215v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08215</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning technology gets applied to actual products and solutions,
new challenges have emerged. Models unexpectedly fail to generalize to small
changes in the distribution, tend to be confident on novel data they have never
seen, or cannot communicate the rationale behind their decisions effectively
with the end users. Collectively, we face a trustworthiness issue with the
current machine learning technology. This textbook on Trustworthy Machine
Learning (TML) covers a theoretical and technical background of four key topics
in TML: Out-of-Distribution Generalization, Explainability, Uncertainty
Quantification, and Evaluation of Trustworthiness. We discuss important
classical and contemporary research papers of the aforementioned fields and
uncover and connect their underlying intuitions. The book evolved from the
homonymous course at the University of T\&quot;ubingen, first offered in the Winter
Semester of 2022/23. It is meant to be a stand-alone product accompanied by
code snippets and various pointers to further sources on topics of TML. The
dedicated website of the book is https://trustworthyml.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mucsanyi_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Mucs&amp;#xe1;nyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchhof_M/0/1/0/all/0/1&quot;&gt;Michael Kirchhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1&quot;&gt;Elisa Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1&quot;&gt;Alexander Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seong Joon Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08217">
<title>TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08217</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) has remained a persistent challenge for deep neural
networks due to catastrophic forgetting (CF) of previously learned tasks.
Several techniques such as weight regularization, experience rehearsal, and
parameter isolation have been proposed to alleviate CF. Despite their relative
success, these research directions have predominantly remained orthogonal and
suffer from several shortcomings, while missing out on the advantages of
competing strategies. On the contrary, the brain continually learns,
accommodates, and transfers knowledge across tasks by simultaneously leveraging
several neurophysiological processes, including neurogenesis, active
forgetting, neuromodulation, metaplasticity, experience rehearsal, and
context-dependent gating, rarely resulting in CF. Inspired by how the brain
exploits multiple mechanisms concurrently, we propose TriRE, a novel CL
paradigm that encompasses retaining the most prominent neurons for each task,
revising and solidifying the extracted knowledge of current and past tasks, and
actively promoting less active neurons for subsequent tasks through rewinding
and relearning. Across CL settings, TriRE significantly reduces task
interference and surpasses different CL approaches considered in isolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1&quot;&gt;Preetha Vijayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1&quot;&gt;Prashant Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1&quot;&gt;Elahe Arani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1&quot;&gt;Bahram Zonooz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08221">
<title>SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08221</link>
<description rdf:parseType="Literal">&lt;p&gt;Keyphrase generation (KG) aims to generate a set of summarizing words or
phrases given a source document, while keyphrase extraction (KE) aims to
identify them from the text. Because the search space is much smaller in KE, it
is often combined with KG to predict keyphrases that may or may not exist in
the corresponding document. However, current unified approaches adopt sequence
labeling and maximization-based generation that primarily operate at a token
level, falling short in observing and scoring keyphrases as a whole. In this
work, we propose SimCKP, a simple contrastive learning framework that consists
of two stages: 1) An extractor-generator that extracts keyphrases by learning
context-aware phrase-level representations in a contrastive manner while also
generating keyphrases that do not appear in the document; 2) A reranker that
adapts scores for each generated phrase by likewise aligning their
representations with the corresponding document. Experimental results on
multiple benchmark datasets demonstrate the effectiveness of our proposed
approach, which outperforms the state-of-the-art models by a significant
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gwak_C/0/1/0/all/0/1&quot;&gt;Chaeheon Gwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Si Hyeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08224">
<title>Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08224</link>
<description rdf:parseType="Literal">&lt;p&gt;We observe the emergence of binary encoding within the latent space of
deep-neural-network classifiers. Such binary encoding is induced by introducing
a linear penultimate layer, which is equipped during training with a loss
function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates
in the latent space. The phenomenon we describe represents a specific instance
of a well-documented occurrence known as \textit{neural collapse}, which arises
in the terminal phase of training and entails the collapse of latent class
means to the vertices of a simplex equiangular tight frame (ETF). We show that
binary encoding accelerates convergence toward the simplex ETF and enhances
classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sbailo_L/0/1/0/all/0/1&quot;&gt;Luigi Sbail&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghiringhelli_L/0/1/0/all/0/1&quot;&gt;Luca Ghiringhelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08235">
<title>GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. Code and video can be
found on the website https://craftjarvis-groot.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shaofei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yitao Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08237">
<title>Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.08237</link>
<description rdf:parseType="Literal">&lt;p&gt;Covariate shift occurs prevalently in practice, where the input distributions
of the source and target data are substantially different. Despite its
practical importance in various learning problems, most of the existing methods
only focus on some specific learning tasks and are not well validated
theoretically and numerically. To tackle this problem, we propose a unified
analysis of general nonparametric methods in a reproducing kernel Hilbert space
(RKHS) under covariate shift. Our theoretical results are established for a
general loss belonging to a rich loss function family, which includes many
commonly used methods as special cases, such as mean regression, quantile
regression, likelihood-based classification, and margin-based classification.
Two types of covariate shift problems are the focus of this paper and the sharp
convergence rates are established for a general loss function to provide a
unified theoretical analysis, which concurs with the optimal results in
literature where the squared loss is used. Extensive numerical studies on
synthetic and real examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xingdong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Caixing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingnan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08252">
<title>MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08252</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Meta-Black-Box Optimization with Reinforcement Learning
(MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to
mitigate manual fine-tuning of low-level black-box optimizers. However, this
field is hindered by the lack of a unified benchmark. To fill this gap, we
introduce MetaBox, the first benchmark platform expressly tailored for
developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible
algorithmic template that allows users to effortlessly implement their unique
designs within the platform. Moreover, it provides a broad spectrum of over 300
problem instances, collected from synthetic to realistic scenarios, and an
extensive library of 19 baseline methods, including both traditional black-box
optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three
standardized performance metrics, enabling a more thorough assessment of the
methods. In a bid to illustrate the utility of MetaBox for facilitating
rigorous evaluation and in-depth analysis, we carry out a wide-ranging
benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source
and accessible at: https://github.com/GMC-DRL/MetaBox.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hongshu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1&quot;&gt;Guojun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yue-Jiao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yining Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08256">
<title>Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08256</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) often make factually incorrect responses despite
their success in various applications. In this paper, we hypothesize that
relying heavily on simple co-occurrence statistics of the pre-training corpora
is one of the main factors that cause factual errors. Our results reveal that
LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently
co-occurred words over the correct answer. Consequently, LLMs struggle to
recall facts whose subject and object rarely co-occur in the pre-training
dataset although they are seen during finetuning. We show that co-occurrence
bias remains despite scaling up model sizes or finetuning. Therefore, we
suggest finetuning on a debiased dataset to mitigate the bias by filtering out
biased samples whose subject-object co-occurrence count is high. Although
debiased finetuning allows LLMs to memorize rare facts in the training set, it
is not effective in recalling rare facts unseen during finetuning. Further
research in mitigation will help build reliable language models by preventing
potential errors. The code is available at
\url{https://github.com/CheongWoong/impact_of_cooccurrence}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1&quot;&gt;Cheongwoong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaesik Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08259">
<title>Invisible Threats: Backdoor Attack in OCR Systems. (arXiv:2310.08259v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.08259</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Character Recognition (OCR) is a widely used tool to extract text
from scanned documents. Today, the state-of-the-art is achieved by exploiting
deep neural networks. However, the cost of this performance is paid at the
price of system vulnerability. For instance, in backdoor attacks, attackers
compromise the training phase by inserting a backdoor in the victim&apos;s model
that will be activated at testing time by specific patterns while leaving the
overall model performance intact. This work proposes a backdoor attack for OCR
resulting in the injection of non-readable characters from malicious input
images. This simple but effective attack exposes the state-of-the-art OCR
weakness, making the extracted text correct to human eyes but simultaneously
unusable for the NLP application that uses OCR as a preprocessing step.
Experimental results show that the attacked models successfully output
non-readable characters for around 90% of the poisoned instances without
harming their performance for the remaining instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1&quot;&gt;Mauro Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farronato_N/0/1/0/all/0/1&quot;&gt;Nicola Farronato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1&quot;&gt;Stefanos Koffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajola_L/0/1/0/all/0/1&quot;&gt;Luca Pajola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08278">
<title>Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08278</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen &quot;out-of-distribution&quot; time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1&quot;&gt;Kashif Rasul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1&quot;&gt;Arjun Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Andrew Robert Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorasani_A/0/1/0/all/0/1&quot;&gt;Arian Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adamopoulos_G/0/1/0/all/0/1&quot;&gt;George Adamopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagwatkar_R/0/1/0/all/0/1&quot;&gt;Rishika Bhagwatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1&quot;&gt;Marin Bilo&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghonia_H/0/1/0/all/0/1&quot;&gt;Hena Ghonia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassen_N/0/1/0/all/0/1&quot;&gt;Nadhir Vincent Hassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1&quot;&gt;Anderson Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1&quot;&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapados_N/0/1/0/all/0/1&quot;&gt;Nicolas Chapados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevmyvaka_Y/0/1/0/all/0/1&quot;&gt;Yuriy Nevmyvaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08282">
<title>Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08282</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiscale modeling of complex systems is crucial for understanding their
intricacies. Data-driven multiscale modeling has emerged as a promising
approach to tackle challenges associated with complex systems. On the other
hand, self-similarity is prevalent in complex systems, hinting that large-scale
complex systems can be modeled at a reduced cost. In this paper, we introduce a
multiscale neural network framework that incorporates self-similarity as prior
knowledge, facilitating the modeling of self-similar dynamical systems. For
deterministic dynamics, our framework can discern whether the dynamics are
self-similar. For uncertain dynamics, it can compare and determine which
parameter set is closer to self-similarity. The framework allows us to extract
scale-invariant kernels from the dynamics for modeling at any scale. Moreover,
our method can identify the power law exponents in self-similar systems.
Preliminary tests on the Ising model yielded critical exponents consistent with
theoretical expectations, providing valuable insights for addressing critical
phase transitions in non-equilibrium systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Ruyi Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_N/0/1/0/all/0/1&quot;&gt;Ningning Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yizhuang You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08287">
<title>A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors. (arXiv:2310.08287v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.08287</link>
<description rdf:parseType="Literal">&lt;p&gt;The distribution of the weights of modern deep neural networks (DNNs) -
crucial for uncertainty quantification and robustness - is an eminently complex
object due to its extremely high dimensionality. This paper proposes one of the
first large-scale explorations of the posterior distribution of deep Bayesian
Neural Networks (BNNs), expanding its study to real-world vision tasks and
architectures. Specifically, we investigate the optimal approach for
approximating the posterior, analyze the connection between posterior quality
and uncertainty quantification, delve into the impact of modes on the
posterior, and explore methods for visualizing the posterior. Moreover, we
uncover weight-space symmetries as a critical aspect for understanding the
posterior. To this extent, we develop an in-depth assessment of the impact of
both permutation and scaling symmetries that tend to obfuscate the Bayesian
posterior. While the first type of transformation is known for duplicating
modes, we explore the relationship between the latter and L2 regularization,
challenging previous misconceptions. Finally, to help the community improve our
understanding of the Bayesian posterior, we will shortly release the first
large-scale checkpoint dataset, including thousands of real-world models and
our codes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Laurent_O/0/1/0/all/0/1&quot;&gt;Olivier Laurent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aldea_E/0/1/0/all/0/1&quot;&gt;Emanuel Aldea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franchi_G/0/1/0/all/0/1&quot;&gt;Gianni Franchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08304">
<title>CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08304</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot object classification is the task of classifying objects in an image
with limited number of examples as supervision. We propose a one-shot/few-shot
classification model that can classify an object of any unseen class into a
relatively general category in an hierarchically based classification. Our
model uses a three-level hierarchical contrastive loss based ResNet152
classifier for classifying an object based on its features extracted from Image
embedding, not used during the training phase. For our experimentation, we have
used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal
classes for training our model and created our own dataset of unseen classes
for evaluating our trained model. Our model provides satisfactory results in
classifying the unknown objects into a generic category which has been later
discussed in greater detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1&quot;&gt;Arpit Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhaveri_H/0/1/0/all/0/1&quot;&gt;Harshil Jhaveri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_S/0/1/0/all/0/1&quot;&gt;Swapnil Mallick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajmera_A/0/1/0/all/0/1&quot;&gt;Abhishek Ajmera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08312">
<title>GePSAn: Generative Procedure Step Anticipation in Cooking Videos. (arXiv:2310.08312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08312</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of future step anticipation in procedural videos. Given
a video of an ongoing procedural activity, we predict a plausible next
procedure step described in rich natural language. While most previous work
focus on the problem of data scarcity in procedural video datasets, another
core challenge of future anticipation is how to account for multiple plausible
future realizations in natural settings. This problem has been largely
overlooked in previous work. To address this challenge, we frame future step
prediction as modelling the distribution of all possible candidates for the
next step. Specifically, we design a generative model that takes a series of
video clips as input, and generates multiple plausible and diverse candidates
(in natural language) for the next step. Following previous work, we side-step
the video annotation scarcity by pretraining our model on a large text-based
corpus of procedural activities, and then transfer the model to the video
domain. Our experiments, both in textual and video domains, show that our model
captures diversity in the next step prediction and generates multiple plausible
future predictions. Moreover, our model establishes new state-of-the-art
results on YouCookII, where it outperforms existing baselines on the next step
anticipation. Finally, we also show that our model can successfully transfer
from text to the video domain zero-shot, ie, without fine-tuning or adaptation,
and produces good-quality future step predictions from video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1&quot;&gt;Mohamed Ashraf Abdelsalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rangrej_S/0/1/0/all/0/1&quot;&gt;Samrudhdhi B. Rangrej&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadji_I/0/1/0/all/0/1&quot;&gt;Isma Hadji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvornik_N/0/1/0/all/0/1&quot;&gt;Nikita Dvornik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1&quot;&gt;Afsaneh Fazly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08320">
<title>Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08320</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-&quot;a person&quot; instead of the person&apos;s name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new &quot;dual-use&quot;
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1&quot;&gt;Daniel Neider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08331">
<title>Impact of multi-armed bandit strategies on deep recurrent reinforcement learning. (arXiv:2310.08331v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2310.08331</link>
<description rdf:parseType="Literal">&lt;p&gt;Incomplete knowledge of the environment leads an agent to make decisions
under uncertainty. One of the major dilemmas in Reinforcement Learning (RL)
where an autonomous agent has to balance two contrasting needs in making its
decisions is: exploiting the current knowledge of the environment to maximize
the cumulative reward as well as exploring actions that allow improving the
knowledge of the environment, hopefully leading to higher reward values
(exploration-exploitation trade-off). Concurrently, another relevant issue
regards the full observability of the states, which may not be assumed in all
applications. Such as when only 2D images are considered as input in a RL
approach used for finding the optimal action within a 3D simulation
environment. In this work, we address these issues by deploying and testing
several techniques to balance exploration and exploitation trade-off on
partially observable systems for predicting steering wheels in autonomous
driving scenario. More precisely, the final aim is to investigate the effects
of using both stochastic and deterministic multi-armed bandit strategies
coupled with a Deep Recurrent Q-Network. Additionally, we adapted and evaluated
the impact of an innovative method to improve the learning phase of the
underlying Convolutional Recurrent Neural Network. We aim to show that adaptive
stochastic methods for exploration better approximate the trade-off between
exploration and exploitation as, in general, Softmax and Max-Boltzmann
strategies are able to outperform epsilon-greedy techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zangirolami_V/0/1/0/all/0/1&quot;&gt;Valentina Zangirolami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Borrotti_M/0/1/0/all/0/1&quot;&gt;Matteo Borrotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08337">
<title>Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08337</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown remarkable performance on many generative tasks.
Despite recent success, most diffusion models are restricted in that they only
allow linear transformation of the data distribution. In contrast, broader
family of transformations can potentially help train generative distributions
more efficiently, simplifying the reverse process and closing the gap between
the true negative log-likelihood and the variational approximation. In this
paper, we present Neural Diffusion Models (NDMs), a generalization of
conventional diffusion models that enables defining and learning time-dependent
non-linear transformations of data. We show how to optimise NDMs using a
variational bound in a simulation-free setting. Moreover, we derive a
time-continuous formulation of NDMs, which allows fast and reliable inference
using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the
utility of NDMs with learnable transformations through experiments on standard
image generation benchmarks, including CIFAR-10, downsampled versions of
ImageNet and CelebA-HQ. NDMs outperform conventional diffusion models in terms
of likelihood and produce high-quality samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartosh_G/0/1/0/all/0/1&quot;&gt;Grigory Bartosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naesseth_C/0/1/0/all/0/1&quot;&gt;Christian A. Naesseth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08339">
<title>A Generic Software Framework for Distributed Topological Analysis Pipelines. (arXiv:2310.08339v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2310.08339</link>
<description rdf:parseType="Literal">&lt;p&gt;This system paper presents a software framework for the support of
topological analysis pipelines in a distributed-memory model. While several
recent papers introduced topology-based approaches for distributed-memory
environments, these were reporting experiments obtained with tailored,
mono-algorithm implementations. In contrast, we describe in this paper a
general-purpose, generic framework for topological analysis pipelines, i.e. a
sequence of topological algorithms interacting together, possibly on distinct
numbers of processes. Specifically, we instantiated our framework with the MPI
model, within the Topology ToolKit (TTK). While developing this framework, we
faced several algorithmic and software engineering challenges, which we
document in this paper. We provide a taxonomy for the distributed-memory
topological algorithms supported by TTK, depending on their communication needs
and provide examples of hybrid MPI+thread parallelizations. Detailed
performance analyses show that parallel efficiencies range from $20\%$ to
$80\%$ (depending on the algorithms), and that the MPI-specific preconditioning
introduced by our framework induces a negligible computation time overhead. We
illustrate the new distributed-memory capabilities of TTK with an example of
advanced analysis pipeline, combining multiple algorithms, run on the largest
publicly available dataset we have found (120 billion vertices) on a standard
cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a
roadmap for the completion of TTK&apos;s MPI extension, along with generic
recommendations for each algorithm communication category.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillou_E/0/1/0/all/0/1&quot;&gt;Eve Le Guillou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Will_M/0/1/0/all/0/1&quot;&gt;Michael Will&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillou_P/0/1/0/all/0/1&quot;&gt;Pierre Guillou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasczyk_J/0/1/0/all/0/1&quot;&gt;Jonas Lukasczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortin_P/0/1/0/all/0/1&quot;&gt;Pierre Fortin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garth_C/0/1/0/all/0/1&quot;&gt;Christoph Garth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1&quot;&gt;Julien Tierny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08348">
<title>LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios. (arXiv:2310.08348v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08348</link>
<description rdf:parseType="Literal">&lt;p&gt;Building agents based on tree-search planning capabilities with learned
models has achieved remarkable success in classic decision-making problems,
such as Go and Atari. However, it has been deemed challenging or even
infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse
real-world applications, especially when these environments involve complex
action spaces and significant simulation costs, or inherent stochasticity. In
this work, we introduce LightZero, the first unified benchmark for deploying
MCTS/MuZero in general sequential decision scenarios. Specificially, we
summarize the most critical challenges in designing a general MCTS-style
decision-making solver, then decompose the tightly-coupled algorithm and system
design of tree-search RL methods into distinct sub-modules. By incorporating
more appropriate exploration and optimization strategies, we can significantly
enhance these sub-modules and construct powerful LightZero agents to tackle
tasks across a wide range of domains, such as board games, Atari, MuJoCo,
MiniGrid and GoBigger. Detailed benchmark results reveal the significant
potential of such methods in building scalable and efficient decision
intelligence. The code is available as part of OpenDILab at
https://github.com/opendilab/LightZero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yazhe Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yuan Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhenjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jiyuan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shuai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1908.04628">
<title>L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes. (arXiv:1908.04628v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1908.04628</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world prediction tasks have outcome variables that have
characteristic heavy-tail distributions. Examples include copies of books sold,
auction prices of art pieces, demand for commodities in warehouses, etc. By
learning heavy-tailed distributions, &quot;big and rare&quot; instances (e.g., the
best-sellers) will have accurate predictions. Most existing approaches are not
dedicated to learning heavy-tailed distribution; thus, they heavily
under-predict such instances. To tackle this problem, we introduce Learning to
Place (L2P), which exploits the pairwise relationships between instances for
learning. In its training phase, L2P learns a pairwise preference classifier:
is instance A &amp;gt; instance B? In its placing phase, L2P obtains a prediction by
placing the new instance among the known instances. Based on its placement, the
new instance is then assigned a value for its outcome variable. Experiments on
real data show that L2P outperforms competing approaches in terms of accuracy
and ability to reproduce heavy-tailed outcome distribution. In addition, L2P
provides an interpretable model by placing each predicted instance in relation
to its comparable neighbors. Interpretable models are highly desirable when
lives and treasure are at stake.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xindi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varol_O/0/1/0/all/0/1&quot;&gt;Onur Varol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1&quot;&gt;Tina Eliassi-Rad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1910.09143">
<title>Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1910.09143</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning in sparse-reward navigation environments with
expensive and limited interactions is challenging and poses a need for
effective exploration. Motivated by complex navigation tasks that require
real-world training (when cheap simulators are not available), we consider an
agent that faces an unknown distribution of environments and must decide on an
exploration strategy. It may leverage a series of training environments to
improve its policy before it is evaluated in a test environment drawn from the
same environment distribution. Most existing approaches focus on fixed
exploration strategies, while the few that view exploration as a
meta-optimization problem tend to ignore the need for cost-efficient
exploration. We propose a cost-aware Bayesian optimization approach that
efficiently searches over a class of dynamic subgoal-based exploration
strategies. The algorithm adjusts a variety of levers -- the locations of the
subgoals, the length of each episode, and the number of replications per trial
-- in order to overcome the challenges of sparse rewards, expensive
interactions, and noise. An experimental evaluation demonstrates that the new
approach outperforms existing baselines across a number of problem domains. We
also provide a theoretical foundation and prove that the method asymptotically
identifies a near-optimal subgoal design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Poloczek_M/0/1/0/all/0/1&quot;&gt;Matthias Poloczek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Daniel R. Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.05421">
<title>Conditional Sig-Wasserstein GANs for Time Series Generation. (arXiv:2006.05421v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2006.05421</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) have been extremely successful in
generating samples, from seemingly high dimensional probability measures.
However, these methods struggle to capture the temporal dependence of joint
probability distributions induced by time-series data. Furthermore, long
time-series data streams hugely increase the dimension of the target space,
which may render generative modelling infeasible. To overcome these challenges,
motivated by the autoregressive models in econometric, we are interested in the
conditional distribution of future time series given the past information. We
propose the generic conditional Sig-WGAN framework by integrating
Wasserstein-GANs (WGANs) with mathematically principled and efficient path
feature extraction called the signature of a path. The signature of a path is a
graded sequence of statistics that provides a universal description for a
stream of data, and its expected value characterises the law of the time-series
model. In particular, we develop the conditional Sig-$W_1$ metric, that
captures the conditional joint law of time series models, and use it as a
discriminator. The signature feature space enables the explicit representation
of the proposed discriminators which alleviates the need for expensive
training. We validate our method on both synthetic and empirical dataset and
observe that our method consistently and significantly outperforms
state-of-the-art benchmarks with respect to measures of similarity and
predictive ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shujian Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Hao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpruch_L/0/1/0/all/0/1&quot;&gt;Lukasz Szpruch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiese_M/0/1/0/all/0/1&quot;&gt;Magnus Wiese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabate_Vidales_M/0/1/0/all/0/1&quot;&gt;Marc Sabate-Vidales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Baoren Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.14432">
<title>Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2107.14432</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel framework that adds the regularizers of the sparse group
lasso to a family of adaptive optimizers in deep learning, such as Momentum,
Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which
are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group
AdaHessian, etc., accordingly. We establish theoretically proven convergence
guarantees in the stochastic convex settings, based on primal-dual methods. We
evaluate the regularized effect of our new optimizers on three large-scale
real-world ad click datasets with state-of-the-art deep learning models. The
experimental results reveal that compared with the original optimizers with the
post-processing procedure which uses the magnitude pruning method, the
performance of the models can be significantly improved on the same sparsity
level. Furthermore, in comparison to the cases without magnitude pruning, our
methods can achieve extremely high sparsity with significantly better or highly
competitive performance. The code is available at
https://github.com/intelligent-machine-learning/dlrover/blob/master/tfplus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yun Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongchao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Suo Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Chunyang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1&quot;&gt;Huanjun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lihong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yixiang Mu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03469">
<title>Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03469</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning allows multiple parties to collaboratively train a joint
model without sharing local data. This enables applications of machine learning
in settings of inherently distributed, undisclosable data such as in the
medical domain. In practice, joint training is usually achieved by aggregating
local models, for which local training objectives have to be in expectation
similar to the joint (global) objective. Often, however, local datasets are so
small that local objectives differ greatly from the global objective, resulting
in federated learning to fail. We propose a novel approach that intertwines
model aggregations with permutations of local models. The permutations expose
each local model to a daisy chain of local datasets resulting in more efficient
training in data-sparse domains. This enables training on extremely small local
datasets, such as patient data across hospitals, while retaining the training
efficiency and privacy benefits of federated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_M/0/1/0/all/0/1&quot;&gt;Michael Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1&quot;&gt;Jonas Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12389">
<title>MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12389</link>
<description rdf:parseType="Literal">&lt;p&gt;Practical real world datasets with plentiful categories introduce new
challenges for unsupervised domain adaptation like small inter-class
discriminability, that existing approaches relying on domain invariance alone
cannot handle sufficiently well. In this work we propose MemSAC, which exploits
sample level similarity across source and target domains to achieve
discriminative transfer, along with architectures that scale to a large number
of categories. For this purpose, we first introduce a memory augmented approach
to efficiently extract pairwise similarity relations between labeled source and
unlabeled target domain instances, suited to handle an arbitrary number of
classes. Next, we propose and theoretically justify a novel variant of the
contrastive loss to promote local consistency among within-class cross domain
samples while enforcing separation between classes, thus preserving
discriminative transfer from source to target. We validate the advantages of
MemSAC with significant improvements over previous state-of-the-art on multiple
challenging transfer tasks designed for large-scale adaptation, such as
DomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds
dataset with 200 classes. We also provide in-depth analysis and insights into
the effectiveness of MemSAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalluri_T/0/1/0/all/0/1&quot;&gt;Tarun Kalluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Astuti Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.14219">
<title>A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v9 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2207.14219</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel model-agnostic algorithm called adaptive
ensemble batch multi-input multi-output conformalized quantile regression
(AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction
intervals for a fixed pre-specified miscoverage rate in a distribution-free
manner. Our method is grounded on conformal prediction principles, however, it
does not require data splitting and provides close to exact coverage even when
the data is not exchangeable. Moreover, the resulting prediction intervals,
besides being empirically valid along the forecast horizon, do not neglect
heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution
shifts, which means that its prediction intervals remain reliable over an
unlimited period of time, without entailing retraining or imposing unrealistic
strict assumptions on the data-generating process. Through methodically
experimentation, we demonstrate that our approach outperforms other competitive
methods on both real-world and synthetic datasets. The code used in the
experimental part and a tutorial on how to use AEnbMIMOCQR can be found at the
following GitHub repository: https://github.com/Quilograma/AEnbMIMOCQR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sousa_M/0/1/0/all/0/1&quot;&gt;Martim Sousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tome_A/0/1/0/all/0/1&quot;&gt;Ana Maria Tom&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreira_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Moreira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07481">
<title>Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information. (arXiv:2209.07481v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07481</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov Chain Monte Carlo methods for sampling from complex distributions and
estimating normalization constants often simulate samples from a sequence of
intermediate distributions along an annealing path, which bridges between a
tractable initial distribution and a target density of interest. Prior work has
constructed annealing paths using quasi-arithmetic means, and interpreted the
resulting intermediate densities as minimizing an expected divergence to the
endpoints. We provide a comprehensive analysis of this &apos;centroid&apos; property
using Bregman divergences under a monotonic embedding of the density function,
thereby associating common divergences such as Amari&apos;s and Renyi&apos;s
${\alpha}$-divergences, ${(\alpha,\beta)}$-divergences, and the Jensen-Shannon
divergence with intermediate densities along an annealing path. Our analysis
highlights the interplay between parametric families, quasi-arithmetic means,
and divergence functions using the rho-tau Bregman divergence framework of
Zhang 2004,2013.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1&quot;&gt;Rob Brekelmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1&quot;&gt;Frank Nielsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10404">
<title>GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10404</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Grasp Proposal Network (GP-net), a Convolutional Neural
Network model which can generate 6-DoF grasps from flexible viewpoints, e.g. as
experienced by mobile manipulators. To train GP-net, we synthetically generate
a dataset containing depth-images and ground-truth grasp information. In
real-world experiments, we use the EGAD evaluation benchmark to evaluate GP-net
against two commonly used algorithms, the Volumetric Grasping Network (VGN) and
the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In
contrast to the state-of-the-art methods in robotic grasping, GP-net can be
used for grasping objects from flexible, unknown viewpoints without the need to
define the workspace and achieves a grasp success of 54.4% compared to 51.6%
for VGN and 44.2% for GPD. We provide a ROS package along with our code and
pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konrad_A/0/1/0/all/0/1&quot;&gt;Anna Konrad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1&quot;&gt;John McDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villing_R/0/1/0/all/0/1&quot;&gt;Rudi Villing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02286">
<title>Efficient probabilistic reconciliation of forecasts for real-valued and count time series. (arXiv:2210.02286v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02286</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical time series are common in several applied fields. The forecasts
for these time series are required to be coherent, that is, to satisfy the
constraints given by the hierarchy. The most popular technique to enforce
coherence is called reconciliation, which adjusts the base forecasts computed
for each time series. However, recent works on probabilistic reconciliation
present several limitations. In this paper, we propose a new approach based on
conditioning to reconcile any type of forecast distribution. We then introduce
a new algorithm, called Bottom-Up Importance Sampling, to efficiently sample
from the reconciled distribution. It can be used for any base forecast
distribution: discrete, continuous, or in the form of samples, providing a
major speedup compared to the current methods. Experiments on several temporal
hierarchies show a significant improvement over base probabilistic forecasts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zambon_L/0/1/0/all/0/1&quot;&gt;Lorenzo Zambon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Azzimonti_D/0/1/0/all/0/1&quot;&gt;Dario Azzimonti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1&quot;&gt;Giorgio Corani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09222">
<title>MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human Activity Recognition. (arXiv:2210.09222v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09222</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal sensors provide complementary information to develop accurate
machine-learning methods for human activity recognition (HAR), but introduce
significantly higher computational load, which reduces efficiency. This paper
proposes an efficient multimodal neural architecture for HAR using an RGB
camera and inertial measurement units (IMUs) called Multimodal Temporal Segment
Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a
temporal and structure-preserving gray-scale image using the Gramian Angular
Field (GAF), representing the inherent properties of human activities. MMTSA
then applies a multimodal sparse sampling method to reduce data redundancy.
Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal
fusion. Using three well-established public datasets, we evaluated MMTSA&apos;s
effectiveness and efficiency in HAR. Results show that our method achieves
superior performance improvements 11.13% of cross-subject F1-score on the MMAct
dataset than the previous state-of-the-art (SOTA) methods. The ablation study
and analysis suggest that MMTSA&apos;s effectiveness in fusing multimodal data for
accurate HAR. The efficiency evaluation on an edge device showed that MMTSA
achieved significantly better accuracy, lower computational load, and lower
inference latency than SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Ziqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianguo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11355">
<title>Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference. (arXiv:2210.11355v2 [econ.EM] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11355</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a generalization of the synthetic controls and synthetic
interventions methodology to incorporate network interference. We consider the
estimation of unit-specific potential outcomes from panel data in the presence
of spillover across units and unobserved confounding. Key to our approach is a
novel latent factor model that takes into account network interference and
generalizes the factor models typically used in panel data settings. We propose
an estimator, Network Synthetic Interventions (NSI), and show that it
consistently estimates the mean outcomes for a unit under an arbitrary set of
counterfactual treatments for the network. We further establish that the
estimator is asymptotically normal. We furnish two validity tests for whether
the NSI estimator reliably generalizes to produce accurate counterfactual
estimates. We provide a novel graph-based experiment design that guarantees the
NSI estimator produces accurate counterfactual estimates, and also analyze the
sample complexity of the proposed design. We conclude with simulations that
corroborate our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Anish Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Cen_S/0/1/0/all/0/1&quot;&gt;Sarah H. Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Devavrat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Christina Lee Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13660">
<title>Multi-SpacePhish: Extending the Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (arXiv:2210.13660v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13660</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing literature on adversarial Machine Learning (ML) focuses either on
showing attacks that break every ML model, or defenses that withstand most
attacks. Unfortunately, little consideration is given to the actual feasibility
of the attack or the defense. Moreover, adversarial samples are often crafted
in the &quot;feature-space&quot;, making the corresponding evaluations of questionable
value. Simply put, the current situation does not allow to estimate the actual
threat posed by adversarial attacks, leading to a lack of secure ML systems.
&lt;/p&gt;
&lt;p&gt;We aim to clarify such confusion in this paper. By considering the
application of ML for Phishing Website Detection (PWD), we formalize the
&quot;evasion-space&quot; in which an adversarial perturbation can be introduced to fool
a ML-PWD -- demonstrating that even perturbations in the &quot;feature-space&quot; are
useful. Then, we propose a realistic threat model describing evasion attacks
against ML-PWD that are cheap to stage, and hence intrinsically more attractive
for real phishers. After that, we perform the first statistically validated
assessment of state-of-the-art ML-PWD against 12 evasion attacks. Our
evaluation shows (i) the true efficacy of evasion attempts that are more likely
to occur; and (ii) the impact of perturbations crafted in different
evasion-spaces. Our realistic evasion attempts induce a statistically
significant degradation (3-10% at p&amp;lt;0.05), and their cheap cost makes them a
subtle threat. Notably, however, some ML-PWD are immune to our most realistic
attacks (p=0.22).
&lt;/p&gt;
&lt;p&gt;Finally, as an additional contribution of this journal publication, we are
the first to consider the intriguing case wherein an attacker introduces
perturbations in multiple evasion-spaces at the same time. These new results
show that simultaneously applying perturbations in the problem- and
feature-space can cause a drop in the detection rate from 0.95 to 0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ying Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apruzzese_G/0/1/0/all/0/1&quot;&gt;Giovanni Apruzzese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1&quot;&gt;Mauro Conti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15889">
<title>Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15889</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural-symbolic computing (NeSy), which pursues the integration of the
symbolic and statistical paradigms of cognition, has been an active research
area of Artificial Intelligence (AI) for many years. As NeSy shows promise of
reconciling the advantages of reasoning and interpretability of symbolic
representation and robust learning in neural networks, it may serve as a
catalyst for the next generation of AI. In the present paper, we provide a
systematic overview of the recent developments and important contributions of
NeSy research. Firstly, we introduce study history of this area, covering early
work and foundations. We further discuss background concepts and identify key
driving factors behind the development of NeSy. Afterward, we categorize recent
landmark approaches along several main characteristics that underline this
research paradigm, including neural-symbolic integration, knowledge
representation, knowledge embedding, and functionality. Next, we briefly
discuss the successful application of modern NeSy approaches in several
domains. Then, we benchmark several NeSy methods on three representative
application tasks. Finally, we identify the open problems together with
potential future research directions. This survey is expected to help new
researchers enter this rapidly evolving field and accelerate the progress
towards data-and knowledge-driven AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12345">
<title>Understanding Sparse Feature Updates in Deep Networks using Iterative Linearisation. (arXiv:2211.12345v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12345</link>
<description rdf:parseType="Literal">&lt;p&gt;Larger and deeper networks generalise well despite their increased capacity
to overfit. Understanding why this happens is theoretically and practically
important. One recent approach looks at the infinitely wide limits of such
networks and their corresponding kernels. However, these theoretical tools
cannot fully explain finite networks as the empirical kernel changes
significantly during gradient-descent-based training in contrast to infinite
networks. In this work, we derive an iterative linearised training method as a
novel empirical tool to further investigate this distinction, allowing us to
control for sparse (i.e. infrequent) feature updates and quantify the frequency
of feature learning needed to achieve comparable performance. We justify
iterative linearisation as an interpolation between a finite analog of the
infinite width regime, which does not learn features, and standard gradient
descent training, which does. Informally, we also show that it is analogous to
a damped version of the Gauss-Newton algorithm -- a second-order method. We
show that in a variety of cases, iterative linearised training surprisingly
performs on par with standard training, noting in particular how much less
frequent feature learning is required to achieve comparable performance. We
also show that feature learning is essential for good performance. Since such
feature learning inevitably causes changes in the NTK kernel, we provide direct
negative evidence for the NTK theory, which states the NTK kernel remains
constant during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldwaser_A/0/1/0/all/0/1&quot;&gt;Adrian Goldwaser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hong Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15363">
<title>On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15363</link>
<description rdf:parseType="Literal">&lt;p&gt;Although it has been demonstrated that Natural Language Processing (NLP)
algorithms are vulnerable to deliberate attacks, the question of whether such
weaknesses can lead to software security threats is under-explored. To bridge
this gap, we conducted vulnerability tests on Text-to-SQL systems that are
commonly used to create natural language interfaces to databases. We showed
that the Text-to-SQL modules within six commercial applications can be
manipulated to produce malicious code, potentially leading to data breaches and
Denial of Service attacks. This is the first demonstration that NLP models can
be exploited as attack vectors in the wild. In addition, experiments using four
open-source language models verified that straightforward backdoor attacks on
Text-to-SQL systems achieve a 100% success rate without affecting their
performance. The aim of this work is to draw the community&apos;s attention to
potential software security issues associated with NLP algorithms and encourage
exploration of methods to mitigate against them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xutan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingfeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1&quot;&gt;Mark Stevenson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10010">
<title>Identifying latent distances with Finslerian geometry. (arXiv:2212.10010v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10010</link>
<description rdf:parseType="Literal">&lt;p&gt;Riemannian geometry provides us with powerful tools to explore the latent
space of generative models while preserving the underlying structure of the
data. The latent space can be equipped it with a Riemannian metric, pulled back
from the data manifold. With this metric, we can systematically navigate the
space relying on geodesics defined as the shortest curves between two points.
Generative models are often stochastic, causing the data space, the Riemannian
metric, and the geodesics, to be stochastic as well. Stochastic objects are at
best impractical, and at worst impossible, to manipulate. A common solution is
to approximate the stochastic pullback metric by its expectation. But the
geodesics derived from this expected Riemannian metric do not correspond to the
expected length-minimising curves. In this work, we propose another metric
whose geodesics explicitly minimise the expected length of the pullback metric.
We show this metric defines a Finsler metric, and we compare it with the
expected Riemannian metric. In high dimensions, we prove that both metrics
converge to each other at a rate of $O\left(\frac{1}{D}\right)$. This
convergence implies that the established expected Riemannian metric is an
accurate approximation of the theoretically more grounded Finsler metric. This
provides justification for using the expected Riemannian metric for practical
implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouplin_A/0/1/0/all/0/1&quot;&gt;Alison Pouplin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eklund_D/0/1/0/all/0/1&quot;&gt;David Eklund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ek_C/0/1/0/all/0/1&quot;&gt;Carl Henrik Ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10886">
<title>Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and
adaptively provides high-quality intrinsic rewards to enhance exploration in
reinforcement learning (RL). More specifically, AIRS selects shaping function
from a predefined set based on the estimated task return in real-time,
providing reliable exploration incentives and alleviating the biased objective
problem. Moreover, we develop an intrinsic reward toolkit to provide efficient
and reliable implementations of diverse intrinsic reward approaches. We test
AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite.
Extensive simulation demonstrates that AIRS can outperform the benchmarking
schemes and achieve superior performance with simple architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10902">
<title>Efficient Hyperdimensional Computing. (arXiv:2301.10902v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10902</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperdimensional computing (HDC) is a method to perform classification that
uses binary vectors with high dimensions and the majority rule. This approach
has the potential to be energy-efficient and hence deemed suitable for
resource-limited platforms due to its simplicity and massive parallelism.
However, in order to achieve high accuracy, HDC sometimes uses hypervectors
with tens of thousands of dimensions. This potentially negates its efficiency
advantage. In this paper, we examine the necessity of such high dimensions and
conduct a detailed theoretical analysis of the relationship between hypervector
dimensions and accuracy. Our results demonstrate that as the dimension of the
hypervectors increases, the worst-case/average-case HDC prediction accuracy
with the majority rule decreases. Building on this insight, we develop HDC
models that use binary hypervectors with dimensions orders of magnitude lower
than those of state-of-the-art HDC models while maintaining equivalent or even
improved accuracy and efficiency. For instance, on the MNIST dataset, we
achieve 91.12% HDC accuracy in image classification with a dimension of only
64. Our methods perform operations that are only 0.35% of other HDC models with
dimensions of 10,000. Furthermore, we evaluate our methods on ISOLET, UCI-HAR,
and Fashion-MNIST datasets and investigate the limits of HDC computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhanglu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kaiwen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1&quot;&gt;Weng-Fai Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13326">
<title>A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13326</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of stochastic, combinatorial multi-armed bandits
where the learner only has access to bandit feedback and the reward function
can be non-linear. We provide a general framework for adapting discrete offline
approximation algorithms into sublinear $\alpha$-regret methods that only
require bandit feedback, achieving
$\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative
$\alpha$-regret dependence on the horizon $T$. The framework only requires the
offline algorithms to be robust to small errors in function evaluation. The
adaptation procedure does not even require explicit knowledge of the offline
approximation algorithm -- the offline algorithm can be used as a black box
subroutine. To demonstrate the utility of the proposed framework, the proposed
framework is applied to diverse applications in submodular maximization. The
new CMAB algorithms for submodular maximization with knapsack constraints
outperform a full-bandit method developed for the adversarial setting in
experiments with real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_G/0/1/0/all/0/1&quot;&gt;Guanyu Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadew_Y/0/1/0/all/0/1&quot;&gt;Yididiya Y Nadew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanhui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinn_C/0/1/0/all/0/1&quot;&gt;Christopher John Quinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00767">
<title>ImageNomer: description of a functional connectivity and omics analysis tool and case study identifying a race confound. (arXiv:2302.00767v2 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00767</link>
<description rdf:parseType="Literal">&lt;p&gt;Most packages for the analysis of fMRI-based functional connectivity (FC) and
genomic data are used with a programming language interface, lacking an
easy-to-navigate GUI frontend. This exacerbates two problems found in these
types of data: demographic confounds and quality control in the face of high
dimensionality of features. The reason is that it is too slow and cumbersome to
use a programming interface to create all the necessary visualizations required
to identify all correlations, confounding effects, or quality control problems
in a dataset. To remedy this situation, we have developed ImageNomer, a data
visualization and analysis tool that allows inspection of both subject-level
and cohort-level demographic, genomic, and imaging features. The software is
Python-based, runs in a self-contained Docker image, and contains a
browser-based GUI frontend. We demonstrate the usefulness of ImageNomer by
identifying an unexpected race confound when predicting achievement scores in
the Philadelphia Neurodevelopmental Cohort (PNC) dataset. In the past, many
studies have attempted to use FC to identify achievement-related features in
fMRI. Using ImageNomer, we find a clear potential for confounding effects of
race. Using correlation analysis in the ImageNomer software, we show that FCs
correlated with Wide Range Achievement Test (WRAT) score are in fact more
highly correlated with race. Investigating further, we find that whereas both
FC and SNP (genomic) features can account for 10-15\% of WRAT score variation,
this predictive ability disappears when controlling for race. In this work, we
demonstrate the advantage of our ImageNomer GUI tool in data exploration and
confound detection. Additionally, this work identifies race as a strong
confound in FC data and casts doubt on the possibility of finding unbiased
achievement-related features in fMRI and SNP data of healthy adolescents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Orlichenko_A/0/1/0/all/0/1&quot;&gt;Anton Orlichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Daly_G/0/1/0/all/0/1&quot;&gt;Grant Daly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hong-Wen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02931">
<title>Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts. (arXiv:2302.02931v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02931</link>
<description rdf:parseType="Literal">&lt;p&gt;Training machine learning models robust to distribution shifts is critical
for real-world applications. Some robust training algorithms (e.g., Group DRO)
specialize to group shifts and require group information on all training
points. Other methods (e.g., CVaR DRO) that do not need group annotations can
be overly conservative, since they naively upweight high loss points which may
form a contrived set that does not correspond to any meaningful group in the
real world (e.g., when the high loss points are randomly mislabeled training
points). In this work, we address limitations in prior approaches by assuming a
more nuanced form of group shift: conditioned on the label, we assume that the
true group function (indicator over group) is simple. For example, we may
expect that group shifts occur along low bitrate features (e.g., image
background, lighting). Thus, we aim to learn a model that maintains high
accuracy on simple group functions realized by these low bitrate features, that
need not spend valuable model capacity achieving high accuracy on contrived
groups of examples. Based on this, we consider the two-player game formulation
of DRO where the adversary&apos;s capacity is bitrate-constrained. Our resulting
practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group
information on training samples yet matches the performance of Group DRO on
datasets that have training group annotations and that of CVaR DRO on
long-tailed distributions. Our theoretical analysis reveals that in some
settings BR-DRO objective can provably yield statistically efficient and less
conservative solutions than unconstrained CVaR DRO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1&quot;&gt;Amrith Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dennis_D/0/1/0/all/0/1&quot;&gt;Don Dennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1&quot;&gt;Benjamin Eysenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1&quot;&gt;Aditi Raghunathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1&quot;&gt;Virginia Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03770">
<title>Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03770</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-conditioned reinforcement learning (GCRL) refers to learning
general-purpose skills that aim to reach diverse goals. In particular, offline
GCRL only requires purely pre-collected datasets to perform training tasks
without additional interactions with the environment. Although offline GCRL has
become increasingly prevalent and many previous works have demonstrated its
empirical success, the theoretical understanding of efficient offline GCRL
algorithms is not well established, especially when the state space is huge and
the offline dataset only covers the policy we aim to learn. In this paper, we
provide a rigorous theoretical analysis of an existing empirically successful
offline GCRL algorithm. We prove that under slight modification, this algorithm
enjoys an $\widetilde{O}(\text{poly}(1/\epsilon))$ sample complexity (where
$\epsilon$ is the desired suboptimality of the learned policy) with general
function approximation thanks to the property of (semi-)strong convexity of the
objective functions. We only require nearly minimal assumptions on the dataset
(single-policy concentrability) and the function class (realizability).
Moreover, this algorithm consists of two uninterleaved optimization steps,
which we refer to as $V$-learning and policy learning, and is computationally
stable since it does not involve minimax optimization. We also empirically
validate our theory by showing that the modified algorithm outperforms the
previous algorithm in various real-world environments. To the best of our
knowledge, this is the first algorithm that is both provably efficient with
general function approximation and single-policy concentrability, and
empirically successful without requiring solving minimax optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanlin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03874">
<title>Participatory Personalization in Classification. (arXiv:2302.03874v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03874</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are often personalized with information that is
protected, sensitive, self-reported, or costly to acquire. These models use
information about people but do not facilitate nor inform their consent.
Individuals cannot opt out of reporting personal information to a model, nor
tell if they benefit from personalization in the first place. We introduce a
family of classification models, called participatory systems, that let
individuals opt into personalization at prediction time. We present a
model-agnostic algorithm to learn participatory systems for personalization
with categorical group attributes. We conduct a comprehensive empirical study
of participatory systems in clinical prediction tasks, benchmarking them with
common approaches for personalization and imputation. Our results demonstrate
that participatory systems can facilitate and inform consent while improving
performance and data use across all groups who report personal data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joren_H/0/1/0/all/0/1&quot;&gt;Hailey Joren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagpal_C/0/1/0/all/0/1&quot;&gt;Chirag Nagpal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heller_K/0/1/0/all/0/1&quot;&gt;Katherine Heller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustun_B/0/1/0/all/0/1&quot;&gt;Berk Ustun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12461">
<title>Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12461</link>
<description rdf:parseType="Literal">&lt;p&gt;Poisoning of data sets is a potential security threat to large language
models that can lead to backdoored models. A description of the internal
mechanisms of backdoored language models and how they process trigger inputs,
e.g., when switching to toxic language, has yet to be found. In this work, we
study the internal representations of transformer-based backdoored language
models and determine early-layer MLP modules as most important for the backdoor
mechanism in combination with the initial embedding projection. We use this
knowledge to remove, insert, and modify backdoor mechanisms with engineered
replacements that reduce the MLP module outputs to essentials for the backdoor
mechanism. To this end, we introduce PCP ablation, where we replace transformer
modules with low-rank matrices based on the principal components of their
activations. We demonstrate our results on backdoored toy, backdoored large,
and non-backdoored open-source models. We show that we can improve the backdoor
robustness of large language models by locally constraining individual modules
during fine-tuning on potentially poisonous data sets.
&lt;/p&gt;
&lt;p&gt;Trigger warning: Offensive language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamparth_M/0/1/0/all/0/1&quot;&gt;Max Lamparth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reuel_A/0/1/0/all/0/1&quot;&gt;Anka Reuel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01748">
<title>A Complete Recipe for Diffusion Generative Models. (arXiv:2303.01748v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01748</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based Generative Models (SGMs) have demonstrated exceptional synthesis
outcomes across various tasks. However, the current design landscape of the
forward diffusion process remains largely untapped and often relies on physical
heuristics or simplifying assumptions. Utilizing insights from the development
of scalable Bayesian posterior samplers, we present a complete recipe for
formulating forward processes in SGMs, ensuring convergence to the desired
target distribution. Our approach reveals that several existing SGMs can be
seen as specific manifestations of our framework. Building upon this method, we
introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based
modeling within an augmented space enriched by auxiliary variables akin to
physical phase space. Empirical results exhibit the superior sample quality and
improved speed-quality trade-off of PSLD compared to various competing
approaches on established image synthesis benchmarks. Remarkably, PSLD achieves
sample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional
CIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in
conditional synthesis using pre-trained score networks, offering an appealing
alternative as an SGM backbone for future advancements. Code and model
checkpoints can be accessed at \url{https://github.com/mandt-lab/PSLD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1&quot;&gt;Kushagra Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07189">
<title>Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07189</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim to optimize the binary detection of Chronic Obstructive Pulmonary
Disease (COPD) based on emphysema presence in the lung with convolutional
neural networks (CNN) by exploring manually adjusted versus automated
window-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT
images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with
COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and
preprocessed. For each image, intensity values were manually clipped to the
emphysema window setting and a baseline &apos;full-range&apos; window setting.
Class-balanced train, validation, and test sets contained 3,392, 1,114, and
2,688 images. The network backbone was optimized by comparing various CNN
architectures. Furthermore, automated WSO was implemented by adding a
customized layer to the model. The image-level area under the Receiver
Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] was
utilized to compare model variations. Repeated inference (n=7) on the test set
showed that the DenseNet was the most efficient backbone and achieved a mean
AUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input images manually
adjusted to the emphysema window, the DenseNet model predicted COPD with a mean
AUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to the DenseNet, an
optimal window in the proximity of the emphysema window setting was learned
automatically, and a mean AUC of 0.82 [0.78, 0.86] was achieved. Detection of
COPD with DenseNet models was improved by WSO of CT data to the emphysema
window setting range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorosti_T/0/1/0/all/0/1&quot;&gt;Tina Dorosti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schultheiss_M/0/1/0/all/0/1&quot;&gt;Manuel Schultheiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hofmann_F/0/1/0/all/0/1&quot;&gt;Felix Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thalhammer_J/0/1/0/all/0/1&quot;&gt;Johannes Thalhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirchner_L/0/1/0/all/0/1&quot;&gt;Luisa Kirchner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Urban_T/0/1/0/all/0/1&quot;&gt;Theresa Urban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfeiffer_F/0/1/0/all/0/1&quot;&gt;Franz Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaff_F/0/1/0/all/0/1&quot;&gt;Florian Schaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfeiffer_D/0/1/0/all/0/1&quot;&gt;Daniela Pfeiffer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09033">
<title>Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09033</link>
<description rdf:parseType="Literal">&lt;p&gt;Most bandit algorithms assume that the reward variances or their upper bounds
are known, and that they are the same for all arms. This naturally leads to
suboptimal performance and higher regret due to variance overestimation. On the
other hand, underestimated reward variances may lead to linear regret due to
committing early to a suboptimal arm. This motivated prior works on
variance-adaptive frequentist algorithms, which have strong instance-dependent
regret bounds but cannot incorporate prior knowledge on reward variances. We
lay foundations for the Bayesian setting, which incorporates prior knowledge.
This results in lower regret in practice, due to using the prior in the
algorithm design, and also improved regret guarantees. Specifically, we study
Gaussian bandits with {unknown heterogeneous reward variances}, and develop a
Thompson sampling algorithm with prior-dependent Bayes regret bounds. We
achieve lower regret with lower reward variances and more informative priors on
them, which is precisely why we pay only for what is uncertain. This is the
first result of its kind. Finally, we corroborate our theory with extensive
experiments, which show the superiority of our variance-adaptive Bayesian
algorithm over prior frequentist approaches. We also show that our approach is
robust to model misspecification and can be applied with estimated priors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10108">
<title>Data-Centric Learning from Unlabeled Graphs with Diffusion Model. (arXiv:2303.10108v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10108</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph property prediction tasks are important and numerous. While each task
offers a small size of labeled examples, unlabeled graphs have been collected
from various sources and at a large scale. A conventional approach is training
a model with the unlabeled graphs on self-supervised tasks and then fine-tuning
the model on the prediction tasks. However, the self-supervised task knowledge
could not be aligned or sometimes conflicted with what the predictions needed.
In this paper, we propose to extract the knowledge underlying the large set of
unlabeled graphs as a specific set of useful data points to augment each
property prediction model. We use a diffusion model to fully utilize the
unlabeled graphs and design two new objectives to guide the model&apos;s denoising
process with each task&apos;s labeled data to generate task-specific graph examples
and their labels. Experiments demonstrate that our data-centric approach
performs significantly better than fifteen existing various methods on fifteen
tasks. The performance improvement brought by unlabeled data is visible as the
generated labeled examples unlike the self-supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inae_E/0/1/0/all/0/1&quot;&gt;Eric Inae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaxin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tengfei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Meng Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17823">
<title>An interpretable neural network-based non-proportional odds model for ordinal regression. (arXiv:2303.17823v3 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17823</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes an interpretable neural network-based non-proportional
odds model (N$^3$POM) for ordinal regression. N$^3$POM is different from
conventional approaches to ordinal regression with non-proportional models in
several ways: (1) N$^3$POM is designed to directly handle continuous responses,
whereas standard methods typically treat de facto ordered continuous variables
as discrete, (2) instead of estimating response-dependent finite coefficients
of linear models from discrete responses as is done in conventional approaches,
we train a non-linear neural network to serve as a coefficient function. Thanks
to the neural network, N$^3$POM offers flexibility while preserving the
interpretability of conventional ordinal regression. We establish a sufficient
condition under which the predicted conditional cumulative probability locally
satisfies the monotonicity constraint over a user-specified region in the
covariate space. Additionally, we provide a monotonicity-preserving stochastic
(MPS) algorithm for effectively training the neural network. We apply N$^3$POM
to several real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okuno_A/0/1/0/all/0/1&quot;&gt;Akifumi Okuno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harada_K/0/1/0/all/0/1&quot;&gt;Kazuharu Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00457">
<title>LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00457</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized natural language processing
and demonstrated impressive capabilities in various tasks. Unfortunately, they
are prone to hallucinations, where the model exposes incorrect or false
information in its responses, which renders diligent evaluation approaches
mandatory. While LLM performance in specific knowledge fields is often
evaluated based on question and answer (Q&amp;amp;A) datasets, such evaluations usually
report only a single accuracy number for the dataset, which often covers an
entire field. This field-based evaluation, is problematic with respect to
transparency and model improvement. A stratified evaluation could instead
reveal subfields, where hallucinations are more likely to occur and thus help
to better assess LLMs&apos; risks and guide their further development. To support
such stratified evaluations, we propose LLMMaps as a novel visualization
technique that enables users to evaluate LLMs&apos; performance with respect to Q&amp;amp;A
datasets. LLMMaps provide detailed insights into LLMs&apos; knowledge capabilities
in different subfields, by transforming Q&amp;amp;A datasets as well as LLM responses
into an internal knowledge structure. An extension for comparative
visualization furthermore, allows for the detailed comparison of multiple LLMs.
To assess LLMMaps we use them to conduct a comparative analysis of several
state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as
well as two qualitative user evaluations. All necessary source code and data
for generating LLMMaps to be used in scientific publications and elsewhere is
available on GitHub: https://github.com/viscom-ulm/LLMMaps
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1&quot;&gt;Patrik Puchert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poonam_P/0/1/0/all/0/1&quot;&gt;Poonam Poonam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onzenoodt_C/0/1/0/all/0/1&quot;&gt;Christian van Onzenoodt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1&quot;&gt;Timo Ropinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02621">
<title>High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation. (arXiv:2304.02621v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02621</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-level weakly-supervised semantic segmentation (WSSS) reduces the
usually vast data annotation cost by surrogate segmentation masks during
training. The typical approach involves training an image classification
network using global average pooling (GAP) on convolutional feature maps. This
enables the estimation of object locations based on class activation maps
(CAMs), which identify the importance of image regions. The CAMs are then used
to generate pseudo-labels, in the form of segmentation masks, to supervise a
segmentation model in the absence of pixel-level ground truth. Our work is
based on two techniques for improving CAMs; importance sampling, which is a
substitute for GAP, and the feature similarity loss, which utilizes a heuristic
that object contours almost always align with color edges in images. However,
both are based on the multinomial posterior with softmax, and implicitly assume
that classes are mutually exclusive, which turns out suboptimal in our
experiments. Thus, we reformulate both techniques based on binomial posteriors
of multiple independent binary problems. This has two benefits; their
performance is improved and they become more general, resulting in an add-on
method that can boost virtually any WSSS method. This is demonstrated on a wide
variety of baselines on the PASCAL VOC dataset, improving the region similarity
and contour quality of all implemented state-of-the-art methods. Experiments on
the MS COCO dataset show that our proposed add-on is well-suited for
large-scale settings. Our code is available at https://github.com/arvijj/hfpl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonnarth_A/0/1/0/all/0/1&quot;&gt;Arvi Jonnarth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yushan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1&quot;&gt;Michael Felsberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04234">
<title>Variational operator learning: A unified paradigm marrying training neural operators and solving partial differential equations. (arXiv:2304.04234v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04234</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural operators as novel neural architectures for fast approximating
solution operators of partial differential equations (PDEs), have shown
considerable promise for future scientific computing. However, the mainstream
of training neural operators is still data-driven, which needs an expensive
ground-truth dataset from various sources (e.g., solving PDEs&apos; samples with the
conventional solvers, real-world experiments) in addition to training stage
costs. From a computational perspective, marrying operator learning and
specific domain knowledge to solve PDEs is an essential step in reducing
dataset costs and label-free learning. We propose a novel paradigm that
provides a unified framework of training neural operators and solving PDEs with
the variational form, which we refer to as the variational operator learning
(VOL). Ritz and Galerkin approach with finite element discretization are
developed for VOL to achieve matrix-free approximation of system functional and
residual, then direct minimization and iterative update are proposed as two
optimization strategies for VOL. Various types of experiments based on
reasonable benchmarks about variable heat source, Darcy flow, and variable
stiffness elasticity are conducted to demonstrate the effectiveness of VOL.
With a label-free training set and a 5-label-only shift set, VOL learns
solution operators with its test errors decreasing in a power law with respect
to the amount of unlabeled data. To the best of the authors&apos; knowledge, this is
the first study that integrates the perspectives of the weak form and efficient
iterative methods for solving sparse linear systems into the end-to-end
operator learning task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tengfei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dachuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_P/0/1/0/all/0/1&quot;&gt;Peng Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09663">
<title>Generative modeling of time-dependent densities via optimal transport and projection pursuit. (arXiv:2304.09663v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09663</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the computational difficulties incurred by popular deep learning
algorithms for the generative modeling of temporal densities, we propose a
cheap alternative which requires minimal hyperparameter tuning and scales
favorably to high dimensional problems. In particular, we use a
projection-based optimal transport solver [Meng et al., 2019] to join
successive samples and subsequently use transport splines [Chewi et al., 2020]
to interpolate the evolving density. When the sampling frequency is
sufficiently high, the optimal maps are close to the identity and are thus
computationally efficient to compute. Moreover, the training process is highly
parallelizable as all optimal maps are independent and can thus be learned
simultaneously. Finally, the approach is based solely on numerical linear
algebra rather than minimizing a nonconvex objective function, allowing us to
easily analyze and control the algorithm. We present several numerical
experiments on both synthetic and real-world datasets to demonstrate the
efficiency of our method. In particular, these experiments show that the
proposed approach is highly competitive compared with state-of-the-art
normalizing flows conditioned on time across a wide range of dimensionalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Botvinick_Greenhouse_J/0/1/0/all/0/1&quot;&gt;Jonah Botvinick-Greenhouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yunan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maulik_R/0/1/0/all/0/1&quot;&gt;Romit Maulik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12233">
<title>Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v3 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12233</link>
<description rdf:parseType="Literal">&lt;p&gt;The exploration of transition state (TS) geometries is crucial for
elucidating chemical reaction mechanisms and modeling their kinetics. Recently,
machine learning (ML) models have shown remarkable performance for prediction
of TS geometries. However, they require 3D conformations of reactants and
products often with their appropriate orientations as input, which demands
substantial efforts and computational cost. Here, we propose a generative
approach based on the stochastic diffusion method, namely TSDiff, for
prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed
the existing ML models with 3D geometries in terms of both accuracy and
efficiency. Moreover, it enables to sample various TS conformations, because it
learned the distribution of TS geometries for diverse reactions in training.
Thus, TSDiff was able to find more favorable reaction pathways with lower
barrier heights than those in the reference database. These results demonstrate
that TSDiff shows promising potential for an efficient and reliable TS
exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seonghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Woo_J/0/1/0/all/0/1&quot;&gt;Jeheon Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Youn Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00152">
<title>Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00152</link>
<description rdf:parseType="Literal">&lt;p&gt;Theoretical studies on transfer learning or domain adaptation have so far
focused on situations with a known hypothesis class or model; however in
practice, some amount of model selection is usually involved, often appearing
under the umbrella term of hyperparameter-tuning: for example, one may think of
the problem of tuning for the right neural network architecture towards a
target task, while leveraging data from a related source task.
&lt;/p&gt;
&lt;p&gt;Now, in addition to the usual tradeoffs on approximation vs estimation errors
involved in model selection, this problem brings in a new complexity term,
namely, the transfer distance between source and target distributions, which is
known to vary with the choice of hypothesis class.
&lt;/p&gt;
&lt;p&gt;We present a first study of this problem, focusing on classification; in
particular, the analysis reveals some remarkable phenomena: adaptive rates,
i.e., those achievable with no distributional information, can be arbitrarily
slower than oracle rates, i.e., when given knowledge on distances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanneke_S/0/1/0/all/0/1&quot;&gt;Steve Hanneke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kpotufe_S/0/1/0/all/0/1&quot;&gt;Samory Kpotufe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahdaviyeh_Y/0/1/0/all/0/1&quot;&gt;Yasaman Mahdaviyeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06118">
<title>NeRF2: Neural Radio-Frequency Radiance Fields. (arXiv:2305.06118v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06118</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Maxwell discovered the physical laws of electromagnetic waves 160
years ago, how to precisely model the propagation of an RF signal in an
electrically large and complex environment remains a long-standing problem. The
difficulty is in the complex interactions between the RF signal and the
obstacles (e.g., reflection, diffraction, etc.). Inspired by the great success
of using a neural network to describe the optical field in computer vision, we
propose a neural radio-frequency radiance field, NeRF$^\textbf{2}$, which
represents a continuous volumetric scene function that makes sense of an RF
signal&apos;s propagation. Particularly, after training with a few signal
measurements, NeRF$^\textbf{2}$ can tell how/what signal is received at any
position when it knows the position of a transmitter. As a physical-layer
neural network, NeRF$^\textbf{2}$ can take advantage of the learned statistic
model plus the physical model of ray tracing to generate a synthetic dataset
that meets the training demands of application-layer artificial neural networks
(ANNs). Thus, we can boost the performance of ANNs by the proposed
turbo-learning, which mixes the true and synthetic datasets to intensify the
training. Our experiment results show that turbo-learning can enhance
performance with an approximate 50% increase. We also demonstrate the power of
NeRF$^\textbf{2}$ in the field of indoor localization and 5G MIMO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1&quot;&gt;Zhenlin An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Qingrui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06292">
<title>Joint Metrics Matter: A Better Standard for Trajectory Forecasting. (arXiv:2305.06292v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06292</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal trajectory forecasting methods commonly evaluate using
single-agent metrics (marginal metrics), such as minimum Average Displacement
Error (ADE) and Final Displacement Error (FDE), which fail to capture joint
performance of multiple interacting agents. Only focusing on marginal metrics
can lead to unnatural predictions, such as colliding trajectories or diverging
trajectories for people who are clearly walking together as a group.
Consequently, methods optimized for marginal metrics lead to overly-optimistic
estimations of performance, which is detrimental to progress in trajectory
forecasting research. In response to the limitations of marginal metrics, we
present the first comprehensive evaluation of state-of-the-art (SOTA)
trajectory forecasting methods with respect to multi-agent metrics (joint
metrics): JADE, JFDE, and collision rate. We demonstrate the importance of
joint metrics as opposed to marginal metrics with quantitative evidence and
qualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We
introduce a new loss function incorporating joint metrics that, when applied to
a SOTA trajectory forecasting method, achieves a 7\% improvement in JADE / JFDE
on the ETH / UCY datasets with respect to the previous SOTA. Our results also
indicate that optimizing for joint metrics naturally leads to an improvement in
interaction modeling, as evidenced by a 16\% decrease in mean collision rate on
the ETH / UCY datasets with respect to the previous SOTA. Code is available at
\texttt{\hyperlink{https://github.com/ericaweng/joint-metrics-matter}{github.com/ericaweng/joint-metrics-matter}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_E/0/1/0/all/0/1&quot;&gt;Erica Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoshino_H/0/1/0/all/0/1&quot;&gt;Hana Hoshino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris Kitani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06648">
<title>Generalization bounds for neural ordinary differential equations and deep residual networks. (arXiv:2305.06648v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06648</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural ordinary differential equations (neural ODEs) are a popular family of
continuous-depth deep learning models. In this work, we consider a large family
of parameterized ODEs with continuous-in-time parameters, which include
time-dependent neural ODEs. We derive a generalization bound for this class by
a Lipschitz-based argument. By leveraging the analogy between neural ODEs and
deep residual networks, our approach yields in particular a generalization
bound for a class of deep residual networks. The bound involves the magnitude
of the difference between successive weight matrices. We illustrate numerically
how this quantity affects the generalization capability of neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marion_P/0/1/0/all/0/1&quot;&gt;Pierre Marion&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14133">
<title>Conditional Mutual Information for Disentangled Representations in Reinforcement Learning. (arXiv:2305.14133v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14133</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) environments can produce training data with
spurious correlations between features due to the amount of training data or
its limited feature coverage. This can lead to RL agents encoding these
misleading correlations in their latent representation, preventing the agent
from generalising if the correlation changes within the environment or when
deployed in the real world. Disentangled representations can improve
robustness, but existing disentanglement techniques that minimise mutual
information between features require independent features, thus they cannot
disentangle correlated features. We propose an auxiliary task for RL algorithms
that learns a disentangled representation of high-dimensional observations with
correlated features by minimising the conditional mutual information between
features in the representation. We demonstrate experimentally, using continuous
control tasks, that our approach improves generalisation under correlation
shifts, as well as improving the training performance of RL algorithms in the
presence of correlated features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunion_M/0/1/0/all/0/1&quot;&gt;Mhairi Dunion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McInroe_T/0/1/0/all/0/1&quot;&gt;Trevor McInroe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luck_K/0/1/0/all/0/1&quot;&gt;Kevin Sebastian Luck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1&quot;&gt;Josiah P. Hanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14259">
<title>Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14259</link>
<description rdf:parseType="Literal">&lt;p&gt;Literature-Based Discovery (LBD) aims to discover new scientific knowledge by
mining papers and generating hypotheses. Standard LBD is limited to predicting
pairwise relations between discrete concepts (e.g., drug-disease links), and
ignores critical contexts like experimental settings (e.g., a specific patient
population where a drug is evaluated) and background motivations (e.g., to find
drugs without specific side effects). We address these limitations with a novel
formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in
natural language, while grounding them in a context that controls the
hypothesis search space. We present a modeling framework using retrieval of
``inspirations&apos;&apos; from past scientific papers. Our evaluations reveal that GPT-4
tends to generate ideas with overall low technical depth and novelty, while our
inspiration prompting approaches partially mitigate this issue. Our work
represents a first step toward building language models that generate new ideas
derived from scientific literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1&quot;&gt;Doug Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1&quot;&gt;Tom Hope&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15394">
<title>Differentially-Private Decision Trees and Provable Robustness to Data Poisoning. (arXiv:2305.15394v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15394</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees are interpretable models that are well-suited to non-linear
learning problems. Much work has been done on extending decision tree learning
algorithms with differential privacy, a system that guarantees the privacy of
samples within the training data. However, current state-of-the-art algorithms
for this purpose sacrifice much utility for a small privacy benefit. These
solutions create random decision nodes that reduce decision tree accuracy or
spend an excessive share of the privacy budget on labeling leaves. Moreover,
many works do not support continuous features or leak information about them.
We propose a new method called PrivaTree based on private histograms that
chooses good splits while consuming a small privacy budget. The resulting trees
provide a significantly better privacy-utility trade-off and accept mixed
numerical and categorical data without leaking information about numerical
features. Finally, while it is notoriously hard to give robustness guarantees
against data poisoning attacks, we demonstrate bounds for the expected accuracy
and success rates of backdoor attacks against differentially-private learners.
By leveraging the better privacy-utility trade-off of PrivaTree we are able to
train decision trees with significantly better robustness against backdoor
attacks compared to regular decision trees and with meaningful theoretical
guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_D/0/1/0/all/0/1&quot;&gt;Dani&amp;#xeb;l Vos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vos_J/0/1/0/all/0/1&quot;&gt;Jelle Vos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erkin_Z/0/1/0/all/0/1&quot;&gt;Zekeriya Erkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1&quot;&gt;Sicco Verwer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19443">
<title>OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19443</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a fuzzy adaptive loss function for enhancing deep
learning performance in classification tasks. Specifically, we redefine the
cross-entropy loss to effectively address class-level noise conditions,
including the challenging problem of class imbalance. Our approach introduces
aggregation operators, leveraging the power of fuzzy logic to improve
classification accuracy. The rationale behind our proposed method lies in the
iterative up-weighting of class-level components within the loss function,
focusing on those with larger errors. To achieve this, we employ the ordered
weighted average (OWA) operator and combine it with an adaptive scheme for
gradient-based learning. Through extensive experimentation, our method
outperforms other commonly used loss functions, such as the standard
cross-entropy or focal loss, across various binary and multiclass
classification tasks. Furthermore, we explore the influence of hyperparameters
associated with the OWA operators and present a default configuration that
performs well across different experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maldonado_S/0/1/0/all/0/1&quot;&gt;Sebasti&amp;#xe1;n Maldonado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vairetti_C/0/1/0/all/0/1&quot;&gt;Carla Vairetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jara_K/0/1/0/all/0/1&quot;&gt;Katherine Jara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrasco_M/0/1/0/all/0/1&quot;&gt;Miguel Carrasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1&quot;&gt;Julio L&amp;#xf3;pez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19838">
<title>Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19838</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Optimization (BO) is typically used to optimize an unknown function
$f$ that is noisy and costly to evaluate, by exploiting an acquisition function
that must be maximized at each optimization step. Even if provably
asymptotically optimal BO algorithms are efficient at optimizing
low-dimensional functions, scaling them to high-dimensional spaces remains an
open problem, often tackled by assuming an additive structure for $f$. By doing
so, BO algorithms typically introduce additional restrictive assumptions on the
additive structure that reduce their applicability domain. This paper contains
two main contributions: (i) we relax the restrictive assumptions on the
additive structure of $f$, at the expense of weakening the maximization
guarantees of the acquisition function, and (ii) we address the
over-exploration problem for decentralized BO algorithms. To these ends, we
propose DumBO, an asymptotically optimal decentralized BO algorithm that
achieves very competitive performance against state-of-the-art BO algorithms,
especially when the additive structure of $f$ comprises high-dimensional
factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bardou_A/0/1/0/all/0/1&quot;&gt;Anthony Bardou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1&quot;&gt;Patrick Thiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Begin_T/0/1/0/all/0/1&quot;&gt;Thomas Begin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02010">
<title>Memorization Capacity of Multi-Head Attention in Transformers. (arXiv:2306.02010v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02010</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have become the go-to architecture for language and vision
tasks, yet their theoretical properties, especially memorization capacity,
remain elusive. This paper investigates the memorization abilities of
multi-head attention mechanisms, examining how many example sequences they can
memorize, as a function of the number of heads and sequence length. Motivated
by experimental findings on vision transformers, we introduce novel assumptions
about the linear independence of input data, distinct from the commonly used
general-position assumption. Under these assumptions, we demonstrate that an
attention layer with $H$ heads, dimension $d$, and context size $n &amp;lt; d$,
featuring $\Theta(Hd^2)$ parameters, can memorize $\Omega(Hn)$ examples. Our
analysis sheds light on how different attention heads handle various example
sequences, aided by the softmax operator&apos;s saturation property. We validate our
findings through experiments on synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mahdavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Renjie Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03410">
<title>Learning to Simulate Tree-Branch Dynamics for Manipulation. (arXiv:2306.03410v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03410</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to use a simulation driven inverse inference approach to model the
dynamics of tree branches under manipulation. Learning branch dynamics and
gaining the ability to manipulate deformable vegetation can help with
occlusion-prone tasks, such as fruit picking in dense foliage, as well as
moving overhanging vines and branches for navigation in dense vegetation. The
underlying deformable tree geometry is encapsulated as coarse spring
abstractions executed on parallel, non-differentiable simulators. The implicit
statistical model defined by the simulator, reference trajectories obtained by
actively probing the ground truth, and the Bayesian formalism, together guide
the spring parameter posterior density estimation. Our non-parametric inference
algorithm, based on Stein Variational Gradient Descent, incorporates
biologically motivated assumptions into the inference process as neural network
driven learnt joint priors; moreover, it leverages the finite difference scheme
for gradient approximations. Real and simulated experiments confirm that our
model can predict deformation trajectories, quantify the estimation
uncertainty, and it can perform better when base-lined against other inference
algorithms, particularly from the Monte Carlo family. The model displays strong
robustness properties in the presence of heteroscedastic sensor noise;
furthermore, it can generalise to unseen grasp locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1&quot;&gt;Jayadeep Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_T/0/1/0/all/0/1&quot;&gt;Tirthankar Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1&quot;&gt;Jason Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1&quot;&gt;Paulo Borges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06323">
<title>Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06323</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the fundamental problem of learning multi-layer generator
models. The multi-layer generator model builds multiple layers of latent
variables as a prior model on top of the generator, which benefits learning
complex data distribution and hierarchical representations. However, such a
prior model usually focuses on modeling inter-layer relations between latent
variables by assuming non-informative (conditional) Gaussian distributions,
which can be limited in model expressivity. To tackle this issue and learn more
expressive prior models, we propose an energy-based model (EBM) on the joint
latent space over all layers of latent variables with the multi-layer generator
as its backbone. Such joint latent space EBM prior model captures the
intra-layer contextual relations at each layer through layer-wise energy terms,
and latent variables across different layers are jointly corrected. We develop
a joint training scheme via maximum likelihood estimation (MLE), which involves
Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior
distributions of the latent variables from different layers. To ensure
efficient inference and learning, we further propose a variational training
scheme where an inference model is used to amortize the costly posterior MCMC
sampling. Our experiments demonstrate that the learned model can be expressive
in generating high-quality images and capturing hierarchical features for
better outlier detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiali Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tian Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06599">
<title>Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing. (arXiv:2306.06599v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06599</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing regression models tend to fall short in both accuracy and
uncertainty estimation when the label distribution is imbalanced. In this
paper, we propose a probabilistic deep learning model, dubbed variational
imbalanced regression (VIR), which not only performs well in imbalanced
regression but naturally produces reasonable uncertainty estimation as a
byproduct. Different from typical variational autoencoders assuming I.I.D.
representations (a data point&apos;s representation is not directly affected by
other data points), our VIR borrows data with similar regression labels to
compute the latent representation&apos;s variational distribution; furthermore,
different from deterministic regression models producing point estimates, VIR
predicts the entire normal-inverse-gamma distributions and modulates the
associated conjugate distributions to impose probabilistic reweighting on the
imbalanced data, thereby providing better uncertainty estimation. Experiments
in several real-world datasets show that our VIR can outperform
state-of-the-art imbalanced regression models in terms of both accuracy and
uncertainty estimation. Code will soon be available at
\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08762">
<title>Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08762</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially observable Markov decision processes (POMDPs) have been widely
applied to capture many real-world applications. However, existing theoretical
results have shown that learning in general POMDPs could be intractable, where
the main challenge lies in the lack of latent state information. A key
fundamental question here is how much online state information (OSI) is
sufficient to achieve tractability. In this paper, we establish a lower bound
that reveals a surprising hardness result: unless we have full OSI, we need an
exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy
solution for POMDPs. Nonetheless, inspired by the key insights in our lower
bound design, we find that there exist important tractable classes of POMDPs
even with only partial OSI. In particular, for two novel classes of POMDPs with
partial OSI, we provide new algorithms that are proved to be near-optimal by
establishing new regret upper and lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Ming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1&quot;&gt;Ness Shroff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11014">
<title>Physics Constrained Unsupervised Deep Learning for Rapid, High Resolution Scanning Coherent Diffraction Reconstruction. (arXiv:2306.11014v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11014</link>
<description rdf:parseType="Literal">&lt;p&gt;By circumventing the resolution limitations of optics, coherent diffractive
imaging (CDI) and ptychography are making their way into scientific fields
ranging from X-ray imaging to astronomy. Yet, the need for time consuming
iterative phase recovery hampers real-time imaging. While supervised deep
learning strategies have increased reconstruction speed, they sacrifice image
quality. Furthermore, these methods&apos; demand for extensive labeled training data
is experimentally burdensome. Here, we propose an unsupervised physics-informed
neural network reconstruction method, PtychoPINN, that retains the factor of
100-to-1000 speedup of deep learning-based reconstruction while improving
reconstruction quality by combining the diffraction forward map with real-space
constraints from overlapping measurements. In particular, PtychoPINN
significantly advances generalizability, accuracy (with a typical 10 dB PSNR
increase), and linear resolution (2- to 6-fold gain). This blend of performance
and speed offers exciting prospects for high-resolution real-time imaging in
high-throughput environments such as X-ray free electron lasers (XFELs) and
diffraction-limited light sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hoidn_O/0/1/0/all/0/1&quot;&gt;Oliver Hoidn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Aashwin Ananda Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Apurva Mehta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11670">
<title>GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11670</link>
<description rdf:parseType="Literal">&lt;p&gt;It is often advantageous to train models on a subset of the available train
examples, because the examples are of variable quality or because one would
like to train with fewer examples, without sacrificing performance. We present
Gradient Information Optimization (GIO), a scalable, task-agnostic approach to
this data selection problem that requires only a small set of (unlabeled)
examples representing a target distribution. GIO begins from a natural,
information-theoretic objective that is intractable in practice. Our
contribution is in showing that it can be made highly scalable through a simple
relaxation of the objective and a highly efficient implementation. In
experiments with machine translation, spelling correction, and image
recognition, we show that GIO delivers outstanding results with very small
train sets. These findings are robust to different representation models and
hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be
applied out-of-the-box to new datasets and domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Everaert_D/0/1/0/all/0/1&quot;&gt;Dante Everaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14041">
<title>Smoothed $f$-Divergence Distributionally Robust Optimization. (arXiv:2306.14041v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14041</link>
<description rdf:parseType="Literal">&lt;p&gt;In data-driven optimization, sample average approximation (SAA) is known to
suffer from the so-called optimizer&apos;s curse that causes an over-optimistic
evaluation of the solution performance. We argue that a special type of
distributionallly robust optimization (DRO) formulation offers theoretical
advantages in correcting for this optimizer&apos;s curse compared to simple
``margin&apos;&apos; adjustments to SAA and other DRO approaches: It attains a
statistical bound on the out-of-sample performance, for a wide class of
objective functions and distributions, that is nearly tightest in terms of
exponential decay rate. This DRO uses an ambiguity set based on a Kullback
Leibler (KL) divergence smoothed by the Wasserstein or L\&apos;evy-Prokhorov (LP)
distance via a suitable distance optimization. Computationally, we also show
that such a DRO, and its generalized versions using smoothed $f$-divergence,
are not harder than DRO problems based on $f$-divergence or Wasserstein
distances, rendering our DRO formulations both statistically optimal and
computationally viable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Parys_B/0/1/0/all/0/1&quot;&gt;Bart P. G. Van Parys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lam_H/0/1/0/all/0/1&quot;&gt;Henry Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16335">
<title>Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX). (arXiv:2306.16335v2 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16335</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel surrogate modelling approach to efficiently and accurately
approximate the response of complex dynamical systems driven by time-varying
exogenous excitations over extended time periods. Our approach, namely manifold
nonlinear autoregressive modelling with exogenous input (mNARX), involves
constructing a problem-specific exogenous input manifold that is optimal for
constructing autoregressive surrogates. The manifold, which forms the core of
mNARX, is constructed incrementally by incorporating the physics of the system,
as well as prior expert- and domain- knowledge. Because mNARX decomposes the
full problem into a series of smaller sub-problems, each with a lower
complexity than the original, it scales well with the complexity of the
problem, both in terms of training and evaluation costs of the final surrogate.
Furthermore, mNARX synergizes well with traditional dimensionality reduction
techniques, making it highly suitable for modelling dynamical systems with
high-dimensional exogenous inputs, a class of problems that is typically
challenging to solve. Since domain knowledge is particularly abundant in
physical systems, such as those found in civil and mechanical engineering,
mNARX is well suited for these applications. We demonstrate that mNARX
outperforms traditional autoregressive surrogates in predicting the response of
a classical coupled spring-mass system excited by a one-dimensional random
excitation. Additionally, we show that mNARX is well suited for emulating very
high-dimensional time- and state-dependent systems, even when affected by
active controllers, by surrogating the dynamics of a realistic
aero-servo-elastic onshore wind turbine simulator. In general, our results
demonstrate that mNARX offers promising prospects for modelling complex
dynamical systems, in terms of accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schar_S/0/1/0/all/0/1&quot;&gt;Styfen Sch&amp;#xe4;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Marelli_S/0/1/0/all/0/1&quot;&gt;Stefano Marelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sudret_B/0/1/0/all/0/1&quot;&gt;Bruno Sudret&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02484">
<title>Elastic Decision Transformer. (arXiv:2307.02484v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02484</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Elastic Decision Transformer (EDT), a significant
advancement over the existing Decision Transformer (DT) and its variants.
Although DT purports to generate an optimal trajectory, empirical evidence
suggests it struggles with trajectory stitching, a process involving the
generation of an optimal or near-optimal trajectory from the best parts of a
set of sub-optimal trajectories. The proposed EDT differentiates itself by
facilitating trajectory stitching during action inference at test time,
achieved by adjusting the history length maintained in DT. Further, the EDT
optimizes the trajectory by retaining a longer history when the previous
trajectory is optimal and a shorter one when it is sub-optimal, enabling it to
&quot;stitch&quot; with a more optimal trajectory. Extensive experimentation demonstrates
EDT&apos;s ability to bridge the performance gap between DT-based and Q
Learning-based approaches. In particular, the EDT outperforms Q Learning-based
methods in a multi-task regime on the D4RL locomotion benchmark and Atari
games. Videos are available at: https://kristery.github.io/edt/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamaya_M/0/1/0/all/0/1&quot;&gt;Masashi Hamaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03135">
<title>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student&apos;s OOD
generalization: (1) by better imitating teacher&apos;s visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher&apos;s language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Poster:
https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf
Code: https://github.com/xuanlinli17/large_vlm_distillation_ood
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03486">
<title>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03486</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering achievements with a hierarchical structure in procedurally
generated environments presents a significant challenge. This requires an agent
to possess a broad range of abilities, including generalization and long-term
reasoning. Many prior methods have been built upon model-based or hierarchical
approaches, with the belief that an explicit module for long-term planning
would be advantageous for learning hierarchical dependencies. However, these
methods demand an excessive number of environment interactions or large model
sizes, limiting their practicality. In this work, we demonstrate that proximal
policy optimization (PPO), a simple yet versatile model-free algorithm,
outperforms previous methods when optimized with recent implementation
practices. Moreover, we find that the PPO agent can predict the next
achievement to be unlocked to some extent, albeit with limited confidence.
Based on this observation, we introduce a novel contrastive learning method,
called achievement distillation, which strengthens the agent&apos;s ability to
predict the next achievement. Our method exhibits a strong capacity for
discovering hierarchical achievements and shows state-of-the-art performance on
the challenging Crafter environment in a sample-efficient manner while
utilizing fewer model parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seungyong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1&quot;&gt;Junyoung Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1&quot;&gt;Bumsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03807">
<title>Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction. (arXiv:2308.03807v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03807</link>
<description rdf:parseType="Literal">&lt;p&gt;Proximal gradient-based optimization is one of the most common strategies to
solve inverse problem of images, and it is easy to implement. However, these
techniques often generate heavy artifacts in image reconstruction. One of the
most popular refinement methods is to fine-tune the regularization parameter to
alleviate such artifacts, but it may not always be sufficient or applicable due
to increased computational costs. In this work, we propose a deep geometric
incremental learning framework based on the second Nesterov proximal gradient
optimization. The proposed end-to-end network not only has the powerful
learning ability for high-/low-frequency image features, but also can
theoretically guarantee that geometric texture details will be reconstructed
from preliminary linear reconstruction. Furthermore, it can avoid the risk of
intermediate reconstruction results falling outside the geometric decomposition
domains and achieve fast convergence. Our reconstruction framework is
decomposed into four modules including general linear reconstruction, cascade
geometric incremental restoration, Nesterov acceleration, and post-processing.
In the image restoration step, a cascade geometric incremental learning module
is designed to compensate for missing texture information from different
geometric spectral decomposition domains. Inspired by the overlap-tile
strategy, we also develop a post-processing module to remove the block effect
in patch-wise-based natural image reconstruction. All parameters in the
proposed model are learnable, an adaptive initialization technique of physical
parameters is also employed to make model flexibility and ensure converging
smoothly. We compare the reconstruction performance of the proposed method with
existing state-of-the-art methods to demonstrate its superiority. Our source
codes are available at https://github.com/fanxiaohong/Nest-DGIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaohong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yujie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04102">
<title>Asynchronous Evolution of Deep Neural Network Architectures. (arXiv:2308.04102v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04102</link>
<description rdf:parseType="Literal">&lt;p&gt;Many evolutionary algorithms (EAs) take advantage of parallel evaluation of
candidates. However, if evaluation times vary significantly, many worker nodes
(i.e.,\ compute clients) are idle much of the time, waiting for the next
generation to be created. Evolutionary neural architecture search (ENAS), a
class of EAs that optimizes the architecture and hyperparameters of deep neural
networks, is particularly vulnerable to this issue. This paper proposes a
generic asynchronous evaluation strategy (AES) that is then adapted to work
with ENAS. AES increases throughput by maintaining a queue of up to $K$
individuals ready to be sent to the workers for evaluation and proceeding to
the next generation as soon as $M&amp;lt;&amp;lt;K$ individuals have been evaluated. A
suitable value for $M$ is determined experimentally, balancing diversity and
efficiency. To showcase the generality and power of AES, it was first evaluated
in eight-line sorting network design (a single-population optimization task
with limited evaluation-time variability), achieving an over two-fold speedup.
Next, it was evaluated in 11-bit multiplexer design (a single-population
discovery task with extended variability), where a 14-fold speedup was
observed. It was then scaled up to ENAS for image captioning (a
multi-population open-ended-optimization task), resulting in an over two-fold
speedup. In all problems, a multifold performance improvement was observed,
suggesting that AES is a promising method for parallelizing the evolution of
complex systems with long and variable evaluation times, such as those in ENAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jason Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahrzad_H/0/1/0/all/0/1&quot;&gt;Hormoz Shahrzad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04263">
<title>BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04263</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces BarlowRL, a data-efficient reinforcement learning agent
that combines the Barlow Twins self-supervised learning framework with DER
(Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its
contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids
dimensional collapse by enforcing information spread to the whole space. This
helps RL algorithms to utilize uniformly spread state representation that
eventually results in a remarkable performance. The integration of Barlow Twins
with DER enhances data efficiency and achieves superior performance in the RL
tasks. BarlowRL demonstrates the potential of incorporating self-supervised
learning techniques to improve RL algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagatan_O/0/1/0/all/0/1&quot;&gt;Omer Veysel Cagatan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akgun_B/0/1/0/all/0/1&quot;&gt;Baris Akgun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08469">
<title>LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08469</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we leverage pre-trained Large Language Models (LLMs) to enhance
time-series forecasting. Mirroring the growing interest in unifying models for
Natural Language Processing and Computer Vision, we envision creating an
analogous model for long-term time-series forecasting. Due to limited
large-scale time-series data for building robust foundation models, our
approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By
combining time-series patching with temporal encoding, we have enhanced the
capability of LLMs to handle time-series data effectively. Inspired by the
supervised fine-tuning in chatbot domains, we prioritize a two-stage
fine-tuning process: first conducting supervised fine-tuning to orient the LLM
towards time-series data, followed by task-specific downstream fine-tuning.
Furthermore, to unlock the flexibility of pre-trained LLMs without extensive
parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT)
techniques. Drawing on these innovations, LLM4TS has yielded state-of-the-art
results in long-term forecasting. Our model has also shown exceptional
capabilities as both a robust representation learner and an effective few-shot
learner, thanks to the knowledge transferred from the pre-trained LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Ching Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wen-Chih Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tien-Fu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12243">
<title>Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hotegni_S/0/1/0/all/0/1&quot;&gt;S. S. Hotegni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1&quot;&gt;S. Peitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkemeier_M/0/1/0/all/0/1&quot;&gt;M. Berkemeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16198">
<title>Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16198</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern communication systems, efficient and reliable information
dissemination is crucial for supporting critical operations across domains like
disaster response, autonomous vehicles, and sensor networks. This paper
introduces a Multi-Agent Reinforcement Learning (MARL) approach as a
significant step forward in achieving more decentralized, efficient, and
collaborative solutions. We propose a Partially Observable Stochastic Game
(POSG) formulation for information dissemination empowering each agent to
decide on message forwarding independently, based on their one-hop
neighborhood. This constitutes a significant paradigm shift from traditional
heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses
Graph Convolutional Reinforcement Learning, employing Graph Attention Networks
(GAT) with dynamic attention to capture essential network features. We propose
two approaches, L-DGN and HL-DGN, which differ in the information that is
exchanged among agents. We evaluate the performance of our decentralized
approaches, by comparing them with a widely-used MPR heuristic, and we show
that our trained policies are able to efficiently cover the network while
bypassing the MPR set selection process. Our approach is a first step toward
supporting the resilience of real-world broadcast communication infrastructures
via learned, collaborative information dissemination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galliera_R/0/1/0/all/0/1&quot;&gt;Raffaele Galliera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venable_K/0/1/0/all/0/1&quot;&gt;Kristen Brent Venable&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassani_M/0/1/0/all/0/1&quot;&gt;Matteo Bassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suri_N/0/1/0/all/0/1&quot;&gt;Niranjan Suri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00848">
<title>Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00848</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using
the YOLOv8 model and innovative post-processing techniques. We tackle
challenges unique to the complex Bengali script by employing data augmentation
for model robustness. After meticulous validation set evaluation, we fine-tune
our approach on the complete dataset, leading to a two-stage prediction
strategy for accurate element segmentation. Our ensemble model, combined with
post-processing, outperforms individual base architectures, addressing issues
identified in the BaDLAD dataset. By leveraging this approach, we aim to
advance Bengali document analysis, contributing to improved OCR and document
comprehension and BaDLAD serves as a foundational resource for this endeavor,
aiding future research in the field. Furthermore, our experiments provided key
insights to incorporate new strategies into the established solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nazmus Sakib Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noor_S/0/1/0/all/0/1&quot;&gt;Saad Sakib Noor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikder_A/0/1/0/all/0/1&quot;&gt;Ashraful Islam Shanto Sikder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1&quot;&gt;Abhijit Paul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02285">
<title>PromptTTS 2: Describing and Generating Voices with Text Prompt. (arXiv:2309.02285v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02285</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech conveys more information than text, as the same word can be uttered in
various voices to convey diverse information. Compared to traditional
text-to-speech (TTS) methods relying on speech prompts (reference speech) for
voice variability, using text prompts (descriptions) is more user-friendly
since speech prompts can be hard to find or may not exist at all. TTS
approaches based on the text prompt face two main challenges: 1) the
one-to-many problem, where not all details about voice variability can be
described in the text prompt, and 2) the limited availability of text prompt
datasets, where vendors and large cost of data labeling are required to write
text prompts for speech. In this work, we introduce PromptTTS 2 to address
these challenges with a variation network to provide variability information of
voice not captured by text prompts, and a prompt generation pipeline to utilize
the large language models (LLM) to compose high quality text prompts.
Specifically, the variation network predicts the representation extracted from
the reference speech (which contains full information about voice variability)
based on the text prompt representation. For the prompt generation pipeline, it
generates text prompts for speech with a speech language understanding model to
recognize voice attributes (e.g., gender, speed) from speech and a large
language model to formulate text prompts based on the recognition results.
Experiments on a large-scale (44K hours) speech dataset demonstrate that
compared to the previous works, PromptTTS 2 generates voices more consistent
with text prompts and supports the sampling of diverse voice variability,
thereby offering users more choices on voice generation. Additionally, the
prompt generation pipeline produces high-quality text prompts, eliminating the
large labeling cost. The demo page of PromptTTS 2 is available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1&quot;&gt;Yichong Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhifang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_K/0/1/0/all/0/1&quot;&gt;Kai Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_Z/0/1/0/all/0/1&quot;&gt;Zeqian Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yufei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dongchao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaitao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang-Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03004">
<title>A Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03004</link>
<description rdf:parseType="Literal">&lt;p&gt;A recent empirical observation (Li et al., 2022b) of activation sparsity in
MLP blocks offers an opportunity to drastically reduce computation costs for
free. Although having attributed it to training dynamics, existing theoretical
explanations of activation sparsity are restricted to shallow networks, small
training steps and special training, despite its emergence in deep models
standardly trained for a large number of steps. To fill these gaps, we propose
the notion of gradient sparsity as one source of activation sparsity and a
theoretical explanation based on it that sees sparsity a necessary step to
adversarial robustness w.r.t. hidden features and parameters, which is
approximately the flatness of minima for well-learned models. The theory
applies to standardly trained LayerNorm-ed MLPs, and further to Transformers or
other architectures trained with weight noises. Eliminating other sources of
flatness except for sparsity, we discover the phenomenon that the ratio between
the largest and smallest non-zero singular values of weight matrices is small.
When discussing the emergence of this spectral concentration, we use random
matrix theory (RMT) as a powerful tool to analyze stochastic gradient noises.
Validational experiments are conducted to verify our gradient-sparsity-based
explanation. We propose two plug-and-play modules for both training and
finetuning for sparsity. Experiments on ImageNet-1k and C4 demonstrate their
50% sparsity improvements, indicating further potential cost reduction in both
training and inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Ze Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lei Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03084">
<title>Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03084</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual Regret Minimization (CFR) and its variants are the best
algorithms so far for solving large-scale incomplete information games.
However, we believe that there are two problems with CFR: First, matrix
multiplication is required in CFR iteration, and the time complexity of one
iteration is too high; Secondly, the game characteristics in the real world are
different. Just using one CFR algorithm will not be perfectly suitable for all
game problems.
&lt;/p&gt;
&lt;p&gt;For these two problems, this paper proposes a new algorithm called Pure CFR
(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious
Play (FP), inheriting the concept of counterfactual regret (value) from CFR,
and using the best response strategy instead of the regret matching strategy
for the next iteration. This algorithm has three advantages. First, PCFR can be
combined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can
significantly reduce the time and space complexity of one iteration. Secondly,
our experiments show that the convergence speed of the PMCCFR is 2$\sim$3 times
that of the MCCFR. Finally, there is a type of game that is very suitable for
PCFR, we call this type of game clear-game, which is characterized by a high
proportion of dominated strategies. Experiments show that in clear-game, the
convergence rate of PMCCFR is two orders of magnitude higher than that of
MCCFR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Ju Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Ting Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hei_F/0/1/0/all/0/1&quot;&gt;Falun Hei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhemei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04370">
<title>Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control. (arXiv:2309.04370v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04370</link>
<description rdf:parseType="Literal">&lt;p&gt;Seeing-eye robots are very useful tools for guiding visually impaired people,
potentially producing a huge societal impact given the low availability and
high cost of real guide dogs. Although a few seeing-eye robot systems have
already been demonstrated, none considered external tugs from humans, which
frequently occur in a real guide dog setting. In this paper, we simultaneously
train a locomotion controller that is robust to external tugging forces via
Reinforcement Learning (RL), and an external force estimator via supervised
learning. The controller ensures stable walking, and the force estimator
enables the robot to respond to the external forces from the human. These
forces are used to guide the robot to the global goal, which is unknown to the
robot, while the robot guides the human around nearby obstacles via a local
planner. Experimental results in simulation and on hardware show that our
controller is robust to external forces, and our seeing-eye system can
accurately detect force direction. We demonstrate our full seeing-eye robot
system on a real quadruped robot with a blindfolded human. The video can be
seen at our project page: https://bu-air-lab.github.io/guide_dog/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeFazio_D/0/1/0/all/0/1&quot;&gt;David DeFazio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirota_E/0/1/0/all/0/1&quot;&gt;Eisuke Hirota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05173">
<title>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer&apos;s quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05925">
<title>On Regularized Sparse Logistic Regression. (arXiv:2309.05925v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05925</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse logistic regression is for classification and feature selection
simultaneously. Although many studies have been done to solve
$\ell_1$-regularized logistic regression, there is no equivalently abundant
work on solving sparse logistic regression with nonconvex regularization term.
In this paper, we propose a unified framework to solve $\ell_1$-regularized
logistic regression, which can be naturally extended to nonconvex
regularization term, as long as certain requirement is satisfied. In addition,
we also utilize a different line search criteria to guarantee monotone
convergence for various regularization terms. Empirical experiments on binary
classification tasks with real-world datasets demonstrate our proposed
algorithms are capable of performing classification and feature selection
effectively at a lower computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10691">
<title>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10691</link>
<description rdf:parseType="Literal">&lt;p&gt;To solve complex tasks, large language models (LLMs) often require multiple
rounds of interactions with the user, sometimes assisted by external tools.
However, current evaluation protocols often emphasize benchmark performance
with single-turn exchanges, neglecting the nuanced interactions among the user,
LLMs, and external tools, while also underestimating the importance of natural
language feedback from users. These oversights contribute to discrepancies
between research benchmark evaluations and real-world use cases. We introduce
MINT, a benchmark that evaluates LLMs&apos; ability to solve tasks with multi-turn
interactions by (1) using tools and (2) leveraging natural language feedback.
To ensure reproducibility, we provide an evaluation framework where LLMs can
access tools by executing Python code and receive users&apos; natural language
feedback simulated by GPT-4. We repurpose a diverse set of established
evaluation datasets focusing on reasoning, coding, and decision-making and
carefully curate them into a compact subset for efficient evaluation. Our
analysis of 20 open- and closed-source LLMs offers intriguing findings. (a)
LLMs generally benefit from tools and language feedback, with performance gains
(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural
language feedback. (b) Better single-turn performance does not guarantee better
multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised
instruction-finetuning (SIFT) and reinforcement learning from human feedback
(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure
progress and incentivize research in improving LLMs&apos; capabilities in multi-turn
interactions, especially for open-source communities where multi-turn human
evaluation can be less accessible compared to commercial LLMs with a larger
user base.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiateng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lifan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15395">
<title>Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15395</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the best policy identification (BPI) problem in online
Constrained Markov Decision Processes (CMDPs). We are interested in algorithms
that are model-free, have low regret, and identify an optimal policy with a
high probability. Existing model-free algorithms for online CMDPs with
sublinear regret and constraint violation do not provide any convergence
guarantee to an optimal policy and provide only average performance guarantees
when a policy is uniformly sampled at random from all previously used policies.
In this paper, we develop a new algorithm, named
Pruning-Refinement-Identification (PRI), based on a fundamental structural
property of CMDPs we discover, called limited stochasticity. The property says
for a CMDP with $N$ constraints, there exists an optimal policy with at most
$N$ stochastic decisions.
&lt;/p&gt;
&lt;p&gt;The proposed algorithm first identifies at which step and in which state a
stochastic decision has to be taken and then fine-tunes the distributions of
these stochastic decisions. PRI achieves trio objectives: (i) PRI is a
model-free algorithm; and (ii) it outputs a near-optimal policy with a high
probability at the end of learning; and (iii) in the tabular setting, PRI
guarantees $\tilde{\mathcal{O}}(\sqrt{K})$ regret and constraint violation,
which significantly improves the best existing regret bound
$\tilde{\mathcal{O}}(K^{\frac{4}{5}})$ under a model-free algorithm, where $K$
is the total number of episodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zihan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Honghao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1&quot;&gt;Lei Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15505">
<title>Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15505</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to replace vector quantization (VQ) in the latent representation
of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where
we project the VAE representation down to a few dimensions (typically less than
10). Each dimension is quantized to a small set of fixed values, leading to an
(implicit) codebook given by the product of these sets. By appropriately
choosing the number of dimensions and values each dimension can take, we obtain
the same codebook size as in VQ. On top of such discrete representations, we
can train the same models that have been trained on VQ-VAE representations. For
example, autoregressive and masked transformer models for image generation,
multimodal generation, and dense prediction computer vision tasks. Concretely,
we employ FSQ with MaskGIT for image generation, and with UViM for depth
estimation, colorization, and panoptic segmentation. Despite the much simpler
design of FSQ, we obtain competitive performance in all these tasks. We
emphasize that FSQ does not suffer from codebook collapse and does not need the
complex machinery employed in VQ (commitment losses, codebook reseeding, code
splitting, entropy penalties, etc.) to learn expressive discrete
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mentzer_F/0/1/0/all/0/1&quot;&gt;Fabian Mentzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1&quot;&gt;David Minnen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agustsson_E/0/1/0/all/0/1&quot;&gt;Eirikur Agustsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1&quot;&gt;Michael Tschannen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16584">
<title>A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16584</link>
<description rdf:parseType="Literal">&lt;p&gt;To leverage data for the sufficient training of machine learning (ML) models
from multiple parties in a confidentiality-preserving way, various
collaborative distributed ML (CDML) system designs have been developed, for
example, to perform assisted learning, federated learning, and split learning.
CDML system designs show different traits, including high agent autonomy, ML
model confidentiality, and fault tolerance. Facing a wide variety of CDML
system designs with different traits, it is difficult for developers to design
CDML systems with traits that match use case requirements in a targeted way.
However, inappropriate CDML system designs may result in CDML systems failing
their envisioned purposes. We developed a CDML design toolbox that can guide
the development of CDML systems. Based on the CDML design toolbox, we present
CDML system archetypes with distinct key traits that can support the design of
CDML systems to meet use case requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;David Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannengiesser_N/0/1/0/all/0/1&quot;&gt;Niclas Kannengie&amp;#xdf;er&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rank_S/0/1/0/all/0/1&quot;&gt;Sascha Rank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunyaev_A/0/1/0/all/0/1&quot;&gt;Ali Sunyaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00177">
<title>A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00177</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a neural-preconditioned iterative solver for Poisson equations
with mixed boundary conditions. The Poisson equation is ubiquitous in
scientific computing: it governs a wide array of physical phenomena, arises as
a subproblem in many numerical algorithms, and serves as a model problem for
the broader class of elliptic PDEs. The most popular Poisson discretizations
yield large sparse linear systems. At high resolution, and for
performance-critical applications, iterative solvers can be advantageous for
these -- but only when paired with powerful preconditioners. The core of our
solver is a neural network trained to approximate the inverse of a discrete
structured-grid Laplace operator for a domain of arbitrary shape and with mixed
boundary conditions. The structure of this problem motivates a novel network
architecture that we demonstrate is highly effective as a preconditioner even
for boundary conditions outside the training set. We show that on challenging
test cases arising from an incompressible fluid simulation, our method
outperforms state-of-the-art solvers like algebraic multigrid as well as some
recent neural preconditioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lan_K/0/1/0/all/0/1&quot;&gt;Kai Weixian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gueidon_E/0/1/0/all/0/1&quot;&gt;Elias Gueidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kaneda_A/0/1/0/all/0/1&quot;&gt;Ayano Kaneda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Panetta_J/0/1/0/all/0/1&quot;&gt;Julian Panetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Teran_J/0/1/0/all/0/1&quot;&gt;Joseph Teran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00327">
<title>Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00327</link>
<description rdf:parseType="Literal">&lt;p&gt;In practice, deep neural networks are often able to easily interpolate their
training data. To understand this phenomenon, many works have aimed to quantify
the memorization capacity of a neural network architecture: the largest number
of points such that the architecture can interpolate any placement of these
points with any assignment of labels. For real-world data, however, one
intuitively expects the presence of a benign structure so that interpolation
already occurs at a smaller network size than suggested by memorization
capacity. In this paper, we investigate interpolation by adopting an
instance-specific viewpoint. We introduce a simple randomized algorithm that,
given a fixed finite dataset with two classes, with high probability constructs
an interpolating three-layer neural network in polynomial time. The required
number of parameters is linked to geometric properties of the two classes and
their mutual arrangement. As a result, we obtain guarantees that are
independent of the number of samples and hence move beyond worst-case
memorization capacity bounds. We illustrate the effectiveness of the algorithm
in non-pathological situations with extensive numerical experiments and link
the insights back to the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dirksen_S/0/1/0/all/0/1&quot;&gt;Sjoerd Dirksen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Finke_P/0/1/0/all/0/1&quot;&gt;Patrick Finke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Genzel_M/0/1/0/all/0/1&quot;&gt;Martin Genzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01649">
<title>On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01649</link>
<description rdf:parseType="Literal">&lt;p&gt;We refer to the setting where the (partial) derivatives of a neural network&apos;s
(NN&apos;s) predictions with respect to its inputs are used as additional training
signal as a derivative-constrained (DC) NN. This situation is common in
physics-informed settings in the natural sciences. We propose an integrated
RELU (IReLU) activation function to improve training of DC NNs. We also
investigate denormalization and label rescaling to help stabilize DC training.
We evaluate our methods on physics-informed settings including quantum
chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that
existing architectures with IReLU activations combined with denormalization and
label rescaling better incorporate training signal provided by derivative
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1&quot;&gt;KaiChieh Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Daniel Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04413">
<title>Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04413</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline policy learning is aimed at learning decision-making policies using
existing datasets of trajectories without collecting additional data. The
primary motivation for using reinforcement learning (RL) instead of supervised
learning techniques such as behavior cloning is to find a policy that achieves
a higher average return than the trajectories constituting the dataset.
However, we empirically find that when a dataset is dominated by suboptimal
trajectories, state-of-the-art offline RL algorithms do not substantially
improve over the average return of trajectories in the dataset. We argue this
is due to an assumption made by current offline RL algorithms of staying close
to the trajectories in the dataset. If the dataset primarily consists of
sub-optimal trajectories, this assumption forces the policy to mimic the
suboptimal actions. We overcome this issue by proposing a sampling strategy
that enables the policy to only be constrained to ``good data&quot; rather than all
actions in the dataset (i.e., uniform sampling). We present a realization of
the sampling strategy and an algorithm that can be used as a plug-and-play
module in standard offline RL algorithms. Our evaluation demonstrates
significant performance gains in 72 imbalanced datasets, D4RL dataset, and
across three different offline RL algorithms. Code is available at
https://github.com/Improbable-AI/dw-offline-rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhang-Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnik_S/0/1/0/all/0/1&quot;&gt;Sathwik Karnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1&quot;&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04610">
<title>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. (arXiv:2310.04610v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04610</link>
<description rdf:parseType="Literal">&lt;p&gt;In the upcoming decade, deep learning may revolutionize the natural sciences,
enhancing our capacity to model and predict natural occurrences. This could
herald a new era of scientific exploration, bringing significant advancements
across sectors from drug development to renewable energy. To answer this call,
we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to
build unique capabilities through AI system technology innovations to help
domain experts to unlock today&apos;s biggest science mysteries. By leveraging
DeepSpeed&apos;s current technology pillars (training, inference and compression) as
base technology enablers, DeepSpeed4Science will create a new set of AI system
technologies tailored for accelerating scientific discoveries by addressing
their unique complexity beyond the common technical approaches used for
accelerating generic large language models (LLMs). In this paper, we showcase
the early progress we made with DeepSpeed4Science in addressing two of the
critical system challenges in structural biology research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruft_B/0/1/0/all/0/1&quot;&gt;Bonnie Kruft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Conglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1&quot;&gt;Masahiro Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasley_J/0/1/0/all/0/1&quot;&gt;Jeff Rasley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1&quot;&gt;Ammar Ahmad Awan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Connor Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1&quot;&gt;Martin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_A/0/1/0/all/0/1&quot;&gt;Adam Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhongzhu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luferenko_P/0/1/0/all/0/1&quot;&gt;Pete Luferenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Divya Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyn_J/0/1/0/all/0/1&quot;&gt;Jonathan Weyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klocek_S/0/1/0/all/0/1&quot;&gt;Sylwester Klocek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vragov_V/0/1/0/all/0/1&quot;&gt;Volodymyr Vragov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlQuraishi_M/0/1/0/all/0/1&quot;&gt;Mohammed AlQuraishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahdritz_G/0/1/0/all/0/1&quot;&gt;Gustaf Ahdritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Floristean_C/0/1/0/all/0/1&quot;&gt;Christina Floristean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negri_C/0/1/0/all/0/1&quot;&gt;Cristina Negri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotamarthi_R/0/1/0/all/0/1&quot;&gt;Rao Kotamarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanath_V/0/1/0/all/0/1&quot;&gt;Venkatram Vishwanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1&quot;&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foreman_S/0/1/0/all/0/1&quot;&gt;Sam Foreman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hippe_K/0/1/0/all/0/1&quot;&gt;Kyle Hippe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcomano_T/0/1/0/all/0/1&quot;&gt;Troy Arcomano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maulik_R/0/1/0/all/0/1&quot;&gt;Romit Maulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zvyagin_M/0/1/0/all/0/1&quot;&gt;Maxim Zvyagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brace_A/0/1/0/all/0/1&quot;&gt;Alexander Brace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohorquez_C/0/1/0/all/0/1&quot;&gt;Cindy Orozco Bohorquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1&quot;&gt;Austin Clyde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_B/0/1/0/all/0/1&quot;&gt;Bharat Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Rivera_D/0/1/0/all/0/1&quot;&gt;Danilo Perez-Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Heng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_C/0/1/0/all/0/1&quot;&gt;Carla M. Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_M/0/1/0/all/0/1&quot;&gt;Michael Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauloski_J/0/1/0/all/0/1&quot;&gt;J. Gregory Pauloski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ward_L/0/1/0/all/0/1&quot;&gt;Logan Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayot_V/0/1/0/all/0/1&quot;&gt;Valerie Hayot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1&quot;&gt;Murali Emani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Diangen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1&quot;&gt;Maulik Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1&quot;&gt;Ian Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;James J. Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papka_M/0/1/0/all/0/1&quot;&gt;Michael E. Papka&lt;/a&gt;, et al. (40 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04948">
<title>TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04948</link>
<description rdf:parseType="Literal">&lt;p&gt;The past decade has witnessed significant advances in time series modeling
with deep learning. While achieving state-of-the-art results, the
best-performing architectures vary highly across applications and domains.
Meanwhile, for natural language processing, the Generative Pre-trained
Transformer (GPT) has demonstrated impressive performance via training one
general-purpose model across various textual datasets. It is intriguing to
explore whether GPT-type architectures can be effective for time series,
capturing the intrinsic dynamic attributes and leading to significant accuracy
improvements. In this paper, we propose a novel framework, TEMPO, that can
effectively learn time series representations. We focus on utilizing two
essential inductive biases of the time series task for pre-trained models: (i)
decomposition of the complex interaction between trend, seasonal and residual
components; and (ii) introducing the selection-based prompts to facilitate
distribution adaptation in non-stationary time series. TEMPO expands the
capability for dynamically modeling real-world temporal phenomena from data
within diverse domains. Our experiments demonstrate the superior performance of
TEMPO over state-of-the-art methods on a number of time series benchmark
datasets. This performance gain is observed not only in standard supervised
learning settings but also in scenarios involving previously unseen datasets as
well as in scenarios with multi-modal inputs. This compelling finding
highlights TEMPO&apos;s potential to constitute a foundational model-building
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1&quot;&gt;Defu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Furong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1&quot;&gt;Sercan O Arik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1&quot;&gt;Tomas Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yixiang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05288">
<title>Clustering Three-Way Data with Outliers. (arXiv:2310.05288v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05288</link>
<description rdf:parseType="Literal">&lt;p&gt;Matrix-variate distributions are a recent addition to the model-based
clustering field, thereby making it possible to analyze data in matrix form
with complex structure such as images and time series. Due to its recent
appearance, there is limited literature on matrix-variate data, with even less
on dealing with outliers in these models. An approach for clustering
matrix-variate normal data with outliers is discussed. The approach, which uses
the distribution of subset log-likelihoods, extends the OCLUST algorithm to
matrix-variate normal data and uses an iterative approach to detect and trim
outliers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Clark_K/0/1/0/all/0/1&quot;&gt;Katharine M. Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McNicholas_P/0/1/0/all/0/1&quot;&gt;Paul D. McNicholas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05624">
<title>Locality-Aware Generalizable Implicit Neural Representation. (arXiv:2310.05624v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05624</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalizable implicit neural representation (INR) enables a single
continuous function, i.e., a coordinate-based neural network, to represent
multiple data instances by modulating its weights or intermediate features
using latent codes. However, the expressive power of the state-of-the-art
modulation is limited due to its inability to localize and capture fine-grained
details of data entities such as specific pixels and rays. To address this
issue, we propose a novel framework for generalizable INR that combines a
transformer encoder with a locality-aware INR decoder. The transformer encoder
predicts a set of latent tokens from a data instance to encode local
information into each latent token. The locality-aware INR decoder extracts a
modulation vector by selectively aggregating the latent tokens via
cross-attention for a coordinate input and then predicts the output by
progressively decoding with coarse-to-fine modulation through multiple
frequency bandwidths. The selective token aggregation and the multi-band
feature modulation enable us to learn locality-aware representation in spatial
and spectral aspects, respectively. Our framework significantly outperforms
previous generalizable INRs and validates the usefulness of the locality-aware
latents for downstream tasks such as image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Doyup Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chiheon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsu Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wook-Shin Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05898">
<title>Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05898</link>
<description rdf:parseType="Literal">&lt;p&gt;Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion&apos;s efficacy.
&lt;/p&gt;
&lt;p&gt;This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lizhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06225">
<title>GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06225</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs&apos; performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4&apos;s ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models&apos; capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Bruno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_L/0/1/0/all/0/1&quot;&gt;Leonardo Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estevao_R/0/1/0/all/0/1&quot;&gt;Roberto Estev&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aski_V/0/1/0/all/0/1&quot;&gt;Vijay Aski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Ranveer Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06488">
<title>SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06488</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) have demonstrated the capability to achieve
comparable performance to deep neural networks (DNNs) in both visual and
linguistic domains while offering the advantages of improved energy efficiency
and adherence to biological plausibility. However, the extension of such
single-modality SNNs into the realm of multimodal scenarios remains an
unexplored territory. Drawing inspiration from the concept of contrastive
language-image pre-training (CLIP), we introduce a novel framework, named
SpikeCLIP, to address the gap between two modalities within the context of
spike-based computing through a two-step recipe involving ``Alignment
Pre-training + Dual-Loss Fine-tuning&quot;. Extensive experiments demonstrate that
SNNs achieve comparable results to their DNN counterparts while significantly
reducing energy consumption across a variety of datasets commonly used for
multimodal model evaluation. Furthermore, SpikeCLIP maintains robust
performance in image classification tasks that involve class labels not
predefined within specific categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Changze Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianhan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cenyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Muling Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06763">
<title>FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06763</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the interaction between proteins and ligands and accurately
predicting their binding structures is a critical yet challenging task in drug
discovery. Recent advancements in deep learning have shown promise in
addressing this challenge, with sampling-based and regression-based methods
emerging as two prominent approaches. However, these methods have notable
limitations. Sampling-based methods often suffer from low efficiency due to the
need for generating multiple candidate structures for selection. On the other
hand, regression-based methods offer fast predictions but may experience
decreased accuracy. Additionally, the variation in protein sizes often requires
external modules for selecting suitable binding pockets, further impacting
efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model
that combines pocket prediction and docking to achieve accurate and fast
protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed
pocket prediction module, which is also leveraged for docking pose estimation.
The model further enhances the docking process by incrementally integrating the
predicted pocket to optimize protein-ligand binding, reducing discrepancies
between training and inference. Through extensive experiments on benchmark
datasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages in
terms of effectiveness and efficiency compared to existing methods. Our code is
available at $\href{https://github.com/QizhiPei/FABind}{Github}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1&quot;&gt;Qizhi Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jinhua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingce Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shufang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06823">
<title>NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06823</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse&apos;&apos;, a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse&apos;&apos; and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. We plan to release the code after the anonymity
period.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ammar_M/0/1/0/all/0/1&quot;&gt;Mou&amp;#xef;n Ben Ammar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belkhir_N/0/1/0/all/0/1&quot;&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Popescu_S/0/1/0/all/0/1&quot;&gt;Sebastian Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Manzanera_A/0/1/0/all/0/1&quot;&gt;Antoine Manzanera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franchi_G/0/1/0/all/0/1&quot;&gt;Gianni Franchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06970">
<title>Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06970</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks are a natural fit for learning algorithms. They can
directly represent tasks through an abstract but versatile graph structure and
handle inputs of different sizes. This opens up the possibility for scaling and
extrapolation to larger graphs, one of the most important advantages of an
algorithm. However, this raises two core questions i) How can we enable nodes
to gather the required information in a given graph ($\textit{information
exchange}$), even if is far away and ii) How can we design an execution
framework which enables this information exchange for extrapolation to larger
graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a
new execution framework that is inspired by the design principles of
distributed algorithms: Flood and Echo Net. It propagates messages through the
entire graph in a wave like activation pattern, which naturally generalizes to
larger instances. Through its sparse but parallel activations it is provably
more efficient in terms of message complexity. We study the proposed model and
provide both empirical evidence and theoretical insights in terms of its
expressiveness, efficiency, information exchange and ability to extrapolate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l Mathys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1&quot;&gt;Florian Gr&amp;#xf6;tschla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadimpalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Varma Nadimpalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07171">
<title>Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07171</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has surged in prominence due to its capability of
collaborative model training without direct data sharing. However, the vast
disparity in local data distributions among clients, often termed the
non-Independent Identically Distributed (non-IID) challenge, poses a
significant hurdle to FL&apos;s generalization efficacy. The scenario becomes even
more complex when not all clients participate in the training process, a common
occurrence due to unstable network connections or limited computational
capacities. This can greatly complicate the assessment of the trained models&apos;
generalization abilities. While a plethora of recent studies has centered on
the generalization gap pertaining to unseen data from participating clients
with diverse distributions, the divergence between the training distributions
of participating clients and the testing distributions of non-participating
ones has been largely overlooked. In response, our paper unveils an
information-theoretic generalization framework for FL. Specifically, it
quantifies generalization errors by evaluating the information entropy of local
distributions and discerning discrepancies across these distributions. Inspired
by our deduced generalization bounds, we introduce a weighted aggregation
approach and a duo of client selection strategies. These innovations aim to
bolster FL&apos;s generalization prowess by encompassing a more varied set of client
data distributions. Our extensive empirical evaluations reaffirm the potency of
our proposed methods, aligning seamlessly with our theoretical construct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zheshun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07297">
<title>Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07297</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in offline reinforcement learning have uncovered the
immense potential of diffusion modeling, which excels at representing
heterogeneous behavior policies. However, sampling from diffusion policies is
considerably slow because it necessitates tens to hundreds of iterative
inference steps for one action. To address this issue, we propose to extract an
efficient deterministic inference policy from critic models and pretrained
diffusion behavior models, leveraging the latter to directly regularize the
policy gradient with the behavior distribution&apos;s score function during
optimization. Our method enjoys powerful generative capabilities of diffusion
modeling while completely circumventing the computationally intensive and
time-consuming diffusion sampling scheme, both during training and evaluation.
Extensive results on D4RL tasks show that our method boosts action sampling
speed by more than 25 times compared with various leading diffusion-based
methods in locomotion tasks, while still maintaining state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07312">
<title>WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07312</link>
<description rdf:parseType="Literal">&lt;p&gt;Innovative foundation models, such as GPT-3 and stable diffusion models, have
made a paradigm shift in the realm of artificial intelligence (AI) towards
generative AI-based systems. In unison, from data communication and networking
perspective, AI and machine learning (AI/ML) algorithms are envisioned to be
pervasively incorporated into the future generations of wireless communications
systems, highlighting the need for novel AI-native solutions for the emergent
communication scenarios. In this article, we outline the applications of
generative AI in wireless communication systems to lay the foundations for
research in this field. Diffusion-based generative models, as the new
state-of-the-art paradigm of generative models, are introduced, and their
applications in wireless communication systems are discussed. Two case studies
are also presented to showcase how diffusion models can be exploited for the
development of resilient AI-native communication systems. Specifically, we
propose denoising diffusion probabilistic models (DDPM) for a wireless
communication scheme with non-ideal transceivers, where 30% improvement is
achieved in terms of bit error rate. As the second application, DDPMs are
employed at the transmitter to shape the constellation symbols, highlighting a
robust out-of-distribution performance. Finally, future directions and open
issues for the development of generative AI-based wireless systems are
discussed to promote future research endeavors towards wireless generative AI
(WiGenAI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letafati_M/0/1/0/all/0/1&quot;&gt;Mehdi Letafati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Samad Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latva_aho_M/0/1/0/all/0/1&quot;&gt;Matti Latva-aho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07365">
<title>GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07365</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-structured data is ubiquitous in the world which models complex
relationships between objects, enabling various Web applications. Daily
influxes of unlabeled graph data on the Web offer immense potential for these
applications. Graph self-supervised algorithms have achieved significant
success in acquiring generic knowledge from abundant unlabeled graph data.
These pre-trained models can be applied to various downstream Web applications,
saving training time and improving downstream (target) performance. However,
different graphs, even across seemingly similar domains, can differ
significantly in terms of attribute semantics, posing difficulties, if not
infeasibility, for transferring the pre-trained models to downstream tasks.
Concretely speaking, for example, the additional task-specific node information
in downstream tasks (specificity) is usually deliberately omitted so that the
pre-trained representation (transferability) can be leveraged. The trade-off as
such is termed as &quot;transferability-specificity dilemma&quot; in this work. To
address this challenge, we introduce an innovative deployment module coined as
GraphControl, motivated by ControlNet, to realize better graph domain transfer
learning. Specifically, by leveraging universal structural pre-trained models
and GraphControl, we align the input space across various graphs and
incorporate unique characteristics of target data as conditional inputs. These
conditions will be progressively integrated into the model during fine-tuning
or prompt tuning through ControlNet, facilitating personalized deployment.
Extensive experiments show that our method significantly enhances the
adaptability of pre-trained models on target attributed datasets, achieving
1.4-3x performance gain. Furthermore, it outperforms training-from-scratch
methods on target data with a comparable margin and exhibits faster
convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaoke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haizhou Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07402">
<title>NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07402</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research on time-series self-supervised models shows great promise in
learning semantic representations. However, it has been limited to small-scale
datasets, e.g., thousands of temporal sequences. In this work, we make key
technical contributions that are tailored to the numerical properties of
time-series data and allow the model to scale to large datasets, e.g., millions
of temporal sequences. We adopt the Transformer architecture by first
partitioning the input into non-overlapping windows. Each window is then
characterized by its normalized shape and two scalar values denoting the mean
and standard deviation within each window. To embed scalar values that may
possess arbitrary numerical scales to high-dimensional vectors, we propose a
numerically multi-scaled embedding module enumerating all possible scales for
the scalar values. The model undergoes pretraining using the proposed
numerically multi-scaled embedding with a simple contrastive objective on a
large-scale dataset containing over a million sequences. We study its transfer
performance on a number of univariate and multivariate classification
benchmarks. Our method exhibits remarkable improvement against previous
representation learning approaches and establishes the new state of the art,
even compared with domain-specific non-learning-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenguo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xumeng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Congrui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Stephen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhirong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07427">
<title>Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07427</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a time series forecasting method named Quantum Gramian Angular
Field (QGAF). This approach merges the advantages of quantum computing
technology with deep learning, aiming to enhance the precision of time series
classification and forecasting. We successfully transformed stock return time
series data into two-dimensional images suitable for Convolutional Neural
Network (CNN) training by designing specific quantum circuits. Distinct from
the classical Gramian Angular Field (GAF) approach, QGAF&apos;s uniqueness lies in
eliminating the need for data normalization and inverse cosine calculations,
simplifying the transformation process from time series data to two-dimensional
images. To validate the effectiveness of this method, we conducted experiments
on datasets from three major stock markets: the China A-share market, the Hong
Kong stock market, and the US stock market. Experimental results revealed that
compared to the classical GAF method, the QGAF approach significantly improved
time series prediction accuracy, reducing prediction errors by an average of
25% for Mean Absolute Error (MAE) and 48% for Mean Squared Error (MSE). This
research confirms the potential and promising prospects of integrating quantum
computing with deep learning techniques in financial time series forecasting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhengmeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hai Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07433">
<title>Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans often acquire new skills through observation and imitation. For
robotic agents, learning from the plethora of unlabeled video demonstration
data available on the Internet necessitates imitating the expert without access
to its action, presenting a challenge known as Imitation Learning from
Observations (ILfO). A common approach to tackle ILfO problems is to convert
them into inverse reinforcement learning problems, utilizing a proxy reward
computed from the agent&apos;s and the expert&apos;s observations. Nonetheless, we
identify that tasks characterized by a progress dependency property pose
significant challenges for such approaches; in these tasks, the agent needs to
initially learn the expert&apos;s preceding behaviors before mastering the
subsequent ones. Our investigation reveals that the main cause is that the
reward signals assigned to later steps hinder the learning of initial
behaviors. To address this challenge, we present a novel ILfO framework that
enables the agent to master earlier behaviors before advancing to later ones.
We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively
alters the discount factor in reinforcement learning during the training phase,
prioritizing earlier rewards initially and gradually engaging later rewards
only when the earlier behaviors have been mastered. Our experiments, conducted
on nine Meta-World tasks, demonstrate that our method significantly outperforms
state-of-the-art methods across all tasks, including those that are unsolvable
by them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Weijun Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Chuan Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhao-Heng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07579">
<title>In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07579</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning, the study of efficiently removing the impact of specific
training points on the trained model, has garnered increased attention of late,
driven by the need to comply with privacy regulations like the Right to be
Forgotten. Although unlearning is particularly relevant for LLMs in light of
the copyright issues they raise, achieving precise unlearning is
computationally infeasible for very large models. To this end, recent work has
proposed several algorithms which approximate the removal of training data
without retraining the model. These algorithms crucially rely on access to the
model parameters in order to update them, an assumption that may not hold in
practice due to computational constraints or when the LLM is accessed via API.
In this work, we propose a new class of unlearning methods for LLMs we call
&apos;&apos;In-Context Unlearning&apos;&apos;, providing inputs in context and without having to
update model parameters. To unlearn a particular training instance, we provide
the instance alongside a flipped label and additional correctly labelled
instances which are prepended as inputs to the LLM at inference time. Our
experimental results demonstrate that these contexts effectively remove
specific information from the training set while maintaining performance levels
that are competitive with (or in some cases exceed) state-of-the-art unlearning
methods that require access to the LLM parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1&quot;&gt;Martin Pawelczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1&quot;&gt;Himabindu Lakkaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07644">
<title>Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07644</link>
<description rdf:parseType="Literal">&lt;p&gt;With the success of large-scale pretraining in NLP, there is an increasing
trend of applying it to the domain of life sciences. In particular, pretraining
methods based on DNA sequences have garnered growing attention due to their
potential to capture generic information about genes. However, existing
pretraining methods for DNA sequences largely rely on direct adoptions of BERT
pretraining from NLP, lacking a comprehensive understanding and a specifically
tailored approach. To address this research gap, we first conducted a series of
exploratory experiments and gained several insightful observations: 1) In the
fine-tuning phase of downstream tasks, when using K-mer overlapping
tokenization instead of K-mer non-overlapping tokenization, both overlapping
and non-overlapping pretraining weights show consistent performance
improvement.2) During the pre-training process, using K-mer overlapping
tokenization quickly produces clear K-mer embeddings and reduces the loss to a
very low level, while using K-mer non-overlapping tokenization results in less
distinct embeddings and continuously decreases the loss. 3) Using overlapping
tokenization causes the self-attention in the intermediate layers of
pre-trained models to tend to overly focus on certain tokens, reflecting that
these layers are not adequately optimized. In summary, overlapping tokenization
can benefit the fine-tuning of downstream tasks but leads to inadequate
pretraining with fast convergence. To unleash the pretraining potential, we
introduce a novel approach called RandomMask, which gradually increases the
task difficulty of BERT-like pretraining by continuously expanding its mask
boundary, forcing the model to learn more knowledge. RandomMask is simple but
effective, achieving top-tier performance across 26 datasets of 28 datasets
spanning 7 downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chaoqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Weiqiang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Lifeng Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuchen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianle Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hongliang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13869">
<title>PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2309.13869</link>
<description rdf:parseType="Literal">&lt;p&gt;Document-level relation extraction (DocRE) aims to extract relations of all
entity pairs in a document. A key challenge in DocRE is the cost of annotating
such data which requires intensive human effort. Thus, we investigate the case
of DocRE in a low-resource setting, and we find that existing models trained on
low data overestimate the NA (&quot;no relation&quot;) label, causing limited
performance. In this work, we approach the problem from a calibration
perspective and propose PRiSM, which learns to adapt logits based on relation
semantic information. We evaluate our method on three DocRE datasets and
demonstrate that integrating existing models with PRiSM improves performance by
as much as 26.38 F1 score, while the calibration error drops as much as 36
times when trained with about 3% of data. The code is publicly available at
https://github.com/brightjade/PRiSM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyesu Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16375">
<title>A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles. (arXiv:2309.16375v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2309.16375</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with
highly cost-effective and flexible usage scenarios. Although many papers have
reviewed the application of UAVs in agriculture, the review of the application
for tree detection is still insufficient. This paper focuses on tree detection
methods applied to UAV data collected by UAVs. There are two kinds of data, the
point cloud and the images, which are acquired by the Light Detection and
Ranging (LiDAR) sensor and camera, respectively. Among the detection methods
using point-cloud data, this paper mainly classifies these methods according to
LiDAR and Digital Aerial Photography (DAP). For the detection methods using
images directly, this paper reviews these methods by whether or not to use the
Deep Learning (DL) method. Our review concludes and analyses the comparison and
combination between the application of LiDAR-based and DAP-based point cloud
data. The performance, relative merits, and application fields of the methods
are also introduced. Meanwhile, this review counts the number of tree detection
studies using different methods in recent years. From our statics, the
detection task using DL methods on the image has become a mainstream trend as
the number of DL-based detection researches increases to 45% of the total
number of tree detection studies up to 2022. As a result, this review could
help and guide researchers who want to carry out tree detection on specific
forests and for farmers to use UAVs in managing agriculture production.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_W/0/1/0/all/0/1&quot;&gt;Weijie Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1&quot;&gt;Hann Woei Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Ye Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suandi_S/0/1/0/all/0/1&quot;&gt;Shahrel Azmin Suandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ismail_F/0/1/0/all/0/1&quot;&gt;Farzad Ismail&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>