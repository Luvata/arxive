<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02329" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.02419">
<title>Moving Object Based Collision-Free Video Synopsis. (arXiv:2401.02419v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02419</link>
<description rdf:parseType="Literal">&lt;p&gt;Video synopsis, summarizing a video to generate a shorter video by exploiting
the spatial and temporal redundancies, is important for surveillance and
archiving. Existing trajectory-based video synopsis algorithms will not able to
work in real time, because of the complexity due to the number of object tubes
that need to be included in the complex energy minimization algorithm. We
propose a real-time algorithm by using a method that incrementally stitches
each frame of the synopsis by extracting object frames from the user specified
number of tubes in the buffer in contrast to global energy-minimization based
systems. This also gives flexibility to the user to set the threshold of
maximum number of objects in the synopsis video according his or her tracking
ability and creates collision-free summarized videos which are visually
pleasing. Experiments with six common test videos, indoors and outdoors with
many moving objects, show that the proposed video synopsis algorithm produces
better frame reduction rates than existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnarajah_A/0/1/0/all/0/1&quot;&gt;Anton Jeran Ratnarajah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goonetilleke_S/0/1/0/all/0/1&quot;&gt;Sahani Goonetilleke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tissera_D/0/1/0/all/0/1&quot;&gt;Dumindu Tissera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balagopalan_K/0/1/0/all/0/1&quot;&gt;Kapilan Balagopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1&quot;&gt;Ranga Rodrigo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02424">
<title>Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning. (arXiv:2401.02424v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02424</link>
<description rdf:parseType="Literal">&lt;p&gt;As the global population continues to expand, the demand for natural
resources increases. Unfortunately, human activities account for 23% of
greenhouse gas emissions. On a positive note, remote sensing technologies have
emerged as a valuable tool in managing our environment. These technologies
allow us to monitor land use, plan urban areas, and drive advancements in areas
such as agriculture, climate change mitigation, disaster recovery, and
environmental monitoring. Recent advances in AI, computer vision, and earth
observation data have enabled unprecedented accuracy in land use mapping. By
using transfer learning and fine-tuning with RGB bands, we achieved an
impressive 99.19% accuracy in land use analysis. Such findings can be used to
inform conservation and urban planning policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunwar_S/0/1/0/all/0/1&quot;&gt;Suman Kunwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdush_J/0/1/0/all/0/1&quot;&gt;Jannatul Ferdush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02430">
<title>Automated Classification of Model Errors on ImageNet. (arXiv:2401.02430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02430</link>
<description rdf:parseType="Literal">&lt;p&gt;While the ImageNet dataset has been driving computer vision research over the
past decade, significant label noise and ambiguity have made top-1 accuracy an
insufficient measure of further progress. To address this, new label-sets and
evaluation protocols have been proposed for ImageNet showing that
state-of-the-art models already achieve over 95% accuracy and shifting the
focus on investigating why the remaining errors persist.
&lt;/p&gt;
&lt;p&gt;Recent work in this direction employed a panel of experts to manually
categorize all remaining classification errors for two selected models.
However, this process is time-consuming, prone to inconsistencies, and requires
trained experts, making it unsuitable for regular model evaluation thus
limiting its utility. To overcome these limitations, we propose the first
automated error classification framework, a valuable tool to study how modeling
choices affect error distributions. We use our framework to comprehensively
evaluate the error distribution of over 900 models. Perhaps surprisingly, we
find that across model architectures, scales, and pre-training corpora, top-1
accuracy is a strong predictor for the portion of all error types. In
particular, we observe that the portion of severe errors drops significantly
with top-1 accuracy indicating that, while it underreports a model&apos;s true
performance, it remains a valuable performance metric.
&lt;/p&gt;
&lt;p&gt;We release all our code at
https://github.com/eth-sri/automated-error-analysis .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peychev_M/0/1/0/all/0/1&quot;&gt;Momchil Peychev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Mark Niklas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1&quot;&gt;Marc Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1&quot;&gt;Martin Vechev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02432">
<title>Partial Coherence for Object Recognition and Depth Sensing. (arXiv:2401.02432v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02432</link>
<description rdf:parseType="Literal">&lt;p&gt;We show a monotonic relationship between performances of various computer
vision tasks versus degrees of coherence of illumination. We simulate partially
coherent illumination using computational methods, propagate the lightwave to
form images, and subsequently employ a deep neural network to perform object
recognition and depth sensing tasks. In each controlled experiment, we discover
that, increased coherent length leads to improved image entropy, as well as
enhanced object recognition and depth sensing performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zichen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ken Xingze Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02433">
<title>FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02433</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of imaging sensor technology in the field of
remote sensing, multi-modal remote sensing data fusion has emerged as a crucial
research direction for land cover classification tasks. While diffusion models
have made great progress in generative models and image classification tasks,
existing models primarily focus on single-modality and single-client control,
that is, the diffusion process is driven by a single modal in a single
computing node. To facilitate the secure fusion of heterogeneous data from
clients, it is necessary to enable distributed multi-modal control, such as
merging the hyperspectral data of organization A and the LiDAR data of
organization B privately on each base station client. In this study, we propose
a multi-modal collaborative diffusion federated learning framework called
FedDiff. Our framework establishes a dual-branch diffusion model feature
extraction setup, where the two modal data are inputted into separate branches
of the encoder. Our key insight is that diffusion models driven by different
modalities are inherently complementary in terms of potential denoising steps
on which bilateral connections can be built. Considering the challenge of
private and efficient communication between multiple clients, we embed the
diffusion model into the federated learning communication structure, and
introduce a lightweight communication module. Qualitative and quantitative
experiments validate the superiority of our framework in terms of image quality
and conditional consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;DaiXun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;ZiXuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;YiBing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Leyuan Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02435">
<title>Image Collage on Arbitrary Shape via Shape-Aware Slicing and Optimization. (arXiv:2401.02435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02435</link>
<description rdf:parseType="Literal">&lt;p&gt;Image collage is a very useful tool for visualizing an image collection. Most
of the existing methods and commercial applications for generating image
collages are designed on simple shapes, such as rectangular and circular
layouts. This greatly limits the use of image collages in some artistic and
creative settings. Although there are some methods that can generate
irregularly-shaped image collages, they often suffer from severe image
overlapping and excessive blank space. This prevents such methods from being
effective information communication tools. In this paper, we present a shape
slicing algorithm and an optimization scheme that can create image collages of
arbitrary shapes in an informative and visually pleasing manner given an input
shape and an image collection. To overcome the challenge of irregular shapes,
we propose a novel algorithm, called Shape-Aware Slicing, which partitions the
input shape into cells based on medial axis and binary slicing tree.
Shape-Aware Slicing, which is designed specifically for irregular shapes, takes
human perception and shape structure into account to generate visually pleasing
partitions. Then, the layout is optimized by analyzing input images with the
goal of maximizing the total salient regions of the images. To evaluate our
method, we conduct extensive experiments and compare our results against
previous work. The evaluations show that our proposed algorithm can efficiently
arrange image collections on irregular shapes and create visually superior
results than prior work and existing commercial tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dong-Yi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thi-Ngoc-Hanh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Sheng-Yi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yun-Chen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tong-Yee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02436">
<title>Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis. (arXiv:2401.02436v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02436</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niedermayr_S/0/1/0/all/0/1&quot;&gt;Simon Niedermayr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stumpfegger_J/0/1/0/all/0/1&quot;&gt;Josef Stumpfegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westermann_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfc;diger Westermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02437">
<title>Randomly Weighted Neuromodulation in Neural Networks Facilitates Learning of Manifolds Common Across Tasks. (arXiv:2401.02437v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.02437</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric Sensitive Hashing functions, a family of Local Sensitive Hashing
functions, are neural network models that learn class-specific manifold
geometry in supervised learning. However, given a set of supervised learning
tasks, understanding the manifold geometries that can represent each task and
the kinds of relationships between the tasks based on them has received little
attention. We explore a formalization of this question by considering a
generative process where each task is associated with a high-dimensional
manifold, which can be done in brain-like models with neuromodulatory systems.
Following this formulation, we define \emph{Task-specific Geometric Sensitive
Hashing~(T-GSH)} and show that a randomly weighted neural network with a
neuromodulation system can realize this function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jinyung Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1&quot;&gt;Theodore P. Pavlic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02460">
<title>Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions. (arXiv:2401.02460v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02460</link>
<description rdf:parseType="Literal">&lt;p&gt;The zero-shot performance of existing vision-language models (VLMs) such as
CLIP is limited by the availability of large-scale, aligned image and text
datasets in specific domains. In this work, we leverage two complementary
sources of information -- descriptions of categories generated by large
language models (LLMs) and abundant, fine-grained image classification datasets
-- to improve the zero-shot classification performance of VLMs across
fine-grained domains. On the technical side, we develop methods to train VLMs
with this &quot;bag-level&quot; image-text supervision. We find that simply using these
attributes at test-time does not improve performance, but our training
strategy, for example, on the iNaturalist dataset, leads to an average
improvement of 4-5% in zero-shot classification accuracy for novel categories
of birds and flowers. Similar improvements are observed in domains where a
subset of the categories was used to fine-tune the model. By prompting LLMs in
various ways, we generate descriptions that capture visual appearance, habitat,
and geographic regions and pair them with existing attributes such as the
taxonomic structure of the categories. We systematically evaluate their ability
to improve zero-shot categorization in natural domains. Our findings suggest
that geographic priors can be just as effective and are complementary to visual
appearance. Our method also outperforms prior work on prompt-based tuning of
VLMs. We plan to release the benchmark, consisting of 7 datasets, which will
contribute to future research in zero-shot recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_O/0/1/0/all/0/1&quot;&gt;Oindrila Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1&quot;&gt;Grant Van Horn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1&quot;&gt;Subhransu Maji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02473">
<title>VASE: Object-Centric Appearance and Shape Manipulation of Real Videos. (arXiv:2401.02473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02473</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, several works tackled the video editing task fostered by the
success of large-scale text-to-image generative models. However, most of these
methods holistically edit the frame using the text, exploiting the prior given
by foundation diffusion models and focusing on improving the temporal
consistency across frames. In this work, we introduce a framework that is
object-centric and is designed to control both the object&apos;s appearance and,
notably, to execute precise and explicit structural modifications on the
object. We build our framework on a pre-trained image-conditioned diffusion
model, integrate layers to handle the temporal dimension, and propose training
strategies and architectural modifications to enable shape control. We evaluate
our method on the image-driven video editing task showing similar performance
to the state-of-the-art, and showcasing novel shape-editing capabilities.
Further details, code and examples are available on our project page:
https://helia95.github.io/vase-website/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peruzzo_E/0/1/0/all/0/1&quot;&gt;Elia Peruzzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1&quot;&gt;Vidit Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dejia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xingqian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02501">
<title>The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02501</link>
<description rdf:parseType="Literal">&lt;p&gt;Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no \emph{a priori} knowledge of expected
pattern dynamics, and no training data. The proposed cell signaling structure
function (SSF) is a Kolmogorov structure function that optimally measures cell
signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a
significant improvement compared to the current state-of-the-art cytonuclear
ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,
or a functional output such as velocity. Patterns of similarity are identified
via the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aho_L/0/1/0/all/0/1&quot;&gt;Layton Aho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winter_M/0/1/0/all/0/1&quot;&gt;Mark Winter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeCarlo_M/0/1/0/all/0/1&quot;&gt;Marc DeCarlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frismantiene_A/0/1/0/all/0/1&quot;&gt;Agne Frismantiene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blum_Y/0/1/0/all/0/1&quot;&gt;Yannick Blum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagliardi_P/0/1/0/all/0/1&quot;&gt;Paolo Armando Gagliardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pertz_O/0/1/0/all/0/1&quot;&gt;Olivier Pertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Andrew R. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02523">
<title>Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02523</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Ruman Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramaniam_M/0/1/0/all/0/1&quot;&gt;Mahadevan Subramaniam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pei-Chi Huang&lt;/a&gt; (Department of Computer Science, University of Nebraska at Omaha, Omaha, NE, USA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02524">
<title>Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.02524</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trapp_S/0/1/0/all/0/1&quot;&gt;Simon Trapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stenger_M/0/1/0/all/0/1&quot;&gt;Michael Stenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leppich_R/0/1/0/all/0/1&quot;&gt;Robert Leppich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kounev_S/0/1/0/all/0/1&quot;&gt;Samuel Kounev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leznik_M/0/1/0/all/0/1&quot;&gt;Mark Leznik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chard_K/0/1/0/all/0/1&quot;&gt;Kyle Chard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1&quot;&gt;Ian Foster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02526">
<title>Branched Variational Autoencoder Classifiers. (arXiv:2401.02526v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.02526</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a modified variational autoencoder (VAEs) that contains
an additional neural network branch. The resulting branched VAE (BVAE)
contributes a classification component based on the class labels to the total
loss and therefore imparts categorical information to the latent
representation. As a result, the latent space distributions of the input
classes are separated and ordered, thereby enhancing the classification
accuracy. The degree of improvement is quantified by numerical calculations
employing the benchmark MNIST dataset for both unrotated and rotated digits.
The proposed technique is then compared to and then incorporated into a VAE
with fixed output distributions. This procedure is found to yield improved
performance for a wide range of output distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salah_A/0/1/0/all/0/1&quot;&gt;Ahmed Salah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yevick_D/0/1/0/all/0/1&quot;&gt;David Yevick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02536">
<title>Novel End-to-End Production-Ready Machine Learning Flow for Nanolithography Modeling and Correction. (arXiv:2401.02536v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.02536</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical lithography is the main enabler to semiconductor manufacturing. It
requires extensive processing to perform the Resolution Enhancement Techniques
(RETs) required to transfer the design data to a working Integrated Circuits
(ICs). The processing power and computational runtime for RETs tasks is ever
increasing due to the continuous reduction of the feature size and the
expansion of the chip area. State-of-the-art research sought Machine Learning
(ML) technologies to reduce runtime and computational power, however they are
still not used in production yet. In this study, we analyze the reasons holding
back ML computational lithography from being production ready and present a
novel highly scalable end-to-end flow that enables production ready ML-RET
correction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1&quot;&gt;Mohamed S. E. Habib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_H/0/1/0/all/0/1&quot;&gt;Hossam A. H. Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_ElYazeed_M/0/1/0/all/0/1&quot;&gt;Mohamed F. Abu-ElYazeed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02537">
<title>Using Singular Value Decomposition in a Convolutional Neural Network to Improve Brain Tumor Segmentation Accuracy. (arXiv:2401.02537v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.02537</link>
<description rdf:parseType="Literal">&lt;p&gt;A brain tumor consists of cells showing abnormal brain growth. The area of
the brain tumor significantly affects choosing the type of treatment and
following the course of the disease during the treatment. At the same time,
pictures of Brain MRIs are accompanied by noise. Eliminating existing noises
can significantly impact the better segmentation and diagnosis of brain tumors.
In this work, we have tried using the analysis of eigenvalues. We have used the
MSVD algorithm, reducing the image noise and then using the deep neural network
to segment the tumor in the images. The proposed method&apos;s accuracy was
increased by 2.4% compared to using the original images. With Using the MSVD
method, convergence speed has also increased, showing the proposed method&apos;s
effectiveness
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahadian_P/0/1/0/all/0/1&quot;&gt;Pegah Ahadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Babaei_M/0/1/0/all/0/1&quot;&gt;Maryam Babaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parand_K/0/1/0/all/0/1&quot;&gt;Kourosh Parand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02539">
<title>Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using Virtual Fixture. (arXiv:2401.02539v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.02539</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots
inside deep veins, which may block blood flow or even cause a life-threatening
pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by
pressing the target vein until its lumen is fully compressed. However, the
compression exam is highly operator-dependent. To alleviate intra- and
inter-variations, we present a robotic US system with a novel hybrid force
motion control scheme ensuring position and force tracking accuracy, and soft
landing of the probe onto the target surface. In addition, a path-based virtual
fixture is proposed to realize easy human-robot interaction for repeat
compression operation at the lesion location. To ensure the biometric
measurements obtained in different examinations are comparable, the 6D scanning
path is determined in a coarse-to-fine manner using both an external RGBD
camera and US images. The RGBD camera is first used to extract a rough scanning
path on the object. Then, the segmented vascular lumen from US images are used
to optimize the scanning path to ensure the visibility of the target object. To
generate a continuous scan path for developing virtual fixtures, an arc-length
based path fitting model considering both position and orientation is proposed.
Finally, the whole system is evaluated on a human-like arm phantom with an
uneven surface.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dianye Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenguang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingchuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlas_A/0/1/0/all/0/1&quot;&gt;Angelos Karlas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02550">
<title>OptFlow: Fast Optimization-based Scene Flow Estimation without Supervision. (arXiv:2401.02550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02550</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene flow estimation is a crucial component in the development of autonomous
driving and 3D robotics, providing valuable information for environment
perception and navigation. Despite the advantages of learning-based scene flow
estimation techniques, their domain specificity and limited generalizability
across varied scenarios pose challenges. In contrast, non-learning
optimization-based methods, incorporating robust priors or regularization,
offer competitive scene flow estimation performance, require no training, and
show extensive applicability across datasets, but suffer from lengthy inference
times. In this paper, we present OptFlow, a fast optimization-based scene flow
estimation method. Without relying on learning or any labeled datasets, OptFlow
achieves state-of-the-art performance for scene flow estimation on popular
autonomous driving benchmarks. It integrates a local correlation weight matrix
for correspondence matching, an adaptive correspondence threshold limit for
nearest-neighbor search, and graph prior rigidity constraints, resulting in
expedited convergence and improved point correspondence identification.
Moreover, we demonstrate how integrating a point cloud registration function
within our objective function bolsters accuracy and differentiates between
static and dynamic points without relying on external odometry data.
Consequently, OptFlow outperforms the baseline graph-prior method by
approximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy,
all while offering the fastest inference time among all non-learning scene flow
estimation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_R/0/1/0/all/0/1&quot;&gt;Rahul Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_C/0/1/0/all/0/1&quot;&gt;Chris Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarting_W/0/1/0/all/0/1&quot;&gt;Wilko Schwarting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02564">
<title>Predicting Future States with Spatial Point Processes in Single Molecule Resolution Spatial Transcriptomics. (arXiv:2401.02564v1 [q-bio.TO])</title>
<link>http://arxiv.org/abs/2401.02564</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a pipeline based on Random Forest Regression to
predict the future distribution of cells that are expressed by the Sog-D gene
(active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral
(DV) axis of the Drosophila in embryogenesis process. This method provides
insights about how cells and living organisms control gene expression in super
resolution whole embryo spatial transcriptomics imaging at sub cellular, single
molecule resolution. A Random Forest Regression model was used to predict the
next stage active distribution based on the previous one. To achieve this goal,
we leveraged temporally resolved, spatial point processes by including Ripley&apos;s
K-function in conjunction with the cell&apos;s state in each stage of embryogenesis,
and found average predictive accuracy of active cell distribution. This tool is
analogous to RNA Velocity for spatially resolved developmental biology, from
one data point we can predict future spatially resolved gene expression using
features from the spatial point processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Malidarreh_P/0/1/0/all/0/1&quot;&gt;Parisa Boodaghi Malidarreh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rout_B/0/1/0/all/0/1&quot;&gt;Biraaj Rout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nasr_M/0/1/0/all/0/1&quot;&gt;Mohammad Sadegh Nasr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Borad_P/0/1/0/all/0/1&quot;&gt;Priyanshi Borad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Saurav_J/0/1/0/all/0/1&quot;&gt;Jillur Rahman Saurav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Veerla_J/0/1/0/all/0/1&quot;&gt;Jai Prakash Veerla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fenelon_K/0/1/0/all/0/1&quot;&gt;Kelli Fenelon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Koromila_T/0/1/0/all/0/1&quot;&gt;Theodora Koromila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luber_J/0/1/0/all/0/1&quot;&gt;Jacob M. Luber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02565">
<title>Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Langauge Model for Pathology Imaging. (arXiv:2401.02565v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.02565</link>
<description rdf:parseType="Literal">&lt;p&gt;In the dynamic landscape of medical artificial intelligence, this study
explores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)
model, a Vision Language Foundation model, under targeted adversarial
conditions. Leveraging the Kather Colon dataset with 7,180 H&amp;amp;E images across
nine tissue types, our investigation employs Projected Gradient Descent (PGD)
adversarial attacks to intentionally induce misclassifications. The outcomes
reveal a 100% success rate in manipulating PLIP&apos;s predictions, underscoring its
susceptibility to adversarial perturbations. The qualitative analysis of
adversarial examples delves into the interpretability challenges, shedding
light on nuanced changes in predictions induced by adversarial manipulations.
These findings contribute crucial insights into the interpretability, domain
adaptation, and trustworthiness of Vision Language Models in medical imaging.
The study emphasizes the pressing need for robust defenses to ensure the
reliability of AI models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Veerla_J/0/1/0/all/0/1&quot;&gt;Jai Prakash Veerla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thota_P/0/1/0/all/0/1&quot;&gt;Poojitha Thota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guttikonda_P/0/1/0/all/0/1&quot;&gt;Partha Sai Guttikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nilizadeh_S/0/1/0/all/0/1&quot;&gt;Shirin Nilizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luber_J/0/1/0/all/0/1&quot;&gt;Jacob M. Luber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02582">
<title>CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs. (arXiv:2401.02582v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02582</link>
<description rdf:parseType="Literal">&lt;p&gt;When exploring the development of Artificial General Intelligence (AGI), a
critical task for these models involves interpreting and processing information
from multiple image inputs. However, Large Multimodal Models (LMMs) encounter
two issues in such scenarios: (1) a lack of fine-grained perception, and (2) a
tendency to blend information across multiple images. We first extensively
investigate the capability of LMMs to perceive fine-grained visual details when
dealing with multiple input images. The research focuses on two aspects: first,
image-to-image matching (to evaluate whether LMMs can effectively reason and
pair relevant images), and second, multi-image-to-text matching (to assess
whether LMMs can accurately capture and summarize detailed image information).
We conduct evaluations on a range of both open-source and closed-source large
models, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model
performance, we further develop a Contrastive Chain-of-Thought (CoCoT)
prompting approach based on multi-input multimodal models. This method requires
LMMs to compare the similarities and differences among multiple image inputs,
and then guide the models to answer detailed questions about multi-image inputs
based on the identified similarities and differences. Our experimental results
showcase CoCoT&apos;s proficiency in enhancing the multi-image comprehension
capabilities of large multimodal models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daoan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Hanjia Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zijian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingkai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02588">
<title>Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting. (arXiv:2401.02588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02588</link>
<description rdf:parseType="Literal">&lt;p&gt;The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target&apos;s geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandidge_E/0/1/0/all/0/1&quot;&gt;Emma Sandidge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendrakar_T/0/1/0/all/0/1&quot;&gt;Trupti Mahendrakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1&quot;&gt;Ryan T. White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02600">
<title>Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02600</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attack against image classification task has been widely studied and
proven to be successful, while there exist little research on the backdoor
attack against vision-language models. In this paper, we explore backdoor
attack towards image captioning models by poisoning training data. Assuming the
attacker has total access to the training dataset, and cannot intervene in
model construction or training process. Specifically, a portion of benign
training samples is randomly selected to be poisoned. Afterwards, considering
that the captions are usually unfolded around objects in an image, we design an
object-oriented method to craft poisons, which aims to modify pixel values by a
slight range with the modification number proportional to the scale of the
current detected object region. After training with the poisoned data, the
attacked model behaves normally on benign images, but for poisoned images, the
model will generate some sentences irrelevant to the given image. The attack
controls the model behavior on specific test images without sacrificing the
generation performance on benign test images. Our method proves the weakness of
image captioning models to backdoor attack and we hope this work can raise the
awareness of defending against backdoor attack in the image captioning field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meiling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_N/0/1/0/all/0/1&quot;&gt;Nan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02606">
<title>Exploiting Polarized Material Cues for Robust Car Detection. (arXiv:2401.02606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02606</link>
<description rdf:parseType="Literal">&lt;p&gt;Car detection is an important task that serves as a crucial prerequisite for
many automated driving functions. The large variations in lighting/weather
conditions and vehicle densities of the scenes pose significant challenges to
existing car detection algorithms to meet the highly accurate perception demand
for safety, due to the unstable/limited color information, which impedes the
extraction of meaningful/discriminative features of cars. In this work, we
present a novel learning-based car detection method that leverages trichromatic
linear polarization as an additional cue to disambiguate such challenging
cases. A key observation is that polarization, characteristic of the light
wave, can robustly describe intrinsic physical properties of the scene objects
in various imaging conditions and is strongly linked to the nature of materials
for cars (e.g., metal and glass) and their surrounding environment (e.g., soil
and trees), thereby providing reliable and discriminative features for robust
car detection in challenging scenes. To exploit polarization cues, we first
construct a pixel-aligned RGB-Polarization car detection dataset, which we
subsequently employ to train a novel multimodal fusion network. Our car
detection network dynamically integrates RGB and polarization features in a
request-and-complement manner and can explore the intrinsic material properties
of cars across all learning samples. We extensively validate our method and
demonstrate that it outperforms state-of-the-art detection methods.
Experimental results show that polarization is a powerful cue for car
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Haiyang Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Ziqi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1&quot;&gt;Ao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Sen Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02607">
<title>Partition-based Nonrigid Registration for 3D Face Model. (arXiv:2401.02607v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02607</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a partition-based surface registration for 3D morphable
model(3DMM). In the 3DMM, it often requires to warp a handcrafted template
model into different captured models. The proposed method first utilizes the
landmarks to partition the template model then scale each part and finally
smooth the boundaries. This method is especially effective when the disparity
between the template model and the target model is huge. The experiment result
shows the method perform well than the traditional warp method and robust to
the local minima.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yuping Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Juan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02610">
<title>DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point Cloud Learning. (arXiv:2401.02610v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02610</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works attempt to extend Graph Convolution Networks (GCNs) to point
clouds for classification and segmentation tasks. These works tend to sample
and group points to create smaller point sets locally and mainly focus on
extracting local features through GCNs, while ignoring the relationship between
point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network
(DHGCN) for explicitly learning the contextual relationships between the
voxelized point parts, which are treated as graph nodes. Motivated by the
intuition that the contextual information between point parts lies in the
pairwise adjacent relationship, which can be depicted by the hop distance of
the graph quantitatively, we devise a novel self-supervised part-level hop
distance reconstruction task and design a novel loss function accordingly to
facilitate training. In addition, we propose the Hop Graph Attention (HGA),
which takes the learned hop distance as input for producing attention weights
to allow edge features to contribute distinctively in aggregation. Eventually,
the proposed DHGCN is a plug-and-play module that is compatible with
point-based backbone networks. Comprehensive experiments on different backbones
and tasks demonstrate that our self-supervised method achieves state-of-the-art
performance. Our source code is available at: https://github.com/Jinec98/DHGCN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jincen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lizhi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xuequan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razzak_I/0/1/0/all/0/1&quot;&gt;Imran Razzak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meili Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02611">
<title>MOODv2: Masked Image Modeling for Out-of-Distribution Detection. (arXiv:2401.02611v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02611</link>
<description rdf:parseType="Literal">&lt;p&gt;The crux of effective out-of-distribution (OOD) detection lies in acquiring a
robust in-distribution (ID) representation, distinct from OOD samples. While
previous methods predominantly leaned on recognition-based techniques for this
purpose, they often resulted in shortcut learning, lacking comprehensive
representations. In our study, we conducted a comprehensive analysis, exploring
distinct pretraining tasks and employing various OOD score functions. The
results highlight that the feature representations pre-trained through
reconstruction yield a notable enhancement and narrow the performance gap among
various score functions. This suggests that even simple score functions can
rival complex ones when leveraging reconstruction-based pretext tasks.
Reconstruction-based pretext tasks adapt well to various score functions. As
such, it holds promising potential for further expansion. Our OOD detection
framework, MOODv2, employs the masked image modeling pretext task. Without
bells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on
ImageNet and achieves 99.98% on CIFAR-10.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shaozuo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02614">
<title>Scaling and Masking: A New Paradigm of Data Sampling for Image and Video Quality Assessment. (arXiv:2401.02614v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02614</link>
<description rdf:parseType="Literal">&lt;p&gt;Quality assessment of images and videos emphasizes both local details and
global semantics, whereas general data sampling methods (e.g., resizing,
cropping or grid-based fragment) fail to catch them simultaneously. To address
the deficiency, current approaches have to adopt multi-branch models and take
as input the multi-resolution data, which burdens the model complexity. In this
work, instead of stacking up models, a more elegant data sampling method (named
as SAMA, scaling and masking) is explored, which compacts both the local and
global content in a regular input size. The basic idea is to scale the data
into a pyramid first, and reduce the pyramid into a regular data dimension with
a masking strategy. Benefiting from the spatial and temporal redundancy in
images and videos, the processed data maintains the multi-scale characteristics
with a regular input size, thus can be processed by a single-branch model. We
verify the sampling method in image and video quality assessment. Experiments
show that our sampling method can improve the performance of current
single-branch models significantly, and achieves competitive performance to the
multi-branch models without extra model complexity. The source code will be
available at https://github.com/Sissuire/SAMA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1&quot;&gt;Yinghui Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1&quot;&gt;Guoyao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Aobo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinjian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02616">
<title>FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face Video Editing on Dynamic NeRF. (arXiv:2401.02616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02616</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of the GAN-NeRF structure has enabled face editing on NeRF to
maintain 3D view consistency. However, achieving simultaneously multi-view
consistency and temporal coherence while editing video sequences remains a
formidable challenge. This paper proposes a novel face video editing
architecture built upon the dynamic face GAN-NeRF structure, which effectively
utilizes video sequences to restore the latent code and 3D face geometry. By
editing the latent code, multi-view consistent editing on the face can be
ensured, as validated by multiview stereo reconstruction on the resulting
edited images in our dynamic NeRF. As the estimation of face geometries occurs
on a frame-by-frame basis, this may introduce a jittering issue. We propose a
stabilizer that maintains temporal coherence by preserving smooth changes of
face expressions in consecutive frames. Quantitative and qualitative analyses
reveal that our method, as the pioneering 4D face video editor, achieves
state-of-the-art performance in comparison to existing 2D or 3D-based
approaches independently addressing identity and motion. Codes will be
released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02633">
<title>A Random Ensemble of Encrypted models for Enhancing Robustness against Adversarial Examples. (arXiv:2401.02633v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.02633</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, which means
AEs generated for a source model can fool another black-box model (target
model) with a non-trivial probability. In previous studies, it was confirmed
that the vision transformer (ViT) is more robust against the property of
adversarial transferability than convolutional neural network (CNN) models such
as ConvMixer, and moreover encrypted ViT is more robust than ViT without any
encryption. In this article, we propose a random ensemble of encrypted ViT
models to achieve much more robust models. In experiments, the proposed scheme
is verified to be more robust against not only black-box attacks but also
white-box ones than convention methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iijima_R/0/1/0/all/0/1&quot;&gt;Ryota Iijima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1&quot;&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1&quot;&gt;Hitoshi Kiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02634">
<title>AG-ReID.v2: Bridging Aerial and Ground Views for Person Re-identification. (arXiv:2401.02634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02634</link>
<description rdf:parseType="Literal">&lt;p&gt;Aerial-ground person re-identification (Re-ID) presents unique challenges in
computer vision, stemming from the distinct differences in viewpoints, poses,
and resolutions between high-altitude aerial and ground-based cameras. Existing
research predominantly focuses on ground-to-ground matching, with aerial
matching less explored due to a dearth of comprehensive datasets. To address
this, we introduce AG-ReID.v2, a dataset specifically designed for person Re-ID
in mixed aerial and ground scenarios. This dataset comprises 100,502 images of
1,615 unique individuals, each annotated with matching IDs and 15 soft
attribute labels. Data were collected from diverse perspectives using a UAV,
stationary CCTV, and smart glasses-integrated camera, providing a rich variety
of intra-identity variations. Additionally, we have developed an explainable
attention network tailored for this dataset. This network features a
three-stream architecture that efficiently processes pairwise image distances,
emphasizes key top-down features, and adapts to variations in appearance due to
altitude differences. Comparative evaluations demonstrate the superiority of
our approach over existing baselines. We plan to release the dataset and
algorithm source code publicly, aiming to advance research in this specialized
field of computer vision. For access, please visit
https://github.com/huynguyen792/AG-ReID.v2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Huy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Kien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02646">
<title>Recent Advancement in 3D Biometrics using Monocular Camera. (arXiv:2401.02646v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02646</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent literature has witnessed significant interest towards 3D biometrics
employing monocular vision for robust authentication methods. Motivated by
this, in this work we seek to provide insight on recent development in the area
of 3D biometrics employing monocular vision. We present the similarity and
dissimilarity of 3D monocular biometrics and classical biometrics, listing the
strengths and challenges. Further, we provide an overview of recent techniques
in 3D biometrics with monocular vision, as well as application systems adopted
by the industry. Finally, we discuss open research problems in this area of
research
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Aritra Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Abhijit Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02649">
<title>Enhancing 3D-Air Signature by Pen Tip Tail Trajectory Awareness: Dataset and Featuring by Novel Spatio-temporal CNN. (arXiv:2401.02649v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02649</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a novel process of using pen tip and tail 3D trajectory
for air signature. To acquire the trajectories we developed a new pen tool and
a stereo camera was used. We proposed SliT-CNN, a novel 2D spatial-temporal
convolutional neural network (CNN) for better featuring of the air signature.
In addition, we also collected an air signature dataset from $45$ signers.
Skilled forgery signatures per user are also collected. A detailed benchmarking
of the proposed dataset using existing techniques and proposed CNN on existing
and proposed dataset exhibit the effectiveness of our methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atreya_S/0/1/0/all/0/1&quot;&gt;Saurabh Atreya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bora_M/0/1/0/all/0/1&quot;&gt;Maheswar Bora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1&quot;&gt;Aritra Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Abhijit Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02651">
<title>Benchmarking PathCLIP for Pathology Image Analysis. (arXiv:2401.02651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02651</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate image classification and retrieval are of importance for clinical
diagnosis and treatment decision-making. The recent contrastive language-image
pretraining (CLIP) model has shown remarkable proficiency in understanding
natural images. Drawing inspiration from CLIP, PathCLIP is specifically
designed for pathology image analysis, utilizing over 200,000 image and text
pairs in training. While the performance the PathCLIP is impressive, its
robustness under a wide range of image corruptions remains unknown. Therefore,
we conduct an extensive evaluation to analyze the performance of PathCLIP on
various corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In
our experiments, we introduce seven corruption types including brightness,
contrast, Gaussian blur, resolution, saturation, hue, and markup at four
severity levels. Through experiments, we find that PathCLIP is relatively
robustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot
classification. Among the seven corruptions, blur and resolution can cause
server performance degradation of the PathCLIP. This indicates that ensuring
the quality of images is crucial before conducting a clinical test.
Additionally, we assess the robustness of PathCLIP in the task of image-image
retrieval, revealing that PathCLIP performs less effectively than PLIP on
Osteosarcoma but performs better on WSSS4LUAD under diverse corruptions.
Overall, PathCLIP presents impressive zero-shot classification and retrieval
performance for pathology images, but appropriate care needs to be taken when
using it. We hope this study provides a qualitative impression of PathCLIP and
helps understand its differences from other CLIP models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sunyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiaonan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingxiong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Honglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pingyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1&quot;&gt;Xueping Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02656">
<title>GTA: Guided Transfer of Spatial Attention from Object-Centric Representations. (arXiv:2401.02656v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02656</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing well-trained representations in transfer learning often results in
superior performance and faster convergence compared to training from scratch.
However, even if such good representations are transferred, a model can easily
overfit the limited training dataset and lose the valuable properties of the
transferred representations. This phenomenon is more severe in ViT due to its
low inductive bias. Through experimental analysis using attention maps in ViT,
we observe that the rich representations deteriorate when trained on a small
dataset. Motivated by this finding, we propose a novel and simple
regularization method for ViT called Guided Transfer of spatial Attention
(GTA). Our proposed method regularizes the self-attention maps between the
source and target models. A target model can fully exploit the knowledge
related to object localization properties through this explicit regularization.
Our experimental results show that the proposed GTA consistently improves the
accuracy across five benchmark datasets especially when the number of training
data is small.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;SeokHyun Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jinwoo Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_J/0/1/0/all/0/1&quot;&gt;JungWoo Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyungyul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sangheum Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02677">
<title>Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02677</link>
<description rdf:parseType="Literal">&lt;p&gt;Stable Diffusion XL (SDXL) has become the best open source text-to-image
model (T2I) for its versatility and top-notch image quality. Efficiently
addressing the computational demands of SDXL models is crucial for wider reach
and applicability. In this work, we introduce two scaled-down variants, Segmind
Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter
UNets, respectively, achieved through progressive removal using layer-level
losses focusing on reducing the model size while preserving generative quality.
We release these models weights at https://hf.co/Segmind. Our methodology
involves the elimination of residual networks and transformer blocks from the
U-Net structure of SDXL, resulting in significant reductions in parameters, and
latency. Our compact models effectively emulate the original SDXL by
capitalizing on transferred knowledge, achieving competitive results against
larger multi-billion parameter SDXL. Our work underscores the efficacy of
knowledge distillation coupled with layer-level losses in reducing model size
while preserving the high-quality generative capabilities of SDXL, thus
facilitating more accessible deployment in resource-constrained environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1&quot;&gt;Yatharth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaddipal_V/0/1/0/all/0/1&quot;&gt;Vishnu V. Jaddipal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhala_H/0/1/0/all/0/1&quot;&gt;Harish Prabhala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Sayak Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1&quot;&gt;Patrick Von Platen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02687">
<title>PAHD: Perception-Action based Human Decision Making using Explainable Graph Neural Networks on SAR Images. (arXiv:2401.02687v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02687</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic Aperture Radar (SAR) images are commonly utilized in military
applications for automatic target recognition (ATR). Machine learning (ML)
methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks
(GNN), are frequently used to identify ground-based objects, including battle
tanks, personnel carriers, and missile launchers. Determining the vehicle
class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is
crucial, as it can help determine whether the target object is an ally or an
enemy. While the ML algorithm provides feedback on the recognized target, the
final decision is left to the commanding officers. Therefore, providing
detailed information alongside the identified target can significantly impact
their actions. This detailed information includes the SAR image features that
contributed to the classification, the classification confidence, and the
probability of the identified object being classified as a different object
type or class. We propose a GNN-based ATR framework that provides the final
classified class and outputs the detailed information mentioned above. This is
the first study to provide a detailed analysis of the classification class,
making final decisions more straightforward. Moreover, our GNN framework
achieves an overall accuracy of 99.2\% when evaluated on the MSTAR dataset,
improving over previous state-of-the-art GNN methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijeratne_S/0/1/0/all/0/1&quot;&gt;Sasindu Wijeratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1&quot;&gt;Rajgopal Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1&quot;&gt;Viktor Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busart_C/0/1/0/all/0/1&quot;&gt;Carl Busart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02695">
<title>VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model. (arXiv:2401.02695v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.02695</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)
task empowers agents to adeptly traverse unfamiliar environments and locate
objects from novel categories without prior explicit training. This paper
introduces VoroNav, a novel semantic exploration framework that proposes the
Reduced Voronoi Graph to extract exploratory paths and planning nodes from a
semantic map constructed in real time. By harnessing topological and semantic
information, VoroNav designs text-based descriptions of paths and images that
are readily interpretable by a large language model (LLM). Our approach
presents a synergy of path and farsight descriptions to represent the
environmental context, enabling the LLM to apply commonsense reasoning to
ascertain the optimal waypoints for navigation. Extensive evaluation on the
HM3D and HSSD datasets validates that VoroNav surpasses existing ZSON
benchmarks in both success rates and exploration efficiency (+2.8% Success and
+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally
introduced metrics that evaluate obstacle avoidance proficiency and perceptual
efficiency further corroborate the enhancements achieved by our method in ZSON
planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Pengying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yao Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bingxian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yi Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Ji Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02702">
<title>VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework for Multi-Modal 3D Object Detection. (arXiv:2401.02702v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02702</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR-camera fusion can enhance the performance of 3D object detection by
utilizing complementary information between depth-aware LiDAR points and
semantically rich images. Existing voxel-based methods face significant
challenges when fusing sparse voxel features with dense image features in a
one-to-one manner, resulting in the loss of the advantages of images, including
semantic and continuity information, leading to sub-optimal detection
performance, especially at long distances. In this paper, we present
VoxelNextFusion, a multi-modal 3D object detection framework specifically
designed for voxel-based methods, which effectively bridges the gap between
sparse point clouds and dense images. In particular, we propose a voxel-based
image pipeline that involves projecting point clouds onto images to obtain both
pixel- and patch-level features. These features are then fused using a
self-attention to obtain a combined representation. Moreover, to address the
issue of background features present in patches, we propose a feature
importance module that effectively distinguishes between foreground and
background features, thus minimizing the impact of the background features.
Extensive experiments were conducted on the widely used KITTI and nuScenes 3D
object detection benchmarks. Notably, our VoxelNextFusion achieved around
+3.20% in AP@0.7 improvement for car detection in hard level compared to the
Voxel R-CNN baseline on the KITTI test dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Ziying Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Caiyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaoqing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhepeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02717">
<title>Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02717</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiologists must utilize multiple modal images for tumor segmentation and
diagnosis due to the limitations of medical imaging and the diversity of tumor
signals. This leads to the development of multimodal learning in segmentation.
However, the redundancy among modalities creates challenges for existing
subtraction-based joint learning methods, such as misjudging the importance of
modalities, ignoring specific modal information, and increasing cognitive load.
These thorny issues ultimately decrease segmentation accuracy and increase the
risk of overfitting. This paper presents the complementary information mutual
learning (CIML) framework, which can mathematically model and address the
negative impact of inter-modal redundant information. CIML adopts the idea of
addition and removes inter-modal redundant information through inductive
bias-driven task decomposition and message passing-based redundancy filtering.
CIML first decomposes the multimodal segmentation task into multiple subtasks
based on expert prior knowledge, minimizing the information dependence between
modalities. Furthermore, CIML introduces a scheme in which each modality can
extract information from other modalities additively through message passing.
To achieve non-redundancy of extracted information, the redundant filtering is
transformed into complementary information learning inspired by the variational
information bottleneck. The complementary information learning procedure can be
efficiently solved by variational inference and cross-modal spatial attention.
Numerical results from the verification task and standard benchmarks indicate
that CIML efficiently removes redundant information between modalities,
outperforming SOTA methods regarding validation accuracy and segmentation
effect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuyun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoqing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengping Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02719">
<title>Learning Image Demoireing from Unpaired Real Data. (arXiv:2401.02719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02719</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on addressing the issue of image demoireing. Unlike the
large volume of existing studies that rely on learning from paired real data,
we attempt to learn a demoireing model from unpaired real data, i.e., moire
images associated with irrelevant clean images. The proposed method, referred
to as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from
unpaired datasets, generating pairs with clean images for training demoireing
models. To achieve this, we divide real moire images into patches and group
them in compliance with their moire complexity. We introduce a novel moire
generation framework to synthesize moire images with diverse moire features,
resembling real moire patches, and details akin to real moire-free images.
Additionally, we introduce an adaptive denoise method to eliminate the
low-quality pseudo moire images that adversely impact the learning of
demoireing models. We conduct extensive experiments on the commonly-used FHDMi
and UHDM datasets. Results manifest that our UnDeM performs better than
existing methods when using existing demoireing models such as MBCNN and
ESDNet-L. Code: https://github.com/zysxmu/UnDeM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yunshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1&quot;&gt;Fei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02723">
<title>Predicting Traffic Flow with Federated Learning and Graph Neural with Asynchronous Computations Network. (arXiv:2401.02723v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.02723</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time traffic flow prediction holds significant importance within the
domain of Intelligent Transportation Systems (ITS). The task of achieving a
balance between prediction precision and computational efficiency presents a
significant challenge. In this article, we present a novel deep-learning method
called Federated Learning and Asynchronous Graph Convolutional Network
(FLAGCN). Our framework incorporates the principles of asynchronous graph
convolutional networks with federated learning to enhance the accuracy and
efficiency of real-time traffic flow prediction. The FLAGCN model employs a
spatial-temporal graph convolution technique to asynchronously address
spatio-temporal dependencies within traffic data effectively. To efficiently
handle the computational requirements associated with this deep learning model,
this study used a graph federated learning technique known as GraphFL. This
approach is designed to facilitate the training process. The experimental
results obtained from conducting tests on two distinct traffic datasets
demonstrate that the utilization of FLAGCN leads to the optimization of both
training and inference durations while maintaining a high level of prediction
accuracy. FLAGCN outperforms existing models with significant improvements by
achieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in
MAPE, compared to the best-performing existing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1&quot;&gt;Muhammad Yaqub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1&quot;&gt;Shahzad Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manan_M/0/1/0/all/0/1&quot;&gt;Malik Abdul Manan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuhan_I/0/1/0/all/0/1&quot;&gt;Imran Shabir Chuhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02727">
<title>Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02727</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. However,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features that contribute to the target class and discourage the
features that contribute to the original class in a middle layer of the source
model. Extensive experiments demonstrate that only a few iterations of
fine-tuning can boost existing attacks in terms of targeted transferability
nontrivially and universally. Our results also verify that the simple iterative
attacks can yield comparable or even better transferability than the
resource-intensive methods, which rely on training target-specific classifiers
or generators with additional data. The code is available at:
github.com/zengh5/TA_feature_FT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Biwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Anjie Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02744">
<title>MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.02744</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fauzulhaq_A/0/1/0/all/0/1&quot;&gt;Alfirsa Damasyifa Fauzulhaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parwitayasa_W/0/1/0/all/0/1&quot;&gt;Wahyu Parwitayasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugihdharma_J/0/1/0/all/0/1&quot;&gt;Joseph Ananda Sugihdharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ridhani_M/0/1/0/all/0/1&quot;&gt;M. Fadli Ridhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1&quot;&gt;Novanto Yudistira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02746">
<title>Reading Between the Frames: Multi-Modal Depression Detection in Videos from Non-Verbal Cues. (arXiv:2401.02746v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02746</link>
<description rdf:parseType="Literal">&lt;p&gt;Depression, a prominent contributor to global disability, affects a
substantial portion of the population. Efforts to detect depression from social
media texts have been prevalent, yet only a few works explored depression
detection from user-generated video content. In this work, we address this
research gap by proposing a simple and flexible multi-modal temporal model
capable of discerning non-verbal depression cues from diverse modalities in
noisy, real-world videos. We show that, for in-the-wild videos, using
additional high-level non-verbal cues is crucial to achieving good performance,
and we extracted and processed audio speech embeddings, face emotion
embeddings, face, body and hand landmarks, and gaze and blinking information.
Through extensive experiments, we show that our model achieves state-of-the-art
results on three key benchmark datasets for depression detection from video by
a substantial margin. Our code is publicly available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_Gomez_D/0/1/0/all/0/1&quot;&gt;David Gimeno-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1&quot;&gt;Ana-Maria Bucur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1&quot;&gt;Adrian Cosma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Hinarejos_C/0/1/0/all/0/1&quot;&gt;Carlos-David Mart&amp;#xed;nez-Hinarejos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1&quot;&gt;Paolo Rosso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02758">
<title>Systematic review of image segmentation using complex networks. (arXiv:2401.02758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02758</link>
<description rdf:parseType="Literal">&lt;p&gt;This review presents various image segmentation methods using complex
networks.
&lt;/p&gt;
&lt;p&gt;Image segmentation is one of the important steps in image analysis as it
helps analyze and understand complex images. At first, it has been tried to
classify complex networks based on how it being used in image segmentation.
&lt;/p&gt;
&lt;p&gt;In computer vision and image processing applications, image segmentation is
essential for analyzing complex images with irregular shapes, textures, or
overlapping boundaries. Advanced algorithms make use of machine learning,
clustering, edge detection, and region-growing techniques. Graph theory
principles combined with community detection-based methods allow for more
precise analysis and interpretation of complex images. Hybrid approaches
combine multiple techniques for comprehensive, robust segmentation, improving
results in computer vision and image processing tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Amin Rezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Asadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02759">
<title>Detection and Classification of Diabetic Retinopathy using Deep Learning Algorithms for Segmentation to Facilitate Referral Recommendation for Test and Treatment Prediction. (arXiv:2401.02759v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.02759</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper addresses the critical challenge of diabetic retinopathy
(DR), a severe complication of diabetes leading to potential blindness. The
proposed methodology leverages transfer learning with convolutional neural
networks (CNNs) for automatic DR detection using a single fundus photograph,
demonstrating high effectiveness with a quadratic weighted kappa score of
0.92546 in the APTOS 2019 Blindness Detection Competition. The paper reviews
existing literature on DR detection, spanning classical computer vision methods
to deep learning approaches, particularly focusing on CNNs. It identifies gaps
in the research, emphasizing the lack of exploration in integrating pretrained
large language models with segmented image inputs for generating
recommendations and understanding dynamic interactions within a web application
context.Objectives include developing a comprehensive DR detection methodology,
exploring model integration, evaluating performance through competition
ranking, contributing significantly to DR detection methodologies, and
identifying research gaps.The methodology involves data preprocessing, data
augmentation, and the use of a U-Net neural network architecture for
segmentation. The U-Net model efficiently segments retinal structures,
including blood vessels, hard and soft exudates, haemorrhages, microaneurysms,
and the optical disc. High evaluation scores in Jaccard, F1, recall, precision,
and accuracy underscore the model&apos;s potential for enhancing diagnostic
capabilities in retinal pathology assessment.The outcomes of this research hold
promise for improving patient outcomes through timely diagnosis and
intervention in the fight against diabetic retinopathy, marking a significant
contribution to the field of medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+H_M/0/1/0/all/0/1&quot;&gt;Manoj S H&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bosale_A/0/1/0/all/0/1&quot;&gt;Arya A Bosale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02764">
<title>Fus-MAE: A cross-attention-based data fusion approach for Masked Autoencoders in remote sensing. (arXiv:2401.02764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02764</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised frameworks for representation learning have recently stirred
up interest among the remote sensing community, given their potential to
mitigate the high labeling costs associated with curating large satellite image
datasets. In the realm of multimodal data fusion, while the often used
contrastive learning methods can help bridging the domain gap between different
sensor types, they rely on data augmentations techniques that require expertise
and careful design, especially for multispectral remote sensing data. A
possible but rather scarcely studied way to circumvent these limitations is to
use a masked image modelling based pretraining strategy. In this paper, we
introduce Fus-MAE, a self-supervised learning framework based on masked
autoencoders that uses cross-attention to perform early and feature-level data
fusion between synthetic aperture radar and multispectral optical data - two
modalities with a significant domain gap. Our empirical findings demonstrate
that Fus-MAE can effectively compete with contrastive learning strategies
tailored for SAR-optical data fusion and outperforms other masked-autoencoders
frameworks trained on a larger corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_To_Hing_H/0/1/0/all/0/1&quot;&gt;Hugo Chan-To-Hing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeravalli_B/0/1/0/all/0/1&quot;&gt;Bharadwaj Veeravalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02791">
<title>Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos. (arXiv:2401.02791v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02791</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical tool detection is essential for analyzing and evaluating minimally
invasive surgery videos. Current approaches are mostly based on supervised
methods that require large, fully instance-level labels (i.e., bounding boxes).
However, large image datasets with instance-level labels are often limited
because of the burden of annotation. Thus, surgical tool detection is important
when providing image-level labels instead of instance-level labels since
image-level annotations are considerably more time-efficient than
instance-level annotations. In this work, we propose to strike a balance
between the extremely costly annotation burden and detection performance. We
further propose a co-occurrence loss, which considers a characteristic that
some tool pairs often co-occur together in an image to leverage image-level
labels. Encapsulating the knowledge of co-occurrence using the co-occurrence
loss helps to overcome the difficulty in classification that originates from
the fact that some tools have similar shapes and textures. Extensive
experiments conducted on the Endovis2018 dataset in various data settings show
the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_R/0/1/0/all/0/1&quot;&gt;Ryo Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1&quot;&gt;Ryo Hachiuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1&quot;&gt;Hideo Saito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02794">
<title>Subjective and Objective Analysis of Indian Social Media Video Quality. (arXiv:2401.02794v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.02794</link>
<description rdf:parseType="Literal">&lt;p&gt;We conducted a large-scale subjective study of the perceptual quality of
User-Generated Mobile Video Content on a set of mobile-originated videos
obtained from the Indian social media platform ShareChat. The content viewed by
volunteer human subjects under controlled laboratory conditions has the benefit
of culturally diversifying the existing corpus of User-Generated Content (UGC)
video quality datasets. There is a great need for large and diverse UGC-VQA
datasets, given the explosive global growth of the visual internet and social
media platforms. This is particularly true in regard to videos obtained by
smartphones, especially in rapidly emerging economies like India. ShareChat
provides a safe and cultural community oriented space for users to generate and
share content in their preferred Indian languages and dialects. Our subjective
quality study, which is based on this data, offers a boost of cultural, visual,
and language diversification to the video quality research community. We expect
that this new data resource will also allow for the development of systems that
can predict the perceived visual quality of Indian social media videos, to
control scaling and compression protocols for streaming, provide better user
recommendations, and guide content analysis and processing. We demonstrate the
value of the new data resource by conducting a study of leading blind video
quality models on it, including a new model, called MoEVA, which deploys a
mixture of experts to predict video quality. Both the new LIVE-ShareChat
dataset and sample source code for MoEVA are being made freely available to the
research community at https://github.com/sandeep-sm/LIVE-SC
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Sandeep Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_M/0/1/0/all/0/1&quot;&gt;Mukul Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1&quot;&gt;Alan C. Bovik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02804">
<title>Diffbody: Diffusion-based Pose and Shape Editing of Human Images. (arXiv:2401.02804v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02804</link>
<description rdf:parseType="Literal">&lt;p&gt;Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person&apos;s identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body&apos;s pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okuyama_Y/0/1/0/all/0/1&quot;&gt;Yuta Okuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Endo_Y/0/1/0/all/0/1&quot;&gt;Yuki Endo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanamori_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Kanamori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02814">
<title>Object-Centric Instruction Augmentation for Robotic Manipulation. (arXiv:2401.02814v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.02814</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans interpret scenes by recognizing both the identities and positions of
objects in their observations. For a robot to perform tasks such as
\enquote{pick and place}, understanding both what the objects are and where
they are located is crucial. While the former has been extensively discussed in
the literature that uses the large language model to enrich the text
descriptions, the latter remains underexplored. In this work, we introduce the
\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment
highly semantic and information-dense language instruction with position cues.
We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of
object locations into natural language instruction, thus aiding the policy
network in mastering actions for versatile manipulation. Additionally, we
present a feature reuse mechanism to integrate the vision-language features
from off-the-shelf pre-trained MLLM into policy networks. Through a series of
simulated and real-world robotic tasks, we demonstrate that robotic manipulator
imitation policies trained with our enhanced instructions outperform those
relying solely on traditional language instructions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Junjie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minjie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chaomin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yaxin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Feifei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02826">
<title>CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras. (arXiv:2401.02826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02826</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing datasets for RGB-DVS tracking are collected with DVS346 camera and
their resolution ($346 \times 260$) is low for practical applications.
Actually, only visible cameras are deployed in many practical systems, and the
newly designed neuromorphic cameras may have different resolutions. The latest
neuromorphic sensors can output high-definition event streams, but it is very
difficult to achieve strict alignment between events and frames on both spatial
and temporal views. Therefore, how to achieve accurate tracking with unaligned
neuromorphic and visible sensors is a valuable but unresearched problem. In
this work, we formally propose the task of object tracking using unaligned
neuromorphic and visible cameras. We build the first unaligned frame-event
dataset CRSOT collected with a specially built data acquisition system, which
contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In
addition, we propose a novel unaligned object tracking framework that can
realize robust tracking even using the loosely aligned RGB-Event data.
Specifically, we extract the template and search regions of RGB and Event data
and feed them into a unified ViT backbone for feature embedding. Then, we
propose uncertainty perception modules to encode the RGB and Event features,
respectively, then, we propose a modality uncertainty fusion module to
aggregate the two modalities. These three branches are jointly optimized in the
training phase. Extensive experiments demonstrate that our tracker can
collaborate the dual modalities for high-performance tracking even without
strictly temporal and spatial alignment. The source code, dataset, and
pre-trained models will be released at
https://github.com/Event-AHU/Cross_Resolution_SOT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yabin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02831">
<title>Two-stage Progressive Residual Dense Attention Network for Image Denoising. (arXiv:2401.02831v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02831</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNNs) for image denoising can effectively
exploit rich hierarchical features and have achieved great success. However,
many deep CNN-based denoising models equally utilize the hierarchical features
of noisy images without paying attention to the more important and useful
features, leading to relatively low performance. To address the issue, we
design a new Two-stage Progressive Residual Dense Attention Network
(TSP-RDANet) for image denoising, which divides the whole process of denoising
into two sub-tasks to remove noise progressively. Two different attention
mechanism-based denoising networks are designed for the two sequential
sub-tasks: the residual dense attention module (RDAM) is designed for the first
stage, and the hybrid dilated residual dense attention module (HDRDAM) is
proposed for the second stage. The proposed attention modules are able to learn
appropriate local features through dense connection between different
convolutional layers, and the irrelevant features can also be suppressed. The
two sub-networks are then connected by a long skip connection to retain the
shallow feature to enhance the denoising performance. The experiments on seven
benchmark datasets have verified that compared with many state-of-the-art
methods, the proposed TSP-RDANet can obtain favorable results both on synthetic
and real noisy image denoising. The code of our TSP-RDANet is available at
https://github.com/WenCongWu/TSP-RDANet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wencong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_A/0/1/0/all/0/1&quot;&gt;An Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1&quot;&gt;Guannan Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yuelong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yungang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wen Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02838">
<title>CrisisViT: A Robust Vision Transformer for Crisis Image Classification. (arXiv:2401.02838v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02838</link>
<description rdf:parseType="Literal">&lt;p&gt;In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1&quot;&gt;Zijun Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCreadie_R/0/1/0/all/0/1&quot;&gt;Richard McCreadie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1&quot;&gt;Muhammad Imran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02841">
<title>Multi-Stage Contrastive Regression for Action Quality Assessment. (arXiv:2401.02841v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02841</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been growing interest in the video-based action
quality assessment (AQA). Most existing methods typically solve AQA problem by
considering the entire video yet overlooking the inherent stage-level
characteristics of actions. To address this issue, we design a novel
Multi-stage Contrastive Regression (MCoRe) framework for the AQA task. This
approach allows us to efficiently extract spatial-temporal information, while
simultaneously reducing computational costs by segmenting the input video into
multiple stages or procedures. Inspired by the graph contrastive learning, we
propose a new stage-wise contrastive learning loss function to enhance
performance. As a result, MCoRe demonstrates the state-of-the-art result so far
on the widely-adopted fine-grained AQA dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Q/0/1/0/all/0/1&quot;&gt;Qi An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02847">
<title>Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02847</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
&quot;self-rectification&quot;, automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1&quot;&gt;Rongjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1&quot;&gt;Dani Lischinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hui Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02861">
<title>Reversing the Irreversible: A Survey on Inverse Biometrics. (arXiv:2401.02861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02861</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread use of biometric recognition, several issues related to
the privacy and security provided by this technology have been recently raised
and analysed. As a result, the early common belief among the biometrics
community of templates irreversibility has been proven wrong. It is now an
accepted fact that it is possible to reconstruct from an unprotected template a
synthetic sample that matches the bona fide one. This reverse engineering
process, commonly referred to as \textit{inverse biometrics}, constitutes a
severe threat for biometric systems from two different angles: on the one hand,
sensitive personal data (i.e., biometric data) can be derived from compromised
unprotected templates; on the other hand, other powerful attacks can be
launched building upon these reconstructed samples. Given its important
implications, biometric stakeholders have produced over the last fifteen years
numerous works analysing the different aspects related to inverse biometrics:
development of reconstruction algorithms for different characteristics;
proposal of methodologies to assess the vulnerabilities of biometric systems to
the aforementioned algorithms; development of countermeasures to reduce the
possible effects of attacks. The present article is an effort to condense all
this information in one comprehensive review of: the problem itself, the
evaluation of the problem, and the mitigation of the problem. The present
article is an effort to condense all this information in one comprehensive
review of: the problem itself, the evaluation of the problem, and the
mitigation of the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1&quot;&gt;Marta Gomez-Barrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1&quot;&gt;Javier Galbally&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02906">
<title>MLLM-Protector: Ensuring MLLM&apos;s Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.02906</link>
<description rdf:parseType="Literal">&lt;p&gt;The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a &quot;foreign language&quot; that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector&apos;s role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model&apos;s overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1&quot;&gt;Renjie Pi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tianyang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yueqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1&quot;&gt;Rui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1&quot;&gt;Qing Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hanze Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02916">
<title>Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction. (arXiv:2401.02916v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02916</link>
<description rdf:parseType="Literal">&lt;p&gt;Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02931">
<title>SPFormer: Enhancing Vision Transformer with Superpixel Representation. (arXiv:2401.02931v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02931</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce SPFormer, a novel Vision Transformer enhanced by
superpixel representation. Addressing the limitations of traditional Vision
Transformers&apos; fixed-size, non-adaptive patch partitioning, SPFormer employs
superpixels that adapt to the image&apos;s content. This approach divides the image
into irregular, semantically coherent regions, effectively capturing intricate
details and applicable at both initial and intermediate feature levels.
&lt;/p&gt;
&lt;p&gt;SPFormer, trainable end-to-end, exhibits superior performance across various
benchmarks. Notably, it exhibits significant improvements on the challenging
ImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S
respectively. A standout feature of SPFormer is its inherent explainability.
The superpixel structure offers a window into the model&apos;s internal processes,
providing valuable insights that enhance the model&apos;s interpretability. This
level of clarity significantly improves SPFormer&apos;s robustness, particularly in
challenging scenarios such as image rotations and occlusions, demonstrating its
adaptability and resilience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jieru Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang-Chieh Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02937">
<title>Locally Adaptive Neural 3D Morphable Models. (arXiv:2401.02937v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02937</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Locally Adaptive Morphable Model (LAMM), a highly flexible
Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.
We train our architecture following a simple self-supervised training scheme in
which input displacements over a set of sparse control vertices are used to
overwrite the encoded geometry in order to transform one training sample into
another. During inference, our model produces a dense output that adheres
locally to the specified sparse geometry while maintaining the overall
appearance of the encoded object. This approach results in state-of-the-art
performance in both disentangling manipulated geometry and 3D mesh
reconstruction. To the best of our knowledge LAMM is the first end-to-end
framework that enables direct local control of 3D vertex geometry in a single
forward pass. A very efficient computational graph allows our network to train
with only a fraction of the memory required by previous methods and run faster
during inference, generating 12k vertex meshes at $&amp;gt;$60fps on a single CPU
thread. We further leverage local geometry control as a primitive for higher
level editing operations and present a set of derivative capabilities such as
swapping and sampling object parts. Code and pretrained models can be found at
https://github.com/michaeltrs/LAMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarasiou_M/0/1/0/all/0/1&quot;&gt;Michail Tarasiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1&quot;&gt;Rolandos Alexandros Potamias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OSullivan_E/0/1/0/all/0/1&quot;&gt;Eimear O&amp;#x27;Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ploumpis_S/0/1/0/all/0/1&quot;&gt;Stylianos Ploumpis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02941">
<title>Unsupervised Federated Domain Adaptation for Segmentation of MRI Images. (arXiv:2401.02941v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02941</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic semantic segmentation of magnetic resonance imaging (MRI) images
using deep neural networks greatly assists in evaluating and planning
treatments for various clinical applications. However, training these models is
conditioned on the availability of abundant annotated data to implement the
end-to-end supervised learning procedure. Even if we annotate enough data, MRI
images display considerable variability due to factors such as differences in
patients, MRI scanners, and imaging protocols. This variability necessitates
retraining neural networks for each specific application domain, which, in
turn, requires manual annotation by expert radiologists for all new domains. To
relax the need for persistent data annotation, we develop a method for
unsupervised federated domain adaptation using multiple annotated source
domains. Our approach enables the transfer of knowledge from several annotated
source domains to adapt a model for effective use in an unannotated target
domain. Initially, we ensure that the target domain data shares similar
representations with each source domain in a latent embedding space, modeled as
the output of a deep encoder, by minimizing the pair-wise distances of the
distributions for the target domain and the source domains. We then employ an
ensemble approach to leverage the knowledge obtained from all domains. We
provide theoretical analysis and perform experiments on the MICCAI 2016
multi-site dataset to demonstrate our method is effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nananukul_N/0/1/0/all/0/1&quot;&gt;Navapat Nananukul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanian_zadeh_H/0/1/0/all/0/1&quot;&gt;Hamid Soltanian-zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1&quot;&gt;Mohammad Rostami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02955">
<title>Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively. (arXiv:2401.02955v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02955</link>
<description rdf:parseType="Literal">&lt;p&gt;The CLIP and Segment Anything Model (SAM) are remarkable vision foundation
models (VFMs). SAM excels in segmentation tasks across diverse domains, while
CLIP is renowned for its zero-shot recognition capabilities. This paper
presents an in-depth exploration of integrating these two models into a unified
framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired
model designed for simultaneous interactive segmentation and recognition,
leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The
former adapts SAM&apos;s knowledge into the CLIP via distillation and learnable
transformer adapters, while the latter transfers CLIP knowledge into SAM,
enhancing its recognition capabilities. Extensive experiments on various
datasets and detectors show the effectiveness of Open-Vocabulary SAM in both
segmentation and recognition tasks, significantly outperforming the naive
baselines of simply combining SAM and CLIP. Furthermore, aided with image
classification data training, our method can segment and recognize
approximately 22,000 classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02957">
<title>Denoising Vision Transformers. (arXiv:2401.02957v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.02957</link>
<description rdf:parseType="Literal">&lt;p&gt;We delve into a nuanced but significant challenge inherent to Vision
Transformers (ViTs): feature maps of these models exhibit grid-like artifacts,
which detrimentally hurt the performance of ViTs in downstream tasks. Our
investigations trace this fundamental issue down to the positional embeddings
at the input stage. To address this, we propose a novel noise model, which is
universally applicable to all ViTs. Specifically, the noise model dissects ViT
outputs into three components: a semantics term free from noise artifacts and
two artifact-related terms that are conditioned on pixel locations. Such a
decomposition is achieved by enforcing cross-view feature consistency with
neural fields in a per-image basis. This per-image optimization process
extracts artifact-free features from raw ViT outputs, providing clean features
for offline applications. Expanding the scope of our solution to support online
functionality, we introduce a learnable denoiser to predict artifact-free
features directly from unprocessed ViT outputs, which shows remarkable
generalization capabilities to novel data without the need for per-image
optimization. Our two-stage approach, termed Denoising Vision Transformers
(DVT), does not require re-training existing pre-trained ViTs and is
immediately applicable to any Transformer-based architecture. We evaluate our
method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,
DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT
consistently and significantly improves existing state-of-the-art
general-purpose models in semantic and geometric tasks across multiple datasets
(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT
design, especially regarding the naive use of positional embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiawei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1&quot;&gt;Katie Z Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiefeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Q Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonglong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02952">
<title>Supervision by Denoising for Medical Image Segmentation. (arXiv:2202.02952v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02952</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based image reconstruction models, such as those based on the U-Net,
require a large set of labeled images if good generalization is to be
guaranteed. In some imaging domains, however, labeled data with pixel- or
voxel-level label accuracy are scarce due to the cost of acquiring them. This
problem is exacerbated further in domains like medical imaging, where there is
no single ground truth label, resulting in large amounts of repeat variability
in the labels. Therefore, training reconstruction networks to generalize better
by learning from both labeled and unlabeled examples (called semi-supervised
learning) is problem of practical and theoretical interest. However,
traditional semi-supervised learning methods for image reconstruction often
necessitate handcrafting a differentiable regularizer specific to some given
imaging problem, which can be extremely time-consuming. In this work, we
propose &quot;supervision by denoising&quot; (SUD), a framework that enables us to
supervise reconstruction models using their own denoised output as soft labels.
SUD unifies stochastic averaging and spatial denoising techniques under a
spatio-temporal denoising framework and alternates denoising and model weight
update steps in an optimization framework for semi-supervision. As example
applications, we apply SUD to two problems arising from biomedical imaging --
anatomical brain reconstruction (3D) and cortical parcellation (2D) -- to
demonstrate a significant improvement in the image reconstructions over
supervised-only and stochastic averaging baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Young_S/0/1/0/all/0/1&quot;&gt;Sean I. Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1&quot;&gt;Enzo Ferrante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1&quot;&gt;Polina Golland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Metzler_C/0/1/0/all/0/1&quot;&gt;Christopher A. Metzler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1&quot;&gt;Bruce Fischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Juan Eugenio Iglesias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10907">
<title>DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant Descriptors in Local Feature Matching. (arXiv:2209.10907v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10907</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of local feature descriptors degrades in the presence of
large rotation variations. To address this issue, we present an efficient
approach to learning rotation invariant descriptors. Specifically, we propose
Rotated Kernel Fusion (RKF) which imposes rotations on the convolution kernel
to improve the inherent nature of CNN. Since RKF can be processed by the
subsequent re-parameterization, no extra computational costs will be introduced
in the inference stage. Moreover, we present Multi-oriented Feature Aggregation
(MOFA) which aggregates features extracted from multiple rotated versions of
the input image and can provide auxiliary knowledge for the training of RKF by
leveraging the distillation strategy. We refer to the distilled RKF model as
DRKF. Besides the evaluation on a rotation-augmented version of the public
dataset HPatches, we also contribute a new dataset named DiverseBEV which is
collected during the drone&apos;s flight and consists of bird&apos;s eye view images with
large viewpoint changes and camera rotations. Extensive experiments show that
our method can outperform other state-of-the-art techniques when exposed to
large rotation variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ranran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiancheng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhuoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinmin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09310">
<title>Language-Assisted Deep Learning for Autistic Behaviors Recognition. (arXiv:2211.09310v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09310</link>
<description rdf:parseType="Literal">&lt;p&gt;Correctly recognizing the behaviors of children with Autism Spectrum Disorder
(ASD) is of vital importance for the diagnosis of Autism and timely early
intervention. However, the observation and recording during the treatment from
the parents of autistic children may not be accurate and objective. In such
cases, automatic recognition systems based on computer vision and machine
learning (in particular deep learning) technology can alleviate this issue to a
large extent. Existing human action recognition models can now achieve
persuasive performance on challenging activity datasets, e.g. daily activity,
and sports activity. However, problem behaviors in children with ASD are very
different from these general activities, and recognizing these problem
behaviors via computer vision is less studied. In this paper, we first evaluate
a strong baseline for action recognition, i.e. Video Swin Transformer, on two
autism behaviors datasets (SSBD and ESBD) and show that it can achieve high
accuracy and outperform the previous methods by a large margin, demonstrating
the feasibility of vision-based problem behaviors recognition. Moreover, we
propose language-assisted training to further enhance the action recognition
performance. Specifically, we develop a two-branch multimodal deep learning
framework by incorporating the &quot;freely available&quot; language description for each
type of problem behavior. Experimental results demonstrate that incorporating
additional language supervision can bring an obvious performance boost for the
autism problem behaviors recognition task as compared to using the video
information only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1&quot;&gt;Andong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Taojiannan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neely_L/0/1/0/all/0/1&quot;&gt;Leslie Neely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyama_S/0/1/0/all/0/1&quot;&gt;Sakiko Oyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12735">
<title>Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration. (arXiv:2211.12735v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12735</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose integrally pre-trained transformer pyramid network (iTPN), towards
jointly optimizing the network backbone and the neck, so that transfer gap
between representation models and downstream tasks is minimal. iTPN is born
with two elaborated designs: 1) The first pre-trained feature pyramid upon
vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid
using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing
computational memory overhead and accelerating inference through two flexible
designs. 1) Token migration: dropping redundant tokens of the backbone while
replenishing them in the feature pyramid without attention operations. 2) Token
gathering: reducing computation cost caused by global attention by introducing
few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1
accuracy on ImageNet-1K. With 1x training schedule using DINO, the
base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object
detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using
MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with
negligible performance loss, demonstrating the potential to be a powerful
backbone for downstream vision tasks. The code is available at:
github.com/sunsmarterjie/iTPN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yunjie Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lingxi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jihao Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jianbin Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qixiang Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06683">
<title>Surgical Aggregation: Federated Class-Heterogeneous Learning. (arXiv:2301.06683v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06683</link>
<description rdf:parseType="Literal">&lt;p&gt;The release of numerous chest x-ray datasets has spearheaded the development
of deep learning models with expert-level performance. However, they have
limited interoperability due to class-heterogeneity -- a result of inconsistent
labeling schemes and partial annotations. Therefore, it is challenging to
leverage these datasets in aggregate to train models with a complete
representation of abnormalities that may occur within the thorax. In this work,
we propose surgical aggregation, a federated learning framework for aggregating
knowledge from class-heterogeneous datasets and learn a model that can
simultaneously predict the presence of all disease labels present across the
datasets. We evaluate our method using simulated and real-world
class-heterogeneous datasets across both independent and identically
distributed (iid) and non-iid settings. Our results show that surgical
aggregation outperforms current methods, has better generalizability, and is a
crucial first step towards tackling class-heterogeneity in federated learning
to facilitate the development of clinically-useful models using previously
non-interoperable chest x-ray datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1&quot;&gt;Pranav Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanhere_A/0/1/0/all/0/1&quot;&gt;Adway Kanhere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1&quot;&gt;Paul H. Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1&quot;&gt;Vishwa S. Parekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04940">
<title>Non-aligned supervision for Real Image Dehazing. (arXiv:2303.04940v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04940</link>
<description rdf:parseType="Literal">&lt;p&gt;Removing haze from real-world images is challenging due to unpredictable
weather conditions, resulting in the misalignment of hazy and clear image
pairs. In this paper, we propose an innovative dehazing framework that operates
under non-aligned supervision. This framework is grounded in the atmospheric
scattering model, and consists of three interconnected networks: dehazing,
airlight, and transmission networks. In particular, we explore a non-alignment
scenario that a clear reference image, unaligned with the input hazy image, is
utilized to supervise the dehazing network. To implement this, we present a
multi-scale reference loss that compares the feature representations between
the referred image and the dehazed output. Our scenario makes it easier to
collect hazy/clear image pairs in real-world environments, even under
conditions of misalignment and shift views. To showcase the effectiveness of
our scenario, we have collected a new hazy dataset including 415 image pairs
captured by mobile Phone in both rural and urban areas, called &quot;Phone-Hazy&quot;.
Furthermore, we introduce a self-attention network based on mean and variance
for modeling real infinite airlight, using the dark channel prior as positional
guidance. Additionally, a channel attention network is employed to estimate the
three-channel transmission. Experimental results demonstrate the superior
performance of our framework over existing state-of-the-art techniques in the
real-world image dehazing task. Phone-Hazy and code will be available at
https://fanjunkai1.github.io/projectpage/NSDNet/index.html.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Junkai Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1&quot;&gt;Jianjun Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15065">
<title>Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations. (arXiv:2303.15065v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15065</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical routine and retrospective cohorts commonly include multi-parametric
Magnetic Resonance Imaging; however, they are mostly acquired in different
anisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.
Thus acquired views suffer from poor out-of-plane resolution and affect
downstream volumetric image analysis that typically requires isotropic 3D
scans. Combining different views of multi-contrast scans into high-resolution
isotropic 3D scans is challenging due to the lack of a large training cohort,
which calls for a subject-specific framework. This work proposes a novel
solution to this problem leveraging Implicit Neural Representations (INR). Our
proposed INR jointly learns two different contrasts of complementary views in a
continuous spatial function and benefits from exchanging anatomical information
between them. Trained within minutes on a single commodity GPU, our model
provides realistic super-resolution across different pairs of contrasts in our
experiments with three datasets. Using Mutual Information (MI) as a metric, we
find that our model converges to an optimum MI amongst sequences, achieving
anatomically faithful reconstruction. Code is available at:
https://github.com/jqmcginnis/multi_contrast_inr/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McGinnis_J/0/1/0/all/0/1&quot;&gt;Julian McGinnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shit_S/0/1/0/all/0/1&quot;&gt;Suprosanna Shit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sideri_Lampretsa_V/0/1/0/all/0/1&quot;&gt;Vasiliki Sideri-Lampretsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Graf_R/0/1/0/all/0/1&quot;&gt;Robert Graf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dannecker_M/0/1/0/all/0/1&quot;&gt;Maik Dannecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiazhen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anso_N/0/1/0/all/0/1&quot;&gt;Nil Stolt Ans&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muhlau_M/0/1/0/all/0/1&quot;&gt;Mark M&amp;#xfc;hlau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan S. Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05292">
<title>MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive Impairment in older adults using facial videos. (arXiv:2304.05292v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05292</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dodge_H/0/1/0/all/0/1&quot;&gt;Hiroko H. Dodge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1&quot;&gt;Mohammad H. Mahoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13775">
<title>Advancing Ischemic Stroke Diagnosis: A Novel Two-Stage Approach for Blood Clot Origin Identification. (arXiv:2304.13775v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13775</link>
<description rdf:parseType="Literal">&lt;p&gt;An innovative two-stage methodology for categorizing blood clot origins is
presented in this paper, which is important for the diagnosis and treatment of
ischemic stroke. First, a background classifier based on MobileNetV3 segments
big whole-slide digital pathology images into numerous tiles to detect the
presence of cellular material. After that, different pre-trained image
classification algorithms are fine-tuned to determine the origin of blood
clots. Due to complex blood flow dynamics and limitations in conventional
imaging methods such as computed tomography (CT), magnetic resonance imaging
(MRI), and ultrasound, identifying the sources of blood clots is a challenging
task. Although these techniques are useful for identifying blood clots, they
are not very good at determining how they originated. To address these
challenges, our method makes use of robust computer vision models that have
been refined using information from whole-slide digital pathology images. Out
of all the models tested, the PoolFormer \cite{yu2022metaformer} performs
better than the others, with 93.4\% accuracy, 93.4\% precision, 93.4\% recall,
and 93.4\% F1-score. Moreover, it achieves the good weighted multi-class
logarithmic loss (WMCLL) of 0.4361, which emphasizes how effective it is in
this particular application. These encouraging findings suggest that our
approach can successfully identify the origin of blood clots in a variety of
vascular locations, potentially advancing ischemic stroke diagnosis and
treatment approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krishnan_K/0/1/0/all/0/1&quot;&gt;Koushik Sivarama Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nikesh_P/0/1/0/all/0/1&quot;&gt;P. J. Joe Nikesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gnanasekar_S/0/1/0/all/0/1&quot;&gt;Swathi Gnanasekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krishnan_K/0/1/0/all/0/1&quot;&gt;Karthik Sivarama Krishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12711">
<title>Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12711</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;De Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaojian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lingfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13301">
<title>Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13301</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation. The project&apos;s website can be
found at &lt;a href=&quot;http://rl-diffusion.github.io&quot;&gt;this http URL&lt;/a&gt; .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_K/0/1/0/all/0/1&quot;&gt;Kevin Black&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1&quot;&gt;Michael Janner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1&quot;&gt;Ilya Kostrikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19787">
<title>DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation. (arXiv:2305.19787v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19787</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation aims to partition an image according to the objects in the
scene and is a fundamental step in analysing very high spatial-resolution (VHR)
remote sensing imagery. Current methods struggle to effectively consider land
objects with diverse shapes and sizes. Additionally, the determination of
segmentation scale parameters frequently adheres to a static and empirical
doctrine, posing limitations on the segmentation of large-scale remote sensing
images and yielding algorithms with limited interpretability. To address the
above challenges, we propose a deep-learning-based region merging method dubbed
DeepMerge to handle the segmentation of complete objects in large VHR images by
integrating deep learning and region adjacency graph (RAG). This is the first
method to use deep learning to learn the similarity and merge similar adjacent
super-pixels in RAG. We propose a modified binary tree sampling method to
generate shift-scale data, serving as inputs for transformer-based deep
learning networks, a shift-scale attention with 3-Dimension relative position
embedding to learn features across scales, and an embedding to fuse learned
features with hand-crafted features. DeepMerge can achieve high segmentation
accuracy in a supervised manner from large-scale remotely sensed images and
provides an interpretable optimal scale parameter, which is validated using a
remote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The
experimental results show that DeepMerge achieves the highest F value (0.9550)
and the lowest total error TE (0.0895), correctly segmenting objects of
different sizes and outperforming all competing segmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xianwei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Persello_C/0/1/0/all/0/1&quot;&gt;Claudio Persello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wangbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_D/0/1/0/all/0/1&quot;&gt;Dongping Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1&quot;&gt;Alfred Stein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02986">
<title>Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models. (arXiv:2306.02986v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02986</link>
<description rdf:parseType="Literal">&lt;p&gt;Large annotated datasets are required for training deep learning models, but
in medical imaging data sharing is often complicated due to ethics,
anonymization and data protection legislation. Generative AI models, such as
generative adversarial networks (GANs) and diffusion models, can today produce
very realistic synthetic images, and can potentially facilitate data sharing.
However, in order to share synthetic medical images it must first be
demonstrated that they can be used for training different networks with
acceptable performance. Here, we therefore comprehensively evaluate four GANs
(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain
tumor segmentation (using two segmentation networks, U-Net and a Swin
transformer). Our results show that segmentation networks trained on synthetic
images reach Dice scores that are 80% - 90% of Dice scores when training with
real images, but that memorization of the training images can be a problem for
diffusion models if the original dataset is too small. Our conclusion is that
sharing synthetic medical images is a viable option to sharing real images, but
that further work is required. The trained generative models and the generated
synthetic images are shared on AIDA data hub
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akbar_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman Akbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Larsson_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;ns Larsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eklund_A/0/1/0/all/0/1&quot;&gt;Anders Eklund&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04512">
<title>Cross-attention learning enables real-time nonuniform rotational distortion correction in OCT. (arXiv:2306.04512v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04512</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonuniform rotational distortion (NURD) correction is vital for endoscopic
optical coherence tomography (OCT) imaging and its functional extensions, such
as angiography and elastography. Current NURD correction methods require
time-consuming feature tracking or cross-correlation calculations and thus
sacrifice temporal resolution. Here we propose a cross-attention learning
method for the NURD correction in OCT. Our method is inspired by the recent
success of the self-attention mechanism in natural language processing and
computer vision. By leveraging its ability to model long-range dependencies, we
can directly obtain the correlation between OCT A-lines at any distance, thus
accelerating the NURD correction. We develop an end-to-end stacked
cross-attention network and design three types of optimization constraints. We
compare our method with two traditional feature-based methods and a CNN-based
method, on two publicly-available endoscopic OCT datasets and a private dataset
collected on our home-built endoscopic OCT system. Our method achieved a
$\sim3\times$ speedup to real time ($26\pm 3$ fps), and superior correction
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianlong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aili Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05411">
<title>R-MAE: Regions Meet Masked Autoencoders. (arXiv:2306.05411v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05411</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we explore regions as a potential visual analogue of words for
self-supervised image representation learning. Inspired by Masked Autoencoding
(MAE), a generative pre-training baseline, we propose masked region
autoencoding to learn from groups of pixels or regions. Specifically, we design
an architecture which efficiently addresses the one-to-many mapping between
images and regions, while being highly effective especially with high-quality
regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent
improvements across various pre-training datasets and downstream detection and
segmentation benchmarks, with negligible computational overheads. Beyond the
quantitative evaluation, our analysis indicates the models pre-trained with
masked region autoencoding unlock the potential for interactive segmentation.
The code is provided at https://github.com/facebookresearch/r-mae.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duy-Kien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaibhav Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1&quot;&gt;Martin R. Oswald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1&quot;&gt;Alexander Kirillov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10159">
<title>Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos. (arXiv:2306.10159v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10159</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing the activities causing distraction in real-world driving
scenarios is critical for ensuring the safety and reliability of both drivers
and pedestrians on the roadways. Conventional computer vision techniques are
typically data-intensive and require a large volume of annotated training data
to detect and classify various distracted driving behaviors, thereby limiting
their efficiency and scalability. We aim to develop a generalized framework
that showcases robust performance with access to limited or no annotated
training data. Recently, vision-language models have offered large-scale
visual-textual pretraining that can be adapted to task-specific learning like
distracted driving activity recognition. Vision-language pretraining models,
such as CLIP, have shown significant promise in learning natural
language-guided visual representations. This paper proposes a CLIP-based driver
activity recognition approach that identifies driver distraction from
naturalistic driving images and videos. CLIP&apos;s vision embedding offers
zero-shot transfer and task-based finetuning, which can classify distracted
activities from driving video data. Our results show that this framework offers
state-of-the-art performance on zero-shot transfer and video-based CLIP for
predicting the driver&apos;s state on two public datasets. We propose both
frame-based and video-based frameworks developed on top of the CLIP&apos;s visual
representation for distracted driving detection and classification tasks and
report the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Zahid Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiajing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mohammed Shaiqur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ameya Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velipasalar_S/0/1/0/all/0/1&quot;&gt;Senem Velipasalar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anuj Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17019">
<title>Histopathology Slide Indexing and Search: Are We There Yet?. (arXiv:2306.17019v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17019</link>
<description rdf:parseType="Literal">&lt;p&gt;The search and retrieval of digital histopathology slides is an important
task that has yet to be solved. In this case study, we investigate the clinical
readiness of three state-of-the-art histopathology slide search engines,
Yottixel, SISH, and RetCCL, on three patients with solid tumors. We provide a
qualitative assessment of each model&apos;s performance in providing retrieval
results that are reliable and useful to pathologists. We found that all three
image search engines fail to produce consistently reliable results and have
difficulties in capturing granular and subtle features of malignancy, limiting
their diagnostic accuracy. Based on our findings, we also propose a minimal set
of requirements to further advance the development of accurate and reliable
histopathology image search engines for successful clinical adoption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shang_H/0/1/0/all/0/1&quot;&gt;Helen H. Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nasr_M/0/1/0/all/0/1&quot;&gt;Mohammad Sadegh Nasr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Veerla_J/0/1/0/all/0/1&quot;&gt;Jai Prakash Veerla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Malidarreh_P/0/1/0/all/0/1&quot;&gt;Parisa Boodaghi Malidarreh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saurav_M/0/1/0/all/0/1&quot;&gt;MD Jillur Rahman Saurav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hajighasemi_A/0/1/0/all/0/1&quot;&gt;Amir Hajighasemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Manfred Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moleta_C/0/1/0/all/0/1&quot;&gt;Chace Moleta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Makker_J/0/1/0/all/0/1&quot;&gt;Jitin Makker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luber_J/0/1/0/all/0/1&quot;&gt;Jacob M. Luber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03538">
<title>Language-free Compositional Action Generation via Decoupling Refinement. (arXiv:2307.03538v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03538</link>
<description rdf:parseType="Literal">&lt;p&gt;Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04074">
<title>Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction on Monocular RGB Video. (arXiv:2308.04074v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04074</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weichao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hezhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+li_L/0/1/0/all/0/1&quot;&gt;Li li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07688">
<title>Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07688</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training datasets, like ImageNet, have become the gold standard in
medical image analysis. However, the emergence of self-supervised learning
(SSL), which leverages unlabeled data to learn robust features, presents an
opportunity to bypass the intensive labeling process. In this study, we
explored if SSL for pre-training on non-medical images can be applied to chest
radiographs and how it compares to supervised pre-training on non-medical
images and on medical images. We utilized a vision transformer and initialized
its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL
pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on
chest radiographs from the MIMIC-CXR database. We tested our approach on over
800,000 chest radiographs from six large global datasets, diagnosing more than
20 different imaging findings. Our SSL pre-training on curated images not only
outperformed ImageNet-based pre-training (P&amp;lt;0.001 for all datasets) but, in
certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest
that selecting the right pre-training strategy, especially with SSL, can be
pivotal for improving artificial intelligence (AI)&apos;s diagnostic accuracy in
medical imaging. By demonstrating the promise of SSL in chest radiograph
analysis, we underline a transformative shift towards more efficient and
accurate AI models in medical imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arasteh_S/0/1/0/all/0/1&quot;&gt;Soroosh Tayebi Arasteh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Misera_L/0/1/0/all/0/1&quot;&gt;Leo Misera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kather_J/0/1/0/all/0/1&quot;&gt;Jakob Nikolas Kather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nebelung_S/0/1/0/all/0/1&quot;&gt;Sven Nebelung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07728">
<title>Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07728</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Seokhyeon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Sunbeom Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwoo Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12910">
<title>SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data. (arXiv:2308.12910v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12910</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Subject-Conditional Relation Detection SCoRD, where conditioned on
an input subject, the goal is to predict all its relations to other objects in
a scene along with their locations. Based on the Open Images dataset, we
propose a challenging OIv6-SCoRD benchmark such that the training and testing
splits have a distribution shift in terms of the occurrence statistics of
$\langle$subject, relation, object$\rangle$ triplets. To solve this problem, we
propose an auto-regressive model that given a subject, it predicts its
relations, objects, and object locations by casting this output as a sequence
of tokens. First, we show that previous scene-graph prediction methods fail to
produce as exhaustive an enumeration of relation-object pairs when conditioned
on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for
our relation-object predictions compared to the 49.75% obtained by a recent
scene graph detector. Then, we show improved generalization on both
relation-object and object-box predictions by leveraging during training
relation-object pairs obtained automatically from textual captions and for
which no object-box annotations are available. Particularly, for
$\langle$subject, relation, object$\rangle$ triplets for which no object
locations are available during training, we are able to obtain a recall@3 of
33.80% for relation-object pairs and 26.75% for their box locations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1&quot;&gt;Kushal Kafle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Scott Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhihong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1&quot;&gt;Vicente Ordonez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15752">
<title>Large-scale data extraction from the UNOS organ donor documents. (arXiv:2308.15752v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15752</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we focus on three major task: 1) discussing our methods: Our
method captures a portion of the data in DCD flowsheets, kidney perfusion data,
and Flowsheet data captured peri-organ recovery surgery. 2) demonstrating the
result: We built a comprehensive, analyzable database from 2022 OPTN data. This
dataset is by far larger than any previously available even in this preliminary
phase; and 3) proving that our methods can be extended to all the past OPTN
data and future data.
&lt;/p&gt;
&lt;p&gt;The scope of our study is all Organ Procurement and Transplantation Network
(OPTN) data of the USA organ donors since 2008. The data was not analyzable in
a large scale in the past because it was captured in PDF documents known as
``Attachments&apos;&apos;, whereby every donor&apos;s information was recorded into dozens of
PDF documents in heterogeneous formats. To make the data analyzable, one needs
to convert the content inside these PDFs to an analyzable data format, such as
a standard SQL database. In this paper we will focus on 2022 OPTN data, which
consists of $\approx 400,000$ PDF documents spanning millions of pages. The
entire OPTN data covers 15 years (2008--20022). This paper assumes that readers
are familiar with the content of the OPTN data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rychlik_M/0/1/0/all/0/1&quot;&gt;Marek Rychlik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanriover_B/0/1/0/all/0/1&quot;&gt;Bekir Tanriover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yan Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16742">
<title>Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in Dual Domains. (arXiv:2308.16742v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16742</link>
<description rdf:parseType="Literal">&lt;p&gt;During the process of computed tomography (CT), metallic implants often cause
disruptive artifacts in the reconstructed images, impeding accurate diagnosis.
Several supervised deep learning-based approaches have been proposed for
reducing metal artifacts (MAR). However, these methods heavily rely on training
with simulated data, as obtaining paired metal artifact CT and clean CT data in
clinical settings is challenging. This limitation can lead to decreased
performance when applying these methods in clinical practice. Existing
unsupervised MAR methods, whether based on learning or not, typically operate
within a single domain, either in the image domain or the sinogram domain. In
this paper, we propose an unsupervised MAR method based on the diffusion model,
a generative model with a high capacity to represent data distributions.
Specifically, we first train a diffusion model using CT images without metal
artifacts. Subsequently, we iteratively utilize the priors embedded within the
pre-trained diffusion model in both the sinogram and image domains to restore
the degraded portions caused by metal artifacts. This dual-domain processing
empowers our approach to outperform existing unsupervised MAR methods,
including another MAR method based on the diffusion model, which we have
qualitatively and quantitatively validated using synthetic datasets. Moreover,
our method demonstrates superior visual results compared to both supervised and
unsupervised methods on clinical datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yaoqin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diao_S/0/1/0/all/0/1&quot;&gt;Songhui Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaokun Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06023">
<title>Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration. (arXiv:2309.06023v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06023</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has emerged as a prevailing paradigm for high-level
vision tasks, which, by introducing properly negative samples, has also been
exploited for low-level vision tasks to achieve a compact optimization space to
account for their ill-posed nature. However, existing methods rely on manually
predefined and task-oriented negatives, which often exhibit pronounced
task-specific biases. To address this challenge, our paper introduces an
innovative method termed &apos;learning from history&apos;, which dynamically generates
negative samples from the target model itself. Our approach, named Model
Contrastive paradigm for Image Restoration (MCIR), rejuvenates latency models
as negative models, making it compatible with diverse image restoration tasks.
We propose the Self-Prior guided Negative loss (SPN) to enable it. This
approach significantly enhances existing models when retrained with the
proposed model contrastive paradigm. The results show significant improvements
in image restoration across various tasks and architectures. For example,
models retrained with SPN outperform the original FFANet and DehazeFormer by
3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,
they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image
deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over
lightweight SwinIR, respectively. Code and retrained models are available at
https://github.com/Aitical/MCIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08471">
<title>TreeLearn: A Comprehensive Deep Learning Method for Segmenting Individual Trees from Ground-Based LiDAR Forest Point Clouds. (arXiv:2309.08471v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08471</link>
<description rdf:parseType="Literal">&lt;p&gt;Laser-scanned point clouds of forests make it possible to extract valuable
information for forest management. To consider single trees, a forest point
cloud needs to be segmented into individual tree point clouds. Existing
segmentation methods are usually based on hand-crafted algorithms, such as
identifying trunks and growing trees from them, and face difficulties in dense
forests with overlapping tree crowns. In this study, we propose TreeLearn, a
deep learning-based approach for tree instance segmentation of forest point
clouds. Unlike previous methods, TreeLearn is trained on already segmented
point clouds in a data-driven manner, making it less reliant on predefined
features and algorithms. Furthermore, TreeLearn is implemented as a fully
automatic pipeline and does not rely on extensive hyperparameter tuning, which
makes it easy to use. Additionally, we introduce a new manually segmented
benchmark forest dataset containing 156 full trees, and 79 partial trees, that
have been cleanly segmented by hand. The data is generated by mobile laser
scanning and contributes to create a larger and more diverse data basis for
model development and fine-grained instance segmentation evaluation. We trained
TreeLearn on forest point clouds of 6665 trees, labeled using the Lidar360
software. An evaluation on the benchmark dataset shows that TreeLearn performs
equally well or better than the algorithm used to generate its training data.
Furthermore, the method&apos;s performance can be vastly improved by fine-tuning on
the cleanly labeled benchmark dataset. The TreeLearn code is available from
https://github.com/ecker-lab/TreeLearn. The data as well as trained models can
be found at https://doi.org/10.25625/VPMPID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henrich_J/0/1/0/all/0/1&quot;&gt;Jonathan Henrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delden_J/0/1/0/all/0/1&quot;&gt;Jan van Delden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seidel_D/0/1/0/all/0/1&quot;&gt;Dominik Seidel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kneib_T/0/1/0/all/0/1&quot;&gt;Thomas Kneib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecker_A/0/1/0/all/0/1&quot;&gt;Alexander Ecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16924">
<title>Incremental Rotation Averaging Revisited and More: A New Rotation Averaging Benchmark. (arXiv:2309.16924v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16924</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to further advance the accuracy and robustness of the incremental
parameter estimation-based rotation averaging methods, in this paper, a new
member of the Incremental Rotation Averaging (IRA) family is introduced, which
is termed as IRAv4. As the most significant feature of the IRAv4, a
task-specific connected dominating set is extracted to serve as a more reliable
and accurate reference for rotation global alignment. In addition, to further
address the limitations of the existing rotation averaging benchmark of relying
on the slightly outdated Bundler camera calibration results as ground truths
and focusing solely on rotation estimation accuracy, this paper presents a new
COLMAP-based rotation averaging benchmark that incorporates a cross check
between COLMAP and Bundler, and employ the accuracy of both rotation and
downstream location estimation as evaluation metrics, which is desired to
provide a more reliable and comprehensive evaluation tool for the rotation
averaging research. Comprehensive comparisons between the proposed IRAv4 and
other mainstream rotation averaging methods on this new benchmark demonstrate
the effectiveness of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hainan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Shuhan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03937">
<title>Diffusion Models as Masked Audio-Video Learners. (arXiv:2310.03937v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03937</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model&apos;s performance
in downstream audio-classification tasks when compared to MAViL&apos;s performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunez_E/0/1/0/all/0/1&quot;&gt;Elvis Nunez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yanzi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1&quot;&gt;Mohammad Rastegari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Sachin Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1&quot;&gt;Maxwell Horton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16221">
<title>Hierarchical Randomized Smoothing. (arXiv:2310.16221v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16221</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1&quot;&gt;Yan Scholten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1&quot;&gt;Jan Schuchardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Bojchevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05197">
<title>Deep learning in computed tomography pulmonary angiography imaging: a dual-pronged approach for pulmonary embolism detection. (arXiv:2311.05197v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05197</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA)
for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need
for improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis
(CAD) of PE. With this aim, we propose a classifier-guided detection approach
that effectively leverages the classifier&apos;s probabilistic inference to direct
the detection predictions, marking a novel contribution in the domain of
automated PE diagnosis. Our classification system includes an Attention-Guided
Convolutional Neural Network (AG-CNN) that uses local context by employing an
attention mechanism. This approach emulates a human expert&apos;s attention by
looking at both global appearances and local lesion regions before making a
decision. The classifier demonstrates robust performance on the FUMPE dataset,
achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an
F1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN
outperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain.
While previous research has mostly focused on finding PE in the main arteries,
our use of cutting-edge object detection models and ensembling techniques
greatly improves the accuracy of detecting small embolisms in the peripheral
arteries. Finally, our proposed classifier-guided detection approach further
refines the detection metrics, contributing new state-of-the-art to the
community: mAP$_{50}$, sensitivity, and F1-score of 0.846, 0.901, and 0.779,
respectively, outperforming the former benchmark with a significant 3.7%
improvement in mAP$_{50}$. Our research aims to elevate PE patient care by
integrating AI solutions into clinical workflows, highlighting the potential of
human-AI collaboration in medical diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bushra_F/0/1/0/all/0/1&quot;&gt;Fabiha Bushra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarmun_R/0/1/0/all/0/1&quot;&gt;Rusab Sarmun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Saidul Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Said_M/0/1/0/all/0/1&quot;&gt;Menatalla Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghoul_S/0/1/0/all/0/1&quot;&gt;Sohaib Bassam Zoghoul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mushtak_A/0/1/0/all/0/1&quot;&gt;Adam Mushtak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hashimi_I/0/1/0/all/0/1&quot;&gt;Israa Al-Hashimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1&quot;&gt;Abdulrahman Alqahtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09215">
<title>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy. (arXiv:2311.09215v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09215</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern computer vision offers a great variety of models to practitioners, and
selecting a model from multiple options for specific applications can be
challenging. Conventionally, competing model architectures and training
protocols are compared by their classification accuracy on ImageNet. However,
this single metric does not fully capture performance nuances critical for
specialized tasks. In this work, we conduct an in-depth comparative analysis of
model behaviors beyond ImageNet accuracy, for both ConvNet and Vision
Transformer architectures, each across supervised and CLIP training paradigms.
Although our selected models have similar ImageNet accuracies and compute
requirements, we find that they differ in many other aspects: types of
mistakes, output calibration, transferability, and feature invariance, among
others. This diversity in model characteristics, not captured by traditional
metrics, highlights the need for more nuanced analysis when choosing among
different models. Our code is available at
https://github.com/kirill-vish/Beyond-INet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishniakov_K/0/1/0/all/0/1&quot;&gt;Kirill Vishniakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11700">
<title>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting. (arXiv:2311.11700v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11700</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1&quot;&gt;Delin Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12144">
<title>Applications of Large Scale Foundation Models for Autonomous Driving. (arXiv:2311.12144v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12144</link>
<description rdf:parseType="Literal">&lt;p&gt;Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13018">
<title>GeoLocator: a location-integrated large multimodal model for inferring geo-privacy. (arXiv:2311.13018v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13018</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic privacy or geo-privacy refers to the keeping private of one&apos;s
geographic location, especially the restriction of geographical data maintained
by personal electronic devices. Geo-privacy is a crucial aspect of personal
security; however, it often goes unnoticed in daily activities. With the surge
in the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source
Intelligence (OSINT), the potential risks associated with geo-privacy breaches
have intensified. This study develops a location-integrated GPT-4 based model
named GeoLocator and designs four-dimensional experiments to demonstrate its
capability in inferring the locational information of input imageries and/or
social media contents. Our experiments reveal that GeoLocator generates
specific geographic details with high accuracy and consequently embeds the risk
of the model users exposing geospatial information to the public
unintentionally, highlighting the thread of online data sharing, information
gathering technologies and LLMs on geo-privacy. We conclude with the broader
implications of GeoLocator and our findings for individuals and the community
at large, by emphasizing the urgency for enhanced awareness and protective
measures against geo-privacy leakage in the era of advanced AI and widespread
social media usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siqin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuju Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junzhou He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01129">
<title>ControlDreamer: Stylized 3D Generation with Multi-View ControlNet. (arXiv:2312.01129v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01129</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-to-3D generation have significantly contributed
to the automation and democratization of 3D content creation. Building upon
these developments, we aim to address the limitations of current methods in
generating 3D models with creative geometry and styles. We introduce multi-view
ControlNet, a novel depth-aware multi-view diffusion model trained on generated
datasets from a carefully curated text corpus. Our multi-view ControlNet is
then integrated into our two-stage pipeline, ControlDreamer, enabling
text-guided generation of stylized 3D models. Additionally, we present a
comprehensive benchmark for 3D style editing, encompassing a broad range of
subjects, including objects, animals, and characters, to further facilitate
research on diverse 3D generation. Our comparative analysis reveals that this
new pipeline outperforms existing text-to-3D methods as evidenced by human
evaluations and CLIP score metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;Yeongtak Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jooyoung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yongsung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1&quot;&gt;Minjun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Chaehun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07180">
<title>Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation. (arXiv:2312.07180v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07180</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing recurrent optical flow estimation networks are computationally
expensive since they use a fixed large number of iterations to update the flow
field for each sample. An efficient network should skip iterations when the
flow improvement is limited. In this paper, we develop a Context-Aware
Iteration Policy Network for efficient optical flow estimation, which
determines the optimal number of iterations per sample. The policy network
achieves this by learning contextual information to realize whether flow
improvement is bottlenecked or minimal. On the one hand, we use iteration
embedding and historical hidden cell, which include previous iterations
information, to convey how flow has changed from previous iterations. On the
other hand, we use the incremental loss to make the policy network implicitly
perceive the magnitude of optical flow improvement in the subsequent iteration.
Furthermore, the computational complexity in our dynamic network is
controllable, allowing us to satisfy various resource preferences with a single
trained model. Our policy network can be easily integrated into
state-of-the-art optical flow networks. Extensive experiments show that our
method maintains performance while reducing FLOPs by about 40%/20% for the
Sintel/KITTI datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ri Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08785">
<title>Managing the unknown: a survey on Open Set Recognition and tangential areas. (arXiv:2312.08785v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08785</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world scenarios classification models are often required to perform
robustly when predicting samples belonging to classes that have not appeared
during its training stage. Open Set Recognition addresses this issue by
devising models capable of detecting unknown classes from samples arriving
during the testing phase, while maintaining a good level of performance in the
classification of samples belonging to known classes. This review
comprehensively overviews the recent literature related to Open Set
Recognition, identifying common practices, limitations, and connections of this
field with other machine learning research areas, such as continual learning,
out-of-distribution detection, novelty detection, and uncertainty estimation.
Our work also uncovers open problems and suggests several research directions
that may motivate and articulate future efforts towards more safe Artificial
Intelligence methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barcina_Blanco_M/0/1/0/all/0/1&quot;&gt;Marcos Barcina-Blanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1&quot;&gt;Jesus L. Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Bringas_P/0/1/0/all/0/1&quot;&gt;Pablo Garcia-Bringas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10983">
<title>MatchDet: A Collaborative Framework for Image Matching and Object Detection. (arXiv:2312.10983v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10983</link>
<description rdf:parseType="Literal">&lt;p&gt;Image matching and object detection are two fundamental and challenging
tasks, while many related applications consider them two individual tasks (i.e.
task-individual). In this paper, a collaborative framework called MatchDet
(i.e. task-collaborative) is proposed for image matching and object detection
to obtain mutual improvements. To achieve the collaborative learning of the two
tasks, we propose three novel modules, including a Weighted Spatial Attention
Module (WSAM) for Detector, and Weighted Attention Module (WAM) and Box Filter
for Matcher. Specifically, the WSAM highlights the foreground regions of target
image to benefit the subsequent detector, the WAM enhances the connection
between the foreground regions of pair images to ensure high-quality matches,
and Box Filter mitigates the impact of false matches. We evaluate the
approaches on a new benchmark with two datasets called Warp-COCO and
miniScanNet. Experimental results show our approaches are effective and achieve
competitive improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jinxiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenlong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1&quot;&gt;Bin-Bin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1&quot;&gt;Congchong Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11153">
<title>Research on Multilingual Natural Scene Text Detection Algorithm. (arXiv:2312.11153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11153</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural scene text detection is a significant challenge in computer vision,
with tremendous potential applications in multilingual, diverse, and complex
text scenarios. We propose a multilingual text detection model to address the
issues of low accuracy and high difficulty in detecting multilingual text in
natural scenes. In response to the challenges posed by multilingual text images
with multiple character sets and various font styles, we introduce the SFM Swin
Transformer feature extraction network to enhance the model&apos;s robustness in
detecting characters and fonts across different languages. Dealing with the
considerable variation in text scales and complex arrangements in natural scene
text images, we present the AS-HRFPN feature fusion network by incorporating an
Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module.
The feature fusion network improvements enhance the model&apos;s ability to detect
text sizes and orientations. Addressing diverse backgrounds and font variations
in multilingual scene text images is a challenge for existing methods. Limited
local receptive fields hinder detection performance. To overcome this, we
propose a Global Semantic Segmentation Branch, extracting and preserving global
features for more effective text detection, aligning with the need for
comprehensive information. In this study, we collected and built a real-world
multilingual natural scene text image dataset and conducted comprehensive
experiments and analyses. The experimental results demonstrate that the
proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher
than the baseline model. We also conducted extensive cross-dataset validation
on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of
our approach. The code and dataset can be found at
https://github.com/wangmelon/CEMLT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14404">
<title>Cross-Covariate Gait Recognition: A Benchmark. (arXiv:2312.14404v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14404</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait datasets are essential for gait research. However, this paper observes
that present benchmarks, whether conventional constrained or emerging
real-world datasets, fall short regarding covariate diversity. To bridge this
gap, we undertake an arduous 20-month effort to collect a cross-covariate gait
recognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6
million sequences; almost every subject has 33 views and 53 different
covariates. Compared to existing datasets, CCGR has both population and
individual-level diversity. In addition, the views and covariates are well
labeled, enabling the analysis of the effects of different factors. CCGR
provides multiple types of gait data, including RGB, parsing, silhouette, and
pose, offering researchers a comprehensive resource for exploration. In order
to delve deeper into addressing cross-covariate gait recognition, we propose
parsing-based gait recognition (ParsingGait) by utilizing the newly proposed
parsing data. We have conducted extensive experiments. Our main results show:
1) Cross-covariate emerges as a pivotal challenge for practical applications of
gait recognition. 2) ParsingGait demonstrates remarkable potential for further
advancement. 3) Alarmingly, existing SOTA methods achieve less than 43%
accuracy on the CCGR, highlighting the urgency of exploring cross-covariate
gait recognition. Link: https://github.com/ShinanZou/CCGR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1&quot;&gt;Shinan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jianbo Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuanfu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shiqi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17163">
<title>FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17163</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. The Code is available at
https://github.com/HanyangZhong/FENet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liman Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17290">
<title>Predicting Parkinson&apos;s disease evolution using deep learning. (arXiv:2312.17290v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17290</link>
<description rdf:parseType="Literal">&lt;p&gt;Parkinson&apos;s disease is a neurological condition that occurs in nearly 1% of
the world&apos;s population. The disease is manifested by a drop in dopamine
production, symptoms are cognitive and behavioural and include a wide range of
personality changes, depressive disorders, memory problems, and emotional
dysregulation, which can occur as the disease progresses. Early diagnosis and
accurate staging of the disease are essential to apply the appropriate
therapeutic approaches to slow cognitive and motor decline.
&lt;/p&gt;
&lt;p&gt;Currently, there is not a single blood test or biomarker available to
diagnose Parkinson&apos;s disease. Magnetic resonance imaging has been used for the
past three decades to diagnose and distinguish between PD and other
neurological conditions. However, in recent years new possibilities have
arisen: several AI algorithms have been developed to increase the precision and
accuracy of differential diagnosis of PD at an early stage.
&lt;/p&gt;
&lt;p&gt;To our knowledge, no AI tools have been designed to identify the stage of
progression. This paper aims to fill this gap. Using the &quot;Parkinson&apos;s
Progression Markers Initiative&quot; dataset, which reports the patient&apos;s MRI and an
indication of the disease stage, we developed a model to identify the level of
progression. The images and the associated scores were used for training and
assessing different deep-learning models. Our analysis distinguished four
distinct disease progression levels based on a standard scale (Hoehn and Yah
scale). The final architecture consists of the cascading of a 3DCNN network,
adopted to reduce and extract the spatial characteristics of the RMI for
efficient training of the successive LSTM layers, aiming at modelling the
temporal dependencies among the data.
&lt;/p&gt;
&lt;p&gt;Our results show that the proposed 3DCNN + LSTM model achieves
state-of-the-art results by classifying the elements with 91.90\% as macro
averaged OVR AUC on four classes
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frasca_M/0/1/0/all/0/1&quot;&gt;Maria Frasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Torre_D/0/1/0/all/0/1&quot;&gt;Davide La Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pravettoni_G/0/1/0/all/0/1&quot;&gt;Gabriella Pravettoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cutica_I/0/1/0/all/0/1&quot;&gt;Ilaria Cutica&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01386">
<title>Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01386</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Himel_G/0/1/0/all/0/1&quot;&gt;Galib Muhammad Shahriar Himel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02309">
<title>TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection. (arXiv:2401.02309v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02309</link>
<description rdf:parseType="Literal">&lt;p&gt;Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenjing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Wei Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02361">
<title>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection. (arXiv:2401.02361v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02361</link>
<description rdf:parseType="Literal">&lt;p&gt;Grounding-DINO is a state-of-the-art open-set detection model that tackles
multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase
Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness
has led to its widespread adoption as a mainstream architecture for various
downstream applications. However, despite its significance, the original
Grounding-DINO model lacks comprehensive public technical details due to the
unavailability of its training code. To bridge this gap, we present
MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,
which is built with the MMDetection toolbox. It adopts abundant vision datasets
for pre-training and various detection and grounding datasets for fine-tuning.
We give a comprehensive analysis of each reported result and detailed settings
for reproduction. The extensive experiments on the benchmarks mentioned
demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny
baseline. We release all our models to the research community. Codes and
trained models are released at
https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yicheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haian Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02329">
<title>Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;Data heterogeneity, characterized by disparities in local data distribution
across clients, poses a significant challenge in federated learning.
Substantial efforts have been devoted to addressing the heterogeneity in local
label distribution. As minority classes suffer from worse accuracy due to
overfitting on local imbalanced data, prior methods often incorporate
class-balanced learning techniques during local training. Despite the improved
mean accuracy across all classes, we observe that empty classes-referring to
categories absent from a client&apos;s data distribution-are still not well
recognized. This paper introduces FedED, a novel approach in heterogeneous
federated learning that integrates both empty-class distillation and logit
suppression simultaneously. Specifically, empty-class distillation leverages
knowledge distillation during local training on each client to retain essential
information related to empty classes from the global model. Moreover, logit
suppression directly penalizes network logits for non-label classes,
effectively addressing misclassifications in minority classes that may be
biased toward majority classes. Extensive experiments validate the efficacy of
FedED, surpassing previous state-of-the-art methods across diverse datasets
with varying degrees of label distribution shift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Kuangpu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yuhe Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>