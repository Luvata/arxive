<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.06134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.14778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.08227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.10736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.07493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.09559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10926" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.11094">
<title>Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility. (arXiv:2307.11094v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.11094</link>
<description rdf:parseType="Literal">&lt;p&gt;A delayed feedback reservoir (DFR) is a type of reservoir computing system
well-suited for hardware implementations owing to its simple structure. Most
existing DFR implementations use analog circuits that require both
digital-to-analog and analog-to-digital converters for interfacing. However,
digital DFRs emulate analog nonlinear components in the digital domain,
resulting in a lack of design flexibility and higher power consumption. In this
paper, we propose a novel modular DFR model that is suitable for fully digital
implementations. The proposed model reduces the number of hyperparameters and
allows flexibility in the selection of the nonlinear function, which improves
the accuracy while reducing the power consumption. We further present two DFR
realizations with different nonlinear functions, achieving 10x power reduction
and 5.3x throughput improvement while maintaining equal or better accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikeda_S/0/1/0/all/0/1&quot;&gt;Sosei Ikeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awano_H/0/1/0/all/0/1&quot;&gt;Hiromitsu Awano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1&quot;&gt;Takashi Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11096">
<title>Towards the Better Ranking Consistency: A Multi-task Learning Framework for Early Stage Ads Ranking. (arXiv:2307.11096v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.11096</link>
<description rdf:parseType="Literal">&lt;p&gt;Dividing ads ranking system into retrieval, early, and final stages is a
common practice in large scale ads recommendation to balance the efficiency and
accuracy. The early stage ranking often uses efficient models to generate
candidates out of a set of retrieved ads. The candidates are then fed into a
more computationally intensive but accurate final stage ranking system to
produce the final ads recommendation. As the early and final stage ranking use
different features and model architectures because of system constraints, a
serious ranking consistency issue arises where the early stage has a low ads
recall, i.e., top ads in the final stage are ranked low in the early stage. In
order to pass better ads from the early to the final stage ranking, we propose
a multi-task learning framework for early stage ranking to capture multiple
final stage ranking components (i.e. ads clicks and ads quality events) and
their task relations. With our multi-task learning framework, we can not only
achieve serving cost saving from the model consolidation, but also improve the
ads recall and ranking consistency. In the online A/B testing, our framework
achieves significantly higher click-through rate (CTR), conversion rate (CVR),
total value and better ads-quality (e.g. reduced ads cross-out rate) in a large
scale industrial ads ranking system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shengyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhengli Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1&quot;&gt;Ellie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chordia_S/0/1/0/all/0/1&quot;&gt;Sagar Chordia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenlin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11099">
<title>Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2307.11099</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving multiphysics-based inverse problems for geological carbon storage
monitoring can be challenging when multimodal time-lapse data are expensive to
collect and costly to simulate numerically. We overcome these challenges by
combining computationally cheap learned surrogates with learned constraints.
Not only does this combination lead to vastly improved inversions for the
important fluid-flow property, permeability, it also provides a natural
platform for inverting multimodal data including well measurements and
active-source time-lapse seismic data. By adding a learned constraint, we
arrive at a computationally feasible inversion approach that remains accurate.
This is accomplished by including a trained deep neural network, known as a
normalizing flow, which forces the model iterates to remain in-distribution,
thereby safeguarding the accuracy of trained Fourier neural operators that act
as surrogates for the computationally expensive multiphase flow simulations
involving partial differential equation solves. By means of carefully selected
experiments, centered around the problem of geological carbon storage, we
demonstrate the efficacy of the proposed constrained optimization method on two
different data modalities, namely time-lapse well and time-lapse seismic data.
While permeability inversions from both these two modalities have their pluses
and minuses, their joint inversion benefits from either, yielding valuable
superior permeability inversions and CO2 plume predictions near, and far away,
from the monitoring wells.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Orozco_R/0/1/0/all/0/1&quot;&gt;Rafael Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Louboutin_M/0/1/0/all/0/1&quot;&gt;Mathias Louboutin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Herrmann_F/0/1/0/all/0/1&quot;&gt;Felix J. Herrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11105">
<title>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games. (arXiv:2307.11105v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.11105</link>
<description rdf:parseType="Literal">&lt;p&gt;Going from research to production, especially for large and complex software
systems, is fundamentally a hard problem. In large-scale game production, one
of the main reasons is that the development environment can be very different
from the final product. In this technical paper we describe an effort to add an
experimental reinforcement learning system to an existing automated game
testing solution based on scripted bots in order to increase its capacity. We
report on how this reinforcement learning system was integrated with the aim to
increase test coverage similar to [1] in a set of AAA games including
Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to
show a use-case of leveraging reinforcement learning in game production and
cover some of the largest time sinks anyone who wants to make the same journey
for their game may encounter. Furthermore, to help the game industry to adopt
this technology faster, we propose a few research directions that we believe
will be valuable and necessary for making machine learning, and especially
reinforcement learning, an effective tool in game production.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillberg_J/0/1/0/all/0/1&quot;&gt;Jonas Gillberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergdahl_J/0/1/0/all/0/1&quot;&gt;Joakim Bergdahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sestini_A/0/1/0/all/0/1&quot;&gt;Alessandro Sestini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eakins_A/0/1/0/all/0/1&quot;&gt;Andrew Eakins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gisslen_L/0/1/0/all/0/1&quot;&gt;Linus Gisslen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11106">
<title>The importance of feature preprocessing for differentially private linear optimization. (arXiv:2307.11106v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11106</link>
<description rdf:parseType="Literal">&lt;p&gt;Training machine learning models with differential privacy (DP) has received
increasing interest in recent years. One of the most popular algorithms for
training differentially private models is differentially private stochastic
gradient descent (DPSGD) and its variants, where at each step gradients are
clipped and combined with some noise. Given the increasing usage of DPSGD, we
ask the question: is DPSGD alone sufficient to find a good minimizer for every
dataset under privacy constraints? As a first step towards answering this
question, we show that even for the simple case of linear classification,
unlike non-private optimization, (private) feature preprocessing is vital for
differentially private optimization. In detail, we first show theoretically
that there exists an example where without feature preprocessing, DPSGD incurs
a privacy error proportional to the maximum norm of features over all samples.
We then propose an algorithm called DPSGD-F, which combines DPSGD with feature
preprocessing and prove that for classification tasks, it incurs a privacy
error proportional to the diameter of the features $\max_{x, x&apos; \in D} \|x -
x&apos;\|_2$. We then demonstrate the practicality of our algorithm on image
classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Ziteng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1&quot;&gt;Ananda Theertha Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1&quot;&gt;Aditya Krishna Menon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11108">
<title>Flatness-Aware Minimization for Domain Generalization. (arXiv:2307.11108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11108</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) seeks to learn robust models that generalize well
under unknown distribution shifts. As a critical aspect of DG, optimizer
selection has not been explored in depth. Currently, most DG methods follow the
widely used benchmark, DomainBed, and utilize Adam as the default optimizer for
all datasets. However, we reveal that Adam is not necessarily the optimal
choice for the majority of current DG methods and datasets. Based on the
perspective of loss landscape flatness, we propose a novel approach,
Flatness-Aware Minimization for Domain Generalization (FAD), which can
efficiently optimize both zeroth-order and first-order flatness simultaneously
for DG. We provide theoretical analyses of the FAD&apos;s out-of-distribution (OOD)
generalization error and convergence. Our experimental results demonstrate the
superiority of FAD on various DG datasets. Additionally, we confirm that FAD is
capable of discovering flatter optima in comparison to other zeroth-order and
first-order flatness-aware optimization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renzhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yancheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Pengfei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cu_P/0/1/0/all/0/1&quot;&gt;Peng Cu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11112">
<title>Comparison between transformers and convolutional models for fine-grained classification of insects. (arXiv:2307.11112v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11112</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained classification is challenging due to the difficulty of finding
discriminatory features. This problem is exacerbated when applied to
identifying species within the same taxonomical class. This is because species
are often sharing morphological characteristics that make them difficult to
differentiate. We consider the taxonomical class of Insecta. The identification
of insects is essential in biodiversity monitoring as they are one of the
inhabitants at the base of many ecosystems. Citizen science is doing brilliant
work of collecting images of insects in the wild giving the possibility to
experts to create improved distribution maps in all countries. We have billions
of images that need to be automatically classified and deep neural network
algorithms are one of the main techniques explored for fine-grained tasks. At
the SOTA, the field of deep learning algorithms is extremely fruitful, so how
to identify the algorithm to use? We focus on Odonata and Coleoptera orders,
and we propose an initial comparative study to analyse the two best-known layer
structures for computer vision: transformer and convolutional layers. We
compare the performance of T2TViT, a fully transformer-base, EfficientNet, a
fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of
the three models in identical conditions evaluating the performance per
species, per morph together with sex, the inference time, and the overall
performance with unbalanced datasets of images from smartphones. Although we
observe high performances with all three families of models, our analysis shows
that the hybrid model outperforms the fully convolutional-base and fully
transformer-base models on accuracy performance and the fully transformer-base
model outperforms the others on inference speed and, these prove the
transformer to be robust to the shortage of samples and to be faster at
inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1&quot;&gt;Rita Pucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalkman_V/0/1/0/all/0/1&quot;&gt;Vincent J. Kalkman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stowell_D/0/1/0/all/0/1&quot;&gt;Dan Stowell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11122">
<title>Diffusion Models for Probabilistic Deconvolution of Galaxy Images. (arXiv:2307.11122v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2307.11122</link>
<description rdf:parseType="Literal">&lt;p&gt;Telescopes capture images with a particular point spread function (PSF).
Inferring what an image would have looked like with a much sharper PSF, a
problem known as PSF deconvolution, is ill-posed because PSF convolution is not
an invertible transformation. Deep generative models are appealing for PSF
deconvolution because they can infer a posterior distribution over candidate
images that, if convolved with the PSF, could have generated the observation.
However, classical deep generative models such as VAEs and GANs often provide
inadequate sample diversity. As an alternative, we propose a classifier-free
conditional diffusion model for PSF deconvolution of galaxy images. We
demonstrate that this diffusion model captures a greater diversity of possible
deconvolutions compared to a conditional VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Patel_Y/0/1/0/all/0/1&quot;&gt;Yash Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Regier_J/0/1/0/all/0/1&quot;&gt;Jeffrey Regier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11126">
<title>A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia. (arXiv:2307.11126v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2307.11126</link>
<description rdf:parseType="Literal">&lt;p&gt;Malnutrition and dehydration are strongly associated with increased cognitive
and functional decline in people living with dementia (PLWD), as well as an
increased rate of hospitalisations in comparison to their healthy counterparts.
Extreme changes in eating and drinking behaviours can often lead to
malnutrition and dehydration, accelerating the progression of cognitive and
functional decline and resulting in a marked reduction in quality of life.
Unfortunately, there are currently no established methods by which to
objectively detect such changes. Here, we present the findings of an extensive
quantitative analysis conducted on in-home monitoring data collected from 73
households of PLWD using Internet of Things technologies. The Coronavirus 2019
(COVID-19) pandemic has previously been shown to have dramatically altered the
behavioural habits, particularly the eating and drinking habits, of PLWD. Using
the COVID-19 pandemic as a natural experiment, we conducted linear
mixed-effects modelling to examine changes in mean kitchen activity within a
subset of 21 households of PLWD that were continuously monitored for 499 days.
We report an observable increase in day-time kitchen activity and a significant
decrease in night-time kitchen activity (t(147) = -2.90, p &amp;lt; 0.001). We further
propose a novel analytical approach to detecting changes in behaviours of PLWD
using Markov modelling applied to remote monitoring data as a proxy for
behaviours that cannot be directly measured. Together, these results pave the
way to introduce improvements into the monitoring of PLWD in naturalistic
settings and for shifting from reactive to proactive care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fletcher_Lloyd_N/0/1/0/all/0/1&quot;&gt;Nan Fletcher-Lloyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Serban_A/0/1/0/all/0/1&quot;&gt;Alina-Irina Serban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolanko_M/0/1/0/all/0/1&quot;&gt;Magdalena Kolanko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wingfield_D/0/1/0/all/0/1&quot;&gt;David Wingfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;Danielle Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nilforooshan_R/0/1/0/all/0/1&quot;&gt;Ramin Nilforooshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Barnaghi_P/0/1/0/all/0/1&quot;&gt;Payam Barnaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Soreq_E/0/1/0/all/0/1&quot;&gt;Eyal Soreq&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11127">
<title>Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])</title>
<link>http://arxiv.org/abs/2307.11127</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic control methods (SCMs) have become a crucial tool for causal
inference in comparative case studies. The fundamental idea of SCMs is to
estimate counterfactual outcomes for a treated unit by using a weighted sum of
observed outcomes from untreated units. The accuracy of the synthetic control
(SC) is critical for estimating the causal effect, and hence, the estimation of
SC weights has been the focus of much research. In this paper, we first point
out that existing SCMs suffer from an implicit endogeneity problem, which is
the correlation between the outcomes of untreated units and the error term in
the model of a counterfactual outcome. We show that this problem yields a bias
in the causal effect estimator. We then propose a novel SCM based on density
matching, assuming that the density of outcomes of the treated unit can be
approximated by a weighted average of the densities of untreated units (i.e., a
mixture model). Based on this assumption, we estimate SC weights by matching
moments of treated outcomes and the weighted sum of moments of untreated
outcomes. Our proposed method has three advantages over existing methods.
First, our estimator is asymptotically unbiased under the assumption of the
mixture model. Second, due to the asymptotic unbiasedness, we can reduce the
mean squared error for counterfactual prediction. Third, our method generates
full densities of the treatment effect, not only expected values, which
broadens the applicability of SCMs. We provide experimental results to
demonstrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Masahiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Ohda_A/0/1/0/all/0/1&quot;&gt;Akari Ohda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Imaizumi_M/0/1/0/all/0/1&quot;&gt;Masaaki Imaizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+McAlinn_K/0/1/0/all/0/1&quot;&gt;Kenichiro McAlinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11133">
<title>Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.11133</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional magnetic resonance imaging (fMRI) is a commonly used technique to
measure neural activation. Its application has been particularly important in
identifying underlying neurodegenerative conditions such as Parkinson&apos;s,
Alzheimer&apos;s, and Autism. Recent analysis of fMRI data models the brain as a
graph and extracts features by graph neural networks (GNNs). However, the
unique characteristics of fMRI data require a special design of GNN. Tailoring
GNN to generate effective and domain-explainable features remains challenging.
In this paper, we propose a contrastive dual-attention block and a
differentiable graph pooling method called ContrastPool to better utilize GNN
for brain networks, meeting fMRI-specific requirements. We apply our method to
5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its
superiority over state-of-the-art baselines. Our case study confirms that the
patterns extracted by our method match the domain knowledge in neuroscience
literature, and disclose direct and interesting insights. Our contributions
underscore the potential of ContrastPool for advancing the understanding of
brain networks and neurodegenerative conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bian_Q/0/1/0/all/0/1&quot;&gt;Qingtian Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aihu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ke_Y/0/1/0/all/0/1&quot;&gt;Yiping Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Miao Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sim_W/0/1/0/all/0/1&quot;&gt;Wei Khang Jeremy Sim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gulyas_B/0/1/0/all/0/1&quot;&gt;Bal&amp;#xe1;zs Guly&amp;#xe1;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11166">
<title>Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11166</link>
<description rdf:parseType="Literal">&lt;p&gt;We leverage the fast physics simulator, MuJoCo to run tasks in a continuous
control environment and reveal details like the observation space, action
space, rewards, etc. for each task. We benchmark value-based methods for
continuous control by comparing Q-learning and SARSA through a discretization
approach, and using them as baselines, progressively moving into one of the
state-of-the-art deep policy gradient method DDPG. Over a large number of
episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small
number of episodes. Lastly, we also fine-tuned the model hyper-parameters
expecting to squeeze more performance but using lesser time and resources. We
anticipated that the new design for DDPG would vastly improve performance, yet
after only a few episodes, we were able to achieve decent average rewards. We
expect to improve the performance provided adequate time and computational
resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahul_V/0/1/0/all/0/1&quot;&gt;Vaddadi Sai Rahul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Debajyoti Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11197">
<title>Heuristic Hyperparameter Choice for Image Anomaly Detection. (arXiv:2307.11197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11197</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) in images is a fundamental computer vision problem by
deep learning neural network to identify images deviating significantly from
normality. The deep features extracted from pretrained models have been proved
to be essential for AD based on multivariate Gaussian distribution analysis.
However, since models are usually pretrained on a large dataset for
classification tasks such as ImageNet, they might produce lots of redundant
features for AD, which increases computational cost and degrades the
performance. We aim to do the dimension reduction of Negated Principal
Component Analysis (NPCA) for these features. So we proposed some heuristic to
choose hyperparameter of NPCA algorithm for getting as fewer components of
features as possible while ensuring a good performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o P. C. Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decenciere_E/0/1/0/all/0/1&quot;&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11209">
<title>Clinical Trial Active Learning. (arXiv:2307.11209v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11209</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach to active learning that takes into
account the non-independent and identically distributed (non-i.i.d.) structure
of a clinical trial setting. There exists two types of clinical trials:
retrospective and prospective. Retrospective clinical trials analyze data after
treatment has been performed; prospective clinical trials collect data as
treatment is ongoing. Typically, active learning approaches assume the dataset
is i.i.d. when selecting training samples; however, in the case of clinical
trials, treatment results in a dependency between the data collected at the
current and past visits. Thus, we propose prospective active learning to
overcome the limitations present in traditional active learning methods and
apply it to disease detection in optical coherence tomography (OCT) images,
where we condition on the time an image was collected to enforce the i.i.d.
assumption. We compare our proposed method to the traditional active learning
paradigm, which we refer to as retrospective in nature. We demonstrate that
prospective active learning outperforms retrospective active learning in two
different types of test settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fowler_Z/0/1/0/all/0/1&quot;&gt;Zoe Fowler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokilepersaud_K/0/1/0/all/0/1&quot;&gt;Kiran Kokilepersaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1&quot;&gt;Mohit Prabhushankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1&quot;&gt;Ghassan AlRegib&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11211">
<title>The Effect of Epidemiological Cohort Creation on the Machine Learning Prediction of Homelessness and Police Interaction Outcomes Using Administrative Health Care Data. (arXiv:2307.11211v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11211</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Mental illness can lead to adverse outcomes such as homelessness
and police interaction and understanding of the events leading up to these
adverse outcomes is important. Predictive models may help identify individuals
at risk of such adverse outcomes. Using a fixed observation window cohort with
logistic regression (LR) or machine learning (ML) models can result in lower
performance when compared with adaptive and parcellated windows. Method: An
administrative healthcare dataset was used, comprising of 240,219 individuals
in Calgary, Alberta, Canada who were diagnosed with addiction or mental health
(AMH) between April 1, 2013, and March 31, 2018. The cohort was followed for 2
years to identify factors associated with homelessness and police interactions.
To understand the benefit of flexible windows to predictive models, an
alternative cohort was created. Then LR and ML models, including random forests
(RF), and extreme gradient boosting (XGBoost) were compared in the two cohorts.
Results: Among 237,602 individuals, 0.8% (1,800) experienced first
homelessness, while 0.32% (759) reported initial police interaction among
237,141 individuals. Male sex (AORs: H=1.51, P=2.52), substance disorder (AORs:
H=3.70, P=2.83), psychiatrist visits (AORs: H=1.44, P=1.49), and drug abuse
(AORs: H=2.67, P=1.83) were associated with initial homelessness (H) and police
interaction (P). XGBoost showed superior performance using the flexible method
(sensitivity =91%, AUC =90% for initial homelessness, and sensitivity =90%,
AUC=89% for initial police interaction)
&lt;/p&gt;
&lt;p&gt;Conclusion: This study identified key features associated with initial
homelessness and police interaction and demonstrated that flexible windows can
improve predictive modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahidi_F/0/1/0/all/0/1&quot;&gt;Faezehsadat Shahidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDonald_M/0/1/0/all/0/1&quot;&gt;M. Ethan MacDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seitz_D/0/1/0/all/0/1&quot;&gt;Dallas Seitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messier_G/0/1/0/all/0/1&quot;&gt;Geoffrey Messier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11214">
<title>FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation. (arXiv:2307.11214v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11214</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating realistic human flows across regions is essential for our
understanding of urban structures and population activity patterns, enabling
important applications in the fields of urban planning and management. However,
a notable shortcoming of most existing mobility generation methodologies is
neglect of prediction fairness, which can result in underestimation of mobility
flows across regions with vulnerable population groups, potentially resulting
in inequitable resource distribution and infrastructure development. To
overcome this limitation, our study presents a novel, fairness-aware deep
learning model, FairMobi-Net, for inter-region human flow prediction. The
FairMobi-Net model uniquely incorporates fairness loss into the loss function
and employs a hybrid approach, merging binary classification and numerical
regression techniques for human flow prediction. We validate the FairMobi-Net
model using comprehensive human mobility datasets from four U.S. cities,
predicting human flow at the census-tract level. Our findings reveal that the
FairMobi-Net model outperforms state-of-the-art models (such as the DeepGravity
model) in producing more accurate and equitable human flow predictions across a
variety of region pairs, regardless of regional income differences. The model
maintains a high degree of accuracy consistently across diverse regions,
addressing the previous fairness concern. Further analysis of feature
importance elucidates the impact of physical distances and road network
structures on human flows across regions. With fairness as its touchstone, the
model and results provide researchers and practitioners across the fields of
urban sciences, transportation engineering, and computing with an effective
tool for accurate generation of human mobility flows across regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhewei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lipai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostafavi_A/0/1/0/all/0/1&quot;&gt;Ali Mostafavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11224">
<title>Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.11224</link>
<description rdf:parseType="Literal">&lt;p&gt;Jina Embeddings constitutes a set of high-performance sentence embedding
models adept at translating various textual inputs into numerical
representations, thereby capturing the semantic essence of the text. While
these models are not exclusively designed for text generation, they excel in
applications such as dense retrieval and semantic textual similarity. This
paper details the development of Jina Embeddings, starting with the creation of
a high-quality pairwise and triplet dataset. It underlines the crucial role of
data cleaning in dataset preparation, gives in-depth insights into the model
training process, and concludes with a comprehensive performance evaluation
using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunther_M/0/1/0/all/0/1&quot;&gt;Michael G&amp;#xfc;nther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milliken_L/0/1/0/all/0/1&quot;&gt;Louis Milliken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geuter_J/0/1/0/all/0/1&quot;&gt;Jonathan Geuter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastrapas_G/0/1/0/all/0/1&quot;&gt;Georgios Mastrapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Han Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11228">
<title>From Adaptive Query Release to Machine Unlearning. (arXiv:2307.11228v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11228</link>
<description rdf:parseType="Literal">&lt;p&gt;We formalize the problem of machine unlearning as design of efficient
unlearning algorithms corresponding to learning algorithms which perform a
selection of adaptive queries from structured query classes. We give efficient
unlearning algorithms for linear and prefix-sum query classes. As applications,
we show that unlearning in many problems, in particular, stochastic convex
optimization (SCO), can be reduced to the above, yielding improved guarantees
for the problem. In particular, for smooth Lipschitz losses and any $\rho&amp;gt;0$,
our results yield an unlearning algorithm with excess population risk of
$\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$ with unlearning
query (gradient) complexity $\tilde O(\rho \cdot \text{Retraining
Complexity})$, where $d$ is the model dimensionality and $n$ is the initial
number of samples. For non-smooth Lipschitz losses, we give an unlearning
algorithm with excess population risk $\tilde
O\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}\big)^{1/2}\big)$ with the
same unlearning query (gradient) complexity. Furthermore, in the special case
of Generalized Linear Models (GLMs), such as those in linear and logistic
regression, we get dimension-independent rates of $\tilde
O\big(\frac{1}{\sqrt{n}} +\frac{1}{(n\rho)^{2/3}}\big)$ and $\tilde
O\big(\frac{1}{\sqrt{n}} +\frac{1}{(n\rho)^{1/3}}\big)$ for smooth Lipschitz
and non-smooth Lipschitz losses respectively. Finally, we give generalizations
of the above from one unlearning request to \textit{dynamic} streams consisting
of insertions and deletions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1&quot;&gt;Enayat Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11234">
<title>QDC: Quantum Diffusion Convolution Kernels on Graphs. (arXiv:2307.11234v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11234</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional neural networks (GCNs) operate by aggregating messages
over local neighborhoods given the prediction task under interest. Many GCNs
can be understood as a form of generalized diffusion of input features on the
graph, and significant work has been dedicated to improving predictive accuracy
by altering the ways of message passing. In this work, we propose a new
convolution kernel that effectively rewires the graph according to the
occupation correlations of the vertices by trading on the generalized diffusion
paradigm for the propagation of a quantum particle over the graph. We term this
new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In
addition, we introduce a multiscale variant that combines messages from the QDC
operator and the traditional combinatorial Laplacian. To understand our method,
we explore the spectral dependence of homophily and the importance of quantum
dynamics in the construction of a bandpass filter. Through these studies, as
well as experiments on a range of datasets, we observe that QDC improves
predictive performance on the widely used benchmark datasets when compared to
similar methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markovich_T/0/1/0/all/0/1&quot;&gt;Thomas Markovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11239">
<title>Edgewise outliers of network indexed signals. (arXiv:2307.11239v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.11239</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider models for network indexed multivariate data involving a
dependence between variables as well as across graph nodes.
&lt;/p&gt;
&lt;p&gt;In the framework of these models, we focus on outliers detection and
introduce the concept of edgewise outliers. For this purpose, we first derive
the distribution of some sums of squares, in particular squared Mahalanobis
distances that can be used to fix detection rules and thresholds for outlier
detection. We then propose a robust version of the deterministic MCD algorithm
that we call edgewise MCD. An application on simulated data shows the interest
of taking the dependence structure into account. We also illustrate the utility
of the proposed method with a real data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rieser_C/0/1/0/all/0/1&quot;&gt;Christopher Rieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ruiz_Gazen_A/0/1/0/all/0/1&quot;&gt;Anne Ruiz-Gazen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thomas_Agnan_C/0/1/0/all/0/1&quot;&gt;Christine Thomas-Agnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11242">
<title>On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments. (arXiv:2307.11242v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.11242</link>
<description rdf:parseType="Literal">&lt;p&gt;This work describes the investigation of neuromorphic computing-based spiking
neural network (SNN) models used to filter data from sensor electronics in high
energy physics experiments conducted at the High Luminosity Large Hadron
Collider. We present our approach for developing a compact neuromorphic model
that filters out the sensor data based on the particle&apos;s transverse momentum
with the goal of reducing the amount of data being sent to the downstream
electronics. The incoming charge waveforms are converted to streams of
binary-valued events, which are then processed by the SNN. We present our
insights on the various system design choices - from data encoding to optimal
hyperparameters of the training algorithm - for an accurate and compact SNN
optimized for hardware deployment. Our results show that an SNN trained with an
evolutionary algorithm and an optimized set of hyperparameters obtains a signal
efficiency of about 91% with nearly half as many parameters as a deep neural
network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1&quot;&gt;Shruti R. Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1&quot;&gt;Aaron Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Date_P/0/1/0/all/0/1&quot;&gt;Prasanna Date&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miniskar_N/0/1/0/all/0/1&quot;&gt;Narasinga Rao Miniskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetter_J/0/1/0/all/0/1&quot;&gt;Jeffrey S. Vetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahim_F/0/1/0/all/0/1&quot;&gt;Farah Fahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parpillon_B/0/1/0/all/0/1&quot;&gt;Benjamin Parpillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dickinson_J/0/1/0/all/0/1&quot;&gt;Jennet Dickinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jieun Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_C/0/1/0/all/0/1&quot;&gt;Corrinne Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swartz_M/0/1/0/all/0/1&quot;&gt;Morris Swartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maksimovic_P/0/1/0/all/0/1&quot;&gt;Petar Maksimovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuman_C/0/1/0/all/0/1&quot;&gt;Catherine D. Schuman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bean_A/0/1/0/all/0/1&quot;&gt;Alice Bean&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11249">
<title>On the Fisher-Rao Gradient of the Evidence Lower Bound. (arXiv:2307.11249v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11249</link>
<description rdf:parseType="Literal">&lt;p&gt;This article studies the Fisher-Rao gradient, also referred to as the natural
gradient, of the evidence lower bound, the ELBO, which plays a crucial role
within the theory of the Variational Autonecoder, the Helmholtz Machine and the
Free Energy Principle. The natural gradient of the ELBO is related to the
natural gradient of the Kullback-Leibler divergence from a target distribution,
the prime objective function of learning. Based on invariance properties of
gradients within information geometry, conditions on the underlying model are
provided that ensure the equivalence of minimising the prime objective function
and the maximisation of the ELBO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ay_N/0/1/0/all/0/1&quot;&gt;Nihat Ay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostrum_J/0/1/0/all/0/1&quot;&gt;Jesse van Oostrum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11274">
<title>Screening Mammography Breast Cancer Detection. (arXiv:2307.11274v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11274</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer is a leading cause of cancer-related deaths, but current
programs are expensive and prone to false positives, leading to unnecessary
follow-up and patient anxiety. This paper proposes a solution to automated
breast cancer detection, to improve the efficiency and accuracy of screening
programs. Different methodologies were tested against the RSNA dataset of
radiographic breast images of roughly 20,000 female patients and yielded an
average validation case pF1 score of 0.56 across methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Debajyoti Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11280">
<title>Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11280</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Epsilon*, a new privacy metric for measuring the privacy risk of
a single model instance prior to, during, or after deployment of privacy
mitigation strategies. The metric does not require access to the training data
sampling or model training algorithm. Epsilon* is a function of true positive
and false positive rates in a hypothesis test used by an adversary in a
membership inference attack. We distinguish between quantifying the privacy
loss of a trained model instance and quantifying the privacy loss of the
training mechanism which produces this model instance. Existing approaches in
the privacy auditing literature provide lower bounds for the latter, while our
metric provides a lower bound for the former by relying on an
(${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained
model instance. We establish a relationship between these lower bounds and show
how to implement Epsilon* to avoid numerical and noise amplification
instability. We further show in experiments on benchmark public data sets that
Epsilon* is sensitive to privacy risk mitigation by training with differential
privacy (DP), where the value of Epsilon* is reduced by up to 800% compared to
the Epsilon* values of non-DP trained baseline models. This metric allows
privacy auditors to be independent of model owners, and enables all
decision-makers to visualize the privacy-utility landscape to make informed
decisions regarding the trade-offs between model privacy and utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negoescu_D/0/1/0/all/0/1&quot;&gt;Diana M. Negoescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_H/0/1/0/all/0/1&quot;&gt;Humberto Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orjany_S/0/1/0/all/0/1&quot;&gt;Saad Eddin Al Orjany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jilei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lut_Y/0/1/0/all/0/1&quot;&gt;Yuliia Lut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandra_R/0/1/0/all/0/1&quot;&gt;Rahul Tandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xinyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douglas_Z/0/1/0/all/0/1&quot;&gt;Zach Douglas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolkha_V/0/1/0/all/0/1&quot;&gt;Vidita Nolkha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahammad_P/0/1/0/all/0/1&quot;&gt;Parvez Ahammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samorodnitsky_G/0/1/0/all/0/1&quot;&gt;Gennady Samorodnitsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11285">
<title>MAS: Towards Resource-Efficient Federated Multiple-Task Learning. (arXiv:2307.11285v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11285</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous FL tasks could overload resource-constrained devices. In
this work, we propose the first FL system to effectively coordinate and train
multiple simultaneous FL tasks. We first formalize the problem of training
simultaneous FL tasks. Then, we present our new approach, MAS (Merge and
Split), to optimize the performance of training multiple simultaneous FL tasks.
MAS starts by merging FL tasks into an all-in-one FL task with a multi-task
architecture. After training for a few rounds, MAS splits the all-in-one FL
task into two or more FL tasks by using the affinities among tasks measured
during the all-in-one training. It then continues training each split of FL
tasks based on model parameters from the all-in-one training. Extensive
experiments demonstrate that MAS outperforms other methods while reducing
training time by 2x and reducing energy consumption by 40%. We hope this work
will inspire the community to further study and optimize training simultaneous
FL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yonggang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11288">
<title>Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11288</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference-based feedback is important for many applications where direct
evaluation of a reward function is not feasible. A notable recent example
arises in reinforcement learning from human feedback on large language models.
For many of these applications, the cost of acquiring the human feedback can be
substantial or even prohibitive. In this work, we take advantage of the fact
that often the agent can choose contexts at which to obtain human feedback in
order to most efficiently identify a good policy, and introduce the offline
contextual dueling bandit setting. We give an upper-confidence-bound style
algorithm for this setting and prove a regret bound. We also give empirical
confirmation that this method outperforms a similar strategy that uses
uniformly sampled contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1&quot;&gt;Viraj Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neopane_O/0/1/0/all/0/1&quot;&gt;Ojash Neopane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_V/0/1/0/all/0/1&quot;&gt;Vikramjeet Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Sen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11289">
<title>PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations. (arXiv:2307.11289v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11289</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new category of physics-informed neural networks called physics
informed variational embedding generative adversarial network (PI-VEGAN), that
effectively tackles the forward, inverse, and mixed problems of stochastic
differential equations. In these scenarios, the governing equations are known,
but only a limited number of sensor measurements of the system parameters are
available. We integrate the governing physical laws into PI-VEGAN with
automatic differentiation, while introducing a variational encoder for
approximating the latent variables of the actual distribution of the
measurements. These latent variables are integrated into the generator to
facilitate accurate learning of the characteristics of the stochastic partial
equations. Our model consists of three components, namely the encoder,
generator, and discriminator, each of which is updated alternatively employing
the stochastic gradient descent algorithm. We evaluate the effectiveness of
PI-VEGAN in addressing forward, inverse, and mixed problems that require the
concurrent calculation of system parameters and solutions. Numerical results
demonstrate that the proposed method achieves satisfactory stability and
accuracy in comparison with the previous physics-informed generative
adversarial network (PI-WGAN).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruisong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chuanjun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11314">
<title>Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline. (arXiv:2307.11314v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.11314</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) are bio-plausible computing models with high
energy efficiency. The temporal dynamics of neurons and synapses enable them to
detect temporal patterns and generate sequences. While Backpropagation Through
Time (BPTT) is traditionally used to train SNNs, it is not suitable for online
learning of embedded applications due to its high computation and memory cost
as well as extended latency. Previous works have proposed online learning
algorithms, but they often utilize highly simplified spiking neuron models
without synaptic dynamics and reset feedback, resulting in subpar performance.
In this work, we present Spatiotemporal Online Learning for Synaptic Adaptation
(SOLSA), specifically designed for online learning of SNNs composed of Leaky
Integrate and Fire (LIF) neurons with exponentially decayed synapses and soft
reset. The algorithm not only learns the synaptic weight but also adapts the
temporal filters associated to the synapses. Compared to the BPTT algorithm,
SOLSA has much lower memory requirement and achieves a more balanced temporal
workload distribution. Moreover, SOLSA incorporates enhancement techniques such
as scheduled weight update, early stop training and adaptive synapse filter,
which speed up the convergence and enhance the learning performance. When
compared to other non-BPTT based SNN learning, SOLSA demonstrates an average
learning accuracy improvement of 14.2%. Furthermore, compared to BPTT, SOLSA
achieves a 5% higher average learning accuracy with a 72% reduction in memory
cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jingang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Haowen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11316">
<title>Making Pre-trained Language Models both Task-solvers and Self-calibrators. (arXiv:2307.11316v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.11316</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained language models (PLMs) serve as backbones for various real-world
systems. For high-stake applications, it&apos;s equally essential to have reasonable
confidence estimations in predictions. While the vanilla confidence scores of
PLMs can already be effectively utilized, PLMs consistently become
overconfident in their wrong predictions, which is not desirable in practice.
Previous work shows that introducing an extra calibration task can mitigate
this issue. The basic idea involves acquiring additional data to train models
in predicting the confidence of their initial predictions. However, it only
demonstrates the feasibility of this kind of method, assuming that there are
abundant extra available samples for the introduced calibration task. In this
work, we consider the practical scenario that we need to effectively utilize
training samples to make PLMs both task-solvers and self-calibrators. Three
challenges are presented, including limited training samples, data imbalance,
and distribution shifts. We first conduct pilot experiments to quantify various
decisive factors in the calibration task. Based on the empirical analysis
results, we propose a training algorithm LM-TOAST to tackle the challenges.
Experimental results show that LM-TOAST can effectively utilize the training
data to make PLMs have reasonable confidence estimations while maintaining the
original task performance. Further, we consider three downstream applications,
namely selective classification, adversarial defense, and model cascading, to
show the practical usefulness of LM-TOAST. The code will be made public at
\url{https://github.com/Yangyi-Chen/LM-TOAST}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11317">
<title>XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge. (arXiv:2307.11317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11317</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming Linear Discriminant Analysis (LDA) while proven in
Class-incremental Learning deployments at the edge with limited classes (upto
1000), has not been proven for deployment in extreme classification scenarios.
In this paper, we present: (a) XLDA, a framework for Class-IL in edge
deployment where LDA classifier is proven to be equivalent to FC layer
including in extreme classification scenarios, and (b) optimizations to enable
XLDA-based training and inference for edge deployment where there is a
constraint on available compute resources. We show up to 42x speed up using a
batched training approach and up to 5x inference speedup with nearest neighbor
search on extreme datasets like AliProducts (50k classes) and Google Landmarks
V2 (81k classes)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1&quot;&gt;Karan Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veerendranath_V/0/1/0/all/0/1&quot;&gt;Vishruth Veerendranath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebbar_A/0/1/0/all/0/1&quot;&gt;Anushka Hebbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1&quot;&gt;Raghavendra Bhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11325">
<title>Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives. (arXiv:2307.11325v1 [q-bio.PE])</title>
<link>http://arxiv.org/abs/2307.11325</link>
<description rdf:parseType="Literal">&lt;p&gt;The interaction between elephants and their environment has profound
implications for both ecology and conservation strategies. This study presents
an analytical approach to decipher the intricate patterns of elephant movement
in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal
variations and rainfall patterns. Despite the complexities surrounding these
influential factors, our analysis provides a holistic view of elephant
migratory behavior in the context of the dynamic African landscape. Our
comprehensive approach enables us to predict the potential impact of these
ecological determinants on elephant migration, a critical step in establishing
informed conservation strategies. This projection is particularly crucial given
the impacts of global climate change on seasonal and rainfall patterns, which
could substantially influence elephant movements in the future. The findings of
our work aim to not only advance the understanding of movement ecology but also
foster a sustainable coexistence of humans and elephants in Sub-Saharan Africa.
By predicting potential elephant routes, our work can inform strategies to
minimize human-elephant conflict, effectively manage land use, and enhance
anti-poaching efforts. This research underscores the importance of integrating
movement ecology and climatic variables for effective wildlife management and
conservation planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hines_M/0/1/0/all/0/1&quot;&gt;Matthew Hines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Glatzer_G/0/1/0/all/0/1&quot;&gt;Gregory Glatzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shreya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Prasenjit Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11327">
<title>Systematic Adaptation of Communication-focused Machine Learning Models from Real to Virtual Environments for Human-Robot Collaboration. (arXiv:2307.11327v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.11327</link>
<description rdf:parseType="Literal">&lt;p&gt;Virtual reality has proved to be useful in applications in several fields
ranging from gaming, medicine, and training to development of interfaces that
enable human-robot collaboration. It empowers designers to explore applications
outside of the constraints posed by the real world environment and develop
innovative solutions and experiences. Hand gestures recognition which has been
a topic of much research and subsequent commercialization in the real world has
been possible because of the creation of large, labelled datasets. In order to
utilize the power of natural and intuitive hand gestures in the virtual domain
for enabling embodied teleoperation of collaborative robots, similarly large
datasets must be created so as to keep the working interface easy to learn and
flexible enough to add more gestures. Depending on the application, this may be
computationally or economically prohibitive. Thus, the adaptation of trained
deep learning models that perform well in the real environment to the virtual
may be a solution to this challenge. This paper presents a systematic framework
for the real to virtual adaptation using limited size of virtual dataset along
with guidelines for creating a curated dataset. Finally, while hand gestures
have been considered as the communication mode, the guidelines and
recommendations presented are generic. These are applicable to other modes such
as body poses and facial expressions which have large datasets available in the
real domain which must be adapted to the virtual one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1&quot;&gt;Debasmita Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhai_R/0/1/0/all/0/1&quot;&gt;Ritwik Singhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11332">
<title>Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models. (arXiv:2307.11332v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11332</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) and deep learning models are extensively used for
parameter optimization and regression problems. However, not all inverse
problems in ML are ``identifiable,&apos;&apos; indicating that model parameters may not
be uniquely determined from the available data and the data model&apos;s
input-output relationship. In this study, we investigate the notion of model
parameter identifiability through a case study focused on parameter estimation
from motion sensor data. Utilizing a bipedal-spring mass human walk dynamics
model, we generate synthetic data representing diverse gait patterns and
conditions. Employing a deep neural network, we attempt to estimate
subject-wise parameters, including mass, stiffness, and equilibrium leg length.
The results show that while certain parameters can be identified from the
observation data, others remain unidentifiable, highlighting that
unidentifiability is an intrinsic limitation of the experimental setup,
necessitating a change in data collection and experimental scenarios. Beyond
this specific case study, the concept of identifiability has broader
implications in ML and deep learning. Addressing unidentifiability requires
proven identifiable models (with theoretical support), multimodal data fusion
techniques, and advancements in model-based machine learning. Understanding and
resolving unidentifiability challenges will lead to more reliable and accurate
applications across diverse domains, transcending mere model convergence and
enhancing the reliability of machine learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sameni_R/0/1/0/all/0/1&quot;&gt;Reza Sameni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11333">
<title>Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition. (arXiv:2307.11333v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11333</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an information-theoretic perspective to group
fairness trade-offs in federated learning (FL) with respect to sensitive
attributes, such as gender, race, etc. Existing works mostly focus on either
\emph{global fairness} (overall disparity of the model across all clients) or
\emph{local fairness} (disparity of the model at each individual client),
without always considering their trade-offs. There is a lack of understanding
of the interplay between global and local fairness in FL, and if and when one
implies the other. To address this gap, we leverage a body of work in
information theory called partial information decomposition (PID) which first
identifies three sources of unfairness in FL, namely, \emph{Unique Disparity},
\emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical
examples, we demonstrate how these three disparities contribute to global and
local fairness. This decomposition helps us derive fundamental limits and
trade-offs between global or local fairness, particularly under data
heterogeneity, as well as, derive conditions under which one implies the other.
We also present experimental results on benchmark datasets to support our
theoretical findings. This work offers a more nuanced understanding of the
sources of disparity in FL that can inform the use of local disparity
mitigation techniques, and their convergence and effectiveness when deployed in
practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamman_F/0/1/0/all/0/1&quot;&gt;Faisal Hamman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sanghamitra Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11334">
<title>Improving Transferability of Adversarial Examples via Bayesian Attacks. (arXiv:2307.11334v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11334</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a substantial extension of our work published at ICLR.
Our ICLR work advocated for enhancing transferability in adversarial examples
by incorporating a Bayesian formulation into model parameters, which
effectively emulates the ensemble of infinitely many deep neural networks,
while, in this paper, we introduce a novel extension by incorporating the
Bayesian formulation into the model input as well, enabling the joint
diversification of both the model input and model parameters. Our empirical
findings demonstrate that: 1) the combination of Bayesian formulations for both
the model input and model parameters yields significant improvements in
transferability; 2) by introducing advanced approximations of the posterior
distribution over the model input, adversarial transferability achieves further
enhancement, surpassing all state-of-the-arts when attacking without model
fine-tuning. Moreover, we propose a principled approach to fine-tune model
parameters in such an extended Bayesian formulation. The derived optimization
objective inherently encourages flat minima in the parameter space and input
space. Extensive experiments demonstrate that our method achieves a new
state-of-the-art on transfer-based attacks, improving the average success rate
on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with
our ICLR basic Bayesian method. We will make our code publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qizhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11351">
<title>Bounded P-values in Parametric Programming-based Selective Inference. (arXiv:2307.11351v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.11351</link>
<description rdf:parseType="Literal">&lt;p&gt;Selective inference (SI) has been actively studied as a promising framework
for statistical hypothesis testing for data-driven hypotheses. The basic idea
of SI is to make inferences conditional on an event that a hypothesis is
selected. In order to perform SI, this event must be characterized in a
traceable form. When selection event is too difficult to characterize,
additional conditions are introduced for tractability. This additional
conditions often causes the loss of power, and this issue is referred to as
over-conditioning. Parametric programming-based SI (PP-based SI) has been
proposed as one way to address the over-conditioning issue. The main problem of
PP-based SI is its high computational cost due to the need to exhaustively
explore the data space. In this study, we introduce a procedure to reduce the
computational cost while guaranteeing the desired precision, by proposing a
method to compute the upper and lower bounds of p-values. We also proposed
three types of search strategies that efficiently improve these bounds. We
demonstrate the effectiveness of the proposed method in hypothesis testing
problems for feature selection in linear models and attention region
identification in deep neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shiraishi_T/0/1/0/all/0/1&quot;&gt;Tomohiro Shiraishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Miwa_D/0/1/0/all/0/1&quot;&gt;Daiki Miwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1&quot;&gt;Vo Nguyen Le Duy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1&quot;&gt;Ichiro Takeuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11352">
<title>Model-based Offline Reinforcement Learning with Count-based Conservatism. (arXiv:2307.11352v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11352</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a model-based offline reinforcement learning method
that integrates count-based conservatism, named $\texttt{Count-MORL}$. Our
method utilizes the count estimates of state-action pairs to quantify model
estimation error, marking the first algorithm of demonstrating the efficacy of
count-based conservatism in model-based offline deep RL to the best of our
knowledge. For our proposed method, we first show that the estimation error is
inversely proportional to the frequency of state-action pairs. Secondly, we
demonstrate that the learned policy under the count-based conservative model
offers near-optimality performance guarantees. Through extensive numerical
experiments, we validate that $\texttt{Count-MORL}$ with hash code
implementation significantly outperforms existing offline RL algorithms on the
D4RL benchmark datasets. The code is accessible at
$\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Byeongchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1&quot;&gt;Min-hwan Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11353">
<title>What can a Single Attention Layer Learn? A Study Through the Random Features Lens. (arXiv:2307.11353v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11353</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention layers -- which map a sequence of inputs to a sequence of outputs
-- are core building blocks of the Transformer architecture which has achieved
significant breakthroughs in modern artificial intelligence. This paper
presents a rigorous theoretical study on the learning and generalization of a
single multi-head attention layer, with a sequence of key vectors and a
separate query vector as input. We consider the random feature setting where
the attention layer has a large number of heads, with randomly sampled frozen
query and key matrices, and trainable value matrices. We show that such a
random-feature attention layer can express a broad class of target functions
that are permutation invariant to the key vectors. We further provide
quantitative excess risk bounds for learning these target functions from finite
samples, using random feature attention with finitely many heads.
&lt;/p&gt;
&lt;p&gt;Our results feature several implications unique to the attention structure
compared with existing random features theory for neural networks, such as (1)
Advantages in the sample complexity over standard two-layer random-feature
networks; (2) Concrete and natural classes of functions that can be learned
efficiently by a random-feature attention layer; and (3) The effect of the
sampling distribution of the query-key weight matrix (the product of the query
and key matrix), where Gaussian random weights with a non-zero mean result in
better sample complexities over the zero-mean counterpart for learning certain
natural target functions. Experiments on simulated data corroborate our
theoretical findings and further illustrate the interplay between the sample
size and the complexity of the target function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hengyu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tianyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Song Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11357">
<title>Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning. (arXiv:2307.11357v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11357</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) has been widely explored in Traffic Signal
Control (TSC) applications, however, still no such system has been deployed in
practice. A key barrier to progress in this area is the reality gap, the
discrepancy that results from differences between simulation models and their
real-world equivalents. In this paper, we address this challenge by first
presenting a comprehensive analysis of potential simulation parameters that
contribute to this reality gap. We then also examine two promising strategies
that can bridge this gap: Domain Randomization (DR) and Model-Agnostic
Meta-Learning (MAML). Both strategies were trained with a traffic simulation
model of an intersection. In addition, the model was embedded in LemgoRL, a
framework that integrates realistic, safety-critical requirements into the
control system. Subsequently, we evaluated the performance of the two methods
on a separate model of the same intersection that was developed with a
different traffic simulator. In this way, we mimic the reality gap. Our
experimental results show that both DR and MAML outperform a state-of-the-art
RL algorithm, therefore highlighting their potential to mitigate the reality
gap in RLbased TSC systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1&quot;&gt;Arthur M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabatelli_M/0/1/0/all/0/1&quot;&gt;Matthia Sabatelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11371">
<title>Random Separating Hyperplane Theorem and Learning Polytopes. (arXiv:2307.11371v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11371</link>
<description rdf:parseType="Literal">&lt;p&gt;The Separating Hyperplane theorem is a fundamental result in Convex Geometry
with myriad applications. Our first result, Random Separating Hyperplane
Theorem (RSH), is a strengthening of this for polytopes. $\rsh$ asserts that if
the distance between $a$ and a polytope $K$ with $k$ vertices and unit diameter
in $\Re^d$ is at least $\delta$, where $\delta$ is a fixed constant in $(0,1)$,
then a randomly chosen hyperplane separates $a$ and $K$ with probability at
least $1/poly(k)$ and margin at least $\Omega \left(\delta/\sqrt{d} \right)$.
An immediate consequence of our result is the first near optimal bound on the
error increase in the reduction from a Separation oracle to an Optimization
oracle over a polytope.
&lt;/p&gt;
&lt;p&gt;RSH has algorithmic applications in learning polytopes. We consider a
fundamental problem, denoted the ``Hausdorff problem&apos;&apos;, of learning a unit
diameter polytope $K$ within Hausdorff distance $\delta$, given an optimization
oracle for $K$. Using RSH, we show that with polynomially many random queries
to the optimization oracle, $K$ can be approximated within error $O(\delta)$.
To our knowledge this is the first provable algorithm for the Hausdorff
Problem. Building on this result, we show that if the vertices of $K$ are
well-separated, then an optimization oracle can be used to generate a list of
points, each within Hausdorff distance $O(\delta)$ of $K$, with the property
that the list contains a point close to each vertex of $K$. Further, we show
how to prune this list to generate a (unique) approximation to each vertex of
the polytope. We prove that in many latent variable settings, e.g., topic
modeling, LDA, optimization oracles do exist provided we project to a suitable
SVD subspace. Thus, our work yields the first efficient algorithm for finding
approximations to the vertices of the latent polytope under the
well-separatedness assumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_C/0/1/0/all/0/1&quot;&gt;Chiranjib Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1&quot;&gt;Ravindran Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Amit Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11373">
<title>Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11373</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been significant recent progress in the area of unsupervised skill
discovery, with various works proposing mutual information based objectives, as
a source of intrinsic motivation. Prior works predominantly focused on
designing algorithms that require online access to the environment. In
contrast, we develop an \textit{offline} skill discovery algorithm. Our problem
formulation considers the maximization of a mutual information objective
constrained by a KL-divergence. More precisely, the constraints ensure that the
state occupancy of each skill remains close to the state occupancy of an
expert, within the support of an offline dataset with good state-action
coverage. Our main contribution is to connect Fenchel duality, reinforcement
learning and unsupervised skill discovery, and to give a simple offline
algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1&quot;&gt;Marin Vlastelica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolev_P/0/1/0/all/0/1&quot;&gt;Pavel Kolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1&quot;&gt;Georg Martius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11375">
<title>LatentAugment: Data Augmentation via Guided Manipulation of GAN&apos;s Latent Space. (arXiv:2307.11375v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11375</link>
<description rdf:parseType="Literal">&lt;p&gt;Data Augmentation (DA) is a technique to increase the quantity and diversity
of the training data, and by that alleviate overfitting and improve
generalisation. However, standard DA produces synthetic data for augmentation
with limited diversity. Generative Adversarial Networks (GANs) may unlock
additional information in a dataset by generating synthetic samples having the
appearance of real images. However, these models struggle to simultaneously
address three key requirements: fidelity and high-quality samples; diversity
and mode coverage; and fast sampling. Indeed, GANs generate high-quality
samples rapidly, but have poor mode coverage, limiting their adoption in DA
applications. We propose LatentAugment, a DA strategy that overcomes the low
diversity of GANs, opening up for use in DA applications. Without external
supervision, LatentAugment modifies latent vectors and moves them into latent
space regions to maximise the synthetic images&apos; diversity and fidelity. It is
also agnostic to the dataset and the downstream task. A wide set of experiments
shows that LatentAugment improves the generalisation of a deep model
translating from MRI-to-CT beating both standard DA as well GAN-based sampling.
Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic
samples show superior mode coverage and diversity. Code is available at:
https://github.com/ltronchin/LatentAugment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tronchin_L/0/1/0/all/0/1&quot;&gt;Lorenzo Tronchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Minh H. Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1&quot;&gt;Paolo Soda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lofstedt_T/0/1/0/all/0/1&quot;&gt;Tommy L&amp;#xf6;fstedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11379">
<title>Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework. (arXiv:2307.11379v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11379</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is widely used to make decisions with societal impact such
as bank loan approving, criminal sentencing, and resume filtering. How to
ensure its fairness while maintaining utility is a challenging but crucial
issue. Fairness is a complex and context-dependent concept with over 70
different measurement metrics. Since existing regulations are often vague in
terms of which metric to use and different organizations may prefer different
fairness metrics, it is important to have means of improving fairness
comprehensively. Existing mitigation techniques often target at one specific
fairness metric and have limitations in improving multiple notions of fairness
simultaneously. In this work, we propose CFU (Comprehensive Fairness-Utility),
a reinforcement learning-based framework, to efficiently improve the
fairness-utility trade-off in machine learning classifiers. A comprehensive
measurement that can simultaneously consider multiple fairness notions as well
as utility is established, and new metrics are proposed based on an in-depth
analysis of the relationship between different fairness metrics. The reward
function of CFU is constructed with comprehensive measurement and new metrics.
We conduct extensive experiments to evaluate CFU on 6 tasks, 3 machine learning
models, and 15 fairness-utility measurements. The results demonstrate that CFU
can improve the classifier on multiple fairness metrics without sacrificing its
utility. It outperforms all state-of-the-art techniques and has witnessed a
37.5% improvement on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Simiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jitao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1&quot;&gt;Menghong Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yihao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yueling Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1&quot;&gt;Geguang Pu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11397">
<title>Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation. (arXiv:2307.11397v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.11397</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a challenging task, particularly due to inter-
and intra-observer variability, even between medical experts. In this paper, we
propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer
variation NetwOrk (Pionono). It captures the labeling behavior of each rater
with a multidimensional probability distribution and integrates this
information with the feature maps of the image to produce probabilistic
segmentation predictions. The model is optimized by variational inference and
can be trained end-to-end. It outperforms state-of-the-art models such as
STAPLE, Probabilistic U-Net, and models based on confusion matrices.
Additionally, Pionono predicts multiple coherent segmentation maps that mimic
the rater&apos;s expert opinion, which provides additional valuable information for
the diagnostic process. Experiments on real-world cancer segmentation datasets
demonstrate the high accuracy and efficiency of Pionono, making it a powerful
tool for medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmidt_A/0/1/0/all/0/1&quot;&gt;Arne Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morales_Alvarez_P/0/1/0/all/0/1&quot;&gt;Pablo Morales-&amp;#xc1;lvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1&quot;&gt;Rafael Molina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11408">
<title>Direct and inverse modeling of soft robots by learning a condensed FEM model. (arXiv:2307.11408v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.11408</link>
<description rdf:parseType="Literal">&lt;p&gt;The Finite Element Method (FEM) is a powerful modeling tool for predicting
the behavior of soft robots. However, its use for control can be difficult for
non-specialists of numerical computation: it requires an optimization of the
computation to make it real-time. In this paper, we propose a learning-based
approach to obtain a compact but sufficiently rich mechanical representation.
Our choice is based on nonlinear compliance data in the actuator/effector space
provided by a condensation of the FEM model. We demonstrate that this compact
model can be learned with a reasonable amount of data and, at the same time, be
very efficient in terms of modeling, since we can deduce the direct and inverse
kinematics of the robot. We also show how to couple some models learned
individually in particular on an example of a gripper composed of two soft
fingers. Other results are shown by comparing the inverse model derived from
the full FEM model and the one from the compact learned version. This work
opens new perspectives, namely for the embedded control of soft robots, but
also for their design. These perspectives are also discussed in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menager_E/0/1/0/all/0/1&quot;&gt;Etienne M&amp;#xe9;nager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navez_T/0/1/0/all/0/1&quot;&gt;Tanguy Navez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goury_O/0/1/0/all/0/1&quot;&gt;Olivier Goury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duriez_C/0/1/0/all/0/1&quot;&gt;Christian Duriez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11423">
<title>Attention to Entropic Communication. (arXiv:2307.11423v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.11423</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of attention, numerical weights that emphasize the importance of
particular data, has proven to be very relevant in artificial intelligence.
Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in
communication theory. Here we combine these concepts, attention and RE. RE
guides optimal encoding of messages in bandwidth-limited communication as well
as optimal message decoding via the maximum entropy principle (MEP). In the
coding scenario, RE can be derived from four requirements, namely being
analytical, local, proper, and calibrated. Weighted RE, used for attention
steering in communications, turns out to be improper. To see how proper
attention communication can emerge, we analyze a scenario of a message sender
who wants to ensure that the receiver of the message can perform well-informed
actions. If the receiver decodes the message using the MEP, the sender only
needs to know the receiver&apos;s utility function to inform optimally, but not the
receiver&apos;s initial knowledge state. In case only the curvature of the utility
function maxima are known, it becomes desirable to accurately communicate an
attention function, in this case a by this curvature weighted and re-normalized
probability function. Entropic attention communication is here proposed as the
desired generalization of entropic communication that permits weighting while
being proper, thereby aiding the design of optimal communication protocols in
technical applications and helping to understand human communication. For
example, our analysis shows how to derive the level of cooperation expected
under misaligned interests of otherwise honest communication partners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ensslin_T/0/1/0/all/0/1&quot;&gt;Torsten En&amp;#xdf;lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weidinger_C/0/1/0/all/0/1&quot;&gt;Carolin Weidinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frank_P/0/1/0/all/0/1&quot;&gt;Philipp Frank&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11432">
<title>An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems. (arXiv:2307.11432v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11432</link>
<description rdf:parseType="Literal">&lt;p&gt;Most solutions to the inventory management problem assume a centralization of
information that is incompatible with organisational constraints in real supply
chain networks. The inventory management problem is a well-known planning
problem in operations research, concerned with finding the optimal re-order
policy for nodes in a supply chain. While many centralized solutions to the
problem exist, they are not applicable to real-world supply chains made up of
independent entities. The problem can however be naturally decomposed into
sub-problems, each associated with an independent entity, turning it into a
multi-agent system. Therefore, a decentralized data-driven solution to
inventory management problems using multi-agent reinforcement learning is
proposed where each entity is controlled by an agent. Three multi-agent
variations of the proximal policy optimization algorithm are investigated
through simulations of different supply chain networks and levels of
uncertainty. The centralized training decentralized execution framework is
deployed, which relies on offline centralization during simulation-based policy
identification, but enables decentralization when the policies are deployed
online to the real system. Results show that using multi-agent proximal policy
optimization with a centralized critic leads to performance very close to that
of a centralized data-driven solution and outperforms a distributed model-based
solution in most cases while respecting the information constraints of the
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousa_M/0/1/0/all/0/1&quot;&gt;Marwan Mousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_D/0/1/0/all/0/1&quot;&gt;Damien van de Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotecha_N/0/1/0/all/0/1&quot;&gt;Niki Kotecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rio_Chanona_E/0/1/0/all/0/1&quot;&gt;Ehecatl Antonio del Rio-Chanona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mowbray_M/0/1/0/all/0/1&quot;&gt;Max Mowbray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11434">
<title>Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11434</link>
<description rdf:parseType="Literal">&lt;p&gt;The batch size is an essential parameter to tune during the development of
new neural networks. Amongst other quality indicators, it has a large degree of
influence on the model&apos;s accuracy, generalisability, training times and
parallelisability. This fact is generally known and commonly studied. However,
during the application phase of a deep learning model, when the model is
utilised by an end-user for inference, we find that there is a disregard for
the potential benefits of introducing a batch size. In this study, we examine
the effect of input batching on the energy consumption and response times of
five fully-trained neural networks for computer vision that were considered
state-of-the-art at the time of their publication. The results suggest that
batching has a significant effect on both of these metrics. Furthermore, we
present a timeline of the energy efficiency and accuracy of neural networks
over the past decade. We find that in general, energy consumption rises at a
much steeper pace than accuracy and question the necessity of this evolution.
Additionally, we highlight one particular network, ShuffleNetV2(2018), that
achieved a competitive performance for its time while maintaining a much lower
energy consumption. Nevertheless, we highlight that the results are model
dependent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarally_T/0/1/0/all/0/1&quot;&gt;Tim Yarally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feitosa_D/0/1/0/all/0/1&quot;&gt;Daniel Feitosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallou_J/0/1/0/all/0/1&quot;&gt;June Sallou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deursen_A/0/1/0/all/0/1&quot;&gt;Arie van Deursen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11436">
<title>Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs. (arXiv:2307.11436v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.11436</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently introduced DeepONet operator-learning framework for PDE control
is extended from the results for basic hyperbolic and parabolic PDEs to an
advanced hyperbolic class that involves delays on both the state and the system
output or input. The PDE backstepping design produces gain functions that are
outputs of a nonlinear operator, mapping functions on a spatial domain into
functions on a spatial domain, and where this gain-generating operator&apos;s inputs
are the PDE&apos;s coefficients. The operator is approximated with a DeepONet neural
network to a degree of accuracy that is provably arbitrarily tight. Once we
produce this approximation-theoretic result in infinite dimension, with it we
establish stability in closed loop under feedback that employs approximate
gains. In addition to supplying such results under full-state feedback, we also
develop DeepONet-approximated observers and output-feedback laws and prove
their own stabilizing properties under neural operator approximations. With
numerical simulations we illustrate the theoretical results and quantify the
numerical effort savings, which are of two orders of magnitude, thanks to
replacing the numerical PDE solving with the DeepONet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jie Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Krstic_M/0/1/0/all/0/1&quot;&gt;Miroslav Krstic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11462">
<title>Improve Long-term Memory Learning Through Rescaling the Error Temporally. (arXiv:2307.11462v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11462</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the error metric selection for long-term memory learning
in sequence modelling. We examine the bias towards short-term memory in
commonly used errors, including mean absolute/squared error. Our findings show
that all temporally positive-weighted errors are biased towards short-term
memory in learning linear functionals. To reduce this bias and improve
long-term memory learning, we propose the use of a temporally rescaled error.
In addition to reducing the bias towards short-term memory, this approach can
also alleviate the vanishing gradient issue. We conduct numerical experiments
on different long-memory tasks and sequence models to validate our claims.
Numerical results confirm the importance of appropriate temporally rescaled
error for effective long-term memory learning. To the best of our knowledge,
this is the first work that quantitatively analyzes different errors&apos; memory
bias towards short-term memory in sequence modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhanglu Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11465">
<title>A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11465</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most challenging fields where Artificial Intelligence (AI) can be
applied is lung cancer research, specifically non-small cell lung cancer
(NSCLC). In particular, overall survival (OS) is a vital indicator of patient
status, helping to identify subgroups with diverse survival probabilities,
enabling tailored treatment and improved OS rates. In this analysis, there are
two challenges to take into account. First, few studies effectively exploit the
information available from each patient, leveraging both uncensored (i.e.,
dead) and censored (i.e., survivors) patients, considering also the death
times. Second, the handling of incomplete data is a common issue in the medical
field. This problem is typically tackled through the use of imputation methods.
Our objective is to present an AI model able to overcome these limits,
effectively learning from both censored and uncensored patients and their
available features, for the prediction of OS for NSCLC patients. We present a
novel approach to survival analysis in the context of NSCLC, which exploits the
strengths of the transformer architecture accounting for only available
features without requiring any imputation strategy. By making use of ad-hoc
losses for OS, it accounts for both censored and uncensored patients,
considering risks over time. We evaluated the results over a period of 6 years
using different time granularities obtaining a Ct-index, a time-dependent
variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1
year and 2 years, respectively, outperforming all state-of-the-art methods
regardless of the imputation method used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caruso_C/0/1/0/all/0/1&quot;&gt;Camillo Maria Caruso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guarrasi_V/0/1/0/all/0/1&quot;&gt;Valerio Guarrasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramella_S/0/1/0/all/0/1&quot;&gt;Sara Ramella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1&quot;&gt;Paolo Soda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11487">
<title>A New Deep State-Space Analysis Framework for Patient Latent State Estimation and Classification from EHR Time Series Data. (arXiv:2307.11487v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11487</link>
<description rdf:parseType="Literal">&lt;p&gt;Many diseases, including cancer and chronic conditions, require extended
treatment periods and long-term strategies. Machine learning and AI research
focusing on electronic health records (EHRs) have emerged to address this need.
Effective treatment strategies involve more than capturing sequential changes
in patient test values. It requires an explainable and clinically interpretable
model by capturing the patient&apos;s internal state over time.
&lt;/p&gt;
&lt;p&gt;In this study, we propose the &quot;deep state-space analysis framework,&quot; using
time-series unsupervised learning of EHRs with a deep state-space model. This
framework enables learning, visualizing, and clustering of temporal changes in
patient latent states related to disease progression.
&lt;/p&gt;
&lt;p&gt;We evaluated our framework using time-series laboratory data from 12,695
cancer patients. By estimating latent states, we successfully discover latent
states related to prognosis. By visualization and cluster analysis, the
temporal transition of patient status and test items during state transitions
characteristic of each anticancer drug were identified. Our framework surpasses
existing methods in capturing interpretable latent space. It can be expected to
enhance our comprehension of disease progression from EHRs, aiding treatment
adjustments and prognostic determinations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1&quot;&gt;Aya Nakamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojima_R/0/1/0/all/0/1&quot;&gt;Ryosuke Kojima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamoto_Y/0/1/0/all/0/1&quot;&gt;Yuji Okamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchino_E/0/1/0/all/0/1&quot;&gt;Eiichiro Uchino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mineharu_Y/0/1/0/all/0/1&quot;&gt;Yohei Mineharu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_Y/0/1/0/all/0/1&quot;&gt;Yohei Harada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamada_M/0/1/0/all/0/1&quot;&gt;Mayumi Kamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muto_M/0/1/0/all/0/1&quot;&gt;Manabu Muto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanagita_M/0/1/0/all/0/1&quot;&gt;Motoko Yanagita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okuno_Y/0/1/0/all/0/1&quot;&gt;Yasushi Okuno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11494">
<title>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11494</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1&quot;&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1&quot;&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohlke_Schneider_M/0/1/0/all/0/1&quot;&gt;Michael Bohlke-Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zschiegner_J/0/1/0/all/0/1&quot;&gt;Jasper Zschiegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11503">
<title>General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11503</link>
<description rdf:parseType="Literal">&lt;p&gt;Sample reweighting is one of the most widely used methods for correcting the
error of least squares learning algorithms in reproducing kernel Hilbert spaces
(RKHS), that is caused by future data distributions that are different from the
training data distribution. In practical situations, the sample weights are
determined by values of the estimated Radon-Nikod\&apos;ym derivative, of the future
data distribution w.r.t.~the training data distribution. In this work, we
review known error bounds for reweighted kernel regression in RKHS and obtain,
by combination, novel results. We show under weak smoothness conditions, that
the amount of samples, needed to achieve the same order of accuracy as in the
standard supervised learning without differences in data distributions, is
smaller than proven by state-of-the-art analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Hoan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereverzyev_S/0/1/0/all/0/1&quot;&gt;Sergei V. Pereverzyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zellinger_W/0/1/0/all/0/1&quot;&gt;Werner Zellinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11519">
<title>Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11519</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous growth of internet users and media content, it is very
hard to track down hateful speech in audio and video. Converting video or audio
into text does not detect hate speech accurately as human sometimes uses
hateful words as humorous or pleasant in sense and also uses different voice
tones or show different action in the video. The state-ofthe-art hate speech
detection models were mostly developed on a single modality. In this research,
a combined approach of multimodal system has been proposed to detect hate
speech from video contents by extracting feature images, feature values
extracted from the audio, text and used machine learning and Natural language
processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boishakhi_F/0/1/0/all/0/1&quot;&gt;Fariha Tahosin Boishakhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shill_P/0/1/0/all/0/1&quot;&gt;Ponkoj Chandra Shill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11532">
<title>Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning. (arXiv:2307.11532v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11532</link>
<description rdf:parseType="Literal">&lt;p&gt;To alleviate the shortage of computing power faced by clients in training
deep neural networks (DNNs) using federated learning (FL), we leverage the edge
computing and split learning to propose a model-splitting allowed FL (SFL)
framework, with the aim to minimize the training latency without loss of test
accuracy. Under the synchronized global update setting, the latency to complete
a round of global training is determined by the maximum latency for the clients
to complete a local training session. Therefore, the training latency
minimization problem (TLMP) is modelled as a minimizing-maximum problem. To
solve this mixed integer nonlinear programming problem, we first propose a
regression method to fit the quantitative-relationship between the cut-layer
and other parameters of an AI-model, and thus, transform the TLMP into a
continuous problem. Considering that the two subproblems involved in the TLMP,
namely, the cut-layer selection problem for the clients and the computing
resource allocation problem for the parameter-server are relative independence,
an alternate-optimization-based algorithm with polynomial time complexity is
developed to obtain a high-quality solution to the TLMP. Extensive experiments
are performed on a popular DNN-model EfficientNetV2 using dataset MNIST, and
the results verify the validity and improved performance of the proposed SFL
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kezhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kun Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11546">
<title>Towards practical reinforcement learning for tokamak magnetic control. (arXiv:2307.11546v1 [physics.plasm-ph])</title>
<link>http://arxiv.org/abs/2307.11546</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has shown promising results for real-time control
systems, including the domain of plasma magnetic control. However, there are
still significant drawbacks compared to traditional feedback control approaches
for magnetic confinement. In this work, we address key drawbacks of the RL
method; achieving higher control accuracy for desired plasma properties,
reducing the steady-state error, and decreasing the required time to learn new
tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic
improvements to the agent architecture and training procedure. We present
simulation results that show up to 65\% improvement in shape accuracy, achieve
substantial reduction in the long-term bias of the plasma current, and
additionally reduce the training time required to learn new tasks by a factor
of 3 or more. We present new experiments using the upgraded RL-based
controllers on the TCV tokamak, which validate the simulation results achieved,
and point the way towards routinely achieving accurate discharges using the RL
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tracey_B/0/1/0/all/0/1&quot;&gt;Brendan D. Tracey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Michi_A/0/1/0/all/0/1&quot;&gt;Andrea Michi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chervonyi_Y/0/1/0/all/0/1&quot;&gt;Yuri Chervonyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Davies_I/0/1/0/all/0/1&quot;&gt;Ian Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Paduraru_C/0/1/0/all/0/1&quot;&gt;Cosmin Paduraru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lazic_N/0/1/0/all/0/1&quot;&gt;Nevena Lazic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Felici_F/0/1/0/all/0/1&quot;&gt;Federico Felici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ewalds_T/0/1/0/all/0/1&quot;&gt;Timo Ewalds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Donner_C/0/1/0/all/0/1&quot;&gt;Craig Donner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Galperti_C/0/1/0/all/0/1&quot;&gt;Cristian Galperti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Buchli_J/0/1/0/all/0/1&quot;&gt;Jonas Buchli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Neunert_M/0/1/0/all/0/1&quot;&gt;Michael Neunert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huber_A/0/1/0/all/0/1&quot;&gt;Andrea Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Evens_J/0/1/0/all/0/1&quot;&gt;Jonathan Evens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kurylowicz_P/0/1/0/all/0/1&quot;&gt;Paula Kurylowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mankowitz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Mankowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Riedmiller_M/0/1/0/all/0/1&quot;&gt;Martin Riedmiller&lt;/a&gt;, The &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Team_TCV/0/1/0/all/0/1&quot;&gt;TCV Team&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11552">
<title>A multi-modal representation of El Ni\~no Southern Oscillation Diversity. (arXiv:2307.11552v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2307.11552</link>
<description rdf:parseType="Literal">&lt;p&gt;The El Ni\~no-Southern Oscillation (ENSO) is characterized by alternating
periods of warm (El Ni\~no) and cold (La Ni\~na) sea surface temperature
anomalies (SSTA) in the equatorial Pacific. Although El Ni\~no and La Ni\~na
are well-defined climate patterns, no two events are alike. To date, ENSO
diversity has been described primarily in terms of the longitudinal location of
peak SSTA, used to define a bimodal classification of events in Eastern Pacific
(EP) and Central Pacific (CP) types. Here, we use low-dimensional
representations of Pacific SSTAs to argue that binary categorical memberships
are unsuitable to describe ENSO events. Using fuzzy unsupervised clustering, we
recover the four known ENSO categories, along with a fifth category: an Extreme
El Ni\~no. We show that Extreme El Ni\~nos differ both in their intensity and
temporal evolution from canonical EP El Ni\~nos. We also find that CP La
Ni\~nas, EP El Ni\~nos, and Extreme El Ni\~nos contribute the most to
interdecadal ENSO variability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Schlor_J/0/1/0/all/0/1&quot;&gt;Jakob Schl&amp;#xf6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Strnad_F/0/1/0/all/0/1&quot;&gt;Felix Strnad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Capotondi_A/0/1/0/all/0/1&quot;&gt;Antonietta Capotondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Goswami_B/0/1/0/all/0/1&quot;&gt;Bedartha Goswami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11565">
<title>FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks. (arXiv:2307.11565v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11565</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have been widely used in many critical applications,
such as autonomous vehicles and medical diagnosis. However, their security is
threatened by backdoor attack, which is achieved by adding artificial patterns
to specific training data. Existing defense strategies primarily focus on using
reverse engineering to reproduce the backdoor trigger generated by attackers
and subsequently repair the DNN model by adding the trigger into inputs and
fine-tuning the model with ground-truth labels. However, once the trigger
generated by the attackers is complex and invisible, the defender can not
successfully reproduce the trigger. Consequently, the DNN model will not be
repaired since the trigger is not effectively removed.
&lt;/p&gt;
&lt;p&gt;In this work, we propose Feature Map Testing~(FMT). Different from existing
defense strategies, which focus on reproducing backdoor triggers, FMT tries to
detect the backdoor feature maps, which are trained to extract backdoor
information from the inputs. After detecting these backdoor feature maps, FMT
will erase them and then fine-tune the model with a secure subset of training
data. Our experiments demonstrate that, compared to existing defense
strategies, FMT can effectively reduce the Attack Success Rate (ASR) even
against the most complex and invisible attack triggers. Second, unlike
conventional defense methods that tend to exhibit low Robust Accuracy (i.e.,
the model&apos;s accuracy on the poisoned data), FMT achieves higher RA, indicating
its superiority in maintaining model performance while mitigating the effects
of backdoor attacks~(e.g., FMT obtains 87.40\% RA in CIFAR10). Third, compared
to existing feature map pruning techniques, FMT can cover more backdoor feature
maps~(e.g., FMT removes 83.33\% of backdoor feature maps from the model in the
CIFAR10 \&amp;amp; BadNet scenario).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1&quot;&gt;Qingwen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_Y/0/1/0/all/0/1&quot;&gt;Yahao Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yichao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Heming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11584">
<title>A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion. (arXiv:2307.11584v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.11584</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech Emotion Recognition (SER) is a challenging task. In this paper, we
introduce a modality conversion concept aimed at enhancing emotion recognition
performance on the MELD dataset. We assess our approach through two
experiments: first, a method named Modality-Conversion that employs automatic
speech recognition (ASR) systems, followed by a text classifier; second, we
assume perfect ASR output and investigate the impact of modality conversion on
SER, this method is called Modality-Conversion++. Our findings indicate that
the first method yields substantial results, while the second method
outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER
weighted-F1 (WF1) score on the MELD dataset. This research highlights the
potential of modality conversion for tasks that can be conducted in alternative
modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taghavi_Z/0/1/0/all/0/1&quot;&gt;Zeinab Sadat Taghavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satvaty_A/0/1/0/all/0/1&quot;&gt;Ali Satvaty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sameti_H/0/1/0/all/0/1&quot;&gt;Hossein Sameti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11588">
<title>Transferability of Convolutional Neural Networks in Stationary Learning Tasks. (arXiv:2307.11588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11588</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in hardware and big data acquisition have accelerated the
development of deep learning techniques. For an extended period of time,
increasing the model complexity has led to performance improvements for various
tasks. However, this trend is becoming unsustainable and there is a need for
alternative, computationally lighter methods. In this paper, we introduce a
novel framework for efficient training of convolutional neural networks (CNNs)
for large-scale spatial problems. To accomplish this we investigate the
properties of CNNs for tasks where the underlying signals are stationary. We
show that a CNN trained on small windows of such signals achieves a nearly
performance on much larger windows without retraining. This claim is supported
by our theoretical analysis, which provides a bound on the performance
degradation. Additionally, we conduct thorough experimental analysis on two
tasks: multi-target tracking and mobile infrastructure on demand. Our results
show that the CNN is able to tackle problems with many hundreds of agents after
being trained with fewer than ten. Thus, CNN architectures provide solutions to
these problems at previously computationally intractable scales.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owerko_D/0/1/0/all/0/1&quot;&gt;Damian Owerko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanatsoulis_C/0/1/0/all/0/1&quot;&gt;Charilaos I. Kanatsoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondarchuk_J/0/1/0/all/0/1&quot;&gt;Jennifer Bondarchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucci_D/0/1/0/all/0/1&quot;&gt;Donald J. Bucci Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11607">
<title>Finding Optimal Diverse Feature Sets with Alternative Feature Selection. (arXiv:2307.11607v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11607</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection is popular for obtaining small, interpretable, yet highly
accurate prediction models. Conventional feature-selection methods typically
yield one feature set only, which might not suffice in some scenarios. For
example, users might be interested in finding alternative feature sets with
similar prediction quality, offering different explanations of the data. In
this article, we introduce alternative feature selection and formalize it as an
optimization problem. In particular, we define alternatives via constraints and
enable users to control the number and dissimilarity of alternatives. Next, we
analyze the complexity of this optimization problem and show NP-hardness.
Further, we discuss how to integrate conventional feature-selection methods as
objectives. Finally, we evaluate alternative feature selection with 30
classification datasets. We observe that alternative feature sets may indeed
have high prediction quality, and we analyze several factors influencing this
outcome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_J/0/1/0/all/0/1&quot;&gt;Jakob Bach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11608">
<title>Learning minimal representations of stochastic processes with variational autoencoders. (arXiv:2307.11608v1 [cond-mat.soft])</title>
<link>http://arxiv.org/abs/2307.11608</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic processes have found numerous applications in science, as they are
broadly used to model a variety of natural phenomena. Due to their intrinsic
randomness and uncertainty, they are however difficult to characterize. Here,
we introduce an unsupervised machine learning approach to determine the minimal
set of parameters required to effectively describe the dynamics of a stochastic
process. Our method builds upon an extended $\beta$-variational autoencoder
architecture. By means of simulated datasets corresponding to paradigmatic
diffusion models, we showcase its effectiveness in extracting the minimal
relevant parameters that accurately describe these dynamics. Furthermore, the
method enables the generation of new trajectories that faithfully replicate the
expected stochastic behavior. Overall, our approach enables for the autonomous
discovery of unknown parameters describing stochastic processes, hence
enhancing our comprehension of complex phenomena across various fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fernandez_Fernandez_G/0/1/0/all/0/1&quot;&gt;Gabriel Fern&amp;#xe1;ndez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Manzo_C/0/1/0/all/0/1&quot;&gt;Carlo Manzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lewenstein_M/0/1/0/all/0/1&quot;&gt;Maciej Lewenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Dauphin_A/0/1/0/all/0/1&quot;&gt;Alexandre Dauphin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Munoz_Gil_G/0/1/0/all/0/1&quot;&gt;Gorka Mu&amp;#xf1;oz-Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11609">
<title>Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains. (arXiv:2307.11609v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.11609</link>
<description rdf:parseType="Literal">&lt;p&gt;Entanglement propagation provides a key routine to understand quantum
many-body dynamics in and out of equilibrium. In this work, we uncover that the
``variational entanglement-enhancing&apos;&apos; field (VEEF) robustly induces a
persistent ballistic spreading of entanglement in quantum spin chains. The VEEF
is time dependent, and is optimally controlled to maximize the bipartite
entanglement entropy (EE) of the final state. Such a linear growth persists
till the EE reaches the genuine saturation $\tilde{S} = - \log_{2}
2^{-\frac{N}{2}}=\frac{N}{2}$ with $N$ the total number of spins. The EE
satisfies $S(t) = v t$ for the time $t \leq \frac{N}{2v}$, with $v$ the
velocity. These results are in sharp contrast with the behaviors without VEEF,
where the EE generally approaches a sub-saturation known as the Page value
$\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$ in the long-time limit, and the
entanglement growth deviates from being linear before the Page value is
reached. The dependence between the velocity and interactions is explored, with
$v \simeq 2.76$, $4.98$, and $5.75$ for the spin chains with Ising, XY, and
Heisenberg interactions, respectively. We further show that the nonlinear
growth of EE emerges with the presence of long-range interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Ying Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shi_P/0/1/0/all/0/1&quot;&gt;Pei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao-Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1&quot;&gt;Shi-Ju Ran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11617">
<title>Robust Fully-Asynchronous Methods for Distributed Training over General Architecture. (arXiv:2307.11617v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.11617</link>
<description rdf:parseType="Literal">&lt;p&gt;Perfect synchronization in distributed machine learning problems is
inefficient and even impossible due to the existence of latency, package losses
and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient
Tracking method (R-FAST), where each device performs local computation and
communication at its own pace without any form of synchronization. Different
from existing asynchronous distributed algorithms, R-FAST can eliminate the
impact of data heterogeneity across devices and allow for packet losses by
employing a robust gradient tracking strategy that relies on properly designed
auxiliary variables for tracking and buffering the overall gradient vector.
More importantly, the proposed method utilizes two spanning-tree graphs for
communication so long as both share at least one common root, enabling flexible
designs in communication architectures. We show that R-FAST converges in
expectation to a neighborhood of the optimum with a geometric rate for smooth
and strongly convex objectives; and to a stationary point with a sublinear rate
for general non-convex settings. Extensive experiments demonstrate that R-FAST
runs 1.5-2 times faster than synchronous benchmark algorithms, such as
Ring-AllReduce and D-PSGD, while still achieving comparable accuracy, and
outperforms existing asynchronous SOTA algorithms, such as AD-PSGD and OSGP,
especially in the presence of stragglers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zehan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shibo He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11620">
<title>Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization. (arXiv:2307.11620v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11620</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) has received considerable attention in
recent years due to its attractive capability of learning policies from offline
datasets without environmental interactions. Despite some success in the
single-agent setting, offline multi-agent RL (MARL) remains to be a challenge.
The large joint state-action space and the coupled multi-agent behaviors pose
extra complexities for offline policy optimization. Most existing offline MARL
studies simply apply offline data-related regularizations on individual agents,
without fully considering the multi-agent system at the global level. In this
work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit
global-to-local v alue regularization. OMIGA provides a principled framework to
convert global-level value regularization into equivalent implicit local value
regularizations and simultaneously enables in-sample learning, thus elegantly
bridging multi-agent value decomposition and policy learning with offline
regularizations. Based on comprehensive experiments on the offline multi-agent
MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves
superior performance over the state-of-the-art offline MARL methods in almost
all tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangsen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yinan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xianyuan Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11629">
<title>Scalable Multi-agent Skill Discovery based on Kronecker Graphs. (arXiv:2307.11629v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11629</link>
<description rdf:parseType="Literal">&lt;p&gt;Covering skill (a.k.a., option) discovery has been developed to improve the
exploration of RL in single-agent scenarios with sparse reward signals, through
connecting the most distant states in the embedding space provided by the
Fiedler vector of the state transition graph. Given that joint state space
grows exponentially with the number of agents in multi-agent systems, existing
researches still relying on single-agent option discovery either become
prohibitive or fail to directly discover joint options that improve the
connectivity of the joint state space. In this paper, we show how to directly
compute multi-agent options with collaborative exploratory behaviors while
still enjoying the ease of decomposition. Our key idea is to approximate the
joint state space as a Kronecker graph, based on which we can directly estimate
its Fiedler vector using the Laplacian spectrum of individual agents&apos;
transition graphs. Further, considering that directly computing the Laplacian
spectrum is intractable for tasks with infinite-scale state spaces, we further
propose a deep learning extension of our method by estimating eigenfunctions
through NN-based representation learning techniques. The evaluation on
multi-agent tasks built with simulators like Mujoco, shows that the proposed
algorithm can successfully identify multi-agent options, and significantly
outperforms the state-of-the-art. Codes are available at:
https://github.itap.purdue.edu/Clan-labs/Scalable_MAOD_via_KP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11655">
<title>Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11655</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a model for learning with bandit feedback while accounting for
deterministically evolving and unobservable states that we call Bandits with
Deterministically Evolving States. The workhorse applications of our model are
learning for recommendation systems and learning for online ads. In both cases,
the reward that the algorithm obtains at each round is a function of the
short-term reward of the action chosen and how ``healthy&apos;&apos; the system is (i.e.,
as measured by its state). For example, in recommendation systems, the reward
that the platform obtains from a user&apos;s engagement with a particular type of
content depends not only on the inherent features of the specific content, but
also on how the user&apos;s preferences have evolved as a result of interacting with
other types of content on the platform. Our general model accounts for the
different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a
user&apos;s preferences shift as a result of previous content consumption) and
encompasses standard multi-armed bandits as a special case. The goal of the
algorithm is to minimize a notion of regret against the best-fixed sequence of
arms pulled. We analyze online learning algorithms for any possible
parametrization of the evolution rate $\lambda$. Specifically, the regret rates
obtained are: for $\lambda \in [0, 1/T^2]$: $\widetilde O(\sqrt{KT})$; for
$\lambda = T^{-a/b}$ with $b &amp;lt; a &amp;lt; 2b$: $\widetilde O (T^{b/a})$; for $\lambda
\in (1/T, 1 - 1/\sqrt{T}): \widetilde O (K^{1/3}T^{2/3})$; and for $\lambda \in
[1 - 1/\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khosravi_K/0/1/0/all/0/1&quot;&gt;Khashayar Khosravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leme_R/0/1/0/all/0/1&quot;&gt;Renato Paes Leme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Podimata_C/0/1/0/all/0/1&quot;&gt;Chara Podimata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsorvantzis_A/0/1/0/all/0/1&quot;&gt;Apostolis Tsorvantzis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11661">
<title>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.11661</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have
revolutionized visual representation learning by providing good performance on
downstream datasets. VLMs are 0-shot adapted to a downstream dataset by
designing prompts that are relevant to the dataset. Such prompt engineering
makes use of domain expertise and a validation dataset. Meanwhile, recent
developments in generative pretrained models like GPT-4 mean they can be used
as advanced internet search tools. They can also be manipulated to provide
visual information in any structure. In this work, we show that GPT-4 can be
used to generate text that is visually descriptive and how this can be used to
adapt CLIP to downstream tasks. We show considerable improvements in 0-shot
transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD
(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP&apos;s default prompt.
We also design a simple few-shot adapter that learns to choose the best
possible sentences to construct generalizable classifiers that outperform the
recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized
fine-grained datasets. We will release the code, prompts, and auxiliary text
dataset upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1&quot;&gt;Mayug Maniparambil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorster_C/0/1/0/all/0/1&quot;&gt;Chris Vorster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_D/0/1/0/all/0/1&quot;&gt;Derek Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_N/0/1/0/all/0/1&quot;&gt;Noel Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1&quot;&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11668">
<title>An Efficient Interior-Point Method for Online Convex Optimization. (arXiv:2307.11668v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11668</link>
<description rdf:parseType="Literal">&lt;p&gt;A new algorithm for regret minimization in online convex optimization is
described. The regret of the algorithm after $T$ time periods is $O(\sqrt{T
\log T})$ - which is the minimum possible up to a logarithmic term. In
addition, the new algorithm is adaptive, in the sense that the regret bounds
hold not only for the time periods $1,\ldots,T$ but also for every sub-interval
$s,s+1,\ldots,t$. The running time of the algorithm matches that of newly
introduced interior point algorithms for regret minimization: in
$n$-dimensional space, during each iteration the new algorithm essentially
solves a system of linear equations of order $n$, rather than solving some
constrained convex optimization problem in $n$ dimensions and possibly many
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1&quot;&gt;Elad Hazan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megiddo_N/0/1/0/all/0/1&quot;&gt;Nimrod Megiddo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11672">
<title>Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11672</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive test-time defenses are used to improve the robustness of deep neural
networks to adversarial examples. However, existing methods significantly
increase the inference time due to additional optimization on the model
parameters or the input at test time. In this work, we propose a novel adaptive
test-time defense strategy that is easy to integrate with any existing (robust)
training procedure without additional test-time computation. Based on the
notion of robustness of features that we present, the key idea is to project
the trained models to the most robust feature space, thereby reducing the
vulnerability to adversarial attacks in non-robust directions. We theoretically
show that the top eigenspace of the feature matrix are more robust for a
generalized additive model and support our argument for a large width neural
network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive
experiments on CIFAR-10 and CIFAR-100 datasets for several robustness
benchmarks, including the state-of-the-art methods in RobustBench, and observe
that the proposed method outperforms existing adaptive test-time defenses at
much lower computation costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Anurag Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabanayagam_M/0/1/0/all/0/1&quot;&gt;Mahalakshmi Sabanayagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1&quot;&gt;Krikamol Muandet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghoshdastidar_D/0/1/0/all/0/1&quot;&gt;Debarghya Ghoshdastidar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11684">
<title>Minibatching Offers Improved Generalization Performance for Second Order Optimizers. (arXiv:2307.11684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11684</link>
<description rdf:parseType="Literal">&lt;p&gt;Training deep neural networks (DNNs) used in modern machine learning is
computationally expensive. Machine learning scientists, therefore, rely on
stochastic first-order methods for training, coupled with significant
hand-tuning, to obtain good performance. To better understand performance
variability of different stochastic algorithms, including second-order methods,
we conduct an empirical study that treats performance as a response variable
across multiple training sessions of the same model. Using 2-factor Analysis of
Variance (ANOVA) with interactions, we show that batch size used during
training has a statistically significant effect on the peak accuracy of the
methods, and that full batch largely performed the worst. In addition, we found
that second-order optimizers (SOOs) generally exhibited significantly lower
variance at specific batch sizes, suggesting they may require less
hyperparameter tuning, leading to a reduced overall time to solution for model
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silk_E/0/1/0/all/0/1&quot;&gt;Eric Silk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Swarnita Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_N/0/1/0/all/0/1&quot;&gt;Nairanjana Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarwate_A/0/1/0/all/0/1&quot;&gt;Anand D. Sarwate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lumsdaine_A/0/1/0/all/0/1&quot;&gt;Andrew Lumsdaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1&quot;&gt;Tony Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11685">
<title>Towards Generalizable Reinforcement Learning for Trade Execution. (arXiv:2307.11685v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/2307.11685</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimized trade execution is to sell (or buy) a given amount of assets in a
given time with the lowest possible trading cost. Recently, reinforcement
learning (RL) has been applied to optimized trade execution to learn smarter
policies from market data. However, we find that many existing RL methods
exhibit considerable overfitting which prevents them from real deployment. In
this paper, we provide an extensive study on the overfitting problem in
optimized trade execution. First, we model the optimized trade execution as
offline RL with dynamic context (ORDC), where the context represents market
variables that cannot be influenced by the trading policy and are collected in
an offline manner. Under this framework, we derive the generalization bound and
find that the overfitting issue is caused by large context space and limited
context samples in the offline setting. Accordingly, we propose to learn
compact representations for context to address the overfitting problem, either
by leveraging prior knowledge or in an end-to-end manner. To evaluate our
algorithms, we also implement a carefully designed simulator based on
historical limit order book (LOB) data to provide a high-fidelity benchmark for
different algorithms. Our experiments on the high-fidelity simulator
demonstrate that our algorithms can effectively alleviate overfitting and
achieve better performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yitong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Li Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11688">
<title>Interpretable Graph Networks Formulate Universal Algebra Conjectures. (arXiv:2307.11688v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11688</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of Artificial Intelligence (AI) recently empowered researchers to
investigate hard mathematical problems which eluded traditional approaches for
decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields
laying the foundations of modern mathematics -- is still completely unexplored.
This work proposes the first use of AI to investigate UA&apos;s conjectures with an
equivalent equational and topological characterization. While topological
representations would enable the analysis of such properties using graph neural
networks, the limited transparency and brittle explainability of these models
hinder their straightforward use to empirically validate existing conjectures
or to formulate new ones. To bridge these gaps, we propose a general algorithm
generating AI-ready datasets based on UA&apos;s conjectures, and introduce a novel
neural layer to build fully interpretable graph networks. The results of our
experiments demonstrate that interpretable graph networks: (i) enhance
interpretability without sacrificing task accuracy, (ii) strongly generalize
when predicting universal algebra&apos;s properties, (iii) generate simple
explanations that empirically validate existing conjectures, and (iv) identify
subgraphs suggesting the formulation of novel conjectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1&quot;&gt;Francesco Giannini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fioravanti_S/0/1/0/all/0/1&quot;&gt;Stefano Fioravanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskin_O/0/1/0/all/0/1&quot;&gt;Oguzhan Keskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupidi_A/0/1/0/all/0/1&quot;&gt;Alisia Maria Lupidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1&quot;&gt;Lucie Charlotte Magister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Lio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1&quot;&gt;Pietro Barbiero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11694">
<title>SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.11694</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting synergistic drug combinations can help accelerate discovery of
cancer treatments, particularly therapies personalized to a patient&apos;s specific
tumor via biopsied cells. In this paper, we propose a novel setting and models
for in-context drug synergy learning. We are given a small &quot;personalized
dataset&quot; of 10-20 drug synergy relationships in the context of specific cancer
cell targets. Our goal is to predict additional drug synergy relationships in
that context. Inspired by recent work that pre-trains a GPT language model (LM)
to &quot;in-context learn&quot; common function classes, we devise novel pre-training
schemes that enable a GPT model to in-context learn &quot;drug synergy functions&quot;.
Our model -- which does not use any textual corpora, molecular fingerprints,
protein interaction or any other domain-specific knowledge -- is able to
achieve competitive results. We further integrate our in-context approach with
a genetic algorithm to optimize model prompts and select synergy candidates to
test after conducting a patient biopsy. Finally, we explore a novel task of
inverse drug design which can potentially enable the design of drugs that
synergize specifically to target a given patient&apos;s &quot;personalized dataset&quot;. Our
findings can potentially have an important impact on precision cancer medicine,
and also raise intriguing questions on non-textual pre-training for LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1&quot;&gt;Carl Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1&quot;&gt;Aakanksha Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1&quot;&gt;Tushar Khot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1&quot;&gt;Martin Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1&quot;&gt;Tom Hope&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11695">
<title>Using simulation to calibrate real data acquisition in veterinary medicine. (arXiv:2307.11695v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11695</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the innovative use of simulation environments to enhance
data acquisition and diagnostics in veterinary medicine, focusing specifically
on gait analysis in dogs. The study harnesses the power of Blender and the
Blenderproc library to generate synthetic datasets that reflect diverse
anatomical, environmental, and behavioral conditions. The generated data,
represented in graph form and standardized for optimal analysis, is utilized to
train machine learning algorithms for identifying normal and abnormal gaits.
Two distinct datasets with varying degrees of camera angle granularity are
created to further investigate the influence of camera perspective on model
accuracy. Preliminary results suggest that this simulation-based approach holds
promise for advancing veterinary diagnostics by enabling more precise data
acquisition and more effective machine learning models. By integrating
synthetic and real-world patient data, the study lays a robust foundation for
improving overall effectiveness and efficiency in veterinary medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strzalka_K/0/1/0/all/0/1&quot;&gt;Krystian Strza&amp;#x142;ka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazurek_S/0/1/0/all/0/1&quot;&gt;Szymon Mazurek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielgosz_M/0/1/0/all/0/1&quot;&gt;Maciej Wielgosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russek_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Russek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caputa_J/0/1/0/all/0/1&quot;&gt;Jakub Caputa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasik_D/0/1/0/all/0/1&quot;&gt;Daria &amp;#x141;ukasik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krupinski_J/0/1/0/all/0/1&quot;&gt;Jan Krupi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grzeszczyk_J/0/1/0/all/0/1&quot;&gt;Jakub Grzeszczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karwatowski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Karwatowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraczek_R/0/1/0/all/0/1&quot;&gt;Rafa&amp;#x142; Fr&amp;#x105;czek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamro_E/0/1/0/all/0/1&quot;&gt;Ernest Jamro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1&quot;&gt;Marcin Pietro&amp;#x144;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koryciak_S/0/1/0/all/0/1&quot;&gt;Sebastian Koryciak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabrowska_Boruch_A/0/1/0/all/0/1&quot;&gt;Agnieszka D&amp;#x105;browska-Boruch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiatr_K/0/1/0/all/0/1&quot;&gt;Kazimierz Wiatr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11704">
<title>JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning. (arXiv:2307.11704v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11704</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present \textsc{JoinGym}, an efficient and lightweight
query optimization environment for reinforcement learning (RL). Join order
selection (JOS) is a classic NP-hard combinatorial optimization problem from
database query optimization and can serve as a practical testbed for the
generalization capabilities of RL algorithms. We describe how to formulate each
of the left-deep and bushy variants of the JOS problem as a Markov Decision
Process (MDP), and we provide an implementation adhering to the standard
Gymnasium API. We highlight that our implementation \textsc{JoinGym} is
completely based on offline traces of all possible joins, which enables RL
practitioners to easily and quickly test their methods on a realistic data
management problem without needing to setup any systems. Moreover, we also
provide all possible join traces on $3300$ novel SQL queries generated from the
IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least
one method can obtain near-optimal performance on train-set queries but their
performance degrades by several orders of magnitude on test-set queries. This
gap motivates further research for RL algorithms that generalize well in
multi-task combinatorial optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junxiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yueying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1&quot;&gt;Nathan Kallus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1&quot;&gt;Immanuel Trummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11714">
<title>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11714</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal Transport has sparked vivid interest in recent years, in particular
thanks to the Wasserstein distance, which provides a geometrically sensible and
intuitive way of comparing probability measures. For computational reasons, the
Sliced Wasserstein (SW) distance was introduced as an alternative to the
Wasserstein distance, and has seen uses for training generative Neural Networks
(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed
practically in such a setting, there is to our knowledge no theoretical
guarantee for this observation. Leveraging recent works on convergence of SGD
on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to
bridge that knowledge gap, and provide a realistic context under which
fixed-step SGD trajectories for the SW loss on NN parameters converge. More
precisely, we show that the trajectories approach the set of (sub)-gradient
flow equations as the step decreases. Under stricter assumptions, we show a
much stronger convergence result for noised and projected SGD schemes, namely
that the long-run limits of the trajectories approach a set of generalised
critical points of the loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanguy_E/0/1/0/all/0/1&quot;&gt;Eloi Tanguy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11730">
<title>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of Decentralized Federated Learning (DFL) has enabled the training
of machine learning models across federated participants, fostering
decentralized model aggregation and reducing dependence on a server. However,
this approach introduces unique communication security challenges that have yet
to be thoroughly addressed in the literature. These challenges primarily
originate from the decentralized nature of the aggregation process, the varied
roles and responsibilities of the participants, and the absence of a central
authority to oversee and mitigate threats. Addressing these challenges, this
paper first delineates a comprehensive threat model, highlighting the potential
risks of DFL communications. In response to these identified risks, this work
introduces a security module designed for DFL platforms to counter
communication-based attacks. The module combines security techniques such as
symmetric and asymmetric encryption with Moving Target Defense (MTD)
techniques, including random neighbor selection and IP/port switching. The
security module is implemented in a DFL platform called Fedstellar, allowing
the deployment and monitoring of the federation. A DFL scenario has been
deployed, involving eight physical devices implementing three security
configurations: (i) a baseline with no security, (ii) an encrypted
configuration, and (iii) a configuration integrating both encryption and MTD
techniques. The effectiveness of the security module is validated through
experiments with the MNIST dataset and eclipse attacks. The results indicated
an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2%
+-3.5%) and network traffic (230 MB +-15 MB) under the most secure
configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltran_E/0/1/0/all/0/1&quot;&gt;Enrique Tom&amp;#xe1;s Mart&amp;#xed;nez Beltr&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro Miguel S&amp;#xe1;nchez S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernal_S/0/1/0/all/0/1&quot;&gt;Sergio L&amp;#xf3;pez Bernal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;r&amp;#xf4;me Bovet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1&quot;&gt;Manuel Gil P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Gregorio Mart&amp;#xed;nez P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1&quot;&gt;Alberto Huertas Celdr&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11732">
<title>Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications. (arXiv:2307.11732v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11732</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a learning model of online ad auctions that allows for
the following four key realistic characteristics of contemporary online
auctions: (1) ad slots can have different values and click-through rates
depending on users&apos; search queries, (2) the number and identity of competing
advertisers are unobserved and change with each auction, (3) advertisers only
receive partial, aggregated feedback, and (4) payment rules are only partially
specified. We model advertisers as agents governed by an adversarial bandit
algorithm, independent of auction mechanism intricacies. Our objective is to
simulate the behavior of advertisers for counterfactual analysis, prediction,
and inference purposes. Our findings reveal that, in such richer environments,
&quot;soft floors&quot; can enhance key performance metrics even when bidders are drawn
from the same population. We further demonstrate how to infer advertiser value
distributions from observed bids, thereby affirming the practical efficacy of
our approach even in a more realistic auction setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Ming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabi_S/0/1/0/all/0/1&quot;&gt;Sareh Nabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siniscalchi_M/0/1/0/all/0/1&quot;&gt;Marciano Siniscalchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11749">
<title>Differentially Private Heavy Hitter Detection using Federated Analytics. (arXiv:2307.11749v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.11749</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study practical heuristics to improve the performance of
prefix-tree based algorithms for differentially private heavy hitter detection.
Our model assumes each user has multiple data points and the goal is to learn
as many of the most frequent data points as possible across all users&apos; data
with aggregate and local differential privacy. We propose an adaptive
hyperparameter tuning algorithm that improves the performance of the algorithm
while satisfying computational, communication and privacy constraints. We
explore the impact of different data-selection schemes as well as the impact of
introducing deny lists during multiple runs of the algorithm. We test these
improvements using extensive experimentation on the Reddit
dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_K/0/1/0/all/0/1&quot;&gt;Karan Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1&quot;&gt;John Duchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1&quot;&gt;Vitaly Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1&quot;&gt;Hanieh Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidbakht_O/0/1/0/all/0/1&quot;&gt;Omid Javidbakht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1&quot;&gt;Audra McMillan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1&quot;&gt;Kunal Talwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.06134">
<title>Is Homophily a Necessity for Graph Neural Networks?. (arXiv:2106.06134v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.06134</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have shown great prowess in learning
representations suitable for numerous graph-based machine learning tasks. When
applied to semi-supervised node classification, GNNs are widely believed to
work well due to the homophily assumption (&quot;like attracts like&quot;), and fail to
generalize to heterophilous graphs where dissimilar nodes connect. Recent works
design new architectures to overcome such heterophily-related limitations,
citing poor baseline performance and new architecture improvements on a few
heterophilous graph benchmark datasets as evidence for this notion. In our
experiments, we empirically find that standard graph convolutional networks
(GCNs) can actually achieve better performance than such carefully designed
methods on some commonly used heterophilous graphs. This motivates us to
reconsider whether homophily is truly necessary for good GNN performance. We
find that this claim is not quite true, and in fact, GCNs can achieve strong
performance on heterophilous graphs under certain conditions. Our work
carefully characterizes these conditions, and provides supporting theoretical
understanding and empirical observations. Finally, we examine existing
heterophilous graphs benchmarks and reconcile how the GCN (under)performs on
them based on this understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.14778">
<title>CALDA: Improving Multi-Source Time Series Domain Adaptation with Contrastive Adversarial Learning. (arXiv:2109.14778v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.14778</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) provides a strategy for improving
machine learning performance in data-rich (target) domains where ground truth
labels are inaccessible but can be found in related (source) domains. In cases
where meta-domain information such as label distributions is available, weak
supervision can further boost performance. We propose a novel framework, CALDA,
to tackle these two problems. CALDA synergistically combines the principles of
contrastive learning and adversarial learning to robustly support multi-source
UDA (MS-UDA) for time series data. Similar to prior methods, CALDA utilizes
adversarial learning to align source and target feature representations. Unlike
prior approaches, CALDA additionally leverages cross-source label information
across domains. CALDA pulls examples with the same label close to each other,
while pushing apart examples with different labels, reshaping the space through
contrastive learning. Unlike prior contrastive adaptation methods, CALDA
requires neither data augmentation nor pseudo labeling, which may be more
challenging for time series. We empirically validate our proposed approach.
Based on results from human activity recognition, electromyography, and
synthetic datasets, we find utilizing cross-source information improves
performance over prior time series and contrastive methods. Weak supervision
further improves performance, even in the presence of noise, allowing CALDA to
offer generalizable strategies for MS-UDA. Code is available at:
https://github.com/floft/calda
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_G/0/1/0/all/0/1&quot;&gt;Garrett Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doppa_J/0/1/0/all/0/1&quot;&gt;Janardhan Rao Doppa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cook_D/0/1/0/all/0/1&quot;&gt;Diane J. Cook&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.08227">
<title>Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2201.08227</link>
<description rdf:parseType="Literal">&lt;p&gt;Covering skill (a.k.a., option) discovery has been developed to improve the
exploration of reinforcement learning in single-agent scenarios with sparse
reward signals, through connecting the most distant states in the embedding
space provided by the Fiedler vector of the state transition graph. However,
these option discovery methods cannot be directly extended to multi-agent
scenarios, since the joint state space grows exponentially with the number of
agents in the system. Thus, existing researches on adopting options in
multi-agent scenarios still rely on single-agent option discovery and fail to
directly discover the joint options that can improve the connectivity of the
joint state space of agents. In this paper, we show that it is indeed possible
to directly compute multi-agent options with collaborative exploratory
behaviors among the agents, while still enjoying the ease of decomposition. Our
key idea is to approximate the joint state space as a Kronecker graph -- the
Kronecker product of individual agents&apos; state transition graphs, based on which
we can directly estimate the Fiedler vector of the joint state space using the
Laplacian spectrum of individual agents&apos; transition graphs. This decomposition
enables us to efficiently construct multi-agent joint options by encouraging
agents to connect the sub-goal joint states which are corresponding to the
minimum or maximum values of the estimated joint Fiedler vector. The evaluation
based on multi-agent collaborative tasks shows that the proposed algorithm can
successfully identify multi-agent options, and significantly outperforms prior
works using single-agent options or no options, in terms of both faster
exploration and higher cumulative rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.10736">
<title>The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.10736</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the fundamental problems in machine learning is generalization. In
neural network models with a large number of weights (parameters), many
solutions can be found to fit the training data equally well. The key question
is which solution can describe testing data not in the training set. Here, we
report the discovery of an exact duality (equivalence) between changes in
activities in a given layer of neurons and changes in weights that connect to
the next layer of neurons in a densely connected layer in any feed forward
neural network. The activity-weight (A-W) duality allows us to map variations
in inputs (data) to variations of the corresponding dual weights. By using this
mapping, we show that the generalization loss can be decomposed into a sum of
contributions from different eigen-directions of the Hessian matrix of the loss
function at the solution in weight space. The contribution from a given
eigen-direction is the product of two geometric factors (determinants): the
sharpness of the loss landscape and the standard deviation of the dual weights,
which is found to scale with the weight norm of the solution. Our results
provide an unified framework, which we used to reveal how different
regularization schemes (weight decay, stochastic gradient descent with
different batch sizes and learning rates, dropout), training data size, and
labeling noise affect generalization performance by controlling either one or
both of these two geometric determinants for generalization. These insights can
be used to guide development of algorithms for finding more generalizable
solutions in overparametrized neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1&quot;&gt;Yuhai Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.07493">
<title>Multi-scale Attention Flow for Probabilistic Time Series Forecasting. (arXiv:2205.07493v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.07493</link>
<description rdf:parseType="Literal">&lt;p&gt;The probability prediction of multivariate time series is a notoriously
challenging but practical task. On the one hand, the challenge is how to
effectively capture the cross-series correlations between interacting time
series, to achieve accurate distribution modeling. On the other hand, we should
consider how to capture the contextual information within time series more
accurately to model multivariate temporal dynamics of time series. In this
work, we proposed a novel non-autoregressive deep learning model, called
Multi-scale Attention Normalizing Flow(MANF), where we integrate multi-scale
attention and relative position information and the multivariate data
distribution is represented by the conditioned normalizing flow. Additionally,
compared with autoregressive modeling methods, our model avoids the influence
of cumulative error and does not increase the time complexity. Extensive
experiments demonstrate that our model achieves state-of-the-art performance on
many popular multivariate datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shibo Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1&quot;&gt;Chunyan Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Ke Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaxiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Pengcheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peilin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09208">
<title>Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures. (arXiv:2205.09208v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09208</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperdimensional computing (HD), also known as vector symbolic architectures
(VSA), is a framework for computing with distributed representations by
exploiting properties of random high-dimensional vector spaces. The commitment
of the scientific community to aggregate and disseminate research in this
particularly multidisciplinary area has been fundamental for its advancement.
Joining these efforts, we present Torchhd, a high-performance open source
Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and
serves as an efficient foundation for further research and application
development. The easy-to-use library builds on top of PyTorch and features
state-of-the-art HD/VSA functionality, clear documentation, and implementation
examples from well-known publications. Comparing publicly available code with
their corresponding Torchhd implementation shows that experiments can run up to
100x faster. Torchhd is available at:
https://github.com/hyperdimensional-computing/torchhd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heddes_M/0/1/0/all/0/1&quot;&gt;Mike Heddes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_I/0/1/0/all/0/1&quot;&gt;Igor Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verges_P/0/1/0/all/0/1&quot;&gt;Pere Verg&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1&quot;&gt;Denis Kleyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abraham_D/0/1/0/all/0/1&quot;&gt;Danny Abraham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Givargis_T/0/1/0/all/0/1&quot;&gt;Tony Givargis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolau_A/0/1/0/all/0/1&quot;&gt;Alexandru Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veidenbaum_A/0/1/0/all/0/1&quot;&gt;Alexander Veidenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09702">
<title>Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09702</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) are among the most powerful tools in deep
learning. They routinely solve complex problems on unstructured networks, such
as node classification, graph classification, or link prediction, with high
accuracy. However, both inference and training of GNNs are complex, and they
uniquely combine the features of irregular graph processing with dense and
regular computations. This complexity makes it very challenging to execute GNNs
efficiently on modern massively parallel architectures. To alleviate this, we
first design a taxonomy of parallelism in GNNs, considering data and model
parallelism, and different forms of pipelining. Then, we use this taxonomy to
investigate the amount of parallelism in numerous GNN models, GNN-driven
machine learning tasks, software frameworks, or hardware accelerators. We use
the work-depth model, and we also assess communication volume and
synchronization. We specifically focus on the sparsity/density of the
associated tensors, in order to understand how to effectively apply techniques
such as vectorization. We also formally analyze GNN pipelining, and we
generalize the established Message-Passing class of GNN models to cover
arbitrary pipeline depths, facilitating future optimizations. Finally, we
investigate different forms of asynchronicity, navigating the path for future
asynchronous parallel GNN pipelines. The outcomes of our analysis are
synthesized in a set of insights that help to maximize GNN performance, and a
comprehensive list of challenges and opportunities for further research into
efficient GNN computations. Our work will help to advance the design of future
GNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1&quot;&gt;Maciej Besta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03269">
<title>Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03269</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of skills (a.k.a., options) can greatly accelerate exploration in
reinforcement learning, especially when only sparse reward signals are
available. While option discovery methods have been proposed for individual
agents, in multi-agent reinforcement learning settings, discovering
collaborative options that can coordinate the behavior of multiple agents and
encourage them to visit the under-explored regions of their joint state space
has not been considered. In this case, we propose Multi-agent Deep Covering
Option Discovery, which constructs the multi-agent options through minimizing
the expected cover time of the multiple agents&apos; joint state space. Also, we
propose a novel framework to adopt the multi-agent options in the MARL process.
In practice, a multi-agent task can usually be divided into some sub-tasks,
each of which can be completed by a sub-group of the agents. Therefore, our
algorithm framework first leverages an attention mechanism to find
collaborative agent sub-groups that would benefit most from coordinated
actions. Then, a hierarchical algorithm, namely HA-MSAC, is developed to learn
the multi-agent options for each sub-group to complete their sub-tasks first,
and then to integrate them through a high-level policy as the solution of the
whole task. This hierarchical option construction allows our framework to
strike a balance between scalability and effective collaboration among the
agents. The evaluation based on multi-agent collaborative tasks shows that the
proposed algorithm can effectively capture the agent interactions with the
attention mechanism, successfully identify multi-agent options, and
significantly outperforms prior works using single-agent options or no options,
in terms of both faster exploration and higher task rewards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1&quot;&gt;Marina Haliem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03297">
<title>Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems. (arXiv:2210.03297v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03297</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-based attacks construct adversarial examples against a machine
learning (ML) model by making only hard-label queries. These attacks have
mainly been applied directly to standalone neural networks. However, in
practice, ML models are just one component of a larger learning system. We find
that by adding a single preprocessor in front of a classifier, state-of-the-art
query-based attacks are up to 7$\times$ less effective at attacking a
prediction pipeline than at attacking the model alone. We explain this
discrepancy by the fact that most preprocessors introduce some notion of
invariance to the input space. Hence, attacks that are unaware of this
invariance inevitably waste a large number of queries to re-discover or
overcome it. We, therefore, develop techniques to (i) reverse-engineer the
preprocessor and then (ii) use this extracted information to attack the
end-to-end system. Our preprocessors extraction method requires only a few
hundred queries, and our preprocessor-aware attacks recover the same efficacy
as when attacking the model alone. The code can be found at
https://github.com/google-research/preprocessor-aware-black-box-attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1&quot;&gt;Chawin Sitawarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1&quot;&gt;Florian Tram&amp;#xe8;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11589">
<title>Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization. (arXiv:2210.11589v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11589</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning systems are often applied to data that is drawn from a
different distribution than the training distribution. Recent work has shown
that for a variety of classification and signal reconstruction problems, the
out-of-distribution performance is strongly linearly correlated with the
in-distribution performance. If this relationship or more generally a monotonic
one holds, it has important consequences. For example, it allows to optimize
performance on one distribution as a proxy for performance on the other. In
this paper, we study conditions under which a monotonic relationship between
the performances of a model on two distributions is expected. We prove an exact
asymptotic linear relation for squared error and a monotonic relation for
misclassification error for ridge-regularized general linear models under
covariate shift, as well as an approximate linear relation for linear inverse
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1&quot;&gt;Daniel LeJeune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiayu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heckel_R/0/1/0/all/0/1&quot;&gt;Reinhard Heckel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00211">
<title>A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process. (arXiv:2212.00211v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00211</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning rich skills through temporal abstractions without supervision of
external rewards is at the frontier of Reinforcement Learning research.
Existing works mainly fall into two distinctive categories: variational and
Laplacian-based skill (a.k.a., option) discovery. The former maximizes the
diversity of the discovered options through a mutual information loss but
overlooks coverage of the state space, while the latter focuses on improving
the coverage of options by increasing connectivity during exploration, but does
not consider diversity. In this paper, we propose a unified framework that
quantifies diversity and coverage through a novel use of the Determinantal
Point Process (DPP) and enables unsupervised option discovery explicitly
optimizing both objectives. Specifically, we define the DPP kernel matrix with
the Laplacian spectrum of the state transition graph and use the expected mode
number in the trajectories as the objective to capture and enhance both
diversity and coverage of the learned options. The proposed option discovery
algorithm is extensively evaluated using challenging tasks built with Mujoco
and Atari, demonstrating that our proposed algorithm substantially outperforms
SOTA baselines from both diversity- and coverage-driven categories. The codes
are available at https://github.com/LucasCJYSDL/ODPP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08736">
<title>A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem. (arXiv:2212.08736v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08736</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the inverse acoustic obstacle problem for sound-soft star-shaped
obstacles in two dimensions wherein the boundary of the obstacle is determined
from measurements of the scattered field at a collection of receivers outside
the object. One of the standard approaches for solving this problem is to
reformulate it as an optimization problem: finding the boundary of the domain
that minimizes the $L^2$ distance between computed values of the scattered
field and the given measurement data. The optimization problem is
computationally challenging since the local set of convexity shrinks with
increasing frequency and results in an increasing number of local minima in the
vicinity of the true solution. In many practical experimental settings, low
frequency measurements are unavailable due to limitations of the experimental
setup or the sensors used for measurement. Thus, obtaining a good initial guess
for the optimization problem plays a vital role in this environment.
&lt;/p&gt;
&lt;p&gt;We present a neural network warm-start approach for solving the inverse
scattering problem, where an initial guess for the optimization problem is
obtained using a trained neural network. We demonstrate the effectiveness of
our method with several numerical examples. For high frequency problems, this
approach outperforms traditional iterative methods such as Gauss-Newton
initialized without any prior (i.e., initialized using a unit circle), or
initialized using the solution of a direct method such as the linear sampling
method. The algorithm remains robust to noise in the scattered field
measurements and also converges to the true solution for limited aperture data.
However, the number of training samples required to train the neural network
scales exponentially in frequency and the complexity of the obstacles
considered. We conclude with a discussion of this phenomenon and potential
directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiequn Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rachh_M/0/1/0/all/0/1&quot;&gt;Manas Rachh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Borges_C/0/1/0/all/0/1&quot;&gt;Carlos Borges&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12606">
<title>A Convergence Rate for Manifold Neural Networks. (arXiv:2212.12606v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12606</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional data arises in numerous applications, and the rapidly
developing field of geometric deep learning seeks to develop neural network
architectures to analyze such data in non-Euclidean domains, such as graphs and
manifolds. Recent work by Z. Wang, L. Ruiz, and A. Ribeiro has introduced a
method for constructing manifold neural networks using the spectral
decomposition of the Laplace Beltrami operator. Moreover, in this work, the
authors provide a numerical scheme for implementing such neural networks when
the manifold is unknown and one only has access to finitely many sample points.
The authors show that this scheme, which relies upon building a data-driven
graph, converges to the continuum limit as the number of sample points tends to
infinity. Here, we build upon this result by establishing a rate of convergence
that depends on the intrinsic dimension of the manifold but is independent of
the ambient dimension. We also discuss how the rate of convergence depends on
the depth of the network and the number of filters used in each layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chew_J/0/1/0/all/0/1&quot;&gt;Joyce Chew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlmutter_M/0/1/0/all/0/1&quot;&gt;Michael Perlmutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.09559">
<title>SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.09559</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks (NNs) have various applications in AI, but explaining their
decisions remains challenging. Existing approaches often focus on explaining
how changing individual inputs affects NNs&apos; outputs. However, an explanation
that is consistent with the input-output behaviour of an NN is not necessarily
faithful to the actual mechanics thereof. In this paper, we exploit
relationships between multi-layer perceptrons (MLPs) and quantitative
argumentation frameworks (QAFs) to create argumentative explanations for the
mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining
as much of the original structure as possible. It then translates the sparse
MLP into an equivalent QAF to shed light on the underlying decision process of
the MLP, producing global and/or local explanations. We demonstrate
experimentally that SpArX can give more faithful explanations than existing
approaches, while simultaneously providing deeper insights into the actual
reasoning process of MLPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayoobi_H/0/1/0/all/0/1&quot;&gt;Hamed Ayoobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1&quot;&gt;Nico Potyka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13807">
<title>Identifying the Hazard Boundary of ML-enabled Autonomous Systems Using Cooperative Co-Evolutionary Search. (arXiv:2301.13807v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13807</link>
<description rdf:parseType="Literal">&lt;p&gt;In Machine Learning (ML)-enabled autonomous systems (MLASs), it is essential
to identify the hazard boundary of ML Components (MLCs) in the MLAS under
analysis. Given that such boundary captures the conditions in terms of MLC
behavior and system context that can lead to hazards, it can then be used to,
for example, build a safety monitor that can take any predefined fallback
mechanisms at runtime when reaching the hazard boundary. However, determining
such hazard boundary for an ML component is challenging. This is due to the
problem space combining system contexts (i.e., scenarios) and MLC behaviors
(i.e., inputs and outputs) being far too large for exhaustive exploration and
even to handle using conventional metaheuristics, such as genetic algorithms.
Additionally, the high computational cost of simulations required to determine
any MLAS safety violations makes the problem even more challenging.
Furthermore, it is unrealistic to consider a region in the problem space
deterministically safe or unsafe due to the uncontrollable parameters in
simulations and the non-linear behaviors of ML models (e.g., deep neural
networks) in the MLAS under analysis. To address the challenges, we propose
MLCSHE (ML Component Safety Hazard Envelope), a novel method based on a
Cooperative Co-Evolutionary Algorithm (CCEA), which aims to tackle a
high-dimensional problem by decomposing it into two lower-dimensional search
subproblems. Moreover, we take a probabilistic view of safe and unsafe regions
and define a novel fitness function to measure the distance from the
probabilistic hazard boundary and thus drive the search effectively. We
evaluate the effectiveness and efficiency of MLCSHE on a complex Autonomous
Vehicle (AV) case study. Our evaluation results show that MLCSHE is
significantly more effective and efficient compared to a standard genetic
algorithm and random search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifi_S/0/1/0/all/0/1&quot;&gt;Sepehr Sharifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1&quot;&gt;Donghwan Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1&quot;&gt;Lionel C. Briand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aschbacher_N/0/1/0/all/0/1&quot;&gt;Nathan Aschbacher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04246">
<title>Shortcut Detection with Variational Autoencoders. (arXiv:2302.04246v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;For real-world applications of machine learning (ML), it is essential that
models make predictions based on well-generalizing features rather than
spurious correlations in the data. The identification of such spurious
correlations, also known as shortcuts, is a challenging problem and has so far
been scarcely addressed. In this work, we present a novel approach to detect
shortcuts in image and audio datasets by leveraging variational autoencoders
(VAEs). The disentanglement of features in the latent space of VAEs allows us
to discover feature-target correlations in datasets and semi-automatically
evaluate them for ML shortcuts. We demonstrate the applicability of our method
on several real-world datasets and identify shortcuts that have not been
discovered before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_N/0/1/0/all/0/1&quot;&gt;Nicolas M. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roschmann_S/0/1/0/all/0/1&quot;&gt;Simon Roschmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperl_P/0/1/0/all/0/1&quot;&gt;Philip Sperl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1&quot;&gt;Konstantin B&amp;#xf6;ttinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04973">
<title>Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames. (arXiv:2302.04973v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04973</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically discovering composable abstractions from raw perceptual data is
a long-standing challenge in machine learning. Recent slot-based neural
networks that learn about objects in a self-supervised manner have made
exciting progress in this direction. However, they typically fall short at
adequately capturing spatial symmetries present in the visual world, which
leads to sample inefficiency, such as when entangling object appearance and
pose. In this paper, we present a simple yet highly effective method for
incorporating spatial symmetries via slot-centric reference frames. We
incorporate equivariance to per-object pose transformations into the attention
and generation mechanism of Slot Attention by translating, scaling, and
rotating position encodings. These changes result in little computational
overhead, are easy to implement, and can result in large gains in terms of data
efficiency and overall improvements to object discovery. We evaluate our method
on a wide range of synthetic object discovery benchmarks namely CLEVR,
Tetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising
improvements on the challenging real-world Waymo Open dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1&quot;&gt;Ondrej Biza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1&quot;&gt;Sjoerd van Steenkiste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1&quot;&gt;Mehdi S. M. Sajjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1&quot;&gt;Gamaleldin F. Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1&quot;&gt;Aravindh Mahendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1&quot;&gt;Thomas Kipf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08647">
<title>Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures. (arXiv:2302.08647v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08647</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary graph learning algorithms are not well-defined for large
molecules since they do not consider the hierarchical interactions among the
atoms, which are essential to determine the molecular properties of
macromolecules. In this work, we propose Multiresolution Graph Transformers
(MGT), the first graph transformer architecture that can learn to represent
large molecules at multiple scales. MGT can learn to produce representations
for the atoms and group them into meaningful functional groups or repeating
units. We also introduce Wavelet Positional Encoding (WavePE), a new positional
encoding method that can guarantee localization in both spectral and spatial
domains. Our proposed model achieves competitive results on two macromolecule
datasets consisting of polymers and peptides, and one drug-like molecule
dataset. Importantly, our model outperforms other state-of-the-art methods and
achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO
and LUMO) calculated by Density Functional Theory (DFT) in the polymers
dataset. Furthermore, the visualizations, including clustering results on
macromolecules and low-dimensional spaces of their representations, demonstrate
the capability of our methodology in learning to represent long-range and
hierarchical structures. Our PyTorch implementation is publicly available at
https://github.com/HySonLab/Multires-Graph-Transformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_N/0/1/0/all/0/1&quot;&gt;Nhat Khang Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1&quot;&gt;Truong Son Hy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondor_R/0/1/0/all/0/1&quot;&gt;Risi Kondor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09738">
<title>Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning. (arXiv:2302.09738v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09738</link>
<description rdf:parseType="Literal">&lt;p&gt;Riemannian submanifold optimization with momentum is computationally
challenging because, to ensure that the iterates remain on the submanifold, we
often need to solve difficult differential equations. Here, we simplify such
difficulties for a class of structured symmetric positive-definite matrices
with the affine-invariant metric. We do so by proposing a generalized version
of the Riemannian normal coordinates that dynamically orthonormalizes the
metric and locally converts the problem into an unconstrained problem in the
Euclidean space. We use our approach to simplify existing approaches for
structured covariances and develop matrix-inverse-free $2^\text{nd}$-order
optimizers for deep learning with low precision by using only matrix
multiplications. Code: https://github.com/yorkerlin/StructuredNGD-DL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duruisseaux_V/0/1/0/all/0/1&quot;&gt;Valentin Duruisseaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Leok_M/0/1/0/all/0/1&quot;&gt;Melvin Leok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1&quot;&gt;Frank Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1&quot;&gt;Mark Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10870">
<title>On Provable Copyright Protection for Generative Models. (arXiv:2302.10870v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10870</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing concern that learned conditional generative models may
output samples that are substantially similar to some copyrighted data $C$ that
was in their training set. We give a formal definition of $\textit{near
access-freeness (NAF)}$ and prove bounds on the probability that a model
satisfying this definition outputs a sample similar to $C$, even if $C$ is
included in its training set. Roughly speaking, a generative model $p$ is
$\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of
$p$ diverges by at most $k$-bits from the output of a model $q$ that
$\textit{did not access $C$ at all}$. We also give generative model learning
algorithms, which efficiently modify the original generative model learning
algorithm in a black box manner, that output generative models with strong
bounds on the probability of sampling protected content. Furthermore, we
provide promising experiments for both language (transformers) and image
(diffusion) generative models, showing minimal degradation in output quality
while ensuring strong protections against sampling protected content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyas_N/0/1/0/all/0/1&quot;&gt;Nikhil Vyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1&quot;&gt;Boaz Barak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03327">
<title>Tight Bounds for $\gamma$-Regret via the Decision-Estimation Coefficient. (arXiv:2303.03327v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03327</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we give a statistical characterization of the $\gamma$-regret
for arbitrary structured bandit problems, the regret which arises when
comparing against a benchmark that is $\gamma$ times the optimal solution. The
$\gamma$-regret emerges in structured bandit problems over a function class
$\mathcal{F}$ where finding an exact optimum of $f \in \mathcal{F}$ is
intractable. Our characterization is given in terms of the $\gamma$-DEC, a
statistical complexity parameter for the class $\mathcal{F}$, which is a
modification of the constrained Decision-Estimation Coefficient (DEC) of Foster
et al., 2023 (and closely related to the original offset DEC of Foster et al.,
2021). Our lower bound shows that the $\gamma$-DEC is a fundamental limit for
any model class $\mathcal{F}$: for any algorithm, there exists some $f \in
\mathcal{F}$ for which the $\gamma$-regret of that algorithm scales (nearly)
with the $\gamma$-DEC of $\mathcal{F}$. We provide an upper bound showing that
there exists an algorithm attaining a nearly matching $\gamma$-regret. Due to
significant challenges in applying the prior results on the DEC to the
$\gamma$-regret case, both our lower and upper bounds require novel techniques
and a new algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasgow_M/0/1/0/all/0/1&quot;&gt;Margalit Glasgow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1&quot;&gt;Alexander Rakhlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06067">
<title>Modeling Events and Interactions through Temporal Processes -- A Survey. (arXiv:2303.06067v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06067</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world scenario, many phenomena produce a collection of events that
occur in continuous time. Point Processes provide a natural mathematical
framework for modeling these sequences of events. In this survey, we
investigate probabilistic models for modeling event sequences through temporal
processes. We revise the notion of event modeling and provide the mathematical
foundations that characterize the literature on the topic. We define an
ontology to categorize the existing approaches in terms of three families:
simple, marked, and spatio-temporal point processes. For each family, we
systematically review the existing approaches based based on deep learning.
Finally, we analyze the scenarios where the proposed techniques can be used for
addressing prediction and modeling aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liguori_A/0/1/0/all/0/1&quot;&gt;Angelica Liguori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caroprese_L/0/1/0/all/0/1&quot;&gt;Luciano Caroprese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minici_M/0/1/0/all/0/1&quot;&gt;Marco Minici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1&quot;&gt;Bruno Veloso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spinnato_F/0/1/0/all/0/1&quot;&gt;Francesco Spinnato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanni_M/0/1/0/all/0/1&quot;&gt;Mirco Nanni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manco_G/0/1/0/all/0/1&quot;&gt;Giuseppe Manco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gama_J/0/1/0/all/0/1&quot;&gt;Joao Gama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06146">
<title>StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces. (arXiv:2303.06146v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in face manipulation using StyleGAN have produced impressive
results. However, StyleGAN is inherently limited to cropped aligned faces at a
fixed image resolution it is pre-trained on. In this paper, we propose a simple
and effective solution to this limitation by using dilated convolutions to
rescale the receptive fields of shallow layers in StyleGAN, without altering
any model parameters. This allows fixed-size small features at shallow layers
to be extended into larger ones that can accommodate variable resolutions,
making them more robust in characterizing unaligned faces. To enable real face
inversion and manipulation, we introduce a corresponding encoder that provides
the first-layer feature of the extended StyleGAN in addition to the latent
style code. We validate the effectiveness of our method using unaligned face
inputs of various resolutions in a diverse set of face manipulation tasks,
including facial attribute editing, super-resolution, sketch/mask-to-face
translation, and face toonification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Liming Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09975">
<title>MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09975</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been exploding interest in embracing Transformer-based
architectures for medical image segmentation. However, the lack of large-scale
annotated medical datasets make achieving performances equivalent to those in
natural images challenging. Convolutional networks, in contrast, have higher
inductive biases and consequently, are easily trainable to high performance.
Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet
by mirroring Transformer blocks. In this work, we improve upon this to design a
modernized and scalable convolutional architecture customized to challenges of
data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired
large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D
Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up
and downsampling blocks to preserve semantic richness across scales, 3) A novel
technique to iteratively increase kernel sizes by upsampling small kernel
networks, to prevent performance saturation on limited medical data, 4)
Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt.
This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities
and varying dataset sizes, representing a modernized deep architecture for
medical image segmentation. Our code is made publicly available at:
https://github.com/MIC-DKFZ/MedNeXt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Saikat Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Koehler_G/0/1/0/all/0/1&quot;&gt;Gregor Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ulrich_C/0/1/0/all/0/1&quot;&gt;Constantin Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1&quot;&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1&quot;&gt;Jens Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15471">
<title>Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football. (arXiv:2303.15471v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15471</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence has been used to help human complete difficult tasks
in complicated environments by providing optimized strategies for
decision-making or replacing the manual labour. In environments including
multiple agents, such as football, the most common methods to train agents are
Imitation Learning and Multi-Agent Reinforcement Learning (MARL). However, the
agents trained by Imitation Learning cannot outperform the expert demonstrator,
which makes humans hardly get new insights from the learnt policy. Besides,
MARL is prone to the credit assignment problem. In environments with sparse
reward signal, this method can be inefficient. The objective of our research is
to create a novel reward shaping method by embedding contextual information in
reward function to solve the aforementioned challenges. We demonstrate this in
the Google Research Football (GRF) environment. We quantify the contextual
information extracted from game state observation and use this quantification
together with original sparse reward to create the shaped reward. The
experiment results in the GRF environment prove that our reward shaping method
is a useful addition to state-of-the-art MARL algorithms for training agents in
environments with sparse reward signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1&quot;&gt;Varuna De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artaud_C/0/1/0/all/0/1&quot;&gt;Corentin Artaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pina_R/0/1/0/all/0/1&quot;&gt;Rafael Pina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17555">
<title>Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17555</link>
<description rdf:parseType="Literal">&lt;p&gt;Intersectionality is a critical framework that, through inquiry and praxis,
allows us to examine how social inequalities persist through domains of
structure and discipline. Given AI fairness&apos; raison d&apos;etre of &quot;fairness&quot;, we
argue that adopting intersectionality as an analytical framework is pivotal to
effectively operationalizing fairness. Through a critical review of how
intersectionality is discussed in 30 papers from the AI fairness literature, we
deductively and inductively: 1) map how intersectionality tenets operate within
the AI fairness paradigm and 2) uncover gaps between the conceptualization and
operationalization of intersectionality. We find that researchers
overwhelmingly reduce intersectionality to optimizing for fairness metrics over
demographic subgroups. They also fail to discuss their social context and when
mentioning power, they mostly situate it only within the AI pipeline. We: 3)
outline and assess the implications of these gaps for critical inquiry and
praxis, and 4) provide actionable recommendations for AI fairness researchers
to engage with intersectionality in their work by grounding it in AI
epistemology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1&quot;&gt;Anaelia Ovalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1&quot;&gt;Arjun Subramonian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_V/0/1/0/all/0/1&quot;&gt;Vagrant Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_G/0/1/0/all/0/1&quot;&gt;Gilbert Gee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04250">
<title>Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04250</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for making high-quality recommendations often rely on learning latent
representations from interaction data. These methods, while performant, do not
provide ready mechanisms for users to control the recommendation they receive.
Our work tackles this problem by proposing LACE, a novel concept value
bottleneck model for controllable text recommendations. LACE represents each
user with a succinct set of human-readable concepts through retrieval given
user-interacted documents and learns personalized representations of the
concepts based on user documents. This concept based user profile is then
leveraged to make recommendations. The design of our model affords control over
the recommendations through a number of intuitive interactions with a
transparent user profile. We first establish the quality of recommendations
obtained from LACE in an offline evaluation on three recommendation tasks
spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we
validate the controllability of LACE under simulated user interactions.
Finally, we implement LACE in an interactive controllable recommender system
and conduct a user study to demonstrate that users are able to improve the
quality of recommendations they receive through interactions with an editable
user profile.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1&quot;&gt;Sheshera Mysore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jasim_M/0/1/0/all/0/1&quot;&gt;Mahmood Jasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1&quot;&gt;Andrew McCallum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1&quot;&gt;Hamed Zamani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14118">
<title>Learning Neural PDE Solvers with Parameter-Guided Channel Attention. (arXiv:2304.14118v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14118</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific Machine Learning (SciML) is concerned with the development of
learned emulators of physical systems governed by partial differential
equations (PDE). In application domains such as weather forecasting, molecular
dynamics, and inverse design, ML-based surrogate models are increasingly used
to augment or replace inefficient and often non-differentiable numerical
simulation algorithms. While a number of ML-based methods for approximating the
solutions of PDEs have been proposed in recent years, they typically do not
adapt to the parameters of the PDEs, making it difficult to generalize to PDE
parameters not seen during training. We propose a Channel Attention mechanism
guided by PDE Parameter Embeddings (CAPE) component for neural surrogate models
and a simple yet effective curriculum learning strategy. The CAPE module can be
combined with neural PDE solvers allowing them to adapt to unseen PDE
parameters. The curriculum learning strategy provides a seamless transition
between teacher-forcing and fully auto-regressive training. We compare CAPE in
conjunction with the curriculum learning strategy using a popular PDE benchmark
and obtain consistent and significant improvements over the baseline models.
The experiments also show several advantages of CAPE, such as its increased
ability to generalize to unseen PDE parameters without large increases
inference time and parameter count.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takamoto_M/0/1/0/all/0/1&quot;&gt;Makoto Takamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alesiani_F/0/1/0/all/0/1&quot;&gt;Francesco Alesiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13503">
<title>Asynchronous Multi-Model Dynamic Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13503</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has emerged as a key technique for distributed
machine learning (ML). Most literature on FL has focused on ML model training
for (i) a single task/model, with (ii) a synchronous scheme for uplink/downlink
transfer of model parameters, and (iii) a static data distribution setting
across devices. These assumptions are often not well representative of
conditions encountered in practical FL environments. To address this, we
develop DMA-FL, which considers dynamic FL with multiple downstream tasks to be
trained over an asynchronous model transmission architecture. We first
characterize the convergence of ML model training under DMA-FL via introducing
a family of scheduling tensors and rectangular functions to capture the
scheduling of devices. Our convergence analysis sheds light on the impact of
resource allocation, device scheduling, and individual model states on the
performance of ML models. We then formulate a non-convex mixed integer
optimization problem for jointly configuring the resource allocation and device
scheduling to strike an efficient trade-off between energy consumption and ML
performance. We develop a solution methodology employing successive convex
approximations with convergence guarantee to a stationary point. Through
numerical simulations, we reveal the advantages of DMA-FL in terms of model
performance and network resource savings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Zhan-Lun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1&quot;&gt;Seyyedali Hosseinalipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1&quot;&gt;Mung Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15342">
<title>Is Your Model &quot;MADD&quot;? A Novel Metric to Evaluate Algorithmic Fairness for Predictive Student Models. (arXiv:2305.15342v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15342</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive student models are increasingly used in learning environments due
to their ability to enhance educational outcomes and support stakeholders in
making informed decisions. However, predictive models can be biased and produce
unfair outcomes, leading to potential discrimination against some students and
possible harmful long-term implications. This has prompted research on fairness
metrics meant to capture and quantify such biases. Nonetheless, so far,
existing fairness metrics used in education are predictive
performance-oriented, focusing on assessing biased outcomes across groups of
students, without considering the behaviors of the models nor the severity of
the biases in the outcomes. Therefore, we propose a novel metric, the Model
Absolute Density Distance (MADD), to analyze models&apos; discriminatory behaviors
independently from their predictive performance. We also provide a
complementary visualization-based analysis to enable fine-grained human
assessment of how the models discriminate between groups of students. We
evaluate our approach on the common task of predicting student success in
online courses, using several common predictive classification models on an
open educational dataset. We also compare our metric to the only predictive
performance-oriented fairness metric developed in education, ABROCA. Results on
this dataset show that: (1) fair predictive performance does not guarantee fair
models&apos; behaviors and thus fair outcomes, (2) there is no direct relationship
between data bias and predictive performance bias nor discriminatory behaviors
bias, and (3) trained on the same data, models exhibit different discriminatory
behaviors, according to different sensitive features too. We thus recommend
using the MADD on models that show satisfying predictive performance, to gain a
finer-grained understanding on how they behave and to refine models selection
and their usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verger_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lina Verger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalle_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lall&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchet_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Bouchet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luengo_V/0/1/0/all/0/1&quot;&gt;Vanda Luengo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17282">
<title>Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. II. (arXiv:2305.17282v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17282</link>
<description rdf:parseType="Literal">&lt;p&gt;We continue to investigate the $k$ nearest neighbour learning rule in
separable metric spaces. Thanks to the results of C\&apos;erou and Guyader (2006)
and Preiss (1983), this rule is known to be universally consistent in every
metric space $X$ that is sigma-finite dimensional in the sense of Nagata. Here
we show that the rule is strongly universally consistent in such spaces in the
absence of ties. Under the tie-breaking strategy applied by Devroye,
Gy\&quot;{o}rfi, Krzy\.{z}ak, and Lugosi (1994) in the Euclidean setting, we manage
to show the strong universal consistency in non-Archimedian metric spaces (that
is, those of Nagata dimension zero). Combining the theorem of C\&apos;erou and
Guyader with results of Assouad and Quentin de Gromard (2006), one deduces that
the $k$-NN rule is universally consistent in metric spaces having finite
dimension in the sense of de Groot. In particular, the $k$-NN rule is
universally consistent in the Heisenberg group which is not sigma-finite
dimensional in the sense of Nagata as follows from an example independently
constructed by Kor\&apos;anyi and Reimann (1995) and Sawyer and Wheeden (1992).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1&quot;&gt;Sushma Kumari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pestov_V/0/1/0/all/0/1&quot;&gt;Vladimir G. Pestov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18451">
<title>Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18451</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, molecular relational learning, whose goal is to predict the
interaction behavior between molecular pairs, got a surge of interest in
molecular sciences due to its wide range of applications. In this work, we
propose CMRL that is robust to the distributional shift in molecular relational
learning by detecting the core substructure that is causally related to
chemical reactions. To do so, we first assume a causal relationship based on
the domain knowledge of molecular sciences and construct a structural causal
model (SCM) that reveals the relationship between variables. Based on the SCM,
we introduce a novel conditional intervention framework whose intervention is
conditioned on the paired molecule. With the conditional intervention
framework, our model successfully learns from the causal substructure and
alleviates the confounding effect of shortcut substructures that are spuriously
correlated to chemical reactions. Extensive experiments on various tasks with
real-world and synthetic datasets demonstrate the superiority of CMRL over
state-of-the-art baseline models. Our code is available at
https://github.com/Namkyeong/CMRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Namkyeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_G/0/1/0/all/0/1&quot;&gt;Gyoung S. Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sein Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18453">
<title>Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18453</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for artificial intelligence (AI) in healthcare is rapidly
increasing. However, significant challenges arise from data scarcity and
privacy concerns, particularly in medical imaging. While existing generative
models have achieved success in image synthesis and image-to-image translation
tasks, there remains a gap in the generation of 3D semantic medical images. To
address this gap, we introduce Med-DDPM, a diffusion model specifically
designed for semantic 3D medical image synthesis, effectively tackling data
scarcity and privacy issues. The novelty of Med-DDPM lies in its incorporation
of semantic conditioning, enabling precise control during the image generation
process. Our model outperforms Generative Adversarial Networks (GANs) in terms
of stability and performance, generating diverse and anatomically coherent
images with high visual fidelity. Comparative analysis against state-of-the-art
augmentation techniques demonstrates that Med-DDPM produces comparable results,
highlighting its potential as a data augmentation tool for enhancing model
accuracy. In conclusion, Med-DDPM pioneers 3D semantic medical image synthesis
by delivering high-quality and anatomically coherent images. Furthermore, the
integration of semantic conditioning with Med-DDPM holds promise for image
anonymization in the field of biomedical imaging, showcasing the capabilities
of the model in addressing challenges related to data scarcity and privacy
concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorjsembe_Z/0/1/0/all/0/1&quot;&gt;Zolnamar Dorjsembe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pao_H/0/1/0/all/0/1&quot;&gt;Hsing-Kuo Pao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Odonchimed_S/0/1/0/all/0/1&quot;&gt;Sodtavilan Odonchimed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Furen Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00988">
<title>Continual Learning for Abdominal Multi-Organ and Tumor Segmentation. (arXiv:2306.00988v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00988</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to dynamically extend a model to new data and classes is critical
for multiple organ and tumor segmentation. However, due to privacy regulations,
accessing previous data and annotations can be problematic in the medical
domain. This poses a significant barrier to preserving the high segmentation
accuracy of the old classes when learning from new classes because of the
catastrophic forgetting problem. In this paper, we first empirically
demonstrate that simply using high-quality pseudo labels can fairly mitigate
this problem in the setting of organ segmentation. Furthermore, we put forward
an innovative architecture designed specifically for continuous organ and tumor
segmentation, which incurs minimal computational overhead. Our proposed design
involves replacing the conventional output layer with a suite of lightweight,
class-specific heads, thereby offering the flexibility to accommodate newly
emerging classes. These heads enable independent predictions for newly
introduced and previously learned classes, effectively minimizing the impact of
new classes on old ones during the course of continual learning. We further
propose incorporating Contrastive Language-Image Pretraining (CLIP) embeddings
into the organ-specific heads. These embeddings encapsulate the semantic
information of each class, informed by extensive image-text co-training. The
proposed method is evaluated on both in-house and public abdominal CT datasets
under organ and tumor segmentation tasks. Empirical results suggest that the
proposed design improves the segmentation performance of a baseline neural
network on newly-introduced and previously-learned classes along the learning
trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huimiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01639">
<title>Reduction of finite sampling noise in quantum neural networks. (arXiv:2306.01639v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01639</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum neural networks (QNNs) use parameterized quantum circuits with
data-dependent inputs and generate outputs through the evaluation of
expectation values. Calculating these expectation values necessitates repeated
circuit evaluations, thus introducing fundamental finite-sampling noise even on
error-free quantum computers. We reduce this noise by introducing the variance
regularization, a technique for reducing the variance of the expectation value
during the quantum model training. This technique requires no additional
circuit evaluations if the QNN is properly constructed. Our empirical findings
demonstrate the reduced variance speeds up the training and lowers the output
noise as well as decreases the number of necessary evaluations of gradient
circuits. This regularization method is benchmarked on the regression of
multiple functions. We show that in our examples, it lowers the variance by an
order of magnitude on average and leads to a significantly reduced noise level
of the QNN. We finally demonstrate QNN training on a real quantum device and
evaluate the impact of error mitigation. Here, the optimization is feasible
only due to the reduced number of necessary shots in the gradient evaluation
resulting from the reduced variance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kreplin_D/0/1/0/all/0/1&quot;&gt;David A. Kreplin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Roth_M/0/1/0/all/0/1&quot;&gt;Marco Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03933">
<title>High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03933</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for anomaly detection of new physics processes are often limited to
low-dimensional spaces due to the difficulty of learning high-dimensional
probability densities. Particularly at the constituent level, incorporating
desirable properties such as permutation invariance and variable-length inputs
becomes difficult within popular density estimation methods. In this work, we
introduce a permutation-invariant density estimator for particle physics data
based on diffusion models, specifically designed to handle variable-length
inputs. We demonstrate the efficacy of our methodology by utilizing the learned
density as a permutation-invariant anomaly detection score, effectively
identifying jets with low likelihood under the background-only hypothesis. To
validate our density estimation method, we investigate the ratio of learned
densities and compare to those obtained by a supervised classification
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Mikuni_V/0/1/0/all/0/1&quot;&gt;Vinicius Mikuni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Nachman_B/0/1/0/all/0/1&quot;&gt;Benjamin Nachman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07308">
<title>Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior. (arXiv:2306.07308v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07308</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral
bands, conveying a wealth of spatial and spectral information. However, due to
the instrumental errors and the atmospheric changes, the HSI obtained in
practice are often contaminated by noise and dead pixels(lines), resulting in
missing information that may severely compromise the subsequent applications.
We introduce here a novel HSI missing pixel prediction algorithm, called Low
Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP
is able to predict missing pixels and bands even when all spectral bands of the
image are missing. The proposed LRS-PnP algorithm is further extended to a
self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP),
called LRS-PnP-DIP. In a series of experiments with real data, It is shown that
the LRS-PnP-DIP either achieves state-of-the-art inpainting performance
compared to other learning-based methods, or outperforms them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yaghoobi_M/0/1/0/all/0/1&quot;&gt;Mehrdad Yaghoobi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09087">
<title>Deep learning based Meta-modeling for Multi-objective Technology Optimization of Electrical Machines. (arXiv:2306.09087v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09087</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization of rotating electrical machines is both time- and
computationally expensive. Because of the different parametrization, design
optimization is commonly executed separately for each machine technology. In
this paper, we present the application of a variational auto-encoder (VAE) to
optimize two different machine technologies simultaneously, namely an
asynchronous machine and a permanent magnet synchronous machine. After
training, we employ a deep neural network and a decoder as meta-models to
predict global key performance indicators (KPIs) and generate associated new
designs, respectively, through unified latent space in the optimization loop.
Numerical results demonstrate concurrent parametric multi-objective technology
optimization in the high-dimensional design space. The VAE-based approach is
quantitatively compared to a classical deep learning-based direct approach for
KPIs prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1&quot;&gt;Vivek Parekh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flore_D/0/1/0/all/0/1&quot;&gt;Dominik Flore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schops_S/0/1/0/all/0/1&quot;&gt;Sebastian Sch&amp;#xf6;ps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09260">
<title>IsoEx: an explainable unsupervised approach to process event logs cyber investigation. (arXiv:2306.09260v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09260</link>
<description rdf:parseType="Literal">&lt;p&gt;39 seconds. That is the timelapse between two consecutive cyber attacks as of
2023. Meaning that by the time you are done reading this abstract, about 1 or 2
additional cyber attacks would have occurred somewhere in the world. In this
context of highly increased frequency of cyber threats, Security Operation
Centers (SOC) and Computer Emergency Response Teams (CERT) can be overwhelmed.
In order to relieve the cybersecurity teams in their investigative effort and
help them focus on more added-value tasks, machine learning approaches and
methods started to emerge. This paper introduces a novel method, IsoEx, for
detecting anomalous and potentially problematic command lines during the
investigation of contaminated devices. IsoEx is built around a set of features
that leverages the log structure of the command line, as well as its
parent/child relationship, to achieve a greater accuracy than traditional
methods. To detect anomalies, IsoEx resorts to an unsupervised anomaly
detection technique that is both highly sensitive and lightweight. A key
contribution of the paper is its emphasis on interpretability, achieved through
the features themselves and the application of eXplainable Artificial
Intelligence (XAI) techniques and visualizations. This is critical to ensure
the adoption of the method by SOC and CERT teams, as the paper argues that the
current literature on machine learning for log investigation has not adequately
addressed the issue of explainability. This method was proven efficient in a
real-life environment as it was built to support a company\&apos;s SOC and CERT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavieille_P/0/1/0/all/0/1&quot;&gt;Pierre Lavieille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atlas_I/0/1/0/all/0/1&quot;&gt;Ismail Alaoui Hassani Atlas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09382">
<title>Sound Demixing Challenge 2023 Music Demixing Track Technical Report: TFC-TDF-UNet v3. (arXiv:2306.09382v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09382</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we present our award-winning solutions for the Music Demixing
Track of Sound Demixing Challenge 2023. First, we propose TFC-TDF-UNet v3, a
time-efficient music source separation model that achieves state-of-the-art
results on the MUSDB benchmark. We then give full details regarding our
solutions for each Leaderboard, including a loss masking approach for
noise-robust training. Code for reproducing model training and final
submissions is available at github.com/kuielab/sdx23.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jun Hyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Soonyoung Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02953">
<title>SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks. (arXiv:2307.02953v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02953</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, U-shaped networks have dominated the field of medical image
segmentation due to their simple and easily tuned structure. However, existing
U-shaped segmentation networks: 1) mostly focus on designing complex
self-attention modules to compensate for the lack of long-term dependence based
on convolution operation, which increases the overall number of parameters and
computational complexity of the network; 2) simply fuse the features of encoder
and decoder, ignoring the connection between their spatial locations. In this
paper, we rethink the above problem and build a lightweight medical image
segmentation network, called SegNetr. Specifically, we introduce a novel
SegNetr block that can perform local-global interactions dynamically at any
stage and with only linear complexity. At the same time, we design a general
information retention skip connection (IRSC) to preserve the spatial location
information of encoder features and achieve accurate fusion with the decoder
features. We validate the effectiveness of SegNetr on four mainstream medical
image segmentation datasets, with 59\% and 76\% fewer parameters and GFLOPs
than vanilla U-Net, while achieving segmentation performance comparable to
state-of-the-art methods. Notably, the components proposed in this paper can be
applied to other U-shaped networks to improve their segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junlong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chengrui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Min Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05825">
<title>Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05825</link>
<description rdf:parseType="Literal">&lt;p&gt;The number of modes in a probability density function is representative of
the model&apos;s complexity and can also be viewed as the number of existing
subpopulations. Despite its relevance, little research has been devoted to its
estimation. Focusing on the univariate setting, we propose a novel approach
targeting prediction accuracy inspired by some overlooked aspects of the
problem. We argue for the need for structure in the solutions, the subjective
and uncertain nature of modes, and the convenience of a holistic view blending
global and local density properties. Our method builds upon a combination of
flexible kernel estimators and parsimonious compositional splines. Feature
exploration, model selection and mode testing are implemented in the Bayesian
inference paradigm, providing soft solutions and allowing to incorporate expert
judgement in the process. The usefulness of our proposal is illustrated through
a case study in sports analytics, showcasing multiple companion visualisation
tools. A thorough simulation study demonstrates that traditional
modality-driven approaches paradoxically struggle to provide accurate results.
In this context, our method emerges as a top-tier alternative offering
innovative solutions for analysts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chacon_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; E. Chac&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Serrano_J/0/1/0/all/0/1&quot;&gt;Javier Fern&amp;#xe1;ndez Serrano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06092">
<title>Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06092</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma&amp;gt;0$, with the exponent
depending on the metric used to measure discrepancy. Our bounds are strictly
stronger in terms of their dependence on network width than any previously
available in the literature; in the one-dimensional case, we also prove that
they are optimal, i.e., we establish matching lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1&quot;&gt;Stefano Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1&quot;&gt;Domenico Marinucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1&quot;&gt;Ivan Nourdin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1&quot;&gt;Giovanni Peccati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06324">
<title>Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06324</link>
<description rdf:parseType="Literal">&lt;p&gt;This work establishes provably faster convergence rates for gradient descent
in smooth convex optimization via a computer-assisted analysis technique. Our
theory allows nonconstant stepsize policies with frequent long steps
potentially violating descent by analyzing the overall effect of many
iterations at once rather than the typical one-iteration inductions used in
most first-order method analyses. We show that long steps, which may increase
the objective value in the short term, lead to provably faster convergence in
the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for
gradient descent is also motivated along with simple numerical validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grimmer_B/0/1/0/all/0/1&quot;&gt;Benjamin Grimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08167">
<title>Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08167</link>
<description rdf:parseType="Literal">&lt;p&gt;When computing the gradients of a quantum neural network using the
parameter-shift rule, the cost function needs to be calculated twice for the
gradient with respect to a single adjustable parameter of the network. When the
total number of parameters is high, the quantum circuit for the computation has
to be adjusted and run for many times. Here we propose an approach to compute
all the gradients using a single circuit only, with a much reduced circuit
depth and less classical registers. We also demonstrate experimentally, on both
real quantum hardware and simulator, that our approach has the advantages that
the circuit takes a significantly shorter time to compile than the conventional
approach, resulting in a speedup on the total runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+He_G/0/1/0/all/0/1&quot;&gt;Guang Ping He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09484">
<title>MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09484</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular knowledge resides within three different modalities of information
sources: molecular structures, biomedical documents, and knowledge bases.
Effective incorporation of molecular knowledge from these modalities holds
paramount significance in facilitating biomedical research. However, existing
multimodal molecular foundation models exhibit limitations in capturing
intricate connections between molecular structures and texts, and more
importantly, none of them attempt to leverage a wealth of molecular expertise
derived from knowledge graphs. In this study, we introduce MolFM, a multimodal
molecular foundation model designed to facilitate joint representation learning
from molecular structures, biomedical texts, and knowledge graphs. We propose
cross-modal attention between atoms of molecular structures, neighbors of
molecule entities and semantically related texts to facilitate cross-modal
comprehension. We provide theoretical analysis that our cross-modal
pre-training captures local and global molecular knowledge by minimizing the
distance in the feature space between different modalities of the same
molecule, as well as molecules sharing similar structures or functions. MolFM
achieves state-of-the-art performance on various downstream tasks. On
cross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04%
absolute gains under the zero-shot and fine-tuning settings, respectively.
Furthermore, qualitative analysis showcases MolFM&apos;s implicit ability to provide
grounding from molecular substructures and knowledge graphs. Code and models
are available on https://github.com/BioFM/OpenBioMed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yizhen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Massimo Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xing Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nie_Z/0/1/0/all/0/1&quot;&gt;Zaiqing Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09782">
<title>ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09782</link>
<description rdf:parseType="Literal">&lt;p&gt;In the complex domain of large language models (LLMs), striking a balance
between computational efficiency and maintaining model quality is a formidable
challenge. Navigating the inherent limitations of uniform quantization,
particularly when dealing with outliers, and motivated by the launch of
NVIDIA&apos;s H100 hardware, this study delves into the viability of floating-point
(FP) quantization, particularly focusing on FP8 and FP4, as a potential
solution. Our comprehensive investigation reveals that for LLMs, FP8 activation
consistently outshines its integer (INT8) equivalent, with the performance edge
becoming more noticeable in models possessing parameters beyond one billion.
For weight quantization, our findings indicate that FP4 exhibits comparable, if
not superior, performance to INT4, simplifying deployment on FP-supported
hardware like H100. To mitigate the overhead from precision alignment caused by
the disparity between weights and activations, we propose two scaling
constraints for weight quantization that negligibly impact the performance
compared to the standard W4A8 model. We additionally enhance our quantization
methods by integrating the Low Rank Compensation (LoRC) strategy, yielding
improvements especially in smaller models. The results of our investigation
emphasize the immense potential of FP quantization for LLMs, paving the way for
high-efficiency deployment in resource-limited settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10490">
<title>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10490</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate how images and sounds can be used for indirect prompt and
instruction injection in multi-modal LLMs. An attacker generates an adversarial
perturbation corresponding to the prompt and blends it into an image or audio
recording. When the user asks the (unmodified, benign) model about the
perturbed image or audio, the perturbation steers the model to output the
attacker-chosen text and/or make the subsequent dialog follow the attacker&apos;s
instruction. We illustrate this attack with several proof-of-concept examples
targeting LLaVa and PandaGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1&quot;&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1&quot;&gt;Tsung-Yin Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nassi_B/0/1/0/all/0/1&quot;&gt;Ben Nassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shmatikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10496">
<title>A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes. (arXiv:2307.10496v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10496</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex systems in science and engineering sometimes exhibit behavior that
changes across different regimes. Traditional global models struggle to capture
the full range of this complex behavior, limiting their ability to accurately
represent the system. In response to this challenge, we propose a novel
competitive learning approach for obtaining data-driven models of physical
systems. The primary idea behind the proposed approach is to employ dynamic
loss functions for a set of models that are trained concurrently on the data.
Each model competes for each observation during training, allowing for the
identification of distinct functional regimes within the dataset. To
demonstrate the effectiveness of the learning approach, we coupled it with
various regression methods that employ gradient-based optimizers for training.
The proposed approach was tested on various problems involving model discovery
and function approximation, demonstrating its ability to successfully identify
functional regimes, discover true governing equations, and reduce test errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ukorigho_O/0/1/0/all/0/1&quot;&gt;Okezzi F. Ukorigho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owoyele_O/0/1/0/all/0/1&quot;&gt;Opeoluwa Owoyele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10579">
<title>SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10579</link>
<description rdf:parseType="Literal">&lt;p&gt;SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to
protect data privacy in vertical federated learning setting. It is widely used
in fields such as finance and healthcare due to its interpretability,
effectiveness, and privacy-preserving capability. However, SecureBoost suffers
from high computational complexity and risk of label leakage. To harness the
full potential of SecureBoost, hyperparameters of SecureBoost should be
carefully chosen to strike an optimal balance between utility, efficiency, and
privacy. Existing methods either set hyperparameters empirically or
heuristically, which are far from optimal. To fill this gap, we propose a
Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto
optimal solutions that each solution is a set of hyperparameters achieving
optimal tradeoff between utility loss, training cost, and privacy leakage. We
design measurements of the three objectives. In particular, the privacy leakage
is measured using our proposed instance clustering attack. Experimental results
demonstrate that the CMOSB yields not only hyperparameters superior to the
baseline but also optimal sets of hyperparameters that can support the flexible
requirements of FL participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Ziyao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linghua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yongxin Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10617">
<title>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques. (arXiv:2307.10617v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10617</link>
<description rdf:parseType="Literal">&lt;p&gt;In the contemporary digital landscape, online reviews have become an
indispensable tool for promoting products and services across various
businesses. Marketers, advertisers, and online businesses have found incentives
to create deceptive positive reviews for their products and negative reviews
for their competitors&apos; offerings. As a result, the writing of deceptive reviews
has become an unavoidable practice for businesses seeking to promote themselves
or undermine their rivals. Detecting such deceptive reviews has become an
intense and ongoing area of research. This research paper proposes a machine
learning model to identify deceptive reviews, with a particular focus on
restaurants. This study delves into the performance of numerous experiments
conducted on a dataset of restaurant reviews known as the Deceptive Opinion
Spam Corpus. To accomplish this, an n-gram model and max features are developed
to effectively identify deceptive content, particularly focusing on fake
reviews. A benchmark study is undertaken to explore the performance of two
different feature extraction techniques, which are then coupled with five
distinct machine learning classification algorithms. The experimental results
reveal that the passive aggressive classifier stands out among the various
algorithms, showcasing the highest accuracy not only in text classification but
also in identifying fake reviews. Moreover, the research delves into data
augmentation and implements various deep learning techniques to further enhance
the process of detecting deceptive reviews. The findings shed light on the
efficacy of the proposed machine learning approach and offer valuable insights
into dealing with deceptive reviews in the realm of online businesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1&quot;&gt;Anusuya Baby Hari Krishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10926">
<title>Confidence intervals for performance estimates in 3D medical image segmentation. (arXiv:2307.10926v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10926</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical segmentation models are evaluated empirically. As such an evaluation
is based on a limited set of example images, it is unavoidably noisy. Beyond a
mean performance measure, reporting confidence intervals is thus crucial.
However, this is rarely done in medical image segmentation. The width of the
confidence interval depends on the test set size and on the spread of the
performance measure (its standard-deviation across of the test set). For
classification, many test images are needed to avoid wide confidence intervals.
Segmentation, however, has not been studied, and it differs by the amount of
information brought by a given test image. In this paper, we study the typical
confidence intervals in medical image segmentation. We carry experiments on 3D
image segmentation using the standard nnU-net framework, two datasets from the
Medical Decathlon challenge and two performance measures: the Dice accuracy and
the Hausdorff distance. We show that the parametric confidence intervals are
reasonable approximations of the bootstrap estimates for varying test set sizes
and spread of the performance metric. Importantly, we show that the test size
needed to achieve a given precision is often much lower than for classification
tasks. Typically, a 1% wide confidence interval requires about 100-200 test
samples when the spread is low (standard-deviation around 3%). More difficult
segmentation tasks may lead to higher spreads and require over 1000 samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jurdi_R/0/1/0/all/0/1&quot;&gt;R. El Jurdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Varoquaux_G/0/1/0/all/0/1&quot;&gt;G. Varoquaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Colliot_O/0/1/0/all/0/1&quot;&gt;O. Colliot&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>