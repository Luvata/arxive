<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02814" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.09118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.14517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01848" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.02489">
<title>Visual Question Answering (VQA) on Images with Superimposed Text. (arXiv:2307.02489v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02489</link>
<description rdf:parseType="Literal">&lt;p&gt;Superimposed text annotations have been under-investigated, yet are
ubiquitous, useful and important, especially in medical images. Medical images
also highlight the challenges posed by low resolution, noise and superimposed
textual meta-information. Therefor we probed the impact of superimposing text
onto medical images on VQA. Our results revealed that this textual
meta-information can be added without severely degrading key measures of VQA
performance. Our findings are significant because they validate the practice of
superimposing text on images, even for medical images subjected to the VQA task
using AI techniques. The work helps advance understanding of VQA in general
and, in particular, in the domain of healthcare and medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodali_V/0/1/0/all/0/1&quot;&gt;Venkat Kodali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1&quot;&gt;Daniel Berleant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02495">
<title>Anomaly detection in image or latent space of patch-based auto-encoders for industrial image analysis. (arXiv:2307.02495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02495</link>
<description rdf:parseType="Literal">&lt;p&gt;We study several methods for detecting anomalies in color images, constructed
on patch-based auto-encoders. Wecompare the performance of three types of
methods based, first, on the error between the original image and its
reconstruction,second, on the support estimation of the normal image
distribution in the latent space, and third, on the error between the
originalimage and a restored version of the reconstructed image. These methods
are evaluated on the industrial image database MVTecADand compared to two
competitive state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinon_N/0/1/0/all/0/1&quot;&gt;Nicolas Pinon&lt;/a&gt; (MYRIAD), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trombetta_R/0/1/0/all/0/1&quot;&gt;Robin Trombetta&lt;/a&gt; (MYRIAD), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lartizien_C/0/1/0/all/0/1&quot;&gt;Carole Lartizien&lt;/a&gt; (MYRIAD)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02496">
<title>Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.02496</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles
generated during the process hinder reactions, reduce cell efficiency, and
increase energy consumption. Additionally, these gas bubbles cause changes in
the conductivity inside the cell, resulting in corresponding variations in the
induced magnetic field around the cell. Therefore, measuring these gas
bubble-induced magnetic field fluctuations using external magnetic sensors and
solving the inverse problem of Biot-Savart Law allows for estimating the
conductivity in the cell and, thus, bubble size and location. However,
determining high-resolution conductivity maps from only a few induced magnetic
field measurements is an ill-posed inverse problem. To overcome this, we
exploit Invertible Neural Networks (INNs) to reconstruct the conductivity
field. Our qualitative results and quantitative evaluation using random error
diffusion show that INN achieves far superior performance compared to Tikhonov
regularization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Nishant Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krause_L/0/1/0/all/0/1&quot;&gt;Lukas Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wondrak_T/0/1/0/all/0/1&quot;&gt;Thomas Wondrak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eckert_S/0/1/0/all/0/1&quot;&gt;Sven Eckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eckert_K/0/1/0/all/0/1&quot;&gt;Kerstin Eckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gumhold_S/0/1/0/all/0/1&quot;&gt;Stefan Gumhold&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02500">
<title>Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection. (arXiv:2307.02500v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02500</link>
<description rdf:parseType="Literal">&lt;p&gt;With the perpetual increase of complexity of the state-of-the-art deep neural
networks, it becomes a more and more challenging task to maintain their
interpretability. Our work aims to evaluate the effects of adversarial training
utilized to produce robust models - less vulnerable to adversarial attacks. It
has been shown to make computer vision models more interpretable.
Interpretability is as essential as robustness when we deploy the models to the
real world. To prove the correlation between these two problems, we extensively
examine the models using local feature-importance methods (SHAP, Integrated
Gradients) and feature visualization techniques (Representation Inversion,
Class Specific Image Generation). Standard models, compared to robust are more
susceptible to adversarial attacks, and their learned representations are less
meaningful to humans. Conversely, these models focus on distinctive regions of
the images which support their predictions. Moreover, the features learned by
the robust model are closer to the real ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boychev_D/0/1/0/all/0/1&quot;&gt;Delyan Boychev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02508">
<title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking. (arXiv:2307.02508v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02508</link>
<description rdf:parseType="Literal">&lt;p&gt;The Associating Objects with Transformers (AOT) framework has exhibited
exceptional performance in a wide range of complex scenarios for video object
tracking and segmentation. In this study, we convert the bounding boxes to
masks in reference frames with the help of the Segment Anything Model (SAM) and
Alpha-Refine, and then propagate the masks to the current frame, transforming
the task from Video Object Tracking (VOT) to video object segmentation (VOS).
Furthermore, we introduce MSDeAOT, a variant of the AOT series that
incorporates transformers at multiple feature scales. MSDeAOT efficiently
propagates object masks from previous frames to the current frame using two
feature scales of 16 and 8. As a testament to the effectiveness of our design,
we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking
Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02509">
<title>Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02509</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a computational framework for the Wasserstein
auto-encoding of merge trees (MT-WAE), a novel extension of the classical
auto-encoder neural network architecture to the Wasserstein metric space of
merge trees. In contrast to traditional auto-encoders which operate on
vectorized data, our formulation explicitly manipulates merge trees on their
associated metric space at each layer of the network, resulting in superior
accuracy and interpretability. Our novel neural network approach can be
interpreted as a non-linear generalization of previous linear attempts [65] at
merge tree encoding. It also trivially extends to persistence diagrams.
Extensive experiments on public ensembles demonstrate the efficiency of our
algorithms, with MT-WAE computations in the orders of minutes on average. We
show the utility of our contributions in two applications adapted from previous
work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and
reliably compress merge trees by concisely representing them with their
coordinates in the final layer of our auto-encoder. Second, we document an
application to dimensionality reduction, by exploiting the latent space of our
auto-encoder, for the visual analysis of ensemble data. We illustrate the
versatility of our framework by introducing two penalty terms, to help preserve
in the latent space both the Wasserstein distances between merge trees, as well
as their clusters. In both applications, quantitative experiments assess the
relevance of our framework. Finally, we provide a C++ implementation that can
be used for reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1&quot;&gt;Mahieu Pont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1&quot;&gt;Julien Tierny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02516">
<title>Exploring new ways: Enforcing representational dissimilarity to learn new features and reduce error consistency. (arXiv:2307.02516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02516</link>
<description rdf:parseType="Literal">&lt;p&gt;Independently trained machine learning models tend to learn similar features.
Given an ensemble of independently trained models, this results in correlated
predictions and common failure modes. Previous attempts focusing on
decorrelation of output predictions or logits yielded mixed results,
particularly due to their reduction in model accuracy caused by conflicting
optimization objectives. In this paper, we propose the novel idea of utilizing
methods of the representational similarity field to promote dissimilarity
during training instead of measuring similarity of trained models. To this end,
we promote intermediate representations to be dissimilar at different depths
between architectures, with the goal of learning robust ensembles with disjoint
failure modes. We show that highly dissimilar intermediate representations
result in less correlated output predictions and slightly lower error
consistency, resulting in higher ensemble accuracy. With this, we shine first
light on the connection between intermediate representations and their impact
on the output predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wald_T/0/1/0/all/0/1&quot;&gt;Tassilo Wald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_C/0/1/0/all/0/1&quot;&gt;Constantin Ulrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmerer_D/0/1/0/all/0/1&quot;&gt;David Zimmerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehler_G/0/1/0/all/0/1&quot;&gt;Gregor Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_M/0/1/0/all/0/1&quot;&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02574">
<title>Semi-supervised Learning from Street-View Images and OpenStreetMap for Automatic Building Height Estimation. (arXiv:2307.02574v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02574</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate building height estimation is key to the automatic derivation of 3D
city models from emerging big geospatial data, including Volunteered
Geographical Information (VGI). However, an automatic solution for large-scale
building height estimation based on low-cost VGI data is currently missing. The
fast development of VGI data platforms, especially OpenStreetMap (OSM) and
crowdsourced street-view images (SVI), offers a stimulating opportunity to fill
this research gap. In this work, we propose a semi-supervised learning (SSL)
method of automatically estimating building height from Mapillary SVI and OSM
data to generate low-cost and open-source 3D city modeling in LoD1. The
proposed method consists of three parts: first, we propose an SSL schema with
the option of setting a different ratio of &quot;pseudo label&quot; during the supervised
regression; second, we extract multi-level morphometric features from OSM data
(i.e., buildings and streets) for the purposed of inferring building height;
last, we design a building floor estimation workflow with a pre-trained facade
object detection network to generate &quot;pseudo label&quot; from SVI and assign it to
the corresponding OSM building footprint. In a case study, we validate the
proposed SSL method in the city of Heidelberg, Germany and evaluate the model
performance against the reference data of building heights. Based on three
different regression models, namely Random Forest (RF), Support Vector Machine
(SVM), and Convolutional Neural Network (CNN), the SSL method leads to a clear
performance boosting in estimating building heights with a Mean Absolute Error
(MAE) around 2.1 meters, which is competitive to state-of-the-art approaches.
The preliminary result is promising and motivates our future work in scaling up
the proposed method based on low-cost VGI data, with possibilities in even
regions and areas with diverse data quality and availability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhendong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dax_G/0/1/0/all/0/1&quot;&gt;Gabriel Dax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_G/0/1/0/all/0/1&quot;&gt;Gefei Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Hongchao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zipf_A/0/1/0/all/0/1&quot;&gt;Alexander Zipf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_M/0/1/0/all/0/1&quot;&gt;Martin Werner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02586">
<title>Mainline Automatic Train Horn and Brake Performance Metric. (arXiv:2307.02586v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.02586</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper argues for the introduction of a mainline rail-oriented
performance metric for driver-replacing on-board perception systems. Perception
at the head of a train is divided into several subfunctions. This article
presents a preliminary submetric for the obstacle detection subfunction. To the
best of the author&apos;s knowledge, no other such proposal for obstacle detection
exists. A set of submetrics for the subfunctions should facilitate the
comparison of perception systems among each other and guide the measurement of
human driver performance. It should also be useful for a standardized
prediction of the number of accidents for a given perception system in a given
operational design domain. In particular, for the proposal of the obstacle
detection submetric, the professional readership is invited to provide their
feedback and quantitative information to the author. The analysis results of
the feedback will be published separately later.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagiew_R/0/1/0/all/0/1&quot;&gt;Rustam Tagiew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02595">
<title>GNEP Based Dynamic Segmentation and Motion Estimation for Neuromorphic Imaging. (arXiv:2307.02595v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02595</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of event-based cameras in the domains of
image segmentation and motion estimation. These cameras offer a groundbreaking
technology by capturing visual information as a continuous stream of
asynchronous events, departing from the conventional frame-based image
acquisition. We introduce a Generalized Nash Equilibrium based framework that
leverages the temporal and spatial information derived from the event stream to
carry out segmentation and velocity estimation. To establish the theoretical
foundations, we derive an existence criteria and propose a multi-level
optimization method for calculating equilibrium. The efficacy of this approach
is shown through a series of experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antil_H/0/1/0/all/0/1&quot;&gt;Harbir Antil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayre_D/0/1/0/all/0/1&quot;&gt;David Sayre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02609">
<title>MRecGen: Multimodal Appropriate Reaction Generator. (arXiv:2307.02609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02609</link>
<description rdf:parseType="Literal">&lt;p&gt;Verbal and non-verbal human reaction generation is a challenging task, as
different reactions could be appropriate for responding to the same behaviour.
This paper proposes the first multiple and multimodal (verbal and nonverbal)
appropriate human reaction generation framework that can generate appropriate
and realistic human-style reactions (displayed in the form of synchronised
text, audio and video streams) in response to an input user behaviour. This
novel technique can be applied to various human-computer interaction scenarios
by generating appropriate virtual agent/robot behaviours. Our demo is available
at \url{https://github.com/SSYSteve/MRecGen}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Cheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weicheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1&quot;&gt;Hatice Gunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Siyang Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02625">
<title>Retinex-based Image Denoising / Contrast Enhancement using Gradient Graph Laplacian Regularizer. (arXiv:2307.02625v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.02625</link>
<description rdf:parseType="Literal">&lt;p&gt;Images captured in poorly lit conditions are often corrupted by acquisition
noise. Leveraging recent advances in graph-based regularization, we propose a
fast Retinex-based restoration scheme that denoises and contrast-enhances an
image. Specifically, by Retinex theory we first assume that each image pixel is
a multiplication of its reflectance and illumination components. We next assume
that the reflectance and illumination components are piecewise constant (PWC)
and continuous piecewise planar (PWP) signals, which can be recovered via graph
Laplacian regularizer (GLR) and gradient graph Laplacian regularizer (GGLR)
respectively. We formulate quadratic objectives regularized by GLR and GGLR,
which are minimized alternately until convergence by solving linear systems --
with improved condition numbers via proposed preconditioners -- via conjugate
gradient (CG) efficiently. Experimental results show that our algorithm
achieves competitive visual image quality while reducing computation complexity
noticeably.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gharedaghi_Y/0/1/0/all/0/1&quot;&gt;Yeganeh Gharedaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheung_G/0/1/0/all/0/1&quot;&gt;Gene Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02641">
<title>Active Class Selection for Few-Shot Class-Incremental Learning. (arXiv:2307.02641v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.02641</link>
<description rdf:parseType="Literal">&lt;p&gt;For real-world applications, robots will need to continually learn in their
environments through limited interactions with their users. Toward this,
previous works in few-shot class incremental learning (FSCIL) and active class
selection (ACS) have achieved promising results but were tested in constrained
setups. Therefore, in this paper, we combine ideas from FSCIL and ACS to
develop a novel framework that can allow an autonomous agent to continually
learn new objects by asking its users to label only a few of the most
informative objects in the environment. To this end, we build on a
state-of-the-art (SOTA) FSCIL model and extend it with techniques from ACS
literature. We term this model Few-shot Incremental Active class SeleCtiOn
(FIASco). We further integrate a potential field-based navigation technique
with our model to develop a complete framework that can allow an agent to
process and reason on its sensory data through the FIASco model, navigate
towards the most informative object in the environment, gather data about the
object through its sensors and incrementally update the FIASco model.
Experimental results on a simulated agent and a real robot show the
significance of our approach for long-term real-world robotics applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McClurg_C/0/1/0/all/0/1&quot;&gt;Christopher McClurg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayub_A/0/1/0/all/0/1&quot;&gt;Ali Ayub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1&quot;&gt;Harsh Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajtmajer_S/0/1/0/all/0/1&quot;&gt;Sarah M. Rajtmajer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1&quot;&gt;Alan R. Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02658">
<title>Spherical Feature Pyramid Networks For Semantic Segmentation. (arXiv:2307.02658v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02658</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation for spherical data is a challenging problem in machine
learning since conventional planar approaches require projecting the spherical
image to the Euclidean plane. Representing the signal on a fundamentally
different topology introduces edges and distortions which impact network
performance. Recently, graph-based approaches have bypassed these challenges to
attain significant improvements by representing the signal on a spherical mesh.
Current approaches to spherical segmentation exclusively use variants of the
UNet architecture, meaning more successful planar architectures remain
unexplored. Inspired by the success of feature pyramid networks (FPNs) in
planar image segmentation, we leverage the pyramidal hierarchy of graph-based
spherical CNNs to design spherical FPNs. Our spherical FPN models show
consistent improvements over spherical UNets, whilst using fewer parameters. On
the Stanford 2D-3D-S dataset, our models achieve state-of-the-art performance
with an mIOU of 48.75, an improvement of 3.75 IoU points over the previous best
spherical CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_T/0/1/0/all/0/1&quot;&gt;Thomas Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_V/0/1/0/all/0/1&quot;&gt;Varun Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreadis_P/0/1/0/all/0/1&quot;&gt;Pavlos Andreadis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02672">
<title>GIT: Detecting Uncertainty, Out-Of-Distribution and Adversarial Samples using Gradients and Invariance Transformations. (arXiv:2307.02672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02672</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks tend to make overconfident predictions and often require
additional detectors for misclassifications, particularly for safety-critical
applications. Existing detection methods usually only focus on adversarial
attacks or out-of-distribution samples as reasons for false predictions.
However, generalization errors occur due to diverse reasons often related to
poorly learning relevant invariances. We therefore propose GIT, a holistic
approach for the detection of generalization errors that combines the usage of
gradient information and invariance transformations. The invariance
transformations are designed to shift misclassified samples back into the
generalization area of the neural network, while the gradient information
measures the contradiction between the initial prediction and the corresponding
inherent computations of the neural network using the transformed sample. Our
experiments demonstrate the superior performance of GIT compared to the
state-of-the-art on a variety of network architectures, problem setups and
perturbation types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1&quot;&gt;Julia Lust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1&quot;&gt;Alexandru P. Condurache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02679">
<title>A Study on the Impact of Face Image Quality on Face Recognition in the Wild. (arXiv:2307.02679v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02679</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has received increasing interests in face recognition recently.
Large quantities of deep learning methods have been proposed to handle various
problems appeared in face recognition. Quite a lot deep methods claimed that
they have gained or even surpassed human-level face verification performance in
certain databases. As we know, face image quality poses a great challenge to
traditional face recognition methods, e.g. model-driven methods with
hand-crafted features. However, a little research focus on the impact of face
image quality on deep learning methods, and even human performance. Therefore,
we raise a question: Is face image quality still one of the challenges for deep
learning based face recognition, especially in unconstrained condition. Based
on this, we further investigate this problem on human level. In this paper, we
partition face images into three different quality sets to evaluate the
performance of deep learning methods on cross-quality face images in the wild,
and then design a human face verification experiment on these cross-quality
data. The result indicates that quality issue still needs to be studied
thoroughly in deep learning, human own better capability in building the
relations between different face images with large quality gaps, and saying
deep learning method surpasses human-level is too optimistic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Na Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02682">
<title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02682</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense video captioning, a task of localizing meaningful moments and
generating relevant captions for videos, often requires a large, expensive
corpus of annotated video segments paired with text. In an effort to minimize
the annotation cost, we propose ZeroTA, a novel method for dense video
captioning in a zero-shot manner. Our method does not require any videos or
annotations for training; instead, it localizes and describes events within
each input video at test time by optimizing solely on the input. This is
accomplished by introducing a soft moment mask that represents a temporal
segment in the video and jointly optimizing it with the prefix parameters of a
language model. This joint optimization aligns a frozen language generation
model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,
CLIP) by maximizing the matching score between the generated text and a moment
within the video. We also introduce a pairwise temporal IoU loss to let a set
of soft moment masks capture multiple distinct events within the video. Our
method effectively discovers diverse significant events within the video, with
the resulting captions appropriately describing these events. The empirical
results demonstrate that ZeroTA surpasses zero-shot baselines and even
outperforms the state-of-the-art few-shot method on the widely-used benchmark
ActivityNet Captions. Moreover, our method shows greater robustness compared to
supervised methods when evaluated in out-of-domain scenarios. This research
provides insight into the potential of aligning widely-used models, such as
language generation models and vision-language models, to unlock a new
capability: understanding temporal aspects of videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yongrae Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seongyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Aiden SJ Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunji Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1&quot;&gt;Hanseok Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minjoon Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02694">
<title>Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02694</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the essential components of deep learning is the choice of the loss
function and performance metrics used to train and evaluate models. This paper
reviews the most prevalent loss functions and performance measurements in deep
learning. We examine the benefits and limits of each technique and illustrate
their application to various deep-learning problems. Our review aims to give a
comprehensive picture of the different loss functions and performance
indicators used in the most common deep learning tasks and help practitioners
choose the best method for their specific task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terven_J/0/1/0/all/0/1&quot;&gt;Juan Terven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordova_Esparza_D/0/1/0/all/0/1&quot;&gt;Diana M. Cordova-Esparza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_Pedraza_A/0/1/0/all/0/1&quot;&gt;Alfonzo Ramirez-Pedraza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavez_Urbiola_E/0/1/0/all/0/1&quot;&gt;Edgar A. Chavez-Urbiola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02698">
<title>Applying a Color Palette with Local Control using Diffusion Models. (arXiv:2307.02698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02698</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate two novel editing procedures in the context of fantasy card
art. Palette transfer applies a specified reference palette to a given card.
For fantasy art, the desired change in palette can be very large, leading to
huge changes in the &quot;look&quot; of the art. We demonstrate that a pipeline of vector
quantization; matching; and &quot;vector dequantization&quot; (using a diffusion model)
produces successful extreme palette transfers. Segment control allows an artist
to move one or more image segments, and to optionally specify the desired color
of the result. The combination of these two types of edit yields valuable
workflows, including: move a segment, then recolor; recolor, then force some
segments to take a prescribed color. We demonstrate our methods on the
challenging Yu-Gi-Oh card art dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1&quot;&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02716">
<title>CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization. (arXiv:2307.02716v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.02716</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal summarization usually suffers from the problem that the
contribution of the visual modality is unclear. Existing multimodal
summarization approaches focus on designing the fusion methods of different
modalities, while ignoring the adaptive conditions under which visual
modalities are useful. Therefore, we propose a novel Coarse-to-Fine
contribution network for multimodal Summarization (CFSum) to consider different
contributions of images for summarization. First, to eliminate the interference
of useless images, we propose a pre-filter module to abandon useless images.
Second, to make accurate use of useful images, we propose two levels of visual
complement modules, word level and phrase level. Specifically, image
contributions are calculated and are adopted to guide the attention of both
textual and visual modalities. Experimental results have shown that CFSum
significantly outperforms multiple strong baselines on the standard benchmark.
Furthermore, the analysis verifies that useful images can even help generate
non-visual words which are implicitly represented in the image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Min Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Junnan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haitao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1&quot;&gt;Chengqing Zong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02730">
<title>Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02730</link>
<description rdf:parseType="Literal">&lt;p&gt;The fine-grained action analysis of the existing action datasets is
challenged by insufficient action categories, low fine granularities, limited
modalities, and tasks. In this paper, we propose a Multi-modality and
Multi-task dataset of Figure Skating (MMFS) which was collected from the World
Figure Skating Championships. MMFS, which possesses action recognition and
action quality assessment, captures RGB, skeleton, and is collected the score
of actions from 11671 clips with 256 categories including spatial and temporal
labels. The key contributions of our dataset fall into three aspects as
follows. (1) Independently spatial and temporal categories are first proposed
to further explore fine-grained action recognition and quality assessment. (2)
MMFS first introduces the skeleton modality for complex fine-grained action
quality assessment. (3) Our multi-modality and multi-task dataset encourage
more action analysis models. To benchmark our dataset, we adopt RGB-based and
skeleton-based baseline methods for action recognition and action quality
assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng-Lan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yu-Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Si-Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wen-Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Ning Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_G/0/1/0/all/0/1&quot;&gt;Gui-Hong Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02733">
<title>MMNet: Multi-Collaboration and Multi-Supervision Network for Sequential Deepfake Detection. (arXiv:2307.02733v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02733</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced manipulation techniques have provided criminals with opportunities
to make social panic or gain illicit profits through the generation of
deceptive media, such as forged face images. In response, various deepfake
detection methods have been proposed to assess image authenticity. Sequential
deepfake detection, which is an extension of deepfake detection, aims to
identify forged facial regions with the correct sequence for recovery.
Nonetheless, due to the different combinations of spatial and sequential
manipulations, forged face images exhibit substantial discrepancies that
severely impact detection performance. Additionally, the recovery of forged
images requires knowledge of the manipulation model to implement inverse
transformations, which is difficult to ascertain as relevant techniques are
often concealed by attackers. To address these issues, we propose
Multi-Collaboration and Multi-Supervision Network (MMNet) that handles various
spatial scales and sequential permutations in forged face images and achieve
recovery without requiring knowledge of the corresponding manipulation method.
Furthermore, existing evaluation metrics only consider detection accuracy at a
single inferring step, without accounting for the matching degree with
ground-truth under continuous multiple steps. To overcome this limitation, we
propose a novel evaluation metric called Complete Sequence Matching (CSM),
which considers the detection accuracy at multiple inferring steps, reflecting
the ability to detect integrally forged sequences. Extensive experiments on
several typical datasets demonstrate that MMNet achieves state-of-the-art
detection performance and independent recovery performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1&quot;&gt;Ruiyang Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02736">
<title>An Uncertainty Aided Framework for Learning based Liver $T_1\rho$ Mapping and Analysis. (arXiv:2307.02736v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2307.02736</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Quantitative $T_1\rho$ imaging has potential for assessment of
biochemical alterations of liver pathologies. Deep learning methods have been
employed to accelerate quantitative $T_1\rho$ imaging. To employ artificial
intelligence-based quantitative imaging methods in complicated clinical
environment, it is valuable to estimate the uncertainty of the predicated
$T_1\rho$ values to provide the confidence level of the quantification results.
The uncertainty should also be utilized to aid the post-hoc quantitative
analysis and model learning tasks. Approach: To address this need, we propose a
parametric map refinement approach for learning-based $T_1\rho$ mapping and
train the model in a probabilistic way to model the uncertainty. We also
propose to utilize the uncertainty map to spatially weight the training of an
improved $T_1\rho$ mapping network to further improve the mapping performance
and to remove pixels with unreliable $T_1\rho$ values in the region of
interest. The framework was tested on a dataset of 51 patients with different
liver fibrosis stages. Main results: Our results indicate that the
learning-based map refinement method leads to a relative mapping error of less
than 3% and provides uncertainty estimation simultaneously. The estimated
uncertainty reflects the actual error level, and it can be used to further
reduce relative $T_1\rho$ mapping error to 2.60% as well as removing unreliable
pixels in the region of interest effectively. Significance: Our studies
demonstrate the proposed approach has potential to provide a learning-based
quantitative MRI system for trustworthy $T_1\rho$ mapping of the liver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wong_V/0/1/0/all/0/1&quot;&gt;Vincent Wai Sun Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chan_Q/0/1/0/all/0/1&quot;&gt;Queen Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Winnie Chiu Wing Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02744">
<title>Active Learning with Contrastive Pre-training for Facial Expression Recognition. (arXiv:2307.02744v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02744</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has played a significant role in the success of facial
expression recognition (FER), thanks to large models and vast amounts of
labelled data. However, obtaining labelled data requires a tremendous amount of
human effort, time, and financial resources. Even though some prior works have
focused on reducing the need for large amounts of labelled data using different
unsupervised methods, another promising approach called active learning is
barely explored in the context of FER. This approach involves selecting and
labelling the most representative samples from an unlabelled set to make the
best use of a limited &apos;labelling budget&apos;. In this paper, we implement and study
8 recent active learning methods on three public FER datasets, FER13, RAF-DB,
and KDEF. Our findings show that existing active learning methods do not
perform well in the context of FER, likely suffering from a phenomenon called
&apos;Cold Start&apos;, which occurs when the initial set of labelled samples is not well
representative of the entire dataset. To address this issue, we propose
contrastive self-supervised pre-training, which first learns the underlying
representations based on the entire unlabelled dataset. We then follow this
with the active learning methods and observe that our 2-step approach shows up
to 9.2% improvement over random sampling and up to 6.7% improvement over the
best existing active learning baseline without the pre-training. We will make
the code for this study public upon publication at:
github.com/ShuvenduRoy/ActiveFER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Shuvendu Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1&quot;&gt;Ali Etemad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02753">
<title>CityTrack: Improving City-Scale Multi-Camera Multi-Target Tracking by Location-Aware Tracking and Box-Grained Matching. (arXiv:2307.02753v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02753</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Camera Multi-Target Tracking (MCMT) is a computer vision technique that
involves tracking multiple targets simultaneously across multiple cameras. MCMT
in urban traffic visual analysis faces great challenges due to the complex and
dynamic nature of urban traffic scenes, where multiple cameras with different
views and perspectives are often used to cover a large city-scale area. Targets
in urban traffic scenes often undergo occlusion, illumination changes, and
perspective changes, making it difficult to associate targets across different
cameras accurately. To overcome these challenges, we propose a novel systematic
MCMT framework, called CityTrack. Specifically, we present a Location-Aware
SCMT tracker which integrates various advanced techniques to improve its
effectiveness in the MCMT task and propose a novel Box-Grained Matching (BGM)
method for the ICA module to solve the aforementioned problems. We evaluated
our approach on the public test set of the CityFlowV2 dataset and achieved an
IDF1 of 84.91%, ranking 1st in the 2022 AI CITY CHALLENGE. Our experimental
results demonstrate the effectiveness of our approach in overcoming the
challenges posed by urban traffic scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jincheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xipeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zhikang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xiao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02770">
<title>Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback. (arXiv:2307.02770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02770</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently shown remarkable success in high-quality image
generation. Sometimes, however, a pre-trained diffusion model exhibits partial
misalignment in the sense that the model can generate good images, but it
sometimes outputs undesirable images. If so, we simply need to prevent the
generation of the bad images, and we call this task censoring. In this work, we
present censored generation with a pre-trained diffusion model using a reward
model trained on minimal human feedback. We show that censoring can be
accomplished with extreme human feedback efficiency and that labels generated
with a mere few minutes of human feedback are sufficient. Code available at:
https://github.com/tetrzim/diffusion-human-feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1&quot;&gt;TaeHo Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myoung_K/0/1/0/all/0/1&quot;&gt;Kibeom Myoung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Keon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+No_A/0/1/0/all/0/1&quot;&gt;Albert No&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1&quot;&gt;Ernest K. Ryu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02773">
<title>SeLiNet: Sentiment enriched Lightweight Network for Emotion Recognition in Images. (arXiv:2307.02773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02773</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a sentiment-enriched lightweight network SeLiNet
and an end-to-end on-device pipeline for contextual emotion recognition in
images. SeLiNet model consists of body feature extractor, image aesthetics
feature extractor, and learning-based fusion network which jointly estimates
discrete emotion and human sentiments tasks. On the EMOTIC dataset, the
proposed approach achieves an Average Precision (AP) score of 27.17 in
comparison to the baseline AP score of 27.38 while reducing the model size by
&amp;gt;85%. In addition, we report an on-device AP score of 26.42 with reduction in
model size by &amp;gt;93% when compared to the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khargonkar_T/0/1/0/all/0/1&quot;&gt;Tuneer Khargonkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1&quot;&gt;Shwetank Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sumit Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+KR_B/0/1/0/all/0/1&quot;&gt;Barath Raj KR&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02783">
<title>UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering. (arXiv:2307.02783v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02783</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, artificial intelligence has played an important role in
medicine and disease diagnosis, with many applications to be mentioned, one of
which is Medical Visual Question Answering (MedVQA). By combining computer
vision and natural language processing, MedVQA systems can assist experts in
extracting relevant information from medical image based on a given question
and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023
challenge carried out visual question answering task in the gastrointestinal
domain, which includes gastroscopy and colonoscopy images. Our team approached
Task 1 of the challenge by proposing a multimodal learning method with image
enhancement to improve the VQA performance on gastrointestinal images. The
multimodal architecture is set up with BERT encoder and different pre-trained
vision models based on convolutional neural network (CNN) and Transformer
architecture for features extraction from question and endoscopy image. The
result of this study highlights the dominance of Transformer-based vision
models over the CNNs and demonstrates the effectiveness of the image
enhancement process, with six out of the eight vision models achieving better
F1-Score. Our best method, which takes advantages of BERT+BEiT fusion and image
enhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on the
development test set, while also producing good result on the private test set
with accuracy of 82.01%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thai_T/0/1/0/all/0/1&quot;&gt;Triet M. Thai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_A/0/1/0/all/0/1&quot;&gt;Anh T. Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tieu_H/0/1/0/all/0/1&quot;&gt;Hao K. Tieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_L/0/1/0/all/0/1&quot;&gt;Linh N.P. Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thien T.B. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02791">
<title>The Role of Subgroup Separability in Group-Fair Medical Image Classification. (arXiv:2307.02791v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02791</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate performance disparities in deep classifiers. We find that the
ability of classifiers to separate individuals into subgroups varies
substantially across medical imaging modalities and protected characteristics;
crucially, we show that this property is predictive of algorithmic bias.
Through theoretical analysis and extensive empirical evaluation, we find a
relationship between subgroup separability, subgroup disparities, and
performance degradation when models are trained on data with systematic bias
such as underdiagnosis. Our findings shed new light on the question of how
models become biased, providing important insights for the development of fair
medical imaging AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1&quot;&gt;Charles Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roschewitz_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe9;lanie Roschewitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02798">
<title>Semi-supervised Domain Adaptive Medical Image Segmentation through Consistency Regularized Disentangled Contrastive Learning. (arXiv:2307.02798v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02798</link>
<description rdf:parseType="Literal">&lt;p&gt;Although unsupervised domain adaptation (UDA) is a promising direction to
alleviate domain shift, they fall short of their supervised counterparts. In
this work, we investigate relatively less explored semi-supervised domain
adaptation (SSDA) for medical image segmentation, where access to a few labeled
target samples can improve the adaptation performance substantially.
Specifically, we propose a two-stage training process. First, an encoder is
pre-trained in a self-learning paradigm using a novel domain-content
disentangled contrastive learning (CL) along with a pixel-level feature
consistency constraint. The proposed CL enforces the encoder to learn
discriminative content-specific but domain-invariant semantics on a global
scale from the source and target images, whereas consistency regularization
enforces the mining of local pixel-level information by maintaining spatial
sensitivity. This pre-trained encoder, along with a decoder, is further
fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a
semi-supervised setting. Furthermore, we experimentally validate that our
proposed method can easily be extended for UDA settings, adding to the
superiority of the proposed strategy. Upon evaluation on two domain adaptive
image segmentation tasks, our proposed method outperforms the SoTA methods,
both in SSDA and UDA settings. Code is available at
https://github.com/hritam-98/GFDA-disentangled
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basak_H/0/1/0/all/0/1&quot;&gt;Hritam Basak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaozheng Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02808">
<title>Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation. (arXiv:2307.02808v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.02808</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital humans have witnessed extensive applications in various domains,
necessitating related quality assessment studies. However, there is a lack of
comprehensive digital human quality assessment (DHQA) databases. To address
this gap, we propose SJTU-H3D, a subjective quality assessment database
specifically designed for full-body digital humans. It comprises 40
high-quality reference digital humans and 1,120 labeled distorted counterparts
generated with seven types of distortions. The SJTU-H3D database can serve as a
benchmark for DHQA research, allowing evaluation and refinement of processing
algorithms. Further, we propose a zero-shot DHQA approach that focuses on
no-reference (NR) scenarios to ensure generalization capabilities while
mitigating database bias. Our method leverages semantic and distortion features
extracted from projections, as well as geometry features derived from the mesh
structure of digital humans. Specifically, we employ the Contrastive
Language-Image Pre-training (CLIP) model to measure semantic affinity and
incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture
low-level distortion information. Additionally, we utilize dihedral angles as
geometry descriptors to extract mesh features. By aggregating these measures,
we introduce the Digital Human Quality Index (DHQI), which demonstrates
significant improvements in zero-shot performance. The DHQI can also serve as a
robust baseline for DHQA tasks, facilitating advancements in the field. The
database and the code are available at https://github.com/zzc-1998/SJTU-H3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingjie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02814">
<title>Single Image LDR to HDR Conversion using Conditional Diffusion. (arXiv:2307.02814v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02814</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital imaging aims to replicate realistic scenes, but Low Dynamic Range
(LDR) cameras cannot represent the wide dynamic range of real scenes, resulting
in under-/overexposed images. This paper presents a deep learning-based
approach for recovering intricate details from shadows and highlights while
reconstructing High Dynamic Range (HDR) images. We formulate the problem as an
image-to-image (I2I) translation task and propose a conditional Denoising
Diffusion Probabilistic Model (DDPM) based framework using classifier-free
guidance. We incorporate a deep CNN-based autoencoder in our proposed framework
to enhance the quality of the latent representation of the input LDR image used
for conditioning. Moreover, we introduce a new loss function for LDR-HDR
translation tasks, termed Exposure Loss. This loss helps direct gradients in
the opposite direction of the saturation, further improving the results&apos;
quality. By conducting comprehensive quantitative and qualitative experiments,
we have effectively demonstrated the proficiency of our proposed method. The
results indicate that a simple conditional diffusion-based method can replace
the complex camera pipeline-based architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_D/0/1/0/all/0/1&quot;&gt;Dwip Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vashishtha_G/0/1/0/all/0/1&quot;&gt;Gautam Vashishtha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Prajwal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1&quot;&gt;Shanmuganathan Raman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02825">
<title>Bundle-specific Tractogram Distribution Estimation Using Higher-order Streamline Differential Equation. (arXiv:2307.02825v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02825</link>
<description rdf:parseType="Literal">&lt;p&gt;Tractography traces the peak directions extracted from fiber orientation
distribution (FOD) suffering from ambiguous spatial correspondences between
diffusion directions and fiber geometry, which is prone to producing erroneous
tracks while missing true positive connections. The peaks-based tractography
methods &apos;locally&apos; reconstructed streamlines in &apos;single to single&apos; manner, thus
lacking of global information about the trend of the whole fiber bundle. In
this work, we propose a novel tractography method based on a bundle-specific
tractogram distribution function by using a higher-order streamline
differential equation, which reconstructs the streamline bundles in &apos;cluster to
cluster&apos; manner. A unified framework for any higher-order streamline
differential equation is presented to describe the fiber bundles with disjoint
streamlines defined based on the diffusion tensor vector field. At the global
level, the tractography process is simplified as the estimation of
bundle-specific tractogram distribution (BTD) coefficients by minimizing the
energy optimization model, and is used to characterize the relations between
BTD and diffusion tensor vector under the prior guidance by introducing the
tractogram bundle information to provide anatomic priors. Experiments are
performed on simulated Hough, Sine, Circle data, ISMRM 2015 Tractography
Challenge data, FiberCup data, and in vivo data from the Human Connectome
Project (HCP) data for qualitative and quantitative evaluation. The results
demonstrate that our approach can reconstruct the complex global fiber bundles
directly. BTD reduces the error deviation and accumulation at the local level
and shows better results in reconstructing long-range, twisting, and large
fanning tracts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yuanjing Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jianzhong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Fei Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02828">
<title>Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2307.02828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02828</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are known to be vulnerable to adversarial examples
crafted by adding human-imperceptible perturbations to the benign input. After
achieving nearly 100% attack success rates in white-box setting, more focus is
shifted to black-box attacks, of which the transferability of adversarial
examples has gained significant attention. In either case, the common
gradient-based methods generally use the sign function to generate
perturbations on the gradient update, that offers a roughly correct direction
and has gained great success. But little work pays attention to its possible
limitation. In this work, we observe that the deviation between the original
gradient and the generated noise may lead to inaccurate gradient update
estimation and suboptimal solutions for adversarial transferability. To this
end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM).
Specifically, we use data rescaling to substitute the sign function without
extra computational cost. We further propose a Depth First Sampling method to
eliminate the fluctuation of rescaling and stabilize the gradient update. Our
method could be used in any gradient-based attacks and is extensible to be
integrated with various input transformation or ensemble methods to further
improve the adversarial transferability. Extensive experiments on the standard
ImageNet dataset show that our method could significantly boost the
transferability of gradient-based attacks and outperform the state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anmin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Chenxuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yanbo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02836">
<title>Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization. (arXiv:2307.02836v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02836</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection has a wide range of applications and is especially
important in industrial quality inspection. Currently, many top-performing
anomaly-detection models rely on feature-embedding methods. However, these
methods do not perform well on datasets with large variations in object
locations. Reconstruction-based methods use reconstruction errors to detect
anomalies without considering positional differences between samples. In this
study, a reconstruction-based method using the noise-to-norm paradigm is
proposed, which avoids the invariant reconstruction of anomalous regions. Our
reconstruction network is based on M-net and incorporates multiscale fusion and
residual attention modules to enable end-to-end anomaly detection and
localization. Experiments demonstrate that the method is effective in
reconstructing anomalous regions into normal patterns and achieving accurate
anomaly detection and localization. On the MPDD and VisA datasets, our proposed
method achieved more competitive results than the latest methods, and it set a
new state-of-the-art standard on the MPDD dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shiqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_R/0/1/0/all/0/1&quot;&gt;Ruiyan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1&quot;&gt;Jun Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02848">
<title>Revisiting Computer-Aided Tuberculosis Diagnosis. (arXiv:2307.02848v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02848</link>
<description rdf:parseType="Literal">&lt;p&gt;Tuberculosis (TB) is a major global health threat, causing millions of deaths
annually. Although early diagnosis and treatment can greatly improve the
chances of survival, it remains a major challenge, especially in developing
countries. Recently, computer-aided tuberculosis diagnosis (CTD) using deep
learning has shown promise, but progress is hindered by limited training data.
To address this, we establish a large-scale dataset, namely the Tuberculosis
X-ray (TBX11K) dataset, which contains 11,200 chest X-ray (CXR) images with
corresponding bounding box annotations for TB areas. This dataset enables the
training of sophisticated detectors for high-quality CTD. Furthermore, we
propose a strong baseline, SymFormer, for simultaneous CXR image classification
and TB infection area detection. SymFormer incorporates Symmetric Search
Attention (SymAttention) to tackle the bilateral symmetry property of CXR
images for learning discriminative features. Since CXR images may not strictly
adhere to the bilateral symmetry property, we also propose Symmetric Positional
Encoding (SPE) to facilitate SymAttention through feature recalibration. To
promote future research on CTD, we build a benchmark by introducing evaluation
metrics, evaluating baseline models reformed from existing detectors, and
running an online challenge. Experiments show that SymFormer achieves
state-of-the-art performance on the TBX11K dataset. The data, code, and models
will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shi-Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02858">
<title>Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing. (arXiv:2307.02858v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02858</link>
<description rdf:parseType="Literal">&lt;p&gt;Face presentation attacks, also known as spoofing attacks, pose a significant
threat to biometric systems that rely on facial recognition systems, such as
access control systems, mobile payments, and identity verification systems. To
prevent spoofing, several video-based methods have been presented in the
literature that analyze facial motion in successive video frames. However,
estimating the motion between adjacent frames is a challenging task and
requires high computational cost. In this paper, we reformulate the face
anti-spoofing task as a motion prediction problem and introduce a deep ensemble
learning model with a frame skipping mechanism. The proposed frame skipping is
based on a uniform sampling approach where the original video is divided into
fixed size video clips. In this way, every nth frame of the clip is selected to
ensure that the temporal patterns can easily be perceived during the training
of three different recurrent neural networks (RNNs). Motivated by the
performance of each RNNs, a meta-model is developed to improve the overall
recognition performance by combining the predictions of the individual RNNs.
Extensive experiments were conducted on four datasets, and state-of-the-art
performance is reported for MSU-MFSD (3.12\%), Replay-Attack (11.19\%), and
OULU-NPU (12.23\%) using half total error rate (HTER) in the most challenging
cross-dataset test scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhammad_U/0/1/0/all/0/1&quot;&gt;Usman Muhammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1&quot;&gt;Md Ziaul Hoque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1&quot;&gt;Mourad Oussalah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1&quot;&gt;Jorma Laaksonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02862">
<title>A Critical Look at the Current Usage of Foundation Model for Dense Recognition Task. (arXiv:2307.02862v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02862</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years large model trained on huge amount of cross-modality data,
which is usually be termed as foundation model, achieves conspicuous
accomplishment in many fields, such as image recognition and generation. Though
achieving great success in their original application case, it is still unclear
whether those foundation models can be applied to other different downstream
tasks. In this paper, we conduct a short survey on the current methods for
discriminative dense recognition tasks, which are built on the pretrained
foundation model. And we also provide some preliminary experimental analysis of
an existing open-vocabulary segmentation method based on Stable Diffusion,
which indicates the current way of deploying diffusion model for segmentation
is not optimal. This aims to provide insights for future research on adopting
foundation model for downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shiqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1&quot;&gt;Atsushi Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02869">
<title>MomentDiff: Generative Video Moment Retrieval from Random to Real. (arXiv:2307.02869v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;Video moment retrieval pursues an efficient and generalized solution to
identify the specific temporal segments within an untrimmed video that
correspond to a given language description. To achieve this goal, we provide a
generative diffusion-based framework called MomentDiff, which simulates a
typical human retrieval process from random browsing to gradual localization.
Specifically, we first diffuse the real span to random noise, and learn to
denoise the random noise to the original span with the guidance of similarity
between text and video. This allows the model to learn a mapping from arbitrary
random locations to real moments, enabling the ability to locate segments from
random initialization. Once trained, MomentDiff could sample random temporal
segments as initial guesses and iteratively refine them to generate an accurate
temporal boundary. Different from discriminative works (e.g., based on
learnable proposals or queries), MomentDiff with random initialized spans could
resist the temporal location biases from datasets. To evaluate the influence of
the temporal location biases, we propose two anti-bias datasets with location
distribution shifts, named Charades-STA-Len and Charades-STA-Mom. The
experimental results demonstrate that our efficient framework consistently
outperforms state-of-the-art methods on three public benchmarks, and exhibits
better generalization and robustness on the proposed anti-bias datasets. The
code, model, and anti-bias evaluation datasets are available at
https://github.com/IMCCretrieval/MomentDiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pandeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chen-Wei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hongtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Deli Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02875">
<title>Reference-based Motion Blur Removal: Learning to Utilize Sharpness in the Reference Image. (arXiv:2307.02875v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02875</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent advancement in the study of removing motion blur in an
image, it is still hard to deal with strong blurs. While there are limits in
removing blurs from a single image, it has more potential to use multiple
images, e.g., using an additional image as a reference to deblur a blurry
image. A typical setting is deburring an image using a nearby sharp image(s) in
a video sequence, as in the studies of video deblurring. This paper proposes a
better method to use the information present in a reference image. The method
does not need a strong assumption on the reference image. We can utilize an
alternative shot of the identical scene, just like in video deblurring, or we
can even employ a distinct image from another scene. Our method first matches
local patches of the target and reference images and then fuses their features
to estimate a sharp image. We employ a patch-based feature matching strategy to
solve the difficult problem of matching the blurry image with the sharp
reference. Our method can be integrated into pre-existing networks designed for
single image deblurring. The experimental results show the effectiveness of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Han Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1&quot;&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02877">
<title>Towards accurate instance segmentation in large-scale LiDAR point clouds. (arXiv:2307.02877v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic segmentation is the combination of semantic and instance
segmentation: assign the points in a 3D point cloud to semantic categories and
partition them into distinct object instances. It has many obvious applications
for outdoor scene understanding, from city mapping to forest management.
Existing methods struggle to segment nearby instances of the same semantic
category, like adjacent pieces of street furniture or neighbouring trees, which
limits their usability for inventory- or management-type applications that rely
on object instances. This study explores the steps of the panoptic segmentation
pipeline concerned with clustering points into object instances, with the goal
to alleviate that bottleneck. We find that a carefully designed clustering
strategy, which leverages multiple types of learned point embeddings,
significantly improves instance segmentation. Experiments on the NPM3D urban
mobile mapping dataset and the FOR-instance forest dataset demonstrate the
effectiveness and versatility of the proposed strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1&quot;&gt;Binbin Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_T/0/1/0/all/0/1&quot;&gt;Torben Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontogianni_T/0/1/0/all/0/1&quot;&gt;Theodora Kontogianni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetterli_F/0/1/0/all/0/1&quot;&gt;Frawa Vetterli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puliti_S/0/1/0/all/0/1&quot;&gt;Stefano Puliti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astrup_R/0/1/0/all/0/1&quot;&gt;Rasmus Astrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02881">
<title>Probabilistic and Semantic Descriptions of Image Manifolds and Their Applications. (arXiv:2307.02881v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02881</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper begins with a description of methods for estimating probability
density functions for images that reflects the observation that such data is
usually constrained to lie in restricted regions of the high-dimensional image
space - not every pattern of pixels is an image. It is common to say that
images lie on a lower-dimensional manifold in the high-dimensional space.
However, although images may lie on such lower-dimensional manifolds, it is not
the case that all points on the manifold have an equal probability of being
images. Images are unevenly distributed on the manifold, and our task is to
devise ways to model this distribution as a probability distribution. In
pursuing this goal, we consider generative models that are popular in AI and
computer vision community. For our purposes, generative/probabilistic models
should have the properties of 1) sample generation: it should be possible to
sample from this distribution according to the modelled density function, and
2) probability computation: given a previously unseen sample from the dataset
of interest, one should be able to compute the probability of the sample, at
least up to a normalising constant. To this end, we investigate the use of
methods such as normalising flow and diffusion models. We then show that such
probabilistic descriptions can be used to construct defences against
adversarial attacks. In addition to describing the manifold in terms of
density, we also consider how semantic interpretations can be used to describe
points on the manifold. To this end, we consider an emergent language framework
which makes use of variational encoders to produce a disentangled
representation of points that reside on a given manifold. Trajectories between
points on a manifold can then be described in terms of evolving semantic
descriptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peter Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1&quot;&gt;Dylan Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02897">
<title>RefVSR++: Exploiting Reference Inputs for Reference-based Video Super-resolution. (arXiv:2307.02897v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02897</link>
<description rdf:parseType="Literal">&lt;p&gt;Smartphones equipped with a multi-camera system comprising multiple cameras
with different field-of-view (FoVs) are becoming more prevalent. These camera
configurations are compatible with reference-based SR and video SR, which can
be executed simultaneously while recording video on the device. Thus, combining
these two SR methods can improve image quality. Recently, Lee et al. have
presented such a method, RefVSR. In this paper, we consider how to optimally
utilize the observations obtained, including input low-resolution (LR) video
and reference (Ref) video. RefVSR extends conventional video SR quite simply,
aggregating the LR and Ref inputs over time in a single bidirectional stream.
However, considering the content difference between LR and Ref images due to
their FoVs, we can derive the maximum information from the two image sequences
by aggregating them independently in the temporal direction. Then, we propose
an improved method, RefVSR++, which can aggregate two features in parallel in
the temporal direction, one for aggregating the fused LR and Ref inputs and the
other for Ref inputs over time. Furthermore, we equip RefVSR++ with enhanced
mechanisms to align image features over time, which is the key to the success
of video SR. We experimentally show that RefVSR++ outperforms RefVSR by over
1dB in PSNR, achieving the new state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Han Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1&quot;&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02906">
<title>A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition. (arXiv:2307.02906v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02906</link>
<description rdf:parseType="Literal">&lt;p&gt;Sensor-based Human Activity Recognition facilitates unobtrusive monitoring of
human movements. However, determining the most effective sensor placement for
optimal classification performance remains challenging. This paper introduces a
novel methodology to resolve this issue, using real-time 2D pose estimations
derived from video recordings of target activities. The derived skeleton data
provides a unique strategy for identifying the optimal sensor location. We
validate our approach through a feasibility study, applying inertial sensors to
monitor 13 different activities across ten subjects. Our findings indicate that
the vision-based method for sensor placement offers comparable results to the
conventional deep learning approach, demonstrating its efficacy. This research
significantly advances the field of Human Activity Recognition by providing a
lightweight, on-device solution for determining the optimal sensor placement,
thereby enhancing data anonymization and supporting a multimodal classification
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konak_O/0/1/0/all/0/1&quot;&gt;Orhan Konak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wischmann_A/0/1/0/all/0/1&quot;&gt;Alexander Wischmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Water_R/0/1/0/all/0/1&quot;&gt;Robin van de Water&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnrich_B/0/1/0/all/0/1&quot;&gt;Bert Arnrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02935">
<title>DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms using Self-adversarial Learning. (arXiv:2307.02935v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02935</link>
<description rdf:parseType="Literal">&lt;p&gt;Asymmetry is a crucial characteristic of bilateral mammograms (Bi-MG) when
abnormalities are developing. It is widely utilized by radiologists for
diagnosis. The question of &apos;what the symmetrical Bi-MG would look like when the
asymmetrical abnormalities have been removed ?&apos; has not yet received strong
attention in the development of algorithms on mammograms. Addressing this
question could provide valuable insights into mammographic anatomy and aid in
diagnostic interpretation. Hence, we propose a novel framework, DisAsymNet,
which utilizes asymmetrical abnormality transformer guided self-adversarial
learning for disentangling abnormalities and symmetric Bi-MG. At the same time,
our proposed method is partially guided by randomly synthesized abnormalities.
We conduct experiments on three public and one in-house dataset, and
demonstrate that our method outperforms existing methods in abnormality
classification, segmentation, and localization tasks. Additionally,
reconstructed normal mammograms can provide insights toward better
interpretable visual cues for clinical diagnosis. The code will be accessible
to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Luyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chunyao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beets_Tan_R/0/1/0/all/0/1&quot;&gt;Regina Beets-Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1&quot;&gt;Ruisheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_R/0/1/0/all/0/1&quot;&gt;Ritse Mann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02953">
<title>SegNetr: Rethinking the local-global interactions and skip connections in U-shaped networks. (arXiv:2307.02953v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.02953</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, U-shaped networks have dominated the field of medical image
segmentation due to their simple and easily tuned structure. However, existing
U-shaped segmentation networks: 1) mostly focus on designing complex
self-attention modules to compensate for the lack of long-term dependence based
on convolution operation, which increases the overall number of parameters and
computational complexity of the network; 2) simply fuse the features of encoder
and decoder, ignoring the connection between their spatial locations. In this
paper, we rethink the above problem and build a lightweight medical image
segmentation network, called SegNetr. Specifically, we introduce a novel
SegNetr block that can perform local-global interactions dynamically at any
stage and with only linear complexity. At the same time, we design a general
information retention skip connection (IRSC) to preserve the spatial location
information of encoder features and achieve accurate fusion with the decoder
features. We validate the effectiveness of SegNetr on four mainstream medical
image segmentation datasets, with 59\% and 76\% fewer parameters and GFLOPs
than vanilla U-Net, while achieving segmentation performance comparable to
state-of-the-art methods. Notably, the components proposed in this paper can be
applied to other U-shaped networks to improve their segmentation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junlong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chengrui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Min Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02971">
<title>On the Cultural Gap in Text-to-Image Generation. (arXiv:2307.02971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02971</link>
<description rdf:parseType="Literal">&lt;p&gt;One challenge in text-to-image (T2I) generation is the inadvertent reflection
of culture gaps present in the training data, which signifies the disparity in
generated image quality when the cultural elements of the input text are rarely
collected in the training set. Although various T2I models have shown
impressive but arbitrary examples, there is no benchmark to systematically
evaluate a T2I model&apos;s ability to generate cross-cultural images. To bridge the
gap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive
evaluation criteria, which can assess how well-suited a model is to a target
culture. By analyzing the flawed images generated by the Stable Diffusion model
on the C3 benchmark, we find that the model often fails to generate certain
cultural objects. Accordingly, we propose a novel multi-modal metric that
considers object-text alignment to filter the fine-tuning data in the target
culture, which is used to fine-tune a T2I model to improve cross-cultural
generation. Experimental results show that our multi-modal metric provides
stronger data selection performance on the C3 benchmark than existing metrics,
in which the object-text alignment is crucial. We release the benchmark, data,
code, and generated images to facilitate future research on culturally diverse
T2I generation (https://github.com/longyuewangdcu/C3-Bench).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingshuai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1&quot;&gt;Chenyang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinsong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02974">
<title>Cross-Spatial Pixel Integration and Cross-Stage Feature Fusion Based Transformer Network for Remote Sensing Image Super-Resolution. (arXiv:2307.02974v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02974</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing image super-resolution (RSISR) plays a vital role in enhancing
spatial detials and improving the quality of satellite imagery. Recently,
Transformer-based models have shown competitive performance in RSISR. To
mitigate the quadratic computational complexity resulting from global
self-attention, various methods constrain attention to a local window,
enhancing its efficiency. Consequently, the receptive fields in a single
attention layer are inadequate, leading to insufficient context modeling.
Furthermore, while most transform-based approaches reuse shallow features
through skip connections, relying solely on these connections treats shallow
and deep features equally, impeding the model&apos;s ability to characterize them.
To address these issues, we propose a novel transformer architecture called
Cross-Spatial Pixel Integration and Cross-Stage Feature Fusion Based
Transformer Network (SPIFFNet) for RSISR. Our proposed model effectively
enhances global cognition and understanding of the entire image, facilitating
efficient integration of features cross-stages. The model incorporates
cross-spatial pixel integration attention (CSPIA) to introduce contextual
information into a local window, while cross-stage feature fusion attention
(CSFFA) adaptively fuses features from the previous stage to improve feature
expression in line with the requirements of the current stage. We conducted
comprehensive experiments on multiple benchmark datasets, demonstrating the
superior performance of our proposed SPIFFNet in terms of both quantitative
metrics and visual quality when compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuting Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_L/0/1/0/all/0/1&quot;&gt;Lingtong Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Le Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1&quot;&gt;Teng Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02978">
<title>Multi-modal multi-class Parkinson disease classification using CNN and decision level fusion. (arXiv:2307.02978v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.02978</link>
<description rdf:parseType="Literal">&lt;p&gt;Parkinson disease is the second most common neurodegenerative disorder, as
reported by the World Health Organization. In this paper, we propose a direct
three-Class PD classification using two different modalities, namely, MRI and
DTI. The three classes used for classification are PD, Scans Without Evidence
of Dopamine Deficit and Healthy Control. We use white matter and gray matter
from the MRI and fractional anisotropy and mean diffusivity from the DTI to
achieve our goal. We train four separate CNNs on the above four types of data.
At the decision level, the outputs of the four CNN models are fused with an
optimal weighted average fusion technique. We achieve an accuracy of 95.53
percentage for the direct three class classification of PD, HC and SWEDD on the
publicly available PPMI database. Extensive comparisons including a series of
ablation studies clearly demonstrate the effectiveness of our proposed
solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1&quot;&gt;Sushanta Kumar Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1&quot;&gt;Ananda S. Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02984">
<title>A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications. (arXiv:2307.02984v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.02984</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) have demonstrated their ability to
generate synthetic samples that match a target distribution. However, from a
privacy perspective, using GANs as a proxy for data sharing is not a safe
solution, as they tend to embed near-duplicates of real samples in the latent
space. Recent works, inspired by k-anonymity principles, address this issue
through sample aggregation in the latent space, with the drawback of reducing
the dataset by a factor of k. Our work aims to mitigate this problem by
proposing a latent space navigation strategy able to generate diverse synthetic
samples that may support effective training of deep models, while addressing
privacy concerns in a principled way. Our approach leverages an auxiliary
identity classifier as a guide to non-linearly walk between points in the
latent space, minimizing the risk of collision with near-duplicates of real
samples. We empirically demonstrate that, given any random pair of points in
the latent space, our walking strategy is safer than linear interpolation. We
then test our path-finding strategy combined to k-same methods and demonstrate,
on two benchmarks for tuberculosis and diabetic retinopathy classification,
that training a model using samples generated by our approach mitigate drops in
performance, while keeping privacy preservation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennisi_M/0/1/0/all/0/1&quot;&gt;Matteo Pennisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salanitri_F/0/1/0/all/0/1&quot;&gt;Federica Proietto Salanitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellitto_G/0/1/0/all/0/1&quot;&gt;Giovanni Bellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1&quot;&gt;Simone Palazzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02997">
<title>Fourier-Net+: Leveraging Band-Limited Representation for Efficient 3D Medical Image Registration. (arXiv:2307.02997v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.02997</link>
<description rdf:parseType="Literal">&lt;p&gt;U-Net style networks are commonly utilized in unsupervised image registration
to predict dense displacement fields, which for high-resolution volumetric
image data is a resource-intensive and time-consuming task. To tackle this
challenge, we first propose Fourier-Net, which replaces the costly U-Net style
expansive path with a parameter-free model-driven decoder. Instead of directly
predicting a full-resolution displacement field, our Fourier-Net learns a
low-dimensional representation of the displacement field in the band-limited
Fourier domain which our model-driven decoder converts to a full-resolution
displacement field in the spatial domain. Expanding upon Fourier-Net, we then
introduce Fourier-Net+, which additionally takes the band-limited spatial
representation of the images as input and further reduces the number of
convolutional layers in the U-Net style network&apos;s contracting path. Finally, to
enhance the registration performance, we propose a cascaded version of
Fourier-Net+. We evaluate our proposed methods on three datasets, on which our
proposed Fourier-Net and its variants achieve comparable results with current
state-of-the art methods, while exhibiting faster inference speeds, lower
memory footprint, and fewer multiply-add operations. With such small
computational cost, our Fourier-Net+ enables the efficient training of
large-scale 3D registration on low-VRAM GPUs. Our code is publicly available at
\url{https://github.com/xi-jia/Fourier-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thorley_A/0/1/0/all/0/1&quot;&gt;Alexander Thorley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gomez_A/0/1/0/all/0/1&quot;&gt;Alberto Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenqi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kotecha_D/0/1/0/all/0/1&quot;&gt;Dipak Kotecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinming Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03007">
<title>Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning. (arXiv:2307.03007v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03007</link>
<description rdf:parseType="Literal">&lt;p&gt;Manual assembly workers face increasing complexity in their work.
Human-centered assistance systems could help, but object recognition as an
enabling technology hinders sophisticated human-centered design of these
systems. At the same time, activity recognition based on hand poses suffers
from poor pose estimation in complex usage scenarios, such as wearing gloves.
This paper presents a self-supervised pipeline for adapting hand pose
estimation to specific use cases with minimal human interaction. This enables
cheap and robust hand posebased activity recognition. The pipeline consists of
a general machine learning model for hand pose estimation trained on a
generalized dataset, spatial and temporal filtering to account for anatomical
constraints of the hand, and a retraining step to improve the model. Different
parameter combinations are evaluated on a publicly available and annotated
dataset. The best parameter and model combination is then applied to unlabelled
videos from a manual assembly scenario. The effectiveness of the pipeline is
demonstrated by training an activity recognition as a downstream task in the
manual assembly scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jauch_C/0/1/0/all/0/1&quot;&gt;Christian Jauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitritz_T/0/1/0/all/0/1&quot;&gt;Timo Leitritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco F. Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03008">
<title>Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation. (arXiv:2307.03008v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03008</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has become a valuable tool for the automation of certain
medical image segmentation tasks, significantly relieving the workload of
medical specialists. Some of these tasks require segmentation to be performed
on a subset of the input dimensions, the most common case being 3D-to-2D.
However, the performance of existing methods is strongly conditioned by the
amount of labeled data available, as there is currently no data efficient
method, e.g. transfer learning, that has been validated on these tasks. In this
work, we propose a novel convolutional neural network (CNN) and self-supervised
learning (SSL) method for label-efficient 3D-to-2D segmentation. The CNN is
composed of a 3D encoder and a 2D decoder connected by novel 3D-to-2D blocks.
The SSL method consists of reconstructing image pairs of modalities with
different dimensionality. The approach has been validated in two tasks with
clinical relevance: the en-face segmentation of geographic atrophy and
reticular pseudodrusen in optical coherence tomography. Results on different
datasets demonstrate that the proposed CNN significantly improves the state of
the art in scenarios with limited labeled data by up to 8% in Dice score.
Moreover, the proposed SSL method allows further improvement of this
performance by up to 23%, and we show that the SSL is beneficial regardless of
the network architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Morano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aresta_G/0/1/0/all/0/1&quot;&gt;Guilherme Aresta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lachinov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Lachinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1&quot;&gt;Julia Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1&quot;&gt;Ursula Schmidt-Erfurth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03017">
<title>EffLiFe: Efficient Light Field Generation via Hierarchical Sparse Gradient Descent. (arXiv:2307.03017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03017</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of Extended Reality (XR) technology, there is a growing need
for real-time light field generation from sparse view inputs. Existing methods
can be classified into offline techniques, which can generate high-quality
novel views but at the cost of long inference/training time, and online
methods, which either lack generalizability or produce unsatisfactory results.
However, we have observed that the intrinsic sparse manifold of Multi-plane
Images (MPI) enables a significant acceleration of light field generation while
maintaining rendering quality. Based on this insight, we introduce EffLiFe, a
novel light field optimization method, which leverages the proposed
Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light
fields from sparse view images in real time. Technically, the coarse MPI of a
scene is first generated using a 3D CNN, and it is further sparsely optimized
by focusing only on important MPI gradients in a few iterations. Nevertheless,
relying solely on optimization can lead to artifacts at occlusion boundaries.
Therefore, we propose an occlusion-aware iterative refinement module that
removes visual artifacts in occluded regions by iteratively filtering the
input. Extensive experiments demonstrate that our method achieves comparable
visual quality while being 100x faster on average than state-of-the-art offline
methods and delivering better performance (about 2 dB higher in PSNR) compared
to other online approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yijie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Lu Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03039">
<title>Art Authentication with Vision Transformers. (arXiv:2307.03039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03039</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1&quot;&gt;Carina Popovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1&quot;&gt;Eric Postma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03073">
<title>Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning. (arXiv:2307.03073v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03073</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel framework for few-shot learning by leveraging large-scale
vision-language models such as CLIP. Motivated by the unimodal prototypical
networks for few-shot learning, we introduce PROTO-CLIP that utilizes image
prototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIP
adapts the image encoder and text encoder in CLIP in a joint fashion using
few-shot examples. The two encoders are used to compute prototypes of image
classes for classification. During adaptation, we propose aligning the image
and text prototypes of corresponding classes. Such a proposed alignment is
beneficial for few-shot classification due to the contributions from both types
of prototypes. We demonstrate the effectiveness of our method by conducting
experiments on benchmark datasets for few-shot learning as well as in the real
world for robot perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+P_J/0/1/0/all/0/1&quot;&gt;Jishnu Jaykumar P&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palanisamy_K/0/1/0/all/0/1&quot;&gt;Kamalesh Palanisamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1&quot;&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinya Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03101">
<title>Contextual Affinity Distillation for Image Anomaly Detection. (arXiv:2307.03101v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03101</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous works on unsupervised industrial anomaly detection mainly focus on
local structural anomalies such as cracks and color contamination. While
achieving significantly high detection performance on this kind of anomaly,
they are faced with logical anomalies that violate the long-range dependencies
such as a normal object placed in the wrong position. In this paper, based on
previous knowledge distillation works, we propose to use two students (local
and global) to better mimic the teacher&apos;s behavior. The local student, which is
used in previous studies mainly focuses on structural anomaly detection while
the global student pays attention to logical anomalies. To further encourage
the global student&apos;s learning to capture long-range dependencies, we design the
global context condensing block (GCCB) and propose a contextual affinity loss
for the student training and anomaly scoring. Experimental results show the
proposed method doesn&apos;t need cumbersome training techniques and achieves a new
state-of-the-art performance on the MVTec LOCO AD dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1&quot;&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03108">
<title>How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models. (arXiv:2307.03108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03108</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image diffusion models have shown surprising performance in
generating high-quality images. However, concerns have arisen regarding the
unauthorized usage of data during the training process. One example is when a
model trainer collects a set of images created by a particular artist and
attempts to train a model capable of generating similar images without
obtaining permission from the artist. To address this issue, it becomes crucial
to detect unauthorized data usage. In this paper, we propose a method for
detecting such unauthorized data usage by planting injected memorization into
the text-to-image diffusion models trained on the protected dataset.
Specifically, we modify the protected image dataset by adding unique contents
on the images such as stealthy image wrapping functions that are imperceptible
to human vision but can be captured and memorized by diffusion models. By
analyzing whether the model has memorization for the injected content (i.e.,
whether the generated images are processed by the chosen post-processing
function), we can detect models that had illegally utilized the unauthorized
data. Our experiments conducted on Stable Diffusion and LoRA model demonstrate
the effectiveness of the proposed method in detecting unauthorized data usages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shiqing Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03110">
<title>LISSNAS: Locality-based Iterative Search Space Shrinkage for Neural Architecture Search. (arXiv:2307.03110v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03110</link>
<description rdf:parseType="Literal">&lt;p&gt;Search spaces hallmark the advancement of Neural Architecture Search (NAS).
Large and complex search spaces with versatile building operators and
structures provide more opportunities to brew promising architectures, yet pose
severe challenges on efficient exploration and exploitation. Subsequently,
several search space shrinkage methods optimize by selecting a single
sub-region that contains some well-performing networks. Small performance and
efficiency gains are observed with these methods but such techniques leave room
for significantly improved search performance and are ineffective at retaining
architectural diversity. We propose LISSNAS, an automated algorithm that
shrinks a large space into a diverse, small search space with SOTA search
performance. Our approach leverages locality, the relationship between
structural and performance similarity, to efficiently extract many pockets of
well-performing networks. We showcase our method on an array of search spaces
spanning various sizes and datasets. We accentuate the effectiveness of our
shrunk spaces when used in one-shot search by achieving the best Top-1 accuracy
in two different search spaces. Our method achieves a SOTA Top-1 accuracy of
77.6\% in ImageNet under mobile constraints, best-in-class Kendal-Tau,
architectural diversity, and search space size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopal_B/0/1/0/all/0/1&quot;&gt;Bhavna Gopal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Arjun Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tunhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiran Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03128">
<title>Principal subbundles for dimension reduction. (arXiv:2307.03128v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.03128</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we demonstrate how sub-Riemannian geometry can be used for
manifold learning and surface reconstruction by combining local linear
approximations of a point cloud to obtain lower dimensional bundles. Local
approximations obtained by local PCAs are collected into a rank $k$ tangent
subbundle on $\mathbb{R}^d$, $k&amp;lt;d$, which we call a principal subbundle. This
determines a sub-Riemannian metric on $\mathbb{R}^d$. We show that
sub-Riemannian geodesics with respect to this metric can successfully be
applied to a number of important problems, such as: explicit construction of an
approximating submanifold $M$, construction of a representation of the
point-cloud in $\mathbb{R}^k$, and computation of distances between
observations, taking the learned geometry into account. The reconstruction is
guaranteed to equal the true submanifold in the limit case where tangent spaces
are estimated exactly. Via simulations, we show that the framework is robust
when applied to noisy data. Furthermore, the framework generalizes to
observations on an a priori known Riemannian manifold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Akhoj_M/0/1/0/all/0/1&quot;&gt;Morten Akh&amp;#xf8;j&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benn_J/0/1/0/all/0/1&quot;&gt;James Benn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grong_E/0/1/0/all/0/1&quot;&gt;Erlend Grong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sommer_S/0/1/0/all/0/1&quot;&gt;Stefan Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pennec_X/0/1/0/all/0/1&quot;&gt;Xavier Pennec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03132">
<title>T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03132</link>
<description rdf:parseType="Literal">&lt;p&gt;Large web-sourced multimodal datasets have powered a slew of new methods for
learning general-purpose visual representations, advancing the state of the art
in computer vision and revolutionizing zero- and few-shot recognition. One
crucial decision facing practitioners is how, if at all, to curate these
ever-larger datasets. For example, the creators of the LAION-5B dataset chose
to retain only image-caption pairs whose CLIP similarity score exceeded a
designated threshold. In this paper, we propose a new state-of-the-art data
filtering approach motivated by our observation that nearly 40% of LAION&apos;s
images contain text that overlaps significantly with the caption. Intuitively,
such data could be wasteful as it incentivizes models to perform optical
character recognition rather than learning visual features. However, naively
removing all such data could also be wasteful, as it throws away images that
contain visual features (in addition to overlapping text). Our simple and
scalable approach, T-MARS (Text Masking and Re-Scoring), filters out only those
pairs where the text dominates the remaining visual features -- by first
masking out the text and then filtering out those with a low CLIP similarity
score of the masked image. Experimentally, T-MARS outperforms the top-ranked
method on the &quot;medium scale&quot; of DataComp (a data filtering benchmark) by a
margin of 6.5% on ImageNet and 4.7% on VTAB. Additionally, our systematic
evaluation on various data pool sizes from 2M to 64M shows that the accuracy
gains enjoyed by T-MARS linearly increase as data and compute are scaled
exponentially. Code is available at https://github.com/locuslab/T-MARS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1&quot;&gt;Pratyush Maini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1&quot;&gt;Sachin Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1&quot;&gt;Aditi Raghunathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03133">
<title>Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification. (arXiv:2307.03133v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03133</link>
<description rdf:parseType="Literal">&lt;p&gt;Test-time adaptation (TTA) is a technique aimed at enhancing the
generalization performance of models by leveraging unlabeled samples solely
during prediction. Given the need for robustness in neural network systems when
faced with distribution shifts, numerous TTA methods have recently been
proposed. However, evaluating these methods is often done under different
settings, such as varying distribution shifts, backbones, and designing
scenarios, leading to a lack of consistent and fair benchmarks to validate
their effectiveness. To address this issue, we present a benchmark that
systematically evaluates 13 prominent TTA methods and their variants on five
widely used image classification datasets: CIFAR-10-C, CIFAR-100-C, ImageNet-C,
DomainNet, and Office-Home. These methods encompass a wide range of adaptation
scenarios (e.g. online adaptation v.s. offline adaptation, instance adaptation
v.s. batch adaptation v.s. domain adaptation). Furthermore, we explore the
compatibility of different TTA methods with diverse network backbones. To
implement this benchmark, we have developed a unified framework in PyTorch,
which allows for consistent evaluation and comparison of the TTA methods across
the different datasets and network architectures. By establishing this
benchmark, we aim to provide researchers and practitioners with a reliable
means of assessing and comparing the effectiveness of TTA methods in improving
model robustness and generalization performance. Our code is available at
https://github.com/yuyongcan/Benchmark-TTA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yongcan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lijun Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jian Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03135">
<title>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student&apos;s OOD
generalization: (1) by better imitating teacher&apos;s visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher&apos;s language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Our code will be released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03137">
<title>Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03137</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation networks are not explicitly imposed to learn global invariants
of an image, such as the shape of an object and the geometry between multiple
objects, when they are trained with a standard loss function. On the other
hand, incorporating such invariants into network training may help improve
performance for various segmentation tasks when they are the intrinsic
characteristics of the objects to be segmented. One example is segmentation of
aorta and great vessels in computed tomography (CT) images where vessels are
found in a particular geometry in the body due to the human anatomy and they
mostly seem as round objects on a 2D CT image. This paper addresses this issue
by introducing a new topology-aware loss function that penalizes topology
dissimilarities between the ground truth and prediction through persistent
homology. Different from the previously suggested segmentation network designs,
which apply the threshold filtration on a likelihood function of the prediction
map and the Betti numbers of the ground truth, this paper proposes to apply the
Vietoris-Rips filtration to obtain persistence diagrams of both ground truth
and prediction maps and calculate the dissimilarity with the Wasserstein
distance between the corresponding persistence diagrams. The use of this
filtration has advantage of modeling shape and geometry at the same time, which
may not happen when the threshold filtration is applied. Our experiments on
4327 CT images of 24 subjects reveal that the proposed topology-aware loss
function leads to better results than its counterparts, indicating the
effectiveness of this use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozcelik_S/0/1/0/all/0/1&quot;&gt;Seher Ozcelik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Unver_S/0/1/0/all/0/1&quot;&gt;Sinan Unver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gurses_I/0/1/0/all/0/1&quot;&gt;Ilke Ali Gurses&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turkay_R/0/1/0/all/0/1&quot;&gt;Rustu Turkay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gunduz_Demir_C/0/1/0/all/0/1&quot;&gt;Cigdem Gunduz-Demir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03153">
<title>MultiVENT: Multilingual Videos of Events with Aligned Natural Text. (arXiv:2307.03153v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.03153</link>
<description rdf:parseType="Literal">&lt;p&gt;Everyday news coverage has shifted from traditional broadcasts towards a wide
range of presentation formats such as first-hand, unedited video footage.
Datasets that reflect the diverse array of multimodal, multilingual news
sources available online could be used to teach models to benefit from this
shift, but existing news video datasets focus on traditional news broadcasts
produced for English-speaking audiences. We address this limitation by
constructing MultiVENT, a dataset of multilingual, event-centric videos
grounded in text documents across five target languages. MultiVENT includes
both news broadcast videos and non-professional event footage, which we use to
analyze the state of online news videos and how they can be leveraged to build
robust, factually accurate models. Finally, we provide a model for complex,
multilingual video retrieval to serve as a baseline for information retrieval
using MultiVENT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanders_K/0/1/0/all/0/1&quot;&gt;Kate Sanders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etter_D/0/1/0/all/0/1&quot;&gt;David Etter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kriz_R/0/1/0/all/0/1&quot;&gt;Reno Kriz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Durme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03157">
<title>Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?. (arXiv:2307.03157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03157</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based diagnostic system has demonstrated potential in
classifying skin cancer conditions when labeled training example are abundant.
However, skin lesion analysis often suffers from a scarcity of labeled data,
hindering the development of an accurate and reliable diagnostic system. In
this work, we leverage multiple skin lesion datasets and investigate the
feasibility of various unsupervised domain adaptation (UDA) methods in binary
and multi-class skin lesion classification. In particular, we assess three UDA
training schemes: single-, combined-, and multi-source. Our experiment results
show that UDA is effective in binary classification, with further improvement
being observed when imbalance is mitigated. In multi-class task, its
performance is less prominent, and imbalance problem again needs to be
addressed to achieve above-baseline accuracy. Through our quantitative
analysis, we find that the test error of multi-class tasks is strongly
correlated with label shift, and feature-level UDA methods have limitations
when handling imbalanced datasets. Finally, our study reveals that UDA can
effectively reduce bias against minority groups and promote fairness, even
without the explicit use of fairness-focused techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Janet Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunbei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhengming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamm_J/0/1/0/all/0/1&quot;&gt;Jihun Hamm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03166">
<title>VideoGLUE: Video General Understanding Evaluation of Foundation Models. (arXiv:2307.03166v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03166</link>
<description rdf:parseType="Literal">&lt;p&gt;We evaluate existing foundation models video understanding capabilities using
a carefully designed experiment protocol consisting of three hallmark tasks
(action recognition, temporal localization, and spatiotemporal localization),
eight datasets well received by the community, and four adaptation methods
tailoring a foundation model (FM) for a downstream task. Moreover, we propose a
scalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency when
adapting to general video understanding tasks. Our main findings are as
follows. First, task-specialized models significantly outperform the six FMs
studied in this work, in sharp contrast to what FMs have achieved in natural
language and image understanding. Second,video-native FMs, whose pretraining
data contains the video modality, are generally better than image-native FMs in
classifying motion-rich videos, localizing actions in time, and understanding a
video of more than one action. Third, the video-native FMs can perform well on
video tasks under light adaptations to downstream tasks(e.g., freezing the FM
backbones), while image-native FMs win in full end-to-end finetuning. The first
two observations reveal the need and tremendous opportunities to conduct
research on video-focused FMs, and the last confirms that both tasks and
adaptation methods matter when it comes to the evaluation of FMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Liangzhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundavarapu_N/0/1/0/all/0/1&quot;&gt;Nitesh Bharadwaj Gundavarapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Long Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1&quot;&gt;Menglin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1&quot;&gt;Tobias Weyand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_L/0/1/0/all/0/1&quot;&gt;Luke Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1&quot;&gt;Mikhail Sirotenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1&quot;&gt;Florian Schroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1&quot;&gt;Boqing Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03175">
<title>Push Past Green: Learning to Look Behind Plant Foliage by Moving It. (arXiv:2307.03175v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.03175</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous agriculture applications (e.g., inspection, phenotyping, plucking
fruits) require manipulating the plant foliage to look behind the leaves and
the branches. Partial visibility, extreme clutter, thin structures, and unknown
geometry and dynamics for plants make such manipulation challenging. We tackle
these challenges through data-driven methods. We use self-supervision to train
SRPNet, a neural network that predicts what space is revealed on execution of a
candidate action on a given plant. We use SRPNet with the cross-entropy method
to predict actions that are effective at revealing space beneath plant foliage.
Furthermore, as SRPNet does not just predict how much space is revealed but
also where it is revealed, we can execute a sequence of actions that
incrementally reveal more and more space beneath the plant foliage. We
experiment with a synthetic (vines) and a real plant (Dracaena) on a physical
test-bed across 5 settings including 2 settings that test generalization to
novel plant configurations. Our experiments reveal the effectiveness of our
overall method, PPG, over a competitive hand-crafted exploration method, and
the effectiveness of SRPNet over a hand-crafted dynamics model and relevant
ablations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Saurabh Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03177">
<title>IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via Latent Diffusion Model. (arXiv:2307.03177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03177</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating complete 360-degree panoramas from narrow field of view images is
ongoing research as omnidirectional RGB data is not readily available. Existing
GAN-based approaches face some barriers to achieving higher quality output, and
have poor generalization performance over different mask types. In this paper,
we present our 360-degree indoor RGB panorama outpainting model using latent
diffusion models (LDM), called IPO-LDM. We introduce a new bi-modal latent
diffusion structure that utilizes both RGB and depth panoramic data during
training, but works surprisingly well to outpaint normal depth-free RGB images
during inference. We further propose a novel technique of introducing
progressive camera rotations during each diffusion denoising step, which leads
to substantial improvement in achieving panorama wraparound consistency.
Results show that our IPO-LDM not only significantly outperforms
state-of-the-art methods on RGB panorama outpainting, but can also produce
multiple and diverse well-structured results for different types of masks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanxia Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1&quot;&gt;Tat-Jen Cham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03190">
<title>Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Artistic Cinemagraph, a fully automated method for creating
cinemagraphs from text descriptions - an especially challenging task when
prompts feature imaginary elements and artistic styles, given the complexity of
interpreting the semantics and motions of these images. Existing single-image
animation methods fall short on artistic inputs, and recent text-based video
methods frequently introduce temporal inconsistencies, struggling to keep
certain regions static. To address these challenges, we propose an idea of
synthesizing image twins from a single text prompt - a pair of an artistic
image and its pixel-aligned corresponding natural-looking twin. While the
artistic image depicts the style and appearance detailed in our text prompt,
the realistic counterpart greatly simplifies layout and motion analysis.
Leveraging existing natural image and video datasets, we can accurately segment
the realistic image and predict plausible motion given the semantic
information. The predicted motion can then be transferred to the artistic image
to create the final cinemagraph. Our method outperforms existing approaches in
creating cinemagraphs for natural landscapes as well as artistic and
other-worldly scenes, as validated by automated metrics and user studies.
Finally, we demonstrate two extensions: animating existing paintings and
controlling motion directions using text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1&quot;&gt;Aniruddha Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1&quot;&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1&quot;&gt;Sergey Tulyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.09118">
<title>Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.09118</link>
<description rdf:parseType="Literal">&lt;p&gt;There are demographic biases present in current facial recognition (FR)
models. To measure these biases across different ethnic and gender subgroups,
we introduce our Balanced Faces in the Wild (BFW) dataset. This dataset allows
for the characterization of FR performance per subgroup. We found that relying
on a single score threshold to differentiate between genuine and imposters
sample pairs leads to suboptimal results. Additionally, performance within
subgroups often varies significantly from the global average. Therefore,
specific error rates only hold for populations that match the validation data.
To mitigate imbalanced performances, we propose a novel domain adaptation
learning scheme that uses facial features extracted from state-of-the-art
neural networks. This scheme boosts the average performance and preserves
identity information while removing demographic knowledge. Removing demographic
knowledge prevents potential biases from affecting decision-making and protects
privacy by eliminating demographic information. We explore the proposed method
and demonstrate that subgroup classifiers can no longer learn from features
projected using our domain adaptation scheme. For access to the source code and
data, please visit https://github.com/visionjo/facerec-bias-bfw.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1&quot;&gt;Joseph P Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Can Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henon_Y/0/1/0/all/0/1&quot;&gt;Yann Henon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timoner_S/0/1/0/all/0/1&quot;&gt;Samson Timoner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.14517">
<title>Robust and Accurate Superquadric Recovery: a Probabilistic Approach. (arXiv:2111.14517v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.14517</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting objects with basic geometric primitives has long been studied in
computer vision. Among geometric primitives, superquadrics are well known for
their ability to represent a wide range of shapes with few parameters. However,
as the first and foremost step, recovering superquadrics accurately and
robustly from 3D data still remains challenging. The existing methods are
subject to local optima and sensitive to noise and outliers in real-world
scenarios, resulting in frequent failure in capturing geometric shapes. In this
paper, we propose the first probabilistic method to recover superquadrics from
point clouds. Our method builds a Gaussian-uniform mixture model (GUM) on the
parametric surface of a superquadric, which explicitly models the generation of
outliers and noise. The superquadric recovery is formulated as a Maximum
Likelihood Estimation (MLE) problem. We propose an algorithm, Expectation,
Maximization, and Switching (EMS), to solve this problem, where: (1) outliers
are predicted from the posterior perspective; (2) the superquadric parameter is
optimized by the trust-region reflective algorithm; and (3) local optima are
avoided by globally searching and switching among parameters encoding similar
superquadrics. We show that our method can be extended to the
multi-superquadrics recovery for complex objects. The proposed method
outperforms the state-of-the-art in terms of accuracy, efficiency, and
robustness on both synthetic and real-world datasets. The code is at
&lt;a href=&quot;http://github.com/bmlklwx/EMS-superquadric_fitting.git.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weixiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Sipu Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chirikjian_G/0/1/0/all/0/1&quot;&gt;Gregory S. Chirikjian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05145">
<title>Fully Adaptive Bayesian Algorithm for Data Analysis, FABADA. (arXiv:2201.05145v2 [astro-ph.IM] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05145</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this paper is to describe a novel non-parametric noise reduction
technique from the point of view of Bayesian inference that may automatically
improve the signal-to-noise ratio of one- and two-dimensional data, such as
e.g. astronomical images and spectra. The algorithm iteratively evaluates
possible smoothed versions of the data, the smooth models, obtaining an
estimation of the underlying signal that is statistically compatible with the
noisy measurements. Iterations stop based on the evidence and the $\chi^2$
statistic of the last smooth model, and we compute the expected value of the
signal as a weighted average of the whole set of smooth models. In this paper,
we explain the mathematical formalism and numerical implementation of the
algorithm, and we evaluate its performance in terms of the peak signal to noise
ratio, the structural similarity index, and the time payload, using a battery
of real astronomical observations. Our Fully Adaptive Bayesian Algorithm for
Data Analysis (FABADA) yields results that, without any parameter tuning, are
comparable to standard image processing algorithms whose parameters have been
optimized based on the true signal to be recovered, something that is
impossible in a real application. State-of-the-art non-parametric methods, such
as BM3D, offer slightly better performance at high signal-to-noise ratio, while
our algorithm is significantly more accurate for extremely noisy data (higher
than $20-40\%$ relative errors, a situation of particular interest in the field
of astronomy). In this range, the standard deviation of the residuals obtained
by our reconstruction may become more than an order of magnitude lower than
that of the original measurements. The source code needed to reproduce all the
results presented in this report, including the implementation of the method,
is publicly available at https://github.com/PabloMSanAla/fabada
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sanchez_Alarcon_P/0/1/0/all/0/1&quot;&gt;Pablo M Sanchez-Alarcon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sequeiros_Y/0/1/0/all/0/1&quot;&gt;Yago Ascasibar Sequeiros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04779">
<title>Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations. (arXiv:2206.04779v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04779</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning has shown great promise in leveraging large
pre-collected datasets for policy learning, allowing agents to forgo
often-expensive online data collection. However, offline reinforcement learning
from visual observations with continuous action spaces remains under-explored,
with a limited understanding of the key challenges in this complex domain. In
this paper, we establish simple baselines for continuous control in the visual
domain and introduce a suite of benchmarking tasks for offline reinforcement
learning from visual observations designed to better represent the data
distributions present in real-world offline RL problems and guided by a set of
desiderata for offline RL from visual observations, including robustness to
visual distractions and visually identifiable changes in dynamics. Using this
suite of benchmarking tasks, we show that simple modifications to two popular
vision-based online reinforcement learning algorithms, DreamerV2 and DrQ-v2,
suffice to outperform existing offline RL methods and establish competitive
baselines for continuous control in the visual domain. We rigorously evaluate
these algorithms and perform an empirical evaluation of the differences between
state-of-the-art model-based and model-free offline RL methods for continuous
control from visual observations. All code and data used in this evaluation are
open-sourced to facilitate progress in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1&quot;&gt;Philip J. Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudner_T/0/1/0/all/0/1&quot;&gt;Tim G. J. Rudner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1&quot;&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osborne_M/0/1/0/all/0/1&quot;&gt;Michael A. Osborne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05251">
<title>Spotting Virus from Satellites: Modeling the Circulation of West Nile Virus Through Graph Neural Networks. (arXiv:2209.05251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05251</link>
<description rdf:parseType="Literal">&lt;p&gt;The occurrence of West Nile Virus (WNV) represents one of the most common
mosquito-borne zoonosis viral infections. Its circulation is usually associated
with climatic and environmental conditions suitable for vector proliferation
and virus replication. On top of that, several statistical models have been
developed to shape and forecast WNV circulation: in particular, the recent
massive availability of Earth Observation (EO) data, coupled with the
continuous advances in the field of Artificial Intelligence, offer valuable
opportunities.
&lt;/p&gt;
&lt;p&gt;In this paper, we seek to predict WNV circulation by feeding Deep Neural
Networks (DNNs) with satellite images, which have been extensively shown to
hold environmental and climatic features. Notably, while previous approaches
analyze each geographical site independently, we propose a spatial-aware
approach that considers also the characteristics of close sites. Specifically,
we build upon Graph Neural Networks (GNN) to aggregate features from
neighbouring places, and further extend these modules to consider multiple
relations, such as the difference in temperature and soil moisture between two
sites, as well as the geographical distance. Moreover, we inject time-related
information directly into the model to take into account the seasonality of
virus spread.
&lt;/p&gt;
&lt;p&gt;We design an experimental setting that combines satellite images - from
Landsat and Sentinel missions - with ground truth observations of WNV
circulation in Italy. We show that our proposed Multi-Adjacency Graph Attention
Network (MAGAT) consistently leads to higher performance when paired with an
appropriate pre-training stage. Finally, we assess the importance of each
component of MAGAT in our ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonicelli_L/0/1/0/all/0/1&quot;&gt;Lorenzo Bonicelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porrello_A/0/1/0/all/0/1&quot;&gt;Angelo Porrello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincenzi_S/0/1/0/all/0/1&quot;&gt;Stefano Vincenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ippoliti_C/0/1/0/all/0/1&quot;&gt;Carla Ippoliti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iapaolo_F/0/1/0/all/0/1&quot;&gt;Federica Iapaolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conte_A/0/1/0/all/0/1&quot;&gt;Annamaria Conte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1&quot;&gt;Simone Calderara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06293">
<title>Do Androids Laugh at Electric Sheep? Humor &quot;Understanding&quot; Benchmarks from The New Yorker Caption Contest. (arXiv:2209.06293v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06293</link>
<description rdf:parseType="Literal">&lt;p&gt;Large neural networks can now generate jokes, but do they really &quot;understand&quot;
humor? We challenge AI models with three tasks derived from the New Yorker
Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning
caption, and explaining why a winning caption is funny. These tasks encapsulate
progressively more sophisticated aspects of &quot;understanding&quot; a cartoon; key
elements are the complex, often surprising relationships between images and
captions and the frequent inclusion of indirect and playful allusions to human
experience and culture. We investigate both multimodal and language-only
models: the former are challenged with the cartoon images directly, while the
latter are given multifaceted descriptions of the visual scene to simulate
human-level visual understanding. We find that both types of models struggle at
all three tasks. For example, our best multimodal models fall 30 accuracy
points behind human performance on the matching task, and, even when provided
ground-truth visual scene descriptors, human-authored explanations are
preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in
more than 2/3 of cases. We release models, code, leaderboard, and corpus, which
includes newly-gathered annotations describing the image&apos;s locations/entities,
what&apos;s unusual in the scene, and an explanation of the joke.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1&quot;&gt;Ana Marasovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jena D. Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1&quot;&gt;Lillian Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_J/0/1/0/all/0/1&quot;&gt;Jeff Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1&quot;&gt;Rowan Zellers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mankoff_R/0/1/0/all/0/1&quot;&gt;Robert Mankoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13802">
<title>Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention. (arXiv:2209.13802v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13802</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformer has emerged as a new paradigm in computer vision, showing
excellent performance while accompanied by expensive computational cost. Image
token pruning is one of the main approaches for ViT compression, due to the
facts that the complexity is quadratic with respect to the token number, and
many tokens containing only background regions do not truly contribute to the
final prediction. Existing works either rely on additional modules to score the
importance of individual tokens, or implement a fixed ratio pruning strategy
for different input instances. In this work, we propose an adaptive sparse
token pruning framework with a minimal cost. Specifically, we firstly propose
an inexpensive attention head importance weighted class attention scoring
mechanism. Then, learnable parameters are inserted as thresholds to distinguish
informative tokens from unimportant ones. By comparing token attention scores
and thresholds, we can discard useless tokens hierarchically and thus
accelerate inference. The learnable thresholds are optimized in budget-aware
training to balance accuracy and complexity, performing the corresponding
pruning configurations for different input instances. Extensive experiments
demonstrate the effectiveness of our approach. Our method improves the
throughput of DeiT-S by 50% and brings only 0.2% drop in top-1 accuracy, which
achieves a better trade-off between accuracy and latency than the previous
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1&quot;&gt;Guodong Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14896">
<title>DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. (arXiv:2210.14896v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14896</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent advancements in diffusion models, users can generate high-quality
images by writing text prompts in natural language. However, generating images
with desired details requires proper prompts, and it is often unclear how a
model reacts to different prompts or what the best prompts are. To help
researchers tackle these critical challenges, we introduce DiffusionDB, the
first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14
million images generated by Stable Diffusion, 1.8 million unique prompts, and
hyperparameters specified by real users. We analyze the syntactic and semantic
characteristics of prompts. We pinpoint specific hyperparameter values and
prompt styles that can lead to model errors and present evidence of potentially
harmful model usage, such as the generation of misinformation. The
unprecedented scale and diversity of this human-actuated dataset provide
exciting research opportunities in understanding the interplay between prompts
and generative models, detecting deepfakes, and designing human-AI interaction
tools to help users more easily use these models. DiffusionDB is publicly
available at: https://poloclub.github.io/diffusiondb.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montoya_E/0/1/0/all/0/1&quot;&gt;Evan Montoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munechika_D/0/1/0/all/0/1&quot;&gt;David Munechika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haoyang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1&quot;&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10580">
<title>Normal Transformer: Extracting Surface Geometry from LiDAR Points Enhanced by Visual Semantics. (arXiv:2211.10580v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10580</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality estimation of surface normal can help reduce ambiguity in many
geometry understanding problems, such as collision avoidance and occlusion
inference. This paper presents a technique for estimating the normal from 3D
point clouds and 2D colour images. We have developed a transformer neural
network that learns to utilise the hybrid information of visual semantic and 3D
geometric data, as well as effective learning strategies. Compared to existing
methods, the information fusion of the proposed method is more effective, which
is supported by experiments. We have also built a simulation environment of
outdoor traffic scenes in a 3D rendering engine to obtain annotated data to
train the normal estimator. The model trained on synthetic data is tested on
the real scenes in the KITTI dataset. And subsequent tasks built upon the
estimated normal directions in the KITTI dataset show that the proposed
estimator has advantage over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1&quot;&gt;Ancheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16342">
<title>Fourier-Net: Fast Image Registration with Band-limited Deformation. (arXiv:2211.16342v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16342</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image registration commonly adopts U-Net style networks to
predict dense displacement fields in the full-resolution spatial domain. For
high-resolution volumetric image data, this process is however
resource-intensive and time-consuming. To tackle this problem, we propose the
Fourier-Net, replacing the expansive path in a U-Net style network with a
parameter-free model-driven decoder. Specifically, instead of our Fourier-Net
learning to output a full-resolution displacement field in the spatial domain,
we learn its low-dimensional representation in a band-limited Fourier domain.
This representation is then decoded by our devised model-driven decoder
(consisting of a zero padding layer and an inverse discrete Fourier transform
layer) to the dense, full-resolution displacement field in the spatial domain.
These changes allow our unsupervised Fourier-Net to contain fewer parameters
and computational operations, resulting in faster inference speeds. Fourier-Net
is then evaluated on two public 3D brain datasets against various
state-of-the-art approaches. For example, when compared to a recent
transformer-based method, named TransMorph, our Fourier-Net, which only uses
2.2\% of its parameters and 6.66\% of the multiply-add operations, achieves a
0.5\% higher Dice score and an 11.48 times faster inference speed. Code is
available at \url{https://github.com/xi-jia/Fourier-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartlett_J/0/1/0/all/0/1&quot;&gt;Joseph Bartlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Siyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xinxing Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenqi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zhaowen Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinming Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08490">
<title>LOANet: A Lightweight Network Using Object Attention for Extracting Buildings and Roads from UAV Aerial Remote Sensing Images. (arXiv:2212.08490v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08490</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation for extracting buildings and roads from uncrewed aerial
vehicle (UAV) remote sensing images by deep learning becomes a more efficient
and convenient method than traditional manual segmentation in surveying and
mapping fields. In order to make the model lightweight and improve the model
accuracy, a Lightweight Network Using Object Attention (LOANet) for Buildings
and Roads from UAV Aerial Remote Sensing Images is proposed. The proposed
network adopts an encoder-decoder architecture in which a Lightweight Densely
Connected Network (LDCNet) is developed as the encoder. In the decoder part,
the dual multi-scale context modules which consist of the Atrous Spatial
Pyramid Pooling module (ASPP) and the Object Attention Module (OAM) are
designed to capture more context information from feature maps of UAV remote
sensing images. Between ASPP and OAM, a Feature Pyramid Network (FPN) module is
used to fuse multi-scale features extracted from ASPP. A private dataset of
remote sensing images taken by UAV which contains 2431 training sets, 945
validation sets, and 475 test sets is constructed. The proposed basic model
performs well on this dataset, with only 1.4M parameters and 5.48G floating
point operations (FLOPs), achieving excellent mean Intersection-over-Union
(mIoU). Further experiments on the publicly available LoveDA and CITY-OSM
datasets have been conducted to further validate the effectiveness of the
proposed basic and large model, and outstanding mIoU results have been
achieved. All codes are available on https://github.com/GtLinyer/LOANet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoxiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiman Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuanjie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiaohong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01054">
<title>Benchmarking common uncertainty estimation methods with histopathological images under domain shift and label noise. (arXiv:2301.01054v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01054</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past years, deep learning has seen an increase in usage in the domain
of histopathological applications. However, while these approaches have shown
great potential, in high-risk environments deep learning models need to be able
to judge their uncertainty and be able to reject inputs when there is a
significant chance of misclassification. In this work, we conduct a rigorous
evaluation of the most commonly used uncertainty and robustness methods for the
classification of Whole Slide Images, with a focus on the task of selective
classification, where the model should reject the classification in situations
in which it is uncertain. We conduct our experiments on tile-level under the
aspects of domain shift and label noise, as well as on slide-level. In our
experiments, we compare Deep Ensembles, Monte-Carlo Dropout, Stochastic
Variational Inference, Test-Time Data Augmentation as well as ensembles of the
latter approaches. We observe that ensembles of methods generally lead to
better uncertainty estimates as well as an increased robustness towards domain
shifts and label noise, while contrary to results from classical computer
vision benchmarks no systematic gain of the other methods can be shown. Across
methods, a rejection of the most uncertain samples reliably leads to a
significant increase in classification accuracy on both in-distribution as well
as out-of-distribution data. Furthermore, we conduct experiments comparing
these methods under varying conditions of label noise. Lastly, we publish our
code framework to facilitate further research on uncertainty estimation on
histopathological data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehrtens_H/0/1/0/all/0/1&quot;&gt;Hendrik A. Mehrtens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurz_A/0/1/0/all/0/1&quot;&gt;Alexander Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bucher_T/0/1/0/all/0/1&quot;&gt;Tabea-Clara Bucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brinker_T/0/1/0/all/0/1&quot;&gt;Titus J. Brinker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13166">
<title>ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation. (arXiv:2301.13166v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13166</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to accurately locate and navigate to a specific object is a
crucial capability for embodied agents that operate in the real world and
interact with objects to complete tasks. Such object navigation tasks usually
require large-scale training in visual environments with labeled objects, which
generalizes poorly to novel objects in unknown environments. In this work, we
present a novel zero-shot object navigation method, Exploration with Soft
Commonsense constraints (ESC), that transfers commonsense knowledge in
pre-trained models to open-world object navigation without any navigation
experience nor any other training on the visual environments. First, ESC
leverages a pre-trained vision and language model for open-world prompt-based
grounding and a pre-trained commonsense language model for room and object
reasoning. Then ESC converts commonsense knowledge into navigation actions by
modeling it as soft logic predicates for efficient exploration. Extensive
experiments on MP3D, HM3D, and RoboTHOR benchmarks show that our ESC method
improves significantly over baselines, and achieves new state-of-the-art
results for zero-shot object navigation (e.g., 288% relative Success Rate
improvement than CoW on MP3D).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaiwen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaizhi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1&quot;&gt;Connor Pryor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yilin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongxia Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1&quot;&gt;Lise Getoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06685">
<title>The Sum of Its Parts: Visual Part Segmentation for Inertial Parameter Identification of Manipulated Objects. (arXiv:2302.06685v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06685</link>
<description rdf:parseType="Literal">&lt;p&gt;To operate safely and efficiently alongside human workers, collaborative
robots (cobots) require the ability to quickly understand the dynamics of
manipulated objects. However, traditional methods for estimating the full set
of inertial parameters rely on motions that are necessarily fast and unsafe (to
achieve a sufficient signal-to-noise ratio). In this work, we take an
alternative approach: by combining visual and force-torque measurements, we
develop an inertial parameter identification algorithm that requires slow or
&apos;stop-and-go&apos; motions only, and hence is ideally tailored for use around
humans. Our technique, called Homogeneous Part Segmentation (HPS), leverages
the observation that man-made objects are often composed of distinct,
homogeneous parts. We combine a surface-based point clustering method with a
volumetric shape segmentation algorithm to quickly produce a part-level
segmentation of a manipulated object; the segmented representation is then used
by HPS to accurately estimate the object&apos;s inertial parameters. To benchmark
our algorithm, we create and utilize a novel dataset consisting of realistic
meshes, segmented point clouds, and inertial parameters for 20 common workshop
tools. Finally, we demonstrate the real-world performance and accuracy of HPS
by performing an intricate &apos;hammer balancing act&apos; autonomously and online with
a low-cost collaborative robotic arm. Our code and dataset are open source and
freely available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeau_P/0/1/0/all/0/1&quot;&gt;Philippe Nadeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giamou_M/0/1/0/all/0/1&quot;&gt;Matthew Giamou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1&quot;&gt;Jonathan Kelly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10473">
<title>Oriented Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey. (arXiv:2302.10473v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10473</link>
<description rdf:parseType="Literal">&lt;p&gt;Oriented object detection is one of the most fundamental and challenging
tasks in remote sensing, aiming at locating the oriented objects of numerous
predefined object categories. Recently, deep learning based methods have
achieved remarkable performance in detecting oriented objects in optical remote
sensing imagery. However, a thorough review of the literature in remote sensing
has not yet emerged. Therefore, we give a comprehensive survey of recent
advances and cover many aspects of oriented object detection, including problem
definition, commonly used datasets, evaluation protocols, detection frameworks,
oriented object representations, and feature representations. Besides, the
state-of-the-art methods are analyzed and discussed. We finally discuss future
research directions to put forward some useful research guidance. We believe
that this survey shall be valuable to researchers across academia and industry
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lia_Z/0/1/0/all/0/1&quot;&gt;Zhang Lia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sua_A/0/1/0/all/0/1&quot;&gt;Ang Sua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenga_X/0/1/0/all/0/1&quot;&gt;Xichao Tenga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liua_M/0/1/0/all/0/1&quot;&gt;Minhao Liua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yua_Q/0/1/0/all/0/1&quot;&gt;Qifeng Yua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03108">
<title>Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization. (arXiv:2303.03108v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03108</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, flat minima are proven to be effective for improving generalization
and sharpness-aware minimization (SAM) achieves state-of-the-art performance.
Yet the current definition of flatness discussed in SAM and its follow-ups are
limited to the zeroth-order flatness (i.e., the worst-case loss within a
perturbation radius). We show that the zeroth-order flatness can be
insufficient to discriminate minima with low generalization error from those
with high generalization error both when there is a single minimum or multiple
minima within the given perturbation radius. Thus we present first-order
flatness, a stronger measure of flatness focusing on the maximal gradient norm
within a perturbation radius which bounds both the maximal eigenvalue of
Hessian at local minima and the regularization function of SAM. We also present
a novel training procedure named Gradient norm Aware Minimization (GAM) to seek
minima with uniformly small curvature across all directions. Experimental
results show that GAM improves the generalization of models trained with
current optimizers such as SGD and AdamW on various datasets and networks.
Furthermore, we show that GAM can help SAM find flatter minima and achieve
better generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renzhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Hao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09412">
<title>NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters. (arXiv:2303.09412v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09412</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis using neural radiance fields (NeRF) is the
state-of-the-art technique for generating high-quality images from novel
viewpoints. Existing methods require a priori knowledge about extrinsic and
intrinsic camera parameters. This limits their applicability to synthetic
scenes, or real-world scenarios with the necessity of a preprocessing step.
Current research on the joint optimization of camera parameters and NeRF
focuses on refining noisy extrinsic camera parameters and often relies on the
preprocessing of intrinsic camera parameters. Further approaches are limited to
cover only one single camera intrinsic. To address these limitations, we
propose a novel end-to-end trainable approach called NeRFtrinsic Four. We
utilize Gaussian Fourier features to estimate extrinsic camera parameters and
dynamically predict varying intrinsic camera parameters through the supervision
of the projection error. Our approach outperforms existing joint optimization
methods on LLFF and BLEFF. In addition to these existing datasets, we introduce
a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic
Four is a step forward in joint optimization NeRF-based view synthesis and
enables more realistic and flexible rendering in real-world scenarios with
varying camera parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schieber_H/0/1/0/all/0/1&quot;&gt;Hannah Schieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuser_F/0/1/0/all/0/1&quot;&gt;Fabian Deuser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1&quot;&gt;Bernhard Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1&quot;&gt;Norbert Oswald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Daniel Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08069">
<title>DETRs Beat YOLOs on Real-time Object Detection. (arXiv:2304.08069v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08069</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, end-to-end transformer-based detectors~(DETRs) have achieved
remarkable performance. However, the issue of the high computational cost of
DETRs has not been effectively addressed, limiting their practical application
and preventing them from fully exploiting the benefits of no post-processing,
such as non-maximum suppression (NMS). In this paper, we first analyze the
influence of NMS in modern real-time object detectors on inference speed, and
establish an end-to-end speed benchmark. To avoid the inference delay caused by
NMS, we propose a Real-Time DEtection TRansformer (RT-DETR), the first
real-time end-to-end object detector to our best knowledge. Specifically, we
design an efficient hybrid encoder to efficiently process multi-scale features
by decoupling the intra-scale interaction and cross-scale fusion, and propose
IoU-aware query selection to improve the initialization of object queries. In
addition, our proposed detector supports flexibly adjustment of the inference
speed by using different decoder layers without the need for retraining, which
facilitates the practical application of real-time object detectors. Our
RT-DETR-L achieves 53.0% AP on COCO val2017 and 114 FPS on T4 GPU, while
RT-DETR-X achieves 54.8% AP and 74 FPS, outperforming all YOLO detectors of the
same scale in both speed and accuracy. Furthermore, our RT-DETR-R50 achieves
53.1% AP and 108 FPS, outperforming DINO-Deformable-DETR-R50 by 2.2% AP in
accuracy and by about 21 times in FPS. ource code and pre-trained models are
available at https://github.com/lyuwenyu/RT-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Wenyu Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shangliang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jinman Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Cheng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuning Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Q/0/1/0/all/0/1&quot;&gt;Qingqing Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13242">
<title>Learning to Predict Navigational Patterns from Partial Observations. (arXiv:2304.13242v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13242</link>
<description rdf:parseType="Literal">&lt;p&gt;Human beings cooperatively navigate rule-constrained environments by adhering
to mutually known navigational patterns, which may be represented as
directional pathways or road lanes. Inferring these navigational patterns from
incompletely observed environments is required for intelligent mobile robots
operating in unmapped locations. However, algorithmically defining these
navigational patterns is nontrivial. This paper presents the first
self-supervised learning (SSL) method for learning to infer navigational
patterns in real-world environments from partial observations only. We explain
how geometric data augmentation, predictive world modeling, and an
information-theoretic regularizer enables our model to predict an unbiased
local directional soft lane probability (DSLP) field in the limit of infinite
data. We demonstrate how to infer global navigational patterns by fitting a
maximum likelihood graph to the DSLP field. Experiments show that our SSL model
outperforms two SOTA supervised lane graph prediction models on the nuScenes
dataset. We propose our SSL method as a scalable and interpretable continual
learning paradigm for navigation by perception. Code is available at
https://github.com/robin-karlsson0/dslp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1&quot;&gt;Robin Karlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carballo_A/0/1/0/all/0/1&quot;&gt;Alexander Carballo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepe_Salazar_F/0/1/0/all/0/1&quot;&gt;Francisco Lepe-Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohtani_K/0/1/0/all/0/1&quot;&gt;Kento Ohtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02677">
<title>Caption Anything: Interactive Image Description with Diverse Multimodal Controls. (arXiv:2305.02677v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02677</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable image captioning is an emerging multimodal topic that aims to
describe the image with natural language following human purpose,
$\textit{e.g.}$, looking at the specified regions or telling in a particular
text style. State-of-the-art methods are trained on annotated pairs of input
controls and output captions. However, the scarcity of such well-annotated
multimodal data largely limits their usability and scalability for interactive
AI systems. Leveraging unimodal instruction-following foundation models is a
promising alternative that benefits from broader sources of data. In this
paper, we present Caption AnyThing (CAT), a foundation model augmented image
captioning framework supporting a wide range of multimodel controls: 1) visual
controls, including points, boxes, and trajectories; 2) language controls, such
as sentiment, length, language, and factuality. Powered by Segment Anything
Model (SAM) and ChatGPT, we unify the visual and language prompts into a
modularized framework, enabling the flexible combination between different
controls. Extensive case studies demonstrate the user intention alignment
capabilities of our framework, shedding light on effective user interaction
modeling in vision-language applications. Our code is publicly available at
https://github.com/ttengwang/Caption-Anything.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Teng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1&quot;&gt;Junjie Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Mingqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03944">
<title>Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation. (arXiv:2305.03944v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03944</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing knowledge distillation works for semantic segmentation mainly focus
on transferring high-level contextual knowledge from teacher to student.
However, low-level texture knowledge is also of vital importance for
characterizing the local structural pattern and global statistical property,
such as boundary, smoothness, regularity and color contrast, which may not be
well addressed by high-level deep features. In this paper, we are intended to
take full advantage of both structural and statistical texture knowledge and
propose a novel Structural and Statistical Texture Knowledge Distillation
(SSTKD) framework for semantic segmentation. Specifically, for structural
texture knowledge, we introduce a Contourlet Decomposition Module (CDM) that
decomposes low-level features with iterative Laplacian pyramid and directional
filter bank to mine the structural texture knowledge. For statistical
knowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM)
to adaptively extract and enhance statistical texture knowledge through
heuristics iterative quantization and denoised operation. Finally, each
knowledge learning is supervised by an individual loss function, forcing the
student network to mimic the teacher better from a broader perspective.
Experiments show that the proposed method achieves state-of-the-art performance
on Cityscapes, Pascal VOC 2012 and ADE20K datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1&quot;&gt;Deyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Mingyuan Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1&quot;&gt;Xian-Sheng Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongtao Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10424">
<title>ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10424</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene flow estimation is the task of describing the 3D motion field between
temporally successive point clouds. State-of-the-art methods use strong priors
and test-time optimization techniques, but require on the order of tens of
seconds for large-scale point clouds, making them unusable as computer vision
primitives for real-time applications such as open world object detection. Feed
forward methods are considerably faster, running on the order of tens to
hundreds of milliseconds for large-scale point clouds, but require expensive
human supervision. To address both limitations, we propose Scene Flow via
Distillation, a simple distillation framework that uses a label-free
optimization method to produce pseudo-labels to supervise a feed forward model.
Our instantiation of this framework, ZeroFlow, produces scene flow estimates in
real-time on large-scale point clouds at quality competitive with
state-of-the-art methods while using zero human labels. Notably, at test-time
ZeroFlow is over 1000$\times$ faster than label-free state-of-the-art
optimization-based methods on large-scale point clouds and over 1000$\times$
cheaper to train on unlabeled data compared to the cost of human annotation of
that data. To facilitate research reuse, we release our code, trained model
weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1&quot;&gt;Kyle Vedder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1&quot;&gt;Neehar Peri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chodosh_N/0/1/0/all/0/1&quot;&gt;Nathaniel Chodosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_I/0/1/0/all/0/1&quot;&gt;Ishan Khatri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1&quot;&gt;Eric Eaton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1&quot;&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1&quot;&gt;James Hays&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17673">
<title>OSPC: Online Sequential Photometric Calibration. (arXiv:2305.17673v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17673</link>
<description rdf:parseType="Literal">&lt;p&gt;Photometric calibration is essential to many computer vision applications.
One of its key benefits is enhancing the performance of Visual SLAM, especially
when it depends on a direct method for tracking, such as the standard KLT
algorithm. Another advantage could be in retrieving the sensor irradiance
values from measured intensities, as a pre-processing step for some vision
algorithms, such as shape-from-shading. Current photometric calibration systems
rely on a joint optimization problem and encounter an ambiguity in the
estimates, which can only be resolved using ground truth information. We
propose a novel method that solves for photometric parameters using a
sequential estimation approach. Our proposed method achieves high accuracy in
estimating all parameters; furthermore, the formulations are linear and convex,
which makes the solution fast and suitable for online applications. Experiments
on a Visual Odometry system validate the proposed method and demonstrate its
advantages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haidar_J/0/1/0/all/0/1&quot;&gt;Jawad Haidar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_D/0/1/0/all/0/1&quot;&gt;Douaa Khalil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asmar_D/0/1/0/all/0/1&quot;&gt;Daniel Asmar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05414">
<title>Improving Tuning-Free Real Image Editing with Proximal Guidance. (arXiv:2306.05414v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05414</link>
<description rdf:parseType="Literal">&lt;p&gt;DDIM inversion has revealed the remarkable potential of real image editing
within diffusion-based methods. However, the accuracy of DDIM reconstruction
degrades as larger classifier-free guidance (CFG) scales being used for
enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align
the reconstruction and inversion trajectories with larger CFG scales, enabling
real image editing with cross-attention control. Negative-prompt inversion
(NPI) further offers a training-free closed-form solution of NTI. However, it
may introduce artifacts and is still constrained by DDIM reconstruction
quality. To overcome these limitations, we propose proximal guidance and
incorporate it to NPI with cross-attention control. We enhance NPI with a
regularization term and reconstruction guidance, which reduces artifacts while
capitalizing on its training-free nature. Additionally, we extend the concepts
to incorporate mutual self-attention control, enabling geometry and layout
alterations in the editing process. Our method provides an efficient and
straightforward approach, effectively addressing real image editing tasks with
minimal computational overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Ligong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Song Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhixing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kunpeng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruijiang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stathopoulos_A/0/1/0/all/0/1&quot;&gt;Anastasis Stathopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Di Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhangli_Q/0/1/0/all/0/1&quot;&gt;Qilong Zhangli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jindong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06767">
<title>The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases. (arXiv:2306.06767v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06767</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates the transformative potential of Large Language Models
(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public
data, these models, which possess remarkable language understanding and
generation capabilities, are augmenting the interpretive skills of
radiologists, enhancing patient-physician communication, and streamlining
clinical workflows. The paper introduces an analytic framework for presenting
the complex interactions between LLMs and the broader ecosystem of medical
imaging stakeholders, including businesses, insurance entities, governments,
research institutions, and hospitals (nicknamed BIGR-H). Through detailed
analyses, illustrative use cases, and discussions on the broader implications
and future directions, this perspective seeks to raise discussion in strategic
planning and decision-making in the era of AI-enabled healthcare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Donglai Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10239">
<title>Multi-scale Spatial-temporal Interaction Network for Video Anomaly Detection. (arXiv:2306.10239v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10239</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Anomaly Detection (VAD) is an essential yet challenging task in signal
processing. Since certain anomalies cannot be detected by isolated analysis of
either temporal or spatial information, the interaction between these two types
of data is considered crucial for VAD. However, current dual-stream
architectures either confine this integral interaction to the bottleneck of the
autoencoder or introduce anomaly-irrelevant background pixels into the
interactive process, hindering the accuracy of VAD. To address these
deficiencies, we propose a Multi-scale Spatial-Temporal Interaction Network
(MSTI-Net) for VAD. First, to prioritize the detection of moving objects in the
scene and harmonize the substantial semantic discrepancies between the two
types of data, we propose an Attention-based Spatial-Temporal Fusion Module
(ASTFM) as a substitute for the conventional direct fusion. Furthermore, we
inject multi-ASTFM-based connections that bridge the appearance and motion
streams of the dual-stream network, thus fostering multi-scale spatial-temporal
interaction. Finally, to bolster the delineation between normal and abnormal
activities, our system records the regular information in a memory module.
Experimental results on three benchmark datasets validate the effectiveness of
our approach, which achieves AUCs of 96.8%, 87.6%, and 73.9% on the UCSD Ped2,
CUHK Avenue, and ShanghaiTech datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhangxun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zile Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Liang Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14708">
<title>A Simple and Effective Baseline for Attentional Generative Adversarial Networks. (arXiv:2306.14708v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14708</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesising a text-to-image model of high-quality images by guiding the
generative model through the Text description is an innovative and challenging
task. In recent years, AttnGAN based on the Attention mechanism to guide GAN
training has been proposed, SD-GAN, which adopts a self-distillation technique
to improve the performance of the generator and the quality of image
generation, and Stack-GAN++, which gradually improves the details and quality
of the image by stacking multiple generators and discriminators. However, this
series of improvements to GAN all have redundancy to a certain extent, which
affects the generation performance and complexity to a certain extent. We use
the popular simple and effective idea (1) to remove redundancy structure and
improve the backbone network of AttnGAN. (2) to integrate and reconstruct
multiple losses of DAMSM. Our improvements have significantly improved the
model size and training efficiency while ensuring that the model&apos;s performance
is unchanged and finally proposed our SEAttnGAN. Code is avalilable at
https://github.com/jmyissb/SEAttnGAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Mingyu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qinkai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Haochen Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaobo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15782">
<title>UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15782</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel approach to address the challenges of
printed Urdu text recognition using high-resolution, multi-scale semantic
feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model,
demonstrates state-of-the-art performance on benchmark datasets. To address the
limitations of previous works, which struggle to generalize to the intricacies
of the Urdu script and the lack of sufficient annotated real-world data, we
have introduced the UTRSet-Real, a large-scale annotated real-world dataset
comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000
lines closely resembling real-world and made corrections to the ground truth of
the existing IIITH dataset, making it a more reliable resource for future
research. We also provide UrduDoc, a benchmark dataset for Urdu text line
detection in scanned documents. Additionally, we have developed an online tool
for end-to-end Urdu OCR from printed documents by integrating UTRNet with a
text detection model. Our work not only addresses the current limitations of
Urdu OCR but also paves the way for future research in this area and
facilitates the continued advancement of Urdu OCR technology. The project page
with source code, datasets, annotations, trained models, and online tool is
available at abdur75648.github.io/UTRNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Abdur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Arjun Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1&quot;&gt;Chetan Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15880">
<title>Towards Open Vocabulary Learning: A Survey. (arXiv:2306.15880v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15880</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of visual scene understanding, deep neural networks have made
impressive advancements in various core tasks like segmentation, tracking, and
detection. However, most approaches operate on the close-set assumption,
meaning that the model can only identify pre-defined categories that are
present in the training set. Recently, open vocabulary settings were proposed
due to the rapid progress of vision language pre-training. These new approaches
seek to locate and recognize categories beyond the annotated label space. The
open vocabulary approach is more general, practical, and effective compared to
weakly supervised and zero-shot settings. This paper provides a thorough review
of open vocabulary learning, summarizing and analyzing recent developments in
the field. In particular, we begin by comparing it to related concepts such as
zero-shot learning, open-set recognition, and out-of-distribution detection.
Then, we review several closely related tasks in the case of segmentation and
detection, including long-tail problems, few-shot, and zero-shot settings. For
the method survey, we first present the basic knowledge of detection and
segmentation in close-set as the preliminary knowledge. Next, we examine
various scenarios in which open vocabulary learning is used, identifying common
design elements and core ideas. Then, we compare the recent detection and
segmentation approaches in commonly used datasets and benchmarks. Finally, we
conclude with insights, issues, and discussions regarding future research
directions. To our knowledge, this is the first comprehensive literature review
of open vocabulary learning. We keep tracing related works at
https://github.com/jianzongwu/Awesome-Open-Vocabulary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianzong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yunhai Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16605">
<title>KITE: Keypoint-Conditioned Policies for Semantic Manipulation. (arXiv:2306.16605v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16605</link>
<description rdf:parseType="Literal">&lt;p&gt;While natural language offers a convenient shared interface for humans and
robots, enabling robots to interpret and follow language commands remains a
longstanding challenge in manipulation. A crucial step to realizing a
performant instruction-following robot is achieving semantic manipulation,
where a robot interprets language at different specificities, from high-level
instructions like &quot;Pick up the stuffed animal&quot; to more detailed inputs like
&quot;Grab the left ear of the elephant.&quot; To tackle this, we propose Keypoints +
Instructions to Execution (KITE), a two-step framework for semantic
manipulation which attends to both scene semantics (distinguishing between
different objects in a visual scene) and object semantics (precisely localizing
different parts within an object instance). KITE first grounds an input
instruction in a visual scene through 2D image keypoints, providing a highly
accurate object-centric bias for downstream action inference. Provided an RGB-D
scene observation, KITE then executes a learned keypoint-conditioned skill to
carry out the instruction. The combined precision of keypoints and
parameterized skills enables fine-grained manipulation with generalization to
scene and object variations. Empirically, we demonstrate KITE in 3 real-world
environments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and
a high-precision coffee-making task. In these settings, KITE achieves a 75%,
70%, and 71% overall success rate for instruction-following, respectively. KITE
outperforms frameworks that opt for pre-trained visual language models over
keypoint-based grounding, or omit skills in favor of end-to-end visuomotor
control, all while being trained from fewer or comparable amounts of
demonstrations. Supplementary material, datasets, code, and videos can be found
on our website: &lt;a href=&quot;http://tinyurl.com/kite-site.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1&quot;&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkhale_S/0/1/0/all/0/1&quot;&gt;Suneel Belkhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1&quot;&gt;Jeannette Bohg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00209">
<title>Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection
of hyperbole is an important part of understanding human expression. There have
been several studies on hyperbole detection, but most of which focus on text
modality only. However, with the development of social media, people can create
hyperbolic expressions with various modalities, including text, images, videos,
etc. In this paper, we focus on multimodal hyperbole detection. We create a
multimodal detection dataset\footnote{The dataset will be released to the
community.} from Weibo (a Chinese social media) and carry out some studies on
it. We treat the text and image from a piece of weibo as two modalities and
explore the role of text and image for hyperbole detection. Different
pre-trained multimodal encoders are also evaluated on this downstream task to
show their performance. Besides, since this dataset is constructed from five
different topics, we also evaluate the cross-domain performance of different
models. These studies can serve as a benchmark and point out the direction of
further study on multimodal hyperbole detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00481">
<title>Seeing is not Believing: An Identity Hider for Human Vision Privacy Protection. (arXiv:2307.00481v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00481</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive captured face images are stored in the database for the
identification of individuals. However, the stored images can be observed
intentionally or unintentionally by data managers, which is not at the will of
individuals and may cause privacy violations. Existing protection works only
slightly change the visual content of the face while maintaining the utility of
identification, making it susceptible to the inference of the true identity by
human vision. In this paper, we propose an identity hider that enables
significant visual content change for human vision while preserving high
identifiability for face recognizers. Firstly, the identity hider generates a
virtual face with new visual content by manipulating the latent space in
StyleGAN2. In particular, the virtual face has the same irrelevant attributes
as the original face, e.g., pose and expression. Secondly, the visual content
of the virtual face is transferred into the original face and then the
background is replaced with the original one. In addition, the identity hider
has strong transferability, which ensures an arbitrary face recognizer can
achieve satisfactory accuracy. Adequate experiments show that the proposed
identity hider achieves excellent performance on privacy protection and
identifiability preservation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yushu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Z/0/1/0/all/0/1&quot;&gt;Zhongyun Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00711">
<title>Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for Ultra-High Resolution Segmentation. (arXiv:2307.00711v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00711</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing ultra-high resolution (UHR) segmentation methods always
struggle in the dilemma of balancing memory cost and local characterization
accuracy, which are both taken into account in our proposed Guided
Patch-Grouping Wavelet Transformer (GPWFormer) that achieves impressive
performances. In this work, GPWFormer is a Transformer ($\mathcal{T}$)-CNN
($\mathcal{C}$) mutual leaning framework, where $\mathcal{T}$ takes the whole
UHR image as input and harvests both local details and fine-grained long-range
contextual dependencies, while $\mathcal{C}$ takes downsampled image as input
for learning the category-wise deep context. For the sake of high inference
speed and low computation complexity, $\mathcal{T}$ partitions the original UHR
image into patches and groups them dynamically, then learns the low-level local
details with the lightweight multi-head Wavelet Transformer (WFormer) network.
Meanwhile, the fine-grained long-range contextual dependencies are also
captured during this process, since patches that are far away in the spatial
domain can also be assigned to the same group. In addition, masks produced by
$\mathcal{C}$ are utilized to guide the patch grouping process, providing a
heuristics decision. Moreover, the congruence constraints between the two
branches are also exploited to maintain the spatial consistency among the
patches. Overall, we stack the multi-stage process in a pyramid way.
Experiments show that GPWFormer outperforms the existing methods with
significant improvements on five benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1&quot;&gt;Deyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongtao Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01148">
<title>Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis. (arXiv:2307.01148v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01148</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative latent diffusion models have been established as state-of-the-art
in data generation. One promising application is generation of realistic
synthetic medical imaging data for open data sharing without compromising
patient privacy. Despite the promise, the capacity of such models to memorize
sensitive patient training data and synthesize samples showing high resemblance
to training data samples is relatively unexplored. Here, we assess the
memorization capacity of 3D latent diffusion models on photon-counting coronary
computed tomography angiography and knee magnetic resonance imaging datasets.
To detect potential memorization of training samples, we utilize
self-supervised models based on contrastive learning. Our results suggest that
such latent diffusion models indeed memorize training data, and there is a dire
need for devising strategies to mitigate memorization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dar_S/0/1/0/all/0/1&quot;&gt;Salman Ul Hassan Dar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanaat_A/0/1/0/all/0/1&quot;&gt;Arman Ghanaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahmann_J/0/1/0/all/0/1&quot;&gt;Jannik Kahmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayx_I/0/1/0/all/0/1&quot;&gt;Isabelle Ayx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papavassiliu_T/0/1/0/all/0/1&quot;&gt;Theano Papavassiliu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenberg_S/0/1/0/all/0/1&quot;&gt;Stefan O. Schoenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1&quot;&gt;Sandy Engelhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01663">
<title>Exploring Transformers for On-Line Handwritten Signature Verification. (arXiv:2307.01663v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01663</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of mobile biometrics as a user-friendly authentication method
has increased in the last years. Recent studies have proposed novel behavioral
biometric recognition systems based on Transformers, which currently outperform
the state of the art in several application scenarios. On-line handwritten
signature verification aims to verify the identity of subjects, based on their
biometric signatures acquired using electronic devices such as tablets or
smartphones. This paper investigates the suitability of architectures based on
recent Transformers for on-line signature verification. In particular, four
different configurations are studied, two of them rely on the Vanilla
Transformer encoder, and the two others have been successfully applied to the
tasks of gait and activity recognition. We evaluate the four proposed
configurations according to the experimental protocol proposed in the
SVC-onGoing competition. The results obtained in our experiments are promising,
and promote the use of Transformers for on-line signature verification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melzi_P/0/1/0/all/0/1&quot;&gt;Pietro Melzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delgado_Santos_P/0/1/0/all/0/1&quot;&gt;Paula Delgado-Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stragapede_G/0/1/0/all/0/1&quot;&gt;Giuseppe Stragapede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02334">
<title>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI. (arXiv:2307.02334v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02334</link>
<description rdf:parseType="Literal">&lt;p&gt;Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging
(MRI) images from partial measurement is essential to medical imaging research.
Benefiting from the diverse and complementary information of multi-contrast MR
images in different imaging modalities, multi-contrast Super-Resolution (SR)
reconstruction is promising to yield SR images with higher quality. In the
medical scenario, to fully visualize the lesion, radiologists are accustomed to
zooming the MR images at arbitrary scales rather than using a fixed scale, as
used by most MRI SR methods. In addition, existing multi-contrast MRI SR
methods often require a fixed resolution for the reference image, which makes
acquiring reference images difficult and imposes limitations on arbitrary scale
SR tasks. To address these issues, we proposed an implicit neural
representations based dual-arbitrary multi-contrast MRI super-resolution
method, called Dual-ArbNet. First, we decouple the resolution of the target and
reference images by a feature encoder, enabling the network to input target and
reference images at arbitrary scales. Then, an implicit fusion decoder fuses
the multi-contrast features and uses an Implicit Decoding Function~(IDF) to
obtain the final MRI SR results. Furthermore, we introduce a curriculum
learning strategy to train our network, which improves the generalization and
performance of our Dual-ArbNet. Extensive experiments in two public MRI
datasets demonstrate that our method outperforms state-of-the-art approaches
under different scale factors and has great potential in clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiamiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yichen Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jun Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01848">
<title>Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.01848</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Haibin Yan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>