<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-28T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.15075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14111" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.14414">
<title>Fuzzy Logic-Based System for Brain Tumour Detection and Classification. (arXiv:2401.14414v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14414</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain Tumours (BT) are extremely dangerous and difficult to treat. Currently,
doctors must manually examine images and manually mark out tumour regions to
diagnose BT; this process is time-consuming and error-prone. In recent times,
experts have proposed automating approaches for detecting BT at an early stage.
The poor accuracy and highly incorrect prediction results of these methods
caused them to start the research. In this study, we suggest a fuzzy
logic-based system for categorising BT. This study used a dataset of 253
Magnetic Resonance Imaging (MRI) brain images that included tumour and healthy
images. The images were first pre-processed. After that, we pull out features
like tumour size and the image&apos;s global threshold value. The watershed and
region-growing approach is used to calculate the tumour size. After that, the
fuzzy system receives the two features as input. Accuracy, F1-score, precision,
and recall are used to assess the results of the fuzzy by employing both size
determination approaches. With the size input variable discovered by the region
growth method and global threshold values, the fuzzy system outperforms the
watershed method. The significance of this research lies in its potential to
revolutionize brain tumour diagnosis by offering a more accurate and efficient
automated classification system. By reducing human intervention and providing
reliable results, this approach could assist medical professionals in making
timely and precise decisions, leading to improved patient outcomes and
potentially saving lives. The advancement of such automated techniques has the
potential to pave the way for enhanced medical imaging analysis and,
ultimately, better management of brain tumour cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Narasimham_N/0/1/0/all/0/1&quot;&gt;NVSL Narasimham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+K_K/0/1/0/all/0/1&quot;&gt;Keshav Kumar K&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14425">
<title>No Longer Trending on Artstation: Prompt Analysis of Generative AI Art. (arXiv:2401.14425v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14425</link>
<description rdf:parseType="Literal">&lt;p&gt;Image generation using generative AI is rapidly becoming a major new source
of visual media, with billions of AI generated images created using diffusion
models such as Stable Diffusion and Midjourney over the last few years. In this
paper we collect and analyse over 3 million prompts and the images they
generate. Using natural language processing, topic analysis and visualisation
methods we aim to understand collectively how people are using text prompts,
the impact of these systems on artists, and more broadly on the visual cultures
they promote. Our study shows that prompting focuses largely on surface
aesthetics, reinforcing cultural norms, popular conventional representations
and imagery. We also find that many users focus on popular topics (such as
making colouring books, fantasy art, or Christmas cards), suggesting that the
dominant use for the systems analysed is recreational rather than artistic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCormack_J/0/1/0/all/0/1&quot;&gt;Jon McCormack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Llano_M/0/1/0/all/0/1&quot;&gt;Maria Teresa Llano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krol_S/0/1/0/all/0/1&quot;&gt;Stephen James Krol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajcic_N/0/1/0/all/0/1&quot;&gt;Nina Rajcic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14434">
<title>Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14434</link>
<description rdf:parseType="Literal">&lt;p&gt;The explication of Convolutional Neural Networks (CNN) through xAI techniques
often poses challenges in interpretation. The inherent complexity of input
features, notably pixels extracted from images, engenders complex correlations.
Gradient-based methodologies, exemplified by Integrated Gradients (IG),
effectively demonstrate the significance of these features. Nevertheless, the
conversion of these explanations into images frequently yields considerable
noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a
supportive framework for gradient-based techniques. Its primary objective is to
accentuate influential regions by establishing distinctions between classes.
The essence of GAD is to limit the scope of analysis during visualization and,
consequently reduce image noise. Empirical investigations involving occluded
images have demonstrated that the identified regions through this methodology
indeed play a pivotal role in facilitating class differentiation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_C/0/1/0/all/0/1&quot;&gt;Caroline Mazini Rodrigues&lt;/a&gt; (LRDE, LIGM), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1&quot;&gt;Nicolas Boutry&lt;/a&gt; (LRDE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1&quot;&gt;Laurent Najman&lt;/a&gt; (LIGM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14444">
<title>ICASSP 2024 Speech Signal Improvement Challenge. (arXiv:2401.14444v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.14444</link>
<description rdf:parseType="Literal">&lt;p&gt;The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to
stimulate research in the area of improving the speech signal quality in
communication systems. This marks our second challenge, building upon the
success from the previous ICASSP 2023 Grand Challenge. We enhance the
competition by introducing a dataset synthesizer, enabling all participating
teams to start at a higher baseline, an objective metric for our extended P.804
tests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a
metric. We evaluate a total of 13 systems in the real-time track and 11 systems
in the non-real-time track using both subjective P.804 and objective Word
Accuracy metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1&quot;&gt;Nicolae Catalin Ristea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saabas_A/0/1/0/all/0/1&quot;&gt;Ando Saabas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutler_R/0/1/0/all/0/1&quot;&gt;Ross Cutler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naderi_B/0/1/0/all/0/1&quot;&gt;Babak Naderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_S/0/1/0/all/0/1&quot;&gt;Sebastian Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branets_S/0/1/0/all/0/1&quot;&gt;Solomiya Branets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14469">
<title>Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels. (arXiv:2401.14469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14469</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in depthwise-separable convolutional neural networks
(DS-CNNs) have led to novel architectures, that surpass the performance of
classical CNNs, by a considerable scalability and accuracy margin. This paper
reveals another striking property of DS-CNN architectures: discernible and
explainable patterns emerge in their trained depthwise convolutional kernels in
all layers. Through an extensive analysis of millions of trained filters, with
different sizes and from various models, we employed unsupervised clustering
with autoencoders, to categorize these filters. Astonishingly, the patterns
converged into a few main clusters, each resembling the difference of Gaussian
(DoG) functions, and their first and second-order derivatives. Notably, we were
able to classify over 95\% and 90\% of the filters from state-of-the-art
ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a
technological curiosity; it echoes the foundational models neuroscientists have
long proposed for the vision systems of mammals. Our results thus deepen our
understanding of the emergent properties of trained DS-CNNs and provide a
bridge between artificial and biological visual processing systems. More
broadly, they pave the way for more interpretable and biologically-inspired
neural network designs in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaiee_Z/0/1/0/all/0/1&quot;&gt;Zahra Babaiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiasari_P/0/1/0/all/0/1&quot;&gt;Peyman M. Kiasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14486">
<title>CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of Clouds. (arXiv:2401.14486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14486</link>
<description rdf:parseType="Literal">&lt;p&gt;Clouds play a significant role in global temperature regulation through their
effect on planetary albedo. Anthropogenic emissions of aerosols can alter the
albedo of clouds, but the extent of this effect, and its consequent impact on
temperature change, remains uncertain. Human-induced clouds caused by ship
aerosol emissions, commonly referred to as ship tracks, provide visible
manifestations of this effect distinct from adjacent cloud regions and
therefore serve as a useful sandbox to study human-induced clouds. However, the
lack of large-scale ship track data makes it difficult to deduce their general
effects on cloud formation. Towards developing automated approaches to localize
ship tracks at scale, we present CloudTracks, a dataset containing 3,560
satellite images labeled with more than 12,000 ship track instance annotations.
We train semantic segmentation and instance segmentation model baselines on our
dataset and find that our best model substantially outperforms previous
state-of-the-art for ship track localization (61.29 vs. 48.65 IoU). We also
find that the best instance segmentation model is able to identify the number
of ship tracks in each image more accurately than the previous state-of-the-art
(1.64 vs. 4.99 MAE). However, we identify cases where the best model struggles
to accurately localize and count ship tracks, so we believe CloudTracks will
stimulate novel machine learning approaches to better detect elongated and
overlapping features in satellite images. We release our dataset openly at
{zenodo.org/records/10042922}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhry_M/0/1/0/all/0/1&quot;&gt;Muhammad Ahmed Chaudhry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_L/0/1/0/all/0/1&quot;&gt;Lyna Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1&quot;&gt;Jeremy Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ido_Y/0/1/0/all/0/1&quot;&gt;Yuzu Ido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_S/0/1/0/all/0/1&quot;&gt;Sonia Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isobe_J/0/1/0/all/0/1&quot;&gt;Jared Thomas Isobe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_Parris_D/0/1/0/all/0/1&quot;&gt;Duncan Watson-Parris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14487">
<title>Neighbor-Aware Calibration of Segmentation Networks with Penalty-Based Constraints. (arXiv:2401.14487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14487</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring reliable confidence scores from deep neural networks is of paramount
significance in critical decision-making systems, particularly in real-world
domains such as healthcare. Recent literature on calibrating deep segmentation
networks has resulted in substantial progress. Nevertheless, these approaches
are strongly inspired by the advancements in classification tasks, and thus
their uncertainty is usually modeled by leveraging the information of
individual pixels, disregarding the local structure of the object of interest.
Indeed, only the recent Spatially Varying Label Smoothing (SVLS) approach
considers pixel spatial relationships across classes, by softening the pixel
label assignments with a discrete spatial Gaussian kernel. In this work, we
first present a constrained optimization perspective of SVLS and demonstrate
that it enforces an implicit constraint on soft class proportions of
surrounding pixels. Furthermore, our analysis shows that SVLS lacks a mechanism
to balance the contribution of the constraint with the primary objective,
potentially hindering the optimization process. Based on these observations, we
propose NACL (Neighbor Aware CaLibration), a principled and simple solution
based on equality constraints on the logit values, which enables to control
explicitly both the enforced constraint and the weight of the penalty, offering
more flexibility. Comprehensive experiments on a wide variety of well-known
segmentation benchmarks demonstrate the superior calibration performance of the
proposed approach, without affecting its discriminative power. Furthermore,
ablation studies empirically show the model agnostic nature of our approach,
which can be used to train a wide span of deep segmentation networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_B/0/1/0/all/0/1&quot;&gt;Balamurali Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasudeva_S/0/1/0/all/0/1&quot;&gt;Sukesh Adiga Vasudeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Lombaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14497">
<title>Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets. (arXiv:2401.14497v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14497</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable progress of deep learning in dermatological tasks has brought
us closer to achieving diagnostic accuracies comparable to those of human
experts. However, while large datasets play a crucial role in the development
of reliable deep neural network models, the quality of data therein and their
correct usage are of paramount importance. Several factors can impact data
quality, such as the presence of duplicates, data leakage across train-test
partitions, mislabeled images, and the absence of a well-defined test
partition. In this paper, we conduct meticulous analyses of two popular
dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these
data quality issues, measure the effects of these problems on the benchmark
results, and propose corrections to the datasets. Besides ensuring the
reproducibility of our analysis, by making our analysis pipeline and the
accompanying code publicly available, we aim to encourage similar explorations
and to facilitate the identification and addressing of potential data quality
issues in other large datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1&quot;&gt;Kumar Abhishek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aditi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1&quot;&gt;Ghassan Hamarneh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14502">
<title>MResT: Multi-Resolution Sensing for Real-Time Control with Vision-Language Models. (arXiv:2401.14502v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14502</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging sensing modalities across diverse spatial and temporal resolutions
can improve performance of robotic manipulation tasks. Multi-spatial resolution
sensing provides hierarchical information captured at different spatial scales
and enables both coarse and precise motions. Simultaneously multi-temporal
resolution sensing enables the agent to exhibit high reactivity and real-time
control. In this work, we propose a framework, MResT (Multi-Resolution
Transformer), for learning generalizable language-conditioned multi-task
policies that utilize sensing at different spatial and temporal resolutions
using networks of varying capacities to effectively perform real time control
of precise and reactive tasks. We leverage off-the-shelf pretrained
vision-language models to operate on low-frequency global features along with
small non-pretrained models to adapt to high frequency local feedback. Through
extensive experiments in 3 domains (coarse, precise and dynamic manipulation
tasks), we show that our approach significantly improves (2X on average) over
recent multi-task baselines. Further, our approach generalizes well to visual
and geometric variations in target objects and to varying interaction forces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1&quot;&gt;Saumya Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Mohit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kroemer_O/0/1/0/all/0/1&quot;&gt;Oliver Kroemer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14510">
<title>RPNR: Robust-Perception Neural Reshading. (arXiv:2401.14510v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14510</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented Reality (AR) applications necessitates methods of inserting needed
objects into scenes captured by cameras in a way that is coherent with the
surroundings. Common AR applications require the insertion of predefined 3D
objects with known properties and shape. This simplifies the problem since it
is reduced to extracting an illumination model for the object in that scene by
understanding the surrounding light sources. However, it is often not the case
that we have information about the properties of an object, especially when we
depart from a single source image. Our method renders such source fragments in
a coherent way with the target surroundings using only these two images. Our
pipeline uses a Deep Image Prior (DIP) network based on a U-Net architecture as
the main renderer, alongside robust-feature extracting networks that are used
to apply needed losses. Our method does not require any pair-labeled data, and
no extensive training on a dataset. We compare our method using qualitative
metrics to the baseline methods such as Cut and Paste, Cut And Paste Neural
Rendering, and Image Harmonization
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afiouni_F/0/1/0/all/0/1&quot;&gt;Fouad Afiouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakih_M/0/1/0/all/0/1&quot;&gt;Mohamad Fakih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sleiman_J/0/1/0/all/0/1&quot;&gt;Joey Sleiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14535">
<title>CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process. (arXiv:2401.14535v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14535</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying the underlying time-delayed latent causal processes in sequential
data is vital for grasping temporal dynamics and making downstream reasoning.
While some recent methods can robustly identify these latent causal variables,
they rely on strict assumptions about the invertible generation process from
latent variables to observed data. However, these assumptions are often hard to
satisfy in real-world applications containing information loss. For instance,
the visual perception process translates a 3D space into 2D images, or the
phenomenon of persistence of vision incorporates historical data into current
perceptions. To address this challenge, we establish an identifiability theory
that allows for the recovery of independent latent components even when they
come from a nonlinear and non-invertible mix. Using this theory as a
foundation, we propose a principled approach, CaRiNG, to learn the CAusal
RepresentatIon of Non-invertible Generative temporal data with identifiability
guarantees. Specifically, we utilize temporal context to recover lost latent
information and apply the conditions in our theory to guide the training
process. Through experiments conducted on synthetic datasets, we validate that
our CaRiNG method reliably identifies the causal process, even when the
generation process is non-invertible. Moreover, we demonstrate that our
approach considerably improves temporal understanding and reasoning in
practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yifan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiangchen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuewen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Weiran Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14555">
<title>Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14555</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation vision or vision-language models are trained on large unlabeled or
noisy data and learn robust representations that can achieve impressive zero-
or few-shot performance on diverse tasks. Given these properties, they are a
natural fit for active learning (AL), which aims to maximize labeling
efficiency, but the full potential of foundation models has not been explored
in the context of AL, specifically in the low-budget regime. In this work, we
evaluate how foundation models influence three critical components of effective
AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling,
and 3) the trade-off between representative and uncertainty sampling. We
systematically study how the robust representations of foundation models
(DINOv2, OpenCLIP) challenge existing findings in active learning. Our
observations inform the principled construction of a new simple and elegant AL
strategy that balances uncertainty estimated via dropout with sample diversity.
We extensively test our strategy on many challenging image classification
benchmarks, including natural images as well as out-of-domain biomedical images
that are relatively understudied in the AL literature. Source code will be made
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupte_S/0/1/0/all/0/1&quot;&gt;Sanket Rajan Gupte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aklilu_J/0/1/0/all/0/1&quot;&gt;Josiah Aklilu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nirschl_J/0/1/0/all/0/1&quot;&gt;Jeffrey J. Nirschl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_Levy_S/0/1/0/all/0/1&quot;&gt;Serena Yeung-Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14565">
<title>TIFu: Tri-directional Implicit Function for High-Fidelity 3D Character Reconstruction. (arXiv:2401.14565v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14565</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in implicit function-based approaches have shown promising
results in 3D human reconstruction from a single RGB image. However, these
methods are not sufficient to extend to more general cases, often generating
dragged or disconnected body parts, particularly for animated characters. We
argue that these limitations stem from the use of the existing point-level 3D
shape representation, which lacks holistic 3D context understanding.
Voxel-based reconstruction methods are more suitable for capturing the entire
3D space at once, however, these methods are not practical for high-resolution
reconstructions due to their excessive memory usage. To address these
challenges, we introduce Tri-directional Implicit Function (TIFu), which is a
vector-level representation that increases global 3D consistencies while
significantly reducing memory usage compared to voxel representations. We also
introduce a new algorithm in 3D reconstruction at an arbitrary resolution by
aggregating vectors along three orthogonal axes, resolving inherent problems
with regressing fixed dimension of vectors. Our approach achieves
state-of-the-art performances in both our self-curated character dataset and
the benchmark 3D human dataset. We provide both quantitative and qualitative
analyses to support our findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Byoungsung Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14579">
<title>Recognizing Multiple Ingredients in Food Images Using a Single-Ingredient Classification Model. (arXiv:2401.14579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14579</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing food images presents unique challenges due to the variable
spatial layout and shape changes of ingredients with different cooking and
cutting methods. This study introduces an advanced approach for recognizing
ingredients segmented from food images. The method localizes the candidate
regions of the ingredients using the locating and sliding window techniques.
Then, these regions are assigned into ingredient classes using a CNN
(Convolutional Neural Network)-based single-ingredient classification model
trained on a dataset of single-ingredient images. To address the challenge of
processing speed in multi-ingredient recognition, a novel model pruning method
is proposed that enhances the efficiency of the classification model.
Subsequently, the multi-ingredient identification is achieved through a
decision-making scheme, incorporating two novel algorithms. The
single-ingredient image dataset, designed in accordance with the book entitled
&quot;New Food Ingredients List FOODS 2021&quot;, encompasses 9982 images across 110
diverse categories, emphasizing variety in ingredient shapes. In addition, a
multi-ingredient image dataset is developed to rigorously evaluate the
performance of our approach. Experimental results validate the effectiveness of
our method, particularly highlighting its improved capability in recognizing
multiple ingredients. This marks a significant advancement in the field of food
image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Ying Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14587">
<title>CNA-TTA: Clean and Noisy Region Aware Feature Learning within Clusters for Online-Offline Test-Time Adaptation. (arXiv:2401.14587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14587</link>
<description rdf:parseType="Literal">&lt;p&gt;A domain shift occurs when training (source) and test (target) data diverge
in their distribution. Test-time adaptation (TTA) addresses the domain shift
problem, aiming to adopt a trained model on the source domain to the target
domain in a scenario where only a well-trained source model and unlabeled
target data are available. In this scenario, handling false labels in the
target domain is crucial because they negatively impact the model performance.
To deal with this problem, we propose to utilize cluster structure (i.e.,
{`Clean&apos;} and {`Noisy&apos;} regions within each cluster) in the target domain
formulated by the source model. Given an initial clustering of target samples,
we first partition clusters into {`Clean&apos;} and {`Noisy&apos;} regions defined based
on cluster prototype (i.e., centroid of each cluster). As these regions have
totally different distributions of the true pseudo-labels, we adopt distinct
training strategies for the clean and noisy regions: we selectively train the
target with clean pseudo-labels in the clean region, whereas we introduce mixup
inputs representing intermediate features between clean and noisy regions to
increase the compactness of the cluster. We conducted extensive experiments on
multiple datasets in online/offline TTA settings, whose results demonstrate
that our method, {CNA-TTA}, achieves state-of-the-art for most cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hyeonwoo Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanmin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Won Hwa Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14626">
<title>Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning. (arXiv:2401.14626v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14626</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene graph generation (SGG) endeavors to predict visual relationships
between pairs of objects within an image. Prevailing SGG methods traditionally
assume a one-off learning process for SGG. This conventional paradigm may
necessitate repetitive training on all previously observed samples whenever new
relationships emerge, mitigating the risk of forgetting previously acquired
knowledge. This work seeks to address this pitfall inherent in a suite of prior
relationship predictions. Motivated by the achievements of in-context learning
in pretrained language models, our approach imbues the model with the
capability to predict relationships and continuously acquire novel knowledge
without succumbing to catastrophic forgetting. To achieve this goal, we
introduce a novel and pragmatic framework for scene graph generation, namely
Lifelong Scene Graph Generation (LSGG), where tasks, such as predicates, unfold
in a streaming fashion. In this framework, the model is constrained to
exclusive training on the present task, devoid of access to previously
encountered training data, except for a limited number of exemplars, but the
model is tasked with inferring all predicates it has encountered thus far.
Rigorous experiments demonstrate the superiority of our proposed method over
state-of-the-art SGG models in the context of LSGG across a diverse array of
metrics. Besides, extensive experiments on the two mainstream benchmark
datasets, VG and Open-Image(v6), show the superiority of our proposed model to
a number of competitive SGG models in terms of continuous learning and
conventional settings. Moreover, comprehensive ablation experiments demonstrate
the effectiveness of each component in our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tongtong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_G/0/1/0/all/0/1&quot;&gt;Guiduo Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1&quot;&gt;Ke Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14641">
<title>Super Efficient Neural Network for Compression Artifacts Reduction and Super Resolution. (arXiv:2401.14641v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14641</link>
<description rdf:parseType="Literal">&lt;p&gt;Video quality can suffer from limited internet speed while being streamed by
users. Compression artifacts start to appear when the bitrate decreases to
match the available bandwidth. Existing algorithms either focus on removing the
compression artifacts at the same video resolution, or on upscaling the video
resolution but not removing the artifacts. Super resolution-only approaches
will amplify the artifacts along with the details by default. We propose a
lightweight convolutional neural network (CNN)-based algorithm which
simultaneously performs artifacts reduction and super resolution (ARSR) by
enhancing the feature extraction layers and designing a custom training
dataset. The output of this neural network is evaluated for test streams
compressed at low bitrates using variable bitrate (VBR) encoding. The output
video quality shows a 4-6 increase in video multi-method assessment fusion
(VMAF) score compared to traditional interpolation upscaling approaches such as
Lanczos or Bicubic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1&quot;&gt;Qiuwen Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1&quot;&gt;Arman Kazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faraone_J/0/1/0/all/0/1&quot;&gt;Julian Faraone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afzal_T/0/1/0/all/0/1&quot;&gt;Tariq Afzal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14661">
<title>From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution. (arXiv:2401.14661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14661</link>
<description rdf:parseType="Literal">&lt;p&gt;The demand for accurate object detection in aerial imagery has surged with
the widespread use of drones and satellite technology. Traditional object
detection models, trained on datasets biased towards large objects, struggle to
perform optimally in aerial scenarios where small, densely clustered objects
are prevalent. To address this challenge, we present an innovative approach
that combines super-resolution and an adapted lightweight YOLOv5 architecture.
We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and
NWPU VHR-10, to evaluate our model&apos;s performance. Our Super Resolved YOLOv5
architecture features Transformer encoder blocks, allowing the model to capture
global context and context information, leading to improved detection results,
especially in high-density, occluded conditions. This lightweight model not
only delivers improved accuracy but also ensures efficient resource
utilization, making it well-suited for real-time applications. Our experimental
results demonstrate the model&apos;s superior performance in detecting small and
densely clustered objects, underlining the significance of dataset choice and
architectural adaptation for this specific task. In particular, the method
achieves 52.5% mAP on VisDrone, exceeding top prior works. This approach
promises to significantly advance object detection in aerial imagery,
contributing to more accurate and reliable results in a variety of real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nihal_R/0/1/0/all/0/1&quot;&gt;Ragib Amin Nihal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_B/0/1/0/all/0/1&quot;&gt;Benjamin Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itoyama_K/0/1/0/all/0/1&quot;&gt;Katsutoshi Itoyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakadai_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Nakadai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14675">
<title>Multi-model learning by sequential reading of untrimmed videos for action recognition. (arXiv:2401.14675v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14675</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method for learning videos by aggregating multiple models by
sequentially extracting video clips from untrimmed video. The proposed method
reduces the correlation between clips by feeding clips to multiple models in
turn and synchronizes these models through federated learning. Experimental
results show that the proposed method improves the performance compared to the
no synchronization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamiya_K/0/1/0/all/0/1&quot;&gt;Kodai Kamiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1&quot;&gt;Toru Tamaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14686">
<title>SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation. (arXiv:2401.14686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14686</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduced SSR, which utilizes SAM (segment-anything) as a strong
regularizer during training, to greatly enhance the robustness of the image
encoder for handling various domains. Specifically, given the fact that SAM is
pre-trained with a large number of images over the internet, which cover a
diverse variety of domains, the feature encoding extracted by the SAM is
obviously less dependent on specific domains when compared to the traditional
ImageNet pre-trained image encoder. Meanwhile, the ImageNet pre-trained image
encoder is still a mature choice of backbone for the semantic segmentation
task, especially when the SAM is category-irrelevant. As a result, our SSR
provides a simple yet highly effective design. It uses the ImageNet pre-trained
image encoder as the backbone, and the intermediate feature of each stage (ie
there are 4 stages in MiT-B5) is regularized by SAM during training. After
extensive experimentation on GTA5$\rightarrow$Cityscapes, our SSR significantly
improved performance over the baseline without introducing any extra inference
overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yanqi Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Ye Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1&quot;&gt;Lixin Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14705">
<title>Additional Look into GAN-based Augmentation for Deep Learning COVID-19 Image Classification. (arXiv:2401.14705v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14705</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of training data is one of the main limitations in deep
learning applications for medical imaging. Data augmentation is a popular
approach to overcome this problem. A new approach is a Machine Learning based
augmentation, in particular usage of Generative Adversarial Networks (GAN). In
this case, GANs generate images similar to the original dataset so that the
overall training data amount is bigger, which leads to better performance of
trained networks. A GAN model consists of two networks, a generator and a
discriminator interconnected in a feedback loop which creates a competitive
environment. This work is a continuation of the previous research where we
trained StyleGAN2-ADA by Nvidia on the limited COVID-19 chest X-ray image
dataset. In this paper, we study the dependence of the GAN-based augmentation
performance on dataset size with a focus on small samples. Two datasets are
considered, one with 1000 images per class (4000 images in total) and the
second with 500 images per class (2000 images in total). We train StyleGAN2-ADA
with both sets and then, after validating the quality of generated images, we
use trained GANs as one of the augmentations approaches in multi-class
classification problems. We compare the quality of the GAN-based augmentation
approach to two different approaches (classical augmentation and no
augmentation at all) by employing transfer learning-based classification of
COVID-19 chest X-ray images. The results are quantified using different
classification quality metrics and compared to the results from the literature.
The GAN-based augmentation approach is found to be comparable with classical
augmentation in the case of medium and large datasets but underperforms in the
case of smaller datasets. The correlation between the size of the original
dataset and the quality of classification is visible independently from the
augmentation approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fedoruk_O/0/1/0/all/0/1&quot;&gt;Oleksandr Fedoruk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klimaszewski_K/0/1/0/all/0/1&quot;&gt;Konrad Klimaszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ogonowski_A/0/1/0/all/0/1&quot;&gt;Aleksander Ogonowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kruk_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Kruk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14707">
<title>Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14707</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are vulnerable to adversarial samples. Adversarial
fine-tuning methods aim to enhance adversarial robustness through fine-tuning
the naturally pre-trained model in an adversarial training manner. However, we
identify that some latent features of adversarial samples are confused by
adversarial perturbation and lead to an unexpectedly increasing gap between
features in the last hidden layer of natural and adversarial samples. To
address this issue, we propose a disentanglement-based approach to explicitly
model and further remove the latent features that cause the feature gap.
Specifically, we introduce a feature disentangler to separate out the latent
features from the features of the adversarial samples, thereby boosting
robustness by eliminating the latent features. Besides, we align features in
the pre-trained model with features of adversarial samples in the fine-tuned
model, to further benefit from the features from natural samples without
confusion. Empirical evaluations on three benchmark datasets demonstrate that
our approach surpasses existing adversarial fine-tuning methods and adversarial
training baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Nuoyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14718">
<title>A Survey on Video Prediction: From Deterministic to Generative Approaches. (arXiv:2401.14718v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14718</link>
<description rdf:parseType="Literal">&lt;p&gt;Video prediction, a fundamental task in computer vision, aims to enable
models to generate sequences of future frames based on existing video content.
This task has garnered widespread application across various domains. In this
paper, we comprehensively survey both historical and contemporary works in this
field, encompassing the most widely used datasets and algorithms. Our survey
scrutinizes the challenges and evolving landscape of video prediction within
the realm of computer vision. We propose a novel taxonomy centered on the
stochastic nature of video prediction algorithms. This taxonomy accentuates the
gradual transition from deterministic to generative prediction methodologies,
underlining significant advancements and shifts in approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_R/0/1/0/all/0/1&quot;&gt;Ruibo Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1&quot;&gt;Zhuoxuan Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jianming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Lihui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuchang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14719">
<title>pLitterStreet: Street Level Plastic Litter Detection and Mapping. (arXiv:2401.14719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14719</link>
<description rdf:parseType="Literal">&lt;p&gt;Plastic pollution is a critical environmental issue, and detecting and
monitoring plastic litter is crucial to mitigate its impact. This paper
presents the methodology of mapping street-level litter, focusing primarily on
plastic waste and the location of trash bins. Our methodology involves
employing a deep learning technique to identify litter and trash bins from
street-level imagery taken by a camera mounted on a vehicle. Subsequently, we
utilized heat maps to visually represent the distribution of litter and trash
bins throughout cities. Additionally, we provide details about the creation of
an open-source dataset (&quot;pLitterStreet&quot;) which was developed and utilized in
our approach. The dataset contains more than 13,000 fully annotated images
collected from vehicle-mounted cameras and includes bounding box labels. To
evaluate the effectiveness of our dataset, we tested four well known
state-of-the-art object detection algorithms (Faster R-CNN, RetinaNet, YOLOv3,
and YOLOv5), achieving an average precision (AP) above 40%. While the results
show average metrics, our experiments demonstrated the reliability of using
vehicle-mounted cameras for plastic litter mapping. The &quot;pLitterStreet&quot; can
also be a valuable resource for researchers and practitioners to develop and
further improve existing machine learning models for detecting and mapping
plastic litter in an urban environment. The dataset is open-source and more
details about the dataset and trained models can be found at
https://github.com/gicait/pLitter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandhati_S/0/1/0/all/0/1&quot;&gt;Sriram Reddy Mandhati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshapriya_N/0/1/0/all/0/1&quot;&gt;N. Lakmal Deshapriya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendis_C/0/1/0/all/0/1&quot;&gt;Chatura Lavanga Mendis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunasekara_K/0/1/0/all/0/1&quot;&gt;Kavinda Gunasekara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yrle_F/0/1/0/all/0/1&quot;&gt;Frank Yrle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaksan_A/0/1/0/all/0/1&quot;&gt;Angsana Chaksan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanjeev_S/0/1/0/all/0/1&quot;&gt;Sujit Sanjeev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14726">
<title>3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field. (arXiv:2401.14726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14726</link>
<description rdf:parseType="Literal">&lt;p&gt;Simultaneously achieving 3D reconstruction and new view synthesis for indoor
environments has widespread applications but is technically very challenging.
State-of-the-art methods based on implicit neural functions can achieve
excellent 3D reconstruction results, but their performances on new view
synthesis can be unsatisfactory. The exciting development of neural radiance
field (NeRF) has revolutionized new view synthesis, however, NeRF-based models
can fail to reconstruct clean geometric surfaces. We have developed a dual
neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry
reconstruction and view rendering. Du-NeRF contains two geometric fields, one
derived from the SDF field to facilitate geometric reconstruction and the other
derived from the density field to boost new view synthesis. One of the
innovative features of Du-NeRF is that it decouples a view-independent
component from the density field and uses it as a label to supervise the
learning process of the SDF field. This reduces shape-radiance ambiguity and
enables geometry and color to benefit from each other during the learning
process. Extensive experiments demonstrate that Du-NeRF can significantly
improve the performance of novel view synthesis and 3D reconstruction for
indoor environments and it is particularly effective in constructing areas
containing fine geometries that do not obey multi-view color consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_G/0/1/0/all/0/1&quot;&gt;Guibiao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kanglin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1&quot;&gt;Guoping Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14729">
<title>Sketch and Refine: Towards Fast and Accurate Lane Detection. (arXiv:2401.14729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14729</link>
<description rdf:parseType="Literal">&lt;p&gt;Lane detection is to determine the precise location and shape of lanes on the
road. Despite efforts made by current methods, it remains a challenging task
due to the complexity of real-world scenarios. Existing approaches, whether
proposal-based or keypoint-based, suffer from depicting lanes effectively and
efficiently. Proposal-based methods detect lanes by distinguishing and
regressing a collection of proposals in a streamlined top-down way, yet lack
sufficient flexibility in lane representation. Keypoint-based methods, on the
other hand, construct lanes flexibly from local descriptors, which typically
entail complicated post-processing. In this paper, we present a
&quot;Sketch-and-Refine&quot; paradigm that utilizes the merits of both keypoint-based
and proposal-based methods. The motivation is that local directions of lanes
are semantically simple and clear. At the &quot;Sketch&quot; stage, local directions of
keypoints can be easily estimated by fast convolutional layers. Then we can
build a set of lane proposals accordingly with moderate accuracy. At the
&quot;Refine&quot; stage, we further optimize these proposals via a novel Lane Segment
Association Module (LSAM), which allows adaptive lane segment adjustment. Last
but not least, we propose multi-level feature integration to enrich lane
feature representations more efficiently. Based on the proposed &quot;Sketch and
Refine&quot; paradigm, we propose a fast yet effective lane detector dubbed
&quot;SRLane&quot;. Experiments show that our SRLane can run at a fast speed (i.e., 278
FPS) while yielding an F1 score of 78.9\%. The source code is available at:
https://github.com/passerer/SRLane.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gangshan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14733">
<title>Personality Perception in Human Videos Altered by Motion Transfer Networks. (arXiv:2401.14733v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14733</link>
<description rdf:parseType="Literal">&lt;p&gt;The successful portrayal of personality in digital characters improves
communication and immersion. Current research focuses on expressing personality
through modifying animations using heuristic rules or data-driven models. While
studies suggest motion style highly influences the apparent personality, the
role of appearance can be similarly essential. This work analyzes the influence
of movement and appearance on the perceived personality of short videos altered
by motion transfer networks. We label the personalities in conference video
clips with a user study to determine the samples that best represent the
Five-Factor model&apos;s high, neutral, and low traits. We alter these videos using
the Thin-Plate Spline Motion Model, utilizing the selected samples as the
source and driving inputs. We follow five different cases to study the
influence of motion and appearance on personality perception. Our comparative
study reveals that motion and appearance influence different factors: motion
strongly affects perceived extraversion, and appearance helps convey
agreeableness and neuroticism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurtoglu_A/0/1/0/all/0/1&quot;&gt;Ayda Yurto&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonlu_S/0/1/0/all/0/1&quot;&gt;Sinan Sonlu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dogan_Y/0/1/0/all/0/1&quot;&gt;Yal&amp;#x131;m Do&amp;#x11f;an&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gudukbay_U/0/1/0/all/0/1&quot;&gt;U&amp;#x11f;ur G&amp;#xfc;d&amp;#xfc;kbay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14749">
<title>Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes. (arXiv:2401.14749v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.14749</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a method for achieving equilibrium in the ISING
Hamiltonian when confronted with unevenly distributed charges on an irregular
grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our
approach involves dimensionally expanding the system, substituting charges with
circulants, and representing distances through circulant shifts. This results
in a systematic mapping of the charge system onto a space, transforming the
irregular grid into a uniform configuration, applicable to Torical and Circular
Hyperboloid Topologies. The paper covers fundamental definitions and notations
related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine.
It explores the marginalization problem in code on the graph probabilistic
models for evaluating the partition function, encompassing exact and
approximate estimation techniques. Rigorous proof is provided for the
attainability of equilibrium states for the Boltzmann machine under Torical and
Circular Hyperboloid, paving the way for the application of our methodology.
Practical applications of our approach are investigated in Finite Geometry
QC-LDPC Codes, specifically in Material Science. The paper further explores its
effectiveness in the realm of Natural Language Processing Transformer Deep
Neural Networks, examining Generalized Repeat Accumulate Codes,
Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful
nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium
method is showcased across diverse scientific domains without the use of
specific section delineations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usatyuk_V/0/1/0/all/0/1&quot;&gt;Vasiliy Usatyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapozhnikov_D/0/1/0/all/0/1&quot;&gt;Denis Sapozhnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egorov_S/0/1/0/all/0/1&quot;&gt;Sergey Egorov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14754">
<title>VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising. (arXiv:2401.14754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14754</link>
<description rdf:parseType="Literal">&lt;p&gt;Video restoration task aims to recover high-quality videos from low-quality
observations. This contains various important sub-tasks, such as video
denoising, deblurring and low-light enhancement, since video often faces
different types of degradation, such as blur, low light, and noise. Even worse,
these kinds of degradation could happen simultaneously when taking videos in
extreme environments. This poses significant challenges if one wants to remove
these artifacts at the same time. In this paper, to the best of our knowledge,
we are the first to propose an efficient end-to-end video transformer approach
for the joint task of video deblurring, low-light enhancement, and denoising.
This work builds a novel multi-tier transformer where each tier uses a
different level of degraded video as a target to learn the features of video
effectively. Moreover, we carefully design a new tier-to-tier feature fusion
scheme to learn video features incrementally and accelerate the training
process with a suitable adaptive weighting scheme. We also provide a new
Multiscene-Lowlight-Blur-Noise (MLBN) dataset, which is generated according to
the characteristics of the joint task based on the RealBlur dataset and YouTube
videos to simulate realistic scenes as far as possible. We have conducted
extensive experiments, compared with many previous state-of-the-art methods, to
show the effectiveness of our approach clearly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaofang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinshan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1&quot;&gt;Raymond Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14762">
<title>A Comparative Study of Compressive Sensing Algorithms for Hyperspectral Imaging Reconstruction. (arXiv:2401.14762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14762</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral Imaging comprises excessive data consequently leading to
significant challenges for data processing, storage and transmission.
Compressive Sensing has been used in the field of Hyperspectral Imaging as a
technique to compress the large amount of data. This work addresses the
recovery of hyperspectral images 2.5x compressed. A comparative study in terms
of the accuracy and the performance of the convex FISTA/ADMM in addition to the
greedy gOMP/BIHT/CoSaMP recovery algorithms is presented. The results indicate
that the algorithms recover successfully the compressed data, yet the gOMP
algorithm achieves superior accuracy and faster recovery in comparison to the
other algorithms at the expense of high dependence on unknown sparsity level of
the data to recover.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Justo_J/0/1/0/all/0/1&quot;&gt;Jon Alvarez Justo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupu_D/0/1/0/all/0/1&quot;&gt;Daniela Lupu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orlandic_M/0/1/0/all/0/1&quot;&gt;Milica Orlandic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Necoara_I/0/1/0/all/0/1&quot;&gt;Ion Necoara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansen_T/0/1/0/all/0/1&quot;&gt;Tor Arne Johansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14772">
<title>Spatial Transcriptomics Analysis of Zero-shot Gene Expression Prediction. (arXiv:2401.14772v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14772</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial transcriptomics (ST) captures gene expression within distinct regions
(i.e., windows) of a tissue slide. Traditional supervised learning frameworks
applied to model ST are constrained to predicting expression from slide image
windows for gene types seen during training, failing to generalize to unseen
gene types. To overcome this limitation, we propose a semantic guided network
(SGN), a pioneering zero-shot framework for predicting gene expression from
slide image windows. Considering a gene type can be described by functionality
and phenotype, we dynamically embed a gene type to a vector per its
functionality and phenotype, and employ this vector to project slide image
windows to gene expression in feature space, unleashing zero-shot expression
prediction for unseen gene types. The gene type functionality and phenotype are
queried with a carefully designed prompt from a pre-trained large language
model (LLM). On standard benchmark datasets, we demonstrate competitive
zero-shot performance compared to past state-of-the-art supervised learning
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Md Zakir Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuesong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1&quot;&gt;Shafin Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_E/0/1/0/all/0/1&quot;&gt;Eric Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14785">
<title>SimpleEgo: Predicting Probabilistic Body Pose from Egocentric Cameras. (arXiv:2401.14785v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14785</link>
<description rdf:parseType="Literal">&lt;p&gt;Our work addresses the problem of egocentric human pose estimation from
downwards-facing cameras on head-mounted devices (HMD). This presents a
challenging scenario, as parts of the body often fall outside of the image or
are occluded. Previous solutions minimize this problem by using fish-eye camera
lenses to capture a wider view, but these can present hardware design issues.
They also predict 2D heat-maps per joint and lift them to 3D space to deal with
self-occlusions, but this requires large network architectures which are
impractical to deploy on resource-constrained HMDs. We predict pose from images
captured with conventional rectilinear camera lenses. This resolves hardware
design issues, but means body parts are often out of frame. As such, we
directly regress probabilistic joint rotations represented as matrix Fisher
distributions for a parameterized body model. This allows us to quantify pose
uncertainties and explain out-of-frame or occluded joints. This also removes
the need to compute 2D heat-maps and allows for simplified DNN architectures
which require less compute. Given the lack of egocentric datasets using
rectilinear camera lenses, we introduce the SynthEgo dataset, a synthetic
dataset with 60K stereo images containing high diversity of pose, shape,
clothing and skin tone. Our approach achieves state-of-the-art results for this
challenging configuration, reducing mean per-joint position error by 23%
overall and 58% for the lower body. Our architecture also has eight times fewer
parameters and runs twice as fast as the current state-of-the-art. Experiments
show that training on our synthetic dataset leads to good generalization to
real world images without fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuevas_Velasquez_H/0/1/0/all/0/1&quot;&gt;Hanz Cuevas-Velasquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hewitt_C/0/1/0/all/0/1&quot;&gt;Charlie Hewitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliakbarian_S/0/1/0/all/0/1&quot;&gt;Sadegh Aliakbarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baltrusaitis_T/0/1/0/all/0/1&quot;&gt;Tadas Baltru&amp;#x161;aitis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14786">
<title>Study of the gOMP Algorithm for Recovery of Compressed Sensed Hyperspectral Images. (arXiv:2401.14786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14786</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral Imaging (HSI) is used in a wide range of applications such as
remote sensing, yet the transmission of the HS images by communication data
links becomes challenging due to the large number of spectral bands that the HS
images contain together with the limited data bandwidth available in real
applications. Compressive Sensing reduces the images by randomly subsampling
the spectral bands of each spatial pixel and then it performs the image
reconstruction of all the bands using recovery algorithms which impose sparsity
in a certain transform domain. Since the image pixels are not strictly sparse,
this work studies a data sparsification pre-processing stage prior to
compression to ensure the sparsity of the pixels. The sparsified images are
compressed $2.5\times$ and then recovered using the Generalized Orthogonal
Matching Pursuit algorithm (gOMP) characterized by high accuracy, low
computational requirements and fast convergence. The experiments are performed
in five conventional hyperspectral images where the effect of different
sparsification levels in the quality of the uncompressed as well as the
recovered images is studied. It is concluded that the gOMP algorithm
reconstructs the hyperspectral images with higher accuracy as well as faster
convergence when the pixels are highly sparsified and hence at the expense of
reducing the quality of the recovered images with respect to the original
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Justo_J/0/1/0/all/0/1&quot;&gt;Jon Alvarez Justo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orlandic_M/0/1/0/all/0/1&quot;&gt;Milica Orlandic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14792">
<title>Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition. (arXiv:2401.14792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14792</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we harness the information-theoretic Privacy Funnel (PF) model
to develop a method for privacy-preserving representation learning using an
end-to-end training framework. We rigorously address the trade-off between
obfuscation and utility. Both are quantified through the logarithmic loss, a
measure also recognized as self-information loss. This exploration deepens the
interplay between information-theoretic privacy and representation learning,
offering substantive insights into data protection mechanisms for both
discriminative and generative models. Importantly, we apply our model to
state-of-the-art face recognition systems. The model demonstrates adaptability
across diverse inputs, from raw facial images to both derived or refined
embeddings, and is competent in tasks such as classification, reconstruction,
and generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razeghi_B/0/1/0/all/0/1&quot;&gt;Behrooz Razeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_P/0/1/0/all/0/1&quot;&gt;Parsa Rahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Marcel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14807">
<title>PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning. (arXiv:2401.14807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14807</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-Shot Class-Incremental Learning (FSCIL) aims to enable deep neural
networks to learn new tasks incrementally from a small number of labeled
samples without forgetting previously learned tasks, closely mimicking human
learning patterns. In this paper, we propose a novel approach called Prompt
Learning for FSCIL (PL-FSCIL), which harnesses the power of prompts in
conjunction with a pre-trained Vision Transformer (ViT) model to address the
challenges of FSCIL effectively. Our work pioneers the use of visual prompts in
FSCIL, which is characterized by its notable simplicity. PL-FSCIL consists of
two distinct prompts: the Domain Prompt and the FSCIL Prompt. Both are vectors
that augment the model by embedding themselves into the attention layer of the
ViT model. Specifically, the Domain Prompt assists the ViT model in adapting to
new data domains. The task-specific FSCIL Prompt, coupled with a prototype
classifier, amplifies the model&apos;s ability to effectively handle FSCIL tasks. We
validate the efficacy of PL-FSCIL on widely used benchmark datasets such as
CIFAR-100 and CUB-200. The results showcase competitive performance,
underscoring its promising potential for real-world applications where
high-quality data is often scarce. The source code is available at:
https://github.com/TianSongS/PL-FSCIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1&quot;&gt;Songsong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lusi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1&quot;&gt;Hang Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xin Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14828">
<title>TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts. (arXiv:2401.14828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14828</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-driven 3D scene editing has gained significant attention owing to its
convenience and user-friendliness. However, existing methods still lack
accurate control of the specified appearance and location of the editing result
due to the inherent limitations of the text description. To this end, we
propose a 3D scene editing framework, TIPEditor, that accepts both text and
image prompts and a 3D bounding box to specify the editing region. With the
image prompt, users can conveniently specify the detailed appearance/style of
the target content in complement to the text description, enabling accurate
control of the appearance. Specifically, TIP-Editor employs a stepwise 2D
personalization strategy to better learn the representation of the existing
scene and the reference image, in which a localization loss is proposed to
encourage correct object placement as specified by the bounding box.
Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as
the 3D representation to facilitate local editing while keeping the background
unchanged. Extensive experiments have demonstrated that TIP-Editor conducts
accurate editing following the text and image prompts in the specified bounding
box region, consistently outperforming the baselines in editing quality, and
the alignment to the prompts, qualitatively and quantitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Di Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14831">
<title>The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Circumstances. (arXiv:2401.14831v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14831</link>
<description rdf:parseType="Literal">&lt;p&gt;Are we heading for an iceberg with the current testing of machine vision?
This work delves into the landscape of Machine Vision (MV) testing, which is
heavily required in Highly Automated Driving (HAD) systems. Utilizing the
metaphorical notion of navigating towards an iceberg, we discuss the potential
shortcomings concealed within current testing strategies. We emphasize the
urgent need for a deeper understanding of how to deal with the opaque functions
of MV in development processes. As overlooked considerations can cost lives.
Our main contribution is the hierarchical level model, which we call
Granularity Grades. The model encourages a refined exploration of the
multi-scaled depths of understanding about the circumstances of environments in
which MV is intended to operate. This model aims to provide a holistic overview
of all entities that may impact MV functions, ranging from relations of
individual entities like object attributes to entire environmental scenes. The
application of our model delivers a structured exploration of entities in a
specific domain, their relationships and assigning results of a MV-under-test
to construct an entity-relationship graph. Through clustering patterns of
relations in the graph general MV deficits are arguable. In Summary, our work
contributes to a more nuanced and systematized identification of deficits of a
MV test object in correlation to holistic circumstances in HAD operating
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padusinski_H/0/1/0/all/0/1&quot;&gt;Hubert Padusinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Thilo Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhauser_C/0/1/0/all/0/1&quot;&gt;Christian Steinhauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ries_L/0/1/0/all/0/1&quot;&gt;Lennart Ries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sax_E/0/1/0/all/0/1&quot;&gt;Eric Sax&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14832">
<title>Text Image Inpainting via Global Structure-Guided Diffusion Models. (arXiv:2401.14832v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14832</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world text can be damaged by corrosion issues caused by environmental or
human factors, which hinder the preservation of the complete styles of texts,
e.g., texture and structure. These corrosion issues, such as graffiti signs and
incomplete signatures, bring difficulties in understanding the texts, thereby
posing significant challenges to downstream applications, e.g., scene text
recognition and signature identification. Notably, current inpainting
techniques often fail to adequately address this problem and have difficulties
restoring accurate text images along with reasonable and consistent styles.
Formulating this as an open problem of text image inpainting, this paper aims
to build a benchmark to facilitate its study. In doing so, we establish two
specific text inpainting datasets which contain scene text images and
handwritten text images, respectively. Each of them includes images revamped by
real-life and synthetic datasets, featuring pairs of original images, corrupted
images, and other assistant information. On top of the datasets, we further
develop a novel neural framework, Global Structure-guided Diffusion Model
(GSDM), as a potential solution. Leveraging the global structure of the text as
a prior, the proposed GSDM develops an efficient diffusion model to recover
clean texts. The efficacy of our approach is demonstrated by thorough empirical
study, including a substantial boost in both recognition accuracy and image
quality. These findings not only highlight the effectiveness of our method but
also underscore its potential to enhance the broader field of text image
understanding and processing. Code and datasets are available at:
https://github.com/blackprotoss/GSDM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shipeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1&quot;&gt;Pengfei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenjie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zuoyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hui Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14838">
<title>Multi-modality action recognition based on dual feature shift in vehicle cabin monitoring. (arXiv:2401.14838v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14838</link>
<description rdf:parseType="Literal">&lt;p&gt;Driver Action Recognition (DAR) is crucial in vehicle cabin monitoring
systems. In real-world applications, it is common for vehicle cabins to be
equipped with cameras featuring different modalities. However, multi-modality
fusion strategies for the DAR task within car cabins have rarely been studied.
In this paper, we propose a novel yet efficient multi-modality driver action
recognition method based on dual feature shift, named DFS. DFS first integrates
complementary features across modalities by performing modality feature
interaction. Meanwhile, DFS achieves the neighbour feature propagation within
single modalities, by feature shifting among temporal frames. To learn common
patterns and improve model efficiency, DFS shares feature extracting stages
among multiple modalities. Extensive experiments have been carried out to
verify the effectiveness of the proposed DFS model on the Drive\&amp;amp;Act dataset.
The results demonstrate that DFS achieves good performance and improves the
efficiency of multi-modality driver action recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1&quot;&gt;Philip Hann Yung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1&quot;&gt;Kim-Hui Yap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingbing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngim_Y/0/1/0/all/0/1&quot;&gt;You Shing Ngim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14845">
<title>Adaptive Point Transformer. (arXiv:2401.14845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14845</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge in 3D data acquisition has spurred the development of
geometric deep learning models for point cloud processing, boosted by the
remarkable success of transformers in natural language processing. While point
cloud transformers (PTs) have achieved impressive results recently, their
quadratic scaling with respect to the point cloud size poses a significant
scalability challenge for real-world applications. To address this issue, we
propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model
augmented by an adaptive token selection mechanism. AdaPT dynamically reduces
the number of tokens during inference, enabling efficient processing of large
point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust
the computational cost of the model at inference time without the need for
retraining or fine-tuning separate models. Our extensive experimental
evaluation on point cloud classification tasks demonstrates that AdaPT
significantly reduces computational complexity while maintaining competitive
accuracy compared to standard PTs. The code for AdaPT is made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baiocchi_A/0/1/0/all/0/1&quot;&gt;Alessandro Baiocchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spinelli_I/0/1/0/all/0/1&quot;&gt;Indro Spinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolosi_A/0/1/0/all/0/1&quot;&gt;Alessandro Nicolosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1&quot;&gt;Simone Scardapane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14846">
<title>Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14846</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the rapid development of machine learning algorithms for domain
generalization (DG), there is no clear empirical evidence that the existing DG
algorithms outperform the classic empirical risk minimization (ERM) across
standard benchmarks. To better understand this phenomenon, we investigate
whether there are benefits of DG algorithms over ERM through the lens of label
noise. Specifically, our finite-sample analysis reveals that label noise
exacerbates the effect of spurious correlations for ERM, undermining
generalization. Conversely, we illustrate that DG algorithms exhibit implicit
label-noise robustness during finite-sample training even when spurious
correlation is present. Such desirable property helps mitigate spurious
correlations and improve generalization in synthetic experiments. However,
additional comprehensive experiments on real-world benchmark datasets indicate
that label-noise robustness does not necessarily translate to better
performance compared to ERM. We conjecture that the failure mode of ERM arising
from spurious correlations may be less pronounced in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1&quot;&gt;Rui Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1&quot;&gt;Bryan Kian Hsiang Low&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14856">
<title>Memory-Inspired Temporal Prompt Interaction for Text-Image Classification. (arXiv:2401.14856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14856</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, large-scale pre-trained multimodal models (LMM) generally
emerge to integrate the vision and language modalities, achieving considerable
success in various natural language processing and computer vision tasks. The
growing size of LMMs, however, results in a significant computational cost for
fine-tuning these models for downstream tasks. Hence, prompt-based interaction
strategy is studied to align modalities more efficiently. In this contex, we
propose a novel prompt-based multimodal interaction strategy inspired by human
memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our
proposed method involves in two stages as in human memory strategy: the
acquiring stage, and the consolidation and activation stage. We utilize
temporal prompts on intermediate layers to imitate the acquiring stage,
leverage similarity-based prompt interaction to imitate memory consolidation,
and employ prompt generation strategy to imitate memory activation. The main
strength of our paper is that we interact the prompt vectors on intermediate
layers to leverage sufficient information exchange between modalities, with
compressed trainable parameters and memory usage. We achieve competitive
results on several datasets with relatively small memory usage and 2.0M of
trainable parameters (about 1% of the pre-trained foundation model).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xinyao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Rui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lanfen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14861">
<title>Implicit Neural Representation for Physics-driven Actuated Soft Bodies. (arXiv:2401.14861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14861</link>
<description rdf:parseType="Literal">&lt;p&gt;Active soft bodies can affect their shape through an internal actuation
mechanism that induces a deformation. Similar to recent work, this paper
utilizes a differentiable, quasi-static, and physics-based simulation layer to
optimize for actuation signals parameterized by neural networks. Our key
contribution is a general and implicit formulation to control active soft
bodies by defining a function that enables a continuous mapping from a spatial
point in the material space to the actuation value. This property allows us to
capture the signal&apos;s dominant frequencies, making the method discretization
agnostic and widely applicable. We extend our implicit model to mandible
kinematics for the particular case of facial animation and show that we can
reliably reproduce facial expressions captured with high-quality capture
systems. We apply the method to volumetric soft bodies, human poses, and facial
expressions, demonstrating artist-friendly properties, such as simple control
over the latent space and resolution invariance at test time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lingchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Byungsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoss_G/0/1/0/all/0/1&quot;&gt;Gaspard Zoss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gozcu_B/0/1/0/all/0/1&quot;&gt;Baran G&amp;#xf6;zc&amp;#xfc;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_M/0/1/0/all/0/1&quot;&gt;Markus Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solenthaler_B/0/1/0/all/0/1&quot;&gt;Barbara Solenthaler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14895">
<title>MPTQ-ViT:Mixed-PrecisionPost-TrainingQuantizationforVisionTransformer. (arXiv:2401.14895v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14895</link>
<description rdf:parseType="Literal">&lt;p&gt;While vision transformers (ViTs) have shown great potential in computer
vision tasks, their intense computation and memory requirements pose challenges
for practical applications. Existing post-training quantization methods
leverage value redistribution or specialized quantizers to address the
non-normal distribution in ViTs. However, without considering the asymmetry in
activations and relying on hand-crafted settings, these methods often struggle
to maintain performance under low-bit quantization. To overcome these
challenges, we introduce SmoothQuant with bias term (SQ-b) to alleviate the
asymmetry issue and reduce the clamping loss. We also introduce optimal scaling
factor ratio search (OPT-m) to determine quantization parameters by a
data-dependent mechanism automatically. To further enhance the compressibility,
we incorporate the above-mentioned techniques and propose a mixed-precision
post-training quantization framework for vision transformers (MPTQ-ViT). We
develop greedy mixed-precision quantization (Greedy MP) to allocate layer-wise
bit-width considering both model performance and compressibility. Our
experiments on ViT, DeiT, and Swin demonstrate significant accuracy
improvements compared with SOTA on the ImageNet dataset. Specifically, our
proposed methods achieve accuracy improvements ranging from 0.90% to 23.35% on
4-bit ViTs with single-precision and from 3.82% to 78.14% on 5-bit fully
quantized ViTs with mixed-precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Shan Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An-Yeu/0/1/0/all/0/1&quot;&gt;An-Yeu&lt;/a&gt; (Andy)Wu</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14919">
<title>PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus. (arXiv:2401.14919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14919</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a real-time method for robust estimation of multiple instances of
geometric models from noisy data. Geometric models such as vanishing points,
planar homographies or fundamental matrices are essential for 3D scene
analysis. Previous approaches discover distinct model instances in an iterative
manner, thus limiting their potential for speedup via parallel computation. In
contrast, our method detects all model instances independently and in parallel.
A neural network segments the input data into clusters representing potential
model instances by predicting multiple sets of sample and inlier weights. Using
the predicted weights, we determine the model parameters for each potential
instance separately in a RANSAC-like fashion. We train the neural network via
task-specific loss functions, i.e. we do not require a ground-truth
segmentation of the input data. As suitable training data for homography and
fundamental matrix fitting is scarce, we additionally present two new synthetic
datasets. We demonstrate state-of-the-art performance on these as well as
multiple established datasets, with inference times as small as five
milliseconds per image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kluger_F/0/1/0/all/0/1&quot;&gt;Florian Kluger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1&quot;&gt;Bodo Rosenhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14938">
<title>DAM: Diffusion Activation Maximization for 3D Global Explanations. (arXiv:2401.14938v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14938</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the performance of point cloud models has been rapidly
improved. However, due to the limited amount of relevant explainability
studies, the unreliability and opacity of these black-box models may lead to
potential risks in applications where human lives are at stake, e.g. autonomous
driving or healthcare. This work proposes a DDPM-based point cloud global
explainability method (DAM) that leverages Point Diffusion Transformer (PDT), a
novel point-wise symmetric model, with dual-classifier guidance to generate
high-quality global explanations. In addition, an adapted path gradient
integration method for DAM is proposed, which not only provides a global
overview of the saliency maps for point cloud categories, but also sheds light
on how the attributions of the explanations vary during the generation process.
Extensive experiments indicate that our method outperforms existing ones in
terms of perceptibility, representativeness, and diversity, with a significant
reduction in generation time. Our code is available at:
https://github.com/Explain3D/DAM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hanxiao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14948">
<title>Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14948</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training improves the robustness of neural networks against
adversarial attacks, albeit at the expense of the trade-off between standard
and robust generalization. To unveil the underlying factors driving this
phenomenon, we examine the layer-wise learning capabilities of neural networks
during the transition from a standard to an adversarial setting. Our empirical
findings demonstrate that selectively updating specific layers while preserving
others can substantially enhance the network&apos;s learning capacity. We therefore
propose CURE, a novel training framework that leverages a gradient prominence
criterion to perform selective conservation, updating, and revision of weights.
Importantly, CURE is designed to be dataset- and architecture-agnostic,
ensuring its applicability across various scenarios. It effectively tackles
both memorization and overfitting issues, thus enhancing the trade-off between
robustness and generalization and additionally, this training approach also
aids in mitigating &quot;robust overfitting&quot;. Furthermore, our study provides
valuable insights into the mechanisms of selective adversarial training and
offers a promising avenue for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1&quot;&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1&quot;&gt;Bahram Zonooz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1&quot;&gt;Elahe Arani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14966">
<title>Masked Pre-trained Model Enables Universal Zero-shot Denoiser. (arXiv:2401.14966v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14966</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we observe that the model, which is trained on vast general
images using masking strategy, has been naturally embedded with the
distribution knowledge regarding natural images, and thus spontaneously attains
the underlying potential for strong image denoising. Based on this observation,
we propose a novel zero-shot denoising paradigm, i.e., Masked Pre-train then
Iterative fill (MPI). MPI pre-trains a model with masking and fine-tunes it for
denoising of a single image with unseen noise degradation. Concretely, the
proposed MPI comprises two key procedures: 1) Masked Pre-training involves
training a model on multiple natural images with random masks to gather
generalizable representations, allowing for practical applications in varying
noise degradation and even in distinct image types. 2) Iterative filling is
devised to efficiently fuse pre-trained knowledge for denoising. Similar to but
distinct from pre-training, random masking is retained to bridge the gap, but
only the predicted parts covered by masks are assembled for efficiency, which
enables high-quality denoising within a limited number of iterations.
Comprehensive experiments across various noisy scenarios underscore the notable
advances of proposed MPI over previous approaches with a marked reduction in
inference time. Code is available at https://github.com/krennic999/MPI.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_P/0/1/0/all/0/1&quot;&gt;Pengyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianle Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Ben Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Junkang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15002">
<title>BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning. (arXiv:2401.15002v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15002</link>
<description rdf:parseType="Literal">&lt;p&gt;As an emerging and vital topic for studying deep neural networks&apos;
vulnerability (DNNs), backdoor learning has attracted increasing interest in
recent years, and many seminal backdoor attack and defense algorithms are being
developed successively or concurrently, in the status of a rapid arms race.
However, mainly due to the diverse settings, and the difficulties of
implementation and reproducibility of existing works, there is a lack of a
unified and standardized benchmark of backdoor learning, causing unfair
comparisons, and unreliable conclusions (e.g., misleading, biased or even false
conclusions). Consequently, it is difficult to evaluate the current progress
and design the future development roadmap of this literature. To alleviate this
dilemma, we build a comprehensive benchmark of backdoor learning called
BackdoorBench. Our benchmark makes three valuable contributions to the research
community. 1) We provide an integrated implementation of state-of-the-art
(SOTA) backdoor learning algorithms (currently including 16 attack and 27
defense algorithms), based on an extensible modular-based codebase. 2) We
conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5
poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of
evaluations in total. 3) Based on above evaluations, we present abundant
analysis from 8 perspectives via 18 useful analysis tools, and provide several
inspiring insights about backdoor learning. We hope that our efforts could
build a solid foundation of backdoor learning to facilitate researchers to
investigate existing algorithms, develop more innovative algorithms, and
explore the intrinsic mechanism of backdoor learning. Finally, we have created
a user-friendly website at &lt;a href=&quot;http://backdoorbench.com&quot;&gt;this http URL&lt;/a&gt;, which collects all
important information of BackdoorBench, including codebase, docs, leaderboard,
and model Zoo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongrui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaokui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1&quot;&gt;Danni Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingli Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruotong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15022">
<title>Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.15022</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the diagnosis of gliomas has become increasingly complex.
Histological assessment of glioma tissue using modern machine learning
techniques offers new opportunities to support diagnosis and outcome
prediction. To give an overview of the current state of research, this review
examines 70 publicly available research studies on machine learning-based
analysis of stained human glioma tissue sections, covering the diagnostic tasks
of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and
survival prediction (27/70). All studies were reviewed with regard to
methodological aspects as well as clinical applicability. It was found that the
focus of current research is the assessment of hematoxylin and eosin-stained
tissue sections of adult-type diffuse gliomas. The majority of studies (49/70)
are based on the publicly available glioblastoma and low-grade glioma datasets
from The Cancer Genome Atlas (TCGA) and only a few studies employed other
datasets in isolation (10/70) or in addition to the TCGA datasets (11/70).
Current approaches mostly rely on convolutional neural networks (53/70) for
analyzing tissue at 20x magnification (30/70). A new field of research is the
integration of clinical data, omics data, or magnetic resonance imaging
(27/70). So far, machine learning-based methods have achieved promising
results, but are not yet used in real clinical settings. Future work should
focus on the independent validation of methods on larger, multi-site datasets
with high-quality and up-to-date clinical and molecular pathology annotations
to demonstrate routine applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Redlich_J/0/1/0/all/0/1&quot;&gt;Jan-Philipp Redlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feuerhake_F/0/1/0/all/0/1&quot;&gt;Friedrich Feuerhake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weis_J/0/1/0/all/0/1&quot;&gt;Joachim Weis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaadt_N/0/1/0/all/0/1&quot;&gt;Nadine S. Schaadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Teuber_Hanselmann_S/0/1/0/all/0/1&quot;&gt;Sarah Teuber-Hanselmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Buck_C/0/1/0/all/0/1&quot;&gt;Christoph Buck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luttmann_S/0/1/0/all/0/1&quot;&gt;Sabine Luttmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eberle_A/0/1/0/all/0/1&quot;&gt;Andrea Eberle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nikolin_S/0/1/0/all/0/1&quot;&gt;Stefan Nikolin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Appenzeller_A/0/1/0/all/0/1&quot;&gt;Arno Appenzeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Portmann_A/0/1/0/all/0/1&quot;&gt;Andreas Portmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Homeyer_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Homeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15029">
<title>Learning Neural Radiance Fields of Forest Structure for Scalable and Fine Monitoring. (arXiv:2401.15029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15029</link>
<description rdf:parseType="Literal">&lt;p&gt;This work leverages neural radiance fields and remote sensing for forestry
applications. Here, we show neural radiance fields offer a wide range of
possibilities to improve upon existing remote sensing methods in forest
monitoring. We present experiments that demonstrate their potential to: (1)
express fine features of forest 3D structure, (2) fuse available remote sensing
modalities and (3), improve upon 3D structure derived forest metrics.
Altogether, these properties make neural fields an attractive computational
tool with great potential to further advance the scalability and accuracy of
forest monitoring programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castorena_J/0/1/0/all/0/1&quot;&gt;Juan Castorena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15048">
<title>Unrecognizable Yet Identifiable: Image Distortion with Preserved Embeddings. (arXiv:2401.15048v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15048</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of security applications, biometric authentication systems play
a crucial role, yet one often encounters challenges concerning privacy and
security while developing one. One of the most fundamental challenges lies in
avoiding storing biometrics directly in the storage but still achieving
decently high accuracy. Addressing this issue, we contribute to both artificial
intelligence and engineering fields. We introduce an innovative image
distortion technique that effectively renders facial images unrecognizable to
the eye while maintaining their identifiability by neural network models. From
the theoretical perspective, we explore how reliable state-of-the-art
biometrics recognition neural networks are by checking the maximal degree of
image distortion, which leaves the predicted identity unchanged. On the other
hand, applying this technique demonstrates a practical solution to the
engineering challenge of balancing security, precision, and performance in
biometric authentication systems. Through experimenting on the widely used
datasets, we assess the effectiveness of our method in preserving AI feature
representation and distorting relative to conventional metrics. We also compare
our method with previously used approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharov_D/0/1/0/all/0/1&quot;&gt;Dmytro Zakharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_O/0/1/0/all/0/1&quot;&gt;Oleksandr Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frontoni_E/0/1/0/all/0/1&quot;&gt;Emanuele Frontoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15055">
<title>Deep learning-based approach for tomato classification in complex scenes. (arXiv:2401.15055v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15055</link>
<description rdf:parseType="Literal">&lt;p&gt;Tracking ripening tomatoes is time consuming and labor intensive. Artificial
intelligence technologies combined with those of computer vision can help users
optimize the process of monitoring the ripening status of plants. To this end,
we have proposed a tomato ripening monitoring approach based on deep learning
in complex scenes. The objective is to detect mature tomatoes and harvest them
in a timely manner. The proposed approach is declined in two parts. Firstly,
the images of the scene are transmitted to the pre-processing layer. This
process allows the detection of areas of interest (area of the image containing
tomatoes). Then, these images are used as input to the maturity detection
layer. This layer, based on a deep neural network learning algorithm,
classifies the tomato thumbnails provided to it in one of the following five
categories: green, brittle, pink, pale red, mature red. The experiments are
based on images collected from the internet gathered through searches using
tomato state across diverse languages including English, German, French, and
Spanish. The experimental results of the maturity detection layer on a dataset
composed of images of tomatoes taken under the extreme conditions, gave a good
classification rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousse_M/0/1/0/all/0/1&quot;&gt;Mikael A. Mousse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atohoun_B/0/1/0/all/0/1&quot;&gt;Bethel C. A. R. K. Atohoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motamed_C/0/1/0/all/0/1&quot;&gt;Cina Motamed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15071">
<title>From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities. (arXiv:2401.15071v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15071</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) have shown impressive abilities in
generating reasonable responses with respect to multi-modal contents. However,
there is still a wide gap between the performance of recent MLLM-based
applications and the expectation of the broad public, even though the most
powerful OpenAI&apos;s GPT-4 and Google&apos;s Gemini have been deployed. This paper
strives to enhance understanding of the gap through the lens of a qualitative
study on the generalizability, trustworthiness, and causal reasoning
capabilities of recent proprietary and open-source MLLMs across four
modalities: ie, text, code, image, and video, ultimately aiming to improve the
transparency of MLLMs. We believe these properties are several representative
factors that define the reliability of MLLMs, in supporting various downstream
applications. To be specific, we evaluate the closed-source GPT-4 and Gemini
and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed
cases, where the qualitative results are then summarized into 12 scores (ie, 4
modalities times 3 properties). In total, we uncover 14 empirical findings that
are useful to understand the capabilities and limitations of both proprietary
and open-source MLLMs, towards more reliable downstream multi-modal
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chaochao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guodong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Hongxing Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongzhi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jingyi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jinlan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kexin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunchang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Meiqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qibing Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sirui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yali Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1&quot;&gt;Yan Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yinan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yingchun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yujiong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_Y/0/1/0/all/0/1&quot;&gt;Yurong Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zaibin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhelun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhenfei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhipin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.15075">
<title>Annotated Hands for Generative Models. (arXiv:2401.15075v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.15075</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1&quot;&gt;Atith N Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turk_G/0/1/0/all/0/1&quot;&gt;Greg Turk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10981">
<title>MGTUNet: An new UNet for colon nuclei instance segmentation and quantification. (arXiv:2210.10981v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10981</link>
<description rdf:parseType="Literal">&lt;p&gt;Colorectal cancer (CRC) is among the top three malignant tumor types in terms
of morbidity and mortality. Histopathological images are the gold standard for
diagnosing colon cancer. Cellular nuclei instance segmentation and
classification, and nuclear component regression tasks can aid in the analysis
of the tumor microenvironment in colon tissue. Traditional methods are still
unable to handle both types of tasks end-to-end at the same time, and have poor
prediction accuracy and high application costs. This paper proposes a new UNet
model for handling nuclei based on the UNet framework, called MGTUNet, which
uses Mish, Group normalization and transposed convolution layer to improve the
segmentation model, and a ranger optimizer to adjust the SmoothL1Loss values.
Secondly, it uses different channels to segment and classify different types of
nucleus, ultimately completing the nuclei instance segmentation and
classification task, and the nuclei component regression task simultaneously.
Finally, we did extensive comparison experiments using eight segmentation
models. By comparing the three evaluation metrics and the parameter sizes of
the models, MGTUNet obtained 0.6254 on PQ, 0.6359 on mPQ, and 0.8695 on R2.
Thus, the experiments demonstrated that MGTUNet is now a state-of-the-art
method for quantifying histopathological images of colon cancer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangrui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhichao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhujun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shaoliang Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16380">
<title>Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri. (arXiv:2210.16380v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16380</link>
<description rdf:parseType="Literal">&lt;p&gt;Performing classification on noisy, crowdsourced image datasets can prove
challenging even for the best neural networks. Two issues which complicate the
problem on such datasets are class imbalance and ground-truth uncertainty in
labeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped,
individual characters from images of ancient Greek papyri - are strongly
affected by both issues. The application of ensemble modeling to such datasets
can help identify images where the ground-truth is questionable and quantify
the trustworthiness of those samples. As such, we apply stacked generalization
consisting of nearly identical ResNets with different loss functions: one
utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence
(KLD). Both networks use labels drawn from a crowd-sourced consensus. This
consensus is derived from a Normalized Distribution of Annotations (NDA) based
on all annotations for a given character in the dataset. For the second
network, the KLD is calculated with respect to the NDA. For our ensemble model,
we apply a k-nearest neighbors model to the outputs of the CXE and KLD
networks. Individually, the ResNet models have approximately 93% accuracy,
while the ensemble model achieves an accuracy of &amp;gt; 95%, increasing the
classification trustworthiness. We also perform an analysis of the Shannon
entropy of the various models&apos; output distributions to measure classification
uncertainty. Our results suggest that entropy is useful for predicting model
misclassifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_G/0/1/0/all/0/1&quot;&gt;Graham West&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swindall_M/0/1/0/all/0/1&quot;&gt;Matthew I. Swindall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keener_B/0/1/0/all/0/1&quot;&gt;Ben Keener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Player_T/0/1/0/all/0/1&quot;&gt;Timothy Player&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Alex C. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brusuelas_J/0/1/0/all/0/1&quot;&gt;James H. Brusuelas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallin_J/0/1/0/all/0/1&quot;&gt;John F. Wallin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04492">
<title>Few-View Object Reconstruction with Unknown Categories and Camera Poses. (arXiv:2212.04492v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04492</link>
<description rdf:parseType="Literal">&lt;p&gt;While object reconstruction has made great strides in recent years, current
methods typically require densely captured images and/or known camera poses,
and generalize poorly to novel object categories. To step toward object
reconstruction in the wild, this work explores reconstructing general
real-world objects from a few images without known camera poses or object
categories. The crux of our work is solving two fundamental 3D vision problems
-- shape reconstruction and pose estimation -- in a unified approach. Our
approach captures the synergies of these two problems: reliable camera pose
estimation gives rise to accurate shape reconstruction, and the accurate
reconstruction, in turn, induces robust correspondence between different views
and facilitates pose estimation. Our method FORGE predicts 3D features from
each view and leverages them in conjunction with the input images to establish
cross-view correspondence for estimating relative camera poses. The 3D features
are then transformed by the estimated poses into a shared space and are fused
into a neural radiance field. The reconstruction results are rendered by volume
rendering techniques, enabling us to train the model without 3D shape
ground-truth. Our experiments show that FORGE reliably reconstructs objects
from five views. Our pose estimation method outperforms existing ones by a
large margin. The reconstruction results under predicted poses are comparable
to the ones using ground-truth poses. The performance on novel testing
categories matches the results on categories seen during training. Project
page: https://ut-austin-rpl.github.io/FORGE/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hanwen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04582">
<title>Towards Holistic Surgical Scene Understanding. (arXiv:2212.04582v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04582</link>
<description rdf:parseType="Literal">&lt;p&gt;Most benchmarks for studying surgical interventions focus on a specific
challenge instead of leveraging the intrinsic complementarity among different
tasks. In this work, we present a new experimental framework towards holistic
surgical scene understanding. First, we introduce the Phase, Step, Instrument,
and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes
annotations for both long-term (Phase and Step recognition) and short-term
reasoning (Instrument detection and novel Atomic Action recognition) in
robot-assisted radical prostatectomy videos. Second, we present Transformers
for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong
baseline for surgical scene understanding. TAPIR leverages our dataset&apos;s
multi-level annotations as it benefits from the learned representation on the
instrument detection task to improve its classification capacity. Our
experimental results in both PSI-AVA and other publicly available databases
demonstrate the adequacy of our framework to spur future research on holistic
surgical scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valderrama_N/0/1/0/all/0/1&quot;&gt;Natalia Valderrama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puentes_P/0/1/0/all/0/1&quot;&gt;Paola Ruiz Puentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1&quot;&gt;Isabela Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verlyk_M/0/1/0/all/0/1&quot;&gt;Mathilde Verlyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santander_J/0/1/0/all/0/1&quot;&gt;Jessica Santander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1&quot;&gt;Juan Caicedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11464">
<title>Gap-closing Matters: Perceptual Quality Evaluation and Optimization of Low-Light Image Enhancement. (arXiv:2302.11464v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11464</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing consensus in the research community that the optimization
of low-light image enhancement approaches should be guided by the visual
quality perceived by end users. Despite the substantial efforts invested in the
design of low-light enhancement algorithms, there has been comparatively
limited focus on assessing subjective and objective quality systematically. To
mitigate this gap and provide a clear path towards optimizing low-light image
enhancement for better visual quality, we propose a gap-closing framework. In
particular, our gap-closing framework starts with the creation of a large-scale
dataset for Subjective QUality Assessment of REconstructed LOw-Light Images
(SQUARE-LOL). This database serves as the foundation for studying the quality
of enhanced images and conducting a comprehensive subjective user study.
Subsequently, we propose an objective quality assessment measure that plays a
critical role in bridging the gap between visual quality and enhancement.
Finally, we demonstrate that our proposed objective quality measure can be
incorporated into the process of optimizing the learning of the enhancement
model toward perceptual optimality. We validate the effectiveness of our
proposed framework through both the accuracy of quality prediction and the
perceptual quality of image enhancement. Our database and code will be made
publicly available to facilitate further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lingyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linqi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00612">
<title>Has the Virtualization of the Face Changed Facial Perception? A Study of the Impact of Augmented Reality on Facial Perception. (arXiv:2303.00612v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00612</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented reality and other photo editing filters are popular methods used to
modify faces online. Considering the important role of facial perception in
communication, how do we perceive this increasing number of modified faces? In
this paper we present the results of six surveys that measure familiarity with
different styles of facial filters, perceived strangeness of faces edited with
different filters, and ability to discern whether images are filtered. Our
results demonstrate that faces modified with more traditional face filters are
perceived similarly to unmodified faces, and faces filtered with augmented
reality filters are perceived differently from unmodified faces. We discuss
possible explanations for these results, including a societal adjustment to
traditional photo editing techniques or the inherent differences in the
different types of filters. We conclude with a discussion of how to build
online spaces more responsibly based on our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conwill_L/0/1/0/all/0/1&quot;&gt;Louisa Conwill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_S/0/1/0/all/0/1&quot;&gt;Samuel Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1&quot;&gt;Walter Scheirer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09514">
<title>MATIS: Masked-Attention Transformers for Surgical Instrument Segmentation. (arXiv:2303.09514v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09514</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Masked-Attention Transformers for Surgical Instrument Segmentation
(MATIS), a two-stage, fully transformer-based method that leverages modern
pixel-wise attention mechanisms for instrument segmentation. MATIS exploits the
instance-level nature of the task by employing a masked attention module that
generates and classifies a set of fine instrument region proposals. Our method
incorporates long-term video-level information through video transformers to
improve temporal consistency and enhance mask classification. We validate our
approach in the two standard public benchmarks, Endovis 2017 and Endovis 2018.
Our experiments demonstrate that MATIS&apos; per-frame baseline outperforms previous
state-of-the-art methods and that including our temporal consistency module
boosts our model&apos;s performance further.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Rondon_A/0/1/0/all/0/1&quot;&gt;Alejandra P&amp;#xe9;rez-Rond&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07250">
<title>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments. (arXiv:2304.07250v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;The localization of objects is a crucial task in various applications such as
robotics, virtual and augmented reality, and the transportation of goods in
warehouses. Recent advances in deep learning have enabled the localization
using monocular visual cameras. While structure from motion (SfM) predicts the
absolute pose from a point cloud, absolute pose regression (APR) methods learn
a semantic understanding of the environment through neural networks. However,
both fields face challenges caused by the environment such as motion blur,
lighting changes, repetitive patterns, and feature-less structures. This study
aims to address these challenges by incorporating additional information and
regularizing the absolute pose using relative pose regression (RPR) methods.
RPR methods suffer under different challenges, i.e., motion blur. The optical
flow between consecutive images is computed using the Lucas-Kanade algorithm,
and the relative pose is predicted using an auxiliary small recurrent
convolutional network. The fusion of absolute and relative poses is a complex
task due to the mismatch between the global and local coordinate systems.
State-of-the-art methods fusing absolute and relative poses use pose graph
optimization (PGO) to regularize the absolute pose predictions using relative
poses. In this work, we propose recurrent fusion networks to optimally align
absolute and relative pose predictions to improve the absolute pose prediction.
We evaluate eight different recurrent units and construct a simulation
environment to pre-train the APR and RPR networks for better generalized
training. Additionally, we record a large database of different scenarios in a
challenging large-scale indoor environment that mimics a warehouse with
transportation robots. We conduct hyperparameter searches and experiments to
show the effectiveness of our recurrent fusion method compared to PGO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1&quot;&gt;Felix Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1&quot;&gt;Lucas Heublein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1&quot;&gt;Christopher Mutschler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00393">
<title>DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization. (arXiv:2305.00393v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning of object-centric representations in dynamic visual
scenes is challenging. Unlike most previous approaches that learn to decompose
2D images, we present DynaVol, a 3D scene generative model that unifies
geometric structures and object-centric learning in a differentiable volume
rendering framework. The key idea is to perform object-centric voxelization to
capture the 3D nature of the scene, which infers the probability distribution
over objects at individual spatial locations. These voxel features evolve over
time through a canonical-space deformation function, forming the basis for
global representation learning via slot attention. The voxel features and
global features are complementary and are both leveraged by a compositional
NeRF decoder for volume rendering. DynaVol remarkably outperforms existing
approaches for unsupervised dynamic scene decomposition. Once trained, the
explicitly meaningful voxel features enable additional capabilities that 2D
scene decomposition methods cannot achieve: it is possible to freely edit the
geometric shapes or manipulate the motion trajectories of the objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yanpeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Siyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01747">
<title>Expectation Maximization Pseudo Labels. (arXiv:2305.01747v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01747</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study pseudo-labelling. Pseudo-labelling employs raw
inferences on unlabelled data as pseudo-labels for self-training. We elucidate
the empirical successes of pseudo-labelling by establishing a link between this
technique and the Expectation Maximisation algorithm. Through this, we realise
that the original pseudo-labelling serves as an empirical estimation of its
more comprehensive underlying formulation. Following this insight, we present a
full generalisation of pseudo-labels under Bayes&apos; theorem, termed Bayesian
Pseudo Labels. Subsequently, we introduce a variational approach to generate
these Bayesian Pseudo Labels, involving the learning of a threshold to
automatically select high-quality pseudo labels. In the remainder of the paper,
we showcase the applications of pseudo-labelling and its generalised form,
Bayesian Pseudo-Labelling, in the semi-supervised segmentation of medical
images. Specifically, we focus on: 1) 3D binary segmentation of lung vessels
from CT volumes; 2) 2D multi-class segmentation of brain tumours from MRI
volumes; 3) 3D binary segmentation of whole brain tumours from MRI volumes; and
4) 3D binary segmentation of prostate from MRI volumes. We further demonstrate
that pseudo-labels can enhance the robustness of the learned representations.
The code is released in the following GitHub repository:
https://github.com/moucheng2017/EMSSL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Moucheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yukun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groot_M/0/1/0/all/0/1&quot;&gt;Marius de Groot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oxtoby_N/0/1/0/all/0/1&quot;&gt;Neil P. Oxtoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1&quot;&gt;Joseph Jacob&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19956">
<title>MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19956</link>
<description rdf:parseType="Literal">&lt;p&gt;Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that
provides 3-4 times higher resolution than traditional ultrasound, potentially
enabling low-cost, accurate diagnosis of prostate cancer. Accurate prostate
segmentation is crucial for prostate volume measurement, cancer diagnosis,
prostate biopsy, and treatment planning. However, prostate segmentation on
micro-US is challenging due to artifacts and indistinct borders between the
prostate, bladder, and urethra in the midline. This paper presents MicroSegNet,
a multi-scale annotation-guided transformer UNet model designed specifically to
tackle these challenges. During the training process, MicroSegNet focuses more
on regions that are hard to segment (hard regions), characterized by
discrepancies between expert and non-expert annotations. We achieve this by
proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns
a larger weight to prediction errors in hard regions and a lower weight to
prediction errors in easy regions. The AG-BCE loss was seamlessly integrated
into the training process through the utilization of multi-scale deep
supervision, enabling MicroSegNet to capture global contextual dependencies and
local information at various scales. We trained our model using micro-US images
from 55 patients, followed by evaluation on 20 patients. Our MicroSegNet model
achieved a Dice coefficient of 0.939 and a Hausdorff distance of 2.02 mm,
outperforming several state-of-the-art segmentation methods, as well as three
human annotators with different experience levels. Our code is publicly
available at https://github.com/mirthAI/MicroSegNet and our dataset is publicly
available at https://zenodo.org/records/10475293.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hongxu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1&quot;&gt;Muhammad Imran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muralidharan_P/0/1/0/all/0/1&quot;&gt;Preethika Muralidharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Anjali Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pensa_J/0/1/0/all/0/1&quot;&gt;Jake Pensa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1&quot;&gt;Muxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benidir_T/0/1/0/all/0/1&quot;&gt;Tarik Benidir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grajo_J/0/1/0/all/0/1&quot;&gt;Joseph R. Grajo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_J/0/1/0/all/0/1&quot;&gt;Jason P. Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terry_R/0/1/0/all/0/1&quot;&gt;Russell Terry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DiBianco_J/0/1/0/all/0/1&quot;&gt;John Michael DiBianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Li-Ming Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brisbane_W/0/1/0/all/0/1&quot;&gt;Wayne G. Brisbane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01344">
<title>Adjustable Visual Appearance for Generalizable Novel View Synthesis. (arXiv:2306.01344v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01344</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a generalizable novel view synthesis method which enables
modifying the visual appearance of an observed scene so rendered views match a
target weather or lighting condition without any scene specific training or
access to reference views at the target condition. Our method is based on a
pretrained generalizable transformer architecture and is fine-tuned on
synthetically generated scenes under different appearance conditions. This
allows for rendering novel views in a consistent manner for 3D scenes that were
not included in the training set, along with the ability to (i) modify their
appearance to match the target condition and (ii) smoothly interpolate between
different conditions. Experiments on real and synthetic scenes show that our
method is able to generate 3D consistent renderings while making realistic
appearance changes, including qualitative and quantitative comparisons. Please
refer to our project page for video results: https://ava-nvs.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengtson_J/0/1/0/all/0/1&quot;&gt;Josef Bengtson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nilsson_D/0/1/0/all/0/1&quot;&gt;David Nilsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Che-Tsung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busching_M/0/1/0/all/0/1&quot;&gt;Marcel B&amp;#xfc;sching&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1&quot;&gt;Fredrik Kahl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05879">
<title>FedWon: Triumphing Multi-domain Federated Learning Without Normalization. (arXiv:2306.05879v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05879</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enhances data privacy with collaborative in-situ
training on decentralized clients. Nevertheless, FL encounters challenges due
to non-independent and identically distributed (non-i.i.d) data, leading to
potential performance degradation and hindered convergence. While prior studies
predominantly addressed the issue of skewed label distribution, our research
addresses a crucial yet frequently overlooked problem known as multi-domain FL.
In this scenario, clients&apos; data originate from diverse domains with distinct
feature distributions, instead of label distributions. To address the
multi-domain problem in FL, we propose a novel method called Federated learning
Without normalizations (FedWon). FedWon draws inspiration from the observation
that batch normalization (BN) faces challenges in effectively modeling the
statistics of multiple domains, while existing normalization techniques possess
their own limitations. In order to address these issues, FedWon eliminates the
normalization layers in FL and reparameterizes convolution layers with scaled
weight standardization. Through extensive experimentation on five datasets and
five models, our comprehensive experimental results demonstrate that FedWon
surpasses both FedAvg and the current state-of-the-art method (FedBN) across
all experimental setups, achieving notable accuracy improvements of more than
10% in certain domains. Furthermore, FedWon is versatile for both cross-silo
and cross-device FL, exhibiting robust domain generalization capability,
showcasing strong performance even with a batch size as small as 1, thereby
catering to resource-constrained devices. Additionally, FedWon can also
effectively tackle the challenge of skewed label distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1&quot;&gt;Lingjuan Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09818">
<title>HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation. (arXiv:2306.09818v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09818</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based video compression is currently a popular research topic,
offering the potential to compete with conventional standard video codecs. In
this context, Implicit Neural Representations (INRs) have previously been used
to represent and compress image and video content, demonstrating relatively
high decoding speed compared to other methods. However, existing INR-based
methods have failed to deliver rate quality performance comparable with the
state of the art in video compression. This is mainly due to the simplicity of
the employed network architectures, which limit their representation
capability. In this paper, we propose HiNeRV, an INR that combines light weight
layers with novel hierarchical positional encodings. We employs depth-wise
convolutional, MLP and interpolation layers to build the deep and wide network
architecture with high capacity. HiNeRV is also a unified representation
encoding videos in both frames and patches at the same time, which offers
higher performance and flexibility than existing methods. We further build a
video codec based on HiNeRV and a refined pipeline for training, pruning and
quantization that can better preserve HiNeRV&apos;s performance during lossy model
compression. The proposed method has been evaluated on both UVG and MCL-JCV
datasets for video compression, demonstrating significant improvement over all
existing INRs baselines and competitive performance when compared to
learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over
DCVC on the UVG dataset, measured in PSNR).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kwan_H/0/1/0/all/0/1&quot;&gt;Ho Man Kwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Ge Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gower_A/0/1/0/all/0/1&quot;&gt;Andrew Gower&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11305">
<title>Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11305</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Implicit Representation (NIR) has recently gained significant
attention due to its remarkable ability to encode complex and high-dimensional
data into representation space and easily reconstruct it through a trainable
mapping function. However, NIR methods assume a one-to-one mapping between the
target data and representation models regardless of data relevancy or
similarity. This results in poor generalization over multiple complex data and
limits their efficiency and scalability. Motivated by continual learning, this
work investigates how to accumulate and transfer neural implicit
representations for multiple complex video data over sequential encoding
sessions. To overcome the limitation of NIR, we propose a novel method,
Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive
and compact sub-module in Fourier space to encode videos in each training
session. This sparsified neural encoding allows the neural network to hold free
weights, enabling an improved adaptation for future videos. In addition, when
learning a representation for a new video, PFNR transfers the representation of
previous videos with frozen weights. This design allows the model to
continuously accumulate high-quality neural representations for multiple videos
while ensuring lossless decoding that perfectly preserves the learned
representations for previous videos. We validate our PFNR method on the UVG8/17
and DAVIS50 video sequence benchmarks and achieve impressive performance gains
over strong continual learning baselines. The PFNR code is available at
https://github.com/ihaeyong/PFNR.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Haeyong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;DaHyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00818">
<title>Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset. (arXiv:2307.00818v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00818</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present Motion-X, a large-scale 3D expressive whole-body
motion dataset. Existing motion datasets predominantly contain body-only poses,
lacking facial expressions, hand gestures, and fine-grained pose descriptions.
Moreover, they are primarily collected from limited laboratory scenes with
textual descriptions manually labeled, which greatly limits their scalability.
To overcome these limitations, we develop a whole-body motion and text
annotation pipeline, which can automatically annotate motion from either
single- or multi-view videos and provide comprehensive semantic labels for each
video and fine-grained whole-body pose descriptions for each frame. This
pipeline is of high precision, cost-effective, and scalable for further
research. Based on it, we construct Motion-X, which comprises 15.6M precise 3D
whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from
massive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose
descriptions and 81.1K sequence-level semantic labels. Comprehensive
experiments demonstrate the accuracy of the annotation pipeline and the
significant benefit of Motion-X in enhancing expressive, diverse, and natural
motion generation, as well as 3D whole-body human mesh recovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shunlin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruimao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoqian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01946">
<title>ECG-Image-Kit: A Synthetic Image Generation Toolbox to Facilitate Deep Learning-Based Electrocardiogram Digitization. (arXiv:2307.01946v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01946</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce ECG-Image-Kit, an open-source toolbox for generating synthetic
ECG images with realistic artifacts from time-series data, and showcase its
application in developing algorithms for data augmentation and ECG image
digitization. Synthetic data is generated by producing distortionless ECG
images on a standard ECG paper background. Subsequently, various distortions,
including handwritten text artifacts, wrinkles, creases, and perspective
transformations, are applied to these ECG images. The artifacts and text are
synthetically generated, excluding personally identifiable information. The
toolbox is used for data augmentation in the 2024 PhysioNet Challenge on
Digitization and Classification of ECG Images.
&lt;/p&gt;
&lt;p&gt;As a case study, we employed ECG-Image-Kit to create an ECG image dataset of
21,801 records from the PhysioNet QT database. A denoising convolutional neural
network (DnCNN)-based model was developed and trained on this synthetic dataset
and used to convert the synthetically generated images back into time-series
data for evaluation. SNR was calculated to assess the quality of image
digitization compared to the ground truth ECG time-series. The results show an
average signal recovery SNR of 11.17 +/- 9.19 dB, indicating the synthetic ECG
image dataset&apos;s significance for training deep learning models. For clinical
evaluation, we measured the error between the estimated and ground-truth
time-series data&apos;s RR and QT-intervals. The accuracy of the estimated RR and
QT-intervals also suggests that the respective clinical parameters are
maintained. These results demonstrate the effectiveness of a deep
learning-based pipeline in accurately digitizing paper ECGs and highlight a
generative approach to digitization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivashankara_K/0/1/0/all/0/1&quot;&gt;Kshama Kodthalu Shivashankara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deepanshi/0/1/0/all/0/1&quot;&gt;Deepanshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shervedani_A/0/1/0/all/0/1&quot;&gt;Afagh Mehri Shervedani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1&quot;&gt;Gari D. Clifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyna_M/0/1/0/all/0/1&quot;&gt;Matthew A. Reyna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sameni_R/0/1/0/all/0/1&quot;&gt;Reza Sameni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11082">
<title>Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11082</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xuzheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qingpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wei Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00517">
<title>Assessing the Generalizability of Deep Neural Networks-Based Models for Black Skin Lesions. (arXiv:2310.00517v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00517</link>
<description rdf:parseType="Literal">&lt;p&gt;Melanoma is the most severe type of skin cancer due to its ability to cause
metastasis. It is more common in black people, often affecting acral regions:
palms, soles, and nails. Deep neural networks have shown tremendous potential
for improving clinical care and skin cancer diagnosis. Nevertheless, prevailing
studies predominantly rely on datasets of white skin tones, neglecting to
report diagnostic outcomes for diverse patient skin tones. In this work, we
evaluate supervised and self-supervised models in skin lesion images extracted
from acral regions commonly observed in black individuals. Also, we carefully
curate a dataset containing skin lesions in acral regions and assess the
datasets concerning the Fitzpatrick scale to verify performance on black skin.
Our results expose the poor generalizability of these models, revealing their
favorable performance for lesions on white skin. Neglecting to create diverse
datasets, which necessitates the development of specialized models, is
unacceptable. Deep neural networks have great potential to improve diagnosis,
particularly for populations with limited access to dermatology. However,
including black skin lesions is necessary to ensure these populations can
access the benefits of inclusive technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barros_L/0/1/0/all/0/1&quot;&gt;Luana Barros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1&quot;&gt;Levy Chaves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1&quot;&gt;Sandra Avila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02601">
<title>MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02601</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird&apos;s-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02998">
<title>ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02998</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
advancements on various multimodal downstream tasks. However, deploying LVLMs
is often problematic due to their massive computational/energy costs and carbon
consumption. Such issues make it infeasible to adopt conventional iterative
global pruning, which is costly due to computing the Hessian matrix of the
entire large model for sparsification. Alternatively, several studies have
recently proposed layer-wise pruning approaches to avoid the expensive
computation of global pruning and efficiently compress model weights according
to their importance within a layer. However, they often suffer from suboptimal
model compression due to their lack of a global perspective. To address this
limitation in recent efficient pruning methods for large models, we propose
Efficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage
coarse-to-fine weight pruning approach for LVLMs. We first determine the
sparsity ratios of different layers or blocks by leveraging the global
importance score, which is efficiently computed based on the zeroth-order
approximation of the global model gradients. Then, the model performs local
layer-wise unstructured weight pruning based on globally-informed sparsity
ratios. We validate our proposed method across various multimodal and unimodal
models and datasets, demonstrating significant performance improvements over
prevalent pruning techniques in the high-sparsity regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1&quot;&gt;Yi-Lin Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18564">
<title>A General Framework for Robust G-Invariance in G-Equivariant Networks. (arXiv:2310.18564v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18564</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a general method for achieving robust group-invariance in
group-equivariant convolutional neural networks ($G$-CNNs), which we call the
$G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the
triple-correlation on groups, which is the unique, lowest-degree polynomial
invariant map that is also complete. Many commonly used invariant maps--such as
the max--are incomplete: they remove both group and signal structure. A
complete invariant, by contrast, removes only the variation due to the actions
of the group, while preserving all information about the structure of the
signal. The completeness of the triple correlation endows the $G$-TC layer with
strong robustness, which can be observed in its resistance to invariance-based
adversarial attacks. In addition, we observe that it yields measurable
improvements in classification accuracy over standard Max $G$-Pooling in
$G$-CNN architectures. We provide a general and efficient implementation of the
method for any discretized group, which requires only a table defining the
group&apos;s product structure. We demonstrate the benefits of this method for
$G$-CNNs defined on both commutative and non-commutative groups--$SO(2)$,
$O(2)$, $SO(3)$, and $O(3)$ (discretized as the cyclic $C8$, dihedral $D16$,
chiral octahedral $O$ and full octahedral $O_h$ groups)--acting on
$\mathbb{R}^2$ and $\mathbb{R}^3$ on both $G$-MNIST and $G$-ModelNet10
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1&quot;&gt;Sophia Sanborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1&quot;&gt;Nina Miolane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19731">
<title>ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19731</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have attracted a lot of popularity in recent
years, due to their exceptional capabilities in modeling long-range spatial
dependencies and scalability for large scale training. Although the training
parallelism of self-attention mechanism plays an important role in retaining
great performance, its quadratic complexity baffles the application of ViTs in
many scenarios which demand fast inference. This effect is even more pronounced
in applications in which autoregressive modeling of input features is required.
In Natural Language Processing (NLP), a new stream of efforts has proposed
parallelizable models with recurrent formulation that allows for efficient
inference in generative applications. Inspired by this trend, we propose a new
class of computer vision models, dubbed Vision Retention Networks (ViR), with
dual parallel and recurrent formulations, which strike an optimal balance
between fast inference and parallel training with competitive performance. In
particular, ViR scales favorably for image throughput and memory consumption in
tasks that require higher-resolution images due to its flexible formulation in
processing large sequence lengths. The ViR is the first attempt to realize dual
parallel and recurrent equivalency in a general vision backbone for recognition
tasks. We have validated the effectiveness of ViR through extensive experiments
with different dataset sizes and various image resolutions and achieved
competitive performance. Code: https://github.com/NVlabs/ViR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1&quot;&gt;Ali Hatamizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranzinger_M/0/1/0/all/0/1&quot;&gt;Michael Ranzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Shiyi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12401">
<title>CASR: Refining Action Segmentation via Marginalizing Frame-levle Causal Relationships. (arXiv:2311.12401v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12401</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating deep learning and causal discovery has increased the
interpretability of Temporal Action Segmentation (TAS) tasks. However,
frame-level causal relationships exist many complicated noises outside the
segment-level, making it infeasible to directly express macro action semantics.
Thus, we propose Causal Abstraction Segmentation Refiner (CASR), which can
refine TAS results from various models by enhancing video causality in
marginalizing frame-level casual relationships. Specifically, we define the
equivalent frame-level casual model and segment-level causal model, so that the
causal adjacency matrix constructed from marginalized frame-level causal
relationships has the ability to represent the segmnet-level causal
relationships. CASR works out by reducing the difference in the causal
adjacency matrix between we constructed and pre-segmentation results of
backbone models. In addition, we propose a novel evaluation metric Causal Edit
Distance (CED) to evaluate the causal interpretability. Extensive experimental
results on mainstream datasets indicate that CASR significantly surpasses
existing various methods in action segmentation performance, as well as in
causal explainability and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1&quot;&gt;Keqing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14786">
<title>GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14786</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting pedestrian behavior is the key to ensure safety and reliability of
autonomous vehicles. While deep learning methods have been promising by
learning from annotated video frame sequences, they often fail to fully grasp
the dynamic interactions between pedestrians and traffic, crucial for accurate
predictions. These models also lack nuanced common sense reasoning. Moreover,
the manual annotation of datasets for these models is expensive and challenging
to adapt to new situations. The advent of Vision Language Models (VLMs)
introduces promising alternatives to these issues, thanks to their advanced
visual and causal reasoning skills. To our knowledge, this research is the
first to conduct both quantitative and qualitative evaluations of VLMs in the
context of pedestrian behavior prediction for autonomous driving. We evaluate
GPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our
quantitative analysis focuses on GPT-4V&apos;s ability to predict pedestrian
behavior in current and future frames. The model achieves a 57% accuracy in a
zero-shot manner, which, while impressive, is still behind the state-of-the-art
domain-specific models (70%) in predicting pedestrian crossing actions.
Qualitatively, GPT-4V shows an impressive ability to process and interpret
complex traffic scenarios, differentiate between various pedestrian behaviors,
and detect and analyze groups. However, it faces challenges, such as difficulty
in detecting smaller pedestrians and assessing the relative motion between
pedestrians and the ego vehicle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1&quot;&gt;Alvika Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1&quot;&gt;Srikanth Saripalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15153">
<title>Self-Supervised Learning for SAR ATR with a Knowledge-Guided Predictive Architecture. (arXiv:2311.15153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15153</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the emergence of a large number of Synthetic Aperture Radar (SAR)
sensors and target datasets has made it possible to unify downstream tasks with
self-supervised learning techniques, which can pave the way for building the
foundation model in the SAR target recognition field. The major challenge of
self-supervised learning for SAR target recognition lies in the generalizable
representation learning in low data quality and noise.To address the
aforementioned problem, we propose a knowledge-guided predictive architecture
that uses local masked patches to predict the multiscale SAR feature
representations of unseen context. The core of the proposed architecture lies
in combining traditional SAR domain feature extraction with state-of-the-art
scalable self-supervised learning for accurate generalized feature
representations. The proposed framework is validated on various downstream
datasets (MSTAR, FUSAR-Ship, SAR-ACD and SSDD), and can bring consistent
performance improvement for SAR target recognition. The experimental results
strongly demonstrate the unified performance improvement of the self-supervised
learning technique for SAR target recognition across diverse targets, scenes
and sensors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuenan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongxiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09553">
<title>Prompt-based Distribution Alignment for Unsupervised Domain Adaptation. (arXiv:2312.09553v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09553</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, despite the unprecedented success of large pre-trained
visual-language models (VLMs) on a wide range of downstream tasks, the
real-world unsupervised domain adaptation (UDA) problem is still not well
explored. Therefore, in this paper, we first experimentally demonstrate that
the unsupervised-trained VLMs can significantly reduce the distribution
discrepancy between source and target domains, thereby improving the
performance of UDA. However, a major challenge for directly deploying such
models on downstream UDA tasks is prompt engineering, which requires aligning
the domain knowledge of source and target domains, since the performance of UDA
is severely influenced by a good domain-invariant representation. We further
propose a Prompt-based Distribution Alignment (PDA) method to incorporate the
domain knowledge into prompt learning. Specifically, PDA employs a two-branch
prompt-tuning paradigm, namely base branch and alignment branch. The base
branch focuses on integrating class-related representation into prompts,
ensuring discrimination among different classes. To further minimize domain
discrepancy, for the alignment branch, we construct feature banks for both the
source and target domains and propose image-guided feature tuning (IFT) to make
the input attend to feature banks, which effectively integrates self-enhanced
and cross-domain features into the model. In this way, these two branches can
be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct
extensive experiments on three benchmarks to demonstrate that our proposed PDA
achieves state-of-the-art performance. The code is available at
https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuanghao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wanqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siteng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_Z/0/1/0/all/0/1&quot;&gt;Zhirong Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10986">
<title>Long-Tailed 3D Detection via 2D Late Fusion. (arXiv:2312.10986v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10986</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles (AVs) must accurately detect objects from both common and
rare classes for safe navigation, motivating the problem of Long-Tailed 3D
Object Detection (LT3D). Contemporary LiDAR-based 3D detectors perform poorly
on rare classes (e.g., CenterPoint only achieves 5.1 AP on stroller) as it is
difficult to recognize objects from sparse LiDAR points alone. RGB images
provide visual evidence to help resolve such ambiguities, motivating the study
of RGB-LiDAR fusion. In this paper, we delve into a simple late-fusion
framework that ensembles independently trained RGB and LiDAR detectors. Unlike
recent end-to-end methods which require paired multi-modal training data, our
late-fusion approach can easily leverage large-scale uni-modal datasets,
significantly improving rare class detection. In particular, we examine three
critical components in this late-fusion framework from first principles,
including whether to train 2D or 3D RGB detectors, whether to match RGB and
LiDAR detections in 3D or the projected 2D image plane, and how to fuse matched
detections.Extensive experiments reveal that 2D RGB detectors achieve better
recognition accuracy than 3D RGB detectors, matching on the 2D image plane
mitigates depth estimation errors, and fusing scores probabilistically with
calibration leads to state-of-the-art LT3D performance. Our late-fusion
approach achieves 51.4 mAP on the established nuScenes LT3D benchmark,
improving over prior work by 5.9 mAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yechi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1&quot;&gt;Neehar Peri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shuoquan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wei Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shu Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12429">
<title>The Endoscapes Dataset for Surgical Scene Segmentation, Object Detection, and Critical View of Safety Assessment: Official Splits and Benchmark. (arXiv:2312.12429v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12429</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report provides a detailed overview of Endoscapes, a dataset
of laparoscopic cholecystectomy (LC) videos with highly intricate annotations
targeted at automated assessment of the Critical View of Safety (CVS).
Endoscapes comprises 201 LC videos with frames annotated sparsely but regularly
with segmentation masks, bounding boxes, and CVS assessment by three different
clinical experts. Altogether, there are 11090 frames annotated with CVS and
1933 frames annotated with tool and anatomy bounding boxes from the 201 videos,
as well as an additional 422 frames from 50 of the 201 videos annotated with
tool and anatomy segmentation masks. In this report, we provide detailed
dataset statistics (size, class distribution, dataset splits, etc.) and a
comprehensive performance benchmark for instance segmentation, object
detection, and CVS prediction. The dataset and model checkpoints are publically
available at https://github.com/CAMMA-public/Endoscapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Aditya Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alapatt_D/0/1/0/all/0/1&quot;&gt;Deepak Alapatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardazaryan_A/0/1/0/all/0/1&quot;&gt;Armine Vardazaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alain Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamoto_N/0/1/0/all/0/1&quot;&gt;Nariaki Okamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costamagna_G/0/1/0/all/0/1&quot;&gt;Guido Costamagna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marescaux_J/0/1/0/all/0/1&quot;&gt;Jacques Marescaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dallemagne_B/0/1/0/all/0/1&quot;&gt;Bernard Dallemagne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04345">
<title>RomniStereo: Recurrent Omnidirectional Stereo Matching. (arXiv:2401.04345v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04345</link>
<description rdf:parseType="Literal">&lt;p&gt;Omnidirectional stereo matching (OSM) is an essential and reliable means for
$360^{\circ}$ depth sensing. However, following earlier works on conventional
stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D
encoder-decoder block to regularize the cost volume, causing the whole system
complicated and sub-optimal results. Recently, the Recurrent All-pairs Field
Transforms (RAFT) based approach employs the recurrent update in 2D and has
efficiently improved image-matching tasks, ie, optical flow, and stereo
matching. To bridge the gap between OSM and RAFT, we mainly propose an opposite
adaptive weighting scheme to seamlessly transform the outputs of spherical
sweeping of OSM into the required inputs for the recurrent update, thus
creating a recurrent omnidirectional stereo matching (RomniStereo) algorithm.
Furthermore, we introduce two techniques, ie, grid embedding and adaptive
context feature generation, which also contribute to RomniStereo&apos;s performance.
Our best model improves the average MAE metric by 40.7\% over the previous SOTA
baseline across five datasets. When visualizing the results, our models
demonstrate clear advantages on both synthetic and realistic examples. The code
is available at \url{https://github.com/HalleyJiang/RomniStereo}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hualie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Minglang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenjie Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08115">
<title>No-Clean-Reference Image Super-Resolution: Application to Electron Microscopy. (arXiv:2401.08115v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08115</link>
<description rdf:parseType="Literal">&lt;p&gt;The inability to acquire clean high-resolution (HR) electron microscopy (EM)
images over a large brain tissue volume hampers many neuroscience studies. To
address this challenge, we propose a deep-learning-based image super-resolution
(SR) approach to computationally reconstruct clean HR 3D-EM with a large field
of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are
I) Investigating training with no-clean references for $\ell_2$ and $\ell_1$
loss functions; II) Introducing a novel network architecture, named EMSR, for
enhancing the resolution of LR EM images while reducing inherent noise; and,
III) Comparing different training strategies including using acquired LR and HR
image pairs, i.e., real pairs with no-clean references contaminated with real
corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR
and denoised HR pairs. Experiments with nine brain datasets showed that
training with real pairs can produce high-quality super-resolved results,
demonstrating the feasibility of training with non-clean references for both
loss functions. Additionally, comparable results were observed, both visually
and numerically, when employing denoised and noisy references for training.
Moreover, utilizing the network trained with synthetically generated LR images
from HR counterparts proved effective in yielding satisfactory SR results, even
in certain cases, outperforming training with real pairs. The proposed SR
network was compared quantitatively and qualitatively with several established
SR techniques, showcasing either the superiority or competitiveness of the
proposed method in mitigating noise while recovering fine details.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khateri_M/0/1/0/all/0/1&quot;&gt;Mohammad Khateri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahremani_M/0/1/0/all/0/1&quot;&gt;Morteza Ghahremani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sierra_A/0/1/0/all/0/1&quot;&gt;Alejandra Sierra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tohka_J/0/1/0/all/0/1&quot;&gt;Jussi Tohka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09603">
<title>Rethinking FID: Towards a Better Evaluation Metric for Image Generation. (arXiv:2401.09603v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09603</link>
<description rdf:parseType="Literal">&lt;p&gt;As with many machine learning problems, the progress of image generation
methods hinges on good evaluation metrics. One of the most popular is the
Frechet Inception Distance (FID). FID estimates the distance between a
distribution of Inception-v3 features of real images, and those of images
generated by the algorithm. We highlight important drawbacks of FID:
Inception&apos;s poor representation of the rich and varied content generated by
modern text-to-image models, incorrect normality assumptions, and poor sample
complexity. We call for a reevaluation of FID&apos;s use as the primary quality
metric for generated images. We empirically demonstrate that FID contradicts
human raters, it does not reflect gradual improvement of iterative
text-to-image models, it does not capture distortion levels, and that it
produces inconsistent results when varying the sample size. We also propose an
alternative new metric, CMMD, based on richer CLIP embeddings and the maximum
mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased
estimator that does not make any assumptions on the probability distribution of
the embeddings and is sample efficient. Through extensive experiments and
analysis, we demonstrate that FID-based evaluations of text-to-image models may
be unreliable, and that CMMD offers a more robust and reliable assessment of
image quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1&quot;&gt;Sadeep Jayasumana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1&quot;&gt;Srikumar Ramalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1&quot;&gt;Andreas Veit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1&quot;&gt;Daniel Glasner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11143">
<title>Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11143</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM&apos;s
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannides_G/0/1/0/all/0/1&quot;&gt;Georgios Ioannides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elkins_A/0/1/0/all/0/1&quot;&gt;Aaron Elkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11174">
<title>Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11174</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach enables a multi-level comprehension
of surgical activities, encompassing long-term tasks such as surgical phases
and steps recognition and short-term tasks including surgical instrument
segmentation and atomic visual actions detection. To exploit our proposed
benchmark, we introduce the Transformers for Actions, Phases, Steps, and
Instrument Segmentation (TAPIS) model, a general architecture that combines a
global video feature extractor with localized region proposals from an
instrument segmentation model to tackle the multi-granularity of our benchmark.
Through extensive experimentation, we demonstrate the impact of including
segmentation annotations in short-term recognition tasks, highlight the varying
granularity requirements of each task, and establish TAPIS&apos;s superiority over
previously proposed baselines and conventional CNN-based models. Additionally,
we validate the robustness of our method across multiple public benchmarks,
confirming the reliability and applicability of our dataset. This work
represents a significant step forward in Endoscopic Vision, offering a novel
and comprehensive framework for future research towards a holistic
understanding of surgical procedures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Ayobi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1&quot;&gt;Santiago Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_A/0/1/0/all/0/1&quot;&gt;Alejandra P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1&quot;&gt;Isabela Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Aparicio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dessevres_E/0/1/0/all/0/1&quot;&gt;Eug&amp;#xe9;nie Dessevres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pena_S/0/1/0/all/0/1&quot;&gt;Sebasti&amp;#xe1;n Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santander_J/0/1/0/all/0/1&quot;&gt;Jessica Santander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caicedo_J/0/1/0/all/0/1&quot;&gt;Juan Ignacio Caicedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11511">
<title>MobileARLoc: On-device Robust Absolute Localisation for Pervasive Markerless Mobile AR. (arXiv:2401.11511v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11511</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen significant improvement in absolute camera pose
estimation, paving the way for pervasive markerless Augmented Reality (AR).
However, accurate absolute pose estimation techniques are computation- and
storage-heavy, requiring computation offloading. As such, AR systems rely on
visual-inertial odometry (VIO) to track the device&apos;s relative pose between
requests to the server. However, VIO suffers from drift, requiring frequent
absolute repositioning. This paper introduces MobileARLoc, a new framework for
on-device large-scale markerless mobile AR that combines an absolute pose
regressor (APR) with a local VIO tracking system. Absolute pose regressors
(APRs) provide fast on-device pose estimation at the cost of reduced accuracy.
To address APR accuracy and reduce VIO drift, MobileARLoc creates a feedback
loop where VIO pose estimations refine the APR predictions. The VIO system
identifies reliable predictions of APR, which are then used to compensate for
the VIO drift. We comprehensively evaluate MobileARLoc through dataset
simulations. MobileARLoc halves the error compared to the underlying APR and
achieve fast (80\,ms) on-device inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changkun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yukun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braud_T/0/1/0/all/0/1&quot;&gt;Tristan Braud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12888">
<title>Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies. (arXiv:2401.12888v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12888</link>
<description rdf:parseType="Literal">&lt;p&gt;The aspiration of the next generation&apos;s autonomous driving (AD) technology
relies on the dedicated integration and interaction among intelligent
perception, prediction, planning, and low-level control. There has been a huge
bottleneck regarding the upper bound of autonomous driving algorithm
performance, a consensus from academia and industry believes that the key to
surmount the bottleneck lies in data-centric autonomous driving technology.
Recent advancement in AD simulation, closed-loop model training, and AD big
data engine have gained some valuable experience. However, there is a lack of
systematic knowledge and deep understanding regarding how to build efficient
data-centric AD technology for AD algorithm self-evolution and better AD big
data accumulation. To fill in the identified research gaps, this article will
closely focus on reviewing the state-of-the-art data-driven autonomous driving
technologies, with an emphasis on the comprehensive taxonomy of autonomous
driving datasets characterized by milestone generations, key features, data
acquisition settings, etc. Furthermore, we provide a systematic review of the
existing benchmark closed-loop AD big data pipelines from the industrial
frontier, including the procedure of closed-loop frameworks, key technologies,
and empirical studies. Finally, the future directions, potential applications,
limitations and concerns are discussed to arouse efforts from both academia and
industry for promoting the further development of autonomous driving. The
project repository is available at:
https://github.com/LincanLi98/Awesome-Data-Centric-Autonomous-Driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yijun Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13721">
<title>Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13721</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model
from a labeled source domain to an unlabeled target domain for regression
tasks. Recent successful works in UDAR mostly focus on subspace alignment,
involving the alignment of a selected subspace within the entire feature space.
This contrasts with the feature alignment methods used for classification,
which aim at aligning the entire feature space and have proven effective but
are less so in regression settings. Specifically, while classification aims to
identify separate clusters across the entire embedding dimension, regression
induces less structure in the data representation, necessitating additional
guidance for efficient alignment. In this paper, we propose an effective method
for UDAR by incorporating guidance from uncertainty. Our approach serves a dual
purpose: providing a measure of confidence in predictions and acting as a
regularization of the embedding space. Specifically, we leverage the Deep
Evidential Learning framework, which outputs both predictions and uncertainties
for each input sample. We propose aligning the parameters of higher-order
evidential distributions between the source and target domains using
traditional alignment methods at the feature or posterior level. Additionally,
we propose to augment the feature space representation by mixing source samples
with pseudo-labeled target samples based on label similarity. This cross-domain
mixing strategy produces more realistic samples than random mixing and
introduces higher uncertainty, facilitating further alignment. We demonstrate
the effectiveness of our approach on four benchmarks for UDAR, on which we
outperform existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejjar_I/0/1/0/all/0/1&quot;&gt;Ismail Nejjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frusque_G/0/1/0/all/0/1&quot;&gt;Gaetan Frusque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forest_F/0/1/0/all/0/1&quot;&gt;Florent Forest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14111">
<title>Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs. (arXiv:2401.14111v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.14111</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in generative models have sparked significant interest in
generating images while adhering to specific structural guidelines. Scene graph
to image generation is one such task of generating images which are consistent
with the given scene graph. However, the complexity of visual scenes poses a
challenge in accurately aligning objects based on specified relations within
the scene graph. Existing methods approach this task by first predicting a
scene layout and generating images from these layouts using adversarial
training. In this work, we introduce a novel approach to generate images from
scene graphs which eliminates the need of predicting intermediate layouts. We
leverage pre-trained text-to-image diffusion models and CLIP guidance to
translate graph knowledge into images. Towards this, we first pre-train our
graph encoder to align graph features with CLIP features of corresponding
images using a GAN based training. Further, we fuse the graph features with
CLIP embedding of object labels present in the given scene graph to create a
graph consistent CLIP guided conditioning signal. In the conditioning input,
object embeddings provide coarse structure of the image and graph features
provide structural alignment based on relationships among objects. Finally, we
fine tune a pre-trained diffusion model with the graph consistent conditioning
signal with reconstruction and CLIP alignment loss. Elaborate experiments
reveal that our method outperforms existing methods on standard benchmarks of
COCO-stuff and Visual Genome dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1&quot;&gt;Rameshwar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanyam_A/0/1/0/all/0/1&quot;&gt;A V Subramanyam&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>