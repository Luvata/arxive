<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-30T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.12672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14643" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.15089">
<title>Information Gained Subgroup Discovery in Datasets. (arXiv:2307.15089v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15089</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer is the leading cause of cancer death. More than 238,340 new cases
of lung cancer patients are expected in 2023, with an estimation of more than
127,070 deaths. Choosing the correct treatment is an important element to
enhance the probability of survival and to improve patient&apos;s quality of life.
Cancer treatments might provoke secondary effects. These toxicities cause
different health problems that impact the patient&apos;s quality of life. Hence,
reducing treatments toxicities while maintaining or improving their
effectivenes is an important goal that aims to be pursued from the clinical
perspective. On the other hand, clinical guidelines include general knowledge
about cancer treatment recommendations to assist clinicians. Although they
provide treatment recommendations based on cancer disease aspects and
individual patient features, a statistical analysis taking into account
treatment outcomes is not provided here. Therefore, the comparison between
clinical guidelines with treatment patterns found in clinical data, would allow
to validate the patterns found, as well as discovering alternative treatment
patterns. In this work, we present Information Gained Subgroup Discovery, a
Subgroup Discovery algorithm that aims to find most relevant patterns taking
into account Information gain and Odds ratio. Thus, we analyze a dataset
containing lung cancer patients information including patients&apos; data,
prescribed treatments and their outcomes. Obtained results are validated
through clinicians and compared with clinical guidelines. We conclude that this
new algorithm achieves highest acceptance of found patterns in this dataset,
while also improving indices of Subgroup Discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Bravo_D/0/1/0/all/0/1&quot;&gt;Daniel G&amp;#xf3;mez-Bravo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Aaron Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigueras_G/0/1/0/all/0/1&quot;&gt;Guillermo Vigueras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_B/0/1/0/all/0/1&quot;&gt;Bel&amp;#xe9;n R&amp;#xed;os&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Provencio_M/0/1/0/all/0/1&quot;&gt;Mariano Provencio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Gonzalez_A/0/1/0/all/0/1&quot;&gt;Alejandro Rodr&amp;#xed;guez-Gonz&amp;#xe1;lez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15090">
<title>Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15090</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reveal the selective rotation in the CNNs&apos; forward processing. It
elucidates the activation function as a discerning mechanism that unifies and
quantizes the rotational aspects of the input data. Experiments show how this
defined methodology reflects the progress network distinguish inputs based on
statistical indicators, which can be comprehended or analyzed by applying
structured mathematical tools. Our findings also unveil the consistency between
artificial neural networks and the human brain in their data processing
pattern.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Peixin Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15092">
<title>A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning. (arXiv:2307.15092v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.15092</link>
<description rdf:parseType="Literal">&lt;p&gt;Reservoir computing (RC), first applied to temporal signal processing, is a
recurrent neural network in which neurons are randomly connected. Once
initialized, the connection strengths remain unchanged. Such a simple structure
turns RC into a non-linear dynamical system that maps low-dimensional inputs
into a high-dimensional space. The model&apos;s rich dynamics, linear separability,
and memory capacity then enable a simple linear readout to generate adequate
responses for various applications. RC spans areas far beyond machine learning,
since it has been shown that the complex dynamics can be realized in various
physical hardware implementations and biological devices. This yields greater
flexibility and shorter computation time. Moreover, the neuronal responses
triggered by the model&apos;s dynamics shed light on understanding brain mechanisms
that also exploit similar dynamical processes. While the literature on RC is
vast and fragmented, here we conduct a unified review of RC&apos;s recent
developments from machine learning to physics, biology, and neuroscience. We
first review the early RC models, and then survey the state-of-the-art models
and their applications. We further introduce studies on modeling the brain&apos;s
mechanisms by RC. Finally, we offer new perspectives on RC development,
including reservoir design, coding frameworks unification, physical RC
implementations, and interaction between RC, cognitive neuroscience and
evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Heng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1&quot;&gt;Danilo Vasconcellos Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15095">
<title>Cortex Inspired Learning to Recover Damaged Signal Modality with ReD-SOM Model. (arXiv:2307.15095v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.15095</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in the fields of AI and cognitive sciences opens up new
challenges that were previously inaccessible to study. One of such modern tasks
is recovering lost data of one modality by using the data from another one. A
similar effect (called the McGurk Effect) has been found in the functioning of
the human brain. Observing this effect, one modality of information interferes
with another, changing its perception. In this paper, we propose a way to
simulate such an effect and use it to reconstruct lost data modalities by
combining Variational Auto-Encoders, Self-Organizing Maps, and Hebb connections
in a unified ReD-SOM (Reentering Deep Self-organizing Map) model. We are
inspired by human&apos;s capability to use different zones of the brain in different
modalities, in case of having a lack of information in one of the modalities.
This new approach not only improves the analysis of ambiguous data but also
restores the intended signal! The results obtained on the multimodal dataset
demonstrate an increase of quality of the signal reconstruction. The effect is
remarkable both visually and quantitatively, specifically in presence of a
significant degree of signal&apos;s distortion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Muliukov_A/0/1/0/all/0/1&quot;&gt;Artem Muliukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rodriguez_L/0/1/0/all/0/1&quot;&gt;Laurent Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Miramond_B/0/1/0/all/0/1&quot;&gt;Benoit Miramond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15164">
<title>VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15164</link>
<description rdf:parseType="Literal">&lt;p&gt;Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion
Classification from essays written in reaction to news articles. Emotion
detection from complex dialogues is challenging and often requires
context/domain understanding. Therefore in this research, we have focused on
developing deep learning (DL) models using the combination of word embedding
representations with tailored prepossessing strategies to capture the nuances
of emotions expressed. Our experiments used static and contextual embeddings
(individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and
Transformer based models. We occupied rank tenth in the emotion detection task
by scoring a Macro F1-Score of 0.2717, validating the efficacy of our
implemented approaches for small and imbalanced datasets with mixed categories
of target emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vivek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sushmita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1&quot;&gt;Prayag Tiwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15176">
<title>RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15176</link>
<description rdf:parseType="Literal">&lt;p&gt;Confounding is a significant obstacle to unbiased estimation of causal
effects from observational data. For settings with high-dimensional covariates
-- such as text data, genomics, or the behavioral social sciences --
researchers have proposed methods to adjust for confounding by adapting machine
learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In
this work, we build on a promising empirical evaluation strategy that
simplifies evaluation design and uses real data: subsampling randomized
controlled trials (RCTs) to create confounded observational datasets while
using the average causal effects from the RCTs as ground-truth. We contribute a
new sampling algorithm, which we call RCT rejection sampling, and provide
theoretical guarantees that causal identification holds in the observational
data to allow for valid comparisons to the ground-truth RCT. Using synthetic
data, we show our algorithm indeed results in low bias when oracle estimators
are evaluated on the confounded samples, which is not always the case for a
previously proposed algorithm. In addition to this identification result, we
highlight several finite data considerations for evaluation designers who plan
to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data
considerations with a novel, real-world RCT -- which we release publicly --
consisting of approximately 70k observations and text data as high-dimensional
covariates. Together, these contributions build towards a broader agenda of
improved empirical evaluation for causal estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1&quot;&gt;Katherine A. Keith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1&quot;&gt;Sergey Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1&quot;&gt;David Jurgens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1&quot;&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1&quot;&gt;Rohit Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15189">
<title>Med-Flamingo: a Multimodal Medical Few-shot Learner. (arXiv:2307.15189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15189</link>
<description rdf:parseType="Literal">&lt;p&gt;Medicine, by its nature, is a multifaceted domain that requires the synthesis
of information across various modalities. Medical generative vision-language
models (VLMs) make a first step in this direction and promise many exciting
clinical applications. However, existing models typically have to be fine-tuned
on sizeable down-stream datasets, which poses a significant limitation as in
many medical applications data is scarce, necessitating models that are capable
of learning from few examples in real-time. Here we propose Med-Flamingo, a
multimodal few-shot learner adapted to the medical domain. Based on
OpenFlamingo-9B, we continue pre-training on paired and interleaved medical
image-text data from publications and textbooks. Med-Flamingo unlocks few-shot
generative medical visual question answering (VQA) abilities, which we evaluate
on several datasets including a novel challenging open-ended VQA dataset of
visual USMLE-style problems. Furthermore, we conduct the first human evaluation
for generative medical VQA where physicians review the problems and blinded
generations in an interactive app. Med-Flamingo improves performance in
generative medical VQA by up to 20\% in clinician&apos;s rating and firstly enables
multimodal medical few-shot adaptations, such as rationale generation. We
release our model, code, and evaluation app under
https://github.com/snap-stanford/med-flamingo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1&quot;&gt;Michael Moor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shirley Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1&quot;&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakka_C/0/1/0/all/0/1&quot;&gt;Cyril Zakka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalmia_Y/0/1/0/all/0/1&quot;&gt;Yash Dalmia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reis_E/0/1/0/all/0/1&quot;&gt;Eduardo Pontes Reis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15193">
<title>Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.15193</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and
Procurement Auctions, which all involve the auctioning of homogeneous multiple
units, we consider the problem of learning how to bid in repeated multi-unit
pay-as-bid auctions. In each of these auctions, a large number of (identical)
items are to be allocated to the largest submitted bids, where the price of
each of the winning bids is equal to the bid itself. The problem of learning
how to bid in pay-as-bid auctions is challenging due to the combinatorial
nature of the action space. We overcome this challenge by focusing on the
offline setting, where the bidder optimizes their vector of bids while only
having access to the past submitted bids by other bidders. We show that the
optimal solution to the offline problem can be obtained using a polynomial time
dynamic programming (DP) scheme. We leverage the structure of the DP scheme to
design online learning algorithms with polynomial time and space complexity
under full information and bandit feedback settings. We achieve an upper bound
on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log
|\mathcal{B}|})$ respectively, where $M$ is the number of units demanded by the
bidder, $T$ is the total number of auctions, and $|\mathcal{B}|$ is the size of
the discretized bid space. We accompany these results with a regret lower
bound, which match the linear dependency in $M$. Our numerical results suggest
that when all agents behave according to our proposed no regret learning
algorithms, the resulting market dynamics mainly converge to a welfare
maximizing equilibrium where bidders submit uniform bids. Lastly, our
experiments demonstrate that the pay-as-bid auction consistently generates
significantly higher revenue compared to its popular alternative, the uniform
price auction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galgana_R/0/1/0/all/0/1&quot;&gt;Rigel Galgana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golrezaei_N/0/1/0/all/0/1&quot;&gt;Negin Golrezaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15198">
<title>One-shot Joint Extraction, Registration and Segmentation of Neuroimaging Data. (arXiv:2307.15198v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15198</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain extraction, registration and segmentation are indispensable
preprocessing steps in neuroimaging studies. The aim is to extract the brain
from raw imaging scans (i.e., extraction step), align it with a target brain
image (i.e., registration step) and label the anatomical brain regions (i.e.,
segmentation step). Conventional studies typically focus on developing separate
methods for the extraction, registration and segmentation tasks in a supervised
setting. The performance of these methods is largely contingent on the quantity
of training samples and the extent of visual inspections carried out by experts
for error correction. Nevertheless, collecting voxel-level labels and
performing manual quality control on high-dimensional neuroimages (e.g., 3D
MRI) are expensive and time-consuming in many medical studies. In this paper,
we study the problem of one-shot joint extraction, registration and
segmentation in neuroimaging data, which exploits only one labeled template
image (a.k.a. atlas) and a few unlabeled raw images for training. We propose a
unified end-to-end framework, called JERS, to jointly optimize the extraction,
registration and segmentation tasks, allowing feedback among them.
Specifically, we use a group of extraction, registration and segmentation
modules to learn the extraction mask, transformation and segmentation mask,
where modules are interconnected and mutually reinforced by self-supervision.
Empirical results on real-world datasets demonstrate that our proposed method
performs exceptionally in the extraction, registration and segmentation tasks.
Our code and data can be found at https://github.com/Anonymous4545/JERS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhentian Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lifang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangnan Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15199">
<title>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15199</link>
<description rdf:parseType="Literal">&lt;p&gt;In a joint vision-language space, a text feature (e.g., from &quot;a photo of a
dog&quot;) could effectively represent its relevant image features (e.g., from dog
photos). Inspired by this, we propose PromptStyler which simulates various
distribution shifts in the joint space by synthesizing diverse styles via
prompts without using any images to deal with source-free domain
generalization. Our method learns to generate a variety of style features (from
&quot;a S* style of a&quot;) via learnable style word vectors for pseudo-words S*. To
ensure that learned styles do not distort content information, we force
style-content features (from &quot;a S* style of a [class]&quot;) to be located nearby
their corresponding content features (from &quot;[class]&quot;) in the joint
vision-language space. After learning style word vectors, we train a linear
classifier using synthesized style-content features. PromptStyler achieves the
state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not
require any images and takes just ~30 minutes for training using a single GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Junhyeong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1&quot;&gt;Gilhyun Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hunmin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15217">
<title>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15217</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback (RLHF) is a technique for training
AI systems to align with human goals. RLHF has emerged as the central method
used to finetune state-of-the-art large language models (LLMs). Despite this
popularity, there has been relatively little public work systematizing its
flaws. In this paper, we (1) survey open problems and fundamental limitations
of RLHF and related methods; (2) overview techniques to understand, improve,
and complement RLHF in practice; and (3) propose auditing and disclosure
standards to improve societal oversight of RLHF systems. Our work emphasizes
the limitations of RLHF and highlights the importance of a multi-faceted
approach to the development of safer AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_X/0/1/0/all/0/1&quot;&gt;Xander Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Claudia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_T/0/1/0/all/0/1&quot;&gt;Thomas Krendl Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Scheurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1&quot;&gt;Javier Rando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_R/0/1/0/all/0/1&quot;&gt;Rachel Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1&quot;&gt;Tomasz Korbak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1&quot;&gt;David Lindner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freire_P/0/1/0/all/0/1&quot;&gt;Pedro Freire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_S/0/1/0/all/0/1&quot;&gt;Samuel Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segerie_C/0/1/0/all/0/1&quot;&gt;Charbel-Rapha&amp;#xeb;l Segerie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carroll_M/0/1/0/all/0/1&quot;&gt;Micah Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Andi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christoffersen_P/0/1/0/all/0/1&quot;&gt;Phillip Christoffersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damani_M/0/1/0/all/0/1&quot;&gt;Mehul Damani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slocum_S/0/1/0/all/0/1&quot;&gt;Stewart Slocum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_U/0/1/0/all/0/1&quot;&gt;Usman Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siththaranjan_A/0/1/0/all/0/1&quot;&gt;Anand Siththaranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1&quot;&gt;Max Nadeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaud_E/0/1/0/all/0/1&quot;&gt;Eric J. Michaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfau_J/0/1/0/all/0/1&quot;&gt;Jacob Pfau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krasheninnikov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Krasheninnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langosco_L/0/1/0/all/0/1&quot;&gt;Lauro Langosco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1&quot;&gt;Peter Hase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1&quot;&gt;Erdem B&amp;#x131;y&amp;#x131;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1&quot;&gt;Anca Dragan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15218">
<title>Reachability Poorman Discrete-Bidding Games. (arXiv:2307.15218v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.15218</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider {\em bidding games}, a class of two-player zero-sum {\em graph
games}. The game proceeds as follows. Both players have bounded budgets. A
token is placed on a vertex of a graph, in each turn the players simultaneously
submit bids, and the higher bidder moves the token, where we break bidding ties
in favor of Player 1. Player 1 wins the game iff the token visits a designated
target vertex. We consider, for the first time, {\em poorman discrete-bidding}
in which the granularity of the bids is restricted and the higher bid is paid
to the bank. Previous work either did not impose granularity restrictions or
considered {\em Richman} bidding (bids are paid to the opponent). While the
latter mechanisms are technically more accessible, the former is more appealing
from a practical standpoint. Our study focuses on {\em threshold budgets},
which is the necessary and sufficient initial budget required for Player 1 to
ensure winning against a given Player 2 budget. We first show existence of
thresholds. In DAGs, we show that threshold budgets can be approximated with
error bounds by thresholds under continuous-bidding and that they exhibit a
periodic behavior. We identify closed-form solutions in special cases. We
implement and experiment with an algorithm to find threshold budgets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avni_G/0/1/0/all/0/1&quot;&gt;Guy Avni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meggendorfer_T/0/1/0/all/0/1&quot;&gt;Tobias Meggendorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadhukhan_S/0/1/0/all/0/1&quot;&gt;Suman Sadhukhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tkadlec_J/0/1/0/all/0/1&quot;&gt;Josef Tkadlec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zikelic_%7B/0/1/0/all/0/1&quot;&gt;&amp;#x110;or&amp;#x111;e &amp;#x17d;ikeli&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15220">
<title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15220</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in surgical computer vision applications have been driven
by fully-supervised methods, primarily using only visual data. These methods
rely on manually annotated surgical videos to predict a fixed set of object
categories, limiting their generalizability to unseen surgical procedures and
downstream tasks. In this work, we put forward the idea that the surgical video
lectures available through open surgical e-learning platforms can provide
effective supervisory signals for multi-modal representation learning without
relying on manual annotations. We address the surgery-specific linguistic
challenges present in surgical video lectures by employing multiple
complementary automatic speech recognition systems to generate text
transcriptions. We then present a novel method, SurgVLP - Surgical Vision
Language Pre-training, for multi-modal representation learning. SurgVLP
constructs a new contrastive learning objective to align video clip embeddings
with the corresponding multiple text embeddings by bringing them together
within a joint latent space. To effectively show the representation capability
of the learned joint latent space, we introduce several vision-and-language
tasks for surgery, such as text-based video retrieval, temporal activity
grounding, and video captioning, as benchmarks for evaluation. We further
demonstrate that without using any labeled ground truth, our approach can be
employed for traditional vision-only surgical downstream tasks, such as
surgical tool, phase, and triplet recognition. The code will be made available
at https://github.com/CAMMA-public/SurgVLP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1&quot;&gt;Vinkle Srivastav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1&quot;&gt;Joel Lavanchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15244">
<title>BOURNE: Bootstrapped Self-supervised Learning Framework for Unified Graph Anomaly Detection. (arXiv:2307.15244v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.15244</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph anomaly detection (GAD) has gained increasing attention in recent years
due to its critical application in a wide range of domains, such as social
networks, financial risk management, and traffic analysis. Existing GAD methods
can be categorized into node and edge anomaly detection models based on the
type of graph objects being detected. However, these methods typically treat
node and edge anomalies as separate tasks, overlooking their associations and
frequent co-occurrences in real-world graphs. As a result, they fail to
leverage the complementary information provided by node and edge anomalies for
mutual detection. Additionally, state-of-the-art GAD methods, such as CoLA and
SL-GAD, heavily rely on negative pair sampling in contrastive learning, which
incurs high computational costs, hindering their scalability to large graphs.
To address these limitations, we propose a novel unified graph anomaly
detection framework based on bootstrapped self-supervised learning (named
BOURNE). We extract a subgraph (graph view) centered on each target node as
node context and transform it into a dual hypergraph (hypergraph view) as edge
context. These views are encoded using graph and hypergraph neural networks to
capture the representations of nodes, edges, and their associated contexts. By
swapping the context embeddings between nodes and edges and measuring the
agreement in the embedding space, we enable the mutual detection of node and
edge anomalies. Furthermore, we adopt a bootstrapped training strategy that
eliminates the need for negative sampling, enabling BOURNE to handle large
graphs efficiently. Extensive experiments conducted on six benchmark datasets
demonstrate the superior effectiveness and efficiency of BOURNE in detecting
both node and edge anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1&quot;&gt;Mengting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1&quot;&gt;Xuequn Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jieming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15245">
<title>A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design. (arXiv:2307.15245v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15245</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has been an area of active research in recent years.
There have been numerous studies in FL to make it more successful in the
presence of data heterogeneity. However, despite the existence of many
publications, the state of progress in the field is unknown. Many of the works
use inconsistent experimental settings and there are no comprehensive studies
on the effect of FL-specific experimental variables on the results and
practical insights for a more comparable and consistent FL experimental setup.
Furthermore, the existence of several benchmarks and confounding variables has
further complicated the issue of inconsistency and ambiguity. In this work, we
present the first comprehensive study on the effect of FL-specific experimental
variables in relation to each other and performance results, bringing several
insights and recommendations for designing a meaningful and well-incentivized
FL experimental setup. We further aid the community by releasing FedZoo-Bench,
an open-source library based on PyTorch with pre-implementation of 22
state-of-the-art methods, and a broad set of standardized and customizable
features available at https://github.com/MMorafah/FedZoo-Bench. We also provide
a comprehensive comparison of several state-of-the-art (SOTA) methods to better
understand the current state of the field and existing limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morafah_M/0/1/0/all/0/1&quot;&gt;Mahdi Morafah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weijia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15254">
<title>Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification. (arXiv:2307.15254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15254</link>
<description rdf:parseType="Literal">&lt;p&gt;The whole slide image (WSI) classification is often formulated as a multiple
instance learning (MIL) problem. Since the positive tissue is only a small
fraction of the gigapixel WSI,existing MIL methods intuitively focus on
identifying salient instances via attention mechanisms. However, this leads to
a bias towards easy-to-classify instances while neglecting hard-to-classify
instances.Some literature has revealed that hard examples are beneficial for
modeling a discriminative boundary accurately.By applying such an idea at the
instance level,we elaborate a novel MIL framework with masked hard instance
mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a
consistency constraint to explore the potential hard instances. With several
instance masking strategies based on attention scores, MHIM-MIL employs a
momentum teacher to implicitly mine hard instances for training the student
model, which can be any attention-based MIL model.This counter-intuitive
strategy essentially enables the student to learn a better discriminating
boundary.Moreover, the student is used to update the teacher with an
exponential moving average (EMA), which in turn identifies new hard instances
for subsequent training iterations and stabilizes the optimization.Experimental
results on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that
MHIM-MIL outperforms other latest methods in terms of performance and training
cost. The code is available at:https://github.com/DearCaat/MHIM-MIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wenhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengtao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15293">
<title>WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories. (arXiv:2307.15293v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15293</link>
<description rdf:parseType="Literal">&lt;p&gt;Our research focuses on solving the zero-shot text classification problem in
NLP, with a particular emphasis on innovative self-training strategies. To
achieve this objective, we propose a novel self-training strategy that uses
labels rather than text for training, significantly reducing the model&apos;s
training time. Specifically, we use categories from Wikipedia as our training
set and leverage the SBERT pre-trained model to establish positive correlations
between pairs of categories within the same text, facilitating associative
training. For new test datasets, we have improved the original self-training
approach, eliminating the need for prior training and testing data from each
target dataset. Instead, we adopt Wikipedia as a unified training dataset to
better approximate the zero-shot scenario. This modification allows for rapid
fine-tuning and inference across different datasets, greatly reducing the time
required for self-training. Our experimental results demonstrate that this
method can adapt the model to the target dataset within minutes. Compared to
other BERT-based transformer models, our approach significantly reduces the
amount of training data by training only on labels, not the actual text, and
greatly improves training efficiency by utilizing a unified training set.
Additionally, our method achieves state-of-the-art results on both the Yahoo
Topic and AG News datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1&quot;&gt;Te-Yu Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yu-Meng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chia-Wen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiu-Xia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jyh-Shing Roger Jang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15316">
<title>Efficient Multiuser AI Downloading via Reusable Knowledge Broadcasting. (arXiv:2307.15316v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.15316</link>
<description rdf:parseType="Literal">&lt;p&gt;For the 6G mobile networks, in-situ model downloading has emerged as an
important use case to enable real-time adaptive artificial intelligence on edge
devices. However, the simultaneous downloading of diverse and high-dimensional
models to multiple devices over wireless links presents a significant
communication bottleneck. To overcome the bottleneck, we propose the framework
of model broadcasting and assembling (MBA), which represents the first attempt
on leveraging reusable knowledge, referring to shared parameters among tasks,
to enable parameter broadcasting to reduce communication overhead. The MBA
framework comprises two key components. The first, the MBA protocol, defines
the system operations including parameter selection from a model library, power
control for broadcasting, and model assembling at devices. The second component
is the joint design of parameter-selection-and-power-control (PS-PC), which
provides guarantees on devices&apos; model performance and minimizes the downloading
latency. The corresponding optimization problem is simplified by decomposition
into the sequential PS and PC sub-problems without compromising its optimality.
The PS sub-problem is solved efficiently by designing two efficient algorithms.
On one hand, the low-complexity algorithm of greedy parameter selection
features the construction of candidate model sets and a selection metric, both
of which are designed under the criterion of maximum reusable knowledge among
tasks. On the other hand, the optimal tree-search algorithm gains its
efficiency via the proposed construction of a compact binary tree pruned using
model architecture constraints and an intelligent branch-and-bound search.
Given optimal PS, the optimal PC policy is derived in closed form. Extensive
experiments demonstrate the substantial reduction in downloading latency
achieved by the proposed MBA compared to traditional model downloading.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Qunsong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaibin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15317">
<title>DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall&apos;s Rank Correlation. (arXiv:2307.15317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15317</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot learning aims to adapt models trained on the base dataset to novel
tasks where the categories are not seen by the model before. This often leads
to a relatively uniform distribution of feature values across channels on novel
classes, posing challenges in determining channel importance for novel tasks.
Standard few-shot learning methods employ geometric similarity metrics such as
cosine similarity and negative Euclidean distance to gauge the semantic
relatedness between two features. However, features with high geometric
similarities may carry distinct semantics, especially in the context of
few-shot learning. In this paper, we demonstrate that the importance ranking of
feature channels is a more reliable indicator for few-shot learning than
geometric similarity metrics. We observe that replacing the geometric
similarity metric with Kendall&apos;s rank correlation only during inference is able
to improve the performance of few-shot learning across a wide range of datasets
with different domains. Furthermore, we propose a carefully designed
differentiable loss for meta-training to address the non-differentiability
issue of Kendall&apos;s rank correlation. Extensive experiments demonstrate that the
proposed rank-correlation-based approach substantially enhances few-shot
learning performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15320">
<title>Robust Visual Sim-to-Real Transfer for Robotic Manipulation. (arXiv:2307.15320v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.15320</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning visuomotor policies in simulation is much safer and cheaper than in
the real world. However, due to discrepancies between the simulated and real
data, simulator-trained policies often fail when transferred to real robots.
One common approach to bridge the visual sim-to-real domain gap is domain
randomization (DR). While previous work mainly evaluates DR for disembodied
tasks, such as pose estimation and object detection, here we systematically
explore visual domain randomization methods and benchmark them on a rich set of
challenging robotic manipulation tasks. In particular, we propose an off-line
proxy task of cube localization to select DR parameters for texture
randomization, lighting randomization, variations of object colors and camera
parameters. Notably, we demonstrate that DR parameters have similar impact on
our off-line proxy task and on-line policies. We, hence, use off-line optimized
DR parameters to train visuomotor policies in simulation and directly apply
such policies to a real robot. Our approach achieves 93% success rate on
average when tested on a diverse set of challenging manipulation tasks.
Moreover, we evaluate the robustness of policies to visual variations in real
scenes and show that our simulator-trained policies outperform policies learned
using real but limited data. Code, simulation environment, real robot datasets
and trained models are available at
https://www.di.ens.fr/willow/research/robust_s2r/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1&quot;&gt;Ricardo Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1&quot;&gt;Robin Strudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shizhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arlaud_E/0/1/0/all/0/1&quot;&gt;Etienne Arlaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1&quot;&gt;Ivan Laptev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15331">
<title>Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15331</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents two self-contained tutorials on stance detection in
Twitter data using BERT fine-tuning and prompting large language models (LLMs).
The first tutorial explains BERT architecture and tokenization, guiding users
through training, tuning, and evaluating standard and domain-specific BERT
models with HuggingFace transformers. The second focuses on constructing
prompts and few-shot examples to elicit stances from ChatGPT and open-source
FLAN-T5 without fine-tuning. Various prompting strategies are implemented and
evaluated using confusion matrices and macro F1 scores. The tutorials provide
code, visualizations, and insights revealing the strengths of few-shot ChatGPT
and FLAN-T5 which outperform fine-tuned BERTs. By covering both model
fine-tuning and prompting-based techniques in an accessible, hands-on manner,
these tutorials enable learners to gain applied experience with cutting-edge
methods for stance detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yun-Shiuan Chuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15337">
<title>Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15337</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims at decreasing the end-to-end generation latency of large
language models (LLMs). One of the major causes of the high generation latency
is the sequential decoding approach adopted by almost all state-of-the-art
LLMs. In this work, motivated by the thinking and writing process of humans, we
propose &quot;Skeleton-of-Thought&quot; (SoT), which guides LLMs to first generate the
skeleton of the answer, and then conducts parallel API calls or batched
decoding to complete the contents of each skeleton point in parallel. Not only
does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs),
but it can also potentially improve the answer quality on several question
categories in terms of diversity and relevance. SoT is an initial attempt at
data-centric optimization for efficiency, and reveal the potential of pushing
LLMs to think more like a human for answer quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xuefei Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huazhong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15343">
<title>Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15343</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper focuses on the challenges posed by hallucinations in
large language models (LLMs), particularly in the context of the medical
domain. Hallucination, wherein these models generate plausible yet unverified
or incorrect information, can have serious consequences in healthcare
applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain
Hallucination Test), designed specifically to evaluate and reduce
hallucinations. Med-HALT provides a diverse multinational dataset derived from
medical examinations across various countries and includes multiple innovative
testing modalities. Med-HALT includes two categories of tests reasoning and
memory-based hallucination tests, designed to assess LLMs&apos;s problem-solving and
information retrieval abilities.
&lt;/p&gt;
&lt;p&gt;Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2,
MPT, and Falcon, revealing significant differences in their performance. The
paper provides detailed insights into the dataset, promoting transparency and
reproducibility. Through this work, we aim to contribute to the development of
safer and more reliable language models in healthcare. Our benchmark can be
found at medhalt.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umapathi_L/0/1/0/all/0/1&quot;&gt;Logesh Kumar Umapathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Ankit Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1&quot;&gt;Malaikannan Sankarasubbu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15361">
<title>Confident Feature Ranking. (arXiv:2307.15361v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.15361</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretation of feature importance values often relies on the relative
order of the features rather than on the value itself, referred to as ranking.
However, the order may be unstable due to the small sample sizes used in
calculating the importance values. We propose that post-hoc importance methods
produce a ranking and simultaneous confident intervals for the rankings. Based
on pairwise comparisons of the feature importance values, our method is
guaranteed to include the ``true&apos;&apos; (infinite sample) ranking with high
probability and allows for selecting top-k sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Neuhof_B/0/1/0/all/0/1&quot;&gt;Bitya Neuhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Benjamini_Y/0/1/0/all/0/1&quot;&gt;Yuval Benjamini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15377">
<title>Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning. (arXiv:2307.15377v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15377</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have proven to be effective in processing and
learning from graph-structured data. However, previous works mainly focused on
understanding single graph inputs while many real-world applications require
pair-wise analysis for graph-structured data (e.g., scene graph matching, code
searching, and drug-drug interaction prediction). To this end, recent works
have shifted their focus to learning the interaction between pairs of graphs.
Despite their improved performance, these works were still limited in that the
interactions were considered at the node-level, resulting in high computational
costs and suboptimal performance. To address this issue, we propose a novel and
efficient graph-level approach for extracting interaction representations using
co-attention in graph pooling. Our method, Co-Attention Graph Pooling
(CAGPool), exhibits competitive performance relative to existing methods in
both classification and regression tasks using real-world datasets, while
maintaining lower computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junhyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1&quot;&gt;Minji Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15413">
<title>Improving Social Media Popularity Prediction with Multiple Post Dependencies. (arXiv:2307.15413v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2307.15413</link>
<description rdf:parseType="Literal">&lt;p&gt;Social Media Popularity Prediction has drawn a lot of attention because of
its profound impact on many different applications, such as recommendation
systems and multimedia advertising. Despite recent efforts to leverage the
content of social media posts to improve prediction accuracy, many existing
models fail to fully exploit the multiple dependencies between posts, which are
important to comprehensively extract content information from posts. To tackle
this problem, we propose a novel prediction framework named Dependency-aware
Sequence Network (DSN) that exploits both intra- and inter-post dependencies.
For intra-post dependency, DSN adopts a multimodal feature extractor with an
efficient fine-tuning strategy to obtain task-specific representations from
images and textual information of posts. For inter-post dependency, DSN uses a
hierarchical information propagation method to learn category representations
that could better describe the difference between posts. DSN also exploits
recurrent networks with a series of gating layers for more flexible local
temporal processing abilities and multi-head attention for long-term
dependencies. The experimental results on the Social Media Popularity Dataset
demonstrate the superiority of our method compared to existing state-of-the-art
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yong Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15425">
<title>A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI. (arXiv:2307.15425v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15425</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the comparative effectiveness of a specialized compiled
language model and a general-purpose model like OpenAI&apos;s GPT-3.5 in detecting
SDGs within text data. It presents a critical review of Large Language Models
(LLMs), addressing challenges related to bias and sensitivity. The necessity of
specialized training for precise, unbiased analysis is underlined. A case study
using a company descriptions dataset offers insight into the differences
between the GPT-3.5 and the specialized SDG detection model. While GPT-3.5
boasts broader coverage, it may identify SDGs with limited relevance to the
companies&apos; activities. In contrast, the specialized model zeroes in on highly
pertinent SDGs. The importance of thoughtful model selection is emphasized,
taking into account task requirements, cost, complexity, and transparency.
Despite the versatility of LLMs, the use of specialized models is suggested for
tasks demanding precision and accuracy. The study concludes by encouraging
further research to find a balance between the capabilities of LLMs and the
need for domain-specific expertise and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajikhani_A/0/1/0/all/0/1&quot;&gt;Arash Hajikhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cole_C/0/1/0/all/0/1&quot;&gt;Carolyn Cole&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15429">
<title>Improvable Gap Balancing for Multi-Task Learning. (arXiv:2307.15429v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15429</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-task learning (MTL), gradient balancing has recently attracted more
research interest than loss balancing since it often leads to better
performance. However, loss balancing is much more efficient than gradient
balancing, and thus it is still worth further exploration in MTL. Note that
prior studies typically ignore that there exist varying improvable gaps across
multiple tasks, where the improvable gap per task is defined as the distance
between the current training progress and desired final training progress.
Therefore, after loss balancing, the performance imbalance still arises in many
cases. In this paper, following the loss balancing framework, we propose two
novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple
heuristic, and the other (for the first time) deploys deep reinforcement
learning for MTL. Particularly, instead of directly balancing the losses in
MTL, both algorithms choose to dynamically assign task weights for improvable
gap balancing. Moreover, we combine IGB and gradient balancing to show the
complementarity between the two types of algorithms. Extensive experiments on
two benchmark datasets demonstrate that our IGB algorithms lead to the best
results in MTL via loss balancing and achieve further improvements when
combined with gradient balancing. Code is available at
https://github.com/YanqiDai/IGB4MTL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yanqi Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_N/0/1/0/all/0/1&quot;&gt;Nanyi Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiwu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15439">
<title>Optimal Alignment of Temporal Knowledge Bases. (arXiv:2307.15439v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2307.15439</link>
<description rdf:parseType="Literal">&lt;p&gt;Answering temporal CQs over temporalized Description Logic knowledge bases
(TKB) is a main technique to realize ontology-based situation recognition. In
case the collected data in such a knowledge base is inaccurate, important query
answers can be missed. In this paper we introduce the TKB Alignment problem,
which computes a variant of the TKB that minimally changes the TKB, but entails
the given temporal CQ and is in that sense (cost-)optimal. We investigate this
problem for ALC TKBs and conjunctive queries with LTL operators and devise a
solution technique to compute (cost-optimal) alignments of TKBs that extends
techniques for the alignment problem for propositional LTL over finite traces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Gil_O/0/1/0/all/0/1&quot;&gt;Oliver Fernandez-Gil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrizi_F/0/1/0/all/0/1&quot;&gt;Fabio Patrizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perelli_G/0/1/0/all/0/1&quot;&gt;Giuseppe Perelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turhan_A/0/1/0/all/0/1&quot;&gt;Anni-Yasmin Turhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15451">
<title>DELPHIC: Practical DEL Planning via Possibilities (Extended Version). (arXiv:2307.15451v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15451</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Epistemic Logic (DEL) provides a framework for epistemic planning
that is capable of representing non-deterministic actions, partial
observability, higher-order knowledge and both factual and epistemic change.
The high expressivity of DEL challenges existing epistemic planners, which
typically can handle only restricted fragments of the whole framework. The goal
of this work is to push the envelop of practical DEL planning, ultimately
aiming for epistemic planners to be able to deal with the full range of
features offered by DEL. Towards this goal, we question the traditional
semantics of DEL, defined in terms on Kripke models. In particular, we propose
an equivalent semantics defined using, as main building block, so-called
possibilities: non well-founded objects representing both factual properties of
the world, and what agents consider to be possible. We call the resulting
framework DELPHIC. We argue that DELPHIC indeed provides a more compact
representation of epistemic states. To substantiate this claim, we implement
both approaches in ASP and we set up an experimental evaluation to compare
DELPHIC with the traditional, Kripke-based approach. The evaluation confirms
that DELPHIC outperforms the traditional approach in space and time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burigana_A/0/1/0/all/0/1&quot;&gt;Alessandro Burigana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felli_P/0/1/0/all/0/1&quot;&gt;Paolo Felli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15453">
<title>From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15453</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents the main characteristics and a preliminary implementation
of a novel computational framework named CompLog. Inspired by probabilistic
programming systems like ProbLog, CompLog builds upon the inferential
mechanisms proposed by Simplicity Theory, relying on the computation of two
Kolmogorov complexities (here implemented as min-path searches via ASP
programs) rather than probabilistic inference. The proposed system enables
users to compute ex-post and ex-ante measures of unexpectedness of a certain
situation, mapping respectively to posterior and prior subjective
probabilities. The computation is based on the specification of world and
mental models by means of causal and descriptive relations between predicates
weighted by complexity. The paper illustrates a few examples of application:
generating relevant descriptions, and providing alternative approaches to
disjunction and to negation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sileno_G/0/1/0/all/0/1&quot;&gt;Giovanni Sileno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dessalles_J/0/1/0/all/0/1&quot;&gt;Jean-Louis Dessalles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15456">
<title>Worrisome Properties of Neural Network Controllers and Their Symbolic Representations. (arXiv:2307.15456v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15456</link>
<description rdf:parseType="Literal">&lt;p&gt;We raise concerns about controllers&apos; robustness in simple reinforcement
learning benchmark problems. We focus on neural network controllers and their
low neuron and symbolic abstractions. A typical controller reaching high mean
return values still generates an abundance of persistent low-return solutions,
which is a highly undesirable property, easily exploitable by an adversary. We
find that the simpler controllers admit more persistent bad solutions. We
provide an algorithm for a systematic robustness study and prove existence of
persistent solutions and, in some cases, periodic orbits, using a
computer-assisted proof methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cyranka_J/0/1/0/all/0/1&quot;&gt;Jacek Cyranka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1&quot;&gt;Kevin E M Church&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lessard_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Lessard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15475">
<title>FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines. (arXiv:2307.15475v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.15475</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though machine learning (ML) pipelines affect an increasing array of
stakeholders, there is little work on how input from stakeholders is recorded
and incorporated. We propose FeedbackLogs, addenda to existing documentation of
ML pipelines, to track the input of multiple stakeholders. Each log records
important details about the feedback collection process, the feedback itself,
and how the feedback is used to update the ML pipeline. In this paper, we
introduce and formalise a process for collecting a FeedbackLog. We also provide
concrete use cases where FeedbackLogs can be employed as evidence for
algorithmic auditing and as a tool to record updates based on stakeholder
feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_M/0/1/0/all/0/1&quot;&gt;Matthew Barker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallina_E/0/1/0/all/0/1&quot;&gt;Emma Kallina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_D/0/1/0/all/0/1&quot;&gt;Dhananjay Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1&quot;&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casovan_A/0/1/0/all/0/1&quot;&gt;Ashley Casovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1&quot;&gt;Ameet Talwalkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1&quot;&gt;Valerie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1&quot;&gt;Umang Bhatt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15480">
<title>Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras. (arXiv:2307.15480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15480</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper compares and explores the performance of both mobile device camera
and laptop camera as convenient tool for capturing images for non-invasive
detection of Diabetes Mellitus (DM) using facial block texture features.
Participants within age bracket 20 to 79 years old were chosen for the dataset.
12mp and 7mp mobile cameras, and a laptop camera were used to take the photo
under normal lighting condition. Extracted facial blocks were classified using
k-Nearest Neighbors (k-NN) and Support Vector Machine (SVM). 100 images were
captured, preprocessed, filtered using Gabor, and iterated. Performance of the
system was measured in terms of accuracy, specificity, and sensitivity. Best
performance of 96.7% accuracy, 100% sensitivity, and 93% specificity were
achieved from 12mp back camera using SVM with 100 images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_C/0/1/0/all/0/1&quot;&gt;Christina A. Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_P/0/1/0/all/0/1&quot;&gt;Patricia Angela R. Abu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyes_R/0/1/0/all/0/1&quot;&gt;Rosula SJ. Reyes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15484">
<title>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.15484</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been a growing interest in text-to-speech (TTS) methods
that can be trained with minimal supervision by combining two types of discrete
speech representations and using two sequence-to-sequence tasks to decouple
TTS. To address the challenges associated with high dimensionality and waveform
distortion in discrete representations, we propose Diff-LM-Speech, which models
semantic embeddings into mel-spectrogram based on diffusion models and
introduces a prompt encoder structure based on variational autoencoders and
prosody bottlenecks to improve prompt representation capabilities.
Autoregressive language models often suffer from missing and repeated words,
while non-autoregressive frameworks face expression averaging problems due to
duration prediction models. To address these issues, we propose
Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse
prosodic expressions. While we expect the information content of semantic
coding to be between that of text and acoustic coding, existing models extract
semantic coding with a lot of redundant information and dimensionality
explosion. To verify that semantic coding is not necessary, we propose
Tri-Diff-Speech. Experimental results show that our proposed methods outperform
baseline methods. We provide a website with audio samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_C/0/1/0/all/0/1&quot;&gt;Chunyu Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Hao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;He Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruibo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longbiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1&quot;&gt;Jianwu Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15485">
<title>A Semantic Approach to Decidability in Epistemic Planning (Extended Version). (arXiv:2307.15485v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.15485</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of Dynamic Epistemic Logic (DEL) in multi-agent planning has led to a
widely adopted action formalism that can handle nondeterminism, partial
observability and arbitrary knowledge nesting. As such expressive power comes
at the cost of undecidability, several decidable fragments have been isolated,
mainly based on syntactic restrictions of the action formalism. In this paper,
we pursue a novel semantic approach to achieve decidability. Namely, rather
than imposing syntactical constraints, the semantic approach focuses on the
axioms of the logic for epistemic planning. Specifically, we augment the logic
of knowledge S5$_n$ and with an interaction axiom called (knowledge)
commutativity, which controls the ability of agents to unboundedly reason on
the knowledge of other agents. We then provide a threefold contribution. First,
we show that the resulting epistemic planning problem is decidable. In doing
so, we prove that our framework admits a finitary non-fixpoint characterization
of common knowledge, which is of independent interest. Second, we study
different generalizations of the commutativity axiom, with the goal of
obtaining decidability for more expressive fragments of DEL. Finally, we show
that two well-known epistemic planning systems based on action templates, when
interpreted under the setting of knowledge, conform to the commutativity axiom,
hence proving their decidability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burigana_A/0/1/0/all/0/1&quot;&gt;Alessandro Burigana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felli_P/0/1/0/all/0/1&quot;&gt;Paolo Felli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troquard_N/0/1/0/all/0/1&quot;&gt;Nicolas Troquard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15494">
<title>ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15494</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language instruction following is paramount to enable collaboration
between artificial agents and human beings. Natural language-conditioned
reinforcement learning (RL) agents have shown how natural languages&apos;
properties, such as compositionality, can provide a strong inductive bias to
learn complex policies. Previous architectures like HIGhER combine the benefit
of language-conditioning with Hindsight Experience Replay (HER) to deal with
sparse rewards environments. Yet, like HER, HIGhER relies on an oracle
predicate function to provide a feedback signal highlighting which linguistic
description is valid for which state. This reliance on an oracle limits its
application. Additionally, HIGhER only leverages the linguistic information
contained in successful RL trajectories, thus hurting its final performance and
data-efficiency. Without early successful trajectories, HIGhER is no better
than DQN upon which it is built. In this paper, we propose the Emergent Textual
Hindsight Experience Replay (ETHER) agent, which builds on HIGhER and addresses
both of its limitations by means of (i) a discriminative visual referential
game, commonly studied in the subfield of Emergent Communication (EC), used
here as an unsupervised auxiliary task and (ii) a semantic grounding scheme to
align the emergent language with the natural language of the
instruction-following benchmark. We show that the referential game&apos;s agents
make an artificial language emerge that is aligned with the natural-like
language used to describe goals in the BabyAI benchmark and that it is
expressive enough so as to also describe unsuccessful RL trajectories and thus
provide feedback to the RL agent to leverage the linguistic, structured
information contained in all trajectories. Our work shows that EC is a viable
unsupervised auxiliary task for RL and provides missing pieces to make HER more
widely applicable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1&quot;&gt;Kevin Denamgana&amp;#xef;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1&quot;&gt;Daniel Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardal_O/0/1/0/all/0/1&quot;&gt;Ozan Vardal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1&quot;&gt;Sondess Missaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;James Alfred Walker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15504">
<title>Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15504</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning has emerged as a promising approach to enhancing large
language models in following human instructions. It is shown that increasing
the diversity and number of instructions in the training data can consistently
enhance generalization performance, which facilitates a recent endeavor to
collect various instructions and integrate existing instruction tuning datasets
into larger collections. However, different users have their unique ways of
expressing instructions, and there often exist variations across different
datasets in the instruction styles and formats, i.e., format inconsistency. In
this work, we study how format inconsistency may impact the performance of
instruction tuning. We propose a framework called &quot;Unified Instruction Tuning&quot;
(UIT), which calls OpenAI APIs for automatic format transfer among different
instruction tuning datasets. We show that UIT successfully improves the
generalization performance on unseen instructions, which highlights the
importance of format consistency for instruction tuning. To make the UIT
framework more practical, we further propose a novel perplexity-based denoising
method to reduce the noise of automatic format transfer. We also train a
smaller offline model that achieves comparable format transfer capability than
OpenAI APIs to reduce costs in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shihao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kunlun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Runchu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huadong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaojiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15514">
<title>Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation. (arXiv:2307.15514v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15514</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works on 6D object pose estimation focus on learning keypoint
correspondences between images and object models, and then determine the object
pose through RANSAC-based algorithms or by directly regressing the pose with
end-to-end optimisations. We argue that learning point-level discriminative
features is overlooked in the literature. To this end, we revisit Fully
Convolutional Geometric Features (FCGF) and tailor it for object 6D pose
estimation to achieve state-of-the-art performance. FCGF employs sparse
convolutions and learns point-level features using a fully-convolutional
network by optimising a hardest contrastive loss. We can outperform recent
competitors on popular benchmarks by adopting key modifications to the loss and
to the input data representations, by carefully tuning the training strategies,
and by employing data augmentations suitable for the underlying problem. We
carry out a thorough ablation to study the contribution of each modification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corsetti_J/0/1/0/all/0/1&quot;&gt;Jaime Corsetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1&quot;&gt;Davide Boscaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1&quot;&gt;Fabio Poiesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15524">
<title>Few-shot Image Classification based on Gradual Machine Learning. (arXiv:2307.15524v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15524</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image classification aims to accurately classify unlabeled images
using only a few labeled samples. The state-of-the-art solutions are built by
deep learning, which focuses on designing increasingly complex deep backbones.
Unfortunately, the task remains very challenging due to the difficulty of
transferring the knowledge learned in training classes to new ones. In this
paper, we propose a novel approach based on the non-i.i.d paradigm of gradual
machine learning (GML). It begins with only a few labeled observations, and
then gradually labels target images in the increasing order of hardness by
iterative factor inference in a factor graph. Specifically, our proposed
solution extracts indicative feature representations by deep backbones, and
then constructs both unary and binary factors based on the extracted features
to facilitate gradual learning. The unary factors are constructed based on
class center distance in an embedding space, while the binary factors are
constructed based on k-nearest neighborhood. We have empirically validated the
performance of the proposed approach on benchmark datasets by a comparative
study. Our extensive experiments demonstrate that the proposed approach can
improve the SOTA performance by 1-5% in terms of accuracy. More notably, it is
more robust than the existing deep models in that its performance can
consistently improve as the size of query set increases while the performance
of deep models remains essentially flat or even becomes worse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Na Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_X/0/1/0/all/0/1&quot;&gt;Xianming Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feiyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kehao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15568">
<title>We are all Individuals: The Role of Robot Personality and Human Traits in Trustworthy Interaction. (arXiv:2307.15568v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.15568</link>
<description rdf:parseType="Literal">&lt;p&gt;As robots take on roles in our society, it is important that their
appearance, behaviour and personality are appropriate for the job they are
given and are perceived favourably by the people with whom they interact. Here,
we provide an extensive quantitative and qualitative study exploring robot
personality but, importantly, with respect to individual human traits. Firstly,
we show that we can accurately portray personality in a social robot, in terms
of extroversion-introversion using vocal cues and linguistic features.
Secondly, through garnering preferences and trust ratings for these different
robot personalities, we establish that, for a Robo-Barista, an extrovert robot
is preferred and trusted more than an introvert robot, regardless of the
subject&apos;s own personality. Thirdly, we find that individual attitudes and
predispositions towards robots do impact trust in the Robo-Baristas, and are
therefore important considerations in addition to robot personality, roles and
interaction context when designing any human-robot interaction study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1&quot;&gt;Mei Yii Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopes_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; David Aguas Lopes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robb_D/0/1/0/all/0/1&quot;&gt;David A. Robb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1&quot;&gt;Bruce W. Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moujahid_M/0/1/0/all/0/1&quot;&gt;Meriam Moujahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrin_E/0/1/0/all/0/1&quot;&gt;Emanuele De Pellegrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1&quot;&gt;Helen Hastie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15644">
<title>Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15644</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research in language-guided visual navigation has demonstrated a
significant demand for the diversity of traversable environments and the
quantity of supervision for training generalizable agents. To tackle the common
data scarcity issue in existing vision-and-language navigation datasets, we
propose an effective paradigm for generating large-scale data for learning,
which applies 1200+ photo-realistic environments from HM3D and Gibson datasets
and synthesizes 4.9 million instruction trajectory pairs using fully-accessible
resources on the web. Importantly, we investigate the influence of each
component in this paradigm on the agent&apos;s performance and study how to
adequately apply the augmented data to pre-train and fine-tune an agent. Thanks
to our large-scale dataset, the performance of an existing agent can be pushed
up (+11% absolute with regard to previous SoTA) to a significantly new best of
80% single-run success rate on the R2R test split by simple imitation learning.
The long-lasting generalization gap between navigating in seen and unseen
environments is also reduced to less than 1% (versus 8% in the previous best
method). Moreover, our paradigm also facilitates different models to achieve
new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15678">
<title>Case Studies of Causal Discovery from IT Monitoring Time Series. (arXiv:2307.15678v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.15678</link>
<description rdf:parseType="Literal">&lt;p&gt;Information technology (IT) systems are vital for modern businesses, handling
data storage, communication, and process automation. Monitoring these systems
is crucial for their proper functioning and efficiency, as it allows collecting
extensive observational time series data for analysis. The interest in causal
discovery is growing in IT monitoring systems as knowing causal relations
between different components of the IT system helps in reducing downtime,
enhancing system performance and identifying root causes of anomalies and
incidents. It also allows proactive prediction of future issues through
historical data analysis. Despite its potential benefits, applying causal
discovery algorithms on IT monitoring data poses challenges, due to the
complexity of the data. For instance, IT monitoring data often contains
misaligned time series, sleeping time series, timestamp errors and missing
values. This paper presents case studies on applying causal discovery
algorithms to different IT monitoring datasets, highlighting benefits and
ongoing challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ait_Bachir_A/0/1/0/all/0/1&quot;&gt;Ali A&amp;#xef;t-Bachir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assaad_C/0/1/0/all/0/1&quot;&gt;Charles K. Assaad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bignicourt_C/0/1/0/all/0/1&quot;&gt;Christophe de Bignicourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devijver_E/0/1/0/all/0/1&quot;&gt;Emilie Devijver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_S/0/1/0/all/0/1&quot;&gt;Simon Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaussier_E/0/1/0/all/0/1&quot;&gt;Eric Gaussier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohanna_H/0/1/0/all/0/1&quot;&gt;Hosein Mohanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zan_L/0/1/0/all/0/1&quot;&gt;Lei Zan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15682">
<title>A supervised hybrid quantum machine learning solution to the emergency escape routing problem. (arXiv:2307.15682v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.15682</link>
<description rdf:parseType="Literal">&lt;p&gt;Managing the response to natural disasters effectively can considerably
mitigate their devastating impact. This work explores the potential of using
supervised hybrid quantum machine learning to optimize emergency evacuation
plans for cars during natural disasters. The study focuses on earthquake
emergencies and models the problem as a dynamic computational graph where an
earthquake damages an area of a city. The residents seek to evacuate the city
by reaching the exit points where traffic congestion occurs. The situation is
modeled as a shortest-path problem on an uncertain and dynamically evolving
map. We propose a novel hybrid supervised learning approach and test it on
hypothetical situations on a concrete city graph. This approach uses a novel
quantum feature-wise linear modulation (FiLM) neural network parallel to a
classical FiLM network to imitate Dijkstra&apos;s node-wise shortest path algorithm
on a deterministic dynamic graph. Adding the quantum neural network in parallel
increases the overall model&apos;s expressivity by splitting the dataset&apos;s harmonic
and non-harmonic features between the quantum and classical components. The
hybrid supervised learning agent is trained on a dataset of Dijkstra&apos;s shortest
paths and can successfully learn the navigation task. The hybrid quantum
network improves over the purely classical supervised learning approach by 7%
in accuracy. We show that the quantum part has a significant contribution of
45.(3)% to the prediction and that the network could be executed on an
ion-based quantum computer. The results demonstrate the potential of supervised
hybrid quantum machine learning in improving emergency evacuation planning
during natural disasters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Haboury_N/0/1/0/all/0/1&quot;&gt;Nathan Haboury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kordzanganeh_M/0/1/0/all/0/1&quot;&gt;Mo Kordzanganeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Schmitt_S/0/1/0/all/0/1&quot;&gt;Sebastian Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ayush Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Tokarev_I/0/1/0/all/0/1&quot;&gt;Igor Tokarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Abdallah_L/0/1/0/all/0/1&quot;&gt;Lukas Abdallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kurkin_A/0/1/0/all/0/1&quot;&gt;Andrii Kurkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kyriacou_B/0/1/0/all/0/1&quot;&gt;Basil Kyriacou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Melnikov_A/0/1/0/all/0/1&quot;&gt;Alexey Melnikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15703">
<title>Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15703</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances of powerful Language Models have allowed Natural Language
Generation (NLG) to emerge as an important technology that can not only perform
traditional tasks like summarisation or translation, but also serve as a
natural language interface to a variety of applications. As such, it is crucial
that NLG systems are trustworthy and reliable, for example by indicating when
they are likely to be wrong; and supporting multiple views, backgrounds and
writing styles -- reflecting diverse human sub-populations. In this paper, we
argue that a principled treatment of uncertainty can assist in creating systems
and evaluation protocols better aligned with these goals. We first present the
fundamental theory, frameworks and vocabulary required to represent
uncertainty. We then characterise the main sources of uncertainty in NLG from a
linguistic perspective, and propose a two-dimensional taxonomy that is more
informative and faithful than the popular aleatoric/epistemic dichotomy.
Finally, we move from theory to applications and highlight exciting research
directions that exploit uncertainty to power decoding, controllable generation,
self-assessment, selective answering, active learning and more.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baan_J/0/1/0/all/0/1&quot;&gt;Joris Baan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daheim_N/0/1/0/all/0/1&quot;&gt;Nico Daheim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilia_E/0/1/0/all/0/1&quot;&gt;Evgenia Ilia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1&quot;&gt;Dennis Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haau-Sing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1&quot;&gt;Raquel Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1&quot;&gt;Barbara Plank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1&quot;&gt;Rico Sennrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1&quot;&gt;Chrysoula Zerva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1&quot;&gt;Wilker Aziz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.12672">
<title>SKTR: Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.12672</link>
<description rdf:parseType="Literal">&lt;p&gt;Developments in machine learning together with the increasing usage of sensor
data challenge the reliance on deterministic logs, requiring new process mining
solutions for uncertain, and in particular stochastically known, logs. In this
work we formulate {trace recovery}, the task of generating a deterministic log
from stochastically known logs that is as faithful to reality as possible. An
effective trace recovery algorithm would be a powerful aid for maintaining
credible process mining tools for uncertain settings. We propose an algorithmic
framework for this task that recovers the best alignment between a
stochastically known log and a process model, with three innovative features.
Our algorithm, SKTR, 1) handles both Markovian and non-Markovian processes; 2)
offers a quality-based balance between a process model and a log, depending on
the available process information, sensor quality, and machine learning
predictiveness power; and 3) offers a novel use of a synchronous product
multigraph to create the log. An empirical analysis using five publicly
available datasets, three of which use predictive models over standard video
capturing benchmarks, shows an average relative accuracy improvement of more
than 10 over a common baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdanov_E/0/1/0/all/0/1&quot;&gt;Eli Bogdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1&quot;&gt;Izack Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1&quot;&gt;Avigdor Gal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07734">
<title>Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07734</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has emerged as a promising alternative to
create supervisory signals to real-world problems, avoiding the extensive cost
of manual labeling. SSL is particularly attractive for unsupervised tasks such
as anomaly detection (AD), where labeled anomalies are rare or often
nonexistent. A large catalog of augmentation functions has been used for
SSL-based AD (SSAD) on image data, and recent works have reported that the type
of augmentation has a significant impact on accuracy. Motivated by those, this
work sets out to put image-based SSAD under a larger lens and investigate the
role of data augmentation in SSAD. Through extensive experiments on 3 different
detector models and across 420 AD tasks, we provide comprehensive numerical and
visual evidences that the alignment between data augmentation and
anomaly-generating mechanism is the key to the success of SSAD, and in the lack
thereof, SSL may even impair accuracy. To the best of our knowledge, this is
the first meta-analysis on the role of data augmentation in SSAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaemin Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiancheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10043">
<title>SynthA1c: Towards Clinically Interpretable Patient Representations for Diabetes Risk Stratification. (arXiv:2209.10043v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10043</link>
<description rdf:parseType="Literal">&lt;p&gt;Early diagnosis of Type 2 Diabetes Mellitus (T2DM) is crucial to enable
timely therapeutic interventions and lifestyle modifications. As the time
available for clinical office visits shortens and medical imaging data become
more widely available, patient image data could be used to opportunistically
identify patients for additional T2DM diagnostic workup by physicians. We
investigated whether image-derived phenotypic data could be leveraged in
tabular learning classifier models to predict T2DM risk in an automated fashion
to flag high-risk patients without the need for additional blood laboratory
measurements. In contrast to traditional binary classifiers, we leverage neural
networks and decision tree models to represent patient data as &apos;SynthA1c&apos;
latent variables, which mimic blood hemoglobin A1c empirical lab measurements,
that achieve sensitivities as high as 87.6%. To evaluate how SynthA1c models
may generalize to other patient populations, we introduce a novel generalizable
metric that uses vanilla data augmentation techniques to predict model
performance on input out-of-domain covariates. We show that image-derived
phenotypes and physical examination data together can accurately predict
diabetes risk as a means of opportunistic risk stratification enabled by
artificial intelligence and medical imaging. Our code is available at
https://github.com/allisonjchae/DMT2RiskAssessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1&quot;&gt;Michael S. Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_A/0/1/0/all/0/1&quot;&gt;Allison Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacLean_M/0/1/0/all/0/1&quot;&gt;Matthew T. MacLean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1&quot;&gt;Anurag Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duda_J/0/1/0/all/0/1&quot;&gt;Jeffrey Duda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James Gee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torigian_D/0/1/0/all/0/1&quot;&gt;Drew A. Torigian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rader_D/0/1/0/all/0/1&quot;&gt;Daniel Rader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1&quot;&gt;Charles Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witschey_W/0/1/0/all/0/1&quot;&gt;Walter R. Witschey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagreiya_H/0/1/0/all/0/1&quot;&gt;Hersh Sagreiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11679">
<title>Rethinking Missing Data: Aleatoric Uncertainty-Aware Recommendation. (arXiv:2209.11679v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11679</link>
<description rdf:parseType="Literal">&lt;p&gt;Historical interactions are the default choice for recommender model
training, which typically exhibit high sparsity, i.e., most user-item pairs are
unobserved missing data. A standard choice is treating the missing data as
negative training samples and estimating interaction likelihood between
user-item pairs along with the observed interactions. In this way, some
potential interactions are inevitably mislabeled during training, which will
hurt the model fidelity, hindering the model to recall the mislabeled items,
especially the long-tail ones. In this work, we investigate the mislabeling
issue from a new perspective of aleatoric uncertainty, which describes the
inherent randomness of missing data. The randomness pushes us to go beyond
merely the interaction likelihood and embrace aleatoric uncertainty modeling.
Towards this end, we propose a new Aleatoric Uncertainty-aware Recommendation
(AUR) framework that consists of a new uncertainty estimator along with a
normal recommender model. According to the theory of aleatoric uncertainty, we
derive a new recommendation objective to learn the estimator. As the chance of
mislabeling reflects the potential of a pair, AUR makes recommendations
according to the uncertainty, which is demonstrated to improve the
recommendation performance of less popular items without sacrificing the
overall performance. We instantiate AUR on three representative recommender
models: Matrix Factorization (MF), LightGCN, and VAE from mainstream model
architectures. Extensive results on two real-world datasets validate the
effectiveness of AUR w.r.t. better recommendation results, especially on
long-tail items.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xunhan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05304">
<title>Learning Provably Stabilizing Neural Controllers for Discrete-Time Stochastic Systems. (arXiv:2210.05304v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05304</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning control policies in discrete-time
stochastic systems which guarantee that the system stabilizes within some
specified stabilization region with probability~$1$. Our approach is based on
the novel notion of stabilizing ranking supermartingales (sRSMs) that we
introduce in this work. Our sRSMs overcome the limitation of methods proposed
in previous works whose applicability is restricted to systems in which the
stabilizing region cannot be left once entered under any control policy. We
present a learning procedure that learns a control policy together with an sRSM
that formally certifies probability~$1$ stability, both learned as neural
networks. We show that this procedure can also be adapted to formally verifying
that, under a given Lipschitz continuous control policy, the stochastic system
stabilizes within some stabilizing region with probability~$1$. Our
experimental evaluation shows that our learning procedure can successfully
learn provably stabilizing policies in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansaripour_M/0/1/0/all/0/1&quot;&gt;Matin Ansaripour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_K/0/1/0/all/0/1&quot;&gt;Krishnendu Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1&quot;&gt;Thomas A. Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1&quot;&gt;Mathias Lechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zikelic_%7B/0/1/0/all/0/1&quot;&gt;&amp;#x110;or&amp;#x111;e &amp;#x17d;ikeli&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10890">
<title>Mitigating spectral bias for the multiscale operator learning with hierarchical attention. (arXiv:2210.10890v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10890</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural operators have emerged as a powerful tool for learning the mapping
between infinite-dimensional parameter and solution spaces of partial
differential equations (PDEs). In this work, we focus on multiscale PDEs that
have important applications such as reservoir modeling and turbulence
prediction. We demonstrate that for such PDEs, the spectral bias towards
low-frequency components presents a significant challenge for existing neural
operators. To address this challenge, we propose a hierarchical attention
neural operator (HANO) inspired by the hierarchical matrix approach. HANO
features a scale-adaptive interaction range and self-attentions over a
hierarchy of levels, enabling nested feature computation with controllable
linear cost and encoding/decoding of multiscale solution space. We also
incorporate an empirical $H^1$ loss function to enhance the learning of
high-frequency components. Our numerical experiments demonstrate that HANO
outperforms state-of-the-art (SOTA) methods for representative multiscale
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03235">
<title>Complex-valued Retrievals From Noisy Images Using Diffusion Models. (arXiv:2212.03235v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03235</link>
<description rdf:parseType="Literal">&lt;p&gt;In diverse microscopy modalities, sensors measure only real-valued
intensities. Additionally, the sensor readouts are affected by
Poissonian-distributed photon noise. Traditional restoration algorithms
typically aim to minimize the mean squared error (MSE) between the original and
recovered images. This often leads to blurry outcomes with poor perceptual
quality. Recently, deep diffusion models (DDMs) have proven to be highly
capable of sampling images from the a-posteriori probability of the sought
variables, resulting in visually pleasing high-quality images. These models
have mostly been suggested for real-valued images suffering from Gaussian
noise. In this study, we generalize annealed Langevin Dynamics, a type of DDM,
to tackle the fundamental challenges in optical imaging of complex-valued
objects (and real images) affected by Poisson noise. We apply our algorithm to
various optical scenarios, such as Fourier Ptychography, Phase Retrieval, and
Poisson denoising. Our algorithm is evaluated on simulations and biological
empirical data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torem_N/0/1/0/all/0/1&quot;&gt;Nadav Torem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronen_R/0/1/0/all/0/1&quot;&gt;Roi Ronen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schechner_Y/0/1/0/all/0/1&quot;&gt;Yoav Y. Schechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10839">
<title>Consistent Range Approximation for Fair Predictive Modeling. (arXiv:2212.10839v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10839</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel framework for certifying the fairness of
predictive models trained on biased data. It draws from query answering for
incomplete and inconsistent databases to formulate the problem of consistent
range approximation (CRA) of fairness queries for a predictive model on a
target population. The framework employs background knowledge of the data
collection process and biased data, working with or without limited statistics
about the target population, to compute a range of answers for fairness
queries. Using CRA, the framework builds predictive models that are certifiably
fair on the target population, regardless of the availability of external data
during training. The framework&apos;s efficacy is demonstrated through evaluations
on real data, showing substantial improvement over existing state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiongli Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1&quot;&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabri_N/0/1/0/all/0/1&quot;&gt;Nazanin Sabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1&quot;&gt;Babak Salimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04253">
<title>Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04253</link>
<description rdf:parseType="Literal">&lt;p&gt;The topic of Climate Change (CC) has received limited attention in NLP
despite its urgency. Activists and policymakers need NLP tools to effectively
process the vast and rapidly growing unstructured textual climate reports into
structured form. To tackle this challenge we introduce two new large-scale
climate questionnaire datasets and use their existing structure to train
self-supervised models. We conduct experiments to show that these models can
learn to generalize to climate disclosures of different organizations types
than seen during training. We then use these models to help align texts from
unstructured climate documents to the semi-structured questionnaires in a human
pilot study. Finally, to support further NLP research in the climate domain we
introduce a benchmark of existing climate text classification datasets to
better evaluate and compare existing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spokoyny_D/0/1/0/all/0/1&quot;&gt;Daniel Spokoyny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laud_T/0/1/0/all/0/1&quot;&gt;Tanmay Laud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corringham_T/0/1/0/all/0/1&quot;&gt;Tom Corringham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1&quot;&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08231">
<title>3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection. (arXiv:2302.08231v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08231</link>
<description rdf:parseType="Literal">&lt;p&gt;3D visual perception tasks based on multi-camera images are essential for
autonomous driving systems. Latest work in this field performs 3D object
detection by leveraging multi-view images as an input and iteratively enhancing
object queries (object proposals) by cross-attending multi-view features.
However, individual backbone features are not updated with multi-view features
and it stays as a mere collection of the output of the single-image backbone
network. Therefore we propose 3M3D: A Multi-view, Multi-path,
Multi-representation for 3D Object Detection where we update both multi-view
features and query features to enhance the representation of the scene in both
fine panoramic view and coarse global view. Firstly, we update multi-view
features by multi-view axis self-attention. It will incorporate panoramic
information in the multi-view features and enhance understanding of the global
scene. Secondly, we update multi-view features by self-attention of the ROI
(Region of Interest) windows which encodes local finer details in the features.
It will help exchange the information not only along the multi-view axis but
also along the other spatial dimension. Lastly, we leverage the fact of
multi-representation of queries in different domains to further boost the
performance. Here we use sparse floating queries along with dense BEV (Bird&apos;s
Eye View) queries, which are later post-processed to filter duplicate
detections. Moreover, we show performance improvements on nuScenes benchmark
dataset on top of our baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jongwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Apoorv Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bankiti_V/0/1/0/all/0/1&quot;&gt;Varun Bankiti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10909">
<title>Multi-modal Machine Learning in Engineering Design: A Review and Future Directions. (arXiv:2302.10909v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10909</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly advancing field of multi-modal machine learning (MMML), the
convergence of multiple data modalities has the potential to reshape various
applications. This paper presents a comprehensive overview of the current
state, advancements, and challenges of MMML within the sphere of engineering
design. The review begins with a deep dive into five fundamental concepts of
MMML:multi-modal information representation, fusion, alignment, translation,
and co-learning. Following this, we explore the cutting-edge applications of
MMML, placing a particular emphasis on tasks pertinent to engineering design,
such as cross-modal synthesis, multi-modal prediction, and cross-modal
information retrieval. Through this comprehensive overview, we highlight the
inherent challenges in adopting MMML in engineering design, and proffer
potential directions for future research. To spur on the continued evolution of
MMML in engineering design, we advocate for concentrated efforts to construct
extensive multi-modal design datasets, develop effective data-driven MMML
techniques tailored to design applications, and enhance the scalability and
interpretability of MMML models. MMML models, as the next generation of
intelligent design tools, hold a promising future to impact how products are
designed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Binyang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1&quot;&gt;Faez Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12247">
<title>Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1&quot;&gt;Chun Kai Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1&quot;&gt;Suzanne Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zihao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1&quot;&gt;Nicholas Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1&quot;&gt;Randy Auerbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14339">
<title>Efficient Exploration Using Extra Safety Budget in Constrained Policy Optimization. (arXiv:2302.14339v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14339</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has achieved promising results on most robotic
control tasks. Safety of learning-based controllers is an essential notion of
ensuring the effectiveness of the controllers. Current methods adopt whole
consistency constraints during the training, thus resulting in inefficient
exploration in the early stage. In this paper, we propose an algorithm named
Constrained Policy Optimization with Extra Safety Budget (ESB-CPO) to strike a
balance between the exploration efficiency and the constraints satisfaction. In
the early stage, our method loosens the practical constraints of unsafe
transitions (adding extra safety budget) with the aid of a new metric we
propose. With the training process, the constraints in our optimization problem
become tighter. Meanwhile, theoretical analysis and practical experiments
demonstrate that our method gradually meets the cost limit&apos;s demand in the
final training stage. When evaluated on Safety-Gym and Bullet-Safety-Gym
benchmarks, our method has shown its advantages over baseline algorithms in
terms of safety and optimality. Remarkably, our method gains remarkable
performance improvement under the same cost limit compared with baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haotian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaolei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_Q/0/1/0/all/0/1&quot;&gt;Qing Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01170">
<title>Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning. (arXiv:2303.01170v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01170</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning in Reinforcement Learning (RL) has been widely studied to
overcome training issues of Deep-RL, i.e., exploration cost, data availability
and convergence time, by introducing a way to enhance training phase with
external knowledge. Generally, knowledge is transferred from expert-agents to
novices. While this fixes the issue for a novice agent, a good understanding of
the task on expert agent is required for such transfer to be effective. As an
alternative, in this paper we propose Expert-Free Online Transfer Learning
(EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer
learning in multi-agent system. No dedicated expert exists, and transfer source
agent and knowledge to be transferred are dynamically selected at each transfer
step based on agents&apos; performance and uncertainty. To improve uncertainty
estimation, we also propose State Action Reward Next-State Random Network
Distillation (sars-RND), an extension of RND that estimates uncertainty from RL
agent-environment interaction. We demonstrate EF-OnTL effectiveness against a
no-transfer scenario and advice-based baselines, with and without expert
agents, in three benchmark tasks: Cart-Pole, a grid-based Multi-Team
Predator-Prey (mt-pp) and Half Field Offense (HFO). Our results show that
EF-OnTL achieve overall comparable performance when compared against
advice-based baselines while not requiring any external input nor threshold
tuning. EF-OnTL outperforms no-transfer with an improvement related to the
complexity of the task addressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castagna_A/0/1/0/all/0/1&quot;&gt;Alberto Castagna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusparic_I/0/1/0/all/0/1&quot;&gt;Ivana Dusparic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07810">
<title>VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07810</link>
<description rdf:parseType="Literal">&lt;p&gt;In argumentative writing, writers must brainstorm hierarchical writing goals,
ensure the persuasiveness of their arguments, and revise and organize their
plans through drafting. Recent advances in large language models (LLMs) have
made interactive text generation through a chat interface (e.g., ChatGPT)
possible. However, this approach often neglects implicit writing context and
user intent, lacks support for user control and autonomy, and provides limited
assistance for sensemaking and revising writing plans. To address these
challenges, we introduce VISAR, an AI-enabled writing assistant system designed
to help writers brainstorm and revise hierarchical goals within their writing
context, organize argument structures through synchronized text editing and
visual programming, and enhance persuasiveness with argumentation spark
recommendations. VISAR allows users to explore, experiment with, and validate
their writing plans using automatic draft prototyping. A controlled lab study
confirmed the usability and effectiveness of VISAR in facilitating the
argumentative writing planning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhaliwal_R/0/1/0/all/0/1&quot;&gt;Ranjodh Singh Dhaliwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Toby Jia-Jun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10712">
<title>Adversarial Infrared Blocks: A Multi-view Black-box Attack to Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10712</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared imaging systems have a vast array of potential applications in
pedestrian detection and autonomous driving, and their safety performance is of
great concern. However, few studies have explored the safety of infrared
imaging systems in real-world settings. Previous research has used physical
perturbations such as small bulbs and thermal &quot;QR codes&quot; to attack infrared
imaging detectors, but such methods are highly visible and lack stealthiness.
Other researchers have used hot and cold blocks to deceive infrared imaging
detectors, but this method is limited in its ability to execute attacks from
various angles. To address these shortcomings, we propose a novel physical
attack called adversarial infrared blocks (AdvIB). By optimizing the physical
parameters of the adversarial infrared blocks, this method can execute a
stealthy black-box attack on thermal imaging system from various angles. We
evaluate the proposed method based on its effectiveness, stealthiness, and
robustness. Our physical tests show that the proposed method achieves a success
rate of over 80% under most distance and angle conditions, validating its
effectiveness. For stealthiness, our method involves attaching the adversarial
infrared block to the inside of clothing, enhancing its stealthiness.
Additionally, we test the proposed method on advanced detectors, and
experimental results demonstrate an average attack success rate of 51.2%,
proving its robustness. Overall, our proposed AdvIB method offers a promising
avenue for conducting stealthy, effective and robust black-box attacks on
thermal imaging system, with potential implications for real-world safety and
security applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chengyin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weiwen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Ling Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04076">
<title>SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04076</link>
<description rdf:parseType="Literal">&lt;p&gt;Distantly-Supervised Named Entity Recognition effectively alleviates the
burden of time-consuming and expensive annotation in the supervised setting.
But the context-free matching process and the limited coverage of knowledge
bases introduce inaccurate and incomplete annotation noise respectively.
Previous studies either considered only incomplete annotation noise or
indiscriminately handle two types of noise with the same strategy. In this
paper, we argue that the different causes of two types of noise bring up the
requirement of different strategies in model architecture. Therefore, we
propose the SANTA to handle these two types of noise separately with (1)
Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity
problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate
decision boundary shifting problem caused by incomplete annotation and a
noise-tolerant loss to improve the robustness. Benefiting from our separate
tailored strategies, we confirm in the experiment that the two types of noise
are well mitigated. SANTA also achieves a new state-of-the-art on five public
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1&quot;&gt;Shuzheng Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zefan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shuang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1&quot;&gt;Guoqiang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiaxing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Baobao Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10883">
<title>Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs. (arXiv:2305.10883v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10883</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-assisted transoral tracheal intubation (TI) necessitates using an
endoscope that helps the physician insert a tracheal tube into the glottis
instead of the esophagus. The growing trend of robotic-assisted TI would
require a medical robot to distinguish anatomical features like an experienced
physician which can be imitated by utilizing supervised deep-learning
techniques. However, the real datasets of oropharyngeal organs are often
inaccessible due to limited open-source data and patient privacy. In this work,
we propose a domain adaptive Sim-to-Real framework called IoU-Ranking
Blend-ArtFlow (IRB-AF) for image segmentation of oropharyngeal organs. The
framework includes an image blending strategy called IoU-Ranking Blend (IRB)
and style-transfer method ArtFlow. Here, IRB alleviates the problem of poor
segmentation performance caused by significant datasets domain differences;
while ArtFlow is introduced to reduce the discrepancies between datasets
further. A virtual oropharynx image dataset generated by the SOFA framework is
used as the learning subject for semantic segmentation to deal with the limited
availability of actual endoscopic images. We adapted IRB-AF with the
state-of-the-art domain adaptive segmentation models. The results demonstrate
the superior performance of our approach in further improving the segmentation
accuracy and training stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guankun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tian-Ao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jiewen Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05965">
<title>Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05965</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian state and parameter estimation have been automated effectively in a
variety of probabilistic programming languages. The process of model comparison
on the other hand, which still requires error-prone and time-consuming manual
derivations, is often overlooked despite its importance. This paper efficiently
automates Bayesian model averaging, selection, and combination by message
passing on a Forney-style factor graph with a custom mixture node. Parameter
and state inference, and model comparison can then be executed simultaneously
using message passing with scale factors. This approach shortens the model
design cycle and allows for the straightforward extension to hierarchical and
temporal model priors to accommodate for modeling complicated time-varying
processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1&quot;&gt;Bart van Erp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1&quot;&gt;Wouter W. L. Nuijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laar_T/0/1/0/all/0/1&quot;&gt;Thijs van de Laar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1&quot;&gt;Bert de Vries&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00112">
<title>Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00112</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence is gaining traction in more ways than ever before.
The popularity of language models and AI-based businesses has soared since
ChatGPT was made available to the general public via OpenAI. It is becoming
increasingly common for people to use ChatGPT both professionally and
personally. Considering the widespread use of ChatGPT and the reliance people
place on it, this study determined how reliable ChatGPT can be for answering
complex medical and clinical questions. Harvard University gross anatomy along
with the United States Medical Licensing Examination (USMLE) questionnaire were
used to accomplish the objective. The paper evaluated the obtained results
using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation
between format and prompt. Furthermore, the physician adjudicators
independently rated the outcome&apos;s accuracy, concordance, and insight. As a
result of the analysis, ChatGPT-generated answers were found to be more
context-oriented and represented a better model for deductive reasoning than
regular Google search results. Furthermore, ChatGPT obtained 58.8% on logical
questions and 60% on ethical questions. This means that the ChatGPT is
approaching the passing range for logical questions and has crossed the
threshold for ethical questions. The paper believes ChatGPT and other language
learning models can be invaluable tools for e-learners; however, the study
suggests that there is still room to improve their accuracy. In order to
improve ChatGPT&apos;s performance in the future, further research is needed to
better understand how it can answer different types of questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Prabin Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_K/0/1/0/all/0/1&quot;&gt;Kisan Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_D/0/1/0/all/0/1&quot;&gt;Dikshya Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhakal_P/0/1/0/all/0/1&quot;&gt;Prastab Dhakal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhaya_M/0/1/0/all/0/1&quot;&gt;Mala Deep Upadhaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikari_S/0/1/0/all/0/1&quot;&gt;Santosh Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanal_S/0/1/0/all/0/1&quot;&gt;Salik Ram Khanal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02131">
<title>Beyond Known Reality: Exploiting Counterfactual Explanations for Medical Research. (arXiv:2307.02131v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02131</link>
<description rdf:parseType="Literal">&lt;p&gt;This study employs counterfactual explanations to explore &quot;what if?&quot;
scenarios in medical research, with the aim of expanding our understanding
beyond existing boundaries. Specifically, we focus on utilizing MRI features
for diagnosing pediatric posterior fossa brain tumors as a case study. The
field of artificial intelligence and explainability has witnessed a growing
number of studies and increasing scholarly interest. However, the lack of
human-friendly interpretations in explaining the outcomes of machine learning
algorithms has significantly hindered the acceptance of these methods by
clinicians in their clinical practice. To address this, our approach
incorporates counterfactual explanations, providing a novel way to examine
alternative decision-making scenarios. These explanations offer personalized
and context-specific insights, enabling the validation of predictions and
clarification of variations under diverse circumstances. Importantly, our
approach maintains both statistical and clinical fidelity, allowing for the
examination of distinct tumor features through alternative realities.
Additionally, we explore the potential use of counterfactuals for data
augmentation and evaluate their feasibility as an alternative approach in
medical research. The results demonstrate the promising potential of
counterfactual explanations to enhance trust and acceptance of AI-driven
methods in clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanyel_T/0/1/0/all/0/1&quot;&gt;Toygar Tanyel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayvaz_S/0/1/0/all/0/1&quot;&gt;Serkan Ayvaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keserci_B/0/1/0/all/0/1&quot;&gt;Bilgin Keserci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07686">
<title>Creating a Dataset for High-Performance Computing Code Translation: A Bridge Between HPC Fortran and C++. (arXiv:2307.07686v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07686</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a novel dataset for training machine learning
models translating between OpenMP Fortran and C++ code. To ensure reliability
and applicability, the dataset is initially refined using a meticulous code
similarity test. The effectiveness of our dataset is assessed using both
quantitative (CodeBLEU) and qualitative (human evaluation) methods. We
demonstrate how this dataset can significantly improve the translation
capabilities of large-scale language models, with improvements of
$\mathbf{\times 5.1}$ for models with no prior coding knowledge and
$\mathbf{\times 9.9}$ for models with some coding familiarity. Our work
highlights the potential of this dataset to advance the field of code
translation for high-performance computing. The dataset is available at
https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1&quot;&gt;Bin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Le Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Pei-Hung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chunhua Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09042">
<title>Emotional Intelligence of Large Language Models. (arXiv:2307.09042v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09042</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated remarkable abilities across
numerous disciplines, primarily assessed through tasks in language generation,
knowledge utilization, and complex reasoning. However, their alignment with
human emotions and values, which is critical for real-world applications, has
not been systematically evaluated. Here, we assessed LLMs&apos; Emotional
Intelligence (EI), encompassing emotion recognition, interpretation, and
understanding, which is necessary for effective communication and social
interactions. Specifically, we first developed a novel psychometric assessment
focusing on Emotion Understanding (EU), a core component of EI, suitable for
both humans and LLMs. This test requires evaluating complex emotions (e.g.,
surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite
feeling underperformed, John surprisingly achieved a top score). With a
reference frame constructed from over 500 adults, we tested a variety of
mainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding
89% of human participants with an EQ of 117. Interestingly, a multivariate
pattern analysis revealed that some LLMs apparently did not reply on the
human-like mechanism to achieve human-level performance, as their
representational patterns were qualitatively distinct from humans. In addition,
we discussed the impact of factors such as model size, training method, and
architecture on LLMs&apos; EQ. In summary, our study presents one of the first
psychometric evaluations of the human-like characteristics of LLMs, which may
shed light on the future development of LLMs aiming for both high intellectual
and emotional intelligence. Project website:
https://emotional-intelligence.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuena Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_L/0/1/0/all/0/1&quot;&gt;Liu Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09763">
<title>Towards Building More Robust Models with Frequency Bias. (arXiv:2307.09763v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09763</link>
<description rdf:parseType="Literal">&lt;p&gt;The vulnerability of deep neural networks to adversarial samples has been a
major impediment to their broad applications, despite their success in various
fields. Recently, some works suggested that adversarially-trained models
emphasize the importance of low-frequency information to achieve higher
robustness. While several attempts have been made to leverage this frequency
characteristic, they have all faced the issue that applying low-pass filters
directly to input images leads to irreversible loss of discriminative
information and poor generalizability to datasets with distinct frequency
features. This paper presents a plug-and-play module called the Frequency
Preference Control Module that adaptively reconfigures the low- and
high-frequency components of intermediate feature representations, providing
better utilization of frequency in robust learning. Empirical studies show that
our proposed module can be easily incorporated into any adversarial training
framework, further improving model robustness across different architectures
and datasets. Additionally, experiments were conducted to examine how the
frequency bias of robust models impacts the adversarial training process and
its final robustness, revealing interesting insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1&quot;&gt;Qingwen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Heming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13131">
<title>Why Don&apos;t You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (arXiv:2307.13131v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13131</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera-based autonomous systems that emulate human perception are
increasingly being integrated into safety-critical platforms. Consequently, an
established body of literature has emerged that explores adversarial attacks
targeting the underlying machine learning models. Adapting adversarial attacks
to the physical world is desirable for the attacker, as this removes the need
to compromise digital systems. However, the real world poses challenges related
to the &quot;survivability&quot; of adversarial manipulations given environmental noise
in perception pipelines and the dynamicity of autonomous systems. In this
paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle
perception attack that leverages transparent displays to generate dynamic
physical adversarial examples. EvilEye exploits the camera&apos;s optics to induce
misclassifications under a variety of illumination conditions. To generate
dynamic perturbations, we formalize the projection of a digital attack into the
physical domain by modeling the transformation function of the captured image
through the optical pipeline. Our extensive experiments show that EvilEye&apos;s
generated adversarial perturbations are much more robust across varying
environmental light conditions relative to existing physical perturbation
frameworks, achieving a high attack success rate (ASR) while bypassing
state-of-the-art physical adversarial detection frameworks. We demonstrate that
the dynamic nature of EvilEye enables attackers to adapt adversarial examples
across a variety of objects with a significantly higher ASR compared to
state-of-the-art physical world attack frameworks. Finally, we discuss
mitigation strategies against the EvilEye attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1&quot;&gt;Matthew Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wengrowski_E/0/1/0/all/0/1&quot;&gt;Eric Wengrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuohuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tippenhauer_N/0/1/0/all/0/1&quot;&gt;Nils Ole Tippenhauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1&quot;&gt;Mani Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonouz_S/0/1/0/all/0/1&quot;&gt;Saman Zonouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_L/0/1/0/all/0/1&quot;&gt;Luis Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13494">
<title>Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v4 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13494</link>
<description rdf:parseType="Literal">&lt;p&gt;Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches face the
data and workload drift problem for a long time. Although both query-driven and
hybrid methods are proposed to avoid this problem, even the state-of-the-art of
them suffer from high training and estimation costs, limited scalability,
instability, and long-tailed distribution problem on high cardinality and
high-dimensional tables, which seriously affects the practical application of
learned cardinality estimators. In this paper, we prove that most of these
problems are directly caused by the widely used progressive sampling. We solve
this problem by introducing predicates information into the autoregressive
model and propose Duet, a stable, efficient, and scalable hybrid method to
estimate cardinality directly without sampling or any non-differentiable
process, which can not only reduces the inference complexity from O(n) to O(1)
compared to Naru and UAE but also achieve higher accuracy on high cardinality
and high-dimensional tables. Experimental results show that Duet can achieve
all the design goals above and be much more practical and even has a lower
inference cost on CPU than that of most learned methods on GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yabin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Chang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donghua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14750">
<title>Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14750</link>
<description rdf:parseType="Literal">&lt;p&gt;Training an image captioner without annotated image-sentence pairs has gained
traction in recent years. Previous approaches can be categorized into two
strategies: crawling sentences from mismatching corpora and aligning them with
the given images as pseudo annotations, or pre-training the captioner using
external image-text pairs. However, the aligning setting seems to reach its
performance limit due to the quality problem of pairs, and pre-training
requires significant computational resources. To address these challenges, we
propose a new strategy ``LPM + retrieval-augmented learning&quot; where the prior
knowledge from large pre-trained models (LPMs) is leveraged as supervision, and
a retrieval process is integrated to further reinforce its effectiveness.
Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation
(RaPSG), which adopts an efficient approach to retrieve highly relevant short
region descriptions from the mismatching corpora and use them to generate a
variety of pseudo sentences with distinct representations as well as high
quality via LPMs. In addition, a fluency filter and a CLIP-guided training
objective are further introduced to facilitate model optimization. Experimental
results demonstrate that our method surpasses the SOTA pre-training model
(Flamingo3B) by achieving a CIDEr score of 78.1 (+5.1) while utilizing only
0.3% of its trainable parameters (1.3B VS 33M). Importantly, our approach
eliminates the need of computationally expensive pre-training processes on
external datasets (e.g., the requirement of 312M image-text pairs for
Flamingo3B). We further show that with a simple extension, the generated pseudo
sentences can be deployed as weak supervision to boost the 1% semi-supervised
image caption benchmark up to 93.4 CIDEr score (+8.9) which showcases the
versatility and effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14799">
<title>Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14799</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern semiconductor manufacturing involves intricate production processes
consisting of hundreds of operations, which can take several months from lot
release to completion. The high-tech machines used in these processes are
diverse, operate on individual wafers, lots, or batches in multiple stages, and
necessitate product-specific setups and specialized maintenance procedures.
This situation is different from traditional job-shop scheduling scenarios,
which have less complex production processes and machines, and mainly focus on
solving highly combinatorial but abstract scheduling problems. In this work, we
address the scheduling of realistic semiconductor manufacturing processes by
modeling their specific requirements using hybrid Answer Set Programming with
difference logic, incorporating flexible machine processing, setup, batching
and maintenance operations. Unlike existing methods that schedule semiconductor
manufacturing processes locally with greedy heuristics or by independently
optimizing specific machine group allocations, we examine the potentials of
large-scale scheduling subject to multiple optimization objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Kholany_M/0/1/0/all/0/1&quot;&gt;Mohammed M. S. El-Kholany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1&quot;&gt;Ramsha Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebser_M/0/1/0/all/0/1&quot;&gt;Martin Gebser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14935">
<title>Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14935</link>
<description rdf:parseType="Literal">&lt;p&gt;Data profiling is an essential process in modern data-driven industries. One
of its critical components is the discovery and validation of complex
statistics, including functional dependencies, data constraints, association
rules, and others.
&lt;/p&gt;
&lt;p&gt;However, most existing data profiling systems that focus on complex
statistics do not provide proper integration with the tools used by
contemporary data scientists. This creates a significant barrier to the
adoption of these tools in the industry. Moreover, existing systems were not
created with industrial-grade workloads in mind. Finally, they do not aim to
provide descriptive explanations, i.e. why a given pattern is not found. It is
a significant issue as it is essential to understand the underlying reasons for
a specific pattern&apos;s absence to make informed decisions based on the data.
&lt;/p&gt;
&lt;p&gt;Because of that, these patterns are effectively rest in thin air: their
application scope is rather limited, they are rarely used by the broader
public. At the same time, as we are going to demonstrate in this presentation,
complex statistics can be efficiently used to solve many classic data quality
problems.
&lt;/p&gt;
&lt;p&gt;Desbordante is an open-source data profiler that aims to close this gap. It
is built with emphasis on industrial application: it is efficient, scalable,
resilient to crashes, and provides explanations. Furthermore, it provides
seamless Python integration by offloading various costly operations to the C++
core, not only mining.
&lt;/p&gt;
&lt;p&gt;In this demonstration, we show several scenarios that allow end users to
solve different data quality problems. Namely, we showcase typo detection, data
deduplication, and data anomaly detection scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1&quot;&gt;George Chernishev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyntsov_M/0/1/0/all/0/1&quot;&gt;Michael Polyntsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chizhov_A/0/1/0/all/0/1&quot;&gt;Anton Chizhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stupakov_K/0/1/0/all/0/1&quot;&gt;Kirill Stupakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shchuckin_I/0/1/0/all/0/1&quot;&gt;Ilya Shchuckin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_A/0/1/0/all/0/1&quot;&gt;Alexander Smirnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strutovsky_M/0/1/0/all/0/1&quot;&gt;Maxim Strutovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlyonskikh_A/0/1/0/all/0/1&quot;&gt;Alexey Shlyonskikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firsov_M/0/1/0/all/0/1&quot;&gt;Mikhail Firsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manannikov_S/0/1/0/all/0/1&quot;&gt;Stepan Manannikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobrov_N/0/1/0/all/0/1&quot;&gt;Nikita Bobrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncharov_D/0/1/0/all/0/1&quot;&gt;Daniil Goncharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barutkin_I/0/1/0/all/0/1&quot;&gt;Ilia Barutkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalnev_V/0/1/0/all/0/1&quot;&gt;Vladislav Shalnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muraviev_K/0/1/0/all/0/1&quot;&gt;Kirill Muraviev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhmukova_A/0/1/0/all/0/1&quot;&gt;Anna Rakhmukova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shcheka_D/0/1/0/all/0/1&quot;&gt;Dmitriy Shcheka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernikov_A/0/1/0/all/0/1&quot;&gt;Anton Chernikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyrodov_M/0/1/0/all/0/1&quot;&gt;Mikhail Vyrodov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurbatov_Y/0/1/0/all/0/1&quot;&gt;Yaroslav Kurbatov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fofanov_M/0/1/0/all/0/1&quot;&gt;Maxim Fofanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belokonnyi_S/0/1/0/all/0/1&quot;&gt;Sergei Belokonnyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anosov_P/0/1/0/all/0/1&quot;&gt;Pavel Anosov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saliou_A/0/1/0/all/0/1&quot;&gt;Arthur Saliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaisin_E/0/1/0/all/0/1&quot;&gt;Eduard Gaisin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_K/0/1/0/all/0/1&quot;&gt;Kirill Smirnov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15051">
<title>Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15051</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical trials are vital in advancing drug development and evidence-based
medicine, but their success is often hindered by challenges in patient
recruitment. In this work, we investigate the potential of large language
models (LLMs) to assist individual patients and referral physicians in
identifying suitable clinical trials from an extensive selection. Specifically,
we introduce TrialGPT, a novel architecture employing LLMs to predict
criterion-level eligibility with detailed explanations, which are then
aggregated for ranking and excluding candidate clinical trials based on
free-text patient notes. We evaluate TrialGPT on three publicly available
cohorts of 184 patients and 18,238 annotated clinical trials. The experimental
results demonstrate several key findings: First, TrialGPT achieves high
criterion-level prediction accuracy with faithful explanations. Second, the
aggregated trial-level TrialGPT scores are highly correlated with expert
eligibility annotations. Third, these scores prove effective in ranking
clinical trials and exclude ineligible candidates. Our error analysis suggests
that current LLMs still make some mistakes due to limited medical knowledge and
domain-specific context understanding. Nonetheless, we believe the explanatory
capabilities of LLMs are highly valuable. Future research is warranted on how
such AI assistants can be integrated into the routine trial matching workflow
in real-world settings to improve its efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Floudas_C/0/1/0/all/0/1&quot;&gt;Charalampos S. Floudas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08495">
<title>Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations. (arXiv:2206.08495v5 [cs.DS] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2206.08495</link>
<description rdf:parseType="Literal">&lt;p&gt;We study fair allocation of indivisible goods when agents have matroid rank
valuations. Our main contribution is a simple algorithm based on the colloquial
Yankee Swap procedure that computes provably fair and efficient Lorenz
dominating allocations. While there exist polynomial time algorithms to compute
such allocations, our proposed method improves on them in two ways. (a) Our
approach is easy to understand and does not use complex matroid optimization
algorithms as subroutines. (b) Our approach is scalable; it is provably faster
than all known algorithms to compute Lorenz dominating allocations. These two
properties are key to the adoption of algorithms in any real fair allocation
setting; our contribution brings us one step closer to this goal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1&quot;&gt;Vignesh Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1&quot;&gt;Yair Zick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14643">
<title>MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy. (arXiv:2307.14643v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.14643</link>
<description rdf:parseType="Literal">&lt;p&gt;How to accurately measure the relevance and redundancy of features is an
age-old challenge in the field of feature selection. However, existing
filter-based feature selection methods cannot directly measure redundancy for
continuous data. In addition, most methods rely on manually specifying the
number of features, which may introduce errors in the absence of expert
knowledge. In this paper, we propose a non-parametric feature selection
algorithm based on maximum inter-class variation and minimum redundancy,
abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel
density estimation on the features to capture their similarities and
differences in inter-class and overall distributions. Subsequently, we present
the criteria for maximum inter-class variation and minimum redundancy (MVMR),
wherein the inter-class probability distributions are employed to reflect
feature relevance and the distances between overall probability distributions
are used to quantify redundancy. Finally, we employ an AGA to search for the
feature subset that minimizes the MVMR. Compared with ten state-of-the-art
methods, MVMR-FS achieves the highest average accuracy and improves the
accuracy by 5% to 11%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_H/0/1/0/all/0/1&quot;&gt;Haitao Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1&quot;&gt;Bin Xie&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>