<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.01874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.01962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04547" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.06667">
<title>Optimizing Fault-Tolerant Quality-Guaranteed Sensor Deployments for UAV Localization in Critical Areas via Computational Geometry. (arXiv:2312.06667v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06667</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing spreading of small commercial Unmanned Aerial Vehicles (UAVs,
aka drones) presents serious threats for critical areas such as airports, power
plants, governmental and military facilities. In fact, such UAVs can easily
disturb or jam radio communications, collide with other flying objects, perform
espionage activity, and carry offensive payloads, e.g., weapons or explosives.
A central problem when designing surveillance solutions for the localization of
unauthorized UAVs in critical areas is to decide how many triangulating sensors
to use, and where to deploy them to optimise both coverage and cost
effectiveness.
&lt;/p&gt;
&lt;p&gt;In this article, we compute deployments of triangulating sensors for UAV
localization, optimizing a given blend of metrics, namely: coverage under
multiple sensing quality levels, cost-effectiveness, fault-tolerance. We focus
on large, complex 3D regions, which exhibit obstacles (e.g., buildings),
varying terrain elevation, different coverage priorities, constraints on
possible sensors placement. Our novel approach relies on computational geometry
and statistical model checking, and enables the effective use of off-the-shelf
AI-based black-box optimizers. Moreover, our method allows us to compute a
closed-form, analytical representation of the region uncovered by a sensor
deployment, which provides the means for rigorous, formal certification of the
quality of the latter.
&lt;/p&gt;
&lt;p&gt;We show the practical feasibility of our approach by computing optimal sensor
deployments for UAV localization in two large, complex 3D critical regions, the
Rome Leonardo Da Vinci International Airport (FCO) and the Vienna International
Center (VIC), using NOMAD as our state-of-the-art underlying optimization
engine. Results show that we can compute optimal sensor deployments within a
few hours on a standard workstation and within minutes on a small parallel
infrastructure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esposito_M/0/1/0/all/0/1&quot;&gt;Marco Esposito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancini_T/0/1/0/all/0/1&quot;&gt;Toni Mancini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tronci_E/0/1/0/all/0/1&quot;&gt;Enrico Tronci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06670">
<title>Combating the effects of speed and delays in end-to-end self-driving. (arXiv:2312.06670v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06670</link>
<description rdf:parseType="Literal">&lt;p&gt;In the behavioral cloning approach to end-to-end driving, a dataset of expert
driving is collected and the model learns to guess what the expert would do in
different situations. Situations are summarized in observations and the outputs
are low or mid-level commands (e.g. brake, throttle, and steering; or
trajectories). The models learn to match observations at time T to actions
recorded at T or as simultaneously as possible. However, when deploying the
models to the real world (or to an asynchronous simulation), the action
predicted based on observations at time T gets applied at T + $\Delta$ T. In a
variety of cases, $\Delta$ T can be considerable and significantly influence
performance.
&lt;/p&gt;
&lt;p&gt;We first demonstrate that driving at two different speeds is effectively two
different tasks. Delays partially cause this difference and linearly amplify
it. Even without computational delays, actuator delays and slipping due to
inertia result in the need to perform actions preemptively when driving fast.
The function mapping observations to commands becomes different compared to
slow driving. We experimentally show that models trained to drive fast cannot
perform the seemingly easier task of driving slow and vice-versa. Good driving
models may be judged to be poor due to testing them at &quot;a safe low speed&quot;, a
task they cannot perform.
&lt;/p&gt;
&lt;p&gt;Secondly, we show how to counteract the effect of delays in end-to-end
networks by changing the target labels. This is in contrast to the approaches
attempting to minimize the delays, i.e. the cause, not the effect. To exemplify
the problems and solutions in the real world, we use 1:10 scale minicars with
limited computing power, using behavioral cloning for end-to-end driving. Some
of the ideas discussed here may be transferable to the wider context of
self-driving, to vehicles with more compute power and end-to-mid or modular
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tampuu_A/0/1/0/all/0/1&quot;&gt;Ardi Tampuu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uduste_I/0/1/0/all/0/1&quot;&gt;Ilmar Uduste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roosild_K/0/1/0/all/0/1&quot;&gt;Kristjan Roosild&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06674">
<title>Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. (arXiv:2312.06674v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.06674</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Llama Guard, an LLM-based input-output safeguard model geared
towards Human-AI conversation use cases. Our model incorporates a safety risk
taxonomy, a valuable tool for categorizing a specific set of safety risks found
in LLM prompts (i.e., prompt classification). This taxonomy is also
instrumental in classifying the responses generated by LLMs to these prompts, a
process we refer to as response classification. For the purpose of both prompt
and response classification, we have meticulously gathered a dataset of high
quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our
collected dataset, albeit low in volume, demonstrates strong performance on
existing benchmarks such as the OpenAI Moderation Evaluation dataset and
ToxicChat, where its performance matches or exceeds that of currently available
content moderation tools. Llama Guard functions as a language model, carrying
out multi-class classification and generating binary decision scores.
Furthermore, the instruction fine-tuning of Llama Guard allows for the
customization of tasks and the adaptation of output formats. This feature
enhances the model&apos;s capabilities, such as enabling the adjustment of taxonomy
categories to align with specific use cases, and facilitating zero-shot or
few-shot prompting with diverse taxonomies at the input. We are making Llama
Guard model weights available and we encourage researchers to further develop
and adapt them to meet the evolving needs of the community for AI safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1&quot;&gt;Hakan Inan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upasani_K/0/1/0/all/0/1&quot;&gt;Kartikeya Upasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1&quot;&gt;Jianfeng Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rungta_R/0/1/0/all/0/1&quot;&gt;Rashi Rungta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_K/0/1/0/all/0/1&quot;&gt;Krithika Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuning Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tontchev_M/0/1/0/all/0/1&quot;&gt;Michael Tontchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuller_B/0/1/0/all/0/1&quot;&gt;Brian Fuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Testuggine_D/0/1/0/all/0/1&quot;&gt;Davide Testuggine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1&quot;&gt;Madian Khabsa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06677">
<title>Intelligent Virtual Assistants with LLM-based Process Automation. (arXiv:2312.06677v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06677</link>
<description rdf:parseType="Literal">&lt;p&gt;While intelligent virtual assistants like Siri, Alexa, and Google Assistant
have become ubiquitous in modern life, they still face limitations in their
ability to follow multi-step instructions and accomplish complex goals
articulated in natural language. However, recent breakthroughs in large
language models (LLMs) show promise for overcoming existing barriers by
enhancing natural language processing and reasoning capabilities. Though
promising, applying LLMs to create more advanced virtual assistants still faces
challenges like ensuring robust performance and handling variability in
real-world user commands. This paper proposes a novel LLM-based virtual
assistant that can automatically perform multi-step operations within mobile
apps based on high-level user requests. The system represents an advance in
assistants by providing an end-to-end solution for parsing instructions,
reasoning about goals, and executing actions. LLM-based Process Automation
(LLMPA) has modules for decomposing instructions, generating descriptions,
detecting interface elements, predicting next actions, and error checking.
Experiments demonstrate the system completing complex mobile operation tasks in
Alipay based on natural language instructions. This showcases how large
language models can enable automated assistants to accomplish real-world tasks.
The main contributions are the novel LLMPA architecture optimized for app
process automation, the methodology for applying LLMs to mobile apps, and
demonstrations of multi-step task completion in a real-world environment.
Notably, this work represents the first real-world deployment and extensive
evaluation of a large language model-based virtual assistant in a widely used
mobile application with an enormous user base numbering in the hundreds of
millions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yanchu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_F/0/1/0/all/0/1&quot;&gt;Feiyue Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Ruihua Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1&quot;&gt;Chenyi Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06681">
<title>Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.06681</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors&apos;&apos; by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user&apos;s prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA&apos;s
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA&apos;s mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1&quot;&gt;Nina Rimsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1&quot;&gt;Nick Gabrieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1&quot;&gt;Julian Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1&quot;&gt;Alexander Matt Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06682">
<title>Learning to Denoise Unreliable Interactions for Link Prediction on Biomedical Knowledge Graph. (arXiv:2312.06682v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06682</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction in biomedical knowledge graphs (KGs) aims at predicting
unknown interactions between entities, including drug-target interaction (DTI)
and drug-drug interaction (DDI), which is critical for drug discovery and
therapeutics. Previous methods prefer to utilize the rich semantic relations
and topological structure of the KG to predict missing links, yielding
promising outcomes. However, all these works only focus on improving the
predictive performance without considering the inevitable noise and unreliable
interactions existing in the KGs, which limits the development of KG-based
computational methods. To address these limitations, we propose a Denoised Link
Prediction framework, called DenoisedLP. DenoisedLP obtains reliable
interactions based on the local subgraph by denoising noisy links in a
learnable way, providing a universal module for mining underlying task-relevant
relations. To collaborate with the smoothed semantic information, DenoisedLP
introduces the semantic subgraph by blurring conflict relations around the
predicted link. By maximizing the mutual information between the reliable
structure and smoothed semantic relations, DenoisedLP emphasizes the
informative interactions for predicting relation-specific links. Experimental
results on real-world datasets demonstrate that DenoisedLP outperforms
state-of-the-art methods on DTI and DDI prediction tasks, and verify the
effectiveness and robustness of denoising unreliable interactions on the
contaminated KGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yujie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wen Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dashun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_P/0/1/0/all/0/1&quot;&gt;Patrick Cheong-lao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bosheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06684">
<title>Enhanced E-Commerce Attribute Extraction: Innovating with Decorative Relation Correction and LLAMA 2.0-Based Annotation. (arXiv:2312.06684v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06684</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid proliferation of e-commerce platforms accentuates the need for
advanced search and retrieval systems to foster a superior user experience.
Central to this endeavor is the precise extraction of product attributes from
customer queries, enabling refined search, comparison, and other crucial
e-commerce functionalities. Unlike traditional Named Entity Recognition (NER)
tasks, e-commerce queries present a unique challenge owing to the intrinsic
decorative relationship between product types and attributes. In this study, we
propose a pioneering framework that integrates BERT for classification, a
Conditional Random Fields (CRFs) layer for attribute value extraction, and
Large Language Models (LLMs) for data annotation, significantly advancing
attribute recognition from customer inquiries. Our approach capitalizes on the
robust representation learning of BERT, synergized with the sequence decoding
prowess of CRFs, to adeptly identify and extract attribute values. We introduce
a novel decorative relation correction mechanism to further refine the
extraction process based on the nuanced relationships between product types and
attributes inherent in e-commerce data. Employing LLMs, we annotate additional
data to expand the model&apos;s grasp and coverage of diverse attributes. Our
methodology is rigorously validated on various datasets, including Walmart,
BestBuy&apos;s e-commerce NER dataset, and the CoNLL dataset, demonstrating
substantial improvements in attribute recognition performance. Particularly,
the model showcased promising results during a two-month deployment in
Walmart&apos;s Sponsor Product Search, underscoring its practical utility and
effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianghong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Weizhi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokon_M/0/1/0/all/0/1&quot;&gt;Md Omar Faruk Rokon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaodong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_I/0/1/0/all/0/1&quot;&gt;Isha Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-chih Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1&quot;&gt;Musen Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06685">
<title>Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models. (arXiv:2312.06685v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06685</link>
<description rdf:parseType="Literal">&lt;p&gt;While Multi-modal Language Models (MLMs) demonstrate impressive multimodal
ability, they still struggle on providing factual and precise responses for
tasks like visual question answering (VQA). In this paper, we address this
challenge from the perspective of contextual information. We propose Causal
Context Generation, Causal-CoG, which is a prompting strategy that engages
contextual information to enhance precise VQA during inference. Specifically,
we prompt MLMs to generate contexts, i.e, text description of an image, and
engage the generated contexts for question answering. Moreover, we investigate
the advantage of contexts on VQA from a causality perspective, introducing
causality filtering to select samples for which contextual information is
helpful. To show the effectiveness of Causal-CoG, we run extensive experiments
on 10 multimodal benchmarks and show consistent improvements, e.g., +6.30% on
POPE, +13.69% on Vizwiz and +6.43% on VQAv2 compared to direct decoding,
surpassing existing methods. We hope Casual-CoG inspires explorations of
context knowledge in multimodal models, and serves as a plug-and-play strategy
for MLM decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shitian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuowan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yadong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06695">
<title>Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06695</link>
<description rdf:parseType="Literal">&lt;p&gt;Animals often demonstrate a remarkable ability to adapt to their environments
during their lifetime. They do so partly due to the evolution of morphological
and neural structures. These structures capture features of environments shared
between generations to bias and speed up lifetime learning. In this work, we
propose a computational model for studying a mechanism that can enable such a
process. We adopt a computational framework based on meta reinforcement
learning as a model of the interplay between evolution and development. At the
evolutionary scale, we evolve reservoirs, a family of recurrent neural networks
that differ from conventional networks in that one optimizes not the weight
values but hyperparameters of the architecture: the later control macro-level
properties, such as memory and dynamics. At the developmental scale, we employ
these evolved reservoirs to facilitate the learning of a behavioral policy
through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes
the environment state before providing it to an action policy. We evaluate our
approach on several 2D and 3D simulated environments. Our results show that the
evolution of reservoirs can improve the learning of diverse challenging tasks.
We study in particular three hypotheses: the use of an architecture combining
reservoirs and reinforcement learning could enable (1) solving tasks with
partial observability, (2) generating oscillatory dynamics that facilitate the
learning of locomotion tasks, and (3) facilitating the generalization of
learned behaviors to new tasks unknown during the evolution phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leger_C/0/1/0/all/0/1&quot;&gt;Corentin L&amp;#xe9;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamon_G/0/1/0/all/0/1&quot;&gt;Gautier Hamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nisioti_E/0/1/0/all/0/1&quot;&gt;Eleni Nisioti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinaut_X/0/1/0/all/0/1&quot;&gt;Xavier Hinaut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06712">
<title>Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models. (arXiv:2312.06712v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06712</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent significant strides achieved by diffusion-based Text-to-Image
(T2I) models, current systems are still less capable of ensuring decent
compositional generation aligned with text prompts, particularly for the
multi-object generation. This work illuminates the fundamental reasons for such
misalignment, pinpointing issues related to low attention activation scores and
mask overlaps. While previous research efforts have individually tackled these
issues, we assert that a holistic approach is paramount. Thus, we propose two
novel objectives, the Separate loss and the Enhance loss, that reduce object
mask overlaps and maximize attention scores, respectively. Our method diverges
from conventional test-time-adaptation techniques, focusing on finetuning
critical parameters, which enhances scalability and generalizability.
Comprehensive evaluations demonstrate the superior performance of our model in
terms of image realism, text-image alignment, and adaptability, notably
outperforming prominent baselines. Ultimately, this research paves the way for
T2I diffusion models with enhanced compositional capacities and broader
applicability. The project webpage is available at
https://zpbao.github.io/projects/SepEn/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1&quot;&gt;Martial Hebert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06717">
<title>Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06717</link>
<description rdf:parseType="Literal">&lt;p&gt;This is the first survey of the active area of AI research that focuses on
privacy issues in Large Language Models (LLMs). Specifically, we focus on work
that red-teams models to highlight privacy risks, attempts to build privacy
into the training or inference process, enables efficient data deletion from
trained models to comply with existing privacy regulations, and tries to
mitigate copyright issues. Our focus is on summarizing technical research that
develops algorithms, proves theorems, and runs empirical evaluations. While
there is an extensive body of legal and policy work addressing these challenges
from a different angle, that is not the focus of our survey. Nevertheless,
these works, along with recent legal developments do inform how these technical
problems are formalized, and so we discuss them briefly in Section 1. While we
have made our best effort to include all the relevant work, due to the fast
moving nature of this research we may have missed some recent work. If we have
missed some of your work please contact us, as we will attempt to keep this
survey relatively up to date. We are maintaining a repository with the list of
papers covered in this survey and any relevant code that was publicly available
at https://github.com/safr-ml-lab/survey-llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1&quot;&gt;Peter Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06718">
<title>Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey. (arXiv:2312.06718v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06718</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the applications of artificial intelligence especially deep learning
had greatly improved various aspects of intelligent manufacturing, they still
face challenges for wide employment due to the poor generalization ability,
difficulties to establish high-quality training datasets, and unsatisfactory
performance of deep learning methods. The emergence of large scale foundational
models(LSFMs) had triggered a wave in the field of artificial intelligence,
shifting deep learning models from single-task, single-modal, limited data
patterns to a paradigm encompassing diverse tasks, multimodal, and pre-training
on massive datasets. Although LSFMs had demonstrated powerful generalization
capabilities, automatic high-quality training dataset generation and superior
performance across various domains, applications of LSFMs on intelligent
manufacturing were still in their nascent stage. A systematic overview of this
topic was lacking, especially regarding which challenges of deep learning can
be addressed by LSFMs and how these challenges can be systematically tackled.
To fill this gap, this paper systematically expounded current statue of LSFMs
and their advantages in the context of intelligent manufacturing. and compared
comprehensively with the challenges faced by current deep learning models in
various intelligent manufacturing applications. We also outlined the roadmaps
for utilizing LSFMs to address these challenges. Finally, case studies of
applications of LSFMs in real-world intelligent manufacturing scenarios were
presented to illustrate how LSFMs could help industries, improve their
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haotian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dereck_S/0/1/0/all/0/1&quot;&gt;Semujju Stuart Dereck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1&quot;&gt;Xianwei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Ye Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1&quot;&gt;Zhuo Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wensheng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;X.G. Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_R/0/1/0/all/0/1&quot;&gt;Ruiyan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06727">
<title>A method for recovery of multidimensional time series based on the detection of behavioral patterns and the use of autoencoders. (arXiv:2312.06727v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06727</link>
<description rdf:parseType="Literal">&lt;p&gt;This article presents a method for recovering missing values in
multidimensional time series. The method combines neural network technologies
and an algorithm for searching snippets (behavioral patterns of a time series).
It includes the stages of data preprocessing, recognition and reconstruction,
using convolutional and recurrent neural networks. Experiments have shown high
accuracy of recovery and the advantage of the method over SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurtin_A/0/1/0/all/0/1&quot;&gt;Alexey Yurtin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06731">
<title>Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator. (arXiv:2312.06731v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06731</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) excel in understanding human instructions,
driving the development of Multimodal LLMs (MLLMs) with instruction tuning.
However, acquiring high-quality multimodal instruction tuning data poses a
significant challenge. Previous approaches relying on GPT-4 for data generation
proved expensive and exhibited unsatisfactory performance for certain tasks. To
solve this, we present Genixer, an innovative data generation pipeline
producing high-quality multimodal instruction tuning data for various tasks.
Genixer collects datasets for ten prevalent multimodal tasks and designs
instruction templates to transform these datasets into instruction-tuning data.
It then trains pretrained MLLMs to generate task-specific instruction data and
proposes an effective data filtering strategy to ensure high quality. To
evaluate Genixer, a base MLLM model, Kakapo, is built and achieves SoTA
performance in image captioning and visual question answering (VQA) tasks
across multiple datasets. Experimental results show that filtered data from
Genixer continually improves Kakapo for image captioning and VQA tasks. For the
SoTA Shikra MLLM model on the image-region-related tasks, e.g., region caption
and detection, Genixer also successfully generates corresponding data and
improves its performance. Genixer opens avenues for generating high-quality
multimodal instruction data for diverse tasks, enabling innovative applications
across domains. The code and models will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Henry Hengyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06742">
<title>Honeybee: Locality-enhanced Projector for Multimodal LLM. (arXiv:2312.06742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.06742</link>
<description rdf:parseType="Literal">&lt;p&gt;In Multimodal Large Language Models (MLLMs), a visual projector plays a
crucial role in bridging pre-trained vision encoders with LLMs, enabling
profound visual understanding while harnessing the LLMs&apos; robust capabilities.
Despite the importance of the visual projector, it has been relatively less
explored. In this study, we first identify two essential projector properties:
(i) flexibility in managing the number of visual tokens, crucial for MLLMs&apos;
overall efficiency, and (ii) preservation of local context from visual
features, vital for spatial understanding. Based on these findings, we propose
a novel projector design that is both flexible and locality-enhanced,
effectively satisfying the two desirable properties. Additionally, we present
comprehensive strategies to effectively utilize multiple and multifaceted
instruction datasets. Through extensive experiments, we examine the impact of
individual design choices. Finally, our proposed MLLM, Honeybee, remarkably
outperforms previous state-of-the-art methods across various benchmarks,
including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly
higher efficiency. Code and models are available at
https://github.com/kakaobrain/honeybee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1&quot;&gt;Junbum Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wooyoung Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_J/0/1/0/all/0/1&quot;&gt;Jonghwan Mun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1&quot;&gt;Byungseok Roh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06786">
<title>Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06786</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-term time series forecasting (LTSF) aims to predict future values of a
time series given the past values. The current state-of-the-art (SOTA) on this
problem is attained in some cases by linear-centric models, which primarily
feature a linear mapping layer. However, due to their inherent simplicity, they
are not able to adapt their prediction rules to periodic changes in time series
patterns. To address this challenge, we propose a Mixture-of-Experts-style
augmentation for linear-centric models and propose Mixture-of-Linear-Experts
(MoLE). Instead of training a single model, MoLE trains multiple linear-centric
models (i.e., experts) and a router model that weighs and mixes their outputs.
While the entire framework is trained end-to-end, each expert learns to
specialize in a specific temporal pattern, and the router model learns to
compose the experts adaptively. Experiments show that MoLE reduces forecasting
error of linear-centric models, including DLinear, RLinear, and RMLP, in over
78% of the datasets and settings we evaluated. By using MoLE existing
linear-centric models can achieve SOTA LTSF results in 68% of the experiments
that PatchTST reports and we compare to, whereas existing single-head
linear-centric models achieve SOTA results in only 25% of cases. Additionally,
MoLE models achieve SOTA in all settings for the newly released Weather2K
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Ronghao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1&quot;&gt;Giulia Fanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06798">
<title>Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety. (arXiv:2312.06798v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06798</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google&apos;s MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1&quot;&gt;Manas Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06820">
<title>Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning. (arXiv:2312.06820v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06820</link>
<description rdf:parseType="Literal">&lt;p&gt;Microsoft Windows Feedback Hub is designed to receive customer feedback on a
wide variety of subjects including critical topics such as power and battery.
Feedback is one of the most effective ways to have a grasp of users&apos; experience
with Windows and its ecosystem. However, the sheer volume of feedback received
by Feedback Hub makes it immensely challenging to diagnose the actual cause of
reported issues. To better understand and triage issues, we leverage Double
Machine Learning (DML) to associate users&apos; feedback with telemetry signals. One
of the main challenges we face in the DML pipeline is the necessity of domain
knowledge for model design (e.g., causal graph), which sometimes is either not
available or hard to obtain. In this work, we take advantage of reasoning
capabilities in Large Language Models (LLMs) to generate a prior model that
which to some extent compensates for the lack of domain knowledge and could be
used as a heuristic for measuring feedback informativeness. Our LLM-based
approach is able to extract previously known issues, uncover new bugs, and
identify sequences of events that lead to a bug, while minimizing out-of-domain
outputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1&quot;&gt;Sara Abdali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1&quot;&gt;Anjali Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Steve Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1&quot;&gt;Emre Kiciman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06826">
<title>User Friendly and Adaptable Discriminative AI: Using the Lessons from the Success of LLMs and Image Generation Models. (arXiv:2312.06826v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06826</link>
<description rdf:parseType="Literal">&lt;p&gt;While there is significant interest in using generative AI tools as
general-purpose models for specific ML applications, discriminative models are
much more widely deployed currently. One of the key shortcomings of these
discriminative AI tools that have been already deployed is that they are not
adaptable and user-friendly compared to generative AI tools (e.g., GPT4, Stable
Diffusion, Bard, etc.), where a non-expert user can iteratively refine model
inputs and give real-time feedback that can be accounted for immediately,
allowing users to build trust from the start. Inspired by this emerging
collaborative workflow, we develop a new system architecture that enables users
to work with discriminative models (such as for object detection, sentiment
classification, etc.) in a fashion similar to generative AI tools, where they
can easily provide immediate feedback as well as adapt the deployed models as
desired. Our approach has implications on improving trust, user-friendliness,
and adaptability of these versatile but traditional prediction models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Son The Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulabandhula_T/0/1/0/all/0/1&quot;&gt;Theja Tulabandhula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_Manheim_M/0/1/0/all/0/1&quot;&gt;Mary Beth Watson-Manheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06832">
<title>Symptom-based Machine Learning Models for the Early Detection of COVID-19: A Narrative Review. (arXiv:2312.06832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06832</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the widespread testing protocols for COVID-19, there are still
significant challenges in early detection of the disease, which is crucial for
preventing its spread and optimizing patient outcomes. Owing to the limited
testing capacity in resource-strapped settings and the limitations of the
available traditional methods of testing, it has been established that a fast
and efficient strategy is important to fully stop the virus. Machine learning
models can analyze large datasets, incorporating patient-reported symptoms,
clinical data, and medical imaging. Symptom-based detection methods have been
developed to predict COVID-19, and they have shown promising results. In this
paper, we provide an overview of the landscape of symptoms-only machine
learning models for predicting COVID-19, including their performance and
limitations. The review will also examine the performance of symptom-based
models when compared to image-based models. Because different studies used
varying datasets, methodologies, and performance metrics. Selecting the model
that performs best relies on the context and objectives of the research.
However, based on the results, we observed that ensemble classifier performed
exceptionally well in predicting the occurrence of COVID-19 based on patient
symptoms with the highest overall accuracy of 97.88%. Gradient Boosting
Algorithm achieved an AUC (Area Under the Curve) of 0.90 and identified key
features contributing to the decision-making process. Image-based models, as
observed in the analyzed studies, have consistently demonstrated higher
accuracy than symptom-based models, often reaching impressive levels ranging
from 96.09% to as high as 99%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akinloye_M/0/1/0/all/0/1&quot;&gt;Moyosolu Akinloye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06833">
<title>The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06833</link>
<description rdf:parseType="Literal">&lt;p&gt;$\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided
Detection (CADe) is commonly used for polyp detection, but data seen in
clinical settings can differ from model training. Few studies evaluate how well
CADe detectors perform on colonoscopies from countries not seen during
training, and none are able to evaluate performance without collecting
expensive and time-intensive labels.
&lt;/p&gt;
&lt;p&gt;$\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy
videos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,
128 hours) by measuring the True Positive Rate (TPR) versus false alarms per
minute (FAPM). We introduce a colonoscopy dissimilarity measure called &quot;MAsked
mediCal Embedding Distance&quot; (MACE) to quantify differences between
colonoscopies, without labels. We evaluated CADe on all Japan videos and on
those with the highest MACE.
&lt;/p&gt;
&lt;p&gt;$\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)
and chromoendoscopy (CE) frames are less similar to Israel data than Japan
whitelight (bootstrapped z-test, |z| &amp;gt; 690, p &amp;lt; $10^{-8}$ for both). Despite
differences in the data, CADe performance on Japan colonoscopies was
non-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957
and 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and
Japan; superiority test t &amp;gt; 45.2, p &amp;lt; $10^{-8}$). Despite not being trained on
NBI or CE, TPR on those subsets were non-inferior to Japan overall
(non-inferiority test t &amp;gt; 47.3, p &amp;lt; $10^{-8}$, $\delta$ = 1.5% for both).
&lt;/p&gt;
&lt;p&gt;$\textbf{Conclusion}$: Differences that prevent CADe detectors from
performing well in non-medical settings do not degrade the performance of our
AI CADe polyp detector when applied to data from a new country. MACE can help
medical AI models internationalize by identifying the most &quot;dissimilar&quot; data on
which to evaluate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1&quot;&gt;Joel Shor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamano_H/0/1/0/all/0/1&quot;&gt;Hiro-o Yamano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsurumaru_D/0/1/0/all/0/1&quot;&gt;Daisuke Tsurumaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Intrator_Y/0/1/0/all/0/1&quot;&gt;Yotami Intrator&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayama_H/0/1/0/all/0/1&quot;&gt;Hiroki Kayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1&quot;&gt;Joe Ledsam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamabe_A/0/1/0/all/0/1&quot;&gt;Atsushi Hamabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1&quot;&gt;Koji Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ota_M/0/1/0/all/0/1&quot;&gt;Mitsuhiko Ota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogino_H/0/1/0/all/0/1&quot;&gt;Haruei Ogino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakase_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Kaho Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oki_E/0/1/0/all/0/1&quot;&gt;Eiji Oki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1&quot;&gt;Roman Goldenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemasa_I/0/1/0/all/0/1&quot;&gt;Ichiro Takemasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06853">
<title>LLF-Bench: Benchmark for Interactive Learning from Language Feedback. (arXiv:2312.06853v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06853</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new benchmark, LLF-Bench (Learning from Language Feedback
Benchmark; pronounced as &quot;elf-bench&quot;), to evaluate the ability of AI agents to
interactively learn from natural language feedback and instructions. Learning
from language feedback (LLF) is essential for people, largely because the rich
information this feedback provides can help a learner avoid much of trial and
error and thereby speed up the learning process. Large Language Models (LLMs)
have recently enabled AI agents to comprehend natural language -- and hence AI
agents can potentially benefit from language feedback during learning like
humans do. But existing interactive benchmarks do not assess this crucial
capability: they either use numeric reward feedback or require no learning at
all (only planning or information retrieval). LLF-Bench is designed to fill
this omission. LLF-Bench is a diverse collection of sequential decision-making
tasks that includes user recommendation, poem writing, navigation, and robot
control. The objective of an agent is to interactively solve these tasks based
on their natural-language instructions and the feedback received after taking
actions. Crucially, to ensure that the agent actually &quot;learns&quot; from the
feedback, LLF-Bench implements several randomization techniques (such as
paraphrasing and environment randomization) to ensure that the task isn&apos;t
familiar to the agent and that the agent is robust to various verbalizations.
In addition, LLF-Bench provides a unified OpenAI Gym interface for all its
tasks and allows the users to easily configure the information the feedback
conveys (among suggestion, explanation, and instantaneous performance) to study
how agents respond to different types of feedback. Together, these features
make LLF-Bench a unique research platform for developing and testing LLF
agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1&quot;&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Dipendra Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_A/0/1/0/all/0/1&quot;&gt;Allen Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1&quot;&gt;Adith Swaminathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06858">
<title>Scalable Decentralized Cooperative Platoon using Multi-Agent Deep Reinforcement Learning. (arXiv:2312.06858v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06858</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperative autonomous driving plays a pivotal role in improving road
capacity and safety within intelligent transportation systems, particularly
through the deployment of autonomous vehicles on urban streets. By enabling
vehicle-to-vehicle communication, these systems expand the vehicles
environmental awareness, allowing them to detect hidden obstacles and thereby
enhancing safety and reducing crash rates compared to human drivers who rely
solely on visual perception. A key application of this technology is vehicle
platooning, where connected vehicles drive in a coordinated formation. This
paper introduces a vehicle platooning approach designed to enhance traffic flow
and safety. Developed using deep reinforcement learning in the Unity 3D game
engine, known for its advanced physics, this approach aims for a high-fidelity
physical simulation that closely mirrors real-world conditions. The proposed
platooning model focuses on scalability, decentralization, and fostering
positive cooperation through the introduced predecessor-follower &quot;sharing and
caring&quot; communication framework. The study demonstrates how these elements
collectively enhance autonomous driving performance and robustness, both for
individual vehicles and for the platoon as a whole, in an urban setting. This
results in improved road safety and reduced traffic congestion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelrahman_A/0/1/0/all/0/1&quot;&gt;Ahmed Abdelrahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehata_O/0/1/0/all/0/1&quot;&gt;Omar M. Shehata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basyoni_Y/0/1/0/all/0/1&quot;&gt;Yarah Basyoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1&quot;&gt;Elsayed I. Morgan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06869">
<title>Adversarial Estimation of Topological Dimension with Harmonic Score Maps. (arXiv:2312.06869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06869</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification of the number of variables needed to locally explain complex
data is often the first step to better understanding it. Existing techniques
from intrinsic dimension estimation leverage statistical models to glean this
information from samples within a neighborhood. However, existing methods often
rely on well-picked hyperparameters and ample data as manifold dimension and
curvature increases. Leveraging insight into the fixed point of the score
matching objective as the score map is regularized by its Dirichlet energy, we
show that it is possible to retrieve the topological dimension of the manifold
learned by the score map. We then introduce a novel method to measure the
learned manifold&apos;s topological dimension (i.e., local intrinsic dimension)
using adversarial attacks, thereby generating useful interpretations of the
learned manifold.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeats_E/0/1/0/all/0/1&quot;&gt;Eric Yeats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darwin_C/0/1/0/all/0/1&quot;&gt;Cameron Darwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Frank Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06871">
<title>Using Analytics on Student Created Data to Content Validate Pedagogical Tools. (arXiv:2312.06871v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06871</link>
<description rdf:parseType="Literal">&lt;p&gt;Conceptual and simulation models can function as useful pedagogical tools,
however it is important to categorize different outcomes when evaluating them
in order to more meaningfully interpret results. VERA is a ecology-based
conceptual modeling software that enables users to simulate interactions
between biotics and abiotics in an ecosystem, allowing users to form and then
verify hypothesis through observing a time series of the species populations.
In this paper, we classify this time series into common patterns found in the
domain of ecological modeling through two methods, hierarchical clustering and
curve fitting, illustrating a general methodology for showing content validity
when combining different pedagogical tools. When applied to a diverse sample of
263 models containing 971 time series collected from three different VERA user
categories: a Georgia Tech (GATECH), North Georgia Technical College (NGTC),
and ``Self Directed Learners&apos;&apos;, results showed agreement between both
classification methods on 89.38\% of the sample curves in the test set. This
serves as a good indication that our methodology for determining content
validity was successful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kos_J/0/1/0/all/0/1&quot;&gt;John Kos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_K/0/1/0/all/0/1&quot;&gt;Kenneth Eaton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sareen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dass_R/0/1/0/all/0/1&quot;&gt;Rahul Dass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Stephen Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Sungeun An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1&quot;&gt;Ashok Goel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06876">
<title>Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks. (arXiv:2312.06876v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.06876</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing robotic agents to perform open vocabulary tasks has been the
long-standing goal in robotics and AI. Recently, Large Language Models (LLMs)
have achieved impressive results in creating robotic agents for performing open
vocabulary tasks. However, planning for these tasks in the presence of
uncertainties is challenging as it requires \enquote{chain-of-thought}
reasoning, aggregating information from the environment, updating state
estimates, and generating actions based on the updated state estimates. In this
paper, we present an interactive planning technique for partially observable
tasks using LLMs. In the proposed method, an LLM is used to collect missing
information from the environment using a robot and infer the state of the
underlying problem from collected observations while guiding the robot to
perform the required actions. We also use a fine-tuned Llama 2 model via
self-instruct and compare its performance against a pre-trained LLM like GPT-4.
Results are demonstrated on several tasks in simulation as well as real-world
environments. A video describing our work along with some results could be
found here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lingfeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Devesh K. Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1&quot;&gt;Chiori Hori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Siddarth Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcodel_R/0/1/0/all/0/1&quot;&gt;Radu Corcodel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romeres_D/0/1/0/all/0/1&quot;&gt;Diego Romeres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06887">
<title>Understanding and Leveraging the Learning Phases of Neural Networks. (arXiv:2312.06887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.06887</link>
<description rdf:parseType="Literal">&lt;p&gt;The learning dynamics of deep neural networks are not well understood. The
information bottleneck (IB) theory proclaimed separate fitting and compression
phases. But they have since been heavily debated. We comprehensively analyze
the learning dynamics by investigating a layer&apos;s reconstruction ability of the
input and prediction performance based on the evolution of parameters during
training. We empirically show the existence of three phases using common
datasets and architectures such as ResNet and VGG: (i) near constant
reconstruction loss, (ii) decrease, and (iii) increase. We also derive an
empirically grounded data model and prove the existence of phases for
single-layer networks. Technically, our approach leverages classical complexity
analysis. It differs from IB by relying on measuring reconstruction loss rather
than information theoretic measures to relate information of intermediate
layers and inputs. Our work implies a new best practice for transfer learning:
We show empirically that the pre-training of a classifier should stop well
before its performance is optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Johannes Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhushanka_M/0/1/0/all/0/1&quot;&gt;Mohit Prabhushanka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06901">
<title>Unsupervised Extractive Summarization with Learnable Length Control Strategies. (arXiv:2312.06901v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06901</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised extractive summarization is an important technique in
information extraction and retrieval. Compared with supervised method, it does
not require high-quality human-labelled summaries for training and thus can be
easily applied for documents with different types, domains or languages. Most
of existing unsupervised methods including TextRank and PACSUM rely on
graph-based ranking on sentence centrality. However, this scorer can not be
directly applied in end-to-end training, and the positional-related prior
assumption is often needed for achieving good summaries. In addition, less
attention is paid to length-controllable extractor, where users can decide to
summarize texts under particular length constraint. This paper introduces an
unsupervised extractive summarization model based on a siamese network, for
which we develop a trainable bidirectional prediction objective between the
selected summary and the original document. Different from the centrality-based
ranking methods, our extractive scorer can be trained in an end-to-end manner,
with no other requirement of positional assumption. In addition, we introduce a
differentiable length control module by approximating 0-1 knapsack solver for
end-to-end length-controllable extracting. Experiments show that our
unsupervised method largely outperforms the centrality-based baseline using a
same sentence encoder. In terms of length control ability, via our trainable
knapsack module, the performance consistently outperforms the strong baseline
without utilizing end-to-end training. Human evaluation further evidences that
our method performs the best among baselines in terms of relevance and
consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_R/0/1/0/all/0/1&quot;&gt;Renlong Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiaojun Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06973">
<title>Anytime Approximate Formal Feature Attribution. (arXiv:2312.06973v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06973</link>
<description rdf:parseType="Literal">&lt;p&gt;Widespread use of artificial intelligence (AI) algorithms and machine
learning (ML) models on the one hand and a number of crucial issues pertaining
to them warrant the need for explainable artificial intelligence (XAI). A key
explainability question is: given this decision was made, what are the input
features which contributed to the decision? Although a range of XAI approaches
exist to tackle this problem, most of them have significant limitations.
Heuristic XAI approaches suffer from the lack of quality guarantees, and often
try to approximate Shapley values, which is not the same as explaining which
features contribute to a decision. A recent alternative is so-called formal
feature attribution (FFA), which defines feature importance as the fraction of
formal abductive explanations (AXp&apos;s) containing the given feature. This
measures feature importance from the view of formally reasoning about the
model&apos;s behavior. It is challenging to compute FFA using its definition because
that involves counting AXp&apos;s, although one can approximate it. Based on these
results, this paper makes several contributions. First, it gives compelling
evidence that computing FFA is intractable, even if the set of contrastive
formal explanations (CXp&apos;s) is provided, by proving that the problem is
#P-hard. Second, by using the duality between AXp&apos;s and CXp&apos;s, it proposes an
efficient heuristic to switch from CXp enumeration to AXp enumeration
on-the-fly resulting in an adaptive explanation enumeration algorithm
effectively approximating FFA in an anytime fashion. Finally, experimental
results obtained on a range of widely used datasets demonstrate the
effectiveness of the proposed FFA approximation approach in terms of the error
of FFA approximation as well as the number of explanations computed and their
diversity given a fixed time limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jinqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farr_G/0/1/0/all/0/1&quot;&gt;Graham Farr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06974">
<title>SM70: A Large Language Model for Medical Devices. (arXiv:2312.06974v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.06974</link>
<description rdf:parseType="Literal">&lt;p&gt;We are introducing SM70, a 70 billion-parameter Large Language Model that is
specifically designed for SpassMed&apos;s medical devices under the brand name
&apos;JEE1&apos; (pronounced as G1 and means &apos;Life&apos;). This large language model provides
more accurate and safe responses to medical-domain questions. To fine-tune
SM70, we used around 800K data entries from the publicly available dataset
MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,
and we employed the QLoRA technique for fine-tuning. The evaluation is
conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE
- each representing a unique aspect of medical knowledge and reasoning. The
performance of SM70 is contrasted with other notable LLMs, including Llama2
70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a
comparative understanding of its capabilities within the medical domain. Our
results indicate that SM70 outperforms several established models in these
datasets, showcasing its proficiency in handling a range of medical queries,
from fact-based questions derived from PubMed abstracts to complex clinical
decision-making scenarios. The robust performance of SM70, particularly in the
USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in
clinical decision support and medical information retrieval. Despite its
promising results, the paper also acknowledges the areas where SM70 lags behind
the most advanced model, GPT 4, thereby highlighting the need for further
development, especially in tasks demanding extensive medical knowledge and
intricate reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1&quot;&gt;Anubhav Bhatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_S/0/1/0/all/0/1&quot;&gt;Surajsinh Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;San Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06990">
<title>AI-based Wildfire Prevention, Detection and Suppression System. (arXiv:2312.06990v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.06990</link>
<description rdf:parseType="Literal">&lt;p&gt;Wildfires pose a serious threat to the environment of the world. The global
wildfire season length has increased by 19% and severe wildfires have besieged
nations around the world. Every year, forests are burned by wildfires, causing
vast amounts of carbon dioxide to be released into the atmosphere, contributing
to climate change. There is a need for a system which prevents, detects, and
suppresses wildfires. The AI based Wildfire Prevention, Detection and
Suppression System (WPDSS) is a novel, fully automated, end to end, AI based
solution to effectively predict hotspots and detect wildfires, deploy drones to
spray fire retardant, preventing and suppressing wildfires. WPDSS consists of
four steps. 1. Preprocessing: WPDSS loads real time satellite data from NASA
and meteorological data from NOAA of vegetation, temperature, precipitation,
wind, soil moisture, and land cover for prevention. For detection, it loads the
real time data of Land Cover, Humidity, Temperature, Vegetation, Burned Area
Index, Ozone, and CO2. It uses the process of masking to eliminate not hotspots
and not wildfires such as water bodies, and rainfall. 2. Learning: The AI model
consists of a random forest classifier, which is trained using a labeled
dataset of hotspots and wildfires and not hotspots and not wildfires. 3.
Identification of hotspots and wildfires: WPDSS runs the real time data through
the model to automatically identify hotspots and wildfires. 4. Drone
deployment: The drone flies to the identified hotspot or wildfire location.
WPDSS attained a 98.6% accuracy in identifying hotspots and a 98.7% accuracy in
detecting wildfires. WPDSS will reduce the impacts of climate change, protect
ecosystems and biodiversity, avert huge economic losses, and save human lives.
The power of WPDSS developed can be applied to any location globally to prevent
and suppress wildfires, reducing climate change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shroff_P/0/1/0/all/0/1&quot;&gt;Prisha Shroff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07000">
<title>Alignment for Honesty. (arXiv:2312.07000v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07000</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has made significant strides in applying alignment techniques
to enhance the helpfulness and harmlessness of large language models (LLMs) in
accordance with human intentions. In this paper, we argue for the importance of
alignment for honesty, ensuring that LLMs proactively refuse to answer
questions when they lack knowledge, while still not being overly conservative.
However, a pivotal aspect of alignment for honesty involves discerning the
limits of an LLM&apos;s knowledge, which is far from straightforward. This challenge
demands comprehensive solutions in terms of metric development, benchmark
creation, and training methodologies. In this paper, we address these
challenges by first establishing a precise problem definition and defining
``honesty&apos;&apos; inspired by the Analects of Confucius. This serves as a cornerstone
for developing metrics that effectively measure an LLM&apos;s honesty by quantifying
its progress post-alignment. Furthermore, we introduce a flexible training
framework which is further instantiated by several efficient fine-tuning
techniques that emphasize honesty without sacrificing performance on other
tasks. Our extensive experiments reveal that these aligned models show a marked
increase in honesty, as indicated by our proposed metrics. We open-source a
wealth of resources to facilitate future research at
https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned
models, training and evaluation datasets for honesty alignment, concept
glossary, as well as all relevant source code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chern_E/0/1/0/all/0/1&quot;&gt;Ethan Chern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengfei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07003">
<title>RACER: Rational Artificial Intelligence Car-following-model Enhanced by Reality. (arXiv:2312.07003v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07003</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces RACER, the Rational Artificial Intelligence
Car-following model Enhanced by Reality, a cutting-edge deep learning
car-following model, that satisfies partial derivative constraints, designed to
predict Adaptive Cruise Control (ACC) driving behavior while staying
theoretically feasible. Unlike conventional models, RACER effectively
integrates Rational Driving Constraints (RDCs), crucial tenets of actual
driving, resulting in strikingly accurate and realistic predictions. Against
established models like the Optimal Velocity Relative Velocity (OVRV), a
car-following Neural Network (NN), and a car-following Physics-Informed Neural
Network (PINN), RACER excels across key metrics, such as acceleration,
velocity, and spacing. Notably, it displays a perfect adherence to the RDCs,
registering zero violations, in stark contrast to other models. This study
highlights the immense value of incorporating physical constraints within AI
models, especially for augmenting safety measures in transportation. It also
paves the way for future research to test these models against human driving
data, with the potential to guide safer and more rational driving behavior. The
versatility of the proposed model, including its potential to incorporate
additional derivative constraints and broader architectural applications,
enhances its appeal and broadens its impact within the scientific community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halatsis_A/0/1/0/all/0/1&quot;&gt;Alexander Halatsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1&quot;&gt;Raphael Stern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07006">
<title>Mixed Pseudo Labels for Semi-Supervised Object Detection. (arXiv:2312.07006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07006</link>
<description rdf:parseType="Literal">&lt;p&gt;While the pseudo-label method has demonstrated considerable success in
semi-supervised object detection tasks, this paper uncovers notable limitations
within this approach. Specifically, the pseudo-label method tends to amplify
the inherent strengths of the detector while accentuating its weaknesses, which
is manifested in the missed detection of pseudo-labels, particularly for small
and tail category objects. To overcome these challenges, this paper proposes
Mixed Pseudo Labels (MixPL), consisting of Mixup and Mosaic for pseudo-labeled
data, to mitigate the negative impact of missed detections and balance the
model&apos;s learning across different object scales. Additionally, the model&apos;s
detection performance on tail categories is improved by resampling labeled data
with relevant instances. Notably, MixPL consistently improves the performance
of various detectors and obtains new state-of-the-art results with Faster
R-CNN, FCOS, and DINO on COCO-Standard and COCO-Full benchmarks. Furthermore,
MixPL also exhibits good scalability on large models, improving DINO Swin-L by
2.5% mAP and achieving nontrivial new records (60.2% mAP) on the COCO val2017
benchmark without extra annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07021">
<title>Transferring Modality-Aware Pedestrian Attentive Learning Visible-Infrared Person Re-identification. (arXiv:2312.07021v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07021</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible-infrared person re-identification (VI-ReID) aims to search the same
pedestrian of interest across visible and infrared modalities. Existing models
mainly focus on compensating for modality-specific information to reduce
modality variation. However, these methods often lead to a higher computational
overhead and may introduce interfering information when generating the
corresponding images or features. To address this issue, it is critical to
leverage pedestrian-attentive features and learn modality-complete and
-consistent representation. In this paper, a novel Transferring Modality-Aware
Pedestrian Attentive Learning (TMPA) model is proposed, focusing on the
pedestrian regions to efficiently compensate for missing modality-specific
features. Specifically, we propose a region-based data augmentation module
PedMix to enhance pedestrian region coherence by mixing the corresponding
regions from different modalities. A lightweight hybrid compensation module,
i.e., the Modality Feature Transfer (MFT), is devised to integrate cross
attention and convolution networks to fully explore the discriminative
modality-complete features with minimal computational overhead. Extensive
experiments conducted on the benchmark SYSU-MM01 and RegDB datasets
demonstrated the effectiveness of our proposed TMPA model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07025">
<title>Noise Distribution Decomposition based Multi-Agent Distributional Reinforcement Learning. (arXiv:2312.07025v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07025</link>
<description rdf:parseType="Literal">&lt;p&gt;Generally, Reinforcement Learning (RL) agent updates its policy by
repetitively interacting with the environment, contingent on the received
rewards to observed states and undertaken actions. However, the environmental
disturbance, commonly leading to noisy observations (e.g., rewards and states),
could significantly shape the performance of agent. Furthermore, the learning
performance of Multi-Agent Reinforcement Learning (MARL) is more susceptible to
noise due to the interference among intelligent agents. Therefore, it becomes
imperative to revolutionize the design of MARL, so as to capably ameliorate the
annoying impact of noisy rewards. In this paper, we propose a novel
decomposition-based multi-agent distributional RL method by approximating the
globally shared noisy reward by a Gaussian mixture model (GMM) and decomposing
it into the combination of individual distributional local rewards, with which
each agent can be updated locally through distributional RL. Moreover, a
diffusion model (DM) is leveraged for reward generation in order to mitigate
the issue of costly interaction expenditure for learning distributions.
Furthermore, the optimality of the distribution decomposition is theoretically
validated, while the design of loss function is carefully calibrated to avoid
the decomposition ambiguity. We also verify the effectiveness of the proposed
method through extensive simulation experiments with noisy rewards. Besides,
different risk-sensitive policies are evaluated in order to demonstrate the
superiority of distributional RL in different MARL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1&quot;&gt;Wei Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Baidi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rongpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_N/0/1/0/all/0/1&quot;&gt;Ning Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07028">
<title>Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models. (arXiv:2312.07028v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07028</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the challenging issue of aggressive fine-tuning encountered during
the process of transfer learning of pre-trained language models (PLMs) with
limited labeled downstream data. This problem primarily results in a decline in
performance on the subsequent task. Inspired by the adaptive boosting method in
traditional machine learning, we present an effective dynamic corrective
self-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our
technique involves performing a self-distillation mechanism where, at each
iteration, the student model actively adapts and corrects itself by dynamically
adjusting the weights assigned to individual data points. This iterative
self-correcting process significantly enhances the overall fine-tuning
capability of PLMs, leading to improved performance and robustness. We
conducted comprehensive evaluations using the GLUE benchmark demonstrating the
efficacy of our method in enhancing the fine-tuning process for various PLMs
across diverse downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amara_I/0/1/0/all/0/1&quot;&gt;Ibtihel Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vinija Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07035">
<title>HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts. (arXiv:2312.07035v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07035</link>
<description rdf:parseType="Literal">&lt;p&gt;By routing input tokens to only a few split experts, Sparse
Mixture-of-Experts has enabled efficient training of large language models.
Recent findings suggest that fixing the routers can achieve competitive
performance by alleviating the collapsing problem, where all experts eventually
learn similar representations. However, this strategy has two key limitations:
(i) the policy derived from random routers might be sub-optimal, and (ii) it
requires extensive resources during training and evaluation, leading to limited
efficiency gains. This work introduces \HyperRout, which dynamically generates
the router&apos;s parameters through a fixed hypernetwork and trainable embeddings
to achieve a balance between training the routers and freezing them to learn an
improved routing policy. Extensive experiments across a wide range of tasks
demonstrate the superior performance and efficiency gains of \HyperRouter
compared to existing routing methods. Our implementation is publicly available
at {\url{{https://github.com/giangdip2410/HyperRouter}}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_G/0/1/0/all/0/1&quot;&gt;Giang Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1&quot;&gt;Khiem Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;TrungTin Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thanh-Nam Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Bint T. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasamy_S/0/1/0/all/0/1&quot;&gt;Savitha Ramasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1&quot;&gt;Steven Hoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07040">
<title>Patch-MI: Enhancing Model Inversion Attacks via Patch-Based Reconstruction. (arXiv:2312.07040v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07040</link>
<description rdf:parseType="Literal">&lt;p&gt;Model inversion (MI) attacks aim to reveal sensitive information in training
datasets by solely accessing model weights. Generative MI attacks, a prominent
strand in this field, utilize auxiliary datasets to recreate target data
attributes, restricting the images to remain photo-realistic, but their success
often depends on the similarity between auxiliary and target datasets. If the
distributions are dissimilar, existing MI attack attempts frequently fail,
yielding unrealistic or target-unrelated results. In response to these
challenges, we introduce a groundbreaking approach named Patch-MI, inspired by
jigsaw puzzle assembly. To this end, we build upon a new probabilistic
interpretation of MI attacks, employing a generative adversarial network
(GAN)-like framework with a patch-based discriminator. This approach allows the
synthesis of images that are similar to the target dataset distribution, even
in cases of dissimilar auxiliary dataset distribution. Moreover, we artfully
employ a random transformation block, a sophisticated maneuver that crafts
generalized images, thus enhancing the efficacy of the target classifier. Our
numerical and graphical findings demonstrate that Patch-MI surpasses existing
generative MI methods in terms of accuracy, marking significant advancements
while preserving comparable statistical dataset quality. For reproducibility of
our results, we make our source code publicly available in
https://github.com/jonggyujang0123/Patch-Attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jonggyu Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Hyeonsu Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hyun Jong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07043">
<title>The Complexity of Envy-Free Graph Cutting. (arXiv:2312.07043v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2312.07043</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of fairly dividing a set of heterogeneous divisible
resources among agents with different preferences. We focus on the setting
where the resources correspond to the edges of a connected graph, every agent
must be assigned a connected piece of this graph, and the fairness notion
considered is the classical envy freeness. The problem is NP-complete, and we
analyze its complexity with respect to two natural complexity measures: the
number of agents and the number of edges in the graph. While the problem
remains NP-hard even for instances with 2 agents, we provide a dichotomy
characterizing the complexity of the problem when the number of agents is
constant based on structural properties of the graph. For the latter case, we
design a polynomial-time algorithm when the graph has a constant number of
edges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deligkas_A/0/1/0/all/0/1&quot;&gt;Argyrios Deligkas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiben_E/0/1/0/all/0/1&quot;&gt;Eduard Eiben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamm_T/0/1/0/all/0/1&quot;&gt;Thekla Hamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordyniak_S/0/1/0/all/0/1&quot;&gt;Sebastian Ordyniak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07055">
<title>Communication Cost Reduction for Subgraph Counting under Local Differential Privacy via Hash Functions. (arXiv:2312.07055v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.07055</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest the use of hash functions to cut down the communication costs when
counting subgraphs under edge local differential privacy. While various
algorithms exist for computing graph statistics, including the count of
subgraphs, under the edge local differential privacy, many suffer with high
communication costs, making them less efficient for large graphs. Though data
compression is a typical approach in differential privacy, its application in
local differential privacy requires a form of compression that every node can
reproduce. In our study, we introduce linear congruence hashing. With a
sampling rate of $s$, our method can cut communication costs by a factor of
$s^2$, albeit at the cost of increasing variance in the published graph
statistic by a factor of $s$. The experimental results indicate that, when
matched for communication costs, our method achieves a reduction in the
$\ell_2$-error for triangle counts by up to 1000 times compared to the
performance of leading algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hillebrand_Q/0/1/0/all/0/1&quot;&gt;Quentin Hillebrand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suppakitpaisarn_V/0/1/0/all/0/1&quot;&gt;Vorapong Suppakitpaisarn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1&quot;&gt;Tetsuo Shibuya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07059">
<title>LSTM-CNN Network for Audio Signature Analysis in Noisy Environments. (arXiv:2312.07059v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.07059</link>
<description rdf:parseType="Literal">&lt;p&gt;There are multiple applications to automatically count people and specify
their gender at work, exhibitions, malls, sales, and industrial usage. Although
current speech detection methods are supposed to operate well, in most
situations, in addition to genders, the number of current speakers is unknown
and the classification methods are not suitable due to many possible classes.
In this study, we focus on a long-short-term memory convolutional neural
network (LSTM-CNN) to extract time and / or frequency-dependent features of the
sound data to estimate the number / gender of simultaneous active speakers at
each frame in noisy environments. Considering the maximum number of speakers as
10, we have utilized 19000 audio samples with diverse combinations of males,
females, and background noise in public cities, industrial situations, malls,
exhibitions, workplaces, and nature for learning purposes. This proof of
concept shows promising performance with training/validation MSE values of
about 0.019/0.017 in detecting count and gender.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damacharla_P/0/1/0/all/0/1&quot;&gt;Praveen Damacharla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabalipanah_H/0/1/0/all/0/1&quot;&gt;Hamid Rajabalipanah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakheri_M/0/1/0/all/0/1&quot;&gt;Mohammad Hosein Fakheri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07069">
<title>Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications. (arXiv:2312.07069v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07069</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the challenges inherent to Large Language Models
(LLMs) like GPT-4, particularly their propensity for hallucinations, logic
mistakes, and incorrect conclusions when tasked with answering complex
questions. The capacity of LLMs to present erroneous answers in a coherent and
semantically rigorous manner further complicates the detection of factual
inaccuracies. This issue is especially pronounced in fields that require
specialized expertise. Our work delves into these challenges, aiming to enhance
the understanding and mitigation of such errors, thereby contributing to the
improvement of LLM accuracy and reliability in scientific and other specialized
domains. Our findings reveal a non-linear relationship between the context&apos;s
relevancy and the answers&apos; measured quality. In addition, we demonstrate that
with the correct calibration, it is possible to automate the grading procedure
-- a finding suggesting that, at least to some degree, the LLMs can be used to
self-examine the quality of their own performance. Finally, we describe an
experimental platform that can be seen as a proof-of-concept of the techniques
described in this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haoran Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maravi_A/0/1/0/all/0/1&quot;&gt;Anurag Maravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abram_M/0/1/0/all/0/1&quot;&gt;Marcin Abram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07086">
<title>Navigating the generative AI era: Introducing the AI assessment scale for ethical GenAI assessment. (arXiv:2312.07086v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07086</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in Generative Artificial Intelligence (GenAI) have
created a paradigm shift in multiple areas of society, and the use of these
technologies is likely to become a defining feature of education in coming
decades. GenAI offers transformative pedagogical opportunities, while
simultaneously posing ethical and academic challenges. Against this backdrop,
we outline a practical, simple, and sufficiently comprehensive tool to allow
for the integration of GenAI tools into educational assessment: the AI
Assessment Scale (AIAS). The AIAS empowers educators to select the appropriate
level of GenAI usage in assessments based on the learning outcomes they seek to
address. The AIAS offers greater clarity and transparency for students and
educators, provides a fair and equitable policy tool for institutions to work
with, and offers a nuanced approach which embraces the opportunities of GenAI
while recognising that there are instances where such tools may not be
pedagogically appropriate or necessary. By adopting a practical, flexible
approach that can be implemented quickly, the AIAS can form a much-needed
starting point to address the current uncertainty and anxiety regarding GenAI
in education. As a secondary objective, we engage with the current literature
and advocate for a refocused discourse on GenAI tools in education, one which
foregrounds how technologies can help support and enhance teaching and
learning, which contrasts with the current focus on GenAI as a facilitator of
academic misconduct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perkins_M/0/1/0/all/0/1&quot;&gt;Mike Perkins&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furze_L/0/1/0/all/0/1&quot;&gt;Leon Furze&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roe_J/0/1/0/all/0/1&quot;&gt;Jasper Roe&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacVaugh_J/0/1/0/all/0/1&quot;&gt;Jason MacVaugh&lt;/a&gt; (1) ((1) British University Vietnam, (2) Deakin University, (3) James Cook University Singapore)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07087">
<title>Toward Robustness in Multi-label Classification: A Data Augmentation Strategy against Imbalance and Noise. (arXiv:2312.07087v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07087</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label classification poses challenges due to imbalanced and noisy
labels in training data. We propose a unified data augmentation method, named
BalanceMix, to address these challenges. Our approach includes two samplers for
imbalanced labels, generating minority-augmented instances with high diversity.
It also refines multi-labels at the label-wise granularity, categorizing noisy
labels as clean, re-labeled, or ambiguous for robust optimization. Extensive
experiments on three benchmark datasets demonstrate that BalanceMix outperforms
existing state-of-the-art methods. We release the code at
https://github.com/DISL-Lab/BalanceMix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hwanjun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minseok Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae-Gil Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07088">
<title>BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction. (arXiv:2312.07088v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07088</link>
<description rdf:parseType="Literal">&lt;p&gt;Canonical relation extraction aims to extract relational triples from
sentences, where the triple elements (entity pairs and their relationship) are
mapped to the knowledge base. Recently, methods based on the encoder-decoder
architecture are proposed and achieve promising results. However, these methods
cannot well utilize the entity information, which is merely used as augmented
training data. Moreover, they are incapable of representing novel entities,
since no embeddings have been learned for them. In this paper, we propose a
novel framework, Bi-Encoder-Decoder (BED), to solve the above issues.
Specifically, to fully utilize entity information, we employ an encoder to
encode semantics of this information, leading to high-quality entity
representations. For novel entities, given a trained entity encoder, their
representations can be easily generated. Experimental results on two datasets
show that, our method achieves a significant performance improvement over the
previous state-of-the-art and handle novel entities well without retraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nantao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1&quot;&gt;Siyu Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xinyu Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07101">
<title>Meta-survey on outlier and anomaly detection. (arXiv:2312.07101v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07101</link>
<description rdf:parseType="Literal">&lt;p&gt;The impact of outliers and anomalies on model estimation and data processing
is of paramount importance, as evidenced by the extensive body of research
spanning various fields over several decades: thousands of research papers have
been published on the subject. As a consequence, numerous reviews, surveys, and
textbooks have sought to summarize the existing literature, encompassing a wide
range of methods from both the statistical and data mining communities. While
these endeavors to organize and summarize the research are invaluable, they
face inherent challenges due to the pervasive nature of outliers and anomalies
in all data-intensive applications, irrespective of the specific application
field or scientific discipline. As a result, the resulting collection of papers
remains voluminous and somewhat heterogeneous. To address the need for
knowledge organization in this domain, this paper implements the first
systematic meta-survey of general surveys and reviews on outlier and anomaly
detection. Employing a classical systematic survey approach, the study collects
nearly 500 papers using two specialized scientific search engines. From this
comprehensive collection, a subset of 56 papers that claim to be general
surveys on outlier detection is selected using a snowball search technique to
enhance field coverage. A meticulous quality assessment phase further refines
the selection to a subset of 25 high-quality general surveys. Using this
curated collection, the paper investigates the evolution of the outlier
detection field over a 20-year period, revealing emerging themes and methods.
Furthermore, an analysis of the surveys sheds light on the survey writing
practices adopted by scholars from different communities who have contributed
to this field. Finally, the paper delves into several topics where consensus
has emerged from the literature. These include taxonomies of outlier types,
challenges posed by high-dimensional data, the importance of anomaly scores,
the impact of learning conditions, difficulties in benchmarking, and the
significance of neural networks. Non-consensual aspects are also discussed,
particularly the distinction between local and global outliers and the
challenges in organizing detection methods into meaningful taxonomies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olteanu_M/0/1/0/all/0/1&quot;&gt;Madalina Olteanu&lt;/a&gt; (CEREMADE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1&quot;&gt;Fabrice Rossi&lt;/a&gt; (CEREMADE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1&quot;&gt;Florian Yger&lt;/a&gt; (MILES, LAMSADE)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07102">
<title>Calibration-free quantitative phase imaging in multi-core fiber endoscopes using end-to-end deep learning. (arXiv:2312.07102v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2312.07102</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative phase imaging (QPI) through multi-core fibers (MCFs) has been an
emerging in vivo label-free endoscopic imaging modality with minimal
invasiveness. However, the computational demands of conventional iterative
phase retrieval algorithms have limited their real-time imaging potential. We
demonstrate a learning-based MCF phase imaging method, that significantly
reduced the phase reconstruction time to 5.5 ms, enabling video-rate imaging at
181 fps. Moreover, we introduce an innovative optical system that automatically
generated the first open-source dataset tailored for MCF phase imaging,
comprising 50,176 paired speckle and phase images. Our trained deep neural
network (DNN) demonstrates robust phase reconstruction performance in
experiments with a mean fidelity of up to 99.8\%. Such an efficient fiber phase
imaging approach can broaden the applications of QPI in hard-to-reach areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiawei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koukourakis_N/0/1/0/all/0/1&quot;&gt;Nektarios Koukourakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Czarske_J/0/1/0/all/0/1&quot;&gt;Juergen W. Czarske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07104">
<title>Efficiently Programming Large Language Models using SGLang. (arXiv:2312.07104v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07104</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are increasingly used for complex tasks
requiring multiple chained generation calls, advanced prompting techniques,
control flow, and interaction with external environments. However, efficient
systems for programming and executing these applications are lacking. To bridge
this gap, we introduce SGLang, a Structured Generation Language for LLMs.
SGLang is designed for the efficient programming of LLMs and incorporates
primitives for common LLM programming patterns. We have implemented SGLang as a
domain-specific language embedded in Python, and we developed an interpreter, a
compiler, and a high-performance runtime for SGLang. These components work
together to enable optimizations such as parallelism, batching, caching,
sharing, and other compilation techniques. Additionally, we propose
RadixAttention, a novel technique that maintains a Least Recently Used (LRU)
cache of the Key-Value (KV) cache for all requests in a radix tree, enabling
automatic KV cache reuse across multiple generation calls at runtime. SGLang
simplifies the writing of LLM programs and boosts execution efficiency. Our
experiments demonstrate that SGLang can speed up common LLM tasks by up to 5x,
while reducing code complexity and enhancing control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1&quot;&gt;Liangsheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jeff Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chuyue Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cody Hao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1&quot;&gt;Shiyi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozyrakis_C/0/1/0/all/0/1&quot;&gt;Christos Kozyrakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1&quot;&gt;Clark Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Ying Sheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07107">
<title>The Logic of Doxastic Strategies. (arXiv:2312.07107v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07107</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world situations, there is often not enough information to know
that a certain strategy will succeed in achieving the goal, but there is a good
reason to believe that it will. The paper introduces the term ``doxastic&apos;&apos; for
such strategies.
&lt;/p&gt;
&lt;p&gt;The main technical contribution is a sound and complete logical system that
describes the interplay between doxastic strategy and belief modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junli Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumov_P/0/1/0/all/0/1&quot;&gt;Pavel Naumov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07112">
<title>Generating High-Resolution Regional Precipitation Using Conditional Diffusion Model. (arXiv:2312.07112v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07112</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate downscaling is a crucial technique within climate research, serving
to project low-resolution (LR) climate data to higher resolutions (HR).
Previous research has demonstrated the effectiveness of deep learning for
downscaling tasks. However, most deep learning models for climate downscaling
may not perform optimally for high scaling factors (i.e., 4x, 8x) due to their
limited ability to capture the intricate details required for generating HR
climate data. Furthermore, climate data behaves differently from image data,
necessitating a nuanced approach when employing deep generative models. In
response to these challenges, this paper presents a deep generative model for
downscaling climate data, specifically precipitation on a regional scale. We
employ a denoising diffusion probabilistic model (DDPM) conditioned on multiple
LR climate variables. The proposed model is evaluated using precipitation data
from the Community Earth System Model (CESM) v1.2.2 simulation. Our results
demonstrate significant improvements over existing baselines, underscoring the
effectiveness of the conditional diffusion model in downscaling climate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shidqi_N/0/1/0/all/0/1&quot;&gt;Naufal Shidqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Chaeyoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungwon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_E/0/1/0/all/0/1&quot;&gt;Elke Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nellikkattil_A/0/1/0/all/0/1&quot;&gt;Arjun Babu Nellikkattil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07122">
<title>Neural Reasoning About Agents&apos; Goals, Preferences, and Actions. (arXiv:2312.07122v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07122</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Intuitive Reasoning Network (IRENE) - a novel neural model for
intuitive psychological reasoning about agents&apos; goals, preferences, and actions
that can generalise previous experiences to new situations. IRENE combines a
graph neural network for learning agent and world state representations with a
transformer to encode the task context. When evaluated on the challenging Baby
Intuitions Benchmark, IRENE achieves new state-of-the-art performance on three
out of its five tasks - with up to 48.9% improvement. In contrast to existing
methods, IRENE is able to bind preferences to specific agents, to better
distinguish between rational and irrational agents, and to better understand
the role of blocking obstacles. We also investigate, for the first time, the
influence of the training tasks on test performance. Our analyses demonstrate
the effectiveness of IRENE in combining prior knowledge gained during training
for unseen evaluation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bortoletto_M/0/1/0/all/0/1&quot;&gt;Matteo Bortoletto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulling_A/0/1/0/all/0/1&quot;&gt;Andreas Bulling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07130">
<title>Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the Censorship of Text-to-Image Generation Model. (arXiv:2312.07130v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07130</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generative models offer many innovative services but also raise
ethical concerns due to their potential to generate unethical images. Most
publicly available text-to-image models employ safety filters to prevent
unintended generation intents. In this work, we introduce the
Divide-and-Conquer Attack to circumvent the safety filters of state-of-the-art
text-to-image models. Our attack leverages LLMs as agents for text
transformation, creating adversarial prompts from sensitive ones. We have
developed effective helper prompts that enable LLMs to break down sensitive
drawing prompts into multiple harmless descriptions, allowing them to bypass
safety filters while still generating sensitive images. This means that the
latent harmful meaning only becomes apparent when all individual elements are
drawn together. Our evaluation demonstrates that our attack successfully
circumvents the closed-box safety filter of SOTA DALLE-3 integrated natively
into ChatGPT to generate unethical images. This approach, which essentially
uses LLM-generated adversarial prompts against GPT-4-assisted DALLE-3, is akin
to using one&apos;s own spear to breach their shield. It could have more severe
security implications than previous manual crafting or iterative model querying
methods, and we hope it stimulates more attention towards similar efforts. Our
code and data are available at:
https://github.com/researchcode001/Divide-and-Conquer-Attack
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yimo Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huangxun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07144">
<title>The Parameterized Complexity of Coordinated Motion Planning. (arXiv:2312.07144v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2312.07144</link>
<description rdf:parseType="Literal">&lt;p&gt;In Coordinated Motion Planning (CMP), we are given a rectangular-grid on
which $k$ robots occupy $k$ distinct starting gridpoints and need to reach $k$
distinct destination gridpoints. In each time step, any robot may move to a
neighboring gridpoint or stay in its current gridpoint, provided that it does
not collide with other robots. The goal is to compute a schedule for moving the
$k$ robots to their destinations which minimizes a certain objective target -
prominently the number of time steps in the schedule, i.e., the makespan, or
the total length traveled by the robots. We refer to the problem arising from
minimizing the former objective target as CMP-M and the latter as CMP-L. Both
CMP-M and CMP-L are fundamental problems that were posed as the computational
geometry challenge of SoCG 2021, and CMP also embodies the famous
$(n^2-1)$-puzzle as a special case.
&lt;/p&gt;
&lt;p&gt;In this paper, we settle the parameterized complexity of CMP-M and CMP-L with
respect to their two most fundamental parameters: the number of robots, and the
objective target. We develop a new approach to establish the fixed-parameter
tractability of both problems under the former parameterization that relies on
novel structural insights into optimal solutions to the problem. When
parameterized by the objective target, we show that CMP-L remains
fixed-parameter tractable while CMP-M becomes para-NP-hard. The latter result
is noteworthy, not only because it improves the previously-known boundaries of
intractability for the problem, but also because the underlying reduction
allows us to establish - as a simpler case - the NP-hardness of the classical
Vertex Disjoint and Edge Disjoint Paths problems with constant path-lengths on
grids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiben_E/0/1/0/all/0/1&quot;&gt;Eduard Eiben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanj_I/0/1/0/all/0/1&quot;&gt;Iyad Kanj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07158">
<title>Cost Aware Untargeted Poisoning Attack against Graph Neural Networks,. (arXiv:2312.07158v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07158</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have become widely used in the field of graph
mining. However, these networks are vulnerable to structural perturbations.
While many research efforts have focused on analyzing vulnerability through
poisoning attacks, we have identified an inefficiency in current attack losses.
These losses steer the attack strategy towards modifying edges targeting
misclassified nodes or resilient nodes, resulting in a waste of structural
adversarial perturbation. To address this issue, we propose a novel attack loss
framework called the Cost Aware Poisoning Attack (CA-attack) to improve the
allocation of the attack budget by dynamically considering the classification
margins of nodes. Specifically, it prioritizes nodes with smaller positive
margins while postponing nodes with negative margins. Our experiments
demonstrate that the proposed CA-attack significantly enhances existing attack
strategies
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yuwei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yuni Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yulin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kai Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07165">
<title>Language-Guided Transformer for Federated Multi-Label Classification. (arXiv:2312.07165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07165</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is an emerging paradigm that enables multiple users
to collaboratively train a robust model in a privacy-preserving manner without
sharing their private data. Most existing approaches of FL only consider
traditional single-label image classification, ignoring the impact when
transferring the task to multi-label image classification. Nevertheless, it is
still challenging for FL to deal with user heterogeneity in their local data
distribution in the real-world FL scenario, and this issue becomes even more
severe in multi-label image classification. Inspired by the recent success of
Transformers in centralized settings, we propose a novel FL framework for
multi-label classification. Since partial label correlation may be observed by
local clients during training, direct aggregation of locally updated models
would not produce satisfactory performances. Thus, we propose a novel FL
framework of Language-Guided Transformer (FedLGT) to tackle this challenging
task, which aims to exploit and transfer knowledge across different clients for
learning a robust global model. Through extensive experiments on various
multi-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is
able to achieve satisfactory performance and outperforms standard FL techniques
under multi-label FL scenarios. Code is available at
https://github.com/Jack24658735/FedLGT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1&quot;&gt;I-Jieh Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Ci-Siang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fu-En Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07168">
<title>Equivariant Flow Matching with Hybrid Probability Transport. (arXiv:2312.07168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07168</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of 3D molecules requires simultaneously deciding the
categorical features~(atom types) and continuous features~(atom coordinates).
Deep generative models, especially Diffusion Models (DMs), have demonstrated
effectiveness in generating feature-rich geometries. However, existing DMs
typically suffer from unstable probability dynamics with inefficient sampling
speed. In this paper, we introduce geometric flow matching, which enjoys the
advantages of both equivariant modeling and stabilized probability dynamics.
More specifically, we propose a hybrid probability path where the coordinates
probability path is regularized by an equivariant optimal transport, and the
information between different modalities is aligned. Experimentally, the
proposed method could consistently achieve better performance on multiple
molecule generation benchmarks with 4.75$\times$ speed up of sampling on
average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1&quot;&gt;Jingjing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Ziyao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Ying Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07175">
<title>Instrumental Variable Estimation for Causal Inference in Longitudinal Data with Time-Dependent Latent Confounders. (arXiv:2312.07175v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07175</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference from longitudinal observational data is a challenging
problem due to the difficulty in correctly identifying the time-dependent
confounders, especially in the presence of latent time-dependent confounders.
Instrumental variable (IV) is a powerful tool for addressing the latent
confounders issue, but the traditional IV technique cannot deal with latent
time-dependent confounders in longitudinal studies. In this work, we propose a
novel Time-dependent Instrumental Factor Model (TIFM) for time-varying causal
effect estimation from data with latent time-dependent confounders. At each
time-step, the proposed TIFM method employs the Recurrent Neural Network (RNN)
architecture to infer latent IV, and then uses the inferred latent IV factor
for addressing the confounding bias caused by the latent time-dependent
confounders. We provide a theoretical analysis for the proposed TIFM method
regarding causal effect estimation in longitudinal data. Extensive evaluation
with synthetic datasets demonstrates the effectiveness of TIFM in addressing
causal effect estimation over time. We further apply TIFM to a climate dataset
to showcase the potential of the proposed method in tackling real-world
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Debo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiuyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jixue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wentao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thuc Duy Le&lt;/a&gt; (UniSA STEM, University of South Australia, Adelaide, SA, Australia)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07178">
<title>Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms. (arXiv:2312.07178v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07178</link>
<description rdf:parseType="Literal">&lt;p&gt;Many applications in Reinforcement Learning (RL) usually have noise or
stochasticity present in the environment. Beyond their impact on learning,
these uncertainties lead the exact same policy to perform differently, i.e.
yield different return, from one roll-out to another. Common evaluation
procedures in RL summarise the consequent return distributions using solely the
expected return, which does not account for the spread of the distribution. Our
work defines this spread as the policy reproducibility: the ability of a policy
to obtain similar performance when rolled out many times, a crucial property in
some real-world applications. We highlight that existing procedures that only
use the expected return are limited on two fronts: first an infinite number of
return distributions with a wide range of performance-reproducibility
trade-offs can have the same expected return, limiting its effectiveness when
used for comparing policies; second, the expected return metric does not leave
any room for practitioners to choose the best trade-off value for considered
applications. In this work, we address these limitations by recommending the
use of Lower Confidence Bound, a metric taken from Bayesian optimisation that
provides the user with a preference parameter to choose a desired
performance-reproducibility trade-off. We also formalise and quantify policy
reproducibility, and demonstrate the benefit of our metrics using extensive
experiments of popular RL algorithms on common uncertain RL tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flageat_M/0/1/0/all/0/1&quot;&gt;Manon Flageat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Bryan Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07212">
<title>More than Vanilla Fusion: a Simple, Decoupling-free, Attention Module for Multimodal Fusion Based on Signal Theory. (arXiv:2312.07212v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2312.07212</link>
<description rdf:parseType="Literal">&lt;p&gt;The vanilla fusion methods still dominate a large percentage of mainstream
audio-visual tasks. However, the effectiveness of vanilla fusion from a
theoretical perspective is still worth discussing. Thus, this paper reconsiders
the signal fused in the multimodal case from a bionics perspective and proposes
a simple, plug-and-play, attention module for vanilla fusion based on
fundamental signal theory and uncertainty theory. In addition, previous work on
multimodal dynamic gradient modulation still relies on decoupling the
modalities. So, a decoupling-free gradient modulation scheme has been designed
in conjunction with the aforementioned attention module, which has various
advantages over the decoupled one. Experiment results show that just a few
lines of code can achieve up to 2.0% performance improvements to several
multimodal classification methods. Finally, quantitative evaluation of other
fusion tasks reveals the potential for additional application scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peiwen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Donghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honggang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07213">
<title>Brain-inspired Computing Based on Machine Learning And Deep Learning:A Review. (arXiv:2312.07213v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07213</link>
<description rdf:parseType="Literal">&lt;p&gt;The continuous development of artificial intelligence has a profound impact
on biomedical research and other fields.Brain-inspired computing is an
important intersection of multimodal technology and biomedical field. This
paper provides a comprehensive review of machine learning (ML) and deep
learning (DL) models in brain-inspired computing, tracking their evolution,
application value, challenges, and potential research trajectories. First, the
basic concepts and development history are reviewed, and their evolution is
divided into two stages: recent machine learning and current deep learning,
emphasizing the importance of each stage in the research state of
brain-inspired computing. In addition, the latest progress and key techniques
of deep learning in different tasks of brain-inspired computing are introduced
from six perspectives. Despite significant progress, challenges remain in
making full use of its capabilities. This paper aims to provide a comprehensive
review of brain-inspired computing models based on machine learning and deep
learning, highlighting their potential in various applications and providing a
valuable reference for future academic research. It can be accessed through the
following url: https://github.com/ultracoolHub/brain-inspired-computing
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bihui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sibo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07214">
<title>Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming. (arXiv:2312.07214v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.07214</link>
<description rdf:parseType="Literal">&lt;p&gt;In a rapidly evolving digital landscape autonomous tools and robots are
becoming commonplace. Recognizing the significance of this development, this
paper explores the integration of Large Language Models (LLMs) like Generative
pre-trained transformer (GPT) into human-robot teaming environments to
facilitate variable autonomy through the means of verbal human-robot
communication. In this paper, we introduce a novel framework for such a
GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality
(VR) setting. This system allows users to interact with robot agents through
natural language, each powered by individual GPT cores. By means of OpenAI&apos;s
function calling, we bridge the gap between unstructured natural language input
and structure robot actions. A user study with 12 participants explores the
effectiveness of GPT-4 and, more importantly, user strategies when being given
the opportunity to converse in natural language within a multi-robot
environment. Our findings suggest that users may have preconceived expectations
on how to converse with robots and seldom try to explore the actual language
and cognitive capabilities of their robot collaborators. Still, those users who
did explore where able to benefit from a much more natural flow of
communication and human-like back-and-forth. We provide a set of lessons
learned for future research and technical implementations of similar systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakhnati_Y/0/1/0/all/0/1&quot;&gt;Younes Lakhnati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascher_M/0/1/0/all/0/1&quot;&gt;Max Pascher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1&quot;&gt;Jens Gerken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07221">
<title>Transferring CLIP&apos;s Knowledge into Zero-Shot Point Cloud Semantic Segmentation. (arXiv:2312.07221v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07221</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional 3D segmentation methods can only recognize a fixed range of
classes that appear in the training set, which limits their application in
real-world scenarios due to the lack of generalization ability. Large-scale
visual-language pre-trained models, such as CLIP, have shown their
generalization ability in the zero-shot 2D vision tasks, but are still unable
to be applied to 3D semantic segmentation directly. In this work, we focus on
zero-shot point cloud semantic segmentation and propose a simple yet effective
baseline to transfer the visual-linguistic knowledge implied in CLIP to point
cloud encoder at both feature and output levels. Both feature-level and
output-level alignments are conducted between 2D and 3D encoders for effective
knowledge transfer. Concretely, a Multi-granularity Cross-modal Feature
Alignment (MCFA) module is proposed to align 2D and 3D features from global
semantic and local position perspectives for feature-level alignment. For the
output level, per-pixel pseudo labels of unseen classes are extracted using the
pre-trained CLIP model as supervision for the 3D segmentation model to mimic
the behavior of the CLIP image encoder. Extensive experiments are conducted on
two popular benchmarks of point cloud segmentation. Our method outperforms
significantly previous state-of-the-art methods under zero-shot setting (+29.2%
mIoU on SemanticKITTI and 31.8% mIoU on nuScenes), and further achieves
promising results in the annotation-free point cloud semantic segmentation
setting, showing its great potential for label-efficient learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaofei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yulu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1&quot;&gt;Kehua Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07231">
<title>Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation. (arXiv:2312.07231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07231</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Transformers have recently shown remarkable effectiveness in
generating high-quality 3D point clouds. However, training voxel-based
diffusion models for high-resolution 3D voxels remains prohibitively expensive
due to the cubic complexity of attention operators, which arises from the
additional dimension of voxels. Motivated by the inherent redundancy of 3D
compared to 2D, we propose FastDiT-3D, a novel masked diffusion transformer
tailored for efficient 3D point cloud generation, which greatly reduces
training costs. Specifically, we draw inspiration from masked autoencoders to
dynamically operate the denoising process on masked voxelized point clouds. We
also propose a novel voxel-aware masking strategy to adaptively aggregate
background/foreground information from voxelized point clouds. Our method
achieves state-of-the-art performance with an extreme masking ratio of nearly
99%. Moreover, to improve multi-category 3D generation, we introduce
Mixture-of-Expert (MoE) in 3D diffusion model. Each category can learn a
distinct diffusion path with different experts, relieving gradient conflict.
Experimental results on the ShapeNet dataset demonstrate that our method
achieves state-of-the-art high-fidelity and diverse 3D point cloud generation
performance. Our FastDiT-3D improves 1-Nearest Neighbor Accuracy and Coverage
metrics when generating 128-resolution voxel point clouds, using only 6.5% of
the original training cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07243">
<title>A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models. (arXiv:2312.07243v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07243</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the rapid progress and broad application of
diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as
solving an ordinary differential equation (ODE). Despite the promising
performance, the generation of DPMs usually consumes much time due to the large
number of function evaluations (NFE). Though recent works have accelerated the
sampling to around 20 steps with high-order solvers, the sample quality with
less than 10 NFE can still be improved. In this paper, we propose a unified
sampling framework (USF) to study the optional strategies for solver. Under
this framework, we further reveal that taking different solving strategies at
different timesteps may help further decrease the truncation error, and a
carefully designed \emph{solver schedule} has the potential to improve the
sample quality by a large margin. Therefore, we propose a new sampling
framework based on the exponential integral formulation that allows free
choices of solver strategy at each step and design specific decisions for the
framework. Moreover, we propose $S^3$, a predictor-based search method that
automatically optimizes the solver schedule to get a better time-quality
trade-off of sampling. We demonstrate that $S^3$ can find outstanding solver
schedules which outperform the state-of-the-art sampling methods on CIFAR-10,
CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID
with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA
method significantly. We further apply $S^3$ to Stable-Diffusion model and get
an acceleration ratio of 2$\times$, showing the feasibility of sampling in very
few steps without retraining the neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Enshu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xuefei Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huazhong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07248">
<title>Multi-Granularity Framework for Unsupervised Representation Learning of Time Series. (arXiv:2312.07248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07248</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning plays a critical role in the analysis of time series
data and has high practical value across a wide range of applications.
including trend analysis, time series data retrieval and forecasting. In
practice, data confusion is a significant issue as it can considerably impact
the effectiveness and accuracy of data analysis, machine learning models and
decision-making processes. In general, previous studies did not consider the
variability at various levels of granularity, thus resulting in inadequate
information utilization, which further exacerbated the issue of data confusion.
This paper proposes an unsupervised framework to realize multi-granularity
representation learning for time series. Specifically, we employed a
cross-granularity transformer to develop an association between fine- and
coarse-grained representations. In addition, we introduced a retrieval task as
an unsupervised training task to learn the multi-granularity representation of
time series. Moreover, a novel loss function was designed to obtain the
comprehensive multi-granularity representation of the time series via
unsupervised learning. The experimental results revealed that the proposed
framework demonstrates significant advantages over alternative representation
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chengyang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qiang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07250">
<title>Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning. (arXiv:2312.07250v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;We conduct investigations on clinical text machine translation by examining
multilingual neural network models using deep learning such as Transformer
based structures. Furthermore, to address the language resource imbalance
issue, we also carry out experiments using a transfer learning methodology
based on massive multilingual pre-trained language models (MMPLMs). The
experimental results on three subtasks including 1) clinical case (CC), 2)
clinical terminology (CT), and 3) ontological concept (OC) show that our models
achieved top-level performances in the ClinSpEn-2022 shared task on
English-Spanish clinical domain data. Furthermore, our expert-based human
evaluations demonstrate that the small-sized pre-trained language model (PLM)
won over the other two extra-large language models by a large margin, in the
clinical domain fine-tuning, which finding was never reported in the field.
Finally, the transfer learning method works well in our experimental setting
using the WMT21fb model to accommodate a new language space Spanish that was
not seen at the pre-training stage within WMT21fb itself, which deserves more
exploitation for clinical knowledge transformation, e.g. to investigate into
more languages. These research findings can shed some light on domain-specific
machine translation development, especially in clinical and healthcare fields.
Further research projects can be carried out based on our work to improve
healthcare text analytics and knowledge transformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1&quot;&gt;Serge Gladkoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1&quot;&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1&quot;&gt;Irina Sorokina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galiano_B/0/1/0/all/0/1&quot;&gt;Betty Galiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07271">
<title>Analyze the Robustness of Classifiers under Label Noise. (arXiv:2312.07271v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07271</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores the robustness of label noise classifiers, aiming to
enhance model resilience against noisy data in complex real-world scenarios.
Label noise in supervised learning, characterized by erroneous or imprecise
labels, significantly impairs model performance. This research focuses on the
increasingly pertinent issue of label noise&apos;s impact on practical applications.
Addressing the prevalent challenge of inaccurate training data labels, we
integrate adversarial machine learning (AML) and importance reweighting
techniques. Our approach involves employing convolutional neural networks (CNN)
as the foundational model, with an emphasis on parameter adjustment for
individual training samples. This strategy is designed to heighten the model&apos;s
focus on samples critically influencing performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1&quot;&gt;Cheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yixuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jiaqi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07302">
<title>From Knowledge Representation to Knowledge Organization and Back. (arXiv:2312.07302v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07302</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Representation (KR) and facet-analytical Knowledge Organization
(KO) have been the two most prominent methodologies of data and knowledge
modelling in the Artificial Intelligence community and the Information Science
community, respectively. KR boasts of a robust and scalable ecosystem of
technologies to support knowledge modelling while, often, underemphasizing the
quality of its models (and model-based data). KO, on the other hand, is less
technology-driven but has developed a robust framework of guiding principles
(canons) for ensuring modelling (and model-based data) quality. This paper
elucidates both the KR and facet-analytical KO methodologies in detail and
provides a functional mapping between them. Out of the mapping, the paper
proposes an integrated KO-enriched KR methodology with all the standard
components of a KR methodology plus the guiding canons of modelling quality
provided by KO. The practical benefits of the methodological integration has
been exemplified through a prominent case study of KR-based image annotation
exercise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1&quot;&gt;Mayukh Bagchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07305">
<title>SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion. (arXiv:2312.07305v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07305</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse attention as a efficient method can significantly decrease the
computation cost, but current sparse attention tend to rely on window self
attention which block the global information flow. For this problem, we present
Shifted Cross Chunk Attention (SCCA), using different KV shifting strategy to
extend respective field in each attention layer. Except, we combine Dilated
Attention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted
Dilated Attention(SDA). Both SCCA and SDA can accumulate attention results in
multi head attention to obtain approximate respective field in full attention.
In this paper, we conduct language modeling experiments using different pattern
of SCCA and combination of SCCA and SDA. The proposed shifted cross chunk
attention (SCCA) can effectively extend large language models (LLMs) to longer
context combined with Positional interpolation(PI) and LoRA than current sparse
attention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100.
This attention pattern can provide a Plug-and-play fine-tuning method to extend
model context while retaining their original architectures, and is compatible
with most existing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07311">
<title>Scalable Motion Style Transfer with Constrained Diffusion Generation. (arXiv:2312.07311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07311</link>
<description rdf:parseType="Literal">&lt;p&gt;Current training of motion style transfer systems relies on consistency
losses across style domains to preserve contents, hindering its scalable
application to a large number of domains and private data. Recent image
transfer works show the potential of independent training on each domain by
leveraging implicit bridging between diffusion models, with the content
preservation, however, limited to simple data patterns. We address this by
imposing biased sampling in backward diffusion while maintaining the domain
independence in the training stage. We construct the bias from the source
domain keyframes and apply them as the gradient of content constraints,
yielding a framework with keyframe manifold constraint gradients (KMCGs). Our
validation demonstrates the success of training separate models to transfer
between as many as ten dance motion styles. Comprehensive experiments find a
significant improvement in preserving motion contents in comparison to baseline
and ablative diffusion-based style transfer models. In addition, we perform a
human study for a subjective assessment of the quality of generated dance
motions. The results validate the competitiveness of KMCGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wenjie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;rten Bj&amp;#xf6;rkman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07337">
<title>RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation in Degenerated Environments. (arXiv:2312.07337v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.07337</link>
<description rdf:parseType="Literal">&lt;p&gt;The typical point cloud sampling methods used in state estimation for mobile
robots preserve a high level of point redundancy. The point redundancy slows
down the estimation pipeline and can make real-time estimation drift in
geometrically symmetrical and structureless environments. We propose a novel
point cloud sampling method that is capable of lowering the effects of
geometrical degeneracies by minimizing redundancy within the cloud. The
proposed method is an alternative to the commonly used sparsification methods
that normalize the density of points to comply with the constraints on the
real-time capabilities of a robot. In contrast to density normalization, our
method builds on the fact that linear and planar surfaces contain a high level
of redundancy propagated into iterative estimation pipelines. We define the
concept of gradient flow quantifying the surface underlying a point. We also
show that maximizing the entropy of the gradient flow minimizes point
redundancy for robot ego-motion estimation. We integrate the proposed method
into the point-based KISS-ICP and feature-based LOAM odometry pipelines and
evaluate it experimentally on KITTI, Hilti-Oxford, and custom datasets from
multirotor UAVs. The experiments show that the proposed sampling technique
outperforms state-of-the-art methods in well-conditioned as well as in
geometrically-degenerated settings, in both accuracy and speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petracek_P/0/1/0/all/0/1&quot;&gt;Pavel Petracek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexis_K/0/1/0/all/0/1&quot;&gt;Kostas Alexis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saska_M/0/1/0/all/0/1&quot;&gt;Martin Saska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07343">
<title>Can ChatGPT Play the Role of a Teaching Assistant in an Introductory Programming Course?. (arXiv:2312.07343v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.07343</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of Large language models (LLMs) is expected to have a major
impact on education. This paper explores the potential of using ChatGPT, an
LLM, as a virtual Teaching Assistant (TA) in an Introductory Programming
Course. We evaluate ChatGPT&apos;s capabilities by comparing its performance with
that of human TAs in some TA functions. The TA functions which we focus on
include (1) solving programming assignments, (2) grading student code
submissions, and (3) providing feedback to undergraduate students in an
introductory programming course. Firstly, we investigate how closely ChatGPT&apos;s
solutions align with those submitted by students. This analysis goes beyond
code correctness and also considers code quality. Secondly, we assess ChatGPT&apos;s
proficiency in grading student code submissions using a given grading rubric
and compare its performance with the grades assigned by human TAs. Thirdly, we
analyze the quality and relevance of the feedback provided by ChatGPT. This
evaluation considers how well ChatGPT addresses mistakes and offers suggestions
for improvement in student solutions from both code correctness and code
quality perspectives. We conclude with a discussion on the implications of
integrating ChatGPT into computing education for automated grading,
personalized learning experiences, and instructional support.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anishka/0/1/0/all/0/1&quot;&gt;Anishka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Atharva Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Nipun Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Dhruv Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalote_P/0/1/0/all/0/1&quot;&gt;Pankaj Jalote&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07352">
<title>CholecTrack20: A Dataset for Multi-Class Multiple Tool Tracking in Laparoscopic Surgery. (arXiv:2312.07352v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07352</link>
<description rdf:parseType="Literal">&lt;p&gt;Tool tracking in surgical videos is vital in computer-assisted intervention
for tasks like surgeon skill assessment, safety zone estimation, and
human-machine collaboration during minimally invasive procedures. The lack of
large-scale datasets hampers Artificial Intelligence implementation in this
domain. Current datasets exhibit overly generic tracking formalization, often
lacking surgical context: a deficiency that becomes evident when tools move out
of the camera&apos;s scope, resulting in rigid trajectories that hinder realistic
surgical representation. This paper addresses the need for a more precise and
adaptable tracking formalization tailored to the intricacies of endoscopic
procedures by introducing CholecTrack20, an extensive dataset meticulously
annotated for multi-class multi-tool tracking across three perspectives
representing the various ways of considering the temporal duration of a tool
trajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within
the camera&apos;s scope. The dataset comprises 20 laparoscopic videos with over
35,000 frames and 65,000 annotated tool instances with details on spatial
location, category, identity, operator, phase, and surgical visual conditions.
This detailed dataset caters to the evolving assistive requirements within a
procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1&quot;&gt;Chinedu Innocent Nwoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgohary_K/0/1/0/all/0/1&quot;&gt;Kareem Elgohary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1&quot;&gt;Anvita Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaid_F/0/1/0/all/0/1&quot;&gt;Fauzan Zaid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavanchy_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l L. Lavanchy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07368">
<title>Sequential Planning in Large Partially Observable Environments guided by LLMs. (arXiv:2312.07368v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07368</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential planning in large state space and action space quickly becomes
intractable due to combinatorial explosion of the search space. Heuristic
methods, like monte-carlo tree search, though effective for large state space,
but struggle if action space is large. Pure reinforcement learning methods,
relying only on reward signals, needs prohibitively large interactions with the
environment to device a viable plan. If the state space, observations and
actions can be represented in natural language then Large Language models (LLM)
can be used to generate action plans. Recently several such goal-directed
agents like Reflexion, CLIN, SayCan were able to surpass the performance of
other state-of-the-art methods with minimum or no task specific training. But
they still struggle with exploration and get stuck in local optima. Their
planning capabilities are limited by the limited reasoning capability of the
foundational LLMs on text data. We propose a hybrid agent &quot;neoplanner&quot;, that
synergizes both state space search with queries to foundational LLM to get the
best action plan. The reward signals are quantitatively used to drive the
search. A balance of exploration and exploitation is maintained by maximizing
upper confidence bounds of values of states. In places where random exploration
is needed, the LLM is queried to generate an action plan. Learnings from each
trial are stored as entity relationships in text format. Those are used in
future queries to the LLM for continual improvement. Experiments in the
Scienceworld environment reveals a 124% improvement from the current best
method in terms of average reward gained across multiple tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Swarna Kamal Paul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07371">
<title>Privacy-Aware Energy Consumption Modeling of Connected Battery Electric Vehicles using Federated Learning. (arXiv:2312.07371v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07371</link>
<description rdf:parseType="Literal">&lt;p&gt;Battery Electric Vehicles (BEVs) are increasingly significant in modern
cities due to their potential to reduce air pollution. Precise and real-time
estimation of energy consumption for them is imperative for effective itinerary
planning and optimizing vehicle systems, which can reduce driving range anxiety
and decrease energy costs. As public awareness of data privacy increases,
adopting approaches that safeguard data privacy in the context of BEV energy
consumption modeling is crucial. Federated Learning (FL) is a promising
solution mitigating the risk of exposing sensitive information to third parties
by allowing local data to remain on devices and only sharing model updates with
a central server. Our work investigates the potential of using FL methods, such
as FedAvg, and FedPer, to improve BEV energy consumption prediction while
maintaining user privacy. We conducted experiments using data from 10 BEVs
under simulated real-world driving conditions. Our results demonstrate that the
FedAvg-LSTM model achieved a reduction of up to 67.84\% in the MAE value of the
prediction results. Furthermore, we explored various real-world scenarios and
discussed how FL methods can be employed in those cases. Our findings show that
FL methods can effectively improve the performance of BEV energy consumption
prediction while maintaining user privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Sen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hongyuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Ji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1&quot;&gt;Tomas Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel O&amp;#x27;Connor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07384">
<title>Unsupervised Temporal Action Localization via Self-paced Incremental Learning. (arXiv:2312.07384v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07384</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, temporal action localization (TAL) has garnered significant
interest in information retrieval community. However, existing
supervised/weakly supervised methods are heavily dependent on extensive labeled
temporal boundaries and action categories, which is labor-intensive and
time-consuming. Although some unsupervised methods have utilized the
``iteratively clustering and localization&apos;&apos; paradigm for TAL, they still suffer
from two pivotal impediments: 1) unsatisfactory video clustering confidence,
and 2) unreliable video pseudolabels for model training. To address these
limitations, we present a novel self-paced incremental learning model to
enhance clustering and localization training simultaneously, thereby
facilitating more effective unsupervised TAL. Concretely, we improve the
clustering confidence through exploring the contextual feature-robust visual
information. Thereafter, we design two (constant- and variable- speed)
incremental instance learning strategies for easy-to-hard model training, thus
ensuring the reliability of these video pseudolabels and further improving
overall localization performance. Extensive experiments on two public datasets
have substantiated the superiority of our model over several state-of-the-art
competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haoyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Han Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mingzhu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yupeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jihua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07392">
<title>ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning. (arXiv:2312.07392v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,
its algorithmic robustness, particularly against adversarial perturbations,
remains unexplored. Unfortunately, the attacks and robust representation
training methods specifically designed for traditional RL are not so effective
when applied to GCRL. To address this challenge, we propose the
\textit{Semi-Contrastive Representation} attack, a novel approach inspired by
the adversarial contrastive attack. Unlike existing attacks in RL, it only
necessitates information from the policy function and can be seamlessly
implemented during deployment. Furthermore, to mitigate the vulnerability of
existing GCRL algorithms, we introduce \textit{Adversarial Representation
Tactics}. This strategy combines \textit{Semi-Contrastive Adversarial
Augmentation} with \textit{Sensitivity-Aware Regularizer}. It improves the
adversarial robustness of the underlying agent against various types of
perturbations. Extensive experiments validate the superior performance of our
attack and defence mechanism across multiple state-of-the-art GCRL algorithms.
Our tool {\bf ReRoGCRL} is available at
\url{https://github.com/TrustAI/ReRoGCRL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07398">
<title>LLMEval: A Preliminary Study on How to Evaluate Large Language Models. (arXiv:2312.07398v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07398</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the evaluation of Large Language Models has emerged as a popular
area of research. The three crucial questions for LLM evaluation are ``what,
where, and how to evaluate&apos;&apos;. However, the existing research mainly focuses on
the first two questions, which are basically what tasks to give the LLM during
testing and what kind of knowledge it should deal with. As for the third
question, which is about what standards to use, the types of evaluators, how to
score, and how to rank, there hasn&apos;t been much discussion. In this paper, we
analyze evaluation methods by comparing various criteria with both manual and
automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and
GPT-4, with different scoring methods and ranking systems. We propose a new
dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186
individuals participated, leading to the generation of 243,337 manual
annotations and 57,511 automatic evaluation results. We perform comparisons and
analyses of different settings and conduct 10 conclusions that can provide some
insights for evaluating LLM in the future. The dataset and the results are
publicly available at https://github.com/llmeval .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haipeng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shichun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yongyao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07399">
<title>Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales. (arXiv:2312.07399v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07399</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine reasoning has made great progress in recent years owing to large
language models (LLMs). In the clinical domain, however, most NLP-driven
projects mainly focus on clinical classification or reading comprehension, and
under-explore clinical reasoning for disease diagnosis due to the expensive
rationale annotation with clinicians. In this work, we present a
``reasoning-aware&apos;&apos; diagnosis framework that rationalizes the diagnostic
process via prompt-based learning in a time- and labor-efficient manner, and
learns to reason over the prompt-generated rationales. Specifically, we address
the clinical reasoning for disease diagnosis, where the LLM generates
diagnostic rationales providing its insight on presented patient data and the
reasoning path towards the diagnosis, namely Clinical Chain-of-Thought
(Clinical CoT). We empirically demonstrate LLMs/LMs&apos; ability of clinical
reasoning via extensive experiments and analyses on both rationale generation
and disease diagnosis in various settings. We further propose a novel set of
criteria for evaluating machine-generated rationales&apos; potential for real-world
clinical settings, facilitating and benefiting future research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1&quot;&gt;Taeyoon Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1&quot;&gt;Kai Tzu-iunn Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Dongjin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seungjun Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jeong Ryong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1&quot;&gt;Dosik Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sim_Y/0/1/0/all/0/1&quot;&gt;Yongsik Sim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_B/0/1/0/all/0/1&quot;&gt;Beomseok Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongha Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jinyoung Yeo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07401">
<title>On Diverse Preferences for Large Language Model Alignment. (arXiv:2312.07401v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07401</link>
<description rdf:parseType="Literal">&lt;p&gt;The alignment of large language models (LLMs) with human values is crucial
for the development of artificial general intelligence (AGI). One promising
approach to achieve this alignment is reinforcement learning from human
feedback, which employs a reward model (RM) learned from human preference
datasets to guide LLMs in generating text that aligns with human preferences.
Through intensive experiments and analysis of reward distribution, this paper
finds that preference datasets are diverse from each other, even though they
are all proposed to align human preference. Hence, mixing diverse human
preference datasets to increase data size for enhancing reward modeling could
fail. To address the issue and capture the shared human values from diverse
preferences, a new training policy called MORE is introduced, which minimizes
preference bias by adaptively adjusting the preference objective across diverse
preferences. Experiments with the Pythia-1.4B model and five mixed preference
datasets show that MORE achieves superior reward accuracy and lower calibration
error, highlighting its ability to leverage diverse human preference data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pengyu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wanshun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1&quot;&gt;Nan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07413">
<title>AI capabilities can be significantly improved without expensive retraining. (arXiv:2312.07413v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07413</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art AI systems can be significantly improved without expensive
retraining via &quot;post-training enhancements&quot;-techniques applied after initial
training like fine-tuning the system to use a web browser. We review recent
post-training enhancements, categorizing them into five types: tool-use,
prompting methods, scaffolding, solution selection, and data generation.
Different enhancements improve performance on different tasks, making it hard
to compare their significance. So we translate improvements from different
enhancements into a common currency, the compute-equivalent gain: how much
additional training compute would be needed to improve performance by the same
amount as the enhancement. Our non-experimental work shows that post-training
enhancements have significant benefits: most surveyed enhancements improve
benchmark performance by more than a 5x increase in training compute, some by
more than 20x. Post-training enhancements are relatively cheap to develop:
fine-tuning costs are typically &amp;lt;1% of the original training cost. Governing
the development of capable post-training enhancements may be challenging
because frontier models could be enhanced by a wide range of actors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_T/0/1/0/all/0/1&quot;&gt;Tom Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1&quot;&gt;Jean-Stanislas Denain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villalobos_P/0/1/0/all/0/1&quot;&gt;Pablo Villalobos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bas_G/0/1/0/all/0/1&quot;&gt;Guillem Bas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07424">
<title>How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation. (arXiv:2312.07424v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07424</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, generalization against distribution shifts -- where
deployment conditions diverge from the training scenarios -- is crucial,
particularly in fields like climate modeling, biomedicine, and autonomous
driving. The emergence of foundation models, distinguished by their extensive
pretraining and task versatility, has led to an increased interest in their
adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced
publicly accessible multimodal foundation model, with extensive applications
across various domains, including anomaly detection, video understanding, image
generation, and medical diagnosis. However, its robustness against data
distributions remains largely underexplored. Addressing this gap, this study
rigorously evaluates GPT-4V&apos;s adaptability and generalization capabilities in
dynamic environments, benchmarking against prominent models like CLIP and
LLaVA. We delve into GPT-4V&apos;s zero-shot generalization across 13 diverse
datasets spanning natural, medical, and molecular domains. We further
investigate its adaptability to controlled data perturbations and examine the
efficacy of in-context learning as a tool to enhance its adaptation. Our
findings delineate GPT-4V&apos;s capability boundaries in distribution shifts,
shedding light on its strengths and limitations across various scenarios.
Importantly, this investigation contributes to our understanding of how AI
foundation models generalize to distribution shifts, offering pivotal insights
into their adaptability and robustness. Code is publicly available at
https://github.com/jameszhou-gl/gpt-4v-distribution-shift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guanglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Rundong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tailin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yilong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07428">
<title>Ensemble Federated Learning: an approach for collaborative pneumonia diagnosis. (arXiv:2312.07428v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07428</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a very convenient approach for scenarios where (i) the
exchange of data implies privacy concerns and/or (ii) a quick reaction is
needed. In smart healthcare systems, both aspects are usually required. In this
paper, we work on the first scenario, where preserving privacy is key and,
consequently, building a unique and massive medical image data set by fusing
different data sets from different medical institutions or research centers
(computation nodes) is not an option. We propose an ensemble federated learning
(EFL) approach that is based on the following characteristics: First, each
computation node works with a different data set (but of the same type). They
work locally and apply an ensemble approach combining eight well-known CNN
models (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50,
densenet121, and resnet152v2) on Chest X-ray images. Second, the best two local
models are used to create a local ensemble model that is shared with a central
node. Third, the ensemble models are aggregated to obtain a global model, which
is shared with the computation nodes to continue with a new iteration. This
procedure continues until there are no changes in the best local models. We
have performed different experiments to compare our approach with centralized
ones (with or without an ensemble approach)\color{black}. The results conclude
that our proposal outperforms these ones in Chest X-ray images (achieving an
accuracy of 96.63\%) and offers very competitive results compared to other
proposals in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mabrouk_A/0/1/0/all/0/1&quot;&gt;Alhassan Mabrouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaziz_M/0/1/0/all/0/1&quot;&gt;Mohamed Abd Elaziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayed_M/0/1/0/all/0/1&quot;&gt;Mohammed Kayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07435">
<title>Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval. (arXiv:2312.07435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07435</link>
<description rdf:parseType="Literal">&lt;p&gt;Video moment retrieval is a challenging task requiring fine-grained
interactions between video and text modalities. Recent work in image-text
pretraining has demonstrated that most existing pretrained models suffer from
information asymmetry due to the difference in length between visual and
textual sequences. We question whether the same problem also exists in the
video-text domain with an auxiliary need to preserve both spatial and temporal
information. Thus, we evaluate a recently proposed solution involving the
addition of an asymmetric co-attention network for video grounding tasks.
Additionally, we incorporate momentum contrastive loss for robust,
discriminative representation learning in both modalities. We note that the
integration of these supplementary modules yields better performance compared
to state-of-the-art models on the TACoS dataset and comparable results on
ActivityNet Captions, all while utilizing significantly fewer parameters with
respect to baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panta_L/0/1/0/all/0/1&quot;&gt;Love Panta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1&quot;&gt;Prashant Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapkota_B/0/1/0/all/0/1&quot;&gt;Brabeem Sapkota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1&quot;&gt;Amrita Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1&quot;&gt;Suresh Manandhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1&quot;&gt;Anand Kumar Sah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07437">
<title>Medical Image Classification Using Transfer Learning and Chaos Game Optimization on the Internet of Medical Things. (arXiv:2312.07437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07437</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Medical Things (IoMT) has dramatically benefited medical
professionals that patients and physicians can access from all regions.
Although the automatic detection and prediction of diseases such as melanoma
and leukemia is still being researched and studied in IoMT, existing approaches
are not able to achieve a high degree of efficiency. Thus, with a new approach
that provides better results, patients would access the adequate treatments
earlier and the death rate would be reduced. Therefore, this paper introduces
an IoMT proposal for medical images classification that may be used anywhere,
i.e. it is an ubiquitous approach. It was design in two stages: first, we
employ a Transfer Learning (TL)-based method for feature extraction, which is
carried out using MobileNetV3; second, we use the Chaos Game Optimization (CGO)
for feature selection, with the aim of excluding unnecessary features and
improving the performance, which is key in IoMT. Our methodology was evaluated
using ISIC-2016, PH2, and Blood-Cell datasets. The experimental results
indicated that the proposed approach obtained an accuracy of 88.39% on
ISIC-2016, 97.52% on PH2, and 88.79% on Blood-cell. Moreover, our approach had
successful performances for the metrics employed compared to other existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mabrouk_A/0/1/0/all/0/1&quot;&gt;Alhassan Mabrouk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahou_A/0/1/0/all/0/1&quot;&gt;Abdelghani Dahou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaziz_M/0/1/0/all/0/1&quot;&gt;Mohamed Abd Elaziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kayed_M/0/1/0/all/0/1&quot;&gt;Mohammed Kayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07457">
<title>Dynamics Harmonic Analysis of Robotic Systems: Application in Data-Driven Koopman Modelling. (arXiv:2312.07457v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.07457</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the use of harmonic analysis to decompose the state space of
symmetric robotic systems into orthogonal isotypic subspaces. These are
lower-dimensional spaces that capture distinct, symmetric, and synergistic
motions. For linear dynamics, we characterize how this decomposition leads to a
subdivision of the dynamics into independent linear systems on each subspace, a
property we term dynamics harmonic analysis (DHA). To exploit this property, we
use Koopman operator theory to propose an equivariant deep-learning
architecture that leverages the properties of DHA to learn a global linear
model of system dynamics. Our architecture, validated on synthetic systems and
the dynamics of locomotion of a quadrupedal robot, demonstrates enhanced
generalization, sample efficiency, and interpretability, with less trainable
parameters and computational costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_Apraez_D/0/1/0/all/0/1&quot;&gt;Daniel Ordo&amp;#xf1;ez-Apraez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostic_V/0/1/0/all/0/1&quot;&gt;Vladimir Kostic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turrisi_G/0/1/0/all/0/1&quot;&gt;Giulio Turrisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novelli_P/0/1/0/all/0/1&quot;&gt;Pietro Novelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastalli_C/0/1/0/all/0/1&quot;&gt;Carlos Mastalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semini_C/0/1/0/all/0/1&quot;&gt;Claudio Semini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1&quot;&gt;Massimiliano Pontil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07466">
<title>Efficient Object Detection in Autonomous Driving using Spiking Neural Networks: Performance, Energy Consumption Analysis, and Insights into Open-set Object Discovery. (arXiv:2312.07466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07466</link>
<description rdf:parseType="Literal">&lt;p&gt;Besides performance, efficiency is a key design driver of technologies
supporting vehicular perception. Indeed, a well-balanced trade-off between
performance and energy consumption is crucial for the sustainability of
autonomous vehicles. In this context, the diversity of real-world contexts in
which autonomous vehicles can operate motivates the need for empowering
perception models with the capability to detect, characterize and identify
newly appearing objects by themselves. In this manuscript we elaborate on this
threefold conundrum (performance, efficiency and open-world learning) for
object detection modeling tasks over image data collected from vehicular
scenarios. Specifically, we show that well-performing and efficient models can
be realized by virtue of Spiking Neural Networks (SNNs), reaching competitive
levels of detection performance when compared to their non-spiking counterparts
at dramatic energy consumption savings (up to 85%) and a slightly improved
robustness against image noise. Our experiments herein offered also expose
qualitatively the complexity of detecting new objects based on the preliminary
results of a simple approach to discriminate potential object proposals in the
captured image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seras_A/0/1/0/all/0/1&quot;&gt;Aitor Martinez Seras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Bringas_P/0/1/0/all/0/1&quot;&gt;Pablo Garcia-Bringas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07476">
<title>Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07476</link>
<description rdf:parseType="Literal">&lt;p&gt;In-Context Learning (ICL) is an important paradigm for adapting Large
Language Models (LLMs) to downstream tasks through a few demonstrations.
Despite the great success of ICL, the limitation of the demonstration number
may lead to demonstration bias, i.e. the input-label mapping induced by LLMs
misunderstands the task&apos;s essence. Inspired by human experience, we attempt to
mitigate such bias through the perspective of the inter-demonstration
relationship. Specifically, we construct Comparable Demonstrations (CDs) by
minimally editing the texts to flip the corresponding labels, in order to
highlight the task&apos;s essence and eliminate potential spurious correlations
through the inter-demonstration comparison. Through a series of experiments on
CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can
significantly reduce such bias; (2) CDs exhibit good performance in ICL,
especially in out-of-distribution scenarios. In summary, this study explores
the ICL mechanisms from a novel perspective, providing a deeper insight into
the demonstration selection strategy for ICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Caoyun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jidong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07482">
<title>Classification of retail products: From probabilistic ranking to neural networks. (arXiv:2312.07482v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07482</link>
<description rdf:parseType="Literal">&lt;p&gt;Food retailing is now on an accelerated path to a success penetration into
the digital market by new ways of value creation at all stages of the consumer
decision process. One of the most important imperatives in this path is the
availability of quality data to feed all the process in digital transformation.
But the quality of data is not so obvious if we consider the variety of
products and suppliers in the grocery market. Within this context of digital
transformation of grocery industry, \textit{Midiadia} is Spanish data provider
company that works on converting data from the retailers&apos; products into
knowledge with attributes and insights from the product labels, that is,
maintaining quality data in a dynamic market with a high dispersion of
products. Currently, they manually categorize products (groceries) according to
the information extracted directly (text processing) from the product labelling
and packaging. This paper introduces a solution to automatically categorize the
constantly changing product catalogue into a 3-level food taxonomy. Our
proposal studies three different approaches: a score-based ranking method,
traditional machine learning algorithms, and deep neural networks. Thus, we
provide four different classifiers that support a more efficient and less
error-prone maintenance of groceries catalogues, the main asset of the company.
Finally, we have compared the performance of these three alternatives,
concluding that traditional machine learning algorithms perform better, but
closely followed by the score-based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1&quot;&gt;Manar Mohamed Hafez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Vilas_A/0/1/0/all/0/1&quot;&gt;Ana Fern&amp;#xe1;ndez-Vilas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pazo_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ctor Olivera Paz&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07488">
<title>LMDrive: Closed-Loop End-to-End Driving with Large Language Models. (arXiv:2312.07488v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07488</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant recent progress in the field of autonomous driving,
modern methods still struggle and can incur serious accidents when encountering
long-tail unforeseen events and challenging urban scenarios. On the one hand,
large language models (LLM) have shown impressive reasoning capabilities that
approach &quot;Artificial General Intelligence&quot;. On the other hand, previous
autonomous driving methods tend to rely on limited-format inputs (e.g. sensor
data and navigation waypoints), restricting the vehicle&apos;s ability to understand
language information and interact with humans. To this end, this paper
introduces LMDrive, a novel language-guided, end-to-end, closed-loop autonomous
driving framework. LMDrive uniquely processes and integrates multi-modal sensor
data with natural language instructions, enabling interaction with humans and
navigation software in realistic instructional settings. To facilitate further
research in language-based closed-loop autonomous driving, we also publicly
release the corresponding dataset which includes approximately 64K
instruction-following data clips, and the LangAuto benchmark that tests the
system&apos;s ability to handle complex instructions and challenging driving
scenarios. Extensive closed-loop experiments are conducted to demonstrate
LMDrive&apos;s effectiveness. To the best of our knowledge, we&apos;re the very first
work to leverage LLMs for closed-loop end-to-end autonomous driving. Codes can
be found at https://github.com/opendilab/LMDrive
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Letian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07492">
<title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07492</link>
<description rdf:parseType="Literal">&lt;p&gt;Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. We start with a
comprehensive list of 93 stigmas documented in social science literature and
curate a question-answering (QA) dataset which involves simple social
situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a
variety of prompt styles, carefully constructed to systematically test for both
social bias and model robustness. We present results for SocialStigmaQA with
two widely used open source generative language models and we demonstrate that
the output generated by these models considerably amplifies existing social
bias against stigmatized groups. Specifically, we find that the proportion of
socially biased output ranges from 45% to 59% across a variety of decoding
strategies and prompting styles. We discover that the deliberate design of the
templates in our benchmark (e.g., by adding biasing text to the prompt or
varying the answer that indicates bias) impact the model tendencies to generate
socially biased output. Additionally, we report on patterns in the generated
chain-of-thought output, finding a variety of problems from subtle bias to
evidence of a lack of reasoning.
&lt;/p&gt;
&lt;p&gt;Warning: This paper contains examples of text which is toxic, biased, and
harmful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1&quot;&gt;Manish Nagireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1&quot;&gt;Lamogha Chiazor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Moninder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1&quot;&gt;Ioana Baldini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16218">
<title>APG: Adaptive Parameter Generation Network for Click-Through Rate Prediction. (arXiv:2203.16218v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16218</link>
<description rdf:parseType="Literal">&lt;p&gt;In many web applications, deep learning-based CTR prediction models (deep CTR
models for short) are widely adopted. Traditional deep CTR models learn
patterns in a static manner, i.e., the network parameters are the same across
all the instances. However, such a manner can hardly characterize each of the
instances which may have different underlying distributions. It actually limits
the representation power of deep CTR models, leading to sub-optimal results. In
this paper, we propose an efficient, effective, and universal module, named as
Adaptive Parameter Generation network (APG), which can dynamically generate
parameters for deep CTR models on-the-fly based on different instances.
Extensive experimental evaluation results show that APG can be applied to a
variety of deep CTR models and significantly improve their performance.
Meanwhile, APG can reduce the time cost by 38.7\% and memory usage by 96.6\%
compared to a regular deep CTR model. We have deployed APG in the industrial
sponsored search system and achieved 3\% CTR gain and 1\% RPM gain
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bencheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Hongbo Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bo Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04619">
<title>Risk Preferences of Learning Algorithms. (arXiv:2205.04619v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04619</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents&apos; learning from feedback shapes economic outcomes, and many economic
decision-makers today employ learning algorithms to make consequential choices.
This note shows that a widely used learning algorithm, $\varepsilon$-Greedy,
exhibits emergent risk aversion: it prefers actions with lower variance. When
presented with actions of the same expectation, under a wide range of
conditions, $\varepsilon$-Greedy chooses the lower-variance action with
probability approaching one. This emergent preference can have wide-ranging
consequences, ranging from concerns about fairness to homogenization, and holds
transiently even when the riskier action has a strictly higher expected payoff.
We discuss two methods to correct this bias. The first method requires the
algorithm to reweight data as a function of how likely the actions were to be
chosen. The second requires the algorithm to have optimistic estimates of
actions for which it has not collected much data. We show that risk-neutrality
is restored with these corrections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haupt_A/0/1/0/all/0/1&quot;&gt;Andreas Haupt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1&quot;&gt;Aroon Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.01874">
<title>The Best Decisions Are Not the Best Advice: Making Adherence-Aware Recommendations. (arXiv:2209.01874v4 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2209.01874</link>
<description rdf:parseType="Literal">&lt;p&gt;Many high-stake decisions follow an expert-in-loop structure in that a human
operator receives recommendations from an algorithm but is the ultimate
decision maker. Hence, the algorithm&apos;s recommendation may differ from the
actual decision implemented in practice. However, most algorithmic
recommendations are obtained by solving an optimization problem that assumes
recommendations will be perfectly implemented. We propose an adherence-aware
optimization framework to capture the dichotomy between the recommended and the
implemented policy and analyze the impact of partial adherence on the optimal
recommendation. We show that overlooking the partial adherence phenomenon, as
is currently being done by most recommendation engines, can lead to arbitrarily
severe performance deterioration, compared with both the current human baseline
performance and what is expected by the recommendation algorithm. Our framework
also provides useful tools to analyze the structure and to compute optimal
recommendation policies that are naturally immune against such human
deviations, and are guaranteed to improve upon the baseline policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grand_Clement_J/0/1/0/all/0/1&quot;&gt;Julien Grand-Cl&amp;#xe9;ment&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauphilet_J/0/1/0/all/0/1&quot;&gt;Jean Pauphilet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.01962">
<title>Adversarial Detection: Attacking Object Detection in Real Time. (arXiv:2209.01962v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.01962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent robots rely on object detection models to perceive the
environment. Following advances in deep learning security it has been revealed
that object detection models are vulnerable to adversarial attacks. However,
prior research primarily focuses on attacking static images or offline videos.
Therefore, it is still unclear if such attacks could jeopardize real-world
robotic applications in dynamic environments. This paper bridges this gap by
presenting the first real-time online attack against object detection models.
We devise three attacks that fabricate bounding boxes for nonexistent objects
at desired locations. The attacks achieve a success rate of about 90% within
about 20 iterations. The demo video is available at
https://youtu.be/zJZ1aNlXsMU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Han Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yunas_S/0/1/0/all/0/1&quot;&gt;Syed Yunas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowlands_S/0/1/0/all/0/1&quot;&gt;Sareh Rowlands&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahlstrom_J/0/1/0/all/0/1&quot;&gt;Johan Wahlstrom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01959">
<title>Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot Document-Level Question Answering. (arXiv:2210.01959v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01959</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers produce thousands of scholarly documents containing valuable
technical knowledge. The community faces the laborious task of reading these
documents to identify, extract, and synthesize information. To automate
information gathering, document-level question answering (QA) offers a flexible
framework where human-posed questions can be adapted to extract diverse
knowledge. Finetuning QA systems requires access to labeled data (tuples of
context, question and answer). However, data curation for document QA is
uniquely challenging because the context (i.e. answer evidence passage) needs
to be retrieved from potentially long, ill-formatted documents. Existing QA
datasets sidestep this challenge by providing short, well-defined contexts that
are unrealistic in real-world applications. We present a three-stage document
QA approach: (1) text extraction from PDF; (2) evidence retrieval from
extracted texts to form well-posed contexts; (3) QA to extract knowledge from
contexts to return high-quality answers -- extractive, abstractive, or Boolean.
Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system
achieves a +7.19 improvement in Answer-F1 over existing baselines while
delivering superior context selection. Our results demonstrate that DRC holds
tremendous promise as a flexible framework for practical scientific document
QA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1&quot;&gt;Tavish McDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsan_B/0/1/0/all/0/1&quot;&gt;Brian Tsan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1&quot;&gt;Amar Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_J/0/1/0/all/0/1&quot;&gt;Juanita Ordonez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_L/0/1/0/all/0/1&quot;&gt;Luis Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mason_B/0/1/0/all/0/1&quot;&gt;Blake Mason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1&quot;&gt;Brenda Ng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03169">
<title>A Study on the Generality of Neural Network Structures for Monocular Depth Estimation. (arXiv:2301.03169v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03169</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular depth estimation has been widely studied, and significant
improvements in performance have been recently reported. However, most previous
works are evaluated on a few benchmark datasets, such as KITTI datasets, and
none of the works provide an in-depth analysis of the generalization
performance of monocular depth estimation. In this paper, we deeply investigate
the various backbone networks (e.g.CNN and Transformer models) toward the
generalization of monocular depth estimation. First, we evaluate
state-of-the-art models on both in-distribution and out-of-distribution
datasets, which have never been seen during network training. Then, we
investigate the internal properties of the representations from the
intermediate layers of CNN-/Transformer-based models using synthetic
texture-shifted datasets. Through extensive experiments, we observe that the
Transformers exhibit a strong shape-bias rather than CNNs, which have a strong
texture-bias. We also discover that texture-biased models exhibit worse
generalization performance for monocular depth estimation than shape-biased
models. We demonstrate that similar aspects are observed in real-world driving
datasets captured under diverse environments. Lastly, we conduct a dense
ablation study with various backbone networks which are utilized in modern
strategies. The experiments demonstrate that the intrinsic locality of the CNNs
and the self-attention of the Transformers induce texture-bias and shape-bias,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1&quot;&gt;Jinwoo Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_K/0/1/0/all/0/1&quot;&gt;Kyumin Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1&quot;&gt;Sunghoon Im&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04664">
<title>Algebraic characterizations of least model and uniform equivalence of propositional Krom logic programs. (arXiv:2302.04664v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04664</link>
<description rdf:parseType="Literal">&lt;p&gt;This research note provides algebraic characterizations of the least model,
subsumption, and uniform equivalence of propositional Krom logic programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1&quot;&gt;Christian Anti&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10096">
<title>Generalization-based similarity. (arXiv:2302.10096v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10096</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting and exploiting similarities between seemingly distant objects is
without doubt an important human ability. This paper develops \textit{from the
ground up} an abstract algebraic and qualitative justification-based notion of
similarity based on the observation that sets of generalizations encode
important properties of elements. We show that similarity defined in this way
has appealing mathematical properties. As we construct our notion of similarity
from first principles using only elementary concepts of universal algebra, to
convince the reader of its plausibility, we show that it can be naturally
embedded into first-order logic via model-theoretic types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1&quot;&gt;Christian Anti&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11091">
<title>GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation Learning Method. (arXiv:2302.11091v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11091</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Knowledge Graph (TKG) representation learning embeds entities and
event types into a continuous low-dimensional vector space by integrating the
temporal information, which is essential for downstream tasks, e.g., event
prediction and question answering. Existing methods stack multiple graph
convolution layers to model the influence of distant entities, leading to the
over-smoothing problem. To alleviate the problem, recent studies infuse
reinforcement learning to obtain paths that contribute to modeling the
influence of distant entities. However, due to the limited number of hops,
these studies fail to capture the correlation between entities that are far
apart and even unreachable. To this end, we propose GTRL, an entity Group-aware
Temporal knowledge graph Representation Learning method. GTRL is the first work
that incorporates the entity group modeling to capture the correlation between
entities by stacking only a finite number of layers. Specifically, the entity
group mapper is proposed to generate entity groups from entities in a learning
way. Based on entity groups, the implicit correlation encoder is introduced to
capture implicit correlations between any pairwise entity groups. In addition,
the hierarchical GCNs are exploited to accomplish the message aggregation and
representation updating on the entity group graph and the entity graph.
Finally, GRUs are employed to capture the temporal dependency in TKGs.
Extensive experiments on three real-world datasets demonstrate that GTRL
achieves the state-of-the-art performances on the event prediction task,
outperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and
15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xing Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00855">
<title>Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents. (arXiv:2303.00855v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00855</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in large language models (LLMs) has demonstrated the ability
to learn and leverage Internet-scale knowledge through pre-training with
autoregressive models. Unfortunately, applying such models to settings with
embodied agents, such as robots, is challenging due to their lack of experience
with the physical world, inability to parse non-language observations, and
ignorance of rewards or safety constraints that robots may require. On the
other hand, language-conditioned robotic policies that learn from interaction
data can provide the necessary grounding that allows the agent to be correctly
situated in the real world, but such policies are limited by the lack of
high-level semantic understanding due to the limited breadth of the interaction
data available for training them. Thus, if we want to make use of the semantic
knowledge in a language model while still situating it in an embodied setting,
we must construct an action sequence that is both likely according to the
language model and also realizable according to grounded models of the
environment. We frame this as a problem similar to probabilistic filtering:
decode a sequence that both has high probability under the language model and
high probability under a set of grounded model objectives. We demonstrate how
such grounded models can be obtained across three simulation and real-world
domains, and that the proposed decoding strategy is able to solve complex,
long-horizon embodiment tasks in a robotic setting by leveraging the knowledge
of both models. The project&apos;s website can be found at
grounded-decoding.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Dhruv Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1&quot;&gt;Danny Driess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1&quot;&gt;Pete Florence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01092">
<title>ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations. (arXiv:2303.01092v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01092</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data
for model training. Empirical studies show that SSL can achieve promising
performance in distribution shift scenarios, where the downstream and training
distributions differ. However, the theoretical understanding of its
transferability remains limited. In this paper, we develop a theoretical
framework to analyze the transferability of self-supervised contrastive
learning, by investigating the impact of data augmentation on it. Our results
reveal that the downstream performance of contrastive learning depends largely
on the choice of data augmentation. Moreover, we show that contrastive learning
fails to learn domain-invariant features, which limits its transferability.
Based on these theoretical insights, we propose a novel method called
Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn
domain-invariant features and can be easily integrated with existing
contrastive learning algorithms. We conduct experiments on several datasets and
show that ArCL significantly improves the transferability of contrastive
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xuyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1&quot;&gt;Tianqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08476">
<title>Bayesian Learning for the Robust Verification of Autonomous Robots. (arXiv:2303.08476v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08476</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous robots used in infrastructure inspection, space exploration and
other critical missions operate in highly dynamic environments. As such, they
must continually verify their ability to complete the tasks associated with
these missions safely and effectively. Here we present a Bayesian learning
framework that enables this runtime verification of autonomous robots. The
framework uses prior knowledge and observations of the verified robot to learn
expected ranges for the occurrence rates of regular and singular (e.g.,
catastrophic failure) events. Interval continuous-time Markov models defined
using these ranges are then analysed to obtain expected intervals of variation
for system properties such as mission duration and success probability. We
apply the framework to an autonomous robotic mission for underwater
infrastructure inspection and repair. The formal proofs and experiments
presented in the paper show that our framework produces results that reflect
the uncertainty intrinsic to many real-world systems, enabling the robust
verification of their quantitative properties under parametric uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerasimou_S/0/1/0/all/0/1&quot;&gt;Simos Gerasimou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calinescu_R/0/1/0/all/0/1&quot;&gt;Radu Calinescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imrie_C/0/1/0/all/0/1&quot;&gt;Calum Imrie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robu_V/0/1/0/all/0/1&quot;&gt;Valentin Robu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flynn_D/0/1/0/all/0/1&quot;&gt;David Flynn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07072">
<title>CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction. (arXiv:2304.07072v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07072</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured reconstruction is a non-trivial dense prediction problem, which
extracts structural information (\eg, building corners and edges) from a raster
image, then reconstructs it to a 2D planar graph accordingly. Compared with
common segmentation or detection problems, it significantly relays on the
capability that leveraging holistic geometric information for structural
reasoning. Current transformer-based approaches tackle this challenging problem
in a two-stage manner, which detect corners in the first model and classify the
proposed edges (corner-pairs) in the second model. However, they separate
two-stage into different models and only share the backbone encoder. Unlike the
existing modeling strategies, we present an enhanced corner representation
method: 1) It fuses knowledge between the corner detection and edge prediction
by sharing feature in different granularity; 2) Corner candidates are proposed
in four heatmap channels w.r.t its direction. Both qualitative and quantitative
evaluations demonstrate that our proposed method can better reconstruct
fine-grained structures, such as adjacent corners and tiny edges. Consequently,
it outperforms the state-of-the-art model by +1.9\%@F-1 on Corner and
+3.0\%@F-1 on Edge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hongbo Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yulong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Linzhi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1&quot;&gt;Xu Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiani Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13107">
<title>Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13107</link>
<description rdf:parseType="Literal">&lt;p&gt;Device-free human presence detection is a crucial technology for various
applications, including home automation, security, and healthcare. While
camera-based systems have traditionally been used for this purpose, they raise
privacy concerns. To address this issue, recent research has explored the use
of wireless channel state information (CSI) extracted from commercial WiFi
access points (APs) to provide detailed channel characteristics. In this paper,
we propose a device-free human presence detection system for multi-room
scenarios using a time-selective conditional dual feature extract recurrent
network (TCD-FERN). Our system is designed to capture significant time features
on current human features using a dynamic and static data preprocessing
technique. We extract both moving and spatial features of people and
differentiate between line-of-sight (LoS) and non-line-of-sight (NLoS) cases.
Subcarrier fusion is carried out in order to provide more objective variation
of each sample while reducing the computational complexity. A voting scheme is
further adopted to mitigate the feature attenuation problem caused by room
partitions, with around 3% improvement of human presence detection accuracy.
Experimental results have revealed the significant improvement of leveraging
subcarrier fusion, dual-feature recurrent network, time selection and condition
mechanisms. Compared to the existing works in open literature, our proposed
TCD-FERN system can achieve above 97% of human presence detection accuracy for
multi-room scenarios with the adoption of fewer WiFi APs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li-Hsiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_A/0/1/0/all/0/1&quot;&gt;An-Hung Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1&quot;&gt;Fang-Yu Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kai-Ten Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15486">
<title>SPRING: Studying the Paper and Reasoning to Play Games. (arXiv:2305.15486v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15486</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-world survival games pose significant challenges for AI algorithms due
to their multi-tasking, deep exploration, and goal prioritization requirements.
Despite reinforcement learning (RL) being popular for solving games, its high
sample complexity limits its effectiveness in complex open-world games like
Crafter or Minecraft. We propose a novel approach, SPRING, to read the game&apos;s
original academic paper and use the knowledge learned to reason and play the
game through a large language model (LLM). Prompted with the LaTeX source as
game context and a description of the agent&apos;s current observation, our SPRING
framework employs a directed acyclic graph (DAG) with game-related questions as
nodes and dependencies as edges. We identify the optimal action to take in the
environment by traversing the DAG and calculating LLM responses for each node
in topological order, with the LLM&apos;s answer to final node directly translating
to environment actions. In our experiments, we study the quality of in-context
&quot;reasoning&quot; induced by different forms of prompts under the setting of the
Crafter open-world environment. Our experiments suggest that LLMs, when
prompted with consistent chain-of-thought, have great potential in completing
sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4
outperforms all state-of-the-art RL baselines, trained for 1M steps, without
any training. Finally, we show the potential of games as a test bed for LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1&quot;&gt;Shrimai Prabhumoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;So Yeon Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1&quot;&gt;Amos Azaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1&quot;&gt;Tom Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16318">
<title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation. (arXiv:2305.16318v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16318</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, video object segmentation (VOS) referred by multi-modal signals,
e.g., language and audio, has evoked increasing attention in both industry and
academia. It is challenging for exploring the semantic alignment within
modalities and the visual correspondence across frames. However, existing
methods adopt separate network architectures for different modalities, and
neglect the inter-frame temporal interaction with references. In this paper, we
propose MUTR, a Multi-modal Unified Temporal transformer for Referring video
object segmentation. With a unified framework for the first time, MUTR adopts a
DETR-style transformer and is capable of segmenting video objects designated by
either text or audio reference. Specifically, we introduce two strategies to
fully explore the temporal relations between videos and multi-modal signals.
Firstly, for low-level temporal aggregation before the transformer, we enable
the multi-modal references to capture multi-scale visual cues from consecutive
video frames. This effectively endows the text or audio signals with temporal
knowledge and boosts the semantic alignment between modalities. Secondly, for
high-level temporal interaction after the transformer, we conduct inter-frame
feature communication for different object embeddings, contributing to better
object-wise correspondence for tracking along the video. On Ref-YouTube-VOS and
AVSBench datasets with respective text and audio references, MUTR achieves
+4.2% and +8.7% J&amp;amp;F improvements to state-of-the-art methods, demonstrating our
significance for unified multi-modal VOS. Code is released at
https://github.com/OpenGVLab/MUTR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shilin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhongjiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03241">
<title>Early Weight Averaging meets High Learning Rates for LLM Pre-training. (arXiv:2306.03241v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03241</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Large Language Models (LLMs) incurs significant cost; hence, any
strategy that accelerates model convergence is helpful. In this paper, we
investigate the ability of a simple idea checkpoint averaging along the
trajectory of a training run to improve both convergence and generalization
quite early on during training. Here we show that models trained with high
learning rates observe higher gains due to checkpoint averaging. Furthermore,
these gains are amplified when checkpoints are sampled with considerable
spacing in training steps. Our training recipe outperforms conventional
training and popular checkpoint averaging baselines such as exponential moving
average (EMA) and stochastic moving average (SWA). We evaluate our training
recipe by pre-training LLMs, where high learning rates are inherently preferred
due to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2
models of varying sizes, small (125M), medium (335M), and large (770M)on the
OpenWebText dataset, comprised of 9B tokens. Additionally, we present results
for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained
on the PILE-deduped dataset containing 207B tokens.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1&quot;&gt;Sunny Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neerkaje_A/0/1/0/all/0/1&quot;&gt;Atula Neerkaje&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1&quot;&gt;Jean Kaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhishek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1&quot;&gt;Sujay Sanghavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05036">
<title>Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale. (arXiv:2306.05036v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05036</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as ChatGPT and GPT-4, are gaining
wide-spread real world use. Yet, these LLMs are closed source, and little is
known about their performance in real-world use cases. In this paper, we apply
and evaluate the combination of ChatGPT and GPT-4 for the real-world task of
mining insights from a text corpus in order to identify research challenges in
the field of HCI. We extract 4,392 research challenges in over 100 topics from
the 2023 CHI conference proceedings and visualize the research challenges for
interactive exploration. We critically evaluate the LLMs on this practical task
and conclude that the combination of ChatGPT and GPT-4 makes an excellent
cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is
key for flexibly prototyping research ideas and analyzing text corpora from
different perspectives, with implications for applying LLMs for mining insights
in academia and practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oppenlaender_J/0/1/0/all/0/1&quot;&gt;Jonas Oppenlaender&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamalainen_J/0/1/0/all/0/1&quot;&gt;Joonas H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06871">
<title>Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06871</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is a learning paradigm where an agent
learns from a fixed dataset of experience. However, learning solely from a
static dataset can limit the performance due to the lack of exploration. To
overcome it, offline-to-online RL combines offline pre-training with online
fine-tuning, which enables the agent to further refine its policy by
interacting with the environment in real-time. Despite its benefits, existing
offline-to-online RL methods suffer from performance degradation and slow
improvement during the online phase. To tackle these challenges, we propose a
novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing
the number of Q-networks, we seamlessly bridge offline pre-training and online
fine-tuning without degrading performance. Moreover, to expedite online
performance enhancement, we appropriately loosen the pessimism of Q-value
estimation and incorporate ensemble-based exploration mechanisms into our
framework. Experimental results demonstrate that E2O can substantially improve
the training stability, learning efficiency, and final performance of existing
offline RL methods during online fine-tuning on a range of locomotion and
navigation tasks, significantly outperforming existing offline-to-online RL
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09124">
<title>DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09124</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks, particularly patch attacks, pose significant threats to
the robustness and reliability of deep learning models. Developing reliable
defenses against patch attacks is crucial for real-world applications, yet
current research in this area is unsatisfactory. In this paper, we propose
DIFFender, a novel defense method that leverages a text-guided diffusion model
to defend against adversarial patches. DIFFender includes two main stages:
patch localization and patch restoration. In the localization stage, we find
and exploit an intriguing property of the diffusion model to precisely identify
the locations of adversarial patches. In the restoration stage, we employ the
diffusion model to reconstruct the adversarial regions in the images while
preserving the integrity of the visual content. Thanks to the former finding,
these two stages can be simultaneously guided by a unified diffusion model.
Thus, we can utilize the close interaction between them to improve the whole
defense performance. Moreover, we propose a few-shot prompt-tuning algorithm to
fine-tune the diffusion model, enabling the pre-trained diffusion model to
adapt to the defense task easily. We conduct extensive experiments on image
classification, face recognition, and further in the physical world,
demonstrating that our proposed method exhibits superior robustness under
strong adaptive attacks and generalizes well across various scenarios, diverse
classifiers, and multiple patch attack methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1&quot;&gt;Caixin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shouwei Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13509">
<title>Exploring AI-enhanced Shared Control for an Assistive Robotic Arm. (arXiv:2306.13509v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13509</link>
<description rdf:parseType="Literal">&lt;p&gt;Assistive technologies and in particular assistive robotic arms have the
potential to enable people with motor impairments to live a self-determined
life. More and more of these systems have become available for end users in
recent years, such as the Kinova Jaco robotic arm. However, they mostly require
complex manual control, which can overwhelm users. As a result, researchers
have explored ways to let such robots act autonomously. However, at least for
this specific group of users, such an approach has shown to be futile. Here,
users want to stay in control to achieve a higher level of personal autonomy,
to which an autonomous robot runs counter. In our research, we explore how
Artifical Intelligence (AI) can be integrated into a shared control paradigm.
In particular, we focus on the consequential requirements for the interface
between human and robot and how we can keep humans in the loop while still
significantly reducing the mental load and required motor skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascher_M/0/1/0/all/0/1&quot;&gt;Max Pascher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kronhardt_K/0/1/0/all/0/1&quot;&gt;Kirill Kronhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freienstein_J/0/1/0/all/0/1&quot;&gt;Jan Freienstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1&quot;&gt;Jens Gerken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00910">
<title>CoPL: Contextual Prompt Learning for Vision-Language Understanding. (arXiv:2307.00910v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00910</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in multimodal learning has resulted in powerful
vision-language models, whose representations are generalizable across a
variety of downstream tasks. Recently, their generalization ability has been
further extended by incorporating trainable prompts, borrowed from the natural
language processing literature. While such prompt learning techniques have
shown impressive results, we identify that these prompts are trained based on
global image features which limits itself in two aspects: First, by using
global features, these prompts could be focusing less on the discriminative
foreground image, resulting in poor generalization to various
out-of-distribution test cases. Second, existing work weights all prompts
equally whereas intuitively, prompts should be reweighed according to the
semantics of the image. We address these as part of our proposed Contextual
Prompt Learning (CoPL) framework, capable of aligning the prompts to the
localized features of the image. Our key innovations over earlier works include
using local image features as part of the prompt learning process, and more
crucially, learning to weight these prompts based on local features that are
appropriate for the task at hand. This gives us dynamic prompts that are both
aligned to local image features as well as aware of local contextual
relationships. Our extensive set of experiments on a variety of standard and
few-shot datasets show that our method produces substantially improved
performance when compared to the current state of the art methods. We also
demonstrate both few-shot and out-of-distribution performance to establish the
utility of learning dynamic prompts that are aligned to local image features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_K/0/1/0/all/0/1&quot;&gt;Koustava Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1&quot;&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udhayanan_P/0/1/0/all/0/1&quot;&gt;Prateksha Udhayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1&quot;&gt;K J Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1&quot;&gt;Balaji Vasan Srinivasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08131">
<title>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08131</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging network information for predictive modeling has become widespread
in many domains. Within the realm of referral and targeted marketing,
influencer detection stands out as an area that could greatly benefit from the
incorporation of dynamic network representation due to the ongoing development
of customer-brand relationships. To elaborate this idea, we introduce
INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph
Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural
Networks (RNN) with weighted loss functions, the Synthetic Minority
Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted
rolling-window strategy. To evaluate predictive performance, we utilize a
unique corporate data set with networks of three cities and derive a
profit-driven evaluation methodology for influencer prediction. Our results
show how using RNN to encode temporal attributes alongside GNNs significantly
improves predictive performance. We compare the results of various models to
demonstrate the importance of capturing graph representation, temporal
dependencies, and using a profit-driven methodology for evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiukhova_E/0/1/0/all/0/1&quot;&gt;Elena Tiukhova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penaloza_E/0/1/0/all/0/1&quot;&gt;Emiliano Penaloza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oskarsdottir_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a &amp;#xd3;skarsd&amp;#xf3;ttir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baesens_B/0/1/0/all/0/1&quot;&gt;Bart Baesens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoeck_M/0/1/0/all/0/1&quot;&gt;Monique Snoeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1&quot;&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14360">
<title>InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models. (arXiv:2308.14360v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14360</link>
<description rdf:parseType="Literal">&lt;p&gt;Music editing primarily entails the modification of instrument tracks or
remixing in the whole, which offers a novel reinterpretation of the original
piece through a series of operations. These music processing methods hold
immense potential across various applications but demand substantial expertise.
Prior methodologies, although effective for image and audio modifications,
falter when directly applied to music. This is attributed to music&apos;s
distinctive data nature, where such methods can inadvertently compromise the
intrinsic harmony and coherence of music. In this paper, we develop InstructME,
an Instruction guided Music Editing and remixing framework based on latent
diffusion models. Our framework fortifies the U-Net with multi-scale
aggregation in order to maintain consistency before and after editing. In
addition, we introduce chord progression matrix as condition information and
incorporate it in the semantic space to improve melodic harmony while editing.
For accommodating extended musical pieces, InstructME employs a chunk
transformer, enabling it to discern long-term temporal dependencies within
music sequences. We tested InstructME in instrument-editing, remixing, and
multi-round editing. Both subjective and objective evaluations indicate that
our proposed method significantly surpasses preceding systems in music quality,
text relevance and harmony. Demo samples are available at
https://musicedit.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Junyu Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1&quot;&gt;Weituo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xinyan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jitong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yanmin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xuchen Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14815">
<title>Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14815</link>
<description rdf:parseType="Literal">&lt;p&gt;A particularly challenging problem in AI safety is providing guarantees on
the behavior of high-dimensional autonomous systems. Verification approaches
centered around reachability analysis fail to scale, and purely statistical
approaches are constrained by the distributional assumptions about the sampling
process. Instead, we pose a distributionally robust version of the statistical
verification problem for black-box systems, where our performance guarantees
hold over a large family of distributions. This paper proposes a novel approach
based on a combination of active learning, uncertainty quantification, and
neural network verification. A central piece of our approach is an ensemble
technique called Imprecise Neural Networks, which provides the uncertainty to
guide active learning. The active learning uses an exhaustive neural-network
verification tool Sherlock to collect samples. An evaluation on multiple
physical simulators in the openAI gym Mujoco environments with
reinforcement-learned controllers demonstrates that our approach can provide
useful and scalable guarantees for high-dimensional systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Souradeep Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caprio_M/0/1/0/all/0/1&quot;&gt;Michele Caprio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_V/0/1/0/all/0/1&quot;&gt;Vivian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleaveland_M/0/1/0/all/0/1&quot;&gt;Matthew Cleaveland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_K/0/1/0/all/0/1&quot;&gt;Kuk Jin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruchkin_I/0/1/0/all/0/1&quot;&gt;Ivan Ruchkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolsky_O/0/1/0/all/0/1&quot;&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02603">
<title>Detection of Unknown-Unknowns in Human-in-Plant Human-in-Loop Systems Using Physics Guided Process Models. (arXiv:2309.02603v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02603</link>
<description rdf:parseType="Literal">&lt;p&gt;Unknown-unknowns are operational scenarios in systems that are not accounted
for in the design and test phase. In such scenarios, the operational behavior
of the Human-in-loop (HIL) Human-in-Plant (HIP) systems is not guaranteed to
meet requirements such as safety and efficacy. We propose a novel framework for
analyzing the operational output characteristics of safety-critical HIL-HIP
systems that can discover unknown-unknown scenarios and evaluate potential
safety hazards. We propose dynamics-induced hybrid recurrent neural networks
(DiH-RNN) to mine a physics-guided surrogate model (PGSM) that checks for
deviation of the cyber-physical system (CPS) from safety-certified operational
characteristics. The PGSM enables early detection of unknown-unknowns based on
the physical laws governing the system. We demonstrate the detection of
operational changes in an Artificial Pancreas(AP) due to unknown insulin
cartridge errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maity_A/0/1/0/all/0/1&quot;&gt;Aranyak Maity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ayan Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sandeep Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03648">
<title>Promoting Fairness in GNNs: A Characterization of Stability. (arXiv:2309.03648v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03648</link>
<description rdf:parseType="Literal">&lt;p&gt;The Lipschitz bound, a technique from robust statistics, can limit the
maximum changes in the output concerning the input, taking into account
associated irrelevant biased factors. It is an efficient and provable method
for examining the output stability of machine learning models without incurring
additional computation costs. Recently, Graph Neural Networks (GNNs), which
operate on non-Euclidean data, have gained significant attention. However, no
previous research has investigated the GNN Lipschitz bounds to shed light on
stabilizing model outputs, especially when working on non-Euclidean data with
inherent biases. Given the inherent biases in common graph data used for GNN
training, it poses a serious challenge to constraining the GNN output
perturbations induced by input biases, thereby safeguarding fairness during
training. Recently, despite the Lipschitz constant&apos;s use in controlling the
stability of Euclideanneural networks, the calculation of the precise Lipschitz
constant remains elusive for non-Euclidean neural networks like GNNs,
especially within fairness contexts. To narrow this gap, we begin with the
general GNNs operating on an attributed graph, and formulate a Lipschitz bound
to limit the changes in the output regarding biases associated with the input.
Additionally, we theoretically analyze how the Lipschitz constant of a GNN
model could constrain the output perturbations induced by biases learned from
data for fairness training. We experimentally validate the Lipschitz bound&apos;s
effectiveness in limiting biases of the model output. Finally, from a training
dynamics perspective, we demonstrate why the theoretical Lipschitz bound can
effectively guide the GNN training to better trade-off between accuracy and
fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yaning Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunhui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09357">
<title>Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09357</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the plethora of telehealth applications to assist home-based older
adults and healthcare providers, basic messaging and phone calls are still the
most common communication methods, which suffer from limited availability,
information loss, and process inefficiencies. One promising solution to
facilitate patient-provider communication is to leverage large language models
(LLMs) with their powerful natural conversation and summarization capability.
However, there is a limited understanding of LLMs&apos; role during the
communication. We first conducted two interview studies with both older adults
(N=10) and healthcare providers (N=9) to understand their needs and
opportunities for LLMs in patient-provider asynchronous communication. Based on
the insights, we built an LLM-powered communication system, Talk2Care, and
designed interactive components for both groups: (1) For older adults, we
leveraged the convenience and accessibility of voice assistants (VAs) and built
an LLM-powered VA interface for effective information collection. (2) For
health providers, we built an LLM-based dashboard to summarize and present
important health information based on older adults&apos; conversations with the VA.
We further conducted two user studies with older adults and providers to
evaluate the usability of the system. The results showed that Talk2Care could
facilitate the communication process, enrich the health information collected
from older adults, and considerably save providers&apos; efforts and time. We
envision our work as an initial exploration of LLMs&apos; capability in the
intersection of healthcare and interpersonal communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuhai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1&quot;&gt;Bingsheng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogers_E/0/1/0/all/0/1&quot;&gt;Ethan Rogers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Intille_S/0/1/0/all/0/1&quot;&gt;Stephen Intille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shara_N/0/1/0/all/0/1&quot;&gt;Nawar Shara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Guodong Gordon Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dakuo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09431">
<title>FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training. (arXiv:2309.09431v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09431</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pre-training, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pre-training procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pre-training, we also devise efficient
masking strategies for pre-training each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shaheer Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighat_M/0/1/0/all/0/1&quot;&gt;Maryam Haghighat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1&quot;&gt;Tharindu Fernando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12821">
<title>A Spectral Theory of Neural Prediction and Alignment. (arXiv:2309.12821v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12821</link>
<description rdf:parseType="Literal">&lt;p&gt;The representations of neural networks are often compared to those of
biological systems by performing regression between the neural network
responses and those measured from biological systems. Many different
state-of-the-art deep neural networks yield similar neural predictions, but it
remains unclear how to differentiate among models that perform equally well at
predicting neural responses. To gain insight into this, we use a recent
theoretical framework that relates the generalization error from regression to
the spectral properties of the model and the target. We apply this theory to
the case of regression between model activations and neural responses and
decompose the neural prediction error in terms of the model eigenspectra,
alignment of model eigenvectors and neural responses, and the training set
size. Using this decomposition, we introduce geometrical measures to interpret
the neural prediction error. We test a large number of deep neural networks
that predict visual cortical activity and show that there are multiple types of
geometries that result in low neural prediction error as measured via
regression. The work demonstrates that carefully decomposing representational
metrics can provide interpretability of how models are capturing neural
activity and points the way towards improved models of neural activity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Canatar_A/0/1/0/all/0/1&quot;&gt;Abdulkadir Canatar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feather_J/0/1/0/all/0/1&quot;&gt;Jenelle Feather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wakhloo_A/0/1/0/all/0/1&quot;&gt;Albert Wakhloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;SueYeon Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13788">
<title>Can LLM-Generated Misinformation Be Detected?. (arXiv:2309.13788v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13788</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of Large Language Models (LLMs) has made a transformative impact.
However, the potential that LLMs such as ChatGPT can be exploited to generate
misinformation has posed a serious concern to online safety and public trust. A
fundamental research question is: will LLM-generated misinformation cause more
harm than human-written misinformation? We propose to tackle this question from
the perspective of detection difficulty. We first build a taxonomy of
LLM-generated misinformation. Then we categorize and validate the potential
real-world methods for generating misinformation with LLMs. Then, through
extensive empirical investigation, we discover that LLM-generated
misinformation can be harder to detect for humans and detectors compared to
human-written misinformation with the same semantics, which suggests it can
have more deceptive styles and potentially cause more harm. We also discuss the
implications of our discovery on combating misinformation in the age of LLMs
and the countermeasures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Canyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1&quot;&gt;Kai Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16512">
<title>From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford&apos;s Geometric Algebra and Convexity. (arXiv:2309.16512v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16512</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel analysis of neural networks based on
geometric (Clifford) algebra and convex optimization. We show that optimal
weights of deep ReLU neural networks are given by the wedge product of training
samples when trained with standard regularized loss. Furthermore, the training
problem reduces to convex optimization over wedge product features, which
encode the geometric structure of the training dataset. This structure is given
in terms of signed volumes of triangles and parallelotopes generated by data
vectors. The convex problem finds a small subset of samples via $\ell_1$
regularization to discover only relevant wedge product features. Our analysis
provides a novel perspective on the inner workings of deep neural networks and
sheds light on the role of the hidden layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1&quot;&gt;Mert Pilanci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17453">
<title>Efficient Streaming Language Models with Attention Sinks. (arXiv:2309.17453v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17453</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying Large Language Models (LLMs) in streaming applications such as
multi-round dialogue, where long interactions are expected, is urgently needed
but poses two major challenges. Firstly, during the decoding stage, caching
previous tokens&apos; Key and Value states (KV) consumes extensive memory. Secondly,
popular LLMs cannot generalize to longer texts than the training sequence
length. Window attention, where only the most recent KVs are cached, is a
natural approach -- but we show that it fails when the text length surpasses
the cache size. We observe an interesting phenomenon, namely attention sink,
that keeping the KV of initial tokens will largely recover the performance of
window attention. In this paper, we first demonstrate that the emergence of
attention sink is due to the strong attention scores towards initial tokens as
a ``sink&apos;&apos; even if they are not semantically important. Based on the above
analysis, we introduce StreamingLLM, an efficient framework that enables LLMs
trained with a finite length attention window to generalize to infinite
sequence lengths without any fine-tuning. We show that StreamingLLM can enable
Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language
modeling with up to 4 million tokens and more. In addition, we discover that
adding a placeholder token as a dedicated attention sink during pre-training
can further improve streaming deployment. In streaming settings, StreamingLLM
outperforms the sliding window recomputation baseline by up to 22.2x speedup.
Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1&quot;&gt;Guangxuan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Beidi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04816">
<title>Hacking Generative Models with Differentiable Network Bending. (arXiv:2310.04816v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04816</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a method to &apos;hack&apos; generative models, pushing their
outputs away from the original training distribution towards a new objective.
We inject a small-scale trainable module between the intermediate layers of the
model and train it for a low number of iterations, keeping the rest of the
network frozen. The resulting output images display an uncanny quality, given
by the tension between the original and new objectives that can be exploited
for artistic purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aldegheri_G/0/1/0/all/0/1&quot;&gt;Giacomo Aldegheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogalska_A/0/1/0/all/0/1&quot;&gt;Alina Rogalska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youssef_A/0/1/0/all/0/1&quot;&gt;Ahmed Youssef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1&quot;&gt;Eugenia Iofinova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10998">
<title>Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10998</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse
array of applications. However, the sheer size of large-scale graphs presents a
significant challenge to real-time inference with GNNs. Although existing
Scalable GNNs leverage linear propagation to preprocess the features and
accelerate the training and inference procedure, these methods still suffer
from scalability issues when making inferences on unseen nodes, as the feature
preprocessing requires the graph to be known and fixed. To further accelerate
Scalable GNNs inference in this inductive setting, we propose an online
propagation framework and two novel node-adaptive propagation methods that can
customize the optimal propagation depth for each node based on its topological
information and thereby avoid redundant feature propagation. The trade-off
between accuracy and latency can be flexibly managed through simple
hyper-parameters to accommodate various latency constraints. Moreover, to
compensate for the inference accuracy loss caused by the potential early
termination of propagation, we further propose Inception Distillation to
exploit the multi-scale receptive field information within graphs. The rigorous
and comprehensive experimental study on public datasets with varying scales and
characteristics demonstrates that the proposed inference acceleration framework
outperforms existing state-of-the-art graph inference acceleration methods in
terms of accuracy and efficiency. Particularly, the superiority of our approach
is notable on datasets with larger scales, yielding a 75x inference speedup on
the largest Ogbn-products dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junliang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yingxia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Viet Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19797">
<title>DEFT: Dexterous Fine-Tuning for Real-World Hand Policies. (arXiv:2310.19797v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19797</link>
<description rdf:parseType="Literal">&lt;p&gt;Dexterity is often seen as a cornerstone of complex manipulation. Humans are
able to perform a host of skills with their hands, from making food to
operating tools. In this paper, we investigate these challenges, especially in
the case of soft, deformable objects as well as complex, relatively
long-horizon tasks. However, learning such behaviors from scratch can be data
inefficient. To circumvent this, we propose a novel approach, DEFT (DExterous
Fine-Tuning for Hand Policies), that leverages human-driven priors, which are
executed directly in the real world. In order to improve upon these priors,
DEFT involves an efficient online optimization procedure. With the integration
of human-based learning and online fine-tuning, coupled with a soft robotic
hand, DEFT demonstrates success across various tasks, establishing a robust,
data-efficient pathway toward general dexterous manipulation. Please see our
website at https://dexterous-finetuning.github.io for video results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1&quot;&gt;Aditya Kannan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1&quot;&gt;Kenneth Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahl_S/0/1/0/all/0/1&quot;&gt;Shikhar Bahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannam_P/0/1/0/all/0/1&quot;&gt;Pragna Mannam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02227">
<title>State-Wise Safe Reinforcement Learning With Pixel Observations. (arXiv:2311.02227v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02227</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of safe exploration, Reinforcement Learning (RL) has long
grappled with the challenges of balancing the tradeoff between maximizing
rewards and minimizing safety violations, particularly in complex environments
with contact-rich or non-smooth dynamics, and when dealing with
high-dimensional pixel observations. Furthermore, incorporating state-wise
safety constraints in the exploration and learning process, where the agent
must avoid unsafe regions without prior knowledge, adds another layer of
complexity. In this paper, we propose a novel pixel-observation safe RL
algorithm that efficiently encodes state-wise safety constraints with unknown
hazard regions through a newly introduced latent barrier-like function learning
mechanism. As a joint learning framework, our approach begins by constructing a
latent dynamics model with low-dimensional latent spaces derived from pixel
observations. We then build and learn a latent barrier-like function on top of
the latent dynamics and conduct policy optimization simultaneously, thereby
improving both safety and the total expected return. Experimental evaluations
on the safety-gym benchmark suite demonstrate that our proposed method
significantly reduces safety violations throughout the training process, and
demonstrates faster safety convergence compared to existing methods while
achieving competitive results in reward return.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1&quot;&gt;Simon Sinong Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1&quot;&gt;Ruochen Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08360">
<title>The Transient Nature of Emergent In-Context Learning in Transformers. (arXiv:2311.08360v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08360</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer neural networks can exhibit a surprising capacity for in-context
learning (ICL) despite not being explicitly trained for it. Prior work has
provided a deeper understanding of how ICL emerges in transformers, e.g.
through the lens of mechanistic interpretability, Bayesian inference, or by
examining the distributional properties of training data. However, in each of
these cases, ICL is treated largely as a persistent phenomenon; namely, once
ICL emerges, it is assumed to persist asymptotically. Here, we show that the
emergence of ICL during transformer training is, in fact, often transient. We
train transformers on synthetic data designed so that both ICL and in-weights
learning (IWL) strategies can lead to correct predictions. We find that ICL
first emerges, then disappears and gives way to IWL, all while the training
loss decreases, indicating an asymptotic preference for IWL. The transient
nature of ICL is observed in transformers across a range of model sizes and
datasets, raising the question of how much to &quot;overtrain&quot; transformers when
seeking compact, cheaper-to-run models. We find that L2 regularization may
offer a path to more persistent ICL that removes the need for early stopping
based on ICL-style validation tasks. Finally, we present initial evidence that
ICL transience may be caused by competition between ICL and IWL circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aaditya K. Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Stephanie C.Y. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1&quot;&gt;Ted Moskovitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1&quot;&gt;Erin Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1&quot;&gt;Andrew M. Saxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09247">
<title>Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. (arXiv:2311.09247v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09247</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmarini_A/0/1/0/all/0/1&quot;&gt;Alessandro B. Palmarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1&quot;&gt;Arseny Moskvichev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15497">
<title>Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15497</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration has traditionally been done using two distinct approaches:
learning based methods, relying on robust deep neural networks, and
optimization-based methods, applying complex mathematical transformations to
warp images accordingly. Of course, both paradigms offer advantages and
disadvantages, and, in this work, we seek to combine their respective strengths
into a single streamlined framework, using the outputs of the learning based
method as initial parameters for optimization while prioritizing computational
power for the image pairs that offer the greatest loss. Our investigations
showed that an improvement of 1.5% in testing when utilizing the best
performing state-of-the-art model as the backbone of the framework, while
maintaining the same inference time and a substantial 0.94% points performance
gain in deformation field smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1&quot;&gt;Gabriel De Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shanlin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00258">
<title>Precipitation Nowcasting With Spatial And Temporal Transfer Learning Using Swin-UNETR. (arXiv:2312.00258v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00258</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change has led to an increase in frequency of extreme weather events.
Early warning systems can prevent disasters and loss of life. Managing such
events remain a challenge for both public and private institutions.
Precipitation nowcasting can help relevant institutions to better prepare for
such events. Numerical weather prediction (NWP) has traditionally been used to
make physics based forecasting, and recently deep learning based approaches
have been used to reduce turn-around time for nowcasting. In this work,
recently proposed Swin-UNETR (Swin UNEt TRansformer) is used for precipitation
nowcasting for ten different regions of Europe. Swin-UNETR utilizes a U-shaped
network within which a swin transformer-based encoder extracts multi-scale
features from multiple input channels of satellite image, while CNN-based
decoder makes the prediction. Trained model is capable of nowcasting not only
for the regions for which data is available, but can also be used for new
regions for which data is not available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ajitabh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04316">
<title>Towards Knowledge-driven Autonomous Driving. (arXiv:2312.04316v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04316</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the emerging knowledge-driven autonomous driving
technologies. Our investigation highlights the limitations of current
autonomous driving systems, in particular their sensitivity to data bias,
difficulty in handling long-tail scenarios, and lack of interpretability.
Conversely, knowledge-driven methods with the abilities of cognition,
generalization and life-long learning emerge as a promising way to overcome
these challenges. This paper delves into the essence of knowledge-driven
autonomous driving and examines its core components: dataset \&amp;amp; benchmark,
environment, and driver agent. By leveraging large language models, world
models, neural rendering, and other advanced artificial intelligence
techniques, these components collectively contribute to a more holistic,
adaptive, and intelligent autonomous driving system. The paper systematically
organizes and reviews previous research efforts in this area, and provides
insights and guidance for future research and practical applications of
autonomous driving. We will continually share the latest updates on
cutting-edge developments in knowledge-driven autonomous driving along with the
relevant valuable open-source resources at:
\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yeqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Licheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuemeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04344">
<title>Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies. (arXiv:2312.04344v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04344</link>
<description rdf:parseType="Literal">&lt;p&gt;OpenAI&apos;s latest large vision-language model (LVLM), GPT-4V(ision), has piqued
considerable interest for its potential in medical applications. Despite its
promise, recent studies and internal reviews highlight its underperformance in
specialized medical tasks. This paper explores the boundary of GPT-4V&apos;s
capabilities in medicine, particularly in processing complex imaging data from
endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we
assessed its foundational competencies, identifying substantial areas for
enhancement. Our research emphasizes prompt engineering, an often-underutilized
strategy for improving AI responsiveness. Through iterative testing, we refined
the model&apos;s prompts, significantly improving its interpretative accuracy and
relevance in medical imaging. From our comprehensive evaluations, we distilled
10 effective prompt engineering techniques, each fortifying GPT-4V&apos;s medical
acumen. These methodical enhancements facilitate more reliable, precise, and
clinically valuable insights from GPT-4V, advancing its operability in critical
healthcare environments. Our findings are pivotal for those employing AI in
medicine, providing clear, actionable guidance on harnessing GPT-4V&apos;s full
diagnostic potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengcheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhongying Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yanzhou Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junjun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04691">
<title>Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04691</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) with billions of parameters and pretrained on
massive amounts of data are now capable of near or better than state-of-the-art
performance in a variety of downstream natural language processing tasks.
Neural machine translation (NMT) is one such task that LLMs have been applied
to with great success. However, little research has focused on applying LLMs to
the more difficult subset of NMT called simultaneous translation (SimulMT),
where translation begins before the entire source context is available to the
model. In this paper, we address key challenges facing LLMs fine-tuned for
SimulMT, validate classical SimulMT concepts and practices in the context of
LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,
and introduce Simul-LLM, the first open-source fine-tuning and evaluation
pipeline development framework for LLMs focused on SimulMT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1&quot;&gt;Victor Agostinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wild_M/0/1/0/all/0/1&quot;&gt;Max Wild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1&quot;&gt;Matthew Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuad_K/0/1/0/all/0/1&quot;&gt;Kazi Ahmed Asif Fuad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lizhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04837">
<title>Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction following vision-language (VL) models offer a flexible interface
that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user
to &quot;point to&quot; and access specific regions within images. This capability is
important not only to support reference-grounded VL benchmarks, but also, for
practical applications that require precise within-image reasoning. We build
Localized Visual Commonsense models, which allow users to specify (multiple)
regions as input. We train our model by sampling localized commonsense
knowledge from a large language model (LLM): specifically, we prompt an LLM to
collect commonsense knowledge given a global literal image description and a
local literal region description automatically generated by a set of VL models.
With a separately trained critic model that selects high-quality examples, we
find that training on the localized commonsense corpus can successfully distill
existing VL models to support a reference-as-input interface. Empirical results
and human evaluations in a zero-shot setup demonstrate that our distillation
method results in more precise VL models of reasoning compared to a baseline of
passing a generated referring expression to an LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jae Sung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1&quot;&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Ximing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1&quot;&gt;Peter West&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Youngjae Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiuyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05328">
<title>Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding. (arXiv:2312.05328v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05328</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for accelerating large-scale pre-training with online
data selection policies. For the first time, we demonstrate that model-based
data selection can reduce the total computation needed to reach the performance
of models trained with uniform sampling. The key insight which enables this
&quot;compute-positive&quot; regime is that small models provide good proxies for the
loss of much larger models, such that computation spent on scoring data can be
drastically scaled down but still significantly accelerate training of the
learner.. These data selection policies also strongly generalize across
datasets and tasks, opening an avenue for further amortizing the overhead of
data scoring by re-using off-the-shelf models and training sequences. Our
methods, ClassAct and ActiveCLIP, require 46% and 51% fewer training updates
and up to 25% less total computation when training visual classifiers on JFT
and multimodal models on ALIGN, respectively. Finally, our paradigm seamlessly
applies to the curation of large-scale image-text datasets, yielding a new
state-of-the-art in several multimodal transfer tasks and pre-training regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_T/0/1/0/all/0/1&quot;&gt;Talfan Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1&quot;&gt;Shreya Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merzic_H/0/1/0/all/0/1&quot;&gt;Hamza Merzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_J/0/1/0/all/0/1&quot;&gt;Jonathan Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1&quot;&gt;Olivier J. Henaff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05488">
<title>Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis. (arXiv:2312.05488v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05488</link>
<description rdf:parseType="Literal">&lt;p&gt;Game theory, as an analytical tool, is frequently utilized to analyze human
behavior in social science research. With the high alignment between the
behavior of Large Language Models (LLMs) and humans, a promising research
direction is to employ LLMs as substitutes for humans in game experiments,
enabling social science research. However, despite numerous empirical
researches on the combination of LLMs and game theory, the capability
boundaries of LLMs in game theory remain unclear. In this research, we endeavor
to systematically analyze LLMs in the context of game theory. Specifically,
rationality, as the fundamental principle of game theory, serves as the metric
for evaluating players&apos; behavior -- building a clear desire, refining belief
about uncertainty, and taking optimal actions. Accordingly, we select three
classical games (dictator game, Rock-Paper-Scissors, and ring-network game) to
analyze to what extent LLMs can achieve rationality in these three aspects. The
experimental results indicate that even the current state-of-the-art LLM
(GPT-4) exhibits substantial disparities compared to humans in game theory. For
instance, LLMs struggle to build desires based on uncommon preferences, fail to
refine belief from many simple patterns, and may overlook or modify refined
belief when taking actions. Therefore, we consider that introducing LLMs into
game experiments in the field of social science should be approached with
greater caution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Caoyun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jindou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05603">
<title>Sim-GPT: Text Similarity via GPT Annotated Data. (arXiv:2312.05603v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05603</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the lack of a large collection of high-quality labeled sentence pairs
with textual similarity scores, existing approaches for Semantic Textual
Similarity (STS) mostly rely on unsupervised techniques or training signals
that are only partially correlated with textual similarity, e.g., NLI-based
datasets. To tackle this issue, in this paper, we propose the strategy of
measuring text similarity via GPT annotated data (Sim-GPT for short). The core
idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which
an STS model is trained. Sim-GPT framework utilizes LLMs to provide a
substantial amount of reliable annotated data filling the gap of the lack of
training signals for STS. Sim-GPT is trained on a one-time generated dataset
using BERT or RoBERTa as the backbone, which offers long-term savings in cost
and speed compared to repeatedly invoking LLMs for each sentence pair. Trained
on the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the
widely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over
the current SOTA PromCSE model. To encourage further advancements of the field,
we release both models and the 371K annotated examples from GPT-4. Code, models
and annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Beiming Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoya Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1&quot;&gt;Eduard Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05695">
<title>The Counterattack of CNNs in Self-Supervised Learning: Larger Kernel Size might be All You Need. (arXiv:2312.05695v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers have been rapidly uprising in computer vision thanks to
their outstanding scaling trends, and gradually replacing convolutional neural
networks (CNNs). Recent works on self-supervised learning (SSL) introduce
siamese pre-training tasks, on which Transformer backbones continue to
demonstrate ever stronger results than CNNs. People come to believe that
Transformers or self-attention modules are inherently more suitable than CNNs
in the context of SSL. However, it is noteworthy that most if not all prior
arts of SSL with CNNs chose the standard ResNets as their backbones, whose
architecture effectiveness is known to already lag behind advanced Vision
Transformers. Therefore, it remains unclear whether the self-attention
operation is crucial for the recent advances in SSL - or CNNs can deliver the
same excellence with more advanced designs, too? Can we close the SSL
performance gap between Transformers and CNNs? To answer these intriguing
questions, we apply self-supervised pre-training to the recently proposed,
stronger lager-kernel CNN architecture and conduct an apple-to-apple comparison
with Transformers, in their SSL performance. Our results show that we are able
to build pure CNN SSL architectures that perform on par with or better than the
best SSL-trained Transformers, by just scaling up convolutional kernel sizes
besides other small tweaks. Impressively, when transferring to the downstream
tasks \texttt{MS COCO} detection and segmentation, our SSL pre-trained CNN
model (trained in 100 epochs) achieves the same good performance as the
300-epoch pre-trained Transformer counterpart. We hope this work can help to
better understand what is essential (or not) for self-supervised learning
backbones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianjin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05725">
<title>FP8-BERT: Post-Training Quantization for Transformer. (arXiv:2312.05725v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05725</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models, such as BERT, have been widely applied in a wide
range of natural language processing tasks. However, one inevitable side effect
is that they require massive memory storage and inference cost when deployed in
production. Quantization is one of the popularized ways to alleviate the cost.
However, the previous 8-bit quantization strategy based on INT8 data format
either suffers from the degradation of accuracy in a Post-Training Quantization
(PTQ) fashion or requires an expensive Quantization-Aware Training (QAT)
process. Recently, a new numeric format FP8 (i.e. floating-point of 8-bits) has
been proposed and supported in commercial AI computing platforms such as H100.
In this paper, we empirically validate the effectiveness of FP8 as a way to do
Post-Training Quantization without significant loss of accuracy, with a simple
calibration and format conversion process. We adopt the FP8 standard proposed
by NVIDIA Corp. (2022) in our extensive experiments of BERT variants on GLUE
and SQuAD v1.1 datasets, and show that PTQ with FP8 can significantly improve
the accuracy upon that with INT8, to the extent of the full-precision model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianchi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1&quot;&gt;Ian En-Hsu Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongkuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05756">
<title>A quantitative fusion strategy of stock picking and timing based on Particle Swarm Optimized-Back Propagation Neural Network and Multivariate Gaussian-Hidden Markov Model. (arXiv:2312.05756v2 [cs.CE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05756</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, machine learning (ML) has brought effective approaches and
novel techniques to economic decision, investment forecasting, and risk
management, etc., coping the variable and intricate nature of economic and
financial environments. For the investment in stock market, this research
introduces a pioneering quantitative fusion model combining stock timing and
picking strategy by leveraging the Multivariate Gaussian-Hidden Markov Model
(MGHMM) and Back Propagation Neural Network optimized by Particle Swarm
(PSO-BPNN). After the information coefficients (IC) between fifty-two factors
that have been winsorized, neutralized and standardized and the return of CSI
300 index are calculated, a given amount of factors that rank ahead are choose
to be candidate factors heading for the input of PSO-BPNN after dimension
reduction by Principal Component Analysis (PCA), followed by a certain amount
of constituent stocks outputted. Subsequently, we conduct the prediction and
trading on the basis of the screening stocks and stock market state outputted
by MGHMM trained using inputting CSI 300 index data after Box-Cox
transformation, bespeaking eximious performance during the period of past four
years. Ultimately, some conventional forecast and trading methods are compared
with our strategy in Chinese stock market. Our fusion strategy incorporating
stock picking and timing presented in this article provide a innovative
technique for financial analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huajian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longjian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiajian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weinan Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06037">
<title>Multimodality of AI for Education: Towards Artificial General Intelligence. (arXiv:2312.06037v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06037</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive examination of how multimodal artificial
intelligence (AI) approaches are paving the way towards the realization of
Artificial General Intelligence (AGI) in educational contexts. It scrutinizes
the evolution and integration of AI in educational systems, emphasizing the
crucial role of multimodality, which encompasses auditory, visual, kinesthetic,
and linguistic modes of learning. This research delves deeply into the key
facets of AGI, including cognitive frameworks, advanced knowledge
representation, adaptive learning mechanisms, strategic planning, sophisticated
language processing, and the integration of diverse multimodal data sources. It
critically assesses AGI&apos;s transformative potential in reshaping educational
paradigms, focusing on enhancing teaching and learning effectiveness, filling
gaps in existing methodologies, and addressing ethical considerations and
responsible usage of AGI in educational settings. The paper also discusses the
implications of multimodal AI&apos;s role in education, offering insights into
future directions and challenges in AGI development. This exploration aims to
provide a nuanced understanding of the intersection between AI, multimodality,
and education, setting a foundation for future research and development in AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyeong-Geon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lehong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1&quot;&gt;Ehsan Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yizhu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bewersdorff_A/0/1/0/all/0/1&quot;&gt;Arne Bewersdorff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nyaaba_M/0/1/0/all/0/1&quot;&gt;Matthew Nyaaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shuchen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1&quot;&gt;Gengchen Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06363">
<title>MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples. (arXiv:2312.06363v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06363</link>
<description rdf:parseType="Literal">&lt;p&gt;Although In-Context Learning (ICL) brings remarkable performance gains to
Large Language Models (LLMs), the improvements remain lower than fine-tuning on
downstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),
a novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by
fully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We
propose the Multi-Modal Hub (M-Hub), a unified module that captures various
multi-modal features according to different inputs and objectives. Based on
M-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual
features and subsequently generate outputs conditioned on the textual-guided
visual features. Moreover, leveraging the flexibility of M-Hub, we design a
variety of in-context demonstrations. Extensive experiments on a diverse range
of downstream multi-modal tasks demonstrate that MMICT significantly
outperforms traditional fine-tuning strategy and the vanilla ICT method that
directly takes the concatenation of all information from different modalities
as input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06436">
<title>Reward Certification for Policy Smoothed Reinforcement Learning. (arXiv:2312.06436v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06436</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) has achieved remarkable success in
safety-critical areas, but it can be weakened by adversarial attacks. Recent
studies have introduced &quot;smoothed policies&quot; in order to enhance its robustness.
Yet, it is still challenging to establish a provable guarantee to certify the
bound of its total reward. Prior methods relied primarily on computing bounds
using Lipschitz continuity or calculating the probability of cumulative reward
above specific thresholds. However, these techniques are only suited for
continuous perturbations on the RL agent&apos;s observations and are restricted to
perturbations bounded by the $l_2$-norm. To address these limitations, this
paper proposes a general black-box certification method capable of directly
certifying the cumulative reward of the smoothed policy under various
$l_p$-norm bounded perturbations. Furthermore, we extend our methodology to
certify perturbations on action spaces. Our approach leverages f-divergence to
measure the distinction between the original distribution and the perturbed
distribution, subsequently determining the certification bound by solving a
convex optimisation problem. We provide a comprehensive theoretical analysis
and run sufficient experiments in multiple environments. Our results show that
our method not only improves the certified lower bound of mean cumulative
reward but also demonstrates better efficiency than state-of-the-art
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_R/0/1/0/all/0/1&quot;&gt;Ronghui Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcolino_L/0/1/0/all/0/1&quot;&gt;Leandro Soriano Marcolino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianle Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06564">
<title>Promoting Counterfactual Robustness through Diversity. (arXiv:2312.06564v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06564</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual explanations shed light on the decisions of black-box models
by explaining how an input can be altered to obtain a favourable decision from
the model (e.g., when a loan application has been rejected). However, as noted
recently, counterfactual explainers may lack robustness in the sense that a
minor change in the input can cause a major change in the explanation. This can
cause confusion on the user side and open the door for adversarial attacks. In
this paper, we study some sources of non-robustness. While there are
fundamental reasons for why an explainer that returns a single counterfactual
cannot be robust in all instances, we show that some interesting robustness
guarantees can be given by reporting multiple rather than a single
counterfactual. Unfortunately, the number of counterfactuals that need to be
reported for the theoretical guarantees to hold can be prohibitively large. We
therefore propose an approximation algorithm that uses a diversity criterion to
select a feasible number of most relevant explanations and study its robustness
empirically. Our experiments indicate that our method improves the
state-of-the-art in generating robust explanations, while maintaining other
desirable properties and providing competitive computational performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leofante_F/0/1/0/all/0/1&quot;&gt;Francesco Leofante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1&quot;&gt;Nico Potyka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06643">
<title>Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration. (arXiv:2312.06643v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06643</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative robots (cobots) are widely used in industrial applications, yet
extensive research is still needed to enhance human-robot collaborations and
operator experience. A potential approach to improve the collaboration
experience involves adapting cobot behavior based on natural cues from the
operator. Inspired by the literature on human-human interactions, we conducted
a wizard-of-oz study to examine whether a gaze towards the cobot can serve as a
trigger for initiating joint activities in collaborative sessions. In this
study, 37 participants engaged in an assembly task while their gaze behavior
was analyzed. We employ a gaze-based attention recognition model to identify
when the participants look at the cobot. Our results indicate that in most
cases (84.88\%), the joint activity is preceded by a gaze towards the cobot.
Furthermore, during the entire assembly cycle, the participants tend to look at
the cobot around the time of the joint activity. To the best of our knowledge,
this is the first study to analyze the natural gaze behavior of participants
working on a joint activity with a robot during a collaborative assembly task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prajod_P/0/1/0/all/0/1&quot;&gt;Pooja Prajod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicora_M/0/1/0/all/0/1&quot;&gt;Matteo Lavit Nicora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondellini_M/0/1/0/all/0/1&quot;&gt;Marta Mondellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tauro_G/0/1/0/all/0/1&quot;&gt;Giovanni Tauro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vertechy_R/0/1/0/all/0/1&quot;&gt;Rocco Vertechy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malosio_M/0/1/0/all/0/1&quot;&gt;Matteo Malosio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1&quot;&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06648">
<title>Dense X Retrieval: What Retrieval Granularity Should We Use?. (arXiv:2312.06648v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06648</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense retrieval has become a prominent method to obtain relevant context or
world knowledge in open-domain NLP tasks. When we use a learned dense retriever
on a retrieval corpus at inference time, an often-overlooked design choice is
the retrieval unit in which the corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit choice significantly impacts the
performance of both retrieval and downstream tasks. Distinct from the typical
approach of using passages or sentences, we introduce a novel retrieval unit,
proposition, for dense retrieval. Propositions are defined as atomic
expressions within text, each encapsulating a distinct factoid and presented in
a concise, self-contained natural language format. We conduct an empirical
comparison of different retrieval granularity. Our results reveal that
proposition-based retrieval significantly outperforms traditional passage or
sentence-based methods in dense retrieval. Moreover, retrieval by proposition
also enhances the performance of downstream QA tasks, since the retrieved texts
are more condensed with question-relevant information, reducing the need for
lengthy input tokens and minimizing the inclusion of extraneous, irrelevant
information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kaixin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinran Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04547">
<title>Digital Life Project: Autonomous 3D Characters with Social Intelligence. (arXiv:2312.04547v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.04547</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present Digital Life Project, a framework utilizing language
as the universal medium to build autonomous 3D characters, who are capable of
engaging in social interactions and expressing with articulated body motions,
thereby simulating life in a digital environment. Our framework comprises two
primary components: 1) SocioMind: a meticulously crafted digital brain that
models personalities with systematic few-shot exemplars, incorporates a
reflection process based on psychology principles, and emulates autonomy by
initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis
paradigm for controlling the character&apos;s digital body. It integrates motion
matching, a proven industry technique to ensure motion quality, with
cutting-edge advancements in motion generation for diversity. Extensive
experiments demonstrate that each module achieves state-of-the-art performance
in its respective domain. Collectively, they enable virtual characters to
initiate and sustain dialogues autonomously, while evolving their
socio-psychological states. Concurrently, these characters can perform
contextually relevant bodily movements. Additionally, a motion captioning
module further allows the virtual character to recognize and appropriately
respond to human players&apos; actions. Homepage: https://digital-life-project.com/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jianping Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1&quot;&gt;Zhongfei Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xinying Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Haiyi Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruisi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wanqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiangyu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Han Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhitao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianxiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yukun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>