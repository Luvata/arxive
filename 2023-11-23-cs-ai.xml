<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.07780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.04923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.09076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.00955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.10793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11995" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.12040">
<title>TransCDR: a deep learning model for enhancing the generalizability of cancer drug response prediction through transfer learning and multimodal data fusion for drug representation. (arXiv:2311.12040v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.12040</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and robust drug response prediction is of utmost importance in
precision medicine. Although many models have been developed to utilize the
representations of drugs and cancer cell lines for predicting cancer drug
responses (CDR), their performances can be improved by addressing issues such
as insufficient data modality, suboptimal fusion algorithms, and poor
generalizability for novel drugs or cell lines. We introduce TransCDR, which
uses transfer learning to learn drug representations and fuses multi-modality
features of drugs and cell lines by a self-attention mechanism, to predict the
IC50 values or sensitive states of drugs on cell lines. We are the first to
systematically evaluate the generalization of the CDR prediction model to novel
(i.e., never-before-seen) compound scaffolds and cell line clusters. TransCDR
shows better generalizability than 8 state-of-the-art models. TransCDR
outperforms its 5 variants that train drug encoders (i.e., RNN and AttentiveFP)
from scratch under various scenarios. The most critical contributors among
multiple drug notations and omics profiles are Extended Connectivity
Fingerprint and genetic mutation. Additionally, the attention-based fusion
module further enhances the predictive performance of TransCDR. TransCDR,
trained on the GDSC dataset, demonstrates strong predictive performance on the
external testing set CCLE. It is also utilized to predict missing CDRs on GDSC.
Moreover, we investigate the biological mechanisms underlying drug response by
classifying 7,675 patients from TCGA into drug-sensitive or drug-resistant
groups, followed by a Gene Set Enrichment Analysis. TransCDR emerges as a
potent tool with significant potential in drug response prediction. The source
code and data can be accessed at https://github.com/XiaoqiongXia/TransCDR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xia_X/0/1/0/all/0/1&quot;&gt;Xiaoqiong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chaoyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Yuqi Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhong_F/0/1/0/all/0/1&quot;&gt;Fan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12043">
<title>Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation. (arXiv:2311.12043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12043</link>
<description rdf:parseType="Literal">&lt;p&gt;Although 3D human pose estimation has gained impressive development in recent
years, only a few works focus on infants, that have different bone lengths and
also have limited data. Directly applying adult pose estimation models
typically achieves low performance in the infant domain and suffers from
out-of-distribution issues. Moreover, the limitation of infant pose data
collection also heavily constrains the efficiency of learning-based models to
lift 2D poses to 3D. To deal with the issues of small datasets, domain
adaptation and data augmentation are commonly used techniques. Following this
paradigm, we take advantage of an optimization-based method that utilizes
generative priors to predict 3D infant keypoints from 2D keypoints without the
need of large training data. We further apply a guided diffusion model to
domain adapt 3D adult pose to infant pose to supplement small datasets.
Besides, we also prove that our method, ZeDO-i, could attain efficient domain
adaptation, even if only a small number of data is given. Quantitatively, we
claim that our model attains state-of-the-art MPJPE performance of 43.6 mm on
the SyRIP dataset and 21.2 mm on the MINI-RGBD dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Wenhao Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12047">
<title>Multimodal Machine Unlearning. (arXiv:2311.12047v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12047</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Unlearning is the process of removing specific training data samples
and their corresponding effects from an already trained model. It has
significant practical benefits, such as purging private, inaccurate, or
outdated information from trained models without the need for complete
re-training. Unlearning within a multimodal setting presents unique challenges
due to the intrinsic dependencies between different data modalities and the
expensive cost of training on large multimodal datasets and architectures.
Current approaches to machine unlearning have not fully addressed these
challenges. To bridge this gap, we introduce MMUL, a machine unlearning
approach specifically designed for multimodal data and models. MMUL formulates
the multimodal unlearning task by focusing on three key properties: (a):
modality decoupling, which effectively decouples the association between
individual unimodal data points within multimodal inputs marked for deletion,
rendering them as unrelated data points within the model&apos;s context, (b):
unimodal knowledge retention, which retains the unimodal representation
capability of the model post-unlearning, and (c): multimodal knowledge
retention, which retains the multimodal representation capability of the model
post-unlearning. MMUL is efficient to train and is not constrained by the
requirement of using a strongly convex loss. Experiments on two multimodal
models and four multimodal benchmark datasets, including vision-language and
graph-language datasets, show that MMUL outperforms existing baselines, gaining
an average improvement of +17.6 points against the best-performing unimodal
baseline in distinguishing between deleted and remaining data. In addition,
MMUL can largely maintain pre-existing knowledge of the original model post
unlearning, with a performance gap of only 0.3 points compared to retraining a
new model from scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiali Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1&quot;&gt;Hadi Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12056">
<title>Kuro Siwo: 12.1 billion $m^2$ under the water. A global multi-temporal satellite dataset for rapid flood mapping. (arXiv:2311.12056v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12056</link>
<description rdf:parseType="Literal">&lt;p&gt;Global floods, exacerbated by climate change, pose severe threats to human
life, infrastructure, and the environment. This urgency is highlighted by
recent catastrophic events in Pakistan and New Zealand, underlining the
critical need for precise flood mapping for guiding restoration efforts,
understanding vulnerabilities, and preparing for future events. While Synthetic
Aperture Radar (SAR) offers day-and-night, all-weather imaging capabilities,
harnessing it for deep learning is hindered by the absence of a large annotated
dataset. To bridge this gap, we introduce Kuro Siwo, a meticulously curated
multi-temporal dataset, spanning 32 flood events globally. Our dataset maps
more than 63 billion m2 of land, with 12.1 billion of them being either a
flooded area or a permanent water body. Kuro Siwo stands out for its
unparalleled annotation quality to facilitate rapid flood mapping in a
supervised setting. We also augment learning by including a large unlabeled set
of SAR samples, aimed at self-supervised pretraining. We provide an extensive
benchmark and strong baselines for a diverse set of flood events from Europe,
America, Africa and Australia. Our benchmark demonstrates the quality of Kuro
Siwo annotations, training models that can achieve $\approx$ 85% and $\approx$
87% in F1-score for flooded areas and general water detection respectively.
This work calls on the deep learning community to develop solution-driven
algorithms for rapid flood mapping, with the potential to aid civil protection
and humanitarian agencies amid climate change challenges. Our code and data
will be made available at https://github.com/Orion-AI-Lab/KuroSiwo
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Ioannis Bountos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sdraka_M/0/1/0/all/0/1&quot;&gt;Maria Sdraka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavras_A/0/1/0/all/0/1&quot;&gt;Angelos Zavras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karasante_I/0/1/0/all/0/1&quot;&gt;Ilektra Karasante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karavias_A/0/1/0/all/0/1&quot;&gt;Andreas Karavias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herekakis_T/0/1/0/all/0/1&quot;&gt;Themistocles Herekakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thanasou_A/0/1/0/all/0/1&quot;&gt;Angeliki Thanasou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1&quot;&gt;Dimitrios Michail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12062">
<title>PBWR: Parametric Building Wireframe Reconstruction from Aerial LiDAR Point Clouds. (arXiv:2311.12062v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12062</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an end-to-end 3D building wireframe reconstruction
method to regress edges directly from aerial LiDAR point clouds.Our method,
named Parametric Building Wireframe Reconstruction (PBWR), takes aerial LiDAR
point clouds and initial edge entities as input, and fully uses self-attention
mechanism of transformers to regress edge parameters without any intermediate
steps such as corner prediction. We propose an edge non-maximum suppression
(E-NMS) module based on edge similarityto remove redundant edges. Additionally,
a dedicated edge loss function is utilized to guide the PBWR in regressing
edges parameters, where simple use of edge distance loss isn&apos;t suitable. In our
experiments, we demonstrate state-of-the-art results on the Building3D dataset,
achieving an improvement of approximately 36% in entry-level dataset edge
accuracy and around 42% improvement in the Tallinn dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shangfeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Bo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12065">
<title>Few-Shot Classification &amp; Segmentation Using Large Language Models Agent. (arXiv:2311.12065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12065</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of few-shot image classification and segmentation (FS-CS) requires
the classification and segmentation of target objects in a query image, given
only a few examples of the target classes. We introduce a method that utilises
large language models (LLM) as an agent to address the FS-CS problem in a
training-free manner. By making the LLM the task planner and off-the-shelf
vision models the tools, the proposed method is capable of classifying and
segmenting target objects using only image-level labels. Specifically,
chain-of-thought prompting and in-context learning guide the LLM to observe
support images like human; vision models such as Segment Anything Model (SAM)
and GPT-4Vision assist LLM understand spatial and semantic information at the
same time. Ultimately, the LLM uses its summarizing and reasoning capabilities
to classify and segment the query image. The proposed method&apos;s modular
framework makes it easily extendable. Our approach achieves state-of-the-art
performance on the Pascal-5i dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1&quot;&gt;Tian Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wuliang Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12068">
<title>Enhancing Novel Object Detection via Cooperative Foundational Models. (arXiv:2311.12068v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12068</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1&quot;&gt;Rohit Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12074">
<title>SecureBERT and LLAMA 2 Empowered Control Area Network Intrusion Detection and Classification. (arXiv:2311.12074v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.12074</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous studies have proved their effective strength in detecting Control
Area Network (CAN) attacks. In the realm of understanding the human semantic
space, transformer-based models have demonstrated remarkable effectiveness.
Leveraging pre-trained transformers has become a common strategy in various
language-related tasks, enabling these models to grasp human semantics more
comprehensively. To delve into the adaptability evaluation on pre-trained
models for CAN intrusion detection, we have developed two distinct models:
CAN-SecureBERT and CAN-LLAMA2. Notably, our CAN-LLAMA2 model surpasses the
state-of-the-art models by achieving an exceptional performance 0.999993 in
terms of balanced accuracy, precision detection rate, F1 score, and a
remarkably low false alarm rate of 3.10e-6. Impressively, the false alarm rate
is 52 times smaller than that of the leading model, MTH-IDS (Multitiered Hybrid
Intrusion Detection System). Our study underscores the promise of employing a
Large Language Model as the foundational model, while incorporating adapters
for other cybersecurity-related tasks and maintaining the model&apos;s inherent
language-related capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuemei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huirong Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12081">
<title>Leveraging healthy population variability in deep learning unsupervised anomaly detection in brain FDG PET. (arXiv:2311.12081v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12081</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised anomaly detection is a popular approach for the analysis of
neuroimaging data as it allows to identify a wide variety of anomalies from
unlabelled data. It relies on building a subject-specific model of healthy
appearance to which a subject&apos;s image can be compared to detect anomalies. In
the literature, it is common for anomaly detection to rely on analysing the
residual image between the subject&apos;s image and its pseudo-healthy
reconstruction. This approach however has limitations partly due to the
pseudo-healthy reconstructions being imperfect and to the lack of natural
thresholding mechanism. Our proposed method, inspired by Z-scores, leverages
the healthy population variability to overcome these limitations. Our
experiments conducted on FDG PET scans from the ADNI database demonstrate the
effectiveness of our approach in accurately identifying Alzheimer&apos;s disease
related anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Solal_M/0/1/0/all/0/1&quot;&gt;Ma&amp;#xeb;lys Solal&lt;/a&gt; (ARAMIS), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hassanaly_R/0/1/0/all/0/1&quot;&gt;Ravi Hassanaly&lt;/a&gt; (ARAMIS), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Burgos_N/0/1/0/all/0/1&quot;&gt;Ninon Burgos&lt;/a&gt; (ARAMIS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12088">
<title>PhytNet -- Tailored Convolutional Neural Networks for Custom Botanical Data. (arXiv:2311.12088v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12088</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated disease, weed and crop classification with computer vision will be
invaluable in the future of agriculture. However, existing model architectures
like ResNet, EfficientNet and ConvNeXt often underperform on smaller,
specialised datasets typical of such projects. We address this gap with
informed data collection and the development of a new CNN architecture,
PhytNet. Utilising a novel dataset of infrared cocoa tree images, we
demonstrate PhytNet&apos;s development and compare its performance with existing
architectures. Data collection was informed by analysis of spectroscopy data,
which provided useful insights into the spectral characteristics of cocoa
trees. Such information could inform future data collection and model
development. Cocoa was chosen as a focal species due to the diverse pathology
of its diseases, which pose significant challenges for detection. ResNet18
showed some signs of overfitting, while EfficientNet variants showed distinct
signs of overfitting. By contrast, PhytNet displayed excellent attention to
relevant features, no overfitting, and an exceptionally low computation cost
(1.19 GFLOPS). As such PhytNet is a promising candidate for rapid disease or
plant classification, or precise localisation of disease symptoms for
autonomous systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sykes_J/0/1/0/all/0/1&quot;&gt;Jamie R. Sykes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denby_K/0/1/0/all/0/1&quot;&gt;Katherine Denby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franks_D/0/1/0/all/0/1&quot;&gt;Daniel W. Franks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12089">
<title>Explaining Deep Learning Models for Age-related Gait Classification based on time series acceleration. (arXiv:2311.12089v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12089</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait analysis holds significant importance in monitoring daily health,
particularly among older adults. Advancements in sensor technology enable the
capture of movement in real-life environments and generate big data. Machine
learning, notably deep learning (DL), shows promise to use these big data in
gait analysis. However, the inherent black-box nature of these models poses
challenges for their clinical application. This study aims to enhance
transparency in DL-based gait classification for aged-related gait patterns
using Explainable Artificial Intelligence, such as SHAP.
&lt;/p&gt;
&lt;p&gt;A total of 244 subjects, comprising 129 adults and 115 older adults (age&amp;gt;65),
were included. They performed a 3-minute walking task while accelerometers were
affixed to the lumbar segment L3. DL models, convolutional neural network (CNN)
and gated recurrent unit (GRU), were trained using 1-stride and 8-stride
accelerations, respectively, to classify adult and older adult groups. SHAP was
employed to explain the models&apos; predictions.
&lt;/p&gt;
&lt;p&gt;CNN achieved a satisfactory performance with an accuracy of 81.4% and an AUC
of 0.89, and GRU demonstrated promising results with an accuracy of 84.5% and
an AUC of 0.94. SHAP analysis revealed that both CNN and GRU assigned higher
SHAP values to the data from vertical and walking directions, particularly
emphasizing data around heel contact, spanning from the terminal swing to
loading response phases. Furthermore, SHAP values indicated that GRU did not
treat every stride equally.
&lt;/p&gt;
&lt;p&gt;CNN accurately distinguished between adults and older adults based on the
characteristics of a single stride&apos;s data. GRU achieved accurate classification
by considering the relationships and subtle differences between strides. In
both models, data around heel contact emerged as most critical, suggesting
differences in acceleration and deceleration patterns during walking between
different age groups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaoping Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otten_B/0/1/0/all/0/1&quot;&gt;Bert Otten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reneman_M/0/1/0/all/0/1&quot;&gt;Michiel F Reneman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamoth_C/0/1/0/all/0/1&quot;&gt;Claudine JC Lamoth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12125">
<title>Mixing-Denoising Generalizable Occupancy Networks. (arXiv:2311.12125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12125</link>
<description rdf:parseType="Literal">&lt;p&gt;While current state-of-the-art generalizable implicit neural shape models
rely on the inductive bias of convolutions, it is still not entirely clear how
properties emerging from such biases are compatible with the task of 3D
reconstruction from point cloud. We explore an alternative approach to
generalizability in this context. We relax the intrinsic model bias (i.e. using
MLPs to encode local features as opposed to convolutions) and constrain the
hypothesis space instead with an auxiliary regularization related to the
reconstruction task, i.e. denoising. The resulting model is the first only-MLP
locally conditioned implicit shape reconstruction from point cloud network with
fast feed forward inference. Point cloud borne features and denoising offsets
are predicted from an exclusively MLP-made network in a single forward pass. A
decoder predicts occupancy probabilities for queries anywhere in space by
pooling nearby features from the point cloud order-invariantly, guided by
denoised relative positional encoding. We outperform the state-of-the-art
convolutional method while using half the number of model parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouasfi_A/0/1/0/all/0/1&quot;&gt;Amine Ouasfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1&quot;&gt;Adnane Boukhayma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12151">
<title>Teaching Robots to Build Simulations of Themselves. (arXiv:2311.12151v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12151</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation enables robots to plan and estimate the outcomes of prospective
actions without the need to physically execute them. We introduce a
self-supervised learning framework to enable robots model and predict their
morphology, kinematics and motor control using only brief raw video data,
eliminating the need for extensive real-world data collection and kinematic
priors. By observing their own movements, akin to humans watching their
reflection in a mirror, robots learn an ability to simulate themselves and
predict their spatial motion for various tasks. Our results demonstrate that
this self-learned simulation not only enables accurate motion planning but also
allows the robot to detect abnormalities and recover from damage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuhang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12154">
<title>User-Like Bots for Cognitive Automation: A Survey. (arXiv:2311.12154v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.12154</link>
<description rdf:parseType="Literal">&lt;p&gt;Software bots have attracted increasing interest and popularity in both
research and society. Their contributions span automation, digital twins, game
characters with conscious-like behavior, and social media. However, there is
still a lack of intelligent bots that can adapt to web environments&apos;
variability and dynamic nature. Unlike human users, they have difficulty
understanding and exploiting the affordances across multiple virtual
environments.
&lt;/p&gt;
&lt;p&gt;Despite the hype, bots with human user-like cognition do not currently exist.
Chatbots, for instance, lack situational awareness on the digital platforms
where they operate, preventing them from enacting meaningful and autonomous
intelligent behavior similar to human users.
&lt;/p&gt;
&lt;p&gt;In this survey, we aim to explore the role of cognitive architectures in
supporting efforts towards engineering software bots with advanced general
intelligence. We discuss how cognitive architectures can contribute to creating
intelligent software bots. Furthermore, we highlight key architectural
recommendations for the future development of autonomous, user-like cognitive
bots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidey_H/0/1/0/all/0/1&quot;&gt;Habtom Kahsay Gidey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1&quot;&gt;Peter Hillmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karcher_A/0/1/0/all/0/1&quot;&gt;Andreas Karcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12159">
<title>Conditional Modeling Based Automatic Video Summarization. (arXiv:2311.12159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12159</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of video summarization is to shorten videos automatically while
retaining the key information necessary to convey the overall story. Video
summarization methods mainly rely on visual factors, such as visual
consecutiveness and diversity, which may not be sufficient to fully understand
the content of the video. There are other non-visual factors, such as
interestingness, representativeness, and storyline consistency that should also
be considered for generating high-quality video summaries. Current methods do
not adequately take into account these non-visual factors, resulting in
suboptimal performance. In this work, a new approach to video summarization is
proposed based on insights gained from how humans create ground truth video
summaries. The method utilizes a conditional modeling perspective and
introduces multiple meaningful random variables and joint distributions to
characterize the key components of video summarization. Helper distributions
are employed to improve the training of the model. A conditional attention
module is designed to mitigate potential performance degradation in the
presence of multi-modal input. The proposed video summarization method
incorporates the above innovative design choices that aim to narrow the gap
between human-generated and machine-generated video summaries. Extensive
experiments show that the proposed approach outperforms existing methods and
achieves state-of-the-art performance on commonly used video summarization
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Hong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1&quot;&gt;Marcel Worring&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12182">
<title>Common (good) practices measuring trust in HRI. (arXiv:2311.12182v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12182</link>
<description rdf:parseType="Literal">&lt;p&gt;Trust in robots is widely believed to be imperative for the adoption of
robots into people&apos;s daily lives. It is, therefore, understandable that the
literature of the last few decades focuses on measuring how much people trust
robots -- and more generally, any agent - to foster such trust in these
technologies. Researchers have been exploring how people trust robot in
different ways, such as measuring trust on human-robot interactions (HRI) based
on textual descriptions or images without any physical contact, during and
after interacting with the technology. Nevertheless, trust is a complex
behaviour, and it is affected and depends on several factors, including those
related to the interacting agents (e.g. humans, robots, pets), itself (e.g.
capabilities, reliability), the context (e.g. task), and the environment (e.g.
public spaces vs private spaces vs working spaces). In general, most
roboticists agree that insufficient levels of trust lead to a risk of
disengagement while over-trust in technology can cause over-reliance and
inherit dangers, for example, in emergency situations. It is, therefore, very
important that the research community has access to reliable methods to measure
people&apos;s trust in robots and technology. In this position paper, we outline
current methods and their strengths, identify (some) weakly covered aspects and
discuss the potential for covering a more comprehensive amount of factors
influencing trust in HRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holthaus_P/0/1/0/all/0/1&quot;&gt;Patrick Holthaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_A/0/1/0/all/0/1&quot;&gt;Alessandra Rossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12188">
<title>ChatGPT and post-test probability. (arXiv:2311.12188v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12188</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning-based large language models, such as ChatGPT, are
believed to have potential to aid human experts in many domains, including
healthcare. There is, however, little work on ChatGPT&apos;s ability to perform a
key task in healthcare: formal, probabilistic medical diagnostic reasoning.
This type of reasoning is used, for example, to update a pre-test probability
to a post-test probability. In this work, we probe ChatGPT&apos;s ability to perform
this task. In particular, we ask ChatGPT to give examples of how to use Bayes
rule for medical diagnosis. Our prompts range from queries that use terminology
from pure probability (e.g., requests for a &quot;posterior probability&quot;) to queries
that use terminology from the medical diagnosis literature (e.g., requests for
a &quot;post-test probability&quot;). We show how the introduction of medical variable
names leads to an increase in the number of errors that ChatGPT makes. Given
our results, we also show how one can use prompt engineering to facilitate
ChatGPT&apos;s partial avoidance of these errors. We discuss our results in light of
recent commentaries on sensitivity and specificity. We also discuss how our
results might inform new research directions for large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weisenthal_S/0/1/0/all/0/1&quot;&gt;Samuel J. Weisenthal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12198">
<title>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. (arXiv:2311.12198v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2311.12198</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, &quot;cage meshes,&quot; or
any other geometry embedding, highlighting the principle of &quot;what you see is
what you simulate (WS$^2$).&quot; Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tianyi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zeshun Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yutao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenfanfu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12202">
<title>Nepotistically Trained Generative-AI Models Collapse. (arXiv:2311.12202v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12202</link>
<description rdf:parseType="Literal">&lt;p&gt;Trained on massive amounts of human-generated content, AI (artificial
intelligence) image synthesis is capable of reproducing semantically coherent
images that match the visual appearance of its training data. We show that when
retrained on even small amounts of their own creation, these generative-AI
models produce highly distorted images. We also show that this distortion
extends beyond the text prompts used in retraining, and that once poisoned, the
models struggle to fully heal even after retraining on only real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohacek_M/0/1/0/all/0/1&quot;&gt;Matyas Bohacek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1&quot;&gt;Hany Farid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12207">
<title>Defense semantics of argumentation: revisit. (arXiv:2311.12207v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12207</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a novel semantics, called defense semantics, for
Dung&apos;s abstract argumentation frameworks in terms of a notion of (partial)
defence, which is a triple encoding that one argument is (partially) defended
by another argument via attacking the attacker of the first argument. In terms
of defense semantics, we show that defenses related to self-attacked arguments
and arguments in 3-cycles are unsatifiable under any situation and therefore
can be removed without affecting the defense semantics of an AF. Then, we
introduce a new notion of defense equivalence of AFs, and compare defense
equivalence with standard equivalence and strong equivalence, respectively.
Finally, by exploiting defense semantics, we define two kinds of reasons for
accepting arguments, i.e., direct reasons and root reasons, and a notion of
root equivalence of AFs that can be used in argumentation summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Beishui Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_L/0/1/0/all/0/1&quot;&gt;Leendert van der Torre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12223">
<title>Digital Twin-Based User-Centric Edge Continual Learning in Integrated Sensing and Communication. (arXiv:2311.12223v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2311.12223</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a digital twin (DT)-based user-centric approach for
processing sensing data in an integrated sensing and communication (ISAC)
system with high accuracy and efficient resource utilization. The considered
scenario involves an ISAC device with a lightweight deep neural network (DNN)
and a mobile edge computing (MEC) server with a large DNN. After collecting
sensing data, the ISAC device either processes the data locally or uploads them
to the server for higher-accuracy data processing. To cope with data drifts,
the server updates the lightweight DNN when necessary, referred to as continual
learning. Our objective is to minimize the long-term average computation cost
of the MEC server by optimizing two decisions, i.e., sensing data offloading
and sensing data selection for the DNN update. A DT of the ISAC device is
constructed to predict the impact of potential decisions on the long-term
computation cost of the server, based on which the decisions are made with
closed-form formulas. Experiments on executing DNN-based human motion
recognition tasks are conducted to demonstrate the outstanding performance of
the proposed DT-based approach in computation cost minimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shisheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mushu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_K/0/1/0/all/0/1&quot;&gt;Kaige Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Conghao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuemin/0/1/0/all/0/1&quot;&gt;Xuemin&lt;/a&gt; (Sherman) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen/0/1/0/all/0/1&quot;&gt;Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12224">
<title>Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators. (arXiv:2311.12224v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.12224</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new algorithm called the Free-pipeline Fast Inner Product
(FFIP) and its hardware architecture that improve an under-explored fast
inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the
unrelated Winograd minimal filtering algorithms for convolutional layers, FIP
is applicable to all machine learning (ML) model layers that can mainly
decompose to matrix multiplication, including fully-connected, convolutional,
recurrent, and attention/transformer layers. We implement FIP for the first
time in an ML accelerator then present our FFIP algorithm and generalized
architecture which inherently improve FIP&apos;s clock frequency and, as a
consequence, throughput for a similar hardware cost. Finally, we contribute
ML-specific optimizations for the FIP and FFIP algorithms and architectures. We
show that FFIP can be seamlessly incorporated into traditional fixed-point
systolic array ML accelerators to achieve the same throughput with half the
number of multiply-accumulate (MAC) units, or it can double the maximum
systolic array size that can fit onto devices with a fixed hardware budget. Our
FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point
inputs achieves higher throughput and compute efficiency than the best-in-class
prior solutions on the same type of compute platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pogue_T/0/1/0/all/0/1&quot;&gt;Trevor E. Pogue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolici_N/0/1/0/all/0/1&quot;&gt;Nicola Nicolici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12229">
<title>NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation. (arXiv:2311.12229v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12229</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite impressive recent advances in text-to-image diffusion models,
obtaining high-quality images often requires prompt engineering by humans who
have developed expertise in using them. In this work, we present NeuroPrompts,
an adaptive framework that automatically enhances a user&apos;s prompt to improve
the quality of generations produced by text-to-image models. Our framework
utilizes constrained text decoding with a pre-trained language model that has
been adapted to generate prompts similar to those produced by human prompt
engineers. This approach enables higher-quality text-to-image generations and
provides user control over stylistic features via constraint set specification.
We demonstrate the utility of our framework by creating an interactive
application for prompt enhancement and image generation using Stable Diffusion.
Additionally, we conduct experiments utilizing a large dataset of
human-engineered prompts for text-to-image generation and show that our
approach automatically produces enhanced prompts that result in superior image
quality. We make our code, a screencast video demo and a live demo instance of
NeuroPrompts publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenman_S/0/1/0/all/0/1&quot;&gt;Shachar Rosenman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1&quot;&gt;Vasudev Lal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_P/0/1/0/all/0/1&quot;&gt;Phillip Howard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12236">
<title>Ontological Reasoning over Shy and Warded Datalog$+/-$ for Streaming-based Architectures (technical report). (arXiv:2311.12236v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2311.12236</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years witnessed a rising interest towards Datalog-based ontological
reasoning systems, both in academia and industry. These systems adopt
languages, often shared under the collective name of Datalog$+/-$, that extend
Datalog with the essential feature of existential quantification, while
introducing syntactic limitations to sustain reasoning decidability and achieve
a good trade-off between expressive power and computational complexity. From an
implementation perspective, modern reasoners borrow the vast experience of the
database community in developing streaming-based data processing systems, such
as volcano-iterator architectures, that sustain a limited memory footprint and
good scalability. In this paper, we focus on two extremely promising,
expressive, and tractable languages, namely, Shy and Warded Datalog$+/-$. We
leverage their theoretical underpinnings to introduce novel reasoning
techniques, technically, &quot;chase variants&quot;, that are particularly fit for
efficient reasoning in streaming-based architectures. We then implement them in
Vadalog, our reference streaming-based engine, to efficiently solve ontological
reasoning tasks over real-world settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldazzi_T/0/1/0/all/0/1&quot;&gt;Teodoro Baldazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellomarini_L/0/1/0/all/0/1&quot;&gt;Luigi Bellomarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favorito_M/0/1/0/all/0/1&quot;&gt;Marco Favorito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallinger_E/0/1/0/all/0/1&quot;&gt;Emanuel Sallinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12241">
<title>InteraSSort: Interactive Assortment Planning Using Large Language Models. (arXiv:2311.12241v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12241</link>
<description rdf:parseType="Literal">&lt;p&gt;Assortment planning, integral to multiple commercial offerings, is a key
problem studied in e-commerce and retail settings. Numerous variants of the
problem along with their integration into business solutions have been
thoroughly investigated in the existing literature. However, the nuanced
complexities of in-store planning and a lack of optimization proficiency among
store planners with strong domain expertise remain largely overlooked. These
challenges frequently necessitate collaborative efforts with multiple
stakeholders which often lead to prolonged decision-making processes and
significant delays. To mitigate these challenges and capitalize on the
advancements of Large Language Models (LLMs), we propose an interactive
assortment planning framework, InteraSSort that augments LLMs with optimization
tools to assist store planners in making decisions through interactive
conversations. Specifically, we develop a solution featuring a user-friendly
interface that enables users to express their optimization objectives as input
text prompts to InteraSSort and receive tailored optimized solutions as output.
Our framework extends beyond basic functionality by enabling the inclusion of
additional constraints through interactive conversation, facilitating precise
and highly customized decision-making. Extensive experiments demonstrate the
effectiveness of our framework and potential extensions to a broad range of
operations management challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1&quot;&gt;Saketh Reddy Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulabandhula_T/0/1/0/all/0/1&quot;&gt;Theja Tulabandhula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12244">
<title>Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning. (arXiv:2311.12244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world reinforcement learning problems, the state information is often
only partially observable, which breaks the basic assumption in Markov decision
processes, and thus, leads to inferior performances. Partially Observable
Markov Decision Processes have been introduced to explicitly take the issue
into account for learning, exploration, and planning, but presenting
significant computational and statistical challenges. To address these
difficulties, we exploit the representation view, which leads to a coherent
design framework for a practically tractable reinforcement learning algorithm
upon partial observations. We provide a theoretical analysis for justifying the
statistical efficiency of the proposed algorithm. We also empirically
demonstrate the proposed algorithm can surpass state-of-the-art performance
with partial observations across various benchmarks, therefore, pushing
reliable reinforcement learning towards more practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chenjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12264">
<title>Resilient Control of Networked Microgrids using Vertical Federated Reinforcement Learning: Designs and Real-Time Test-Bed Validations. (arXiv:2311.12264v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.12264</link>
<description rdf:parseType="Literal">&lt;p&gt;Improving system-level resiliency of networked microgrids is an important
aspect with increased population of inverter-based resources (IBRs). This paper
(1) presents resilient control design in presence of adversarial cyber-events,
and proposes a novel federated reinforcement learning (Fed-RL) approach to
tackle (a) model complexities, unknown dynamical behaviors of IBR devices, (b)
privacy issues regarding data sharing in multi-party-owned networked grids, and
(2) transfers learned controls from simulation to hardware-in-the-loop
test-bed, thereby bridging the gap between simulation and real world. With
these multi-prong objectives, first, we formulate a reinforcement learning (RL)
training setup generating episodic trajectories with adversaries (attack
signal) injected at the primary controllers of the grid forming (GFM) inverters
where RL agents (or controllers) are being trained to mitigate the injected
attacks. For networked microgrids, the horizontal Fed-RL method involving
distinct independent environments is not appropriate, leading us to develop
vertical variant Federated Soft Actor-Critic (FedSAC) algorithm to grasp the
interconnected dynamics of networked microgrid. Next, utilizing OpenAI Gym
interface, we built a custom simulation set-up in GridLAB-D/HELICS
co-simulation platform, named Resilient RL Co-simulation (ResRLCoSIM), to train
the RL agents with IEEE 123-bus benchmark test systems comprising 3
interconnected microgrids. Finally, the learned policies in simulation world
are transferred to the real-time hardware-in-the-loop test-bed set-up developed
using high-fidelity Hypersim platform. Experiments show that the
simulator-trained RL controllers produce convincing results with the real-time
test-bed set-up, validating the minimization of sim-to-real gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Sayak Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hossain_R/0/1/0/all/0/1&quot;&gt;Ramij R. Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mohiuddin_S/0/1/0/all/0/1&quot;&gt;Sheik M. Mohiuddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Wei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adetola_V/0/1/0/all/0/1&quot;&gt;Veronica Adetola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jinsiwale_R/0/1/0/all/0/1&quot;&gt;Rohit A. Jinsiwale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiuhua Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yin_T/0/1/0/all/0/1&quot;&gt;Tianzhixi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singhal_A/0/1/0/all/0/1&quot;&gt;Ankit Singhal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12267">
<title>Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity. (arXiv:2311.12267v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12267</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies causal representation learning, the task of recovering
high-level latent variables and their causal relationships from low-level data
that we observe, assuming access to observations generated from multiple
environments. While existing works are able to prove full identifiability of
the underlying data generating process, they typically assume access to
single-node, hard interventions which is rather unrealistic in practice. The
main contribution of this paper is characterize a notion of identifiability
which is provably the best one can achieve when hard interventions are not
available. First, for linear causal models, we provide identifiability
guarantee for data observed from general environments without assuming any
similarities between them. While the causal graph is shown to be fully
recovered, the latent variables are only identified up to an effect-domination
ambiguity (EDA). We then propose an algorithm, LiNGCReL which is guaranteed to
recover the ground-truth model up to EDA, and we demonstrate its effectiveness
via numerical experiments. Moving on to general non-parametric causal models,
we prove the same idenfifiability guarantee assuming access to groups of soft
interventions. Finally, we provide counterparts of our identifiability results,
indicating that EDA is basically inevitable in our setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jikai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syrgkanis_V/0/1/0/all/0/1&quot;&gt;Vasilis Syrgkanis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12279">
<title>Probabilistic Forecast Reconciliation with Kullback-Leibler Divergence Regularization. (arXiv:2311.12279v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12279</link>
<description rdf:parseType="Literal">&lt;p&gt;As the popularity of hierarchical point forecast reconciliation methods
increases, there is a growing interest in probabilistic forecast
reconciliation. Many studies have utilized machine learning or deep learning
techniques to implement probabilistic forecasting reconciliation and have made
notable progress. However, these methods treat the reconciliation step as a
fixed and hard post-processing step, leading to a trade-off between accuracy
and coherency. In this paper, we propose a new approach for probabilistic
forecast reconciliation. Unlike existing approaches, our proposed approach
fuses the prediction step and reconciliation step into a deep learning
framework, making the reconciliation step more flexible and soft by introducing
the Kullback-Leibler divergence regularization term into the loss function. The
approach is evaluated using three hierarchical time series datasets, which
shows the advantages of our approach over other probabilistic forecast
reconciliation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guanyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yanfei Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12287">
<title>Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications. (arXiv:2311.12287v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.12287</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of Large Language Models (LLMs) heralds a pivotal shift in online
user interactions with information. Traditional Information Retrieval (IR)
systems primarily relied on query-document matching, whereas LLMs excel in
comprehending and generating human-like text, thereby enriching the IR
experience significantly. While LLMs are often associated with chatbot
functionalities, this paper extends the discussion to their explicit
application in information retrieval. We explore methodologies to optimize the
retrieval process, select optimal models, and effectively scale and orchestrate
LLMs, aiming for cost-efficiency and enhanced result accuracy. A notable
challenge, model hallucination-where the model yields inaccurate or
misinterpreted data-is addressed alongside other model-specific hurdles. Our
discourse extends to crucial considerations including user privacy, data
optimization, and the necessity for system clarity and interpretability.
Through a comprehensive examination, we unveil not only innovative strategies
for integrating Language Models (LLMs) with Information Retrieval (IR) systems,
but also the consequential considerations that underline the need for a
balanced approach aligned with user-centric principles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghodratnama_S/0/1/0/all/0/1&quot;&gt;Samira Ghodratnama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakershahrak_M/0/1/0/all/0/1&quot;&gt;Mehrdad Zakershahrak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12289">
<title>ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science. (arXiv:2311.12289v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12289</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models record impressive performance on many natural language
processing tasks. However, their knowledge capacity is limited to the
pretraining corpus. Retrieval augmentation offers an effective solution by
retrieving context from external knowledge sources to complement the language
model. However, existing retrieval augmentation techniques ignore the
structural relationships between these documents. Furthermore, retrieval models
are not explored much in scientific tasks, especially in regard to the
faithfulness of retrieved documents. In this paper, we propose a novel
structure-aware retrieval augmented language model that accommodates document
structure during retrieval augmentation. We create a heterogeneous document
graph capturing multiple types of relationships (e.g., citation, co-authorship,
etc.) that connect documents from more than 15 scientific disciplines (e.g.,
Physics, Medicine, Chemistry, etc.). We train a graph neural network on the
curated document graph to act as a structural encoder for the corresponding
passages retrieved during the model pretraining. Particularly, along with text
embeddings of the retrieved passages, we obtain structural embeddings of the
documents (passages) and fuse them together before feeding them to the language
model. We evaluate our model extensively on various scientific benchmarks that
include science question-answering and scientific document classification
tasks. Experimental results demonstrate that structure-aware retrieval improves
retrieving more coherent, faithful and contextually relevant passages, while
showing a comparable performance in the overall accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munikoti_S/0/1/0/all/0/1&quot;&gt;Sai Munikoti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Anurag Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1&quot;&gt;Sridevi Wagle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horawalavithana_S/0/1/0/all/0/1&quot;&gt;Sameera Horawalavithana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12298">
<title>Noise in Relation Classification Dataset TACRED: Characterization and Reduction. (arXiv:2311.12298v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12298</link>
<description rdf:parseType="Literal">&lt;p&gt;The overarching objective of this paper is two-fold. First, to explore
model-based approaches to characterize the primary cause of the noise. in the
RE dataset TACRED Second, to identify the potentially noisy instances. Towards
the first objective, we analyze predictions and performance of state-of-the-art
(SOTA) models to identify the root cause of noise in the dataset. Our analysis
of TACRED shows that the majority of the noise in the dataset originates from
the instances labeled as no-relation which are negative examples. For the
second objective, we explore two nearest-neighbor-based strategies to
automatically identify potentially noisy examples for elimination and
reannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is
based on the assumption that positive examples are clean. Thus, we have used
false-negative predictions to identify noisy negative examples. Whereas, our
second approach, referred to as Extrinsic Strategy, is based on using a clean
subset of the dataset to identify potentially noisy negative examples. Finally,
we retrained the SOTA models on the eliminated and reannotated dataset. Our
empirical results based on two SOTA models trained on TACRED-E following the IS
show an average 4% F1-score improvement, whereas reannotation (TACRED-R) does
not improve the original results. However, following ES, SOTA models show the
average F1-score improvement of 3.8% and 4.4% when trained on respective
eliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We
further extended the ES for cleaning positive examples as well, which resulted
in an average performance improvement of 5.8% and 5.6% for the eliminated
(TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parekh_A/0/1/0/all/0/1&quot;&gt;Akshay Parekh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1&quot;&gt;Ashish Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awekar_A/0/1/0/all/0/1&quot;&gt;Amit Awekar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12303">
<title>Detecting subtle macroscopic changes in a finite temperature classical scalar field with machine learning. (arXiv:2311.12303v1 [cond-mat.stat-mech])</title>
<link>http://arxiv.org/abs/2311.12303</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to detect macroscopic changes is important for probing the
behaviors of experimental many-body systems from the classical to the quantum
realm. Although abrupt changes near phase boundaries can easily be detected,
subtle macroscopic changes are much more difficult to detect as the changes can
be obscured by noise. In this study, as a toy model for detecting subtle
macroscopic changes in many-body systems, we try to differentiate scalar field
samples at varying temperatures. We compare different methods for making such
differentiations, from physics method, statistics method, to AI method. Our
finding suggests that the AI method outperforms both the statistical method and
the physics method in its sensitivity. Our result provides a proof-of-concept
that AI can potentially detect macroscopic changes in many-body systems that
elude physical measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yutong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiahong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huiyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jun Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12304">
<title>Discovering Effective Policies for Land-Use Planning. (arXiv:2311.12304v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2311.12304</link>
<description rdf:parseType="Literal">&lt;p&gt;How areas of land are allocated for different uses, such as forests, urban,
and agriculture, has a large effect on carbon balance, and therefore climate
change. Based on available historical data on changes in land use and a
simulation of carbon emissions/absorption, a surrogate model can be learned
that makes it possible to evaluate the different options available to
decision-makers efficiently. An evolutionary search process can then be used to
discover effective land-use policies for specific locations. Such a system was
built on the Project Resilience platform and evaluated with the Land-Use
Harmonization dataset and the BLUE simulator. It generates Pareto fronts that
trade off carbon impact and amount of change customized to different locations,
thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francon_O/0/1/0/all/0/1&quot;&gt;Olivier Francon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_D/0/1/0/all/0/1&quot;&gt;Daniel Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyerson_E/0/1/0/all/0/1&quot;&gt;Elliot Meyerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodjat_B/0/1/0/all/0/1&quot;&gt;Babak Hodjat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12307">
<title>Causality is all you need. (arXiv:2311.12307v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12307</link>
<description rdf:parseType="Literal">&lt;p&gt;In the fundamental statistics course, students are taught to remember the
well-known saying: &quot;Correlation is not Causation&quot;. Till now, statistics (i.e.,
correlation) have developed various successful frameworks, such as Transformer
and Pre-training large-scale models, which have stacked multiple parallel
self-attention blocks to imitate a wide range of tasks. However, in the
causation community, how to build an integrated causal framework still remains
an untouched domain despite its excellent intervention capabilities. In this
paper, we propose the Causal Graph Routing (CGR) framework, an integrated
causal scheme relying entirely on the intervention mechanisms to reveal the
cause-effect forces hidden in data. Specifically, CGR is composed of a stack of
causal layers. Each layer includes a set of parallel deconfounding blocks from
different causal graphs. We combine these blocks via the concept of the
proposed sufficient cause, which allows the model to dynamically select the
suitable deconfounding methods in each layer. CGR is implemented as the stacked
networks, integrating no confounder, back-door adjustment, front-door
adjustment, and probability of sufficient cause. We evaluate this framework on
two classical tasks of CV and NLP. Experiments show CGR can surpass the current
state-of-the-art methods on both Visual Question Answer and Long Document
Classification tasks. In particular, CGR has great potential in building the
&quot;causal&quot; pre-training large-scale model that effectively generalizes to diverse
tasks. It will improve the machines&apos; comprehension of causal relationships
within a broader semantic space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hongshuo Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;An-An Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12310">
<title>IEKM: A Model Incorporating External Keyword Matrices. (arXiv:2311.12310v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12310</link>
<description rdf:parseType="Literal">&lt;p&gt;A customer service platform system with a core text semantic similarity (STS)
task faces two urgent challenges: Firstly, one platform system needs to adapt
to different domains of customers, i.e., different domains adaptation (DDA).
Secondly, it is difficult for the model of the platform system to distinguish
sentence pairs that are literally close but semantically different, i.e., hard
negative samples. In this paper, we propose an incorporation external keywords
matrices model (IEKM) to address these challenges. The model uses external
tools or dictionaries to construct external matrices and fuses them to the
self-attention layers of the Transformer structure through gating units, thus
enabling flexible corrections to the model results. We evaluate the method on
multiple datasets and the results show that our method has improved performance
on all datasets. To demonstrate that our method can effectively solve all the
above challenges, we conduct a flexible correction experiment, which results in
an increase in the F1 value from 56.61 to 73.53. Our code will be publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Cheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1&quot;&gt;Mengliang Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yunbo Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12316">
<title>Overcoming Pathology Image Data Deficiency: Generating Images from Pathological Transformation Process. (arXiv:2311.12316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12316</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathology serves as the gold standard for medical diagnosis but faces
application limitations due to the shortage of medical resources. Leveraging
deep learning, computer-aided diagnosis has the potential to alleviate the
pathologist scarcity and provide timely clinical analysis. However, developing
a reliable model generally necessitates substantial data for training, which is
challenging in pathological field. In response, we propose an adaptive
depth-controlled bidirectional diffusion (ADBD) network for image data
generation. The domain migration approach can work with small trainset and
overcome the diffusion overfitting by source information guidance.
Specifically, we developed a hybrid attention strategy to blend global and
local attention priorities, which guides the bidirectional diffusion and
ensures the migration success. In addition, we developed the adaptive
depth-controlled strategy to simulate physiological transformations, capable of
yielding unlimited cross-domain intermediate images with corresponding soft
labels. ADBD is effective for overcoming pathological image data deficiency and
supportable for further pathology-related research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zeyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yufang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guanglei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12320">
<title>A Survey on Multimodal Large Language Models for Autonomous Driving. (arXiv:2311.12320v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12320</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of Large Language Models (LLMs) and Vision Foundation
Models (VFMs), multimodal AI systems benefiting from large models have the
potential to equally perceive the real world, make decisions, and control tools
as humans. In recent months, LLMs have shown widespread attention in autonomous
driving and map systems. Despite its immense potential, there is still a lack
of a comprehensive understanding of key challenges, opportunities, and future
endeavors to apply in LLM driving systems. In this paper, we present a
systematic investigation in this field. We first introduce the background of
Multimodal Large Language Models (MLLMs), the multimodal models development
using LLMs, and the history of autonomous driving. Then, we overview existing
MLLM tools for driving, transportation, and map systems together with existing
datasets and benchmarks. Moreover, we summarized the works in The 1st WACV
Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD),
which is the first workshop of its kind regarding LLMs in autonomous driving.
To further promote the development of this field, we also discuss several
important problems regarding using MLLMs in autonomous driving systems that
need to be solved by both academia and industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenqian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jintai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juanwu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Kuei-Da Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tianren Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1&quot;&gt;Erlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Ao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xinrui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Shuqi Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jianguo Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12328">
<title>Quantum-Enhanced Support Vector Machine for Large-Scale Stellar Classification with GPU Acceleration. (arXiv:2311.12328v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.12328</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we introduce an innovative Quantum-enhanced Support Vector
Machine (QSVM) approach for stellar classification, leveraging the power of
quantum computing and GPU acceleration. Our QSVM algorithm significantly
surpasses traditional methods such as K-Nearest Neighbors (KNN) and Logistic
Regression (LR), particularly in handling complex binary and multi-class
scenarios within the Harvard stellar classification system. The integration of
quantum principles notably enhances classification accuracy, while GPU
acceleration using the cuQuantum SDK ensures computational efficiency and
scalability for large datasets in quantum simulators. This synergy not only
accelerates the processing process but also improves the accuracy of
classifying diverse stellar types, setting a new benchmark in astronomical data
analysis. Our findings underscore the transformative potential of quantum
machine learning in astronomical research, marking a significant leap forward
in both precision and processing speed for stellar classification. This
advancement has broader implications for astrophysical and related scientific
fields
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kuan-Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaotian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Makhanov_H/0/1/0/all/0/1&quot;&gt;Henry Makhanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hui-Hsuan Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen-Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12336">
<title>Classification of Instagram fake users using supervised machine learning algorithms. (arXiv:2311.12336v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2311.12336</link>
<description rdf:parseType="Literal">&lt;p&gt;In the contemporary era, online social networks have become integral to
social life, revolutionizing the way individuals manage their social
connections. While enhancing accessibility and immediacy, these networks have
concurrently given rise to challenges, notably the proliferation of fraudulent
profiles and online impersonation. This paper proposes an application designed
to detect and neutralize such dishonest entities, with a focus on safeguarding
companies from potential fraud. The user-centric design of the application
ensures accessibility for investigative agencies, particularly the criminal
branch, facilitating navigation of complex social media landscapes and
integration with existing investigative procedures
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vertika Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolasaria_N/0/1/0/all/0/1&quot;&gt;Naman Tolasaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alpeshkumar_P/0/1/0/all/0/1&quot;&gt;Patel Meet Alpeshkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartwal_S/0/1/0/all/0/1&quot;&gt;Shreyash Bartwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12337">
<title>Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?. (arXiv:2311.12337v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12337</link>
<description rdf:parseType="Literal">&lt;p&gt;A distinction is often drawn between a model&apos;s ability to predict a label for
an evaluation sample that is directly memorised from highly similar training
samples versus an ability to predict the label via some method of
generalisation. In the context of using Language Models for question-answering,
discussion continues to occur as to the extent to which questions are answered
through memorisation. We consider this issue for questions that would ideally
be answered through reasoning over an associated context. We propose a method
of identifying evaluation samples for which it is very unlikely our model would
have memorised the answers. Our method is based on semantic similarity of input
tokens and label tokens between training and evaluation samples. We show that
our method offers advantages upon some prior approaches in that it is able to
surface evaluation-train pairs that have overlap in either contiguous or
discontiguous sequences of tokens. We use this method to identify unmemorisable
subsets of our evaluation datasets. We train two Language Models in a multitask
fashion whereby the second model differs from the first only in that it has two
additional datasets added to the training regime that are designed to impart
simple numerical reasoning strategies of a sort known to improve performance on
some of our evaluation datasets but not on others. We then show that there is
performance improvement between the two models on the unmemorisable subsets of
the evaluation datasets that were expected to benefit from the additional
training datasets. Specifically, performance on unmemorisable subsets of two of
our evaluation datasets, DROP and ROPES significantly improves by 9.0%, and
25.7% respectively while other evaluation datasets have no significant change
in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartill_T/0/1/0/all/0/1&quot;&gt;Tim Hartill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bensemann_J/0/1/0/all/0/1&quot;&gt;Joshua Bensemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witbrock_M/0/1/0/all/0/1&quot;&gt;Michael Witbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riddle_P/0/1/0/all/0/1&quot;&gt;Patricia J. Riddle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12338">
<title>A Survey on Large Language Models for Personalized and Explainable Recommendations. (arXiv:2311.12338v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.12338</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Recommender Systems(RS) have witnessed a transformative
shift with the advent of Large Language Models(LLMs) in the field of Natural
Language Processing(NLP). These models such as OpenAI&apos;s GPT-3.5/4, Llama from
Meta, have demonstrated unprecedented capabilities in understanding and
generating human-like text. This has led to a paradigm shift in the realm of
personalized and explainable recommendations, as LLMs offer a versatile toolset
for processing vast amounts of textual data to enhance user experiences. To
provide a comprehensive understanding of the existing LLM-based recommendation
systems, this survey aims to analyze how RS can benefit from LLM-based
methodologies. Furthermore, we describe major challenges in Personalized
Explanation Generating(PEG) tasks, which are cold-start problems, unfairness
and bias problems in RS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junyi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12345">
<title>Stable Diffusion For Aerial Object Detection. (arXiv:2311.12345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12345</link>
<description rdf:parseType="Literal">&lt;p&gt;Aerial object detection is a challenging task, in which one major obstacle
lies in the limitations of large-scale data collection and the long-tail
distribution of certain classes. Synthetic data offers a promising solution,
especially with recent advances in diffusion-based methods like stable
diffusion (SD). However, the direct application of diffusion methods to aerial
domains poses unique challenges: stable diffusion&apos;s optimization for rich
ground-level semantics doesn&apos;t align with the sparse nature of aerial objects,
and the extraction of post-synthesis object coordinates remains problematic. To
address these challenges, we introduce a synthetic data augmentation framework
tailored for aerial images. It encompasses sparse-to-dense region of interest
(ROI) extraction to bridge the semantic gap, fine-tuning the diffusion model
with low-rank adaptation (LORA) to circumvent exhaustive retraining, and
finally, a Copy-Paste method to compose synthesized objects with backgrounds,
providing a nuanced approach to aerial object detection through synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1&quot;&gt;Yanan Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fuxun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Simranjit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamoulis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Stamoulis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12359">
<title>Post-Training Quantization with Low-precision Minifloats and Integers on FPGAs. (arXiv:2311.12359v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12359</link>
<description rdf:parseType="Literal">&lt;p&gt;Post-Training Quantization (PTQ) is a powerful technique for model
compression, reducing the precision of neural networks without additional
training overhead. Recent works have investigated adopting 8-bit floating-point
quantization (FP8) in the context of PTQ for model inference. However, the
exploration of floating-point formats smaller than 8 bits and their comparison
with integer quantization remains relatively limited. In this work, we present
minifloats, which are reduced-precision floating-point formats capable of
further reducing the memory footprint, latency, and energy cost of a model
while approaching full-precision model accuracy. Our work presents a novel PTQ
design-space exploration, comparing minifloat and integer quantization schemes
across a range of 3 to 8 bits for both weights and activations. We examine the
applicability of various PTQ techniques to minifloats, including weight
equalization, bias correction, SmoothQuant, gradient-based learned rounding,
and the GPTQ method. Our experiments validate the effectiveness of
low-precision minifloats when compared to their integer counterparts across a
spectrum of accuracy-precision trade-offs on a set of reference deep learning
vision workloads. Finally, we evaluate our results against an FPGA-based
hardware cost model, showing that integer quantization often remains the
Pareto-optimal option, given its relatively smaller hardware resource
footprint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_S/0/1/0/all/0/1&quot;&gt;Shivam Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1&quot;&gt;Alessandro Pappalardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damsgaard_H/0/1/0/all/0/1&quot;&gt;Hans Jakob Damsgaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_G/0/1/0/all/0/1&quot;&gt;Giuseppe Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preusser_T/0/1/0/all/0/1&quot;&gt;Thomas B. Preu&amp;#xdf;er&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blott_M/0/1/0/all/0/1&quot;&gt;Michaela Blott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1&quot;&gt;Tulika Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12379">
<title>Infinite forecast combinations based on Dirichlet process. (arXiv:2311.12379v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12379</link>
<description rdf:parseType="Literal">&lt;p&gt;Forecast combination integrates information from various sources by
consolidating multiple forecast results from the target time series. Instead of
the need to select a single optimal forecasting model, this paper introduces a
deep learning ensemble forecasting model based on the Dirichlet process.
Initially, the learning rate is sampled with three basis distributions as
hyperparameters to convert the infinite mixture into a finite one. All
checkpoints are collected to establish a deep learning sub-model pool, and
weight adjustment and diversity strategies are developed during the combination
process. The main advantage of this method is its ability to generate the
required base learners through a single training process, utilizing the
decaying strategy to tackle the challenge posed by the stochastic nature of
gradient descent in determining the optimal learning rate. To ensure the
method&apos;s generalizability and competitiveness, this paper conducts an empirical
analysis using the weekly dataset from the M4 competition and explores
sensitivity to the number of models to be combined. The results demonstrate
that the ensemble model proposed offers substantial improvements in prediction
accuracy and stability compared to a single benchmark model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yinuo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yanfei Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12410">
<title>nach0: Multimodal Natural and Chemical Languages Foundation Model. (arXiv:2311.12410v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12410</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have substantially driven scientific progress in
various domains, and many papers have demonstrated their ability to tackle
complex problems with creative solutions. Our paper introduces a new foundation
model, nach0, capable of solving various chemical and biological tasks:
biomedical question answering, named entity recognition, molecular generation,
molecular synthesis, attributes prediction, and others. nach0 is a multi-domain
and multi-task encoder-decoder LLM pre-trained on unlabeled text from
scientific literature, patents, and molecule strings to incorporate a range of
chemical and linguistic knowledge. We employed instruction tuning, where
specific task-related instructions are utilized to fine-tune nach0 for the
final set of tasks. To train nach0 effectively, we leverage the NeMo framework,
enabling efficient parallel optimization of both base and large model versions.
Extensive experiments demonstrate that our model outperforms state-of-the-art
baselines on single-domain and cross-domain tasks. Furthermore, it can generate
high-quality outputs in molecular and textual formats, showcasing its
effectiveness in multi-domain setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Livne_M/0/1/0/all/0/1&quot;&gt;Micha Livne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miftahutdinov_Z/0/1/0/all/0/1&quot;&gt;Zulfat Miftahutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1&quot;&gt;Elena Tutubalina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1&quot;&gt;Maksim Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polykovskiy_D/0/1/0/all/0/1&quot;&gt;Daniil Polykovskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brundyn_A/0/1/0/all/0/1&quot;&gt;Annika Brundyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhunjhunwala_A/0/1/0/all/0/1&quot;&gt;Aastha Jhunjhunwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1&quot;&gt;Anthony Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliper_A/0/1/0/all/0/1&quot;&gt;Alex Aliper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhavoronkov_A/0/1/0/all/0/1&quot;&gt;Alex Zhavoronkov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12420">
<title>How Far Have We Gone in Vulnerability Detection Using Large Language Models. (arXiv:2311.12420v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12420</link>
<description rdf:parseType="Literal">&lt;p&gt;As software becomes increasingly complex and prone to vulnerabilities,
automated vulnerability detection is critically important, yet challenging.
Given the significant successes of Large Language Models (LLMs) in various
tasks, there is growing anticipation of their efficacy in vulnerability
detection. However, a quantitative understanding of their potential in
vulnerability detection is still missing. To bridge this gap, we introduce a
comprehensive vulnerability benchmark VulBench. This benchmark aggregates
high-quality data from a wide range of CTF (Capture-the-Flag) challenges and
real-world applications, with annotations for each vulnerable function
detailing the vulnerability type and its root cause. Through our experiments
encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models
and static analyzers, we find that several LLMs outperform traditional deep
learning approaches in vulnerability detection, revealing an untapped potential
in LLMs. This work contributes to the understanding and utilization of LLMs for
enhanced software security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zeyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12431">
<title>A recurrent connectionist model of melody perception : An exploration using TRACX2. (arXiv:2311.12431v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12431</link>
<description rdf:parseType="Literal">&lt;p&gt;Are similar, or even identical, mechanisms used in the computational modeling
of speech segmentation, serial image processing and music processing? We
address this question by exploring how TRACX2, (French et al., 2011; French \&amp;amp;
Cottrell, 2014; Mareschal \&amp;amp; French, 2017), a recognition-based, recursive
connectionist autoencoder model of chunking and sequence segmentation, which
has successfully simulated speech and serial-image processing, might be applied
to elementary melody perception. The model, a three-layer autoencoder that
recognizes &apos;&apos;chunks&apos;&apos; of short sequences of intervals that have been frequently
encountered on input, is trained on the tone intervals of melodically simple
French children&apos;s songs. It dynamically incorporates the internal
representations of these chunks into new input. Its internal representations
cluster in a manner that is consistent with &apos;&apos;human-recognizable&apos;&apos; melodic
categories. TRACX2 is sensitive to both contour and proximity information in
the musical chunks that it encounters in its input. It shows the
&apos;&apos;end-of-word&apos;&apos; superiority effect demonstrated by Saffran et al. (1999) for
short musical phrases. The overall findings suggest that the recursive
autoassociative chunking mechanism, as implemented in TRACX2, may be a general
segmentation and chunking mechanism, underlying not only word-and
imagechunking, but also elementary melody processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defays_D/0/1/0/all/0/1&quot;&gt;Daniel Defays&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+French_R/0/1/0/all/0/1&quot;&gt;Robert French&lt;/a&gt; (LEAD), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tillmann_B/0/1/0/all/0/1&quot;&gt;Barbara Tillmann&lt;/a&gt; (LEAD)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12435">
<title>Fair Enough? A map of the current limitations of the requirements to have &quot;fair&apos;&apos; algorithms. (arXiv:2311.12435v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12435</link>
<description rdf:parseType="Literal">&lt;p&gt;In the recent years, the raise in the usage and efficiency of Artificial
Intelligence and, more in general, of Automated Decision-Making systems has
brought with it an increasing and welcome awareness of the risks associated
with such systems. One of such risks is that of perpetuating or even amplifying
bias and unjust disparities present in the data from which many of these
systems learn to adjust and optimise their decisions. This awareness has on one
side encouraged several scientific communities to come up with more and more
appropriate ways and methods to assess, quantify, and possibly mitigate such
biases and disparities. On the other hand, it has prompted more and more layers
of society, including policy makers, to call for ``fair&apos;&apos; algorithms. We
believe that while a lot of excellent and multidisciplinary research is
currently being conducted, what is still fundamentally missing is the awareness
that having ``fair&apos;&apos; algorithms is per s\&apos;e a nearly meaningless requirement,
that needs to be complemented with a lot of additional societal choices to
become actionable. Namely, there is a hiatus between what the society is
demanding from Automated Decision-Making systems, and what this demand actually
means in real-world scenarios. In this work, we outline the key features of
such a hiatus, and pinpoint a list of fundamental ambiguities and attention
points that we as a society must address in order to give a concrete meaning to
the increasing demand of fairness in Automated Decision-Making systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1&quot;&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inverardi_N/0/1/0/all/0/1&quot;&gt;Nicole Inverardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanino_G/0/1/0/all/0/1&quot;&gt;Gabriele Nanino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penco_I/0/1/0/all/0/1&quot;&gt;Ilaria Giuseppina Penco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1&quot;&gt;Daniele Regoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12443">
<title>Knowledge Base Enabled Semantic Communication: A Generative Perspective. (arXiv:2311.12443v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2311.12443</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic communication is widely touted as a key technology for propelling
the sixth-generation (6G) wireless networks. However, providing effective
semantic representation is quite challenging in practice. To address this
issue, this article takes a crack at exploiting semantic knowledge base (KB) to
usher in a new era of generative semantic communication. Via semantic KB,
source messages can be characterized in low-dimensional subspaces without
compromising their desired meaning, thus significantly enhancing the
communication efficiency. The fundamental principle of semantic KB is first
introduced, and a generative semantic communication architecture is developed
by presenting three sub-KBs, namely source, task, and channel KBs. Then, the
detailed construction approaches for each sub-KB are described, followed by
their utilization in terms of semantic coding and transmission. A case study is
also provided to showcase the superiority of generative semantic communication
over conventional syntactic communication and classical semantic communication.
In a nutshell, this article establishes a scientific foundation for the
exciting uncharted frontier of generative semantic communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jinke Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zezhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yaping Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12447">
<title>Designing Long-term Group Fair Policies in Dynamical Systems. (arXiv:2311.12447v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12447</link>
<description rdf:parseType="Literal">&lt;p&gt;Neglecting the effect that decisions have on individuals (and thus, on the
underlying data distribution) when designing algorithmic decision-making
policies may increase inequalities and unfairness in the long term - even if
fairness considerations were taken in the policy design process. In this paper,
we propose a novel framework for achieving long-term group fairness in
dynamical systems, in which current decisions may affect an individual&apos;s
features in the next step, and thus, future decisions. Specifically, our
framework allows us to identify a time-independent policy that converges, if
deployed, to the targeted fair stationary state of the system in the long term,
independently of the initial data distribution. We model the system dynamics
with a time-homogeneous Markov chain and optimize the policy leveraging the
Markov chain convergence theorem to ensure unique convergence. We provide
examples of different targeted fair states of the system, encompassing a range
of long-term goals for society and policymakers. Furthermore, we show how our
approach facilitates the evaluation of different long-term targets by examining
their impact on the group-conditional population distribution in the long term
and how it evolves until convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rateike_M/0/1/0/all/0/1&quot;&gt;Miriam Rateike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1&quot;&gt;Isabel Valera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1&quot;&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12448">
<title>Extracting Definienda in Mathematical Scholarly Articles with Transformers. (arXiv:2311.12448v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12448</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider automatically identifying the defined term within a mathematical
definition from the text of an academic article. Inspired by the development of
transformer-based natural language processing applications, we pose the problem
as (a) a token-level classification task using fine-tuned pre-trained
transformers; and (b) a question-answering task using a generalist large
language model (GPT). We also propose a rule-based approach to build a labeled
dataset from the LATEX source of papers. Experimental results show that it is
possible to reach high levels of precision and recall using either recent (and
expensive) GPT 4 or simpler pre-trained models fine-tuned on our task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shufan Jiang&lt;/a&gt; (VALDA), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senellart_P/0/1/0/all/0/1&quot;&gt;Pierre Senellart&lt;/a&gt; (DI-ENS, VALDA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12454">
<title>HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis. (arXiv:2311.12454v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.12454</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLM)-based speech synthesis has been widely adopted in
zero-shot speech synthesis. However, they require a large-scale data and
possess the same limitations as previous autoregressive speech models,
including slow inference speed and lack of robustness. This paper proposes
HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech
(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis
frameworks could significantly improve the robustness and expressiveness of the
synthetic speech. Furthermore, we significantly improve the naturalness and
speaker similarity of synthetic speech even in zero-shot speech synthesis
scenarios. For text-to-speech, we adopt the text-to-vec framework, which
generates a self-supervised speech representation and an F0 representation
based on text representations and prosody prompts. Then, HierSpeech++ generates
speech from the generated vector, F0, and voice prompt. We further introduce a
high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The
experimental results demonstrated that the hierarchical variational autoencoder
could be a strong zero-shot speech synthesizer given that it outperforms
LLM-based and diffusion-based models. Moreover, we achieved the first
human-level quality zero-shot speech synthesis. Audio samples and source code
are available at https://github.com/sh-lee-prml/HierSpeechpp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Ha-Yeong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung-Bin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12465">
<title>Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and Embedding. (arXiv:2311.12465v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12465</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the significant barriers to the training of statistical models on
knowledge graphs is the difficulty that scientists have in finding the best
input data to address their prediction goal. In addition to this, a key
challenge is to determine how to manipulate these relational data, which are
often in the form of particular triples (i.e., subject, predicate, object), to
enable the learning process. Currently, many high-quality catalogs of knowledge
graphs, are available. However, their primary goal is the re-usability of these
resources, and their interconnection, in the context of the Semantic Web. This
paper describes the LiveSchema initiative, namely, a first version of a gateway
that has the main scope of leveraging the gold mine of data collected by many
existing catalogs collecting relational data like ontologies and knowledge
graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main
sources and offers some key facilities, which allow to: i) evolving LiveSchema,
by aggregating other source catalogs and repositories as input sources; ii)
querying all the collected resources; iii) transforming each given dataset into
formal concept analysis matrices that enable analysis and visualization
services; iv) generating models and tensors from each given dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fumagalli_M/0/1/0/all/0/1&quot;&gt;Mattia Fumagalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boffo_M/0/1/0/all/0/1&quot;&gt;Marco Boffo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Daqian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1&quot;&gt;Mayukh Bagchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12472">
<title>Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling. (arXiv:2311.12472v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12472</link>
<description rdf:parseType="Literal">&lt;p&gt;As an important application of spatio-temporal (ST) data, ST traffic
forecasting plays a crucial role in improving urban travel efficiency and
promoting sustainable development. In practice, the dynamics of traffic data
frequently undergo distributional shifts attributed to external factors such as
time evolution and spatial differences. This entails forecasting models to
handle the out-of-distribution (OOD) issue where test data is distributed
differently from training data. In this work, we first formalize the problem by
constructing a causal graph of past traffic data, future traffic data, and
external ST contexts. We reveal that the failure of prior arts in OOD traffic
data is due to ST contexts acting as a confounder, i.e., the common cause for
past data and future ones. Then, we propose a theoretical solution named
Disentangled Contextual Adjustment (DCA) from a causal lens. It differentiates
invariant causal correlations against variant spurious ones and deconfounds the
effect of ST contexts. On top of that, we devise a Spatio-Temporal
sElf-superVised dEconfounding (STEVE) framework. It first encodes traffic data
into two disentangled representations for associating invariant and variant ST
contexts. Then, we use representative ST contexts from three conceptually
different perspectives (i.e., temporal, spatial, and semantic) as
self-supervised signals to inject context information into both
representations. In this way, we improve the generalization ability of the
learned context-oriented representations to OOD ST traffic forecasting.
Comprehensive experiments on four large-scale benchmark datasets demonstrate
that our STEVE consistently outperforms the state-of-the-art baselines across
various ST OOD scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiahao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yue He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12475">
<title>PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords. (arXiv:2311.12475v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12475</link>
<description rdf:parseType="Literal">&lt;p&gt;While WangchanBERTa has become the de facto standard in transformer-based
Thai language modeling, it still has shortcomings in regard to the
understanding of foreign words, most notably English words, which are often
borrowed without orthographic assimilation into Thai in many contexts. We
identify the lack of foreign vocabulary in WangchanBERTa&apos;s tokenizer as the
main source of these shortcomings. We then expand WangchanBERTa&apos;s vocabulary
via vocabulary transfer from XLM-R&apos;s pretrained tokenizer and pretrain a new
model using the expanded tokenizer, starting from WangchanBERTa&apos;s checkpoint,
on a new dataset that is larger than the one used to train WangchanBERTa. Our
results show that our new pretrained model, PhayaThaiBERT, outperforms
WangchanBERTa in many downstream tasks and datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sriwirote_P/0/1/0/all/0/1&quot;&gt;Panyut Sriwirote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapiang_J/0/1/0/all/0/1&quot;&gt;Jalinee Thapiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timtong_V/0/1/0/all/0/1&quot;&gt;Vasan Timtong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1&quot;&gt;Attapol T. Rutherford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12477">
<title>Fin-QD: A Computational Design Framework for Soft Grippers: Integrating MAP-Elites and High-fidelity FEM. (arXiv:2311.12477v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12477</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational design can excite the full potential of soft robotics that has
the drawbacks of being highly nonlinear from material, structure, and contact.
Up to date, enthusiastic research interests have been demonstrated for
individual soft fingers, but the frame design space (how each soft finger is
assembled) remains largely unexplored. Computationally design remains
challenging for the finger-based soft gripper to grip across multiple
geometrical-distinct object types successfully. Including the design space for
the gripper frame can bring huge difficulties for conventional optimisation
algorithms and fitness calculation methods due to the exponential growth of
high-dimensional design space. This work proposes an automated computational
design optimisation framework that generates gripper diversity to individually
grasp geometrically distinct object types based on a quality-diversity
approach. This work first discusses a significantly large design space (28
design parameters) for a finger-based soft gripper, including the
rarely-explored design space of finger arrangement that is converted to various
configurations to arrange individual soft fingers. Then, a contact-based Finite
Element Modelling (FEM) is proposed in SOFA to output high-fidelity grasping
data for fitness evaluation and feature measurements. Finally, diverse gripper
designs are obtained from the framework while considering features such as the
volume and workspace of grippers. This work bridges the gap of computationally
exploring the vast design space of finger-based soft grippers while grasping
large geometrically distinct object types with a simple control scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yue Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iida_F/0/1/0/all/0/1&quot;&gt;Fumiya Iida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_D/0/1/0/all/0/1&quot;&gt;David Howard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12521">
<title>Classification of Tabular Data by Text Processing. (arXiv:2311.12521v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12521</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Language Processing technology has advanced vastly in the past
decade. Text processing has been successfully applied to a wide variety of
domains. In this paper, we propose a novel framework, Text Based
Classification(TBC), that uses state of the art text processing techniques to
solve classification tasks on tabular data. We provide a set of controlled
experiments where we present the benefits of using this approach against other
classification methods. Experimental results on several data sets also show
that this framework achieves comparable performance to that of several state of
the art models in accuracy, precision and recall of predicted classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramani_K/0/1/0/all/0/1&quot;&gt;Keshav Ramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borrajo_D/0/1/0/all/0/1&quot;&gt;Daniel Borrajo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12524">
<title>ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models. (arXiv:2311.12524v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12524</link>
<description rdf:parseType="Literal">&lt;p&gt;This study concentrates on evaluating the efficacy of Large Language Models
(LLMs) in healthcare, with a specific focus on their application in personal
anomalous health monitoring. Our research primarily investigates the
capabilities of LLMs in interpreting and analyzing physiological data obtained
from FDA-approved devices. We conducted an extensive analysis using anomalous
physiological data gathered in a simulated low-air-pressure plateau
environment. This allowed us to assess the precision and reliability of LLMs in
understanding and evaluating users&apos; health status with notable specificity. Our
findings reveal that LLMs exhibit exceptional performance in determining
medical indicators, including a Mean Absolute Error (MAE) of less than 1 beat
per minute for heart rate and less than 1% for oxygen saturation (SpO2).
Furthermore, the Mean Absolute Percentage Error (MAPE) for these evaluations
remained below 1%, with the overall accuracy of health assessments surpassing
85%. In image analysis tasks, such as interpreting photoplethysmography (PPG)
data, our specially adapted GPT models demonstrated remarkable proficiency,
achieving less than 1 bpm error in cycle count and 7.28 MAE for heart rate
estimation. This study highlights LLMs&apos; dual role as health data analysis tools
and pivotal elements in advanced AI health assistants, offering personalized
health insights and recommendations within the future health assistant
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiankai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kegang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hongming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiyuxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12526">
<title>Neural Network Pruning by Gradient Descent. (arXiv:2311.12526v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12526</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid increase in the parameters of deep learning models has led to
significant costs, challenging computational efficiency and model
interpretability. In this paper, we introduce a novel and straightforward
neural network pruning framework that incorporates the Gumbel-Softmax
technique. This framework enables the simultaneous optimization of a network&apos;s
weights and topology in an end-to-end process using stochastic gradient
descent. Empirical results demonstrate its exceptional compression capability,
maintaining high accuracy on the MNIST dataset with only 0.15\% of the original
network parameters. Moreover, our framework enhances neural network
interpretability, not only by allowing easy extraction of feature importance
directly from the pruned network but also by enabling visualization of feature
symmetry and the pathways of information propagation from features to outcomes.
Although the pruning strategy is learned through deep learning, it is
surprisingly intuitive and understandable, focusing on selecting key
representative features and exploiting data patterns to achieve extreme sparse
pruning. We believe our method opens a promising new avenue for deep learning
pruning and the creation of interpretable machine learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Ruyi Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12537">
<title>Oasis: Data Curation and Assessment System for Pretraining of Large Language Models. (arXiv:2311.12537v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12537</link>
<description rdf:parseType="Literal">&lt;p&gt;Data is one of the most critical elements in building a large language model.
However, existing systems either fail to customize a corpus curation pipeline
or neglect to leverage comprehensive corpus assessment for iterative
optimization of the curation. To this end, we present a pretraining corpus
curation and assessment platform called Oasis -- a one-stop system for data
quality improvement and quantification with user-friendly interactive
interfaces. Specifically, the interactive modular rule filter module can devise
customized rules according to explicit feedback. The debiased neural filter
module builds the quality classification dataset in a negative-centric manner
to remove the undesired bias. The adaptive document deduplication module could
execute large-scale deduplication with limited memory resources. These three
parts constitute the customized data curation module. And in the holistic data
assessment module, a corpus can be assessed in local and global views, with
three evaluation means including human, GPT-4, and heuristic metrics. We
exhibit a complete process to use Oasis for the curation and assessment of
pretraining data. In addition, an 800GB bilingual corpus curated by Oasis is
publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1&quot;&gt;Pengfei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12538">
<title>In-Context Learning Functions with Varying Number of Minima. (arXiv:2311.12538v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12538</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have proven effective at In-Context Learning
(ICL), an ability that allows them to create predictors from labeled examples.
Few studies have explored the interplay between ICL and specific properties of
functions it attempts to approximate. In our study, we use a formal framework
to explore ICL and propose a new task of approximating functions with varying
number of minima. We implement a method that allows for producing functions
with given inputs as minima. We find that increasing the number of minima
degrades ICL performance. At the same time, our evaluation shows that ICL
outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster
than 2NN in all settings. We validate the findings through a set of few-shot
experiments across various hyperparameter configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1&quot;&gt;David Oniani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12548">
<title>Multi-Session Budget Optimization for Forward Auction-based Federated Learning. (arXiv:2311.12548v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12548</link>
<description rdf:parseType="Literal">&lt;p&gt;Auction-based Federated Learning (AFL) has emerged as an important research
field in recent years. The prevailing strategies for FL model users (MUs)
assume that the entire team of the required data owners (DOs) for an FL task
must be assembled before training can commence. In practice, an MU can trigger
the FL training process multiple times. DOs can thus be gradually recruited
over multiple FL model training sessions. Existing bidding strategies for AFL
MUs are not designed to handle such scenarios. Therefore, the problem of
multi-session AFL remains open. To address this problem, we propose the
Multi-session Budget Optimization Strategy for forward Auction-based Federated
Learning (MultiBOS-AFL). Based on hierarchical reinforcement learning,
MultiBOS-AFL jointly optimizes inter-session budget pacing and intra-session
bidding for AFL MUs, with the objective of maximizing the total utility.
Extensive experiments on six benchmark datasets show that it significantly
outperforms seven state-of-the-art approaches. On average, MultiBOS-AFL
achieves 12.28% higher utility, 14.52% more data acquired through auctions for
a given budget, and 1.23% higher test accuracy achieved by the resulting FL
model compared to the best baseline. To the best of our knowledge, it is the
first budget optimization decision support method with budget pacing capability
designed for MUs in multi-session forward auction-based federated learning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12572">
<title>Scheduling Distributed Flexible Assembly Lines using Safe Reinforcement Learning with Soft Shielding. (arXiv:2311.12572v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2311.12572</link>
<description rdf:parseType="Literal">&lt;p&gt;Highly automated assembly lines enable significant productivity gains in the
manufacturing industry, particularly in mass production condition. Nonetheless,
challenges persist in job scheduling for make-to-job and mass customization,
necessitating further investigation to improve efficiency, reduce tardiness,
promote safety and reliability. In this contribution, an advantage actor-critic
based reinforcement learning method is proposed to address scheduling problems
of distributed flexible assembly lines in a real-time manner. To enhance the
performance, a more condensed environment representation approach is proposed,
which is designed to work with the masks made by priority dispatching rules to
generate fixed and advantageous action space. Moreover, a Monte-Carlo tree
search based soft shielding component is developed to help address
long-sequence dependent unsafe behaviors and monitor the risk of overdue
scheduling. Finally, the proposed algorithm and its soft shielding component
are validated in performance evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lele Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liyong Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12573">
<title>Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries. (arXiv:2311.12573v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.12573</link>
<description rdf:parseType="Literal">&lt;p&gt;The AI development community is increasingly making use of hosting
intermediaries such as Hugging Face provide easy access to user-uploaded models
and training data. These model marketplaces lower technical deployment barriers
for hundreds of thousands of users, yet can be used in numerous potentially
harmful and illegal ways. In this article, we explain ways in which AI systems,
which can both `contain&apos; content and be open-ended tools, present one of the
trickiest platform governance challenges seen to date. We provide case studies
of several incidents across three illustrative platforms -- Hugging Face,
GitHub and Civitai -- to examine how model marketplaces moderate models.
Building on this analysis, we outline important (and yet nevertheless limited)
practices that industry has been developing to respond to moderation demands:
licensing, access and use restrictions, automated content moderation, and open
policy development. While the policy challenge at hand is a considerable one,
we conclude with some ideas as to how platforms could better mobilize resources
to act as a careful, fair, and proportionate regulatory access point.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorwa_R/0/1/0/all/0/1&quot;&gt;Robert Gorwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veale_M/0/1/0/all/0/1&quot;&gt;Michael Veale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12574">
<title>IMGTB: A Framework for Machine-Generated Text Detection Benchmarking. (arXiv:2311.12574v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12574</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of large language models generating high quality texts, it is a
necessity to develop methods for detection of machine-generated text to avoid
harmful use or simply due to annotation purposes. It is, however, also
important to properly evaluate and compare such developed methods. Recently, a
few benchmarks have been proposed for this purpose; however, integration of
newest detection methods is rather challenging, since new methods appear each
month and provide slightly different evaluation pipelines. In this paper, we
present the IMGTB framework, which simplifies the benchmarking of
machine-generated text detection methods by easy integration of custom (new)
methods and evaluation datasets. Its configurability and flexibility makes
research and development of new detection methods easier, especially their
comparison to the existing state-of-the-art detectors. The default set of
analyses, metrics and visualizations offered by the tool follows the
established practices of machine-generated text detection benchmarking found in
state-of-the-art literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spiegel_M/0/1/0/all/0/1&quot;&gt;Michal Spiegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macko_D/0/1/0/all/0/1&quot;&gt;Dominik Macko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12582">
<title>Echocardiogram Foundation Model -- Application 1: Estimating Ejection Fraction. (arXiv:2311.12582v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12582</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardiovascular diseases stand as the primary global cause of mortality. Among
the various imaging techniques available for visualising the heart and
evaluating its function, echocardiograms emerge as the preferred choice due to
their safety and low cost. Quantifying cardiac function based on
echocardiograms is very laborious, time-consuming and subject to high
interoperator variability. In this work, we introduce EchoAI, an echocardiogram
foundation model, that is trained using self-supervised learning (SSL) on 1.5
million echocardiograms. We evaluate our approach by fine-tuning EchoAI to
estimate the ejection fraction achieving a mean absolute percentage error of
9.40%. This level of accuracy aligns with the performance of expert
sonographers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dahlan_A/0/1/0/all/0/1&quot;&gt;Adil Dahlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zakka_C/0/1/0/all/0/1&quot;&gt;Cyril Zakka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhinav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Laura Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shad_R/0/1/0/all/0/1&quot;&gt;Rohan Shad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fong_R/0/1/0/all/0/1&quot;&gt;Robyn Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hiesinger_W/0/1/0/all/0/1&quot;&gt;William Hiesinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12589">
<title>Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images. (arXiv:2311.12589v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12589</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawhney_G/0/1/0/all/0/1&quot;&gt;Gauransh Sawhney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1&quot;&gt;Daksh Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Adeel Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiechao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleem_K/0/1/0/all/0/1&quot;&gt;Khalid Saleem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12592">
<title>Visual tracking brain computer interface. (arXiv:2311.12592v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.12592</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-computer interfaces (BCIs) offer a way to interact with computers
without relying on physical movements. Non-invasive electroencephalography
(EEG)-based visual BCIs, known for efficient speed and calibration ease, face
limitations in continuous tasks due to discrete stimulus design and decoding
methods. To achieve continuous control, we implemented a novel spatial encoding
stimulus paradigm and devised a corresponding projection method to enable
continuous modulation of decoded velocity. Subsequently, we conducted
experiments involving 17 participants and achieved Fitt&apos;s ITR of 0.55 bps for
the fixed tracking task and 0.37 bps for the random tracking task. The proposed
BCI with a high Fitt&apos;s ITR was then integrated into two applications, including
painting and gaming. In conclusion, this study proposed a visual BCI-based
control method to go beyond discrete commands, allowing natural continuous
control based on neural activity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Changxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1&quot;&gt;Nanlin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yining Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaogang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaorong Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12604">
<title>Trustworthy AI: Deciding What to Decide. (arXiv:2311.12604v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12604</link>
<description rdf:parseType="Literal">&lt;p&gt;When engaging in strategic decision-making, we are frequently confronted with
overwhelming information and data. The situation can be further complicated
when certain pieces of evidence contradict each other or become paradoxical.
The primary challenge is how to determine which information can be trusted when
we adopt Artificial Intelligence (AI) systems for decision-making. This issue
is known as deciding what to decide or Trustworthy AI. However, the AI system
itself is often considered an opaque black box. We propose a new approach to
address this issue by introducing a novel framework of Trustworthy AI (TAI)
encompassing three crucial components of AI: representation space, loss
function, and optimizer. Each component is loosely coupled with four TAI
properties. Altogether, the framework consists of twelve TAI properties. We aim
to use this framework to conduct the TAI experiments by quantitive and
qualitative research methods to satisfy TAI properties for the decision-making
context. The framework allows us to formulate an optimal prediction model
trained by the given dataset for applying the strategic investment decision of
credit default swaps (CDS) in the technology sector. Finally, we provide our
view of the future direction of TAI research
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Caesar Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascal_B/0/1/0/all/0/1&quot;&gt;Bouvry Pascal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12610">
<title>ChessVision -- A Dataset for Logically Coherent Multi-label Classification. (arXiv:2311.12610v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12610</link>
<description rdf:parseType="Literal">&lt;p&gt;Starting with early successes in computer vision tasks, deep learning based
techniques have since overtaken state of the art approaches in a multitude of
domains. However, it has been demonstrated time and again that these techniques
fail to capture semantic context and logical constraints, instead often relying
on spurious correlations to arrive at the answer. Since application of deep
learning techniques to critical scenarios are dependent on adherence to domain
specific constraints, several attempts have been made to address this issue.
One limitation holding back a thorough exploration of this area, is a lack of
suitable datasets which feature a rich set of rules. In order to address this,
we present the ChessVision Dataset, consisting of 200,000+ images of annotated
chess games in progress, requiring recreation of the game state from its
corresponding image. This is accompanied by a curated set of rules which
constrains the set of predictions to &quot;reasonable&quot; game states, and are designed
to probe key semantic abilities like localization and enumeration. Alongside
standard metrics, additional metrics to measure performance with regards to
logical consistency is presented. We analyze several popular and state of the
art vision models on this task, and show that, although their performance on
standard metrics are laudable, they produce a plethora of incoherent results,
indicating that this dataset presents a significant challenge for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Soumadeep Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1&quot;&gt;Utpal Garain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12639">
<title>KNVQA: A Benchmark for evaluation knowledge-based VQA. (arXiv:2311.12639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12639</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the multimodal field, large vision-language models (LVLMs) have made
significant progress due to their strong perception and reasoning capabilities
in the visual and language systems. However, LVLMs are still plagued by the two
critical issues of object hallucination and factual accuracy, which limit the
practicality of LVLMs in different scenarios. Furthermore, previous evaluation
methods focus more on the comprehension and reasoning of language content but
lack a comprehensive evaluation of multimodal interactions, thereby resulting
in potential limitations. To this end, we propose a novel KNVQA-Eval, which is
devoted to knowledge-based VQA task evaluation to reflect the factuality of
multimodal LVLMs. To ensure the robustness and scalability of the evaluation,
we develop a new KNVQA dataset by incorporating human judgment and perception,
aiming to evaluate the accuracy of standard answers relative to AI-generated
answers in knowledge-based VQA. This work not only comprehensively evaluates
the contextual information of LVLMs using reliable human annotations, but also
further analyzes the fine-grained capabilities of current methods to reveal
potential avenues for subsequent optimization of LVLMs-based estimators. Our
proposed VQA-Eval and corresponding dataset KNVQA will facilitate the
development of automatic evaluation tools with the advantages of low cost,
privacy protection, and reproducibility. Our code will be released upon
publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sirui Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiayi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Muchen Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12651">
<title>Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots. (arXiv:2311.12651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12651</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and rapid delineation of sharp boundaries and robust semantics is
essential for numerous downstream robotic tasks, such as robot grasping and
manipulation, real-time semantic mapping, and online sensor calibration
performed on edge computing units. Although boundary detection and semantic
segmentation are complementary tasks, most studies focus on lightweight models
for semantic segmentation but overlook the critical role of boundary detection.
In this work, we introduce Mobile-Seed, a lightweight, dual-task framework
tailored for simultaneous semantic segmentation and boundary detection. Our
framework features a two-stream encoder, an active fusion decoder (AFD) and a
dual-task regularization approach. The encoder is divided into two pathways:
one captures category-aware semantic information, while the other discerns
boundaries from multi-scale features. The AFD module dynamically adapts the
fusion of semantic and boundary information by learning channel-wise
relationships, allowing for precise weight assignment of each channel.
Furthermore, we introduce a regularization loss to mitigate the conflicts in
dual-task learning and deep diversity supervision. Compared to existing
methods, the proposed Mobile-Seed offers a lightweight framework to
simultaneously improve semantic segmentation performance and accurately locate
object boundaries. Experiments on the Cityscapes dataset have shown that
Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA)
baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while
maintaining an online inference speed of 23.9 frames-per-second (FPS) with
1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on
CamVid and PASCAL Context datasets confirm our method&apos;s generalizability. Code
and additional results are publicly available at
\url{https://martin-liao.github.io/Mobile-Seed/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Youqi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shuhao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bisheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xieyuanli Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12654">
<title>PARK: Parkinson&apos;s Analysis with Remote Kinetic-tasks. (arXiv:2311.12654v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.12654</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a web-based framework to screen for Parkinson&apos;s disease (PD) by
allowing users to perform neurological tests in their homes. Our web framework
guides the users to complete three tasks involving speech, facial expression,
and finger movements. The task videos are analyzed to classify whether the
users show signs of PD. We present the results in an easy-to-understand manner,
along with personalized resources to further access to treatment and care. Our
framework is accessible by any major web browser, improving global access to
neurological care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Saiful Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangwu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelkader_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Abdelkader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sooyong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1&quot;&gt;Ehsan Hoque&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12664">
<title>The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change. (arXiv:2311.12664v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12664</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the DURel tool that implements the annotation of semantic
proximity between uses of words into an online, open source interface. The tool
supports standardized human annotation as well as computational annotation,
building on recent advances with Word-in-Context models. Annotator judgments
are clustered with automatic graph clustering techniques and visualized for
analysis. This allows to measure word senses with simple and intuitive
micro-task judgments between use pairs, requiring minimal preparation efforts.
The tool offers additional functionalities to compare the agreement between
annotators to guarantee the inter-subjectivity of the obtained judgments and to
calculate summary statistics giving insights into sense frequency
distributions, semantic variation or changes of senses over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlechtweg_D/0/1/0/all/0/1&quot;&gt;Dominik Schlechtweg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virk_S/0/1/0/all/0/1&quot;&gt;Shafqat Mumtaz Virk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sander_P/0/1/0/all/0/1&quot;&gt;Pauline Sander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoldberg_E/0/1/0/all/0/1&quot;&gt;Emma Sk&amp;#xf6;ldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linke_L/0/1/0/all/0/1&quot;&gt;Lukas Theuer Linke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tahmasebi_N/0/1/0/all/0/1&quot;&gt;Nina Tahmasebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1&quot;&gt;Jonas Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1&quot;&gt;Sabine Schulte im Walde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12668">
<title>From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design. (arXiv:2311.12668v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12668</link>
<description rdf:parseType="Literal">&lt;p&gt;Engineering Design is undergoing a transformative shift with the advent of
AI, marking a new era in how we approach product, system, and service planning.
Large language models have demonstrated impressive capabilities in enabling
this shift. Yet, with text as their only input modality, they cannot leverage
the large body of visual artifacts that engineers have used for centuries and
are accustomed to. This gap is addressed with the release of multimodal vision
language models, such as GPT-4V, enabling AI to impact many more types of
tasks. In light of these advancements, this paper presents a comprehensive
evaluation of GPT-4V, a vision language model, across a wide spectrum of
engineering design tasks, categorized into four main areas: Conceptual Design,
System-Level and Detailed Design, Manufacturing and Inspection, and Engineering
Education Tasks. Our study assesses GPT-4V&apos;s capabilities in design tasks such
as sketch similarity analysis, concept selection using Pugh Charts, material
selection, engineering drawing analysis, CAD generation, topology optimization,
design for additive and subtractive manufacturing, spatial reasoning
challenges, and textbook problems. Through this structured evaluation, we not
only explore GPT-4V&apos;s proficiency in handling complex design and manufacturing
challenges but also identify its limitations in complex engineering design
applications. Our research establishes a foundation for future assessments of
vision language models, emphasizing their immense potential for innovating and
enhancing the engineering design and manufacturing landscape. It also
contributes a set of benchmark testing datasets, with more than 1000 queries,
for ongoing advancements and applications in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_C/0/1/0/all/0/1&quot;&gt;Cyril Picard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_K/0/1/0/all/0/1&quot;&gt;Kristen M. Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doris_A/0/1/0/all/0/1&quot;&gt;Anna C. Doris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_B/0/1/0/all/0/1&quot;&gt;Brandon Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannone_G/0/1/0/all/0/1&quot;&gt;Giorgio Giannone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md Ferdous Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1&quot;&gt;Faez Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12699">
<title>Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study. (arXiv:2311.12699v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.12699</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have garnered significant attention for their
powerful ability in natural language understanding and reasoning. In this
paper, we present a comprehensive empirical study to explore the performance of
LLMs on misinformation detection tasks. This study stands as the pioneering
investigation into the understanding capabilities of multiple LLMs regarding
both content and propagation across social media platforms. Our empirical
studies on five misinformation detection datasets show that LLMs with diverse
prompts achieve comparable performance in text-based misinformation detection
but exhibit notably constrained capabilities in comprehending propagation
structure compared to existing models in propagation-based misinformation
detection. Besides, we further design four instruction-tuned strategies to
enhance LLMs for both content and propagation-based misinformation detection.
These strategies boost LLMs to actively learn effective features from multiple
instances or hard instances, and eliminate irrelevant propagation structures,
thereby achieving better detection performance. Extensive experiments further
demonstrate LLMs would play a better capacity in content and propagation
structure under these proposed strategies and achieve promising detection
performance. These findings highlight the potential ability of LLMs to detect
misinformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mengyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lingwei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Han Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Songlin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12707">
<title>Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions. (arXiv:2311.12707v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.12707</link>
<description rdf:parseType="Literal">&lt;p&gt;Standardized, validated questionnaires are vital tools in HCI research and
healthcare, offering dependable self-report data. However, their repeated use
in longitudinal or pre-post studies can induce respondent fatigue, impacting
data quality via response biases and decreased response rates. We propose
utilizing large language models (LLMs) to generate diverse questionnaire
versions while retaining good psychometric properties. In a longitudinal study,
participants engaged with our agent system and responded daily for two weeks to
either a standardized depression questionnaire or one of two LLM-generated
questionnaire variants, alongside a validated depression questionnaire.
Psychometric testing revealed consistent covariation between the external
criterion and the focal measure administered across the three conditions,
demonstrating the reliability and validity of the LLM-generated variants.
Participants found the repeated administration of the standardized
questionnaire significantly more repetitive compared to the variants. Our
findings highlight the potential of LLM-generated variants to invigorate
questionnaires, fostering engagement and interest without compromising
validity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Hye Sun Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arjmand_M/0/1/0/all/0/1&quot;&gt;Mehdi Arjmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherlock_P/0/1/0/all/0/1&quot;&gt;Phillip Raymond Sherlock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paasche_Orlow_M/0/1/0/all/0/1&quot;&gt;Michael Paasche-Orlow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffith_J/0/1/0/all/0/1&quot;&gt;James W. Griffith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bickmore_T/0/1/0/all/0/1&quot;&gt;Timothy Bickmore&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12713">
<title>Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics. (arXiv:2311.12713v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2311.12713</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning with neural networks is now becoming a more and more
powerful tool for various tasks, such as natural language processing, image
recognition, winning the game, and even for the issues of physics. Although
there are many studies on the application of machine learning to numerical
calculation and the assistance of experimental detection, the methods of
applying machine learning to find the analytical method are poorly studied. In
this paper, we propose the frameworks of developing analytical methods in
physics by using the symbolic regression with the Alpha Zero algorithm, that is
Alpha Zero for physics (AZfP). As a demonstration, we show that AZfP can derive
the high-frequency expansion in the Floquet systems. AZfP may have the
possibility of developing a new theoretical framework in physics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Michishita_Y/0/1/0/all/0/1&quot;&gt;Yoshihiro Michishita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12716">
<title>minimax: Efficient Baselines for Autocurricula in JAX. (arXiv:2311.12716v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12716</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised environment design (UED) is a form of automatic curriculum
learning for training robust decision-making agents to zero-shot transfer into
unseen environments. Such autocurricula have received much interest from the RL
community. However, UED experiments, based on CPU rollouts and GPU model
updates, have often required several weeks of training. This compute
requirement is a major obstacle to rapid innovation for the field. This work
introduces the minimax library for UED training on accelerated hardware. Using
JAX to implement fully-tensorized environments and autocurriculum algorithms,
minimax allows the entire training loop to be compiled for hardware
acceleration. To provide a petri dish for rapid experimentation, minimax
includes a tensorized grid-world based on MiniGrid, in addition to reusable
abstractions for conducting autocurricula in procedurally-generated
environments. With these components, minimax provides strong UED baselines,
including new parallelized variants, which achieve over 120$\times$ speedups in
wall time compared to previous implementations when training with equal batch
sizes. The minimax library is available under the Apache 2.0 license at
https://github.com/facebookresearch/minimax.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dennis_M/0/1/0/all/0/1&quot;&gt;Michael Dennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1&quot;&gt;Edward Grefenstette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12719">
<title>Development of a Legal Document AI-Chatbot. (arXiv:2311.12719v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12719</link>
<description rdf:parseType="Literal">&lt;p&gt;With the exponential growth of digital data and the increasing complexity of
legal documentation, there is a pressing need for efficient and intelligent
tools to streamline the handling of legal documents.With the recent
developments in the AI field, especially in chatbots, it cannot be ignored as a
very compelling solution to this problem.An insight into the process of
creating a Legal Documentation AI Chatbot with as many relevant features as
possible within the given time frame is presented.The development of each
component of the chatbot is presented in detail.Each component&apos;s workings and
functionality has been discussed.Starting from the build of the Android app and
the Langchain query processing code till the integration of both through a
Flask backend and REST API methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devaraj_P/0/1/0/all/0/1&quot;&gt;Pranav Nataraj Devaraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1&quot;&gt;Rakesh Teja P V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangrade_A/0/1/0/all/0/1&quot;&gt;Aaryav Gangrade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_M/0/1/0/all/0/1&quot;&gt;Manoj Kumar R&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12741">
<title>Content Augmented Graph Neural Networks. (arXiv:2311.12741v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12741</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, graph neural networks (GNNs) have become a popular tool for
solving various problems over graphs. In these models, the link structure of
the graph is typically exploited and nodes&apos; embeddings are iteratively updated
based on adjacent nodes. Nodes&apos; contents are used solely in the form of feature
vectors, served as nodes&apos; first-layer embeddings. However, the filters or
convolutions, applied during iterations/layers to these initial embeddings lead
to their impact diminish and contribute insignificantly to the final
embeddings. In order to address this issue, in this paper we propose augmenting
nodes&apos; embeddings by embeddings generating from their content, at higher GNN
layers. More precisely, we propose models wherein a structural embedding using
a GNN and a content embedding are computed for each node. These two are
combined using a combination layer to form the embedding of a node at a given
layer. We suggest methods such as using an auto-encoder or building a content
graph, to generate content embeddings. In the end, by conducting experiments
over several real-world datasets, we demonstrate the high accuracy and
performance of our models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Gholamzadeh Nasrabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashani_A/0/1/0/all/0/1&quot;&gt;AmirHossein Kashani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahedi_P/0/1/0/all/0/1&quot;&gt;Pegah Zahedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Haghir Chehreghani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12742">
<title>Image Transformation for IoT Time-Series Data: A Review. (arXiv:2311.12742v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12742</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of the Internet of Things (IoT), where smartphones, built-in
systems, wireless sensors, and nearly every smart device connect through local
networks or the internet, billions of smart things communicate with each other
and generate vast amounts of time-series data. As IoT time-series data is
high-dimensional and high-frequency, time-series classification or regression
has been a challenging issue in IoT. Recently, deep learning algorithms have
demonstrated superior performance results in time-series data classification in
many smart and intelligent IoT applications. However, it is hard to explore the
hidden dynamic patterns and trends in time-series. Recent studies show that
transforming IoT data into images improves the performance of the learning
model. In this paper, we present a review of these studies which use image
transformation/encoding techniques in IoT domain. We examine the studies
according to their encoding techniques, data types, and application areas.
Lastly, we emphasize the challenges and future dimensions of image
transformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altunkaya_D/0/1/0/all/0/1&quot;&gt;Duygu Altunkaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okay_F/0/1/0/all/0/1&quot;&gt;Feyza Yildirim Okay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdemir_S/0/1/0/all/0/1&quot;&gt;Suat Ozdemir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12754">
<title>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction. (arXiv:2311.12754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12754</link>
<description rdf:parseType="Literal">&lt;p&gt;3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird&apos;s eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12755">
<title>Digital Twin Framework for Optimal and Autonomous Decision-Making in Cyber-Physical Systems: Enhancing Reliability and Adaptability in the Oil and Gas Industry. (arXiv:2311.12755v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12755</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of creating a virtual copy of a complete Cyber-Physical System
opens up numerous possibilities, including real-time assessments of the
physical environment and continuous learning from the system to provide
reliable and precise information. This process, known as the twinning process
or the development of a digital twin (DT), has been widely adopted across
various industries. However, challenges arise when considering the
computational demands of implementing AI models, such as those employed in
digital twins, in real-time information exchange scenarios. This work proposes
a digital twin framework for optimal and autonomous decision-making applied to
a gas-lift process in the oil and gas industry, focusing on enhancing the
robustness and adaptability of the DT. The framework combines Bayesian
inference, Monte Carlo simulations, transfer learning, online learning, and
novel strategies to confer cognition to the DT, including model
hyperdimensional reduction and cognitive tack. Consequently, creating a
framework for efficient, reliable, and trustworthy DT identification was
possible. The proposed approach addresses the current gap in the literature
regarding integrating various learning techniques and uncertainty management in
digital twin strategies. This digital twin framework aims to provide a reliable
and efficient system capable of adapting to changing environments and
incorporating prediction uncertainty, thus enhancing the overall
decision-making process in complex, real-world scenarios. Additionally, this
work lays the foundation for further developments in digital twins for process
systems engineering, potentially fostering new advancements and applications
across various industrial sectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebello_C/0/1/0/all/0/1&quot;&gt;Carine Menezes Rebello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaschkea_J/0/1/0/all/0/1&quot;&gt;Johannes J&amp;#xe4;schkea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogueira_I/0/1/0/all/0/1&quot;&gt;Idelfonso B. R. Nogueira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12781">
<title>Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects. (arXiv:2311.12781v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12781</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic assessment of impairment and disease severity is a key challenge in
data-driven medicine. We propose a novel framework to address this challenge,
which leverages AI models trained exclusively on healthy individuals. The
COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the
decrease in confidence of these models when presented with impaired or diseased
patients to quantify their deviation from the healthy population. We applied
the COBRA score to address a key limitation of current clinical evaluation of
upper-body impairment in stroke patients. The gold-standard Fugl-Meyer
Assessment (FMA) requires in-person administration by a trained assessor for
30-45 minutes, which restricts monitoring frequency and precludes physicians
from adapting rehabilitation protocols to the progress of each patient. The
COBRA score, computed automatically in under one minute, is shown to be
strongly correlated with the FMA on an independent test cohort for two
different data modalities: wearable sensors ($\rho = 0.845$, 95% CI
[0.743,0.908]) and video ($\rho = 0.746$, 95% C.I [0.594, 0.847]). To
demonstrate the generalizability of the approach to other conditions, the COBRA
score was also applied to quantify severity of knee osteoarthritis from
magnetic-resonance imaging scans, again achieving significant correlation with
an independent clinical assessment ($\rho = 0.644$, 95% C.I [0.585,0.696]).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Boyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1&quot;&gt;Aakash Kaku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parnandi_A/0/1/0/all/0/1&quot;&gt;Avinash Parnandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fokas_E/0/1/0/all/0/1&quot;&gt;Emily Fokas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesan_A/0/1/0/all/0/1&quot;&gt;Anita Venkatesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandit_N/0/1/0/all/0/1&quot;&gt;Natasha Pandit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1&quot;&gt;Rajesh Ranganath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schambra_H/0/1/0/all/0/1&quot;&gt;Heidi Schambra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1&quot;&gt;Carlos Fernandez-Granda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12792">
<title>Intrinsic Image Decomposition via Ordinal Shading. (arXiv:2311.12792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12792</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsic decomposition is a fundamental mid-level vision problem that plays
a crucial role in various inverse rendering and computational photography
pipelines. Generating highly accurate intrinsic decompositions is an inherently
under-constrained task that requires precisely estimating continuous-valued
shading and albedo. In this work, we achieve high-resolution intrinsic
decomposition by breaking the problem into two parts. First, we present a dense
ordinal shading formulation using a shift- and scale-invariant loss in order to
estimate ordinal shading cues without restricting the predictions to obey the
intrinsic model. We then combine low- and high-resolution ordinal estimations
using a second network to generate a shading estimate with both global
coherency and local details. We encourage the model to learn an accurate
decomposition by computing losses on the estimated shading as well as the
albedo implied by the intrinsic model. We develop a straightforward method for
generating dense pseudo ground truth using our model&apos;s predictions and
multi-illumination data, enabling generalization to in-the-wild imagery. We
present an exhaustive qualitative and quantitative analysis of our predicted
intrinsic components against state-of-the-art methods. Finally, we demonstrate
the real-world applicability of our estimations by performing otherwise
difficult editing tasks such as recoloring and relighting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Careaga_C/0/1/0/all/0/1&quot;&gt;Chris Careaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksoy_Y/0/1/0/all/0/1&quot;&gt;Ya&amp;#x11f;&amp;#x131;z Aksoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.07780">
<title>Shortcut Learning in Deep Neural Networks. (arXiv:2004.07780v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2004.07780</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has triggered the current rise of artificial intelligence and
is the workhorse of today&apos;s machine intelligence. Numerous success stories have
rapidly spread all over science, industry and society, but its limitations have
only recently come into focus. In this perspective we seek to distill how many
of deep learning&apos;s problems can be seen as different symptoms of the same
underlying problem: shortcut learning. Shortcuts are decision rules that
perform well on standard benchmarks but fail to transfer to more challenging
testing conditions, such as real-world scenarios. Related issues are known in
Comparative Psychology, Education and Linguistics, suggesting that shortcut
learning may be a common characteristic of learning systems, biological and
artificial alike. Based on these observations, we develop a set of
recommendations for model interpretation and benchmarking, highlighting recent
advances in machine learning to improve robustness and transferability from the
lab to real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn-Henrik Jacobsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaelis_C/0/1/0/all/0/1&quot;&gt;Claudio Michaelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1&quot;&gt;Felix A. Wichmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.04923">
<title>Topological properties of basins of attraction and expressiveness of width bounded neural networks. (arXiv:2011.04923v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.04923</link>
<description rdf:parseType="Literal">&lt;p&gt;In Radhakrishnan et al. [2020], the authors empirically show that
autoencoders trained with usual SGD methods shape out basins of attraction
around their training data. We consider network functions of width not
exceeding the input dimension and prove that in this situation basins of
attraction are bounded and their complement cannot have bounded components. Our
conditions in these results are met in several experiments of the latter work
and we thus address a question posed therein. We also show that under some more
restrictive conditions the basins of attraction are path-connected. The
tightness of the conditions in our results is demonstrated by means of several
examples. Finally, the arguments used to prove the above results allow us to
derive a root cause why scalar-valued neural network functions that fulfill our
bounded width condition are not dense in spaces of continuous functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beise_H/0/1/0/all/0/1&quot;&gt;Hans-Peter Beise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1&quot;&gt;Steve Dias Da Cruz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.09076">
<title>An actor-critic algorithm with policy gradients to solve the job shop scheduling problem using deep double recurrent agents. (arXiv:2110.09076v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2110.09076</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing interest in integrating machine learning techniques and
optimization to solve challenging optimization problems. In this work, we
propose a deep reinforcement learning methodology for the job shop scheduling
problem (JSSP). The aim is to build up a greedy-like heuristic able to learn on
some distribution of JSSP instances, different in the number of jobs and
machines. The need for fast scheduling methods is well known, and it arises in
many areas, from transportation to healthcare. We model the JSSP as a Markov
Decision Process and then we exploit the efficacy of reinforcement learning to
solve the problem. We adopt an actor-critic scheme, where the action taken by
the agent is influenced by policy considerations on the state-value function.
The procedures are adapted to take into account the challenging nature of JSSP,
where the state and the action space change not only for every instance but
also after each decision. To tackle the variability in the number of jobs and
operations in the input, we modeled the agent using two incident LSTM models, a
special type of deep neural network. Experiments show the algorithm reaches
good solutions in a short time, proving that is possible to generate new greedy
heuristics just from learning-based methodologies. Benchmarks have been
generated in comparison with the commercial solver CPLEX. As expected, the
model can generalize, to some extent, to larger problems or instances
originated by a different distribution from the one used in training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Monaci_M/0/1/0/all/0/1&quot;&gt;Marta Monaci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Agasucci_V/0/1/0/all/0/1&quot;&gt;Valerio Agasucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grani_G/0/1/0/all/0/1&quot;&gt;Giorgio Grani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.00955">
<title>Source Free Unsupervised Graph Domain Adaptation. (arXiv:2112.00955v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.00955</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have achieved great success on a variety of
tasks with graph-structural data, among which node classification is an
essential one. Unsupervised Graph Domain Adaptation (UGDA) shows its practical
value of reducing the labeling cost for node classification. It leverages
knowledge from a labeled graph (i.e., source domain) to tackle the same task on
another unlabeled graph (i.e., target domain). Most existing UGDA methods
heavily rely on the labeled graph in the source domain. They utilize labels
from the source domain as the supervision signal and are jointly trained on
both the source graph and the target graph. However, in some real-world
scenarios, the source graph is inaccessible because of privacy issues.
Therefore, we propose a novel scenario named Source Free Unsupervised Graph
Domain Adaptation (SFUGDA). In this scenario, the only information we can
leverage from the source domain is the well-trained source model, without any
exposure to the source graph and its labels. As a result, existing UGDA methods
are not feasible anymore. To address the non-trivial adaptation challenges in
this practical scenario, we propose a model-agnostic algorithm called SOGA for
domain adaptation to fully exploit the discriminative ability of the source
model while preserving the consistency of structural proximity on the target
graph. We prove the effectiveness of the proposed algorithm both theoretically
and empirically. The experimental results on four cross-domain tasks show
consistent improvements in the Macro-F1 score and Macro-AUC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Lun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yujia Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zelin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09831">
<title>Neural Born Iteration Method For Solving Inverse Scattering Problems: 2D Cases. (arXiv:2112.09831v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09831</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose the neural Born iterative method (NeuralBIM) for
solving 2D inverse scattering problems (ISPs) by drawing on the scheme of
physics-informed supervised residual learning (PhiSRL) to emulate the computing
process of the traditional Born iterative method (TBIM). NeuralBIM employs
independent convolutional neural networks (CNNs) to learn the alternate update
rules of two different candidate solutions regarding the residuals. Two
different schemes are presented in this paper, including the supervised and
unsupervised learning schemes. With the data set generated by the method of
moments (MoM), supervised NeuralBIM are trained with the knowledge of total
fields and contrasts. Unsupervised NeuralBIM is guided by the physics-embedded
objective function founding on the governing equations of ISPs, which results
in no requirement of total fields and contrasts for training. Numerical and
experimental results further validate the efficacy of NeuralBIM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shan_T/0/1/0/all/0/1&quot;&gt;Tao Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Maokun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhensheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.10793">
<title>PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs. (arXiv:2202.10793v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.10793</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks are ubiquitous in many real-world applications (e.g., social
networks encoding trust/distrust relationships, correlation networks arising
from time series data). While many networks are signed or directed, or both,
there is a lack of unified software packages on graph neural networks (GNNs)
specially designed for signed and directed networks. In this paper, we present
PyTorch Geometric Signed Directed (PyGSD), a software package which fills this
gap. Along the way, we evaluate the implemented methods with experiments with a
view to providing insights into which method to choose for a given task. The
deep learning framework consists of easy-to-use GNN models, synthetic and
real-world data, as well as task-specific evaluation metrics and loss functions
for signed and directed networks. As an extension library for PyG, our proposed
software is maintained with open-source releases, detailed documentation,
continuous integration, unit tests and code coverage checks. The GitHub
repository of the library is
https://github.com/SherylHYX/pytorch_geometric_signed_directed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yixuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xitong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junjie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1&quot;&gt;Benedek Rozemberczki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1&quot;&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinert_G/0/1/0/all/0/1&quot;&gt;Gesine Reinert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10852">
<title>Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10852</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, vanilla Transformer architectures have not yielded promising
improvements in the Knowledge Graph (KG) representations, where the
translational distance paradigm dominates this area. Note that vanilla
Transformer architectures struggle to capture the intrinsically heterogeneous
structural and semantic information of knowledge graphs. To this end, we
propose a new variant of Transformer for knowledge graph representations dubbed
Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample
contextualized sub-graph sequences as the input to alleviate the heterogeneity
issue. We propose a novel structure-enhanced self-attention mechanism to encode
the relational information and keep the semantic information within entities
and relations. Moreover, we utilize masked knowledge modeling for general
knowledge graph representation learning, which can be applied to various
KG-based tasks including knowledge graph completion, question answering, and
recommendation. Experimental results on six datasets show that Relphormer can
obtain better performance compared with baselines. Code is available in
https://github.com/zjunlp/Relphormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhen Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaozhuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1&quot;&gt;Feiyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04589">
<title>Long-term Causal Effects Estimation via Latent Surrogates Representation Learning. (arXiv:2208.04589v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04589</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating long-term causal effects based on short-term surrogates is a
significant but challenging problem in many real-world applications, e.g.,
marketing and medicine. Despite its success in certain domains, most existing
methods estimate causal effects in an idealistic and simplistic way - ignoring
the causal structure among short-term outcomes and treating all of them as
surrogates. However, such methods cannot be well applied to real-world
scenarios, in which the partially observed surrogates are mixed with their
proxies among short-term outcomes. To this end, we develop our flexible method,
Laser, to estimate long-term causal effects in the more realistic situation
that the surrogates are observed or have observed proxies.Given the
indistinguishability between the surrogates and proxies, we utilize
identifiable variational auto-encoder (iVAE) to recover the whole valid
surrogates on all the surrogates candidates without the need of distinguishing
the observed surrogates or the proxies of latent surrogates. With the help of
the recovered surrogates, we further devise an unbiased estimation of long-term
causal effects. Extensive experimental results on the real-world and
semi-synthetic datasets demonstrate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Ruichu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weilin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zeqin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1&quot;&gt;Shu Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiecheng Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04181">
<title>Attending to Graph Transformers. (arXiv:2302.04181v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04181</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, transformer architectures for graphs emerged as an alternative to
established techniques for machine learning with graphs, such as
(message-passing) graph neural networks. So far, they have shown promising
empirical results, e.g., on molecular prediction datasets, often attributed to
their ability to circumvent graph neural networks&apos; shortcomings, such as
over-smoothing and over-squashing. Here, we derive a taxonomy of graph
transformer architectures, bringing some order to this emerging field. We
overview their theoretical properties, survey structural and positional
encodings, and discuss extensions for important graph classes, e.g., 3D
molecular graphs. Empirically, we probe how well graph transformers can recover
various graph properties, how well they can deal with heterophilic graphs, and
to what extent they prevent over-squashing. Further, we outline open challenges
and research direction to stimulate future work. Our code is available at
https://github.com/luis-mueller/probing-graph-transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1&quot;&gt;Luis M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1&quot;&gt;Christopher Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1&quot;&gt;Ladislav Ramp&amp;#xe1;&amp;#x161;ek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02536">
<title>Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02536</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal abstraction is a promising theoretical framework for explainable
artificial intelligence that defines when an interpretable high-level causal
model is a faithful simplification of a low-level deep learning system.
However, existing causal abstraction methods have two major limitations: they
require a brute-force search over alignments between the high-level model and
the low-level one, and they presuppose that variables in the high-level model
will align with disjoint sets of neurons in the low-level one. In this paper,
we present distributed alignment search (DAS), which overcomes these
limitations. In DAS, we find the alignment between high-level and low-level
models using gradient descent rather than conducting a brute-force search, and
we allow individual neurons to play multiple distinct roles by analyzing
representations in non-standard bases-distributed representations. Our
experiments show that DAS can discover internal structure that prior approaches
miss. Overall, DAS removes previous obstacles to conducting causal abstraction
analyses and allows us to find conceptual structure in trained neural nets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Atticus Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhengxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1&quot;&gt;Thomas Icard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1&quot;&gt;Noah D. Goodman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03898">
<title>The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03898</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, short Text Matching tasks have been widely applied in the
fields ofadvertising search and recommendation. The difficulty lies in the lack
of semantic information and word ambiguity caused by the short length of the
text. Previous works have introduced complement sentences or knowledge bases to
provide additional feature information. However, these methods have not fully
interacted between the original sentence and the complement sentence, and have
not considered the noise issue that may arise from the introduction of external
knowledge bases. Therefore, this paper proposes a short Text Matching model
that combines contrastive learning and external knowledge. The model uses a
generative model to generate corresponding complement sentences and uses the
contrastive learning method to guide the model to obtain more semantically
meaningful encoding of the original sentence. In addition, to avoid noise, we
use keywords as the main semantics of the original sentence to retrieve
corresponding knowledge words in the knowledge base, and construct a knowledge
graph. The graph encoding model is used to integrate the knowledge base
information into the model. Our designed model achieves state-of-the-art
performance on two publicly available Chinese Text Matching datasets,
demonstrating the effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Mengmeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1&quot;&gt;Hanjie Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaohua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangzheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yanlong Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07061">
<title>DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07061</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large
language models (LLMs) to automate the interactions with Android mobile
applications. Given a natural language description of a desired task,
DroidBot-GPT can automatically generate and execute actions that navigate the
app to complete the task. It works by translating the app GUI state information
and the available actions on the smartphone screen to natural language prompts
and asking the LLM to make a choice of actions. Since the LLM is typically
trained on a large amount of data including the how-to manuals of diverse
software applications, it has the ability to make reasonable choices of actions
based on the provided information. We evaluate DroidBot-GPT with a self-created
dataset that contains 33 tasks collected from 17 Android applications spanning
10 categories. It can successfully complete 39.39% of the tasks, and the
average partial completion progress is about 66.76%. Given the fact that our
method is fully unsupervised (no modification required from both the app and
the LLM), we believe there is great potential to enhance automation performance
with better app development paradigms and/or custom model training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14922">
<title>Supervised and Unsupervised Deep Learning Approaches for EEG Seizure Prediction. (arXiv:2304.14922v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14922</link>
<description rdf:parseType="Literal">&lt;p&gt;Epilepsy affects more than 50 million people worldwide, making it one of the
world&apos;s most prevalent neurological diseases. The main symptom of epilepsy is
seizures, which occur abruptly and can cause serious injury or death. The
ability to predict the occurrence of an epileptic seizure could alleviate many
risks and stresses people with epilepsy face. We formulate the problem of
detecting preictal (or pre-seizure) with reference to normal EEG as a precursor
to incoming seizure. To this end, we developed several supervised deep learning
approaches model to identify preictal EEG from normal EEG. We further develop
novel unsupervised deep learning approaches to train the models on only normal
EEG, and detecting pre-seizure EEG as an anomalous event. These deep learning
models were trained and evaluated on two large EEG seizure datasets in a
person-specific manner. We found that both supervised and unsupervised
approaches are feasible; however, their performance varies depending on the
patient, approach and architecture. This new line of research has the potential
to develop therapeutic interventions and save human lives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Georgis_Yap_Z/0/1/0/all/0/1&quot;&gt;Zakary Georgis-Yap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Popovic_M/0/1/0/all/0/1&quot;&gt;Milos R. Popovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shehroz S. Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10229">
<title>How does Contrastive Learning Organize Images?. (arXiv:2305.10229v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10229</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning, a dominant self-supervised technique, emphasizes
similarity in representations between augmentations of the same input and
dissimilarity for different ones. Although low contrastive loss often
correlates with high classification accuracy, recent studies challenge this
direct relationship, spotlighting the crucial role of inductive biases. We
delve into these biases from a clustering viewpoint, noting that contrastive
learning creates locally dense clusters, contrasting the globally dense
clusters from supervised learning. To capture this discrepancy, we introduce
the &quot;RLD (Relative Local Density)&quot; metric. While this cluster property can
hinder linear classification accuracy, leveraging a Graph Convolutional Network
(GCN) based classifier mitigates this, boosting accuracy and reducing parameter
requirements. The code is available
\href{https://github.com/xsgxlz/How-does-Contrastive-Learning-Organize-Images/tree/main}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1&quot;&gt;Qi Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by the Unity engine, M3Act
features multiple semantic groups, highly diverse and photorealistic images,
and a comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments using various input modalities. First, adding our synthetic data
significantly improves the performance of MOTRv2 on DanceTrack, leading to a
hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking
results on par with MOTRv2*, which is trained with 62.5% more real-world data.
Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on
group activity and atomic action accuracy respectively. Moreover, M3Act opens
new research for controllable 3D group activity generation. We define multiple
metrics and propose a competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01452">
<title>Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01452</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning is an essential paradigm for solving sequential
decision problems under uncertainty. Despite many remarkable achievements in
recent decades, applying reinforcement learning methods in the real world
remains challenging. One of the main obstacles is that reinforcement learning
agents lack a fundamental understanding of the world and must therefore learn
from scratch through numerous trial-and-error interactions. They may also face
challenges in providing explanations for their decisions and generalizing the
acquired knowledge. Causality, however, offers a notable advantage as it can
formalize knowledge in a systematic manner and leverage invariance for
effective knowledge transfer. This has led to the emergence of causal
reinforcement learning, a subfield of reinforcement learning that seeks to
enhance existing algorithms by incorporating causal relationships into the
learning process. In this survey, we comprehensively review the literature on
causal reinforcement learning. We first introduce the basic concepts of
causality and reinforcement learning, and then explain how causality can
address core challenges in non-causal reinforcement learning. We categorize and
systematically review existing causal reinforcement learning approaches based
on their target problems and methodologies. Finally, we outline open issues and
future directions in this emerging field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhihong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08927">
<title>Multi-Stage Cable Routing through Hierarchical Imitation Learning. (arXiv:2307.08927v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08927</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of learning to perform multi-stage robotic manipulation
tasks, with applications to cable routing, where the robot must route a cable
through a series of clips. This setting presents challenges representative of
complex multi-stage robotic manipulation scenarios: handling deformable
objects, closing the loop on visual perception, and handling extended behaviors
consisting of multiple steps that must be executed successfully to complete the
entire task. In such settings, learning individual primitives for each stage
that succeed with a high enough rate to perform a complete temporally extended
task is impractical: if each stage must be completed successfully and has a
non-negligible probability of failure, the likelihood of successful completion
of the entire task becomes negligible. Therefore, successful controllers for
such multi-stage tasks must be able to recover from failure and compensate for
imperfections in low-level controllers by smartly choosing which controllers to
trigger at any given time, retrying, or taking corrective action as needed. To
this end, we describe an imitation learning system that uses vision-based
policies trained from demonstrations at both the lower (motor control) and the
upper (sequencing) level, present a system for instantiating this method to
learn the cable routing task, and perform evaluations showing great performance
in generalizing to very challenging clip placement variations. Supplementary
videos, datasets, and code can be found at
https://sites.google.com/view/cablerouting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianlan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Charles Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xinyang Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1&quot;&gt;Gilbert Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1&quot;&gt;Liam Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1&quot;&gt;Stefan Schaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11494">
<title>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11494</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved state-of-the-art performance in generative
modeling tasks across various domains. Prior works on time series diffusion
models have primarily focused on developing conditional models tailored to
specific forecasting or imputation tasks. In this work, we explore the
potential of task-agnostic, unconditional diffusion models for several time
series applications. We propose TSDiff, an unconditionally-trained diffusion
model for time series. Our proposed self-guidance mechanism enables
conditioning TSDiff for downstream tasks during inference, without requiring
auxiliary networks or altering the training procedure. We demonstrate the
effectiveness of our method on three different time series tasks: forecasting,
refinement, and synthetic data generation. First, we show that TSDiff is
competitive with several task-specific conditional forecasting methods
(predict). Second, we leverage the learned implicit probability density of
TSDiff to iteratively refine the predictions of base forecasters with reduced
computational overhead over reverse diffusion (refine). Notably, the generative
performance of the model remains intact -- downstream forecasters trained on
synthetic samples from TSDiff outperform forecasters that are trained on
samples from other state-of-the-art generative time series models, occasionally
even outperforming models trained on real data (synthesize).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1&quot;&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1&quot;&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohlke_Schneider_M/0/1/0/all/0/1&quot;&gt;Michael Bohlke-Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zschiegner_J/0/1/0/all/0/1&quot;&gt;Jasper Zschiegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16113">
<title>survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16113</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to their flexibility and superior performance, machine learning models
frequently complement and outperform traditional statistical survival models.
However, their widespread adoption is hindered by a lack of user-friendly tools
to explain their internal operations and prediction rationales. To tackle this
issue, we introduce the survex R package, which provides a cohesive framework
for explaining any survival model by applying explainable artificial
intelligence techniques. The capabilities of the proposed software encompass
understanding and diagnosing survival models, which can lead to their
improvement. By revealing insights into the decision-making process, such as
variable effects and importances, survex enables the assessment of model
reliability and the detection of biases. Thus, transparency and responsibility
may be promoted in sensitive areas, such as biomedical research and healthcare
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spytek_M/0/1/0/all/0/1&quot;&gt;Miko&amp;#x142;aj Spytek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krzyzinski_M/0/1/0/all/0/1&quot;&gt;Mateusz Krzyzi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langbein_S/0/1/0/all/0/1&quot;&gt;Sophie Hanna Langbein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baniecki_H/0/1/0/all/0/1&quot;&gt;Hubert Baniecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_M/0/1/0/all/0/1&quot;&gt;Marvin N. Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00018">
<title>Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00018</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing interpretability of deep-learning models to non-experts, while
fundamental for a responsible real-world usage, is challenging. Attribution
maps from xAI techniques, such as Integrated Gradients, are a typical example
of a visualization technique containing a high level of information, but with
difficult interpretation. In this paper, we propose two methods, Maximum
Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization
(Ms-IV), to explain the model&apos;s decision, enhancing global interpretability.
MAGE finds, for a given CNN, combinations of features which, globally, form a
semantic meaning, that we call concepts. We group these similar feature
patterns by clustering in ``concepts&apos;&apos;, that we visualize through Ms-IV. This
last method is inspired by Occlusion and Sensitivity analysis (incorporating
causality), and uses a novel metric, called Class-aware Order Correlation
(CaOC), to globally evaluate the most important image regions according to the
model&apos;s decision space. We compare our approach to xAI methods such as LIME and
Integrated Gradients. Experimental results evince the Ms-IV higher localization
and faithfulness values. Finally, qualitative evaluation of combined MAGE and
Ms-IV demonstrates humans&apos; ability to agree, based on the visualization, with
the decision of clusters&apos; concepts; and, to detect, among a given set of
networks, the existence of bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_C/0/1/0/all/0/1&quot;&gt;Caroline Mazini Rodrigues&lt;/a&gt; (LIGM, LRDE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1&quot;&gt;Nicolas Boutry&lt;/a&gt; (LRDE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1&quot;&gt;Laurent Najman&lt;/a&gt; (LIGM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03036">
<title>An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection. (arXiv:2309.03036v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03036</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially spoofed audio detection is a challenging task, lying in the need to
accurately locate the authenticity of audio at the frame level. To address this
issue, we propose a fine-grained partially spoofed audio detection method,
namely Temporal Deepfake Location (TDL), which can effectively capture
information of both features and locations. Specifically, our approach involves
two novel parts: embedding similarity module and temporal convolution
operation. To enhance the identification between the real and fake features,
the embedding similarity module is designed to generate an embedding space that
can separate the real frames from fake frames. To effectively concentrate on
the position information, temporal convolution operation is proposed to
calculate the frame-specific similarities among neighboring frames, and
dynamically select informative neighbors to convolution. Extensive experiments
show that our method outperform baseline models in ASVspoof2019 Partial Spoof
dataset and demonstrate superior performance even in the crossdataset scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuankun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Haonan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yutian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1&quot;&gt;Long Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06255">
<title>Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06255</link>
<description rdf:parseType="Literal">&lt;p&gt;One primary topic of multi-modal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multi-modal cooperation, which could not jointly
utilize all modalities well. Some methods are proposed to identify and enhance
the worse learnt modality, but are often hard to provide the fine-grained
observation of multi-modal cooperation at sample-level with theoretical
support. Hence, it is essential to reasonably observe and improve the
fine-grained cooperation between modalities, especially when facing realistic
scenarios where the modality discrepancy could vary across different samples.
To this end, we introduce a fine-grained modality valuation metric to evaluate
the contribution of each modality at sample-level. Via modality valuation, we
regretfully observe that the multi-modal model tends to rely on one specific
modality, resulting in other modalities being low-contributing. We further
analyze this issue and improve cooperation between modalities by enhancing the
discriminative ability of low-contributing modalities in a targeted manner.
Overall, our methods reasonably observe the fine-grained uni-modal contribution
at sample-level and achieve considerable improvement on different multi-modal
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yake Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruoxuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Di Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17453">
<title>Efficient Streaming Language Models with Attention Sinks. (arXiv:2309.17453v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17453</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying Large Language Models (LLMs) in streaming applications such as
multi-round dialogue, where long interactions are expected, is urgently needed
but poses two major challenges. Firstly, during the decoding stage, caching
previous tokens&apos; Key and Value states (KV) consumes extensive memory. Secondly,
popular LLMs cannot generalize to longer texts than the training sequence
length. Window attention, where only the most recent KVs are cached, is a
natural approach -- but we show that it fails when the text length surpasses
the cache size. We observe an interesting phenomenon, namely attention sink,
that keeping the KV of initial tokens will largely recover the performance of
window attention. In this paper, we first demonstrate that the emergence of
attention sink is due to the strong attention scores towards initial tokens as
a ``sink&apos;&apos; even if they are not semantically important. Based on the above
analysis, we introduce StreamingLLM, an efficient framework that enables LLMs
trained with a finite length attention window to generalize to infinite
sequence lengths without any fine-tuning. We show that StreamingLLM can enable
Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language
modeling with up to 4 million tokens and more. In addition, we discover that
adding a placeholder token as a dedicated attention sink during pre-training
can further improve streaming deployment. In streaming settings, StreamingLLM
outperforms the sliding window recomputation baseline by up to 22.2x speedup.
Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1&quot;&gt;Guangxuan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Beidi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00582">
<title>Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs. (arXiv:2310.00582v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00582</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in various multi-modal tasks. Nevertheless, their performance in fine-grained
image understanding tasks is still limited. To address this issue, this paper
proposes a new framework to enhance the fine-grained image understanding
abilities of MLLMs. Specifically, we present a new method for constructing the
instruction tuning dataset at a low cost by leveraging annotations in existing
datasets. A self-consistent bootstrapping method is also introduced to extend
existing dense object annotations into high-quality
referring-expression-bounding-box pairs. These methods enable the generation of
high-quality instruction data which includes a wide range of fundamental
abilities essential for fine-grained image perception. Moreover, we argue that
the visual encoder should be tuned during instruction tuning to mitigate the
gap between full image perception and fine-grained image perception.
Experimental results demonstrate the superior performance of our method. For
instance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA
and surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We also attain
the top rank on the leaderboard of MMBench. This promising performance is
achieved by training on only publicly available data, making it easily
reproducible. The models, datasets, and codes are publicly available at
https://github.com/SY-Xuan/Pink.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_S/0/1/0/all/0/1&quot;&gt;Shiyu Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qingpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiliang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02129">
<title>Unveiling the Pitfalls of Knowledge Editing for Large Language Models. (arXiv:2310.02129v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there&apos;s
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code is available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhoubo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02168">
<title>Editing Personality for LLMs. (arXiv:2310.02168v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02168</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models&apos;
responses to opinion-related questions on specified topics since an
individual&apos;s personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05140">
<title>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements. (arXiv:2310.05140v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05140</link>
<description rdf:parseType="Literal">&lt;p&gt;Empathetic dialogue is an indispensable part of building harmonious social
relationships and contributes to the development of a helpful AI. Previous
approaches are mainly based on fine small-scale language models. With the
advent of ChatGPT, the application effect of large language models (LLMs) in
this field has attracted great attention. This work empirically investigates
the performance of LLMs in generating empathetic responses and proposes three
improvement methods of semantically similar in-context learning, two-stage
interactive generation, and combination with the knowledge base. Extensive
experiments show that LLMs can significantly benefit from our proposed methods
and is able to achieve state-of-the-art performance in both automatic and human
evaluations. Additionally, we explore the possibility of GPT-4 simulating human
evaluators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yushan Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei-Nan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13019">
<title>Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13019</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have significantly advanced various domains, but
their vulnerability to adversarial attacks poses serious concerns.
Understanding these vulnerabilities and developing effective defense mechanisms
is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016),
finds minimal perturbations to misclassify input images. However, DeepFool
lacks a targeted approach, making it less effective in specific attack
scenarios. Also, in previous related works, researchers primarily focus on
success, not considering how much an image is getting distorted; the integrity
of the image quality, and the confidence level to misclassifying. So, in this
paper, we propose Enhanced Targeted DeepFool, an augmented version of DeepFool
that allows targeting specific classes for misclassification and also introduce
a minimum confidence score requirement hyperparameter to enhance flexibility.
Our experiments demonstrate the effectiveness and efficiency of the proposed
method across different deep neural network architectures while preserving
image integrity as much and perturbation rate as less as possible. By using our
approach, the behavior of models can be manipulated arbitrarily using the
perturbed images, as we can specify both the target class and the associated
confidence score, unlike other DeepFool-derivative works, such as Targeted
DeepFool by Gajjar et al. (2022). Results show that one of the deep
convolutional neural network architectures, AlexNet, and one of the
state-of-the-art model Vision Transformer exhibit high robustness to getting
fooled. This approach can have larger implication, as our tuning of confidence
level can expose the robustness of image recognition models. Our code will be
made public upon acceptance of the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Labib_S/0/1/0/all/0/1&quot;&gt;S. M. Fazle Rabby Labib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Joyanta Jyoti Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manab_M/0/1/0/all/0/1&quot;&gt;Meem Arafat Manab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13230">
<title>Absolute Policy Optimization. (arXiv:2310.13230v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13230</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, trust region on-policy reinforcement learning has achieved
impressive results in addressing complex control tasks and gaming scenarios.
However, contemporary state-of-the-art algorithms within this category
primarily emphasize improvement in expected performance, lacking the ability to
control over the worst-case performance outcomes. To address this limitation,
we introduce a novel objective function; by optimizing which, it will lead to
guaranteed monotonic improvement in the lower bound of near-total performance
samples (absolute performance). Considering this groundbreaking theoretical
advancement, we then refine this theoretically grounded algorithm through a
series of approximations, resulting in a practical solution called Absolute
Policy Optimization (APO). Our experiments demonstrate the effectiveness of our
approach across challenging continuous control benchmark tasks and extend its
applicability to mastering Atari games. Our findings reveal that APO
significantly outperforms state-of-the-art policy gradient algorithms,
resulting in substantial improvements in both expected performance and
worst-case performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiye Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feihan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianhao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17940">
<title>Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17940</link>
<description rdf:parseType="Literal">&lt;p&gt;Simultaneous sequence generation is a pivotal task for real-time scenarios,
such as streaming speech recognition, simultaneous machine translation and
simultaneous speech translation, where the target sequence is generated while
receiving the source sequence. The crux of achieving high-quality generation
with low latency lies in identifying the optimal moments for generating,
accomplished by learning a mapping between the source and target sequences.
However, existing methods often rely on task-specific heuristics for different
sequence types, limiting the model&apos;s capacity to adaptively learn the
source-target mapping and hindering the exploration of multi-task learning for
various simultaneous tasks. In this paper, we propose a unified
segment-to-segment framework (Seg2Seg) for simultaneous sequence generation,
which learns the mapping in an adaptive and unified manner. During the process
of simultaneous generation, the model alternates between waiting for a source
segment and generating a target segment, making the segment serve as the
natural bridge between the source and target. To accomplish this, Seg2Seg
introduces a latent segment as the pivot between source to target and explores
all potential source-target mappings via the proposed expectation training,
thereby learning the optimal moments for generating. Experiments on multiple
simultaneous generation tasks demonstrate that Seg2Seg achieves
state-of-the-art performance and exhibits better generality across various
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaolei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18168">
<title>Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18168</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are trained on vast amounts of text from the
internet, which contains both factual and misleading information about the
world. Can language models discern truth from falsehood in this contradicting
data? Expanding on the view that LLMs can model different communicative agents,
we present the persona hypothesis: LLMs can cluster agents into personas using
common features of their generations. For instance, a truthful persona is a
group of agents that are likely to produce truthful text and that share similar
features like formal writing styles and scientific references. By modeling this
persona, LLMs can generalize truthfulness beyond the specific contexts in which
each agent generated the training text. For example, the model can infer that
the agent ``Wikipedia&apos;&apos; will behave truthfully on topics that were only
generated by ``Science&apos;&apos; because they both belong to the truthful persona. We
show evidence for the persona hypothesis via two observations: (1) we can probe
whether a model&apos;s answer will be truthful before it is generated; (2)
finetuning a model on a set of facts improves its truthfulness on unseen
topics. Next, using arithmetics as a synthetic environment, we show that
language models can separate true and false statements, and generalize
truthfulness across agents; but only if agents in the training data share a
truthful generative process that enables the creation of a truthful persona.
Overall, our findings suggest that models can exploit hierarchical structures
in the data to learn abstract concepts like truthfulness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nitish Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1&quot;&gt;Javier Rando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1&quot;&gt;Abulhair Saparov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Najoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19805">
<title>Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19805</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline-to-online RL can make full use of pre-collected offline datasets to
initialize policies, resulting in higher sample efficiency and better
performance compared to only using online algorithms alone for policy training.
However, direct fine-tuning of the pre-trained policy tends to result in
sub-optimal performance. A primary reason is that conservative offline RL
methods diminish the agent&apos;s capability of exploration, thereby impacting
online fine-tuning performance. To encourage agent&apos;s exploration during online
fine-tuning and enhance the overall online fine-tuning performance, we propose
a generalized reward augmentation method called Sample Efficient Reward
Augmentation (SERA). Specifically, SERA encourages agent to explore by
computing Q conditioned entropy as intrinsic reward. The advantage of SERA is
that it can extensively utilize offline pre-trained Q to encourage agent
uniformly coverage of state space while considering the imbalance between the
distributions of high-value and low-value states. Additionally, SERA can be
effortlessly plugged into various RL algorithms to improve online fine-tuning
and ensure sustained asymptotic improvement. Moreover, extensive experimental
results demonstrate that when conducting offline-to-online problems, SERA
consistently and effectively enhances the performance of various offline
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1&quot;&gt;Xiao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zifeng Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01331">
<title>Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01331</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world scenarios, arbitrary interactions with the environment can
often be costly, and actions of expert demonstrations are not always available.
To reduce the need for both, Offline Learning from Observations (LfO) is
extensively studied, where the agent learns to solve a task with only expert
states and \textit{task-agnostic} non-expert state-action pairs. The
state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the
state occupancy divergence between the learner and expert policies. However,
they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein
distance with Rubinstein duality, the latter of which constrains the underlying
distance metric crucial to the performance of Wasserstein-based solutions. To
address this problem, we propose Primal Wasserstein DICE (PW-DICE), which
minimizes the primal Wasserstein distance between the expert and learner state
occupancies with a pessimistic regularizer and leverages a contrastively
learned distance as the underlying metric for the Wasserstein distance.
Theoretically, we prove that our framework is a generalization of the
state-of-the-art, SMODICE, and unifies $f$-divergence and Wasserstein
minimization. Empirically, we find that PW-DICE improves upon several
state-of-the-art methods on multiple testbeds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-xiong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02198">
<title>Imitation Bootstrapped Reinforcement Learning. (arXiv:2311.02198v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02198</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the considerable potential of reinforcement learning (RL), robotics
control tasks predominantly rely on imitation learning (IL) owing to its better
sample efficiency. However, given the high cost of collecting extensive
demonstrations, RL is still appealing if it can utilize limited imitation data
for efficient autonomous self-improvement. Existing RL methods that utilize
demonstrations either initialize the replay buffer with demonstrations and
oversample them during RL training, which does not benefit from the
generalization potential of modern IL methods, or pretrain the RL policy with
IL on the demonstrations, which requires additional mechanisms to prevent
catastrophic forgetting during RL fine-tuning. We propose imitation
bootstrapped reinforcement learning (IBRL), a novel framework that first trains
an IL policy on a limited number of demonstrations and then uses it to propose
alternative actions for both online exploration and target value bootstrapping.
IBRL achieves SoTA performance and sample efficiency on 7 challenging sparse
reward continuous control tasks in simulation while learning directly from
pixels. As a highlight of our method, IBRL achieves $6.4\times$ higher success
rate than RLPD, a strong method that combines the idea of oversampling
demonstrations with modern RL improvements, under the budget of 10 demos and
100K interactions in the challenging PickPlaceCan task in the Robomimic
benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1&quot;&gt;Suvir Mirchandani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03488">
<title>Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems. (arXiv:2311.03488v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03488</link>
<description rdf:parseType="Literal">&lt;p&gt;While recommender systems have become an integral component of the Web
experience, their heavy reliance on user data raises privacy and security
concerns. Substituting user data with synthetic data can address these
concerns, but accurately replicating these real-world datasets has been a
notoriously challenging problem. Recent advancements in generative AI have
demonstrated the impressive capabilities of diffusion models in generating
realistic data across various domains. In this work we introduce a Score-based
Diffusion Recommendation Module (SDRM), which captures the intricate patterns
of real-world datasets required for training highly accurate recommender
systems. SDRM allows for the generation of synthetic data that can replace
existing datasets to preserve user privacy, or augment existing datasets to
address excessive data sparsity. Our method outperforms competing baselines
such as generative adversarial networks, variational autoencoders, and recently
proposed diffusion models in synthesizing various datasets to replace or
augment the original data by an average improvement of 4.30% in Recall@$k$ and
4.65% in NDCG@$k$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lilienthal_D/0/1/0/all/0/1&quot;&gt;Derek Lilienthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1&quot;&gt;Paul Mello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eirinaki_M/0/1/0/all/0/1&quot;&gt;Magdalini Eirinaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiomkin_S/0/1/0/all/0/1&quot;&gt;Stas Tiomkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05067">
<title>Accelerating Exploration with Unlabeled Prior Data. (arXiv:2311.05067v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05067</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to solve tasks from a sparse reward signal is a major challenge for
standard reinforcement learning (RL) algorithms. However, in the real world,
agents rarely need to solve sparse reward tasks entirely from scratch. More
often, we might possess prior experience to draw on that provides considerable
guidance about which actions and outcomes are possible in the world, which we
can use to explore more effectively for new tasks. In this work, we study how
prior data without reward labels may be used to guide and accelerate
exploration for an agent solving a new sparse reward task. We propose a simple
approach that learns a reward model from online experience, labels the
unlabeled prior data with optimistic rewards, and then uses it concurrently
alongside the online data for downstream policy and critic optimization. This
general formula leads to rapid exploration in several challenging sparse-reward
domains where tabula rasa exploration is insufficient, including the AntMaze
domain, Adroit hand manipulation domain, and a visual simulated robotic
manipulation domain. Our results highlight the ease of incorporating unlabeled
prior data into existing online RL algorithms, and the (perhaps surprising)
effectiveness of doing so.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jason Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1&quot;&gt;Dibya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05481">
<title>META4: Semantically-Aligned Generation of Metaphoric Gestures Using Self-Supervised Text and Speech Representation. (arXiv:2311.05481v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05481</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Schemas are repetitive cognitive patterns that influence the way we
conceptualize and reason about various concepts present in speech. These
patterns are deeply embedded within our cognitive processes and are reflected
in our bodily expressions including gestures. Particularly, metaphoric gestures
possess essential characteristics and semantic meanings that align with Image
Schemas, to visually represent abstract concepts. The shape and form of
gestures can convey abstract concepts, such as extending the forearm and hand
or tracing a line with hand movements to visually represent the image schema of
PATH. Previous behavior generation models have primarily focused on utilizing
speech (acoustic features and text) to drive the generation model of virtual
agents. They have not considered key semantic information as those carried by
Image Schemas to effectively generate metaphoric gestures. To address this
limitation, we introduce META4, a deep learning approach that generates
metaphoric gestures from both speech and Image Schemas. Our approach has two
primary goals: computing Image Schemas from input text to capture the
underlying semantic and metaphorical meaning, and generating metaphoric
gestures driven by speech and the computed image schemas. Our approach is the
first method for generating speech driven metaphoric gestures while leveraging
the potential of Image Schemas. We demonstrate the effectiveness of our
approach and highlight the importance of both speech and image schemas in
modeling metaphoric gestures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fares_M/0/1/0/all/0/1&quot;&gt;Mireille Fares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelachaud_C/0/1/0/all/0/1&quot;&gt;Catherine Pelachaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obin_N/0/1/0/all/0/1&quot;&gt;Nicolas Obin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06233">
<title>Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models. (arXiv:2311.06233v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06233</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Data Contamination Quiz, a simple and effective approach to
detect data contamination in large language models (LLMs) and estimate the
amount of it. Specifically, we frame data contamination detection as a series
of multiple-choice questions. We devise a quiz format wherein three perturbed
versions of each dataset instance are created. These changes only include
word-level perturbations, replacing words with their contextual synonyms,
ensuring both the semantic and sentence structure remain exactly the same as
the original instance. Together with the original instance, these perturbed
versions constitute the choices in the quiz. Given that the only distinguishing
signal among these choices is the exact wording, an LLM, when tasked with
identifying the original instance from the choices, opts for the original if it
has memorized it in its pre-training phase--a trait intrinsic to LLMs. A
dataset partition is then marked as contaminated if the LLM&apos;s performance on
the quiz surpasses what random chance suggests. Our evaluation spans seven
datasets and their respective splits (train and test/validation) on two
state-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to the
pre-training data, our results suggest that our approach not only enhances the
detection of data contamination but also provides an accurate estimation of its
extent, even when the contamination signal is weak.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1&quot;&gt;Shahriar Golchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08577">
<title>Finding AI-Generated Faces in the Wild. (arXiv:2311.08577v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porcile_G/0/1/0/all/0/1&quot;&gt;Gonzalo J. Aniano Porcile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gindi_J/0/1/0/all/0/1&quot;&gt;Jack Gindi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundra_S/0/1/0/all/0/1&quot;&gt;Shivansh Mundra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbus_J/0/1/0/all/0/1&quot;&gt;James R. Verbus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1&quot;&gt;Hany Farid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10770">
<title>Exponentially Faster Language Modelling. (arXiv:2311.10770v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10770</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belcak_P/0/1/0/all/0/1&quot;&gt;Peter Belcak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10801">
<title>Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools. (arXiv:2311.10801v2 [q-fin.PM] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10801</link>
<description rdf:parseType="Literal">&lt;p&gt;Portfolio management (PM) is a fundamental financial trading task, which
explores the optimal periodical reallocation of capitals into different stocks
to pursue long-term profits. Reinforcement learning (RL) has recently shown its
potential to train profitable agents for PM through interacting with financial
markets. However, existing work mostly focuses on fixed stock pools, which is
inconsistent with investors&apos; practical demand. Specifically, the target stock
pool of different investors varies dramatically due to their discrepancy on
market states and individual investors may temporally adjust stocks they desire
to trade (e.g., adding one popular stocks), which lead to customizable stock
pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny
change of the stock pool, which leads to high computational cost and unstable
performance. To tackle this challenge, we propose EarnMore, a rEinforcement
leARNing framework with Maskable stOck REpresentation to handle PM with CSPs
through one-shot training in a global stock pool (GSP). Specifically, we first
introduce a mechanism to mask out the representation of the stocks outside the
target pool. Second, we learn meaningful stock representations through a
self-supervised masking and reconstruction process. Third, a re-weighting
mechanism is designed to make the portfolio concentrate on favorable stocks and
neglect the stocks outside the target pool. Through extensive experiments on 8
subset stock pools of the US stock market, we demonstrate that EarnMore
significantly outperforms 14 state-of-the-art baselines in terms of 6 popular
financial metrics with over 40% improvement on profit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yilei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Ying_J/0/1/0/all/0/1&quot;&gt;Jie Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yonggang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zitao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bo An&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10863">
<title>Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks. (arXiv:2311.10863v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10863</link>
<description rdf:parseType="Literal">&lt;p&gt;Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zihe Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1&quot;&gt;Yiannis Kantaros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10934">
<title>Case Repositories: Towards Case-Based Reasoning for AI Alignment. (arXiv:2311.10934v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10934</link>
<description rdf:parseType="Literal">&lt;p&gt;Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed&apos;&apos; cases -- questions one may
ask an AI system -- in a particular domain from discussions in online
communities, 2) eliciting domain-specific key dimensions for cases through
workshops with domain experts, 3) using LLMs to generate variations of cases
not seen in the wild, and 4) engaging with the public to judge and improve
cases. We then discuss how such a case repository could assist in AI alignment,
both through directly acting as precedents to ground acceptable behaviors, and
as a medium for individuals and communities to engage in moral reasoning around
AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;K. J. Kevin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Ze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_I/0/1/0/all/0/1&quot;&gt;Inyoung Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1&quot;&gt;King Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy X. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11908">
<title>Continual Learning: Applications and the Road Forward. (arXiv:2311.11908v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11908</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: &quot;Why should one care about continual learning in the
first place?&quot;. We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verwimp_E/0/1/0/all/0/1&quot;&gt;Eli Verwimp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1&quot;&gt;Rahaf Aljundi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1&quot;&gt;Shai Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1&quot;&gt;Andrea Cossu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1&quot;&gt;Alexander Gepperth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1&quot;&gt;Tyler L. Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kudithipudi_D/0/1/0/all/0/1&quot;&gt;Dhireesha Kudithipudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1&quot;&gt;Christoph H. Lampert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1&quot;&gt;Martin Mundt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1&quot;&gt;Adrian Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1&quot;&gt;Andreas S. Tolias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1&quot;&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1&quot;&gt;Gido M. van de Ven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11995">
<title>BrainWash: A Poisoning Attack to Forget in Continual Learning. (arXiv:2311.11995v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11995</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning has gained substantial attention within the deep learning
community, offering promising solutions to the challenging problem of
sequential learning. Yet, a largely unexplored facet of this paradigm is its
susceptibility to adversarial attacks, especially with the aim of inducing
forgetting. In this paper, we introduce &quot;BrainWash,&quot; a novel data poisoning
method tailored to impose forgetting on a continual learner. By adding the
BrainWash noise to a variety of baselines, we demonstrate how a trained
continual learner can be induced to forget its previously learned tasks
catastrophically, even when using these continual learning baselines. An
important feature of our approach is that the attacker requires no access to
previous tasks&apos; data and is armed merely with the model&apos;s current parameters
and the data belonging to the most recent task. Our extensive experiments
highlight the efficacy of BrainWash, showcasing degradation in performance
across various regularization-based continual learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1&quot;&gt;Ali Abbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1&quot;&gt;Parsa Nooralinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1&quot;&gt;Hamed Pirsiavash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1&quot;&gt;Soheil Kolouri&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>