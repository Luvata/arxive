<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.01052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.03259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.07881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.03403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.11166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.01001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.03455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.12509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.05216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.03950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.08982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.06362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.09753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.12900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.01110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.14319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08529" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.10177">
<title>Bayesian Spike Train Inference via Non-Local Priors. (arXiv:2307.10177v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10177</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in neuroscience have enabled researchers to measure the activities
of large numbers of neurons simultaneously in behaving animals. We have access
to the fluorescence of each of the neurons which provides a first-order
approximation of the neural activity over time. Determining the exact spike of
a neuron from this fluorescence trace constitutes an active area of research
within the field of computational neuroscience. We propose a novel Bayesian
approach based on a mixture of half-non-local prior densities and point masses
for this task. Instead of a computationally expensive MCMC algorithm, we adopt
a stochastic search-based approach that is capable of taking advantage of
modern computing environments often equipped with multiple processors, to
explore all possible arrangements of spikes and lack thereof in an observed
spike train. It then reports the highest posterior probability arrangement of
spikes and posterior probability for a spike at each location of the spike
train. Our proposals lead to substantial improvements over existing proposals
based on L1 regularization, and enjoy comparable estimation accuracy to the
state-of-the-art L0 proposal, in simulations, and on recent calcium imaging
data sets. Notably, contrary to optimization-based frequentist approaches, our
methodology yields automatic uncertainty quantification associated with the
spike-train inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chakraborty_A/0/1/0/all/0/1&quot;&gt;Abhisek Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10181">
<title>Community-Aware Transformer for Autism Prediction in fMRI Connectome. (arXiv:2307.10181v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10181</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism spectrum disorder(ASD) is a lifelong neurodevelopmental condition that
affects social communication and behavior. Investigating functional magnetic
resonance imaging (fMRI)-based brain functional connectome can aid in the
understanding and diagnosis of ASD, leading to more effective treatments. The
brain is modeled as a network of brain Regions of Interest (ROIs), and ROIs
form communities and knowledge of these communities is crucial for ASD
diagnosis. On the one hand, Transformer-based models have proven to be highly
effective across several tasks, including fMRI connectome analysis to learn
useful representations of ROIs. On the other hand, existing transformer-based
models treat all ROIs equally and overlook the impact of community-specific
associations when learning node embeddings. To fill this gap, we propose a
novel method, Com-BrainTF, a hierarchical local-global transformer architecture
that learns intra and inter-community aware node embeddings for ASD prediction
task. Furthermore, we avoid over-parameterization by sharing the local
transformer parameters for different communities but optimize unique learnable
prompt tokens for each community. Our model outperforms state-of-the-art (SOTA)
architecture on ABIDE dataset and has high interpretability, evident from the
attention module. Our code is available at
https://github.com/ubc-tea/Com-BrainTF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bannadabhavi_A/0/1/0/all/0/1&quot;&gt;Anushree Bannadabhavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soojin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wenlong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10184">
<title>A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. (arXiv:2307.10184v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10184</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attacks pose serious security threats to deep neural networks
(DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on
inputs embedded with well-designed triggers while behaving normally on clean
inputs. Many works have explored the invisibility of backdoor triggers to
improve attack stealthiness. However, most of them only consider the
invisibility in the spatial domain without explicitly accounting for the
generation of invisible triggers in the frequency domain, making the generated
poisoned images be easily detected by recent defense methods. To address this
issue, in this paper, we propose a DUal stealthy BAckdoor attack method named
DUBA, which simultaneously considers the invisibility of triggers in both the
spatial and frequency domains, to achieve desirable attack performance, while
ensuring strong stealthiness. Specifically, we first use Discrete Wavelet
Transform to embed the high-frequency information of the trigger image into the
clean image to ensure attack effectiveness. Then, to attain strong
stealthiness, we incorporate Fourier Transform and Discrete Cosine Transform to
mix the poisoned image and clean image in the frequency domain. Moreover, the
proposed DUBA adopts a novel attack strategy, in which the model is trained
with weak triggers and attacked with strong triggers to further enhance the
attack performance and stealthiness. We extensively evaluate DUBA against
popular image classifiers on four datasets. The results demonstrate that it
significantly outperforms the state-of-the-art backdoor attacks in terms of the
attack success rate and stealthiness
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yudong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Honglong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junjian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10186">
<title>Multi-Scale U-Shape MLP for Hyperspectral Image Classification. (arXiv:2307.10186v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10186</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images have significant applications in various domains, since
they register numerous semantic and spatial information in the spectral band
with spatial variability of spectral signatures. Two critical challenges in
identifying pixels of the hyperspectral image are respectively representing the
correlated information among the local and global, as well as the abundant
parameters of the model. To tackle this challenge, we propose a Multi-Scale
U-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC
(Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron)
structure. MSC transforms the channel dimension and mixes spectral band feature
to embed the deep-level representation adequately. UMLP is designed by the
encoder-decoder structure with multi-layer perceptron layers, which is capable
of compressing the large-scale parameters. Extensive experiments are conducted
to demonstrate our model can outperform state-of-the-art methods
across-the-board on three wide-adopted public datasets, namely Pavia
University, Houston 2013 and Houston 2018
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Moule Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jing_W/0/1/0/all/0/1&quot;&gt;Weipeng Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Di_D/0/1/0/all/0/1&quot;&gt;Donglin Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangsheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Houbing Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10187">
<title>Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10187</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the privacy-enhancing properties of subsampling a data set via
importance sampling as a pre-processing step for differentially private
mechanisms. This extends the established privacy amplification by subsampling
result to importance sampling where each data point is weighted by the
reciprocal of its selection probability. The implications for privacy of
weighting each point are not obvious. On the one hand, a lower selection
probability leads to a stronger privacy amplification. On the other hand, the
higher the weight, the stronger the influence of the point on the output of the
mechanism in the event that the point does get selected. We provide a general
result that quantifies the trade-off between these two effects. We show that
heterogeneous sampling probabilities can lead to both stronger privacy and
better utility than uniform subsampling while retaining the subsample size. In
particular, we formulate and solve the problem of privacy-optimal sampling,
that is, finding the importance weights that minimize the expected subset size
subject to a given privacy budget. Empirically, we evaluate the privacy,
efficiency, and accuracy of importance sampling-based privacy amplification on
the example of k-means clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fay_D/0/1/0/all/0/1&quot;&gt;Dominik Fay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mair_S/0/1/0/all/0/1&quot;&gt;Sebastian Mair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sjolund_J/0/1/0/all/0/1&quot;&gt;Jens Sj&amp;#xf6;lund&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10188">
<title>Several categories of Large Language Models (LLMs): A Short Survey. (arXiv:2307.10188v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10188</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models(LLMs)have become effective tools for natural language
processing and have been used in many different fields. This essay offers a
succinct summary of various LLM subcategories. The survey emphasizes recent
developments and efforts made for various LLM kinds, including task-based
financial LLMs, multilingual language LLMs, biomedical and clinical LLMs,
vision language LLMs, and code language models. The survey gives a general
summary of the methods, attributes, datasets, transformer models, and
comparison metrics applied in each category of LLMs. Furthermore, it highlights
unresolved problems in the field of developing chatbots and virtual assistants,
such as boosting natural language processing, enhancing chatbot intelligence,
and resolving moral and legal dilemmas. The purpose of this study is to provide
readers, developers, academics, and users interested in LLM-based chatbots and
virtual intelligent assistant technologies with useful information and future
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahune_S/0/1/0/all/0/1&quot;&gt;Saurabh Pahune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekharan_M/0/1/0/all/0/1&quot;&gt;Manoj Chandrasekharan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10193">
<title>StyleGAN2-based Out-of-Distribution Detection for Medical Imaging. (arXiv:2307.10193v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10193</link>
<description rdf:parseType="Literal">&lt;p&gt;One barrier to the clinical deployment of deep learning-based models is the
presence of images at runtime that lie far outside the training distribution of
a given model. We aim to detect these out-of-distribution (OOD) images with a
generative adversarial network (GAN). Our training dataset was comprised of
3,234 liver-containing computed tomography (CT) scans from 456 patients. Our
OOD test data consisted of CT images of the brain, head and neck, lung, cervix,
and abnormal livers. A StyleGAN2-ADA architecture was employed to model the
training distribution. Images were reconstructed using backpropagation.
Reconstructions were evaluated using the Wasserstein distance, mean squared
error, and the structural similarity index measure. OOD detection was evaluated
with the area under the receiver operating characteristic curve (AUROC). Our
paradigm distinguished between liver and non-liver CT with greater than 90%
AUROC. It was also completely unable to reconstruct liver artifacts, such as
needles and ascites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Woodland_M/0/1/0/all/0/1&quot;&gt;McKell Woodland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wood_J/0/1/0/all/0/1&quot;&gt;John Wood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+OConnor_C/0/1/0/all/0/1&quot;&gt;Caleb O&amp;#x27;Connor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Ankit B. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brock_K/0/1/0/all/0/1&quot;&gt;Kristy K. Brock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10200">
<title>Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings. (arXiv:2307.10200v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.10200</link>
<description rdf:parseType="Literal">&lt;p&gt;Divorce is the legal dissolution of a marriage by a court. Since this is
usually an unpleasant outcome of a marital union, each party may have reasons
to call the decision to quit which is generally documented in detail in the
court proceedings. Via a substantial corpus of 17,306 court proceedings, this
paper investigates gender inequality through the lens of divorce court
proceedings. While emerging data sources (e.g., public court records) on
sensitive societal issues hold promise in aiding social science research,
biases present in cutting-edge natural language processing (NLP) methods may
interfere with or affect such studies. We thus require a thorough analysis of
potential gaps and limitations present in extant NLP resources. In this paper,
on the methodological side, we demonstrate that existing NLP resources required
several non-trivial modifications to quantify societal inequalities. On the
substantive side, we find that while a large number of court cases perhaps
suggest changing norms in India where women are increasingly challenging
patriarchy, AI-powered analyses of these court proceedings indicate striking
gender inequality with women often subjected to domestic violence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sujan Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1&quot;&gt;Parth Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solunke_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Solunke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1&quot;&gt;Swaprava Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+KhudaBukhsh_A/0/1/0/all/0/1&quot;&gt;Ashiqur R. KhudaBukhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10204">
<title>An IPW-based Unbiased Ranking Metric in Two-sided Markets. (arXiv:2307.10204v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.10204</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern recommendation systems, unbiased learning-to-rank (LTR) is crucial
for prioritizing items from biased implicit user feedback, such as click data.
Several techniques, such as Inverse Propensity Weighting (IPW), have been
proposed for single-sided markets. However, less attention has been paid to
two-sided markets, such as job platforms or dating services, where successful
conversions require matching preferences from both users. This paper addresses
the complex interaction of biases between users in two-sided markets and
proposes a tailored LTR approach. We first present a formulation of feedback
mechanisms in two-sided matching platforms and point out that their implicit
feedback may include position bias from both user groups. On the basis of this
observation, we extend the IPW estimator and propose a new estimator, named
two-sided IPW, to address the position bases in two-sided markets. We prove
that the proposed estimator satisfies the unbiasedness for the ground-truth
ranking metric. We conducted numerical experiments on real-world two-sided
platforms and demonstrated the effectiveness of our proposed method in terms of
both precision and robustness. Our experiments showed that our method
outperformed baselines especially when handling rare items, which are less
frequently observed in the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1&quot;&gt;Keisho Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_N/0/1/0/all/0/1&quot;&gt;Naoki Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1&quot;&gt;Minje Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1&quot;&gt;Ken Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1&quot;&gt;Kazuhide Nakata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10205">
<title>Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10205</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study adversarial training on datasets that obey the
long-tailed distribution, which is practical but rarely explored in previous
works. Compared with conventional adversarial training on balanced datasets,
this process falls into the dilemma of generating uneven adversarial examples
(AEs) and an unbalanced feature embedding space, causing the resulting model to
exhibit low robustness and accuracy on tail data. To combat that, we propose a
new adversarial training framework -- Re-balancing Adversarial Training (REAT).
This framework consists of two components: (1) a new training strategy inspired
by the term effective number to guide the model to generate more balanced and
informative AEs; (2) a carefully constructed penalty function to force a
satisfactory feature space. Evaluation results on different datasets and model
structures prove that REAT can effectively enhance the model&apos;s robustness and
preserve the model&apos;s clean accuracy. The code can be found in
https://github.com/GuanlinLee/REAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10209">
<title>On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (arXiv:2307.10209v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10209</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as load
disaggregation algorithms, are fundamental tools for effective energy
management. Despite the success of deep models in load disaggregation, they
face various challenges, particularly those pertaining to privacy and security.
This paper investigates the sensitivity of prominent deep NILM baselines to
adversarial attacks, which have proven to be a significant threat in domains
such as computer vision and speech recognition. Adversarial attacks entail the
introduction of imperceptible noise into the input data with the aim of
misleading the neural network into generating erroneous outputs. We investigate
the Fast Gradient Sign Method (FGSM), a well-known adversarial attack, to
perturb the input sequences fed into two commonly employed CNN-based NILM
baselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models.
Our findings provide compelling evidence for the vulnerability of these models,
particularly the S2P model which exhibits an average decline of 20\% in the
F1-score even with small amounts of noise. Such weakness has the potential to
generate profound implications for energy management systems in residential and
industrial sectors reliant on NILM models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bousbiat_H/0/1/0/all/0/1&quot;&gt;Hafsa Bousbiat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1&quot;&gt;Abbes Amira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1&quot;&gt;Wathiq Mansoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10214">
<title>Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild. (arXiv:2307.10214v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10214</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber Threat Intelligence (CTI) plays a crucial role in assessing risks and
enhancing security for organizations. However, the process of extracting
relevant information from unstructured text sources can be expensive and
time-consuming. Our empirical experience shows that existing tools for
automated structured CTI extraction have performance limitations. Furthermore,
the community lacks a common benchmark to quantitatively assess their
performance. We fill these gaps providing a new large open benchmark dataset
and aCTIon, a structured CTI information extraction tool. The dataset includes
204 real-world publicly available reports and their corresponding structured
CTI information in STIX format. Our team curated the dataset involving three
independent groups of CTI analysts working over the course of several months.
To the best of our knowledge, this dataset is two orders of magnitude larger
than previously released open source datasets. We then design aCTIon,
leveraging recently introduced large language models (GPT3.5) in the context of
two custom information extraction pipelines. We compare our method with 10
solutions presented in previous work, for which we develop our own
implementations when open-source implementations were lacking. Our results show
that aCTIon outperforms previous work for structured CTI extraction with an
improvement of the F1-score from 10%points to 50%points across all tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siracusano_G/0/1/0/all/0/1&quot;&gt;Giuseppe Siracusano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanvito_D/0/1/0/all/0/1&quot;&gt;Davide Sanvito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_R/0/1/0/all/0/1&quot;&gt;Roberto Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_M/0/1/0/all/0/1&quot;&gt;Manikantan Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamatchi_S/0/1/0/all/0/1&quot;&gt;Sivakaman Kamatchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_W/0/1/0/all/0/1&quot;&gt;Wataru Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawakita_M/0/1/0/all/0/1&quot;&gt;Masaru Kawakita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakumaru_T/0/1/0/all/0/1&quot;&gt;Takahiro Kakumaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bifulco_R/0/1/0/all/0/1&quot;&gt;Roberto Bifulco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10219">
<title>Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.10219</link>
<description rdf:parseType="Literal">&lt;p&gt;Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs)
provide additional key-value pairs (i.e., qualifiers) for each KG fact that
help to better restrict the fact validity. In recent years, there has been an
increasing interest in studying graph reasoning over HKGs. In the meantime, due
to the ever-evolving nature of world knowledge, extensive parallel works have
been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be
viewed as a KG fact coupled with a timestamp (or time period) specifying its
time validity. The existing HKG reasoning approaches do not consider temporal
information because it is not explicitly specified in previous benchmark
datasets. Besides, all the previous TKG reasoning methods only lay emphasis on
temporal reasoning and have no way to learn from qualifiers. To this end, we
aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new
benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, and
propose a HTKG reasoning model that efficiently models both temporal facts and
qualifiers. We further exploit additional time-invariant relational knowledge
from the Wikidata knowledge base and study its effectiveness in HTKG reasoning.
Time-invariant relational knowledge serves as the knowledge that remains
unchanged in time (e.g., Sasha Obama is the child of Barack Obama), and it has
never been fully explored in previous TKG reasoning benchmarks and approaches.
Experimental results show that our model substantially outperforms previous
related methods on HTKG link prediction and can be enhanced by jointly
leveraging both temporal and time-invariant relational knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zifeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingcheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingpei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10231">
<title>Automated Knowledge Modeling for Cancer Clinical Practice Guidelines. (arXiv:2307.10231v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.10231</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to
new evidence generated by active research. Currently, CPGs are primarily
published in a document format that is ill-suited for managing this developing
knowledge. A knowledge model of the guidelines document suitable for
programmatic interaction is required. This work proposes an automated method
for extraction of knowledge from National Comprehensive Cancer Network (NCCN)
CPGs in Oncology and generating a structured model containing the retrieved
knowledge. The proposed method was tested using two versions of NCCN Non-Small
Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful
extraction and modeling of knowledge. Three enrichment strategies using Cancer
staging information, Unified Medical Language System (UMLS) Metathesaurus &amp;amp;
National Cancer Institute thesaurus (NCIt) concepts, and Node classification
are also presented to enhance the model towards enabling programmatic traversal
and querying of cancer care guidelines. The Node classification was performed
using a Support Vector Machine (SVM) model, achieving a classification accuracy
of 0.81 with 10-fold cross-validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ta_P/0/1/0/all/0/1&quot;&gt;Pralaypati Ta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_B/0/1/0/all/0/1&quot;&gt;Bhumika Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Arihant Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+C_S/0/1/0/all/0/1&quot;&gt;Sneha Sree C&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Arunima Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_K/0/1/0/all/0/1&quot;&gt;Keerthi Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivaprakasam_M/0/1/0/all/0/1&quot;&gt;Mohanasankar Sivaprakasam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10234">
<title>SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10234</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a thorough examination of various Generative Pretrained
Transformer (GPT) methodologies in sentiment analysis, specifically in the
context of Task 4 on the SemEval 2017 dataset. Three primary strategies are
employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)
fine-tuning GPT models, and 3) an inventive approach to embedding
classification. The research yields detailed comparative insights among these
strategies and individual GPT models, revealing their unique strengths and
potential limitations. Additionally, the study compares these GPT-based
methodologies with other contemporary, high-performing models previously used
with the same dataset. The results illustrate the significant superiority of
the GPT approaches in terms of predictive performance, more than 22% in
F1-score compared to the state-of-the-art. Further, the paper addresses common
challenges in sentiment analysis tasks, such as understanding context and
detecting sarcasm. It underscores the enhanced capabilities of the GPT models
to effectively navigate these complexities. Collectively, these findings
highlight the promising potential of GPT models in sentiment analysis, setting
the stage for future research in this field. The code can be found at
https://github.com/DSAatUSU/SentimentGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheiri_K/0/1/0/all/0/1&quot;&gt;Kiana Kheiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimi_H/0/1/0/all/0/1&quot;&gt;Hamid Karimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10237">
<title>CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion. (arXiv:2307.10237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10237</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition from image sets acquired under unregulated and uncontrolled
settings, such as at large distances, low resolutions, varying viewpoints,
illumination, pose, and atmospheric conditions, is challenging. Face feature
aggregation, which involves aggregating a set of N feature representations
present in a template into a single global representation, plays a pivotal role
in such recognition systems. Existing works in traditional face feature
aggregation either utilize metadata or high-dimensional intermediate feature
representations to estimate feature quality for aggregation. However,
generating high-quality metadata or style information is not feasible for
extremely low-resolution faces captured in long-range and high altitude
settings. To overcome these limitations, we propose a feature distribution
conditioning approach called CoNAN for template aggregation. Specifically, our
method aims to learn a context vector conditioned over the distribution
information of the incoming feature set, which is utilized to weigh the
features based on their estimated informativeness. The proposed method produces
state-of-the-art results on long-range unconstrained face recognition datasets
such as BTS, and DroneSURF, validating the advantages of such an aggregation
strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1&quot;&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorishin_D/0/1/0/all/0/1&quot;&gt;Dennis Fedorishin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10244">
<title>Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors. (arXiv:2307.10244v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.10244</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep recommendation systems (DRS) heavily depend on specialized HPC hardware
and accelerators to optimize energy, efficiency, and recommendation quality.
Despite the growing number of hardware errors observed in large-scale fleet
systems where DRS are deployed, the robustness of DRS has been largely
overlooked. This paper presents the first systematic study of DRS robustness
against hardware errors. We develop Terrorch, a user-friendly, efficient and
flexible error injection framework on top of the widely-used PyTorch. We
evaluate a wide range of models and datasets and observe that the DRS
robustness against hardware errors is influenced by various factors from model
parameters to input characteristics. We also explore 3 error mitigation methods
including algorithm based fault tolerance (ABFT), activation clipping and
selective bit protection (SBP). We find that applying activation clipping can
recover up to 30% of the degraded AUC-ROC score, making it a promising
mitigation method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1&quot;&gt;Dongning Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1&quot;&gt;Xun Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fred Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengshi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desmaison_A/0/1/0/all/0/1&quot;&gt;Alban Desmaison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellinger_T/0/1/0/all/0/1&quot;&gt;Thomas Sellinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_D/0/1/0/all/0/1&quot;&gt;Daniel Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_S/0/1/0/all/0/1&quot;&gt;Sriram Sankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10246">
<title>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10246</link>
<description rdf:parseType="Literal">&lt;p&gt;How does the brain represent different modes of information? Can we design a
system that automatically understands what the user is thinking? Such questions
can be answered by studying brain recordings like functional magnetic resonance
imaging (fMRI). As a first step, the neuroscience community has contributed
several large cognitive neuroscience datasets related to passive
reading/listening/viewing of concept words, narratives, pictures and movies.
Encoding and decoding models using these datasets have also been proposed in
the past two decades. These models serve as additional tools for basic research
in cognitive science and neuroscience. Encoding models aim at generating fMRI
brain representations given a stimulus automatically. They have several
practical applications in evaluating and diagnosing neurological conditions and
thus also help design therapies for brain damage. Decoding models solve the
inverse problem of reconstructing the stimuli given the fMRI. They are useful
for designing brain-machine or brain-computer interfaces. Inspired by the
effectiveness of deep learning models for natural language processing, computer
vision, and speech, recently several neural encoding and decoding models have
been proposed. In this survey, we will first discuss popular representations of
language, vision and speech stimuli, and present a summary of neuroscience
datasets. Further, we will review popular deep learning based encoding and
decoding architectures and note their benefits and limitations. Finally, we
will conclude with a brief summary and discussion about future trends. Given
the large amount of recently published work in the `computational cognitive
neuroscience&apos; community, we believe that this survey nicely organizes the
plethora of work and presents it as a coherent story.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Oota_S/0/1/0/all/0/1&quot;&gt;Subba Reddy Oota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Manish Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bapi_R/0/1/0/all/0/1&quot;&gt;Raju S. Bapi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jobard_G/0/1/0/all/0/1&quot;&gt;Gael Jobard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Alexandre_F/0/1/0/all/0/1&quot;&gt;Frederic Alexandre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hinaut_X/0/1/0/all/0/1&quot;&gt;Xavier Hinaut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10247">
<title>Automated Action Model Acquisition from Narrative Texts. (arXiv:2307.10247v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10247</link>
<description rdf:parseType="Literal">&lt;p&gt;Action models, which take the form of precondition/effect axioms, facilitate
causal and motivational connections between actions for AI agents. Action model
acquisition has been identified as a bottleneck in the application of planning
technology, especially within narrative planning. Acquiring action models from
narrative texts in an automated way is essential, but challenging because of
the inherent complexities of such texts. We present NaRuto, a system that
extracts structured events from narrative text and subsequently generates
planning-language-style action models based on predictions of commonsense event
relations, as well as textual contradictions and similarities, in an
unsupervised manner. Experimental results in classical narrative planning
domains show that NaRuto can generate action models of significantly better
quality than existing fully automated methods, and even on par with those of
semi-automated methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Leyang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Songtuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haslum_P/0/1/0/all/0/1&quot;&gt;Patrik Haslum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10252">
<title>A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks. (arXiv:2307.10252v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10252</link>
<description rdf:parseType="Literal">&lt;p&gt;Cyber threat attribution is the process of identifying the actor of an attack
incident in cyberspace. An accurate and timely threat attribution plays an
important role in deterring future attacks by applying appropriate and timely
defense mechanisms. Manual analysis of attack patterns gathered by honeypot
deployments, intrusion detection systems, firewalls, and via trace-back
procedures is still the preferred method of security analysts for cyber threat
attribution. Such attack patterns are low-level Indicators of Compromise (IOC).
They represent Tactics, Techniques, Procedures (TTP), and software tools used
by the adversaries in their campaigns. The adversaries rarely re-use them. They
can also be manipulated, resulting in false and unfair attribution. To
empirically evaluate and compare the effectiveness of both kinds of IOC, there
are two problems that need to be addressed. The first problem is that in recent
research works, the ineffectiveness of low-level IOC for cyber threat
attribution has been discussed intuitively. An empirical evaluation for the
measure of the effectiveness of low-level IOC based on a real-world dataset is
missing. The second problem is that the available dataset for high-level IOC
has a single instance for each predictive class label that cannot be used
directly for training machine learning models. To address these problems in
this research work, we empirically evaluate the effectiveness of low-level IOC
based on a real-world dataset that is specifically built for comparative
analysis with high-level IOC. The experimental results show that the high-level
IOC trained models effectively attribute cyberattacks with an accuracy of 95%
as compared to the low-level IOC trained models where accuracy is 40%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noor_U/0/1/0/all/0/1&quot;&gt;Umara Noor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahid_S/0/1/0/all/0/1&quot;&gt;Sawera Shahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanwal_R/0/1/0/all/0/1&quot;&gt;Rimsha Kanwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_Z/0/1/0/all/0/1&quot;&gt;Zahid Rashid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10253">
<title>Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10253</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-core drilling has gradually become the primary exploration method in
geological engineering, and well logging curves have increasingly gained
importance as the main carriers of geological information. However, factors
such as geological environment, logging equipment, borehole quality, and
unexpected events can all impact the quality of well logging curves. Previous
methods of re-logging or manual corrections have been associated with high
costs and low efficiency. This paper proposes a machine learning method that
utilizes existing data to predict missing well logging curves, and its
effectiveness and feasibility have been validated through experiments. The
proposed method builds upon the traditional Long Short-Term Memory (LSTM)
neural network by incorporating a self-attention mechanism to analyze the
spatial dependencies of the data. It selectively includes the dominant
computational results in the LSTM, reducing the computational complexity from
O(n^2) to O(nlogn) and improving model efficiency. Experimental results
demonstrate that the proposed method achieves higher accuracy compared to
traditional curve synthesis methods based on Fully Connected Neural Networks
(FCNN) and LSTM. This accurate, efficient, and cost-effective prediction method
holds practical value in engineering applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuankai Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huanyu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10256">
<title>Hidden Markov Models with Random Restarts vs Boosting for Malware Detection. (arXiv:2307.10256v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10256</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective and efficient malware detection is at the forefront of research
into building secure digital systems. As with many other fields, malware
detection research has seen a dramatic increase in the application of machine
learning algorithms. One machine learning technique that has been used widely
in the field of pattern matching in general-and malware detection in
particular-is hidden Markov models (HMMs). HMM training is based on a hill
climb, and hence we can often improve a model by training multiple times with
different initial values. In this research, we compare boosted HMMs (using
AdaBoost) to HMMs trained with multiple random restarts, in the context of
malware detection. These techniques are applied to a variety of challenging
malware datasets. We find that random restarts perform surprisingly well in
comparison to boosting. Only in the most difficult &quot;cold start&quot; cases (where
training data is severely limited) does boosting appear to offer sufficient
improvement to justify its higher computational cost in the scoring phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1&quot;&gt;Aditya Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1&quot;&gt;Fabio Di Troia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1&quot;&gt;Mark Stamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10260">
<title>Student Assessment in Cybersecurity Training Automated by Pattern Mining and Clustering. (arXiv:2307.10260v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10260</link>
<description rdf:parseType="Literal">&lt;p&gt;Hands-on cybersecurity training allows students and professionals to practice
various tools and improve their technical skills. The training occurs in an
interactive learning environment that enables completing sophisticated tasks in
full-fledged operating systems, networks, and applications. During the
training, the learning environment allows collecting data about trainees&apos;
interactions with the environment, such as their usage of command-line tools.
These data contain patterns indicative of trainees&apos; learning processes, and
revealing them allows to assess the trainees and provide feedback to help them
learn. However, automated analysis of these data is challenging. The training
tasks feature complex problem-solving, and many different solution approaches
are possible. Moreover, the trainees generate vast amounts of interaction data.
This paper explores a dataset from 18 cybersecurity training sessions using
data mining and machine learning techniques. We employed pattern mining and
clustering to analyze 8834 commands collected from 113 trainees, revealing
their typical behavior, mistakes, solution strategies, and difficult training
stages. Pattern mining proved suitable in capturing timing information and tool
usage frequency. Clustering underlined that many trainees often face the same
issues, which can be addressed by targeted scaffolding. Our results show that
data mining methods are suitable for analyzing cybersecurity training data.
Educational researchers and practitioners can apply these methods in their
contexts to assess trainees, support them, and improve the training design.
Artifacts associated with this research are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svabensky_V/0/1/0/all/0/1&quot;&gt;Valdemar &amp;#x160;v&amp;#xe1;bensk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vykopal_J/0/1/0/all/0/1&quot;&gt;Jan Vykopal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celeda_P/0/1/0/all/0/1&quot;&gt;Pavel &amp;#x10c;eleda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tkacik_K/0/1/0/all/0/1&quot;&gt;Kristi&amp;#xe1;n Tk&amp;#xe1;&amp;#x10d;ik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovic_D/0/1/0/all/0/1&quot;&gt;Daniel Popovi&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10262">
<title>Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython. (arXiv:2307.10262v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10262</link>
<description rdf:parseType="Literal">&lt;p&gt;This document provides a comprehensive guide to hyperparameter tuning using
spotPython for scikit-learn, PyTorch, and river. The first part introduces
spotPython&apos;s surrogate model-based optimization process, while the second part
focuses on hyperparameter tuning. Several case studies are presented, including
hyperparameter tuning for sklearn models such as Support Vector Classification,
Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well
as a Hoeffding Adaptive Tree Regressor from river. The integration of
spotPython into the PyTorch and PyTorch Lightning training workflow is also
discussed. With a hands-on approach and step-by-step explanations, this
cookbook serves as a practical starting point for anyone interested in
hyperparameter tuning with Python. Highlights include the interplay between
Tensorboard, PyTorch Lightning, spotPython, and river. This publication is
under development, with updates available on the corresponding webpage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1&quot;&gt;Thomas Bartz-Beielstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10266">
<title>A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10266</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) have emerged as an effective approach to tackling
real-world problems. However, like human-written software,
automatically-generated DNNs can have bugs and be attacked. This thus attracts
many recent interests in developing effective and scalable DNN verification
techniques and tools. In this work, we introduce a NeuralSAT, a new constraint
solving approach to DNN verification. The design of NeuralSAT follows the
DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause
learning, abstraction, and theory solving, and thus NeuralSAT can be considered
as an SMT framework for DNNs. Preliminary results show that the NeuralSAT
prototype is competitive to the state-of-the-art. We hope, with proper
optimization and engineering, NeuralSAT will carry the power and success of
modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from:
https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_H/0/1/0/all/0/1&quot;&gt;Hai Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linhan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;ThanhVu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwyer_M/0/1/0/all/0/1&quot;&gt;Matthew Dwyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10274">
<title>Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.10274</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a method to create domain-sensitive speech
recognition models that utilize textual domain information by conditioning its
generation on a given text prompt. This is accomplished by fine-tuning a
pre-trained, end-to-end model (Whisper) to learn from demonstrations with
prompt examples. We show that this ability can be generalized to different
domains and even various prompt contexts, with our model gaining a Word Error
Rate (WER) reduction of up to 33% on unseen datasets from various domains, such
as medical conversation, air traffic control communication, and financial
meetings. Considering the limited availability of audio-transcript pair data,
we further extend our method to text-only fine-tuning to achieve domain
sensitivity as well as domain adaptation. We demonstrate that our text-only
fine-tuned model can also attend to various prompt contexts, with the model
reaching the most WER reduction of 29% on the medical conversation dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_F/0/1/0/all/0/1&quot;&gt;Feng-Ting Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chan_Y/0/1/0/all/0/1&quot;&gt;Yung-Chieh Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Chang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chan-Jan Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shiu_D/0/1/0/all/0/1&quot;&gt;Da-shan Shiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10284">
<title>ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10284</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present ECSIC, a novel learned method for stereo image
compression. Our proposed method compresses the left and right images in a
joint manner by exploiting the mutual information between the images of the
stereo image pair using a novel stereo cross attention (SCA) module and two
stereo context modules. The SCA module performs cross-attention restricted to
the corresponding epipolar lines of the two images and processes them in
parallel. The stereo context modules improve the entropy estimation of the
second encoded image by using the first image as a context. We conduct an
extensive ablation study demonstrating the effectiveness of the proposed
modules and a comprehensive quantitative and qualitative comparison with
existing methods. ECSIC achieves state-of-the-art performance among stereo
image compression models on the two popular stereo image datasets Cityscapes
and InStereo2k while allowing for fast encoding and decoding, making it highly
practical for real-time applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wodlinger_M/0/1/0/all/0/1&quot;&gt;Matthias W&amp;#xf6;dlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kotera_J/0/1/0/all/0/1&quot;&gt;Jan Kotera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Keglevic_M/0/1/0/all/0/1&quot;&gt;Manuel Keglevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sablatnig_R/0/1/0/all/0/1&quot;&gt;Robert Sablatnig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10296">
<title>Towards Automated Semantic Segmentation in Mammography Images. (arXiv:2307.10296v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10296</link>
<description rdf:parseType="Literal">&lt;p&gt;Mammography images are widely used to detect non-palpable breast lesions or
nodules, preventing cancer and providing the opportunity to plan interventions
when necessary. The identification of some structures of interest is essential
to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection
systems can be helpful in assisting medical interpretation by automatically
segmenting these landmark structures. In this paper, we propose a deep
learning-based framework for the segmentation of the nipple, the pectoral
muscle, the fibroglandular tissue, and the fatty tissue on standard-view
mammography images. We introduce a large private segmentation dataset and
extensive experiments considering different deep-learning model architectures.
Our experiments demonstrate accurate segmentation performance on variate and
challenging cases, showing that this framework can be integrated into clinical
practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sierra_Franco_C/0/1/0/all/0/1&quot;&gt;Cesar A. Sierra-Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hurtado_J/0/1/0/all/0/1&quot;&gt;Jan Hurtado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thomaz_V/0/1/0/all/0/1&quot;&gt;Victor de A. Thomaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Leonardo C. da Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Silva_S/0/1/0/all/0/1&quot;&gt;Santiago V. Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raposo_A/0/1/0/all/0/1&quot;&gt;Alberto B. Raposo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10299">
<title>Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.10299</link>
<description rdf:parseType="Literal">&lt;p&gt;Since distribution shifts are common in real-world applications, there is a
pressing need for developing prediction models that are robust against such
shifts. Existing frameworks, such as empirical risk minimization or
distributionally robust optimization, either lack generalizability for unseen
distributions or rely on postulated distance measures. Alternatively, causality
offers a data-driven and structural perspective to robust predictions. However,
the assumptions necessary for causal inference can be overly stringent, and the
robustness offered by such causal models often lacks flexibility. In this
paper, we focus on causality-oriented robustness and propose Distributional
Robustness via Invariant Gradients (DRIG), a method that exploits general
additive interventions in training data for robust predictions against unseen
interventions, and naturally interpolates between in-distribution prediction
and causality. In a linear setting, we prove that DRIG yields predictions that
are robust among a data-dependent class of distribution shifts. Furthermore, we
show that our framework includes anchor regression (Rothenh\&quot;ausler et al.\
2021) as a special case, and that it yields prediction models that protect
against more diverse perturbations. We extend our approach to the
semi-supervised domain adaptation setting to further improve prediction
performance. Finally, we empirically validate our methods on synthetic
simulations and on single-cell data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xinwei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buhlmann_P/0/1/0/all/0/1&quot;&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taeb_A/0/1/0/all/0/1&quot;&gt;Armeen Taeb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10303">
<title>Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10303</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we carefully investigate how we can use multiple different
Natural Language Processing techniques and methods in order to automatically
recognize the main actions in sports events. We aim to extract insights by
analyzing live sport commentaries from different sources and by classifying
these major actions into different categories. We also study if sentiment
analysis could help detect these main actions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miraoui_Y/0/1/0/all/0/1&quot;&gt;Yanis Miraoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10304">
<title>Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. (arXiv:2307.10304v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.10304</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Polyffusion, a diffusion model that generates polyphonic music
scores by regarding music as image-like piano roll representations. The model
is capable of controllable music generation with two paradigms: internal
control and external control. Internal control refers to the process in which
users pre-define a part of the music and then let the model infill the rest,
similar to the task of masked music generation (or music inpainting). External
control conditions the model with external yet related information, such as
chord, texture, or other features, via the cross-attention mechanism. We show
that by using internal and external controls, Polyffusion unifies a wide range
of music creation tasks, including melody generation given accompaniment,
accompaniment generation given melody, arbitrary music segment inpainting, and
music arrangement given chords or textures. Experimental results show that our
model significantly outperforms existing Transformer and sampling-based
baselines, and using pre-trained disentangled representations as external
conditions yields more effective controls.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_L/0/1/0/all/0/1&quot;&gt;Lejun Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junyan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gus Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10305">
<title>Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows. (arXiv:2307.10305v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10305</link>
<description rdf:parseType="Literal">&lt;p&gt;Human beings always engage in a vast range of activities and tasks that
demonstrate their ability to adapt to different scenarios. Any human activity
can be represented as a temporal sequence of actions performed to achieve a
certain goal. Unlike the time series datasets extracted from electronics or
machines, these action sequences are highly disparate in their nature -- the
time to finish a sequence of actions can vary between different persons.
Therefore, understanding the dynamics of these sequences is essential for many
downstream tasks such as activity length prediction, goal prediction, next
action recommendation, etc. Existing neural network-based approaches that learn
a continuous-time activity sequence (or CTAS) are limited to the presence of
only visual data or are designed specifically for a particular task, i.e.,
limited to next action or goal prediction. In this paper, we present ProActive,
a neural marked temporal point process (MTPP) framework for modeling the
continuous-time distribution of actions in an activity sequence while
simultaneously addressing three high-impact problems -- next action prediction,
sequence-goal prediction, and end-to-end sequence generation. Specifically, we
utilize a self-attention module with temporal normalizing flows to model the
influence and the inter-arrival times between actions in a sequence. In
addition, we propose a novel addition over the ProActive model that can handle
variations in the order of actions, i.e., different methods of achieving a
given goal. We demonstrate that this variant can learn the order in which the
person or actor prefers to do their actions. Extensive experiments on sequences
derived from three activity recognition datasets show the significant accuracy
boost of ProActive over the state-of-the-art in terms of action and goal
prediction, and the first-ever application of end-to-end action sequence
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vinayak Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1&quot;&gt;Srikanta Bedathur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10314">
<title>Mood Classification of Bangla Songs Based on Lyrics. (arXiv:2307.10314v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.10314</link>
<description rdf:parseType="Literal">&lt;p&gt;Music can evoke various emotions, and with the advancement of technology, it
has become more accessible to people. Bangla music, which portrays different
human emotions, lacks sufficient research. The authors of this article aim to
analyze Bangla songs and classify their moods based on the lyrics. To achieve
this, this research has compiled a dataset of 4000 Bangla song lyrics, genres,
and used Natural Language Processing and the Bert Algorithm to analyze the
data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362
for the romantic mood, 886 for happiness, and the rest 239 are classified as
relaxation. By embedding the lyrics of the songs, the authors have classified
the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is
crucial as it enables a multi-class classification of songs&apos; moods, making the
music more relatable to people&apos;s emotions. The article presents the automated
result of the four moods accurately derived from the song lyrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajebin_M/0/1/0/all/0/1&quot;&gt;Maliha Mahajebin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1&quot;&gt;Mohammad Rifat Ahmmad Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_N/0/1/0/all/0/1&quot;&gt;Nafees Mansoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10317">
<title>FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10317</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) offers a collaborative training framework, allowing
multiple clients to contribute to a shared model without compromising data
privacy. Due to the heterogeneous nature of local datasets, updated client
models may overfit and diverge from one another, commonly known as the problem
of client drift. In this paper, we propose FedBug (Federated Learning with
Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively
mitigate client drift. FedBug adaptively leverages the client model parameters,
distributed by the server at each global round, as the reference points for
cross-client alignment. Specifically, on the client side, FedBug begins by
freezing the entire model, then gradually unfreezes the layers, from the input
layer to the output layer. This bottom-up approach allows models to train the
newly thawed layers to project data into a latent space, wherein the separating
hyperplanes remain consistent across all clients. We theoretically analyze
FedBug in a novel over-parameterization FL setup, revealing its superior
convergence rate compared to FedAvg. Through comprehensive experiments,
spanning various datasets, training conditions, and network architectures, we
validate the efficacy of FedBug. Our contributions encompass a novel FL
framework, theoretical analysis, and empirical validation, demonstrating the
wide potential and applicability of FedBug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1&quot;&gt;Chia-Hsiang Kao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10318">
<title>Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10318</link>
<description rdf:parseType="Literal">&lt;p&gt;Vertical federated learning (VFL) enables multiple parties with disjoint
features of a common user set to train a machine learning model without sharing
their private data. Tree-based models have become prevalent in VFL due to their
interpretability and efficiency. However, the vulnerability of tree-based VFL
has not been sufficiently investigated. In this study, we first introduce a
novel label inference attack, ID2Graph, which utilizes the sets of record-IDs
assigned to each node (i.e., instance space) to deduce private training labels.
The ID2Graph attack generates a graph structure from training samples, extracts
communities from the graph, and clusters the local dataset using community
information. To counteract label leakage from the instance space, we propose an
effective defense mechanism, ID-LMID, which prevents label leakage by focusing
on mutual information regularization. Comprehensive experiments conducted on
various datasets reveal that the ID2Graph attack presents significant risks to
tree-based models such as Random Forest and XGBoost. Further evaluations on
these benchmarks demonstrate that ID-LMID effectively mitigates label leakage
in such instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1&quot;&gt;Hideaki Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10320">
<title>Reproducibility in Machine Learning-Driven Research. (arXiv:2307.10320v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10320</link>
<description rdf:parseType="Literal">&lt;p&gt;Research is facing a reproducibility crisis, in which the results and
findings of many studies are difficult or even impossible to reproduce. This is
also the case in machine learning (ML) and artificial intelligence (AI)
research. Often, this is the case due to unpublished data and/or source-code,
and due to sensitivity to ML training conditions. Although different solutions
to address this issue are discussed in the research community such as using ML
platforms, the level of reproducibility in ML-driven research is not increasing
substantially. Therefore, in this mini survey, we review the literature on
reproducibility in ML-driven research with three main aims: (i) reflect on the
current situation of ML reproducibility in various research fields, (ii)
identify reproducibility issues and barriers that exist in these research
fields applying ML, and (iii) identify potential drivers such as tools,
practices, and interventions that support ML reproducibility. With this, we
hope to contribute to decisions on the viability of different solutions for
supporting ML reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semmelrock_H/0/1/0/all/0/1&quot;&gt;Harald Semmelrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopeinik_S/0/1/0/all/0/1&quot;&gt;Simone Kopeinik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theiler_D/0/1/0/all/0/1&quot;&gt;Dieter Theiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_Hellauer_T/0/1/0/all/0/1&quot;&gt;Tony Ross-Hellauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowald_D/0/1/0/all/0/1&quot;&gt;Dominik Kowald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10323">
<title>IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.10323</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentiable Search Index is a recently proposed paradigm for document
retrieval, that encodes information about a corpus of documents within the
parameters of a neural network and directly maps queries to corresponding
documents. These models have achieved state-of-the-art performances for
document retrieval across many benchmarks. These kinds of models have a
significant limitation: it is not easy to add new documents after a model is
trained. We propose IncDSI, a method to add documents in real time (about
20-50ms per document), without retraining the model on the entire dataset (or
even parts thereof). Instead we formulate the addition of documents as a
constrained optimization problem that makes minimal changes to the network
parameters. Although orders of magnitude faster, our approach is competitive
with re-training the model on the whole dataset and enables the development of
document retrieval systems that can be updated with new information in
real-time. Our code for IncDSI is available at
https://github.com/varshakishore/IncDSI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kishore_V/0/1/0/all/0/1&quot;&gt;Varsha Kishore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1&quot;&gt;Chao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovelace_J/0/1/0/all/0/1&quot;&gt;Justin Lovelace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1&quot;&gt;Yoav Artzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Q. Weinberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10343">
<title>ProtiGeno: a prokaryotic short gene finder using protein language models. (arXiv:2307.10343v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2307.10343</link>
<description rdf:parseType="Literal">&lt;p&gt;Prokaryotic gene prediction plays an important role in understanding the
biology of organisms and their function with applications in medicine and
biotechnology. Although the current gene finders are highly sensitive in
finding long genes, their sensitivity decreases noticeably in finding shorter
genes (&amp;lt;180 nts). The culprit is insufficient annotated gene data to identify
distinguishing features in short open reading frames (ORFs). We develop a deep
learning-based method called ProtiGeno, specifically targeting short
prokaryotic genes using a protein language model trained on millions of evolved
proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes,
we demonstrate that ProtiGeno predicts short coding and noncoding genes with
higher accuracy and recall than the current state-of-the-art gene finders. We
discuss the predictive features of ProtiGeno and possible limitations by
visualizing the three-dimensional structure of the predicted short genes. Data,
codes, and models are available at https://github.com/tonytu16/protigeno.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tony Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Krishna_G/0/1/0/all/0/1&quot;&gt;Gautham Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aghazadeh_A/0/1/0/all/0/1&quot;&gt;Amirali Aghazadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10348">
<title>Code Detection for Hardware Acceleration Using Large Language Models. (arXiv:2307.10348v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.10348</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have been massively applied to many tasks, often
surpassing state-of-the-art approaches. While their effectiveness in code
generation has been extensively studied (e.g., AlphaCode), their potential for
code detection remains unexplored.
&lt;/p&gt;
&lt;p&gt;This work presents the first analysis of code detection using LLMs. Our study
examines essential kernels, including matrix multiplication, convolution, and
fast-fourier transform, implemented in C/C++. We propose both a preliminary,
naive prompt and a novel prompting strategy for code detection.
&lt;/p&gt;
&lt;p&gt;Results reveal that conventional prompting achieves great precision but poor
accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)
due to a high number of false positives. Our novel prompting strategy
substantially reduces false positives, resulting in excellent overall accuracy
(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable
challenge to existing state-of-the-art code detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_P/0/1/0/all/0/1&quot;&gt;Pablo Antonio Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernabe_G/0/1/0/all/0/1&quot;&gt;Gregorio Bernab&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Manuel Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10350">
<title>Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10350</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp&apos;s large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10352">
<title>Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.10352</link>
<description rdf:parseType="Literal">&lt;p&gt;The Sliced Wasserstein (SW) distance has become a popular alternative to the
Wasserstein distance for comparing probability measures. Widespread
applications include image processing, domain adaptation and generative
modelling, where it is common to optimise some parameters in order to minimise
SW, which serves as a loss function between discrete probability measures
(since measures admitting densities are numerically unattainable). All these
optimisation problems bear the same sub-problem, which is minimising the Sliced
Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y
\longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between
two uniform discrete measures with the same amount of points as a function of
the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We
investigate the regularity and optimisation properties of this energy, as well
as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in
SW using only $p$ samples) and show convergence results on the critical points
of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform
convergence. Finally, we show that in a certain sense, Stochastic Gradient
Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards
(Clarke) critical points of these energies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanguy_E/0/1/0/all/0/1&quot;&gt;Eloi Tanguy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Flamary_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Delon_J/0/1/0/all/0/1&quot;&gt;Julie Delon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10355">
<title>Selection functions of strong lens finding neural networks. (arXiv:2307.10355v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/2307.10355</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolution Neural Networks trained for the task of lens finding with similar
architecture and training data as is commonly found in the literature are
biased classifiers. An understanding of the selection function of lens finding
neural networks will be key to fully realising the potential of the large
samples of strong gravitational lens systems that will be found in upcoming
wide-field surveys. We use three training datasets, representative of those
used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The
networks preferentially select systems with larger Einstein radii and larger
sources with more concentrated source-light distributions. Increasing the
detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per
cent of the selected strong lens systems having Einstein radii
$\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879
arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and
source S\&apos;ersic indices $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.62 from
$n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.55. The model trained to find lensed
quasars shows a stronger preference for higher lens ellipticities than those
trained to find lensed galaxies. The selection function is independent of the
slope of the power-law of the mass profiles, hence measurements of this
quantity will be unaffected. The lens finder selection function reinforces that
of the lensing cross-section, and thus we expect our findings to be a general
result for all galaxy-galaxy and galaxy-quasar lens finding neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Herle_A/0/1/0/all/0/1&quot;&gt;A. Herle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+ORiordan_C/0/1/0/all/0/1&quot;&gt;C. M. O&amp;#x27;Riordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vegetti_S/0/1/0/all/0/1&quot;&gt;S. Vegetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10404">
<title>Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10404</link>
<description rdf:parseType="Literal">&lt;p&gt;Part-prototype models are explainable-by-design image classifiers, and a
promising alternative to black box AI. This paper explores the applicability
and potential of interpretable machine learning, in particular PIP-Net, for
automated diagnosis support on real-world medical imaging data. PIP-Net learns
human-understandable prototypical image parts and we evaluate its accuracy and
interpretability for fracture detection and skin cancer diagnosis. We find that
PIP-Net&apos;s decision making process is in line with medical classification
standards, while only provided with image-level class labels. Because of
PIP-Net&apos;s unsupervised pretraining of prototypes, data quality problems such as
undesired text in an X-ray or labelling errors can be easily identified.
Additionally, we are the first to show that humans can manually correct the
reasoning of PIP-Net by directly disabling undesired prototypes. We conclude
that part-prototype models are promising for medical applications due to their
interpretability and potential for advanced model debugging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nauta_M/0/1/0/all/0/1&quot;&gt;Meike Nauta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegeman_J/0/1/0/all/0/1&quot;&gt;Johannes H. Hegeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geerdink_J/0/1/0/all/0/1&quot;&gt;Jeroen Geerdink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlotterer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Schl&amp;#xf6;tterer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1&quot;&gt;Maurice van Keulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10422">
<title>PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10422</link>
<description rdf:parseType="Literal">&lt;p&gt;Earth system forecasting has traditionally relied on complex physical models
that are computationally expensive and require significant domain expertise. In
the past decade, the unprecedented increase in spatiotemporal Earth observation
data has enabled data-driven forecasting models using deep learning techniques.
These models have shown promise for diverse Earth system forecasting tasks but
either struggle with handling uncertainty or neglect domain-specific prior
knowledge, resulting in averaging possible futures to blurred forecasts or
generating physically implausible predictions. To address these limitations, we
propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)
We develop PreDiff, a conditional latent diffusion model capable of
probabilistic forecasts. 2) We incorporate an explicit knowledge control
mechanism to align forecasts with domain-specific physical constraints. This is
achieved by estimating the deviation from imposed constraints at each denoising
step and adjusting the transition distribution accordingly. We conduct
empirical studies on two datasets: N-body MNIST, a synthetic dataset with
chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.
Specifically, we impose the law of conservation of energy in N-body MNIST and
anticipated precipitation intensity in SEVIR. Experiments demonstrate the
effectiveness of PreDiff in handling uncertainty, incorporating domain-specific
prior knowledge, and generating forecasts that exhibit high operational
utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhihan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xingjian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Boran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1&quot;&gt;Danielle Maddix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10430">
<title>DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation. (arXiv:2307.10430v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10430</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of synthetic tabular data that preserves differential privacy
is a problem of growing importance. While traditional marginal-based methods
have achieved impressive results, recent work has shown that deep
learning-based approaches tend to lag behind. In this work, we present
Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a
transformer-based autoregressive model that maintains differential privacy and
achieves performance competitive with marginal-based methods on a wide variety
of datasets, capable of even outperforming state-of-the-art methods in certain
settings. We also provide a theoretical framework for understanding the
limitations of marginal-based approaches and where deep learning-based
approaches stand to contribute most. These results suggest that deep
learning-based techniques should be considered as a viable alternative to
marginal-based methods in the generation of differentially private synthetic
tabular data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1&quot;&gt;Rodrigo Castellon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopal_A/0/1/0/all/0/1&quot;&gt;Achintya Gopal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloniarz_B/0/1/0/all/0/1&quot;&gt;Brian Bloniarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1&quot;&gt;David Rosenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10434">
<title>Learning Formal Specifications from Membership and Preference Queries. (arXiv:2307.10434v1 [cs.FL])</title>
<link>http://arxiv.org/abs/2307.10434</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning is a well-studied approach to learning formal specifications,
such as automata. In this work, we extend active specification learning by
proposing a novel framework that strategically requests a combination of
membership labels and pair-wise preferences, a popular alternative to
membership labels. The combination of pair-wise preferences and membership
labels allows for a more flexible approach to active specification learning,
which previously relied on membership labels only. We instantiate our framework
in two different domains, demonstrating the generality of our approach. Our
results suggest that learning from both modalities allows us to robustly and
conveniently identify specifications via membership and preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ameesh Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_Chanlatte_M/0/1/0/all/0/1&quot;&gt;Marcell Vazquez-Chanlatte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junges_S/0/1/0/all/0/1&quot;&gt;Sebastian Junges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1&quot;&gt;Sanjit A. Seshia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10436">
<title>A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks. (arXiv:2307.10436v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.10436</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learners (DLs) are the state-of-art predictive mechanism with
applications in many fields requiring complex high dimensional data processing.
Although conventional DLs get trained via gradient descent with
back-propagation, Kalman Filter (KF)-based techniques that do not need gradient
computation have been developed to approximate DLs. We propose a multi-arm
extension of a KF-based DL approximator that can mimic DL when the sample size
is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman
Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking
that becomes relevant when the training sample has an unequal-size feature set.
Our proposed technique can approximate Long Short-term Memory (LSTM) Networks
and attach uncertainty to the predictions obtained from these LSTMs with
desirable coverage. We demonstrate how MEnKF-ANN can &quot;adequately&quot; approximate
an LSTM network trained to classify what carbohydrate substrates are digested
and utilized by a microbiome sample whose genomic sequences consist of
polysaccharide utilization loci (PULs) and their encoded genes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Piyush_V/0/1/0/all/0/1&quot;&gt;Ved Piyush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuzhen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yanbin Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Souparno Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10437">
<title>A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data. (arXiv:2307.10437v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10437</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic simulation software is used by transportation researchers and
engineers to design and evaluate changes to roadways. These simulators are
driven by models of microscopic driver behavior from which macroscopic measures
like flow and congestion can be derived. Many models are designed for a subset
of possible traffic scenarios and roadway configurations, while others have no
explicit constraints on their application. Work zones (WZs) are one scenario
for which no model to date has reproduced realistic driving behavior. This
makes it difficult to optimize for safety and other metrics when designing a
WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to
develop a car-following (CF) model for use in microscopic simulators that can
capture and reproduce driver behavior accurately within and outside of WZs.
Volpe also performed a naturalistic driving study to collect telematics data
from vehicles driven on roads with WZs for use in model calibration. During
model development, Volpe researchers observed difficulties in calibrating their
model, leaving them to question whether there existed flaws in their model, in
the data, or in the procedure used to calibrate the model using the data. In
this thesis, I use Bayesian methods for data analysis and parameter estimation
to explore and, where possible, address these questions. First, I use Bayesian
inference to measure the sufficiency of the size of the data set. Second, I
compare the procedure and results of the genetic algorithm based calibration
performed by the Volpe researchers with those of Bayesian calibration. Third, I
explore the benefits of modeling CF hierarchically. Finally, I apply what was
learned in the first three phases using an established CF model, Wiedemann 99,
to the probabilistic modeling of the Volpe model. Validation is performed using
information criteria as an estimate of predictive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abodo_F/0/1/0/all/0/1&quot;&gt;Franklin Abodo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10438">
<title>Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10438</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven
methods for molecular property prediction. However, a key limitation of typical
GNN models is their inability to quantify uncertainties in the predictions.
This capability is crucial for ensuring the trustworthy use and deployment of
models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated
uncertainty quantification (UQ) approach for molecular property prediction.
AutoGNNUQ leverages architecture search to generate an ensemble of
high-performing GNNs, enabling the estimation of predictive uncertainties. Our
approach employs variance decomposition to separate data (aleatoric) and model
(epistemic) uncertainties, providing valuable insights for reducing them. In
our computational experiments, we demonstrate that AutoGNNUQ outperforms
existing UQ methods in terms of both prediction accuracy and UQ performance on
multiple benchmark datasets. Additionally, we utilize t-SNE visualization to
explore correlations between molecular features and uncertainty, offering
insight for dataset improvement. AutoGNNUQ has broad applicability in domains
such as drug discovery and materials science, where accurate uncertainty
quantification is crucial for decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shengli Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Shiyi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehn_R/0/1/0/all/0/1&quot;&gt;Reid C. Van Lehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaprakash_P/0/1/0/all/0/1&quot;&gt;Prasanna Balaprakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavala_V/0/1/0/all/0/1&quot;&gt;Victor M. Zavala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10440">
<title>Confidence Estimation Using Unlabeled Data. (arXiv:2307.10440v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10440</link>
<description rdf:parseType="Literal">&lt;p&gt;Overconfidence is a common issue for deep neural networks, limiting their
deployment in real-world applications. To better estimate confidence, existing
methods mostly focus on fully-supervised scenarios and rely on training labels.
In this paper, we propose the first confidence estimation method for a
semi-supervised setting, when most training labels are unavailable. We
stipulate that even with limited training labels, we can still reasonably
approximate the confidence of model on unlabeled samples by inspecting the
prediction consistency through the training process. We use training
consistency as a surrogate function and propose a consistency ranking loss for
confidence estimation. On both image classification and segmentation tasks, our
method achieves state-of-the-art performances in confidence estimation.
Furthermore, we show the benefit of the proposed method through a downstream
active learning task. The code is available at
https://github.com/TopoXLab/consistency-ranking-loss
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaoling Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10443">
<title>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10443</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the significant progress made by transformer models in machine
reading comprehension tasks, they still face limitations in handling complex
reasoning tasks due to the absence of explicit knowledge in the input sequence.
This paper proposes a novel attention pattern to overcome this limitation,
which integrates reasoning knowledge derived from a heterogeneous graph into
the transformer architecture using a graph-enhanced self-attention mechanism.
The proposed attention pattern comprises three key elements: global-local
attention for word tokens, graph attention for entity tokens that exhibit
strong attention towards tokens connected in the graph as opposed to those
unconnected, and the consideration of the type of relationship between each
entity token and word token. This results in optimized attention between the
two if a relationship exists. The pattern is coupled with special relative
position labels, allowing it to integrate with LUKE&apos;s entity-aware
self-attention mechanism. The experimental findings corroborate that our model
outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the
ReCoRD dataset that focuses on commonsense reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foolad_S/0/1/0/all/0/1&quot;&gt;Shima Foolad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1&quot;&gt;Kourosh Kiani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10455">
<title>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10455</link>
<description rdf:parseType="Literal">&lt;p&gt;In an effort to catalog insect biodiversity, we propose a new large dataset
of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is
taxonomically classified by an expert, and also has associated genetic
information including raw nucleotide barcode sequences and assigned barcode
index numbers, which are genetically-based proxies for species classification.
This paper presents a curated million-image dataset, primarily to train
computer-vision models capable of providing image-based taxonomic assessment,
however, the dataset also presents compelling characteristics, the study of
which would be of interest to the broader machine learning community. Driven by
the biological nature inherent to the dataset, a characteristic long-tailed
class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is
a hierarchical classification scheme, presenting a highly fine-grained
classification problem at lower levels. Beyond spurring interest in
biodiversity research within the machine learning community, progress on
creating an image-based taxonomic classifier will also further the ultimate
goal of all BIOSCAN research: to lay the foundation for a comprehensive survey
of global biodiversity. This paper introduces the dataset and explores the
classification task through the implementation and analysis of a baseline
classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1&quot;&gt;Zahra Gharaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;ZeMing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1&quot;&gt;Nicholas Pellegrino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarubiieva_I/0/1/0/all/0/1&quot;&gt;Iuliia Zarubiieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1&quot;&gt;Joakim Bruslund Haurum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1&quot;&gt;Scott C. Lowe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKeown_J/0/1/0/all/0/1&quot;&gt;Jaclyn T.A. McKeown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1&quot;&gt;Chris C.Y. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McLeod_J/0/1/0/all/0/1&quot;&gt;Joschka McLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yi-Yun C Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agda_J/0/1/0/all/0/1&quot;&gt;Jireh Agda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratnasingham_S/0/1/0/all/0/1&quot;&gt;Sujeevan Ratnasingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinke_D/0/1/0/all/0/1&quot;&gt;Dirk Steinke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel X. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1&quot;&gt;Paul Fieguth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10459">
<title>A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10459</link>
<description rdf:parseType="Literal">&lt;p&gt;A new computationally simple method of imposing hard convex constraints on
the neural network output values is proposed. The key idea behind the method is
to map a vector of hidden parameters of the network to a point that is
guaranteed to be inside the feasible set defined by a set of constraints. The
mapping is implemented by the additional neural network layer with constraints
for output. The proposed method is simply extended to the case when constraints
are imposed not only on the output vectors, but also on joint constraints
depending on inputs. The projection approach to imposing constraints on outputs
can simply be implemented in the framework of the proposed method. It is shown
how to incorporate different types of constraints into the proposed method,
including linear and quadratic constraints, equality constraints, and dynamic
constraints, constraints in the form of boundaries. An important feature of the
method is its computational simplicity. Complexities of the forward pass of the
proposed neural network layer by linear and quadratic constraints are O(n*m)
and O(n^2*m), respectively, where n is the number of variables, m is the number
of constraints. Numerical experiments illustrate the method by solving
optimization and classification problems. The code implementing the method is
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantinov_A/0/1/0/all/0/1&quot;&gt;Andrei V. Konstantinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utkin_L/0/1/0/all/0/1&quot;&gt;Lev V. Utkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10460">
<title>A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.10460</link>
<description rdf:parseType="Literal">&lt;p&gt;Data science is not a science. It is a research paradigm with an unfathomed
scope, scale, complexity, and power for knowledge discovery that is not
otherwise possible and can be beyond human reasoning. It is changing our world
practically and profoundly already widely deployed in tens of thousands of
applications in every discipline in an AI Arms Race that, due to its
inscrutability, can lead to unfathomed risks. This paper presents an axiology
of data science, its purpose, nature, importance, risks, and value for problem
solving, by exploring and evaluating its remarkable, definitive features. As
data science is in its infancy, this initial, speculative axiology is intended
to aid in understanding and defining data science to recognize its potential
benefits, risks, and open research challenges. AI based data science is
inherently about uncertainty that may be more realistic than our preference for
the certainty of science. Data science will have impacts far beyond knowledge
discovery and will take us into new ways of understanding the world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brodie_M/0/1/0/all/0/1&quot;&gt;Michael L. Brodie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10471">
<title>Classification of Visualization Types and Perspectives in Patents. (arXiv:2307.10471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10471</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the swift growth of patent applications each year, information and
multimedia retrieval approaches that facilitate patent exploration and
retrieval are of utmost importance. Different types of visualizations (e.g.,
graphs, technical drawings) and perspectives (e.g., side view, perspective) are
used to visualize details of innovations in patents. The classification of
these images enables a more efficient search and allows for further analysis.
So far, datasets for image type classification miss some important
visualization types for patents. Furthermore, related work does not make use of
recent deep learning approaches including transformers. In this paper, we adopt
state-of-the-art deep learning methods for the classification of visualization
types and perspectives in patent images. We extend the CLEF-IP dataset for
image type classification in patents to ten classes and provide manual ground
truth annotations. In addition, we derive a set of hierarchical classes from a
dataset that provides weakly-labeled data for image perspectives. Experimental
results have demonstrated the feasibility of the proposed approaches. Source
code, models, and dataset will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghauri_J/0/1/0/all/0/1&quot;&gt;Junaid Ahmed Ghauri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_Budack_E/0/1/0/all/0/1&quot;&gt;Eric M&amp;#xfc;ller-Budack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1&quot;&gt;Ralph Ewerth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10472">
<title>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10472</link>
<description rdf:parseType="Literal">&lt;p&gt;As the breadth and depth of language model applications continue to expand
rapidly, it is increasingly important to build efficient frameworks for
measuring and mitigating the learned or inherited social biases of these
models. In this paper, we present our work on evaluating instruction fine-tuned
language models&apos; ability to identify bias through zero-shot prompting,
including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction
fine-tuned versions, Alpaca 7B performs best on the bias identification task
with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and
data diversity could lead to further performance gain. This is a
work-in-progress presenting the first component of our bias mitigation
framework. We will keep updating this work as we get more results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dige_O/0/1/0/all/0/1&quot;&gt;Omkar Dige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jacob-Junqi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1&quot;&gt;David Emerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_F/0/1/0/all/0/1&quot;&gt;Faiza Khan Khattak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10485">
<title>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10485</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating human-like texts, which may potentially
revolutionize the finance industry. However, existing LLMs often fall short in
the financial field, which is mainly attributed to the disparities between
general text data and financial text data. Unfortunately, there is only a
limited number of financial text datasets available (quite small size), and
BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the
training logs were released). In light of this, we aim to democratize
Internet-scale financial data for LLMs, which is an open challenge due to
diverse data sources, low signal-to-noise ratio, and high time-validity. To
address the challenges, we introduce an open-sourced and data-centric
framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that
automates the collection and curation of real-time financial data from &amp;gt;34
diverse sources on the Internet, providing researchers and practitioners with
accessible and transparent resources to develop their FinLLMs. Additionally, we
propose a simple yet effective strategy for fine-tuning FinLLM using the
inherent feedback from the market, dubbed Reinforcement Learning with Stock
Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that
enables users to customize their own FinLLMs from open-source general-purpose
LLMs at a low cost. Finally, we showcase several FinGPT applications, including
robo-advisor, sentiment analysis for algorithmic trading, and low-code
development. FinGPT aims to democratize FinLLMs, stimulate innovation, and
unlock new opportunities in open finance. The codes are available at
https://github.com/AI4Finance-Foundation/FinGPT and
https://github.com/AI4Finance-Foundation/FinNLP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao-Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1&quot;&gt;Daochen Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10488">
<title>SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval. (arXiv:2307.10488v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.10488</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, sparse retrieval systems relied on lexical representations to
retrieve documents, such as BM25, dominated information retrieval tasks. With
the onset of pre-trained transformer models such as BERT, neural sparse
retrieval has led to a new paradigm within retrieval. Despite the success,
there has been limited software supporting different sparse retrievers running
in a unified, common environment. This hinders practitioners from fairly
comparing different sparse models and obtaining realistic evaluation results.
Another missing piece is, that a majority of prior work evaluates sparse
retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO.
However, a key requirement in practical retrieval systems requires models that
can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In
this work, we provide SPRINT, a unified Python toolkit based on Pyserini and
Lucene, supporting a common interface for evaluating neural sparse retrieval.
The toolkit currently includes five built-in models: uniCOIL, DeepImpact,
SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by
defining their term weighting method. Using our toolkit, we establish strong
and reproducible zero-shot sparse retrieval baselines across the
well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2
achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural
sparse retrievers. In this work, we further uncover the reasons behind its
performance gain. We show that SPLADEv2 produces sparse representations with a
majority of tokens outside of the original query and document which is often
crucial for its performance gains, i.e. a limitation among its other sparse
counterparts. We provide our SPRINT toolkit, models, and data used in our
experiments publicly here at https://github.com/thakur-nandan/sprint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1&quot;&gt;Nandan Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jimmy Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10490">
<title>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.10490</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate how images and sounds can be used for indirect prompt and
instruction injection in multi-modal LLMs. An attacker generates an adversarial
perturbation corresponding to the prompt and blends it into an image or audio
recording. When the user asks the (unmodified, benign) model about the
perturbed image or audio, the perturbation steers the model to output the
attacker-chosen text and/or make the subsequent dialog follow the attacker&apos;s
instruction. We illustrate this attack with several proof-of-concept examples
targeting LLaVa and PandaGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1&quot;&gt;Eugene Bagdasaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_T/0/1/0/all/0/1&quot;&gt;Tsung-Yin Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nassi_B/0/1/0/all/0/1&quot;&gt;Ben Nassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmatikov_V/0/1/0/all/0/1&quot;&gt;Vitaly Shmatikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10492">
<title>Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior. (arXiv:2307.10492v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10492</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing importance of data sharing for collaboration and
innovation, it is becoming more important to ensure that data is managed and
shared in a secure and trustworthy manner. Data governance is a common approach
to managing data, but it faces many challenges such as data silos, data
consistency, privacy, security, and access control. To address these
challenges, this paper proposes a comprehensive framework that integrates data
trust in federated learning with InterPlanetary File System, blockchain, and
smart contracts to facilitate secure and mutually beneficial data sharing while
providing incentives, access control mechanisms, and penalizing any dishonest
behavior. The experimental results demonstrate that the proposed model is
effective in improving the accuracy of federated learning models while ensuring
the security and fairness of the data-sharing process. The research paper also
presents a decentralized federated learning platform that successfully trained
a CNN model on the MNIST dataset using blockchain technology. The platform
enables multiple workers to train the model simultaneously while maintaining
data privacy and security. The decentralized architecture and use of blockchain
technology allow for efficient communication and coordination between workers.
This platform has the potential to facilitate decentralized machine learning
and support privacy-preserving collaboration in various domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaberzadeh_A/0/1/0/all/0/1&quot;&gt;Amir Jaberzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_A/0/1/0/all/0/1&quot;&gt;Ajay Kumar Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Faijan Ahamad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1&quot;&gt;Mohammed Afaan Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_B/0/1/0/all/0/1&quot;&gt;Bhargav Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1&quot;&gt;Jason Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10495">
<title>Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets. (arXiv:2307.10495v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10495</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning improves the performance of machine learning methods by
judiciously selecting a limited number of unlabeled data points to query for
labels, with the aim of maximally improving the underlying classifier&apos;s
performance. Recent gains have been made using sequential active learning for
synthetic aperture radar (SAR) data &lt;a href=&quot;/abs/2204.00005&quot;&gt;arXiv:2204.00005&lt;/a&gt;. In each iteration,
sequential active learning selects a query set of size one while batch active
learning selects a query set of multiple datapoints. While batch active
learning methods exhibit greater efficiency, the challenge lies in maintaining
model accuracy relative to sequential active learning methods. We developed a
novel, two-part approach for batch active learning: Dijkstra&apos;s Annulus Core-Set
(DAC) for core-set generation and LocalMax for batch sampling. The batch active
learning process that combines DAC and LocalMax achieves nearly identical
accuracy as sequential active learning but is more efficient, proportional to
the batch size. As an application, a pipeline is built based on transfer
learning feature embedding, graph learning, DAC, and LocalMax to classify the
FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the
state-of-the-art CNN-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapman_J/0/1/0/all/0/1&quot;&gt;James Chapman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calder_J/0/1/0/all/0/1&quot;&gt;Jeff Calder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_K/0/1/0/all/0/1&quot;&gt;Kevin Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertozzi_A/0/1/0/all/0/1&quot;&gt;Andrea L. Bertozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10496">
<title>A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes. (arXiv:2307.10496v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10496</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex systems in science and engineering sometimes exhibit behavior that
changes across different regimes. Traditional global models struggle to capture
the full range of this complex behavior, limiting their ability to accurately
represent the system. In response to this challenge, we propose a novel
competitive learning approach for obtaining data-driven models of physical
systems. The primary idea behind the proposed approach is to employ dynamic
loss functions for a set of models that are trained concurrently on the data.
Each model competes for each observation during training, allowing for the
identification of distinct functional regimes within the dataset. To
demonstrate the effectiveness of the learning approach, we coupled it with
various regression methods that employ gradient-based optimizers for training.
The proposed approach was tested on various problems involving model discovery
and function approximation, demonstrating its ability to successfully identify
functional regimes, discover true governing equations, and reduce test errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ukorigho_O/0/1/0/all/0/1&quot;&gt;Okezzi F. Ukorigho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owoyele_O/0/1/0/all/0/1&quot;&gt;Opeoluwa Owoyele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10504">
<title>Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10504</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Automatic Feature Explanation using Contrasting Concepts (FALCON),
an interpretability framework to explain features of image representations. For
a target feature, FALCON captions its highly activating cropped images using a
large captioning dataset (like LAION-400m) and a pre-trained vision-language
model like CLIP. Each word among the captions is scored and ranked leading to a
small number of shared, human-understandable concepts that closely describe the
target feature. FALCON also applies contrastive interpretation using lowly
activating (counterfactual) images, to eliminate spurious concepts. Although
many existing approaches interpret features independently, we observe in
state-of-the-art self-supervised and supervised models, that less than 20% of
the representation space can be explained by individual features. We show that
features in larger spaces become more interpretable when studied in groups and
can be explained with high-order scoring concepts through FALCON. We discuss
how extracted concepts can be used to explain and debug failures in downstream
tasks. Finally, we present a technique to transfer concepts from one
(explainable) representation space to another unseen representation space by
learning a simple linear transformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalibhat_N/0/1/0/all/0/1&quot;&gt;Neha Kalibhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_S/0/1/0/all/0/1&quot;&gt;Shweta Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruss_B/0/1/0/all/0/1&quot;&gt;Bayan Bruss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1&quot;&gt;Hamed Firooz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1&quot;&gt;Maziar Sanjabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1&quot;&gt;Soheil Feizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10507">
<title>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10507</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-silo federated learning (FL) enables the development of machine
learning models on datasets distributed across data centers such as hospitals
and clinical research laboratories. However, recent research has found that
current FL algorithms face a trade-off between local and global performance
when confronted with distribution shifts. Specifically, personalized FL methods
have a tendency to overfit to local data, leading to a sharp valley in the
local model and inhibiting its ability to generalize to out-of-distribution
data. In this paper, we propose a novel federated model soup method (i.e.,
selective interpolation of model parameters) to optimize the trade-off between
local and global performance. Specifically, during the federated training
phase, each client maintains its own global model pool by monitoring the
performance of the interpolated model between the local and global models. This
allows us to alleviate overfitting and seek flat minima, which can
significantly improve the model&apos;s generalization performance. We evaluate our
method on retinal and pathological image classification tasks, and our proposed
method achieves significant improvements for out-of-distribution
generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minghui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Meirui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10524">
<title>Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions. (arXiv:2307.10524v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10524</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the tradeoff between consistency and robustness in the context of a
single-trajectory time-varying Markov Decision Process (MDP) with untrusted
machine-learned advice. Our work departs from the typical approach of treating
advice as coming from black-box sources by instead considering a setting where
additional information about how the advice is generated is available. We prove
a first-of-its-kind consistency and robustness tradeoff given Q-value advice
under a general MDP model that includes both continuous and discrete
state/action spaces. Our results highlight that utilizing Q-value advice
enables dynamic pursuit of the better of machine-learned advice and a robust
baseline, thus result in near-optimal performance guarantees, which provably
improves what can be obtained solely with black-box advice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tongxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shaolei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wierman_A/0/1/0/all/0/1&quot;&gt;Adam Wierman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10529">
<title>Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10529</link>
<description rdf:parseType="Literal">&lt;p&gt;Outlier detection (OD) finds many applications with a rich literature of
numerous techniques. Deep neural network based OD (DOD) has seen a recent surge
of attention thanks to the many advances in deep learning. In this paper, we
consider a critical-yet-understudied challenge with unsupervised DOD, that is,
effective hyperparameter (HP) tuning/model selection. While several prior work
report the sensitivity of OD models to HPs, it becomes ever so critical for the
modern DOD models that exhibit a long list of HPs. We introduce HYPER for
tuning DOD models, tackling two fundamental challenges: (1) validation without
supervision (due to lack of labeled anomalies), and (2) efficient search of the
HP/model space (due to exponential growth in the number of HPs). A key idea is
to design and train a novel hypernetwork (HN) that maps HPs onto optimal
weights of the main DOD model. In turn, HYPER capitalizes on a single HN that
can dynamically generate weights for many DOD models (corresponding to varying
HPs), which offers significant speed-up. In addition, it employs meta-learning
on historical OD tasks with labels to train a proxy validation function,
likewise trained with our proposed HN efficiently. Extensive experiments on 35
OD tasks show that HYPER achieves high performance against 8 baselines with
significant efficiency gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xueying Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10541">
<title>Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter. (arXiv:2307.10541v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.10541</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based optimal control algorithms control unknown systems using past
trajectory data and a learned model of the system dynamics. These controllers
use either a linear approximation of the learned dynamics, trading performance
for faster computation, or nonlinear optimization methods, which typically
perform better but can limit real-time applicability. In this work, we present
a novel nonlinear controller that exploits differential flatness to achieve
similar performance to state-of-the-art learning-based controllers but with
significantly less computational effort. Differential flatness is a property of
dynamical systems whereby nonlinear systems can be exactly linearized through a
nonlinear input mapping. Here, the nonlinear transformation is learned as a
Gaussian process and is used in a safety filter that guarantees, with high
probability, stability as well as input and flat state constraint satisfaction.
This safety filter is then used to refine inputs from a flat model predictive
controller to perform constrained nonlinear learning-based optimal control
through two successive convex optimizations. We compare our method to
state-of-the-art learning-based control strategies and achieve similar
performance, but with significantly better computational efficiency, while also
respecting flat state and input constraints, and guaranteeing stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hall_A/0/1/0/all/0/1&quot;&gt;Adam W. Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Greeff_M/0/1/0/all/0/1&quot;&gt;Melissa Greeff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schoellig_A/0/1/0/all/0/1&quot;&gt;Angela P. Schoellig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10550">
<title>SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer. (arXiv:2307.10550v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.10550</link>
<description rdf:parseType="Literal">&lt;p&gt;Expressive speech synthesis models are trained by adding corpora with diverse
speakers, various emotions, and different speaking styles to the dataset, in
order to control various characteristics of speech and generate the desired
voice. In this paper, we propose a style control (SC) VALL-E model based on the
neural codec language model (called VALL-E), which follows the structure of the
generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input
from text sentences and prompt audio and is designed to generate controllable
speech by not simply mimicking the characteristics of the prompt audio but by
controlling the attributes to produce diverse voices. We identify tokens in the
style embedding matrix of the newly designed style network that represent
attributes such as emotion, speaking rate, pitch, and voice intensity, and
design a model that can control these attributes. To evaluate the performance
of SC VALL-E, we conduct comparative experiments with three representative
expressive speech synthesis models: global style token (GST) Tacotron2,
variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word
error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as
evaluation metrics to assess the accuracy of generated sentences. For comparing
the quality of synthesized speech, we measure comparative mean option score
(CMOS) and similarity mean option score (SMOS). To evaluate the style control
ability of the generated speech, we observe the changes in F0 and
mel-spectrogram by modifying the trained tokens. When using prompt audio that
is not present in the training data, SC VALL-E generates a variety of
expressive sounds and demonstrates competitive performance compared to the
existing models. Our implementation, pretrained models, and audio samples are
located on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daegyeom Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Seongho Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yong-Hoon Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10559">
<title>Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10559</link>
<description rdf:parseType="Literal">&lt;p&gt;Air traffic control (ATC) is a safety-critical service system that demands
constant attention from ground air traffic controllers (ATCos) to maintain
daily aviation operations. The workload of the ATCos can have negative effects
on operational safety and airspace usage. To avoid overloading and ensure an
acceptable workload level for the ATCos, it is important to predict the ATCos&apos;
workload accurately for mitigation actions. In this paper, we first perform a
review of research on ATCo workload, mostly from the air traffic perspective.
Then, we briefly introduce the setup of the human-in-the-loop (HITL)
simulations with retired ATCos, where the air traffic data and workload labels
are obtained. The simulations are conducted under three Phoenix approach
scenarios while the human ATCos are requested to self-evaluate their workload
ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next,
we propose a graph-based deep-learning framework with conformal prediction to
identify the ATCo workload levels. The number of aircraft under the
controller&apos;s control varies both spatially and temporally, resulting in
dynamically evolving graphs. The experiment results suggest that (a) besides
the traffic density feature, the traffic conflict feature contributes to the
workload prediction capabilities (i.e., minimum horizontal/vertical separation
distance); (b) directly learning from the spatiotemporal graph layout of
airspace with graph neural network can achieve higher prediction accuracy,
compare to hand-crafted traffic complexity features; (c) conformal prediction
is a valuable tool to further boost model prediction accuracy, resulting a
range of predicted workload labels. The code used is available at
\href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yutian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jueming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lieber_C/0/1/0/all/0/1&quot;&gt;Christopher S. Lieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooke_N/0/1/0/all/0/1&quot;&gt;Nancy J. Cooke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10560">
<title>Post-variational quantum neural networks. (arXiv:2307.10560v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.10560</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computing has the potential to provide substantial computational
advantages over current state-of-the-art classical supercomputers. However,
current hardware is not advanced enough to execute fault-tolerant quantum
algorithms. An alternative of using hybrid quantum-classical computing with
variational algorithms can exhibit barren plateau issues, causing slow
convergence of gradient-based optimization techniques. In this paper, we
discuss &quot;post-variational strategies&quot;, which shift tunable parameters from the
quantum computer to the classical computer, opting for ensemble strategies when
optimizing quantum models. We discuss various strategies and design principles
for constructing individual quantum circuits, where the resulting ensembles can
be optimized with convex programming. Further, we discuss architectural designs
of post-variational quantum neural networks and analyze the propagation of
estimation errors throughout such neural networks. Lastly, we show that our
algorithm can be applied to real-world applications such as image
classification on handwritten digits, producing a 96% classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Po-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rebentrost_P/0/1/0/all/0/1&quot;&gt;Patrick Rebentrost&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10562">
<title>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (arXiv:2307.10562v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10562</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attacks are serious security threats to machine learning models
where an adversary can inject poisoned samples into the training set, causing a
backdoored model which predicts poisoned samples with particular triggers to
particular target classes, while behaving normally on benign samples. In this
paper, we explore the task of purifying a backdoored model using a small clean
dataset. By establishing the connection between backdoor risk and adversarial
risk, we derive a novel upper bound for backdoor risk, which mainly captures
the risk on the shared adversarial examples (SAEs) between the backdoored model
and the purified model. This upper bound further suggests a novel bi-level
optimization problem for mitigating backdoor using adversarial training
techniques. To solve it, we propose Shared Adversarial Unlearning (SAU).
Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs
such that they are either correctly classified by the purified model and/or
differently classified by the two models, such that the backdoor effect in the
backdoored model will be mitigated in the purified model. Experiments on
various benchmark datasets and network architectures show that our proposed
method achieves state-of-the-art performance for backdoor defense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaokui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10563">
<title>FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation. (arXiv:2307.10563v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10563</link>
<description rdf:parseType="Literal">&lt;p&gt;We present FACADE, a novel probabilistic and geometric framework designed for
unsupervised mechanistic anomaly detection in deep neural networks. Its primary
goal is advancing the understanding and mitigation of adversarial attacks.
FACADE aims to generate probabilistic distributions over circuits, which
provide critical insights to their contribution to changes in the manifold
properties of pseudo-classes, or high-dimensional modes in activation space,
yielding a powerful tool for uncovering and combating adversarial attacks. Our
approach seeks to improve model robustness, enhance scalable model oversight,
and demonstrates promising applications in real-world deployment settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1&quot;&gt;Dhruv Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1&quot;&gt;Andres Carranza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1&quot;&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1&quot;&gt;Arnuv Tandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10569">
<title>Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10569</link>
<description rdf:parseType="Literal">&lt;p&gt;As the capabilities of large machine learning models continue to grow, and as
the autonomy afforded to such models continues to expand, the spectre of a new
adversary looms: the models themselves. The threat that a model might behave in
a seemingly reasonable manner, while secretly and subtly modifying its behavior
for ulterior reasons is often referred to as deceptive alignment in the AI
Safety &amp;amp; Alignment communities. Consequently, we call this new direction
Deceptive Alignment Monitoring. In this work, we identify emerging directions
in diverse machine learning subfields that we believe will become increasingly
important and intertwined in the near future for deceptive alignment
monitoring, and we argue that advances in these fields present both long-term
challenges and new research opportunities. We conclude by advocating for
greater involvement by the adversarial machine learning community in these
emerging directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1&quot;&gt;Andres Carranza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1&quot;&gt;Dhruv Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1&quot;&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1&quot;&gt;Arnuv Tandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10575">
<title>Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10575</link>
<description rdf:parseType="Literal">&lt;p&gt;As a distributed machine learning technique, federated learning (FL) requires
clients to collaboratively train a shared model with an edge server without
leaking their local data. However, the heterogeneous data distribution among
clients often leads to a decrease in model performance. To tackle this issue,
this paper introduces a prototype-based regularization strategy to address the
heterogeneity in the data distribution. Specifically, the regularization
process involves the server aggregating local prototypes from distributed
clients to generate a global prototype, which is then sent back to the
individual clients to guide their local training. The experimental results on
MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3%
and 8.9% in average test accuracy, respectively, compared to the most popular
baseline FedAvg. Furthermore, our approach has a fast convergence rate in
heterogeneous settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Huy Q. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10579">
<title>SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10579</link>
<description rdf:parseType="Literal">&lt;p&gt;SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to
protect data privacy in vertical federated learning setting. It is widely used
in fields such as finance and healthcare due to its interpretability,
effectiveness, and privacy-preserving capability. However, SecureBoost suffers
from high computational complexity and risk of label leakage. To harness the
full potential of SecureBoost, hyperparameters of SecureBoost should be
carefully chosen to strike an optimal balance between utility, efficiency, and
privacy. Existing methods either set hyperparameters empirically or
heuristically, which are far from optimal. To fill this gap, we propose a
Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto
optimal solutions that each solution is a set of hyperparameters achieving
optimal tradeoff between utility loss, training cost, and privacy leakage. We
design measurements of the three objectives. In particular, the privacy leakage
is measured using our proposed instance clustering attack. Experimental results
demonstrate that the CMOSB yields not only hyperparameters superior to the
baseline but also optimal sets of hyperparameters that can support the flexible
requirements of FL participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Ziyao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linghua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yongxin Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10580">
<title>Intelligent model for offshore China sea fog forecasting. (arXiv:2307.10580v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10580</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and timely prediction of sea fog is very important for effectively
managing maritime and coastal economic activities. Given the intricate nature
and inherent variability of sea fog, traditional numerical and statistical
forecasting methods are often proven inadequate. This study aims to develop an
advanced sea fog forecasting method embedded in a numerical weather prediction
model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior
to training our machine learning model, we employ a time-lagged correlation
analysis technique to identify key predictors and decipher the underlying
mechanisms driving sea fog occurrence. In addition, we implement ensemble
learning and a focal loss function to address the issue of imbalanced data,
thereby enhancing the predictive ability of our model. To verify the accuracy
of our method, we evaluate its performance using a comprehensive dataset
spanning one year, which encompasses both weather station observations and
historical forecasts. Remarkably, our machine learning-based approach surpasses
the predictive performance of two conventional methods, the weather research
and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm
developed by the National Oceanic and Atmospheric Administration (NOAA)
Forecast Systems Laboratory (FSL). Specifically, in regard to predicting sea
fog with a visibility of less than or equal to 1 km with a lead time of 60
hours, our methodology achieves superior results by increasing the probability
of detection (POD) while simultaneously reducing the false alarm ratio (FAR).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yanfei Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinghong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1&quot;&gt;Ruixue Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Yang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10586">
<title>A Holistic Assessment of the Reliability of Machine Learning Systems. (arXiv:2307.10586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10586</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning (ML) systems increasingly permeate high-stakes settings
such as healthcare, transportation, military, and national security, concerns
regarding their reliability have emerged. Despite notable progress, the
performance of these systems can significantly diminish due to adversarial
attacks or environmental changes, leading to overconfident predictions,
failures to detect input faults, and an inability to generalize in unexpected
scenarios. This paper proposes a holistic assessment methodology for the
reliability of ML systems. Our framework evaluates five key properties:
in-distribution accuracy, distribution-shift robustness, adversarial
robustness, calibration, and out-of-distribution detection. A reliability score
is also introduced and used to assess the overall system reliability. To
provide insights into the performance of different algorithmic approaches, we
identify and categorize state-of-the-art techniques, then evaluate a selection
on real-world tasks using our proposed reliability metrics and reliability
score. Our analysis of over 500 models reveals that designing for one metric
does not necessarily constrain others but certain algorithmic techniques can
improve reliability across multiple metrics simultaneously. This study
contributes to a more comprehensive understanding of ML reliability and
provides a roadmap for future research and development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1&quot;&gt;Anthony Corso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamadian_D/0/1/0/all/0/1&quot;&gt;David Karamadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentin_R/0/1/0/all/0/1&quot;&gt;Romeo Valentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1&quot;&gt;Mary Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10588">
<title>Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques. (arXiv:2307.10588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10588</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy systems, climate change, and public health are among the primary
reasons for moving toward electrification in transportation. Transportation
electrification is being promoted worldwide to reduce emissions. As a result,
many automakers will soon start making only battery electric vehicles (BEVs).
BEV adoption rates are rising in California, mainly due to climate change and
air pollution concerns. While great for climate and pollution goals, improperly
managed BEV charging can lead to insufficient charging infrastructure and power
outages. This study develops a novel Micro Clustering Deep Neural Network
(MCDNN), an artificial neural network algorithm that is highly effective at
learning BEVs trip and charging data to forecast BEV charging events,
information that is essential for electricity load aggregators and utility
managers to provide charging stations and electricity capacity effectively. The
MCDNN is configured using a robust dataset of trips and charges that occurred
in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a
total of 1570167 vehicle miles traveled. The numerical findings revealed that
the proposed MCDNN is more effective than benchmark approaches in this field,
such as support vector machine, k nearest neighbors, decision tree, and other
neural network-based models in predicting the charging events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayarani_H/0/1/0/all/0/1&quot;&gt;Hanif Tayarani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramadoss_T/0/1/0/all/0/1&quot;&gt;Trisha V. Ramadoss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanam_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Karanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tal_G/0/1/0/all/0/1&quot;&gt;Gil Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitta_C/0/1/0/all/0/1&quot;&gt;Christopher Nitta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10596">
<title>Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis. (arXiv:2307.10596v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10596</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Things (IoT) integrates more than billions of intelligent
devices over the globe with the capability of communicating with other
connected devices with little to no human intervention. IoT enables data
aggregation and analysis on a large scale to improve life quality in many
domains. In particular, data collected by IoT contain a tremendous amount of
information for anomaly detection. The heterogeneous nature of IoT is both a
challenge and an opportunity for cybersecurity. Traditional approaches in
cybersecurity monitoring often require different kinds of data pre-processing
and handling for various data types, which might be problematic for datasets
that contain heterogeneous features. However, heterogeneous types of network
devices can often capture a more diverse set of signals than a single type of
device readings, which is particularly useful for anomaly detection. In this
paper, we present a comprehensive study on using ensemble machine learning
methods for enhancing IoT cybersecurity via anomaly detection. Rather than
using one single machine learning model, ensemble learning combines the
predictive power from multiple models, enhancing their predictive accuracy in
heterogeneous datasets rather than using one single machine learning model. We
propose a unified framework with ensemble learning that utilises Bayesian
hyperparameter optimisation to adapt to a network environment that contains
multiple IoT sensor readings. Experimentally, we illustrate their high
predictive power when compared to traditional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1&quot;&gt;Tin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_F/0/1/0/all/0/1&quot;&gt;Farnaz Farid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bello_A/0/1/0/all/0/1&quot;&gt;Abubakar Bello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabrina_F/0/1/0/all/0/1&quot;&gt;Fariza Sabrina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10616">
<title>Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10616</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has drawn increasing attention owing to its potential
use in large-scale industrial applications. Existing federated learning works
mainly focus on model homogeneous settings. However, practical federated
learning typically faces the heterogeneity of data distributions, model
architectures, network environments, and hardware devices among participant
clients. Heterogeneous Federated Learning (HFL) is much more challenging, and
corresponding solutions are diverse and complex. Therefore, a systematic survey
on this topic about the research challenges and state-of-the-art is essential.
In this survey, we firstly summarize the various research challenges in HFL
from five aspects: statistical heterogeneity, model heterogeneity,
communication heterogeneity, device heterogeneity, and additional challenges.
In addition, recent advances in HFL are reviewed and a new taxonomy of existing
HFL methods is proposed with an in-depth analysis of their pros and cons. We
classify existing methods from three different levels according to the HFL
procedure: data-level, model-level, and server-level. Finally, several critical
and promising future research directions in HFL are discussed, which may
facilitate further developments in this field. A periodically updated
collection on HFL is available at https://github.com/marswhu/HFL_Survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xiuwen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_P/0/1/0/all/0/1&quot;&gt;Pong C. Yuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10617">
<title>Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.10617</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, online reviews play a vital role for promoting any kind of
product or services. Businesses may embed fake reviews in order to attract
customers to purchase their products. They may even highlight the benefits of
their own product or criticize the competition&apos;s product. Marketers,
advertisers, and other online business users have incentive to create fake
positive reviews for products which they want to promote or give fake negative
reviews for products which they really don&apos;t like. So now-a-days writing a
deceptive review is inevitable thing for promoting their own business or
degrading competitor&apos;s reputation. Thus, identifying deceptive reviews is an
intense and on-going research area. This research paper proposes machine
learning model approach to identify deceptive reviews. The paper investigates
the performance of the several experiments done on a Deceptive Opinion Spam
Corpus dataset of restaurants reviews. We developed a n-gram model and max
features to identify deceptive contents with a particular focus on fake
reviews. Further, we conduct a benchmark study to investigate the performance
of two different features extraction techniques and apply five machine learning
classification techniques. The experimental results show that passive
aggressive classifier outperforms other algorithms, and it reaches the highest
accuracy not only in text classification but also to fake reviews. We also
study the data augmentation and implement different deep learning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baby_A/0/1/0/all/0/1&quot;&gt;Anusuya Baby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10633">
<title>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10633</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models have many methods for solving the same problem. This
introduces novel strengths (different methods may work well for different
problems) and weaknesses (it may be difficult for users to know which method to
use). In this paper, we introduce Multi-Method Self-Training (MMST), where one
method is trained on the filtered outputs of another, allowing us to augment
the strengths and ameliorate the weaknesses of each method. Using a 176B
parameter model trained on both language and code, we show that MMST can 1)
improve the less performant method (up to 30%) making the model easier to use,
2) improve the more performant method (up to 32.2%) making the model more
performant, and 3) improve the performance of related but distinct tasks (up to
10.3%) by improving the ability of the model to generate rationales. We then
conduct ablation analyses to explore why MMST works. We show that MMST
generates more data than traditional self-training, but the improvement in
performance is driven by the use of multiple methods. We also analyze
prompt-engineering and anti-correlated performance between methods as means of
making MMST more effective. We hope the evidence from our paper motivates
machine learning researchers to explore ways in which advances in language
models allow for new forms of training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1&quot;&gt;Shriyash K. Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginsberg_E/0/1/0/all/0/1&quot;&gt;Etan J. Ginsberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10634">
<title>Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2307.10634</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models, primarily transformer-based ones, obtained colossal success
in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3
for NLG are very crucial. DNA sequences are very close to natural language in
terms of structure, so if the DNA-related bioinformatics domain is concerned,
discriminative models, like DNABert, exist. Yet, the generative side of the
coin is mainly unexplored to the best of our knowledge. Consequently, we
focused on developing an autoregressive generative language model like GPT-3
for DNA sequences. Because working with whole DNA sequences is challenging
without substantial computational resources, we decided to carry out our study
on a smaller scale, focusing on nucleotide sequences of human genes, unique
parts in DNA with specific functionalities, instead of the whole DNA. This
decision did not change the problem structure a lot due to the fact that both
DNA and genes can be seen as 1D sequences consisting of four different
nucleotides without losing much information and making too much simplification.
First of all, we systematically examined an almost entirely unexplored problem
and observed that RNNs performed the best while simple techniques like N-grams
were also promising. Another beneficial point was learning how to work with
generative models on languages we do not understand, unlike natural language.
How essential using real-life tasks beyond the classical metrics such as
perplexity is observed. Furthermore, checking whether the data-hungry nature of
these models can be changed through selecting a language with minimal
vocabulary size, four owing to four different types of nucleotides, is
examined. The reason for reviewing this was that choosing such a language might
make the problem easier. However, what we observed in this study was it did not
provide that much of a change in the amount of data needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ihtiyar_M/0/1/0/all/0/1&quot;&gt;Musa Nuri Ihtiyar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ozgur_A/0/1/0/all/0/1&quot;&gt;Arzucan Ozgur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10635">
<title>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10635</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models (LLMs) have demonstrated notable
progress on many mathematical benchmarks. However, most of these benchmarks
only feature problems grounded in junior and senior high school subjects,
contain only multiple-choice questions, and are confined to a limited scope of
elementary arithmetic operations. To address these issues, this paper
introduces an expansive benchmark suite SciBench that aims to systematically
examine the reasoning capabilities required for complex scientific problem
solving. SciBench contains two carefully curated datasets: an open set
featuring a range of collegiate-level scientific problems drawn from
mathematics, chemistry, and physics textbooks, and a closed set comprising
problems from undergraduate-level exams in computer science and mathematics.
Based on the two datasets, we conduct an in-depth benchmark study of two
representative LLMs with various prompting strategies. The results reveal that
current LLMs fall short of delivering satisfactory performance, with an overall
score of merely 35.80%. Furthermore, through a detailed user study, we
categorize the errors made by LLMs into ten problem-solving abilities. Our
analysis indicates that no single prompting strategy significantly outperforms
others and some strategies that demonstrate improvements in certain
problem-solving skills result in declines in other skills. We envision that
SciBench will catalyze further developments in the reasoning abilities of LLMs,
thereby ultimately contributing to scientific research and discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Ziniu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramaniam_S/0/1/0/all/0/1&quot;&gt;Satyen Subramaniam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loomba_A/0/1/0/all/0/1&quot;&gt;Arjun R. Loomba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shichang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10644">
<title>Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10644</link>
<description rdf:parseType="Literal">&lt;p&gt;Data sets of multivariate normal distributions abound in many scientific
areas like diffusion tensor imaging, structure tensor computer vision, radar
signal processing, machine learning, just to name a few. In order to process
those normal data sets for downstream tasks like filtering, classification or
clustering, one needs to define proper notions of dissimilarities between
normals and paths joining them. The Fisher-Rao distance defined as the
Riemannian geodesic distance induced by the Fisher information metric is such a
principled metric distance which however is not known in closed-form excepts
for a few particular cases. In this work, we first report a fast and robust
method to approximate arbitrarily finely the Fisher-Rao distance between
multivariate normal distributions. Second, we introduce a class of distances
based on diffeomorphic embeddings of the normal manifold into a submanifold of
the higher-dimensional symmetric positive-definite cone corresponding to the
manifold of centered normal distributions. We show that the projective Hilbert
distance on the cone yields a metric on the embedded normal submanifold and we
pullback that cone distance with its associated straight line Hilbert cone
geodesics to obtain a distance and smooth paths between normal distributions.
Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone
distance is computationally light since it requires to compute only the extreme
minimal and maximal eigenvalues of matrices. Finally, we show how to use those
distances in clustering tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1&quot;&gt;Frank Nielsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10648">
<title>Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities. (arXiv:2307.10648v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.10648</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of new application areas, such as cyber-physical systems
and human-in-the-loop applications, there is a need to guarantee a certain
level of end-to-end network latency with extremely high reliability, e.g.,
99.999%. While mechanisms specified under IEEE 802.1as time-sensitive
networking (TSN) can be used to achieve these requirements for switched
Ethernet networks, implementing TSN mechanisms in wireless networks is
challenging due to their stochastic nature. To conform the wireless link to a
reliability level of 99.999%, the behavior of extremely rare outliers in the
latency probability distribution, or the tail of the distribution, must be
analyzed and controlled. This work proposes predicting the tail of the latency
distribution using state-of-the-art data-driven approaches, such as mixture
density networks (MDN) and extreme value mixture models, to estimate the
likelihood of rare latencies conditioned on the network parameters, which can
be used to make more informed decisions in wireless transmission. Actual
latency measurements of IEEE 802.11g (WiFi), commercial private and a
software-defined 5G network are used to benchmark the proposed approaches and
evaluate their sensitivities concerning the tail probabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostafavi_S/0/1/0/all/0/1&quot;&gt;Samie Mostafavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1&quot;&gt;Gourav Prateek Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1&quot;&gt;James Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10653">
<title>Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services. (arXiv:2307.10653v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10653</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series anomaly detection is crucial for industrial monitoring services
that handle a large volume of data, aiming to ensure reliability and optimize
system performance. Existing methods often require extensive labeled resources
and manual parameter selection, highlighting the need for automation. This
paper proposes a comprehensive framework for automatic parameter optimization
in time series anomaly detection models. The framework introduces three
optimization targets: prediction score, shape score, and sensitivity score,
which can be easily adapted to different model backbones without prior
knowledge or manual labeling efforts. The proposed framework has been
successfully applied online for over six months, serving more than 50,000 time
series every minute. It simplifies the user&apos;s experience by requiring only an
expected sensitive value, offering a user-friendly interface, and achieving
desired detection results. Extensive evaluations conducted on public datasets
and comparison with other methods further confirm the effectiveness of the
proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Manqing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhanxiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yitong Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wentao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huai Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10654">
<title>Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10654</link>
<description rdf:parseType="Literal">&lt;p&gt;A very popular model-agnostic technique for explaining predictive models is
the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP
are a conditional expectation version and an unconditional expectation version
(the latter is also known as interventional SHAP). Except for tree-based
methods, usually the unconditional version is used (for computational reasons).
We provide a (surrogate) neural network approach which allows us to efficiently
calculate the conditional version for both neural networks and other regression
models, and which properly considers the dependence structure in the feature
components. This proposal is also useful to provide drop1 and anova analyses in
complex regression models which are similar to their generalized linear model
(GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart
that considers the right dependence structure in the feature components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richman_R/0/1/0/all/0/1&quot;&gt;Ronald Richman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1&quot;&gt;Mario V. W&amp;#xfc;thrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10655">
<title>A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency. (arXiv:2307.10655v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10655</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has emerged as a highly effective paradigm for
privacy-preserving collaborative training among different parties. Unlike
traditional centralized learning, which requires collecting data from each
party, FL allows clients to share privacy-preserving information without
exposing private datasets. This approach not only guarantees enhanced privacy
protection but also facilitates more efficient and secure collaboration among
multiple participants. Therefore, FL has gained considerable attention from
researchers, promoting numerous surveys to summarize the related works.
However, the majority of these surveys concentrate on methods sharing model
parameters during the training process, while overlooking the potential of
sharing other forms of local information. In this paper, we present a
systematic survey from a new perspective, i.e., what to share in FL, with an
emphasis on the model utility, privacy leakage, and communication efficiency.
This survey differs from previous ones due to four distinct contributions.
First, we present a new taxonomy of FL methods in terms of the sharing methods,
which includes three categories of shared information: model sharing, synthetic
data sharing, and knowledge sharing. Second, we analyze the vulnerability of
different sharing methods to privacy attacks and review the defense mechanisms
that provide certain privacy guarantees. Third, we conduct extensive
experiments to compare the performance and communication overhead of various
sharing methods in FL. Besides, we assess the potential privacy leakage through
model inversion and membership inference attacks, while comparing the
effectiveness of various defense approaches. Finally, we discuss potential
deficiencies in current methods and outline future directions for improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jiawei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zijian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenqiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tailin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuchang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lumin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zehong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10677">
<title>Deep learning for classification of noisy QR codes. (arXiv:2307.10677v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10677</link>
<description rdf:parseType="Literal">&lt;p&gt;We wish to define the limits of a classical classification model based on
deep learning when applied to abstract images, which do not represent visually
identifiable objects.QR codes (Quick Response codes) fall into this category of
abstract images: one bit corresponding to one encoded character, QR codes were
not designed to be decoded manually. To understand the limitations of a deep
learning-based model for abstract image classification, we train an image
classification model on QR codes generated from information obtained when
reading a health pass. We compare a classification model with a classical
(deterministic) decoding method in the presence of noise. This study allows us
to conclude that a model based on deep learning can be relevant for the
understanding of abstract images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leygonie_R/0/1/0/all/0/1&quot;&gt;Rebecca Leygonie&lt;/a&gt; (LIPADE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1&quot;&gt;Sylvain Lobry&lt;/a&gt; (LIPADE)), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+%28LIPADE%29_L/0/1/0/all/0/1&quot;&gt;Laurent Wendling (LIPADE)&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10683">
<title>Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.10683</link>
<description rdf:parseType="Literal">&lt;p&gt;Coordinate denoising is a promising 3D molecular pre-training method, which
has achieved remarkable performance in various downstream drug discovery tasks.
Theoretically, the objective is equivalent to learning the force field, which
is revealed helpful for downstream tasks. Nevertheless, there are two
challenges for coordinate denoising to learn an effective force field, i.e. low
coverage samples and isotropic force field. The underlying reason is that
molecular distributions assumed by existing denoising methods fail to capture
the anisotropic characteristic of molecules. To tackle these challenges, we
propose a novel hybrid noise strategy, including noises on both dihedral angel
and coordinate. However, denoising such hybrid noise in a traditional way is no
more equivalent to learning the force field. Through theoretical deductions, we
find that the problem is caused by the dependency of the input conformation for
covariance. To this end, we propose to decouple the two types of noise and
design a novel fractional denoising method (Frad), which only denoises the
latter coordinate part. In this way, Frad enjoys both the merits of sampling
more low-energy structures and the force field equivalence. Extensive
experiments show the effectiveness of Frad in molecular representation, with a
new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of
MD17.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shikun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuyan Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhi-Ming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Ying Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10695">
<title>Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss. (arXiv:2307.10695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10695</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, denoising methods based on supervised learning have exhibited
promising performance. However, their reliance on external datasets containing
noisy-clean image pairs restricts their applicability. To address this
limitation, researchers have focused on training denoising networks using
solely a set of noisy inputs. To improve the feasibility of denoising
procedures, in this study, we proposed a single-image self-supervised learning
method in which only the noisy input image is used for network training. Gated
convolution was used for feature extraction and no-reference image quality
assessment was used for guiding the training process. Moreover, the proposed
method sampled instances from the input image dataset using Bernoulli sampling
with a certain dropout rate for training. The corresponding result was produced
by averaging the generated predictions from various instances of the trained
network with dropouts. The experimental results indicated that the proposed
method achieved state-of-the-art denoising performance on both synthetic and
real-world datasets. This highlights the effectiveness and practicality of our
method as a potential solution for various noise removal tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Jaekyun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sanghwan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10703">
<title>Graphs in State-Space Models for Granger Causality in Climate Science. (arXiv:2307.10703v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10703</link>
<description rdf:parseType="Literal">&lt;p&gt;Granger causality (GC) is often considered not an actual form of causality.
Still, it is arguably the most widely used method to assess the predictability
of a time series from another one. Granger causality has been widely used in
many applied disciplines, from neuroscience and econometrics to Earth sciences.
We revisit GC under a graphical perspective of state-space models. For that, we
use GraphEM, a recently presented expectation-maximisation algorithm for
estimating the linear matrix operator in the state equation of a
linear-Gaussian state-space model. Lasso regularisation is included in the
M-step, which is solved using a proximal splitting Douglas-Rachford algorithm.
Experiments in toy examples and challenging climate problems illustrate the
benefits of the proposed model and inference technique over standard Granger
causality methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elvira_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Elvira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;milie Chouzenoux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerda_J/0/1/0/all/0/1&quot;&gt;Jordi Cerd&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1&quot;&gt;Gustau Camps-Valls&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10704">
<title>Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits. (arXiv:2307.10704v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10704</link>
<description rdf:parseType="Literal">&lt;p&gt;The drastic growth of electric vehicles and photovoltaics can introduce new
challenges, such as electrical current congestion and voltage limit violations
due to peak load demands. These issues can be mitigated by controlling the
operation of electric vehicles i.e., smart charging. Centralized smart charging
solutions have already been proposed in the literature. But such solutions may
lack scalability and suffer from inherent drawbacks of centralization, such as
a single point of failure, and data privacy concerns. Decentralization can help
tackle these challenges. In this paper, a fully decentralized smart charging
system is proposed using the philosophy of adaptive multi-agent systems. The
proposed system utilizes multi-armed bandit learning to handle uncertainties in
the system. The presented system is decentralized, scalable, real-time,
model-free, and takes fairness among different players into account. A detailed
case study is also presented for performance evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1&quot;&gt;Sharyal Zafar&lt;/a&gt; (ENS Rennes, SATIE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feraud_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Feraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blavette_A/0/1/0/all/0/1&quot;&gt;Anne Blavette&lt;/a&gt; (ENS Rennes, SATIE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_G/0/1/0/all/0/1&quot;&gt;Guy Camilleri&lt;/a&gt; (UT3, IRIT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_H/0/1/0/all/0/1&quot;&gt;Hamid Ben&lt;/a&gt; (SATIE, ENS Rennes)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10705">
<title>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10705</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a common task in autonomous driving to understand
the surrounding environment. Driveable Area Segmentation and Lane Detection are
particularly important for safe and efficient navigation on the road. However,
original semantic segmentation models are computationally expensive and require
high-end hardware, which is not feasible for embedded systems in autonomous
vehicles. This paper proposes a lightweight model for the driveable area and
lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate
and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K
dataset and compare it with modern models. Experimental results show that our
TwinLiteNet performs similarly to existing approaches, requiring significantly
fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score
of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task
with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.
Furthermore, TwinLiteNet can run in real-time on embedded devices with limited
computing power, especially since it achieves 60FPS on Jetson Xavier NX, making
it an ideal solution for self-driving vehicles. Code is available:
url{https://github.com/chequanghuy/TwinLiteNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Q/0/1/0/all/0/1&quot;&gt;Quang Huy Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dinh Phuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh Quan Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_D/0/1/0/all/0/1&quot;&gt;Duc Khai Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10710">
<title>Reparameterized Policy Learning for Multimodal Trajectory Optimization. (arXiv:2307.10710v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10710</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the challenge of parametrizing policies for reinforcement
learning (RL) in high-dimensional continuous action spaces. Our objective is to
develop a multimodal policy that overcomes limitations inherent in the
commonly-used Gaussian parameterization. To achieve this, we propose a
principled framework that models the continuous RL policy as a generative model
of optimal trajectories. By conditioning the policy on a latent variable, we
derive a novel variational bound as the optimization objective, which promotes
exploration of the environment. We then present a practical model-based RL
method, called Reparameterized Policy Gradient (RPG), which leverages the
multimodal policy parameterization and learned world model to achieve strong
exploration capabilities and high data efficiency. Empirical results
demonstrate that our method can help agents evade local optima in tasks with
dense rewards and solve challenging sparse-reward environments by incorporating
an object-centric intrinsic reward. Our method consistently outperforms
previous approaches across a range of tasks. Code and supplementary materials
are available on the project page https://haosulab.github.io/RPG/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Litian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chuang Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10718">
<title>Differences Between Hard and Noisy-labeled Samples: An Empirical Study. (arXiv:2307.10718v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10718</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting noisy or incorrectly labeled samples from a labeled dataset with
hard/difficult samples is an important yet under-explored topic. Two general
and often independent lines of work exist, one focuses on addressing noisy
labels, and another deals with hard samples. However, when both types of data
are present, most existing methods treat them equally, which results in a
decline in the overall performance of the model. In this paper, we first design
various synthetic datasets with custom hardness and noisiness levels for
different samples. Our proposed systematic empirical study enables us to better
understand the similarities and more importantly the differences between
hard-to-learn samples and incorrectly-labeled samples. These controlled
experiments pave the way for the development of methods that distinguish
between hard and noisy samples. Through our study, we introduce a simple yet
effective metric that filters out noisy-labeled samples while keeping the hard
samples. We study various data partitioning methods in the presence of label
noise and observe that filtering out noisy samples from hard samples with this
proposed metric results in the best datasets as evidenced by the high test
accuracy achieved after models are trained on the filtered datasets. We
demonstrate this for both our created synthetic datasets and for datasets with
real-world label noise. Furthermore, our proposed data partitioning method
significantly outperforms other methods when employed within a semi-supervised
learning framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forouzesh_M/0/1/0/all/0/1&quot;&gt;Mahsa Forouzesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1&quot;&gt;Patrick Thiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10736">
<title>Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10736</link>
<description rdf:parseType="Literal">&lt;p&gt;We suggest a simple Gaussian mixture model for data generation that complies
with Feldman&apos;s long tail theory (2020). We demonstrate that a linear classifier
cannot decrease the generalization error below a certain level in the proposed
model, whereas a nonlinear classifier with a memorization capacity can. This
confirms that for long-tailed distributions, rare training examples must be
considered for optimal generalization to new data. Finally, we show that the
performance gap between linear and nonlinear models can be lessened as the tail
becomes shorter in the subpopulation frequency distribution, as confirmed by
experiments on synthetic and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolatov_A/0/1/0/all/0/1&quot;&gt;Arman Bolatov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tezekbayev_M/0/1/0/all/0/1&quot;&gt;Maxat Tezekbayev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnykov_I/0/1/0/all/0/1&quot;&gt;Igor Melnykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pak_A/0/1/0/all/0/1&quot;&gt;Artur Pak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1&quot;&gt;Vassilina Nikoulina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1&quot;&gt;Zhenisbek Assylbekov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10738">
<title>Fairness-Aware Client Selection for Federated Learning. (arXiv:2307.10738v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10738</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients)
to train machine learning models collaboratively without revealing private
data. Since the FL server can only engage a limited number of clients in each
training round, FL client selection has become an important research problem.
Existing approaches generally focus on either enhancing FL model performance or
enhancing the fair treatment of FL clients. The problem of balancing
performance and fairness considerations when selecting FL clients remains open.
To address this problem, we propose the Fairness-aware Federated Client
Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically
adjusts FL clients&apos; selection probabilities by jointly considering their
reputations, times of participation in FL tasks and contributions to the
resulting model performance. By not using threshold-based reputation filtering,
it provides FL clients with opportunities to redeem their reputations after a
perceived poor performance, thereby further enhancing fair client treatment.
Extensive experiments based on real-world multimedia datasets show that
FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on
average than the best-performing state-of-the-art approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zelei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10749">
<title>Mitigating Voter Attribute Bias for Fair Opinion Aggregation. (arXiv:2307.10749v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.10749</link>
<description rdf:parseType="Literal">&lt;p&gt;The aggregation of multiple opinions plays a crucial role in decision-making,
such as in hiring and loan review, and in labeling data for supervised
learning. Although majority voting and existing opinion aggregation models are
effective for simple tasks, they are inappropriate for tasks without
objectively true labels in which disagreements may occur. In particular, when
voter attributes such as gender or race introduce bias into opinions, the
aggregation results may vary depending on the composition of voter attributes.
A balanced group of voters is desirable for fair aggregation results but may be
difficult to prepare. In this study, we consider methods to achieve fair
opinion aggregation based on voter attributes and evaluate the fairness of the
aggregated results. To this end, we consider an approach that combines opinion
aggregation models such as majority voting and the Dawid and Skene model (D&amp;amp;S
model) with fairness options such as sample weighting. To evaluate the fairness
of opinion aggregation, probabilistic soft labels are preferred over discrete
class labels. First, we address the problem of soft label estimation without
considering voter attributes and identify some issues with the D&amp;amp;S model. To
address these limitations, we propose a new Soft D&amp;amp;S model with improved
accuracy in estimating soft labels. Moreover, we evaluated the fairness of an
opinion aggregation model, including Soft D&amp;amp;S, in combination with different
fairness options using synthetic and semi-synthetic data. The experimental
results suggest that the combination of Soft D&amp;amp;S and data splitting as a
fairness option is effective for dense data, whereas weighted majority voting
is effective for sparse data. These findings should prove particularly valuable
in supporting decision-making by human and machine-learning models with
balanced opinion aggregation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ueda_R/0/1/0/all/0/1&quot;&gt;Ryosuke Ueda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1&quot;&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1&quot;&gt;Hisashi Kashima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10763">
<title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing action recognition methods are typically actor-specific due to the
intrinsic topological and apparent differences among the actors. This requires
actor-specific pose estimation (e.g., humans vs. animals), leading to
cumbersome model design complexity and high maintenance costs. Moreover, they
often focus on learning the visual modality alone and single-label
classification whilst neglecting other available information sources (e.g.,
class name text) and the concurrent occurrence of multiple actions. To overcome
these limitations, we propose a new approach called &apos;actor-agnostic multi-modal
multi-label action recognition,&apos; which offers a unified solution for various
types of actors, including humans and animals. We further formulate a novel
Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object
detection framework (e.g., DETR), characterized by leveraging visual and
textual modalities to represent the action classes better. The elimination of
actor-specific model designs is a key advantage, as it removes the need for
actor pose estimation altogether. Extensive experiments on five publicly
available benchmarks show that our MSQNet consistently outperforms the prior
arts of actor-specific alternatives on human and animal single- and multi-label
action recognition tasks by up to 50%. Code will be released at
https://github.com/mondalanindya/MSQNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Anindya Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prada_J/0/1/0/all/0/1&quot;&gt;Joaquin M Prada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Anjan Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10768">
<title>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.10768</link>
<description rdf:parseType="Literal">&lt;p&gt;Working memory (WM), a fundamental cognitive process facilitating the
temporary storage, integration, manipulation, and retrieval of information,
plays a vital role in reasoning and decision-making tasks. Robust benchmark
datasets that capture the multifaceted nature of WM are crucial for the
effective development and evaluation of AI WM models. Here, we introduce a
comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM
comprises 10 tasks and a total of 1 million trials, assessing 4
functionalities, 3 domains, and 11 behavioral and neural characteristics of WM.
We jointly trained and tested state-of-the-art recurrent neural networks and
transformers on all these tasks. We also include human behavioral benchmarks as
an upper bound for comparison. Our results suggest that AI models replicate
some characteristics of WM in the brain, most notably primacy and recency
effects, and neural clusters and correlates specialized for different domains
and functionalities of WM. In the experiments, we also reveal some limitations
in existing models to approximate human behavior. This dataset serves as a
valuable resource for communities in cognitive psychology, neuroscience, and
AI, offering a standardized framework to compare and enhance WM models,
investigate WM&apos;s neural underpinnings, and develop WM models with human-like
capabilities. Our source code and data are available at
https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sikarwar_A/0/1/0/all/0/1&quot;&gt;Ankur Sikarwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10773">
<title>Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms. (arXiv:2307.10773v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.10773</link>
<description rdf:parseType="Literal">&lt;p&gt;Music recommendation systems have emerged as a vital component to enhance
user experience and satisfaction for the music streaming services, which
dominates music consumption. The key challenge in improving these recommender
systems lies in comprehending the complexity of music data, specifically for
the underpinning music genre classification. The limitations of manual genre
classification have highlighted the need for a more advanced system, namely the
Automatic Music Genre Classification (AMGC) system. While traditional machine
learning techniques have shown potential in genre classification, they heavily
rely on manually engineered features and feature selection, failing to capture
the full complexity of music data. On the other hand, deep learning
classification architectures like the traditional Convolutional Neural Networks
(CNN) are effective in capturing the spatial hierarchies but struggle to
capture the temporal dynamics inherent in music data. To address these
challenges, this study proposes a novel approach using visual spectrograms as
input, and propose a hybrid model that combines the strength of the Residual
neural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is
designed to provide a more comprehensive analysis of music data, offering the
potential to improve the music recommender systems through achieving a more
comprehensive analysis of music data and hence potentially more accurate genre
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junfei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10774">
<title>Assessing the Use of AutoML for Data-Driven Software Engineering. (arXiv:2307.10774v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.10774</link>
<description rdf:parseType="Literal">&lt;p&gt;Background. Due to the widespread adoption of Artificial Intelligence (AI)
and Machine Learning (ML) for building software applications, companies are
struggling to recruit employees with a deep understanding of such technologies.
In this scenario, AutoML is soaring as a promising solution to fill the AI/ML
skills gap since it promises to automate the building of end-to-end AI/ML
pipelines that would normally be engineered by specialized team members. Aims.
Despite the growing interest and high expectations, there is a dearth of
information about the extent to which AutoML is currently adopted by teams
developing AI/ML-enabled systems and how it is perceived by practitioners and
researchers. Method. To fill these gaps, in this paper, we present a
mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two
SE datasets and a user survey with follow-up interviews to further our
understanding of AutoML adoption and perception. Results. We found that AutoML
solutions can generate models that outperform those trained and optimized by
researchers to perform classification tasks in the SE domain. Also, our
findings show that the currently available AutoML solutions do not live up to
their names as they do not equally support automation across the stages of the
ML development workflow and for all the team members. Conclusions. We derive
insights to inform the SE research community on how AutoML can facilitate their
activities and tool builders on how to design the next generation of AutoML
technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calefato_F/0/1/0/all/0/1&quot;&gt;Fabio Calefato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quaranta_L/0/1/0/all/0/1&quot;&gt;Luigi Quaranta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanubile_F/0/1/0/all/0/1&quot;&gt;Filippo Lanubile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalinowski_M/0/1/0/all/0/1&quot;&gt;Marcos Kalinowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10779">
<title>Efficient Beam Tree Recursion. (arXiv:2307.10779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10779</link>
<description rdf:parseType="Literal">&lt;p&gt;Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a
simple extension of Gumbel Tree RvNN and it was shown to achieve
state-of-the-art length generalization performance in ListOps while maintaining
comparable performance on other tasks. However, although not the worst in its
kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this
paper, we identify the main bottleneck in BT-RvNN&apos;s memory usage to be the
entanglement of the scorer function and the recursive cell function. We propose
strategies to remove this bottleneck and further simplify its memory usage.
Overall, our strategies not only reduce the memory usage of BT-RvNN by
$10$-$16$ times but also create a new state-of-the-art in ListOps while
maintaining similar performance in other tasks. In addition, we also propose a
strategy to utilize the induced latent-tree node representations produced by
BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n
\times d} \rightarrow \mathbb{R}^{d}$ into a sequence contextualizer of the
form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our
proposals not only open up a path for further scalability of RvNNs but also
standardize a way to use BT-RvNNs as another building block in the deep
learning toolkit that can be easily stacked or interfaced with other popular
models such as Transformers and Structured State Space models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1&quot;&gt;Jishnu Ray Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1&quot;&gt;Cornelia Caragea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10787">
<title>Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10787</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free domain adaptation has become popular because of its practical
usefulness and no need to access source data. However, the adaptation process
still takes a considerable amount of time and is predominantly based on
optimization that relies on back-propagation. In this work we present a simple
feed-forward approach that challenges the need for back-propagation based
adaptation. Our approach is based on computing prototypes of classes under the
domain shift using a pre-trained model. It achieves strong improvements in
accuracy compared to the pre-trained model and requires only a small fraction
of time of existing domain adaptation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Da Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10788">
<title>Adversarial attacks for mixtures of classifiers. (arXiv:2307.10788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10788</link>
<description rdf:parseType="Literal">&lt;p&gt;Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a
way to improve robustness against adversarial attacks. However, it has been
shown that existing attacks are not well suited for this kind of classifiers.
In this paper, we discuss the problem of attacking a mixture in a principled
way and introduce two desirable properties of attacks based on a geometrical
analysis of the problem (effectiveness and maximality). We then show that
existing attacks do not meet both of these properties. Finally, we introduce a
new attack called lattice climber attack with theoretical guarantees on the
binary linear setting, and we demonstrate its performance by conducting
experiments on synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heredia_L/0/1/0/all/0/1&quot;&gt;Lucas Gnecco Heredia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1&quot;&gt;Benjamin Negrevergne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1&quot;&gt;Yann Chevaleyre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10792">
<title>Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10792</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and
tries to distinguish between normal and anomalous data using only few selected
samples. While newly proposed few-shot AD methods do compare against
pre-existing algorithms developed for the full-shot domain as baselines, they
do not dedicatedly optimize them for the few-shot setting. It thus remains
unclear if the performance of such pre-existing algorithms can be further
improved. We address said question in this work. Specifically, we present a
study on the AD/anomaly segmentation (AS) performance of PatchCore, the current
state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the
many-shot settings. We hypothesize that further performance improvements can be
realized by (I) optimizing its various hyperparameters, and by (II)
transferring techniques known to improve few-shot supervised learning to the AD
domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal
that (I) significant performance improvements can be realized by optimizing
hyperparameters such as the underlying feature extractor, and that (II)
image-level augmentations can, but are not guaranteed, to improve performance.
Based on these findings, we achieve a new state of the art in few-shot AD on
VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to
the few-shot setting. Last, we identify the investigation of feature extractors
with a strong inductive bias as a potential future research direction for
(few-shot) AD/AS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Triet Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rippel_O/0/1/0/all/0/1&quot;&gt;Oliver Rippel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10802">
<title>Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10802</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning aims to build models that can process and relate
information from multiple modalities. Despite years of development in this
field, it still remains challenging to design a unified network for processing
various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point
clouds, audio, video, time series, tabular data) due to the inherent gaps among
them. In this work, we propose a framework, named Meta-Transformer, that
leverages a $\textbf{frozen}$ encoder to perform multimodal perception without
any paired multimodal training data. In Meta-Transformer, the raw input data
from various modalities are mapped into a shared token space, allowing a
subsequent encoder with frozen parameters to extract high-level semantic
features of the input data. Composed of three main components: a unified data
tokenizer, a modality-shared encoder, and task-specific heads for downstream
tasks, Meta-Transformer is the first framework to perform unified learning
across 12 modalities with unpaired data. Experiments on different benchmarks
reveal that Meta-Transformer can handle a wide range of tasks including
fundamental perception (text, image, point cloud, audio, video), practical
application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,
tabular, and time-series). Meta-Transformer indicates a promising future for
developing unified multimodal intelligence with transformers. Code will be
available at https://github.com/invictus717/MetaTransformer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10803">
<title>Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10803</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing amount of spatial-temporal~(ST) ocean data, numerous
spatial-temporal data mining (STDM) studies have been conducted to address
various oceanic issues, e.g., climate forecasting and disaster warning.
Compared with typical ST data (e.g., traffic data), ST ocean data is more
complicated with some unique characteristics, e.g., diverse regionality and
high sparsity. These characteristics make it difficult to design and train STDM
models. Unfortunately, an overview of these studies is still missing, hindering
computer scientists to identify the research issues in ocean while discouraging
researchers in ocean science from applying advanced STDM techniques. To remedy
this situation, we provide a comprehensive survey to summarize existing STDM
studies in ocean. Concretely, we first summarize the widely-used ST ocean
datasets and identify their unique characteristics. Then, typical ST ocean data
quality enhancement techniques are discussed. Next, we classify existing STDM
studies for ocean into four types of tasks, i.e., prediction, event detection,
pattern mining, and anomaly detection, and elaborate the techniques for these
tasks. Finally, promising research opportunities are highlighted. This survey
will help scientists from the fields of both computer science and ocean science
have a better understanding of the fundamental concepts, key techniques, and
open challenges of STDM in ocean.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hanchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wengen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jihong Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuigeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiannong Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10805">
<title>Communication-Efficient Split Learning via Adaptive Feature-Wise Compression. (arXiv:2307.10805v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.10805</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel communication-efficient split learning (SL)
framework, named SplitFC, which reduces the communication overhead required for
transmitting intermediate feature and gradient vectors during the SL training
process. The key idea of SplitFC is to leverage different dispersion degrees
exhibited in the columns of the matrices. SplitFC incorporates two compression
strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise
quantization. In the first strategy, the intermediate feature vectors are
dropped with adaptive dropout probabilities determined based on the standard
deviation of these vectors. Then, by the chain rule, the intermediate gradient
vectors associated with the dropped feature vectors are also dropped. In the
second strategy, the non-dropped intermediate feature and gradient vectors are
quantized using adaptive quantization levels determined based on the ranges of
the vectors. To minimize the quantization error, the optimal quantization
levels of this strategy are derived in a closed-form expression. Simulation
results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC
provides more than a 5.6% increase in classification accuracy compared to
state-of-the-art SL frameworks, while they require 320 times less communication
overhead compared to the vanilla SL framework without compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;Yongjeong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1&quot;&gt;Yo-Seb Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10810">
<title>On Combining Expert Demonstrations in Imitation Learning via Optimal Transport. (arXiv:2307.10810v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10810</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning (IL) seeks to teach agents specific tasks through expert
demonstrations. One of the key approaches to IL is to define a distance between
agent and expert and to find an agent policy that minimizes that distance.
Optimal transport methods have been widely used in imitation learning as they
provide ways to measure meaningful distances between agent and expert
trajectories. However, the problem of how to optimally combine multiple expert
demonstrations has not been widely studied. The standard method is to simply
concatenate state (-action) trajectories, which is problematic when
trajectories are multi-modal. We propose an alternative method that uses a
multi-marginal optimal transport distance and enables the combination of
multiple and diverse state-trajectories in the OT sense, providing a more
sensible geometric average of the demonstrations. Our approach enables an agent
to learn from several experts, and its efficiency is analyzed on OpenAI Gym
control environments and demonstrates that the standard method is not always
optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebag_I/0/1/0/all/0/1&quot;&gt;Ilana Sebag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1&quot;&gt;Samuel Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deisenroth_M/0/1/0/all/0/1&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10842">
<title>Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10842</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance of a pre-trained semantic segmentation model is likely to
substantially decrease on data from a new domain. We show a pre-trained model
can be adapted to unlabelled target domain data by calculating soft-label
prototypes under the domain shift and making predictions according to the
prototype closest to the vector with predicted class probabilities. The
proposed adaptation procedure is fast, comes almost for free in terms of
computational resources and leads to considerable performance improvements. We
demonstrate the benefits of such label calibration on the highly-practical
synthetic-to-real semantic segmentation problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Da Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10843">
<title>Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10843</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning architecture for nowcasting of
precipitation almost globally every 30 min with a 4-hour lead time. The
architecture fuses a U-Net and a convolutional long short-term memory (LSTM)
neural network and is trained using data from the Integrated MultisatellitE
Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global
Forecast System (GFS). The impacts of different training loss functions,
including the mean-squared error (regression) and the focal-loss
(classification), on the quality of precipitation nowcasts are studied. The
results indicate that the regression network performs well in capturing light
precipitation (below 1.6 mm/hr), but the classification network can outperform
the regression network for nowcasting of precipitation extremes (&amp;gt;8 mm/hr), in
terms of the critical success index (CSI).. Using the Wasserstein distance, it
is shown that the predicted precipitation by the classification network has a
closer class probability distribution to the IMERG than the regression network.
It is uncovered that the inclusion of the physical variables can improve
precipitation nowcasting, especially at longer lead times in both networks.
Taking IMERG as a relative reference, a multi-scale analysis in terms of
fractions skill score (FSS), shows that the nowcasting machine remains skillful
(FSS &amp;gt; 0.5) at the resolution of 10 km compared to 50 km for GFS. For
precipitation rates greater than 4~mm/hr, only the classification network
remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1&quot;&gt;Reyhaneh Rahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebtehaj_A/0/1/0/all/0/1&quot;&gt;Ardeshir Ebtehaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behrangi_A/0/1/0/all/0/1&quot;&gt;Ali Behrangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jackson Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10845">
<title>Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10845</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning algorithms which keep the parameters of new tasks close to
that of previous tasks, are popular in preventing catastrophic forgetting in
sequential task learning settings. However, 1) the performance for the new
continual learner will be degraded without distinguishing the contributions of
previously learned tasks; 2) the computational cost will be greatly increased
with the number of tasks, since most existing algorithms need to regularize all
previous tasks when learning new tasks. To address the above challenges, we
propose a self-paced Weight Consolidation (spWC) framework to attain robust
continual learning via evaluating the discriminative contributions of previous
tasks. To be specific, we develop a self-paced regularization to reflect the
priorities of past tasks via measuring difficulty based on key performance
indicator (i.e., accuracy). When encountering a new task, all previous tasks
are sorted from &quot;difficult&quot; to &quot;easy&quot; based on the priorities. Then the
parameters of the new continual learner will be learned via selectively
maintaining the knowledge amongst more difficult past tasks, which could well
overcome catastrophic forgetting with less computational cost. We adopt an
alternative convex search to iteratively update the model parameters and
priority weights in the bi-convex formulation. The proposed spWC framework is
plug-and-play, which is applicable to most continual learning algorithms (e.g.,
EWC, MAS and RCIL) in different directions (e.g., classification and
segmentation). Experimental results on several public benchmark datasets
demonstrate that our proposed framework can effectively improve performance
when compared with other popular continual learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1&quot;&gt;Wei Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1&quot;&gt;Yang Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Gan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiahua Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10864">
<title>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10864</link>
<description rdf:parseType="Literal">&lt;p&gt;Emerging large-scale text-to-image generative models, e.g., Stable Diffusion
(SD), have exhibited overwhelming results with high fidelity. Despite the
magnificent progress, current state-of-the-art models still struggle to
generate images fully adhering to the input prompt. Prior work, Attend &amp;amp;
Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming
to optimize cross-attention during inference time to better incorporate the
semantics. It demonstrates promising results in generating simple prompts,
e.g., ``a cat and a dog&apos;&apos;. However, its efficacy declines when dealing with
more complex prompts, and it does not explicitly address the problem of
improper attribute binding. To address the challenges posed by complex prompts
or scenarios involving multiple entities and to achieve improved attribute
binding, we propose Divide &amp;amp; Bind. We introduce two novel loss objectives for
GSN: a novel attendance loss and a binding loss. Our approach stands out in its
ability to faithfully synthesize desired objects with improved attribute
alignment from complex prompts and exhibits superior performance across
multiple evaluation benchmarks. More videos and updates can be found on the
project page \url{https://sites.google.com/view/divide-and-bind}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yumeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1&quot;&gt;Anna Khoreva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10865">
<title>Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10865</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Persistence is a prominent measure for quantifying neural network
complexity, proposed in the emerging field of topological data analysis in deep
learning. In this work, however, we find both theoretically and empirically
that the variance of network weights and spatial concentration of large weights
are the main factors that impact neural persistence. Whilst this captures
useful information for linear classifiers, we find that no relevant spatial
structure is present in later layers of deep neural networks, making neural
persistence roughly equivalent to the variance of weights. Additionally, the
proposed averaging procedure across layers for deep neural networks does not
consider interaction between layers. Based on our analysis, we propose an
extension of the filtration underlying neural persistence to the whole neural
network instead of single layers, which is equivalent to calculating neural
persistence on one particular matrix. This yields our deep graph persistence
measure, which implicitly incorporates persistent paths through the network and
alleviates variance-related issues through standardisation. Code is available
at https://github.com/ExplainableML/Deep-Graph-Persistence .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girrbach_L/0/1/0/all/0/1&quot;&gt;Leander Girrbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1&quot;&gt;Anders Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1&quot;&gt;Ole Winther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1&quot;&gt;A. Sophia Koepke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10867">
<title>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.10867</link>
<description rdf:parseType="Literal">&lt;p&gt;Captions are crucial for understanding scientific visualizations and
documents. Existing captioning methods for scientific figures rely on
figure-caption pairs extracted from documents for training, many of which fall
short with respect to metrics like helpfulness, explainability, and
visual-descriptiveness [15] leading to generated captions being misaligned with
reader preferences. To enable the generation of high-quality figure captions,
we introduce FigCaps-HF a new framework for figure-caption generation that can
incorporate domain expert feedback in generating captions optimized for reader
preferences. Our framework comprises of 1) an automatic method for evaluating
quality of figure-caption pairs, 2) a novel reinforcement learning with human
feedback (RLHF) method to optimize a generative figure-to-caption model for
reader preferences. We demonstrate the effectiveness of our simple learning
framework by improving performance over standard fine-tuning across different
types of models. In particular, when using BLIP as the base model, our RLHF
framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and
Meteor, respectively. Finally, we release a large-scale benchmark dataset with
human feedback on figure-caption pairs to enable further evaluation and
development of RLHF techniques for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ashish Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1&quot;&gt;Prateek Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Arpita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursztyn_V/0/1/0/all/0/1&quot;&gt;Victor Bursztyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1&quot;&gt;Nikos Vlassis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10869">
<title>Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10869</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance issues permeate large-scale cloud service systems, which can lead
to huge revenue losses. To ensure reliable performance, it&apos;s essential to
accurately identify and localize these issues using service monitoring metrics.
Given the complexity and scale of modern cloud systems, this task can be
challenging and may require extensive expertise and resources beyond the
capacity of individual humans. Some existing methods tackle this problem by
analyzing each metric independently to detect anomalies. However, this could
incur overwhelming alert storms that are difficult for engineers to diagnose
manually. To pursue better performance, not only the temporal patterns of
metrics but also the correlation between metrics (i.e., relational patterns)
should be considered, which can be formulated as a multivariate metrics anomaly
detection problem. However, most of the studies fall short of extracting these
two types of features explicitly. Moreover, there exist some unlabeled
anomalies mixed in the training data, which may hinder the detection
performance. To address these limitations, we propose the Relational- Temporal
Anomaly Detection Model (RTAnomaly) that combines the relational and temporal
information of metrics. RTAnomaly employs a graph attention layer to learn the
dependencies among metrics, which will further help pinpoint the anomalous
metrics that may cause the anomaly effectively. In addition, we exploit the
concept of positive unlabeled learning to address the issue of potential
anomalies in the training data. To evaluate our method, we conduct experiments
on a public dataset and two industrial datasets. RTAnomaly outperforms all the
baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920,
demonstrating its superiority.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Wenwei Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuangbin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yuxin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiazhen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Cong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zengyin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10870">
<title>Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.10870</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent theoretical works on \emph{meta-learning} aim to achieve
guarantees in leveraging similar representational structures from related tasks
towards simplifying a target task. Importantly, the main aim in theory works on
the subject is to understand the extent to which convergence rates -- in
learning a common representation -- \emph{may scale with the number $N$ of
tasks} (as well as the number of samples per task). First steps in this setting
demonstrate this property when both the shared representation amongst tasks,
and task-specific regression functions, are linear. This linear setting readily
reveals the benefits of aggregating tasks, e.g., via averaging arguments. In
practice, however, the representation is often highly nonlinear, introducing
nontrivial biases in each task that cannot easily be averaged out as in the
linear case. In the present work, we derive theoretical guarantees for
meta-learning with nonlinear representations. In particular, assuming the
shared nonlinearity maps to an infinite-dimensional RKHS, we show that
additional biases can be mitigated with careful regularization that leverages
the smoothness of task-specific regression functions,
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1&quot;&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kpotufe_S/0/1/0/all/0/1&quot;&gt;Samory Kpotufe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10875">
<title>Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10875</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of point cloud deep models for safety-critical purposes has
increased, but the reliability and security of these models can be compromised
by intentional or naturally occurring point cloud noise. To combat this issue,
we present a novel point cloud outlier removal method called PointCVaR, which
empowers standard-trained models to eliminate additional outliers and restore
the data. Our approach begins by conducting attribution analysis to determine
the influence of each point on the model output, which we refer to as point
risk. We then optimize the process of filtering high-risk points using
Conditional Value at Risk (CVaR) as the objective. The rationale for this
approach is based on the observation that noise points in point clouds tend to
cluster in the tail of the risk distribution, with a low frequency but a high
level of risk, resulting in significant interference with classification
results. Despite requiring no additional training effort, our method produces
exceptional results in various removal-and-classification experiments for noisy
point clouds, which are corrupted by random noise, adversarial noise, and
backdoor trigger noise. Impressively, it achieves 87% accuracy in defense
against the backdoor attack by removing triggers. Overall, the proposed
PointCVaR effectively eliminates noise points and enhances point cloud
classification, making it a promising plug-in module for various models in
different scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Junchi Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10890">
<title>Player-optimal Stable Regret for Bandit Learning in Matching Markets. (arXiv:2307.10890v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.10890</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of matching markets has been studied for a long time in the
literature due to its wide range of applications. Finding a stable matching is
a common equilibrium objective in this problem. Since market participants are
usually uncertain of their preferences, a rich line of recent works study the
online setting where one-side participants (players) learn their unknown
preferences from iterative interactions with the other side (arms). Most
previous works in this line are only able to derive theoretical guarantees for
player-pessimal stable regret, which is defined compared with the players&apos;
least-preferred stable matching. However, under the pessimal stable matching,
players only obtain the least reward among all stable matchings. To maximize
players&apos; profits, player-optimal stable matching would be the most desirable.
Though \citet{basu21beyond} successfully bring an upper bound for
player-optimal stable regret, their result can be exponentially large if
players&apos; preference gap is small. Whether a polynomial guarantee for this
regret exists is a significant but still open problem. In this work, we provide
a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the
optimal stable regret of each player can be upper bounded by $O(K\log
T/\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\Delta$
is the players&apos; minimum preference gap among the first $N+1$-ranked arms. This
result significantly improves previous works which either have a weaker
player-pessimal stable matching objective or apply only to markets with special
assumptions. When the preferences of participants satisfy some special
conditions, our regret upper bound also matches the previously derived lower
bound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10891">
<title>Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2307.10891</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstraction is a key verification technique to improve scalability. However,
its use for neural networks is so far extremely limited. Previous approaches
for abstracting classification networks replace several neurons with one of
them that is similar enough. We can classify the similarity as defined either
syntactically (using quantities on the connections between neurons) or
semantically (on the activation values of neurons for various inputs).
Unfortunately, the previous approaches only achieve moderate reductions, when
implemented at all. In this work, we provide a more flexible framework where a
neuron can be replaced with a linear combination of other neurons, improving
the reduction. We apply this approach both on syntactic and semantic
abstractions, and implement and evaluate them experimentally. Further, we
introduce a refinement method for our abstractions, allowing for finding a
better balance between reduction and precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_C/0/1/0/all/0/1&quot;&gt;Calvin Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretinsky_J/0/1/0/all/0/1&quot;&gt;Jan K&amp;#x159;et&amp;#xed;nsk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohr_S/0/1/0/all/0/1&quot;&gt;Stefanie Mohr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.01052">
<title>How to choose the most appropriate centrality measure? A decision tree approach. (arXiv:2003.01052v5 [physics.soc-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2003.01052</link>
<description rdf:parseType="Literal">&lt;p&gt;Centrality metrics are vital for network analysis, but selecting the most
appropriate measures for specific applications remains challenging among the
400+ proposed indices. Existing approaches -- model-based, data-driven, and
axiomatic -- have limitations. To address this, we introduce the culling
method, leveraging expert preferences regarding centrality behavior on simple
graphs. It involves forming a set of candidate measures, generating a list of
as small graphs as possible needed to ``separate&apos;&apos; measures from each other,
constructing a decision-tree survey, and identifying the measure consistent
with expert responses. We apply this method to a diverse set of 40
centralities, including new kernel-based measures, and combine it with the
axiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all
40 measures, among which there are pairs of close ones. The culling method
offers a low-cost solution in terms of labor and time, complements existing
methods for measure selection, and reveals important peculiarities of
centrality measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chebotarev_P/0/1/0/all/0/1&quot;&gt;Pavel Chebotarev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gubanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Gubanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.03259">
<title>Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2009.03259</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a visualization method to understand the effect of
multidimensional projection on local subspaces, using implicit function
differentiation. Here, we understand the local subspace as the multidimensional
local neighborhood of data points. Existing methods focus on the projection of
multidimensional data points, and the neighborhood information is ignored. Our
method is able to analyze the shape and directional information of the local
subspace to gain more insights into the global structure of the data through
the perception of local structures. Local subspaces are fitted by
multidimensional ellipses that are spanned by basis vectors. An accurate and
efficient vector transformation method is proposed based on analytical
differentiation of multidimensional projections formulated as implicit
functions. The results are visualized as glyphs and analyzed using a full set
of specifically-designed interactions supported in our efficient web-based
visualization tool. The usefulness of our method is demonstrated using various
multi- and high-dimensional benchmark datasets. Our implicit differentiation
vector transformation is evaluated through numerical comparisons; the overall
method is evaluated through exploration examples and use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_R/0/1/0/all/0/1&quot;&gt;Rongzheng Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yumeng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiskopf_D/0/1/0/all/0/1&quot;&gt;Daniel Weiskopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.07881">
<title>Perceptron Theory Can Predict the Accuracy of Neural Networks. (arXiv:2012.07881v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2012.07881</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilayer neural networks set the current state of the art for many
technical classification problems. But, these networks are still, essentially,
black boxes in terms of analyzing them and predicting their performance. Here,
we develop a statistical theory for the one-layer perceptron and show that it
can predict performances of a surprisingly large variety of neural networks
with different architectures. A general theory of classification with
perceptrons is developed by generalizing an existing theory for analyzing
reservoir computing models and connectionist models for symbolic reasoning
known as vector symbolic architectures. Our statistical theory offers three
formulas leveraging the signal statistics with increasing detail. The formulas
are analytically intractable, but can be evaluated numerically. The description
level that captures maximum details requires stochastic sampling methods.
Depending on the network model, the simpler formulas already yield high
prediction accuracy. The quality of the theory predictions is assessed in three
experimental settings, a memorization task for echo state networks (ESNs) from
reservoir computing literature, a collection of classification datasets for
shallow randomly connected networks, and the ImageNet dataset for deep
convolutional neural networks. We find that the second description level of the
perceptron theory can predict the performance of types of ESNs, which could not
be described previously. The theory can predict deep multilayer neural networks
by being applied to their output layer. While other methods for prediction of
neural networks performance commonly require to train an estimator model, the
proposed theory requires only the first two moments of the distribution of the
postsynaptic sums in the output neurons. The perceptron theory compares
favorably to other methods that do not rely on training an estimator model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1&quot;&gt;Denis Kleyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1&quot;&gt;Antonello Rosato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frady_E/0/1/0/all/0/1&quot;&gt;E. Paxon Frady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1&quot;&gt;Massimo Panella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1&quot;&gt;Friedrich T. Sommer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.03403">
<title>Robust Principal Component Analysis: A Median of Means Approach. (arXiv:2102.03403v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2102.03403</link>
<description rdf:parseType="Literal">&lt;p&gt;Principal Component Analysis (PCA) is a fundamental tool for data
visualization, denoising, and dimensionality reduction. It is widely popular in
Statistics, Machine Learning, Computer Vision, and related fields. However, PCA
is well-known to fall prey to outliers and often fails to detect the true
underlying low-dimensional structure within the dataset. Following the Median
of Means (MoM) philosophy, recent supervised learning methods have shown great
success in dealing with outlying observations without much compromise to their
large sample theoretical properties. This paper proposes a PCA procedure based
on the MoM principle. Called the \textbf{M}edian of \textbf{M}eans
\textbf{P}rincipal \textbf{C}omponent \textbf{A}nalysis (MoMPCA), the proposed
method is not only computationally appealing but also achieves optimal
convergence rates under minimal assumptions. In particular, we explore the
non-asymptotic error bounds of the obtained solution via the aid of the
Rademacher complexities while granting absolutely no assumption on the outlying
observations. The derived concentration results are not dependent on the
dimension because the analysis is conducted in a separable Hilbert space, and
the results only depend on the fourth moment of the underlying distribution in
the corresponding norm. The proposal&apos;s efficacy is also thoroughly showcased
through simulations and real data applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Paul_D/0/1/0/all/0/1&quot;&gt;Debolina Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Saptarshi Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Swagatam Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.11166">
<title>AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v6 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2105.11166</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art performance for many edge applications is achieved by deep
neural networks (DNNs). Often, these DNNs are location- and time-sensitive, and
must be delivered over a wireless channel rapidly and efficiently. In this
paper, we introduce AirNet, a family of novel training and transmission methods
that allow DNNs to be efficiently delivered over wireless channels under
stringent transmit power and latency constraints. This corresponds to a new
class of joint source-channel coding problems, aimed at delivering DNNs with
the goal of maximizing their accuracy at the receiver, rather than recovering
them with high fidelity. In AirNet, we propose the direct mapping of the DNN
parameters to transmitted channel symbols, while the network is trained to meet
the channel constraints, and exhibit robustness against channel noise. AirNet
achieves higher accuracy compared to separation-based alternatives. We further
improve the performance of AirNet by pruning the network below the available
bandwidth, and expanding it for improved robustness. We also benefit from
unequal error protection by selectively expanding important layers of the
network. Finally, we develop an approach, which simultaneously trains a
spectrum of DNNs, each targeting a different channel condition, resolving the
impractical memory requirements of training distinct networks for different
channel conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1&quot;&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.01001">
<title>Warming up recurrent neural networks to maximise reachable multistability greatly improves learning. (arXiv:2106.01001v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;Training recurrent neural networks is known to be difficult when time
dependencies become long. In this work, we show that most standard cells only
have one stable equilibrium at initialisation, and that learning on tasks with
long time dependencies generally occurs once the number of network stable
equilibria increases; a property known as multistability. Multistability is
often not easily attained by initially monostable networks, making learning of
long time dependencies between inputs and outputs difficult. This insight leads
to the design of a novel way to initialise any recurrent cell connectivity
through a procedure called &quot;warmup&quot; to improve its capability to learn
arbitrarily long time dependencies. This initialisation procedure is designed
to maximise network reachable multistability, i.e., the number of equilibria
within the network that can be reached through relevant input trajectories, in
few gradient steps. We show on several information restitution, sequence
classification, and reinforcement learning benchmarks that warming up greatly
improves learning speed and performance, for multiple recurrent cells, but
sometimes impedes precision. We therefore introduce a double-layer architecture
initialised with a partial warmup that is shown to greatly improve learning of
long time dependencies while maintaining high levels of precision. This
approach provides a general framework for improving learning abilities of any
recurrent cell when long time dependencies are present. We also show
empirically that other initialisation and pretraining procedures from the
literature implicitly foster reachable multistability of recurrent cells.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambrechts_G/0/1/0/all/0/1&quot;&gt;Gaspard Lambrechts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geeter_F/0/1/0/all/0/1&quot;&gt;Florent De Geeter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vecoven_N/0/1/0/all/0/1&quot;&gt;Nicolas Vecoven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1&quot;&gt;Damien Ernst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drion_G/0/1/0/all/0/1&quot;&gt;Guillaume Drion&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.03455">
<title>Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2107.03455</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of model selection for the general stochastic
contextual bandits under the realizability assumption. We propose a successive
refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}),
that works in phases and successively eliminates model classes that are too
simple to fit the given instance. We prove that this algorithm is adaptive,
i.e., the regret rate order-wise matches that of any provable contextual bandit
algorithm (ex. \cite{falcon}), that needs the knowledge of the true model
class. The price of not knowing the correct model class turns out to be only an
additive term contributing to the second order term in the regret bound. This
cost possess the intuitive property that it becomes smaller as the model class
becomes easier to identify, and vice-versa. We also show that a much simpler
explore-then-commit (ETC) style algorithm also obtains similar regret bound,
despite not knowing the true model class. However, the cost of model selection
is higher in ETC as opposed to in {\ttfamily ACB}, as expected. Furthermore,
for the special case of linear contextual bandits, we propose specialized
algorithms that obtain sharper guarantees compared to the generic setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sankararaman_A/0/1/0/all/0/1&quot;&gt;Abishek Sankararaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1&quot;&gt;Kannan Ramchandran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.12509">
<title>Deep Exploration for Recommendation Systems. (arXiv:2109.12509v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2109.12509</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern recommendation systems ought to benefit by probing for and learning
from delayed feedback. Research has tended to focus on learning from a user&apos;s
response to a single recommendation. Such work, which leverages methods of
supervised and bandit learning, forgoes learning from the user&apos;s subsequent
behavior. Where past work has aimed to learn from subsequent behavior, there
has been a lack of effective methods for probing to elicit informative delayed
feedback. Effective exploration through probing for delayed feedback becomes
particularly challenging when rewards are sparse. To address this, we develop
deep exploration methods for recommendation systems. In particular, we
formulate recommendation as a sequential decision problem and demonstrate
benefits of deep exploration over single-step exploration. Our experiments are
carried out with high-fidelity industrial-grade simulators and establish large
improvements over existing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.05216">
<title>High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.05216</link>
<description rdf:parseType="Literal">&lt;p&gt;We aim at capturing high-order statistics of feature vectors formed by a
neural network, and propose end-to-end second- and higher-order pooling to form
a tensor descriptor. Tensor descriptors require a robust similarity measure due
to low numbers of aggregated vectors and the burstiness phenomenon, when a
given feature appears more/less frequently than statistically expected. The
Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the
Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,
whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN
play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum
thus preventing the burstiness. We equip higher-order tensors with EPN which
acts as a spectral detector of higher-order occurrences to prevent burstiness.
We also prove that for a tensor of order r built from d dimensional feature
descriptors, such a detector gives the likelihood if at least one higher-order
occurrence is &apos;projected&apos; into one of binom(d,r) subspaces represented by the
tensor; thus forming a tensor power normalization metric endowed with
binom(d,r) such &apos;detectors&apos;. For experimental contributions, we apply several
second- and higher-order pooling variants to action recognition, provide
previously not presented comparisons of such pooling variants, and show
state-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.03950">
<title>Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2111.03950</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose simple nonparametric estimators for mediated and time-varying dose
response curves based on kernel ridge regression. By embedding Pearl&apos;s
mediation formula and Robins&apos; g-formula with kernels, we allow treatments,
mediators, and covariates to be continuous in general spaces, and also allow
for nonlinear treatment-confounder feedback. Our key innovation is a
reproducing kernel Hilbert space technique called sequential kernel embedding,
which we use to construct simple estimators for complex causal estimands. Our
estimators preserve the generality of classic identification while also
achieving nonasymptotic uniform rates. In nonlinear simulations with many
covariates, we demonstrate strong performance. We estimate mediated and
time-varying dose response curves of the US Job Corps, and clean data that may
serve as a benchmark in future work. We extend our results to mediated and
time-varying treatment effects and counterfactual distributions, verifying
semiparametric efficiency and weak convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rahul Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1&quot;&gt;Arthur Gretton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.08982">
<title>PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting. (arXiv:2202.08982v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.08982</link>
<description rdf:parseType="Literal">&lt;p&gt;The complex spatial-temporal correlations in transportation networks make the
traffic forecasting problem challenging. Since transportation system inherently
possesses graph structures, much research efforts have been put with graph
neural networks. Recently, constructing adaptive graphs to the data has shown
promising results over the models relying on a single static graph structure.
However, the graph adaptations are applied during the training phases, and do
not reflect the data used during the testing phases. Such shortcomings can be
problematic especially in traffic forecasting since the traffic data often
suffers from the unexpected changes and irregularities in the time series. In
this study, we propose a novel traffic forecasting framework called Progressive
Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by
progressively adapting to input data during the training and the testing
phases. Specifically, we implemented the model to construct progressive
adjacency matrices by learning trend similarities among graph nodes. Then, the
model is combined with the dilated causal convolution and gated activation unit
to extract temporal features. With residual and skip connections, PGCN performs
the traffic prediction. When applied to four real-world traffic datasets of
diverse geometric nature, the proposed model achieves state-of-the-art
performance with consistency in all datasets. We conclude that the ability of
PGCN to progressively adapt to input data enables the model to generalize in
different study sites with robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yuyol Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1&quot;&gt;Yoonjin Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.06362">
<title>A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic. (arXiv:2204.06362v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.06362</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of Machine Learning (ML) has rapidly spread across several fields,
having encountered many applications in Structural Dynamics and Vibroacoustic
(SD\&amp;amp;V). The increasing capabilities of ML to unveil insights from data, driven
by unprecedented data availability, algorithms advances and computational
power, enhance decision making, uncertainty handling, patterns recognition and
real-time assessments. Three main applications in SD\&amp;amp;V have taken advantage of
these benefits. In Structural Health Monitoring, ML detection and prognosis
lead to safe operation and optimized maintenance schedules. System
identification and control design are leveraged by ML techniques in Active
Noise Control and Active Vibration Control. Finally, the so-called ML-based
surrogate models provide fast alternatives to costly simulations, enabling
robust and optimized product design. Despite the many works in the area, they
have not been reviewed and analyzed. Therefore, to keep track and understand
this ongoing integration of fields, this paper presents a survey of ML
applications in SD\&amp;amp;V analyses, shedding light on the current state of
implementation and emerging opportunities. The main methodologies, advantages,
limitations, and recommendations based on scientific knowledge were identified
for each of the three applications. Moreover, the paper considers the role of
Digital Twins and Physics Guided ML to overcome current challenges and power
future research progress. As a result, the survey provides a broad overview of
the present landscape of ML applied in SD\&amp;amp;V and guides the reader to an
advanced understanding of progress and prospects in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_B/0/1/0/all/0/1&quot;&gt;Barbara Cunha&lt;/a&gt; (LTDS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Droz_C/0/1/0/all/0/1&quot;&gt;Christophe Droz&lt;/a&gt; (I4S), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zine_A/0/1/0/all/0/1&quot;&gt;Abdelmalek Zine&lt;/a&gt; (ICJ), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foulard_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Foulard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichchou_M/0/1/0/all/0/1&quot;&gt;Mohamed Ichchou&lt;/a&gt; (LTDS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09208">
<title>Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures. (arXiv:2205.09208v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09208</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperdimensional computing (HD), also known as vector symbolic architectures
(VSA), is a framework for computing with distributed representations by
exploiting properties of random high-dimensional vector spaces. The commitment
of the scientific community to aggregate and disseminate research in this
particularly multidisciplinary area has been fundamental for its advancement.
Joining these efforts, we present Torchhd, a high-performance open source
Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and
serves as an efficient foundation for further research and application
development. The easy-to-use library builds on top of PyTorch and features
state-of-the-art HD/VSA functionality, clear documentation, and implementation
examples from well-known publications. Comparing publicly available code with
their corresponding Torchhd implementation shows that experiments can run up to
100x faster. Torchhd is available at:
https://github.com/hyperdimensional-computing/torchhd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heddes_M/0/1/0/all/0/1&quot;&gt;Mike Heddes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_I/0/1/0/all/0/1&quot;&gt;Igor Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verges_P/0/1/0/all/0/1&quot;&gt;Pere Verg&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1&quot;&gt;Denis Kleyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abraham_D/0/1/0/all/0/1&quot;&gt;Danny Abraham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Givargis_T/0/1/0/all/0/1&quot;&gt;Tony Givargis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolau_A/0/1/0/all/0/1&quot;&gt;Alexandru Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veidenbaum_A/0/1/0/all/0/1&quot;&gt;Alexander Veidenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.09753">
<title>HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2205.09753</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoding a driving scene into vector representations has been an essential
task for autonomous driving that can benefit downstream tasks e.g. trajectory
prediction. The driving scene often involves heterogeneous elements such as the
different types of objects (agents, lanes, traffic signs) and the semantic
relations between objects are rich and diverse. Meanwhile, there also exist
relativity across elements, which means that the spatial relation is a relative
concept and need be encoded in a ego-centric manner instead of in a global
coordinate system. Based on these observations, we propose Heterogeneous
Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a
heterogeneous graph with different types of nodes and edges. For heterogeneous
graph construction, we connect different types of nodes according to diverse
semantic relations. For spatial relation encoding, the coordinates of the node
as well as its in-edges are in the local node-centric coordinate system. For
the aggregation module in the graph neural network (GNN), we adopt the
transformer structure in a hierarchical way to fit the heterogeneous nature of
inputs. Experimental results show that HDGT achieves state-of-the-art
performance for the task of trajectory prediction, on INTERACTION Prediction
Challenge and Waymo Open Motion Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaosong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Penghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Li Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10060">
<title>The Unreasonable Effectiveness of Deep Evidential Regression. (arXiv:2205.10060v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10060</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a significant need for principled uncertainty reasoning in machine
learning systems as they are increasingly deployed in safety-critical domains.
A new approach with uncertainty-aware regression-based neural networks (NNs),
based on learning evidential distributions for aleatoric and epistemic
uncertainties, shows promise over traditional deterministic methods and typical
Bayesian NNs, notably with the capabilities to disentangle aleatoric and
epistemic uncertainties. Despite some empirical success of Deep Evidential
Regression (DER), there are important gaps in the mathematical foundation that
raise the question of why the proposed technique seemingly works. We detail the
theoretical shortcomings and analyze the performance on synthetic and
real-world data sets, showing that Deep Evidential Regression is a heuristic
rather than an exact uncertainty quantification. We go on to discuss
corrections and redefinitions of how aleatoric and epistemic uncertainties
should be extracted from NNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meinert_N/0/1/0/all/0/1&quot;&gt;Nis Meinert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gawlikowski_J/0/1/0/all/0/1&quot;&gt;Jakob Gawlikowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1&quot;&gt;Alexander Lavin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11498">
<title>Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval. (arXiv:2205.11498v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11498</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense retrieval overcome the lexical gap and has shown great success in
ad-hoc information retrieval (IR). Despite their success, dense retrievers are
expensive to serve across practical use cases. For use cases requiring to
search from millions of documents, the dense index becomes bulky and requires
high memory usage for storing the index. More recently, learning-to-hash (LTH)
techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby
reducing the memory requirement to efficiently store the dense index. LTH
techniques are supervised and finetune the retriever using a ranking loss. They
outperform their counterparts, i.e., traditional out-of-the-box vector
compression techniques such as PCA or PQ. A missing piece from prior work is
that existing techniques have been evaluated only in-domain, i.e., on a single
dataset such as MS MARCO. In our work, we evaluate LTH and vector compression
techniques for improving the downstream zero-shot retrieval accuracy of the
TAS-B dense retriever while maintaining efficiency at inference. Our results
demonstrate that, unlike prior work, LTH strategies when applied naively can
underperform the zero-shot TAS-B dense retriever on average by up to 14%
nDCG@10 on the BEIR benchmark. To solve this limitation, in our work, we
propose an easy yet effective solution of injecting domain adaptation with
existing supervised LTH techniques. We experiment with two well-known
unsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptation
injection technique can improve the downstream zero-shot retrieval
effectiveness for both BPR and JPQ variants of the TAS-B model by on average
11.5% and 8.2% nDCG@10 while both maintaining 32$\times$ memory efficiency and
14$\times$ and 2$\times$ speedup respectively in CPU retrieval latency on BEIR.
All our code, models, and data are publicly available at
https://github.com/thakur-nandan/income.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1&quot;&gt;Nandan Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1&quot;&gt;Nils Reimers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jimmy Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.12900">
<title>Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.12900</link>
<description rdf:parseType="Literal">&lt;p&gt;Training even moderately-sized generative models with differentially-private
stochastic gradient descent (DP-SGD) is difficult: the required level of noise
for reasonable levels of privacy is simply too large. We advocate instead
building off a good, relevant representation on an informative public dataset,
then learning to model the private data with that representation. In
particular, we minimize the maximum mean discrepancy (MMD) between private
target data and a generator&apos;s distribution, using a kernel based on perceptual
features learned from a public dataset. With the MMD, we can simply privatize
the data-dependent term once and for all, rather than introducing noise at each
step of optimization as in DP-SGD. Our algorithm allows us to generate
CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive
features in the distribution, far surpassing the current state of the art,
which mostly focuses on datasets such as MNIST and FashionMNIST at a large
$\epsilon \approx 10$. Our work introduces simple yet powerful foundations for
reducing the gap between private and non-private deep generative models. Our
code is available at \url{https://github.com/ParkLabML/DP-MEPF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harder_F/0/1/0/all/0/1&quot;&gt;Fredrik Harder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Asadabadi_M/0/1/0/all/0/1&quot;&gt;Milad Jalali Asadabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sutherland_D/0/1/0/all/0/1&quot;&gt;Danica J. Sutherland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Park_M/0/1/0/all/0/1&quot;&gt;Mijung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08309">
<title>Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case. (arXiv:2206.08309v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08309</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep generative models have attracted increasing interest
due to their capacity to model complex distributions. Among those models,
variational autoencoders have gained popularity as they have proven both to be
computationally efficient and yield impressive results in multiple fields.
Following this breakthrough, extensive research has been done in order to
improve the original publication, resulting in a variety of different VAE
models in response to different tasks. In this paper we present Pythae, a
versatile open-source Python library providing both a unified implementation
and a dedicated framework allowing straightforward, reproducible and reliable
use of generative autoencoder models. We then propose to use this library to
perform a case study benchmark where we present and compare 19 generative
autoencoder models representative of some of the main improvements on
downstream tasks such as image reconstruction, generation, classification,
clustering and interpolation. The open-source library can be found at
https://github.com/clementchadebec/benchmark_VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadebec_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Chadebec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_L/0/1/0/all/0/1&quot;&gt;Louis J. Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allassonniere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phanie Allassonni&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.01110">
<title>Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks. (arXiv:2207.01110v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2207.01110</link>
<description rdf:parseType="Literal">&lt;p&gt;Random noise arising from physical processes is an inherent characteristic of
measurements and a limiting factor for most signal processing and data analysis
tasks. Given the recent interest in generative adversarial networks (GANs) for
data-driven modeling, it is important to determine to what extent GANs can
faithfully reproduce noise in target data sets. In this paper, we present an
empirical investigation that aims to shed light on this issue for time series.
Namely, we assess two general-purpose GANs for time series that are based on
the popular deep convolutional GAN (DCGAN) architecture, a direct time-series
model and an image-based model that uses a short-time Fourier transform (STFT)
data representation. The GAN models are trained and quantitatively evaluated
using distributions of simulated noise time series with known ground-truth
parameters. Target time series distributions include a broad range of noise
types commonly encountered in physical measurements, electronics, and
communication systems: band-limited thermal noise, power law noise, shot noise,
and impulsive noise. We find that GANs are capable of learning many noise
types, although they predictably struggle when the GAN architecture is not well
suited to some aspects of the noise, e.g., impulsive time-series with extreme
outliers. Our findings provide insights into the capabilities and potential
limitations of current approaches to time-series GANs and highlight areas for
further research. In addition, our battery of tests provides a useful benchmark
to aid the development of deep generative models for time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wunderlich_A/0/1/0/all/0/1&quot;&gt;Adam Wunderlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sklar_J/0/1/0/all/0/1&quot;&gt;Jack Sklar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02575">
<title>Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02575</link>
<description rdf:parseType="Literal">&lt;p&gt;While much progress has been made in understanding the minimax sample
complexity of reinforcement learning (RL) -- the complexity of learning on the
&quot;worst-case&quot; instance -- such measures of complexity often do not capture the
true difficulty of learning. In practice, on an &quot;easy&quot; instance, we might hope
to achieve a complexity far better than that achievable on the worst-case
instance. In this work we seek to understand the &quot;instance-dependent&quot;
complexity of learning near-optimal policies (PAC RL) in the setting of RL with
linear function approximation. We propose an algorithm, \textsc{Pedel}, which
achieves a fine-grained instance-dependent measure of complexity, the first of
its kind in the RL with function approximation setting, thereby capturing the
difficulty of learning on each particular problem instance. Through an explicit
example, we show that \textsc{Pedel} yields provable gains over low-regret,
minimax-optimal algorithms and that such algorithms are unable to hit the
instance-optimal rate. Our approach relies on a novel online experiment
design-based procedure which focuses the exploration budget on the &quot;directions&quot;
most relevant to learning a near-optimal policy, and may be of independent
interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1&quot;&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12395">
<title>Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics. (arXiv:2207.12395v3 [stat.CO] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12395</link>
<description rdf:parseType="Literal">&lt;p&gt;The tuning of stochastic gradient algorithms (SGAs) for optimization and
sampling is often based on heuristics and trial-and-error rather than
generalizable theory. We address this theory--practice gap by characterizing
the large-sample statistical asymptotics of SGAs via a joint
step-size--sample-size scaling limit. We show that iterate averaging with a
large fixed step size is robust to the choice of tuning parameters and
asymptotically has covariance proportional to that of the MLE sampling
distribution. We also prove a Bernstein--von Mises-like theorem to guide
tuning, including for generalized posteriors that are robust to model
misspecification. Numerical experiments validate our results and
recommendations in realistic finite-sample regimes. Our work lays the
foundation for a systematic analysis of other stochastic gradient Markov chain
Monte Carlo algorithms for a wide range of models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Negrea_J/0/1/0/all/0/1&quot;&gt;Jeffrey Negrea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Haoyue Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Daniel M. Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huggins_J/0/1/0/all/0/1&quot;&gt;Jonathan H. Huggins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12877">
<title>Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12877</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the successes of deep learning, we propose a class of neural
network-based discrete choice models, called RUMnets, inspired by the random
utility maximization (RUM) framework. This model formulates the agents&apos; random
utility function using a sample average approximation. We show that RUMnets
sharply approximate the class of RUM discrete choice models: any model derived
from random utility maximization has choice probabilities that can be
approximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet is
consistent with the RUM principle. We derive an upper bound on the
generalization error of RUMnets fitted on choice data, and gain theoretical
insights on their ability to predict choices on new, unseen data depending on
critical parameters of the dataset and architecture. By leveraging open-source
libraries for neural networks, we find that RUMnets are competitive against
several choice modeling and machine learning methods in terms of predictive
accuracy on two real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouad_A/0/1/0/all/0/1&quot;&gt;Ali Aouad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desir_A/0/1/0/all/0/1&quot;&gt;Antoine D&amp;#xe9;sir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06501">
<title>ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs. (arXiv:2208.06501v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06501</link>
<description rdf:parseType="Literal">&lt;p&gt;Question answering over temporal knowledge graphs (TKGQA) has recently found
increasing interest. TKGQA requires temporal reasoning techniques to extract
the relevant information from temporal knowledge bases. The only existing TKGQA
dataset, i.e., CronQuestions, consists of temporal questions based on the facts
from a fixed time period, where a temporal knowledge graph (TKG) spanning the
same period can be fully used for answer inference, allowing the TKGQA models
to use even the future knowledge to answer the questions based on the past
facts. In real-world scenarios, however, it is also common that given the
knowledge until now, we wish the TKGQA systems to answer the questions asking
about the future. As humans constantly seek plans for the future, building
TKGQA systems for answering such forecasting questions is important.
Nevertheless, this has still been unexplored in previous research. In this
paper, we propose a novel task: forecasting question answering over temporal
knowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e.,
ForecastTKGQuestions, for this task. It includes three types of questions,
i.e., entity prediction, yes-no, and fact reasoning questions. For every
forecasting question in our dataset, QA models can only have access to the TKG
information before the timestamp annotated in the given question for answer
inference. We find that the state-of-the-art TKGQA methods perform poorly on
forecasting questions, and they are unable to answer yes-no questions and fact
reasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model that
employs a TKG forecasting module for future inference, to answer all three
types of questions. Experimental results show that ForecastTKGQA outperforms
recent TKGQA methods on the entity prediction questions, and it also shows
great effectiveness in answering the other two types of questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zifeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_R/0/1/0/all/0/1&quot;&gt;Ruoxia Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingpei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1&quot;&gt;Bailan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Ruotong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06620">
<title>Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions. (arXiv:2208.06620v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06620</link>
<description rdf:parseType="Literal">&lt;p&gt;Online extremism has severe societal consequences, including normalizing hate
speech, user radicalization, and increased social divisions. Various mitigation
strategies have been explored to address these consequences. One such strategy
uses positive interventions: controlled signals that add attention to the
opinion ecosystem to boost certain opinions. To evaluate the effectiveness of
positive interventions, we introduce the Opinion Market Model (OMM), a two-tier
online opinion ecosystem model that considers both inter-opinion interactions
and the role of positive interventions. The size of the opinion attention
market is modeled in the first tier using the multivariate discrete-time Hawkes
process; in the second tier, opinions cooperate and compete for market share,
given limited attention using the market share attraction model. We demonstrate
the convergence of our proposed estimation scheme on a synthetic dataset. Next,
we test OMM on two learning tasks, applying to two real-world datasets to
predict attention market shares and uncover latent relationships between online
items. The first dataset comprises Facebook and Twitter discussions containing
moderate and far-right opinions about bushfires and climate change. The second
dataset captures popular VEVO artists&apos; YouTube and Twitter attention volumes.
OMM outperforms the state-of-the-art predictive models on both datasets and
captures latent cooperation-competition relations. We uncover (1) self- and
cross-reinforcement between far-right and moderate opinions on the bushfires
and (2) pairwise artist relations that correlate with real-world interactions
such as collaborations and long-lasting feuds. Lastly, we use OMM as a testbed
for positive interventions and show how media coverage modulates the spread of
far-right opinions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderon_P/0/1/0/all/0/1&quot;&gt;Pio Calderon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_R/0/1/0/all/0/1&quot;&gt;Rohit Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizoiu_M/0/1/0/all/0/1&quot;&gt;Marian-Andrei Rizoiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09418">
<title>SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09418</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.
Despite great efforts made by the Explainable AI (XAI) community, explanations
lack robustness -- indistinguishable input perturbations may lead to different
XAI results. Thus, it is vital to assess how robust DL interpretability is,
given an XAI method. In this paper, we identify several challenges that the
state-of-the-art is unable to cope with collectively: i) existing metrics are
not comprehensive; ii) XAI techniques are highly heterogeneous; iii)
misinterpretations are normally rare events. To tackle these challenges, we
introduce two black-box evaluation methods, concerning the worst-case
interpretation discrepancy and a probabilistic notion of how robust in general,
respectively. Genetic Algorithm (GA) with bespoke fitness function is used to
solve constrained optimisation for efficient worst-case evaluation. Subset
Simulation (SS), dedicated to estimate rare event probabilities, is used for
evaluating overall robustness. Experiments show that the accuracy, sensitivity,
and efficiency of our methods outperform the state-of-the-arts. Finally, we
demonstrate two applications of our methods: ranking robust XAI methods and
selecting training schemes to improve both classification and interpretation
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1&quot;&gt;Gaojie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10224">
<title>Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v4 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10224</link>
<description rdf:parseType="Literal">&lt;p&gt;A powerful category of (invisible) data poisoning attacks modify a subset of
training examples by small adversarial perturbations to change the prediction
of certain test-time data. Existing defense mechanisms are not desirable to
deploy in practice, as they often either drastically harm the generalization
performance, or are attack-specific, and prohibitively slow to apply. Here, we
propose a simple but highly effective approach that unlike existing methods
breaks various types of invisible poisoning attacks with the slightest drop in
the generalization performance. We make the key observation that attacks
introduce local sharp regions of high training loss, which when minimized,
results in learning the adversarial perturbations and makes the attack
successful. To break poisoning attacks, our key idea is to alleviate the sharp
loss regions introduced by poisons. To do so, our approach comprises two
components: an optimized friendly noise that is generated to maximally perturb
examples without degrading the performance, and a randomly varying noise
component. The combination of both components builds a very light-weight but
extremely effective defense against the most powerful triggerless targeted and
hidden-trigger backdoor poisoning attacks, including Gradient Matching,
Bulls-eye Polytope, and Sleeper Agent. We show that our friendly noise is
transferable to other architectures, and adaptive attacks cannot break our
defense due to its random noise component. Our code is available at:
https://github.com/tianyu139/friendly-noise
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1&quot;&gt;Baharan Mirzasoleiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07902">
<title>MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07902</link>
<description rdf:parseType="Literal">&lt;p&gt;As a successful approach to self-supervised learning, contrastive learning
aims to learn invariant information shared among distortions of the input
sample. While contrastive learning has yielded continuous advancements in
sampling strategy and architecture design, it still remains two persistent
defects: the interference of task-irrelevant information and sample
inefficiency, which are related to the recurring existence of trivial constant
solutions. From the perspective of dimensional analysis, we find out that the
dimensional redundancy and dimensional confounder are the intrinsic issues
behind the phenomena, and provide experimental evidence to support our
viewpoint. We further propose a simple yet effective approach MetaMask, short
for the dimensional Mask learned by Meta-learning, to learn representations
against dimensional redundancy and confounder. MetaMask adopts the
redundancy-reduction technique to tackle the dimensional redundancy issue and
innovatively introduces a dimensional mask to reduce the gradient effects of
specific dimensions containing the confounder, which is trained by employing a
meta-learning paradigm with the objective of improving the performance of
masked representations on a typical self-supervised task. We provide solid
theoretical analyses to prove MetaMask can obtain tighter risk bounds for
downstream classification compared to typical contrastive methods. Empirically,
our method achieves state-of-the-art performance on various benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_W/0/1/0/all/0/1&quot;&gt;Wenyi Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01834">
<title>Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01834</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is gaining popularity as it enables training high-utility
models across several clients without directly sharing their private data. As a
downside, the federated setting makes the model vulnerable to various
adversarial attacks in the presence of malicious clients. Despite the
theoretical and empirical success in defending against attacks that aim to
degrade models&apos; utility, defense against backdoor attacks that increase model
accuracy on backdoor samples exclusively without hurting the utility on other
samples remains challenging. To this end, we first analyze the vulnerability of
federated learning to backdoor attacks over a flat loss landscape which is
common for well-designed neural networks such as Resnet [He et al., 2015] but
is often overlooked by previous works. Over a flat loss landscape, misleading
federated learning models to exclusively benefit malicious clients with
backdoor samples do not require a significant difference between malicious and
benign client-wise updates, making existing defenses insufficient. In contrast,
we propose an invariant aggregator that redirects the aggregated update to
invariant directions that are generally useful via selectively masking out the
gradient elements that favor few and possibly malicious clients regardless of
the difference magnitude. Theoretical results suggest that our approach
provably mitigates backdoor attacks over both flat and sharp loss landscapes.
Empirical results on three datasets with different modalities and varying
numbers of clients further demonstrate that our approach mitigates a broad
class of backdoor attacks with a negligible cost on the model utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Dimitriadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1&quot;&gt;Shruti Tople&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06089">
<title>When are Local Queries Useful for Robust Learning?. (arXiv:2210.06089v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06089</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributional assumptions have been shown to be necessary for the robust
learnability of concept classes when considering the exact-in-the-ball robust
risk and access to random examples by Gourdeau et al. (2019). In this paper, we
study learning models where the learner is given more power through the use of
local queries, and give the first distribution-free algorithms that perform
robust empirical risk minimization (ERM) for this notion of robustness. The
first learning model we consider uses local membership queries (LMQ), where the
learner can query the label of points near the training sample. We show that,
under the uniform distribution, LMQs do not increase the robustness threshold
of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced
with this negative result, we introduce the local equivalence query
($\mathsf{LEQ}$) oracle, which returns whether the hypothesis and target
concept agree in the perturbation region around a point in the training sample,
as well as a counterexample if it exists. We show a separation result: on the
one hand, if the query radius $\lambda$ is strictly smaller than the
adversary&apos;s perturbation budget $\rho$, then distribution-free robust learning
is impossible for a wide variety of concept classes; on the other hand, the
setting $\lambda=\rho$ allows us to develop robust ERM algorithms. We then
bound the query complexity of these algorithms based on online learning
guarantees and further improve these bounds for the special case of
conjunctions. We finish by giving robust learning algorithms for halfspaces on
$\{0,1\}^n$ and then obtaining robustness guarantees for halfspaces in
$\mathbb{R}^n$ against precision-bounded adversaries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gourdeau_P/0/1/0/all/0/1&quot;&gt;Pascale Gourdeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1&quot;&gt;Varun Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1&quot;&gt;Marta Kwiatkowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worrell_J/0/1/0/all/0/1&quot;&gt;James Worrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08363">
<title>Data-Efficient Augmentation for Training Neural Networks. (arXiv:2210.08363v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08363</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is essential to achieve state-of-the-art performance in
many deep learning applications. However, the most effective augmentation
techniques become computationally prohibitive for even medium-sized datasets.
To address this, we propose a rigorous technique to select subsets of data
points that when augmented, closely capture the training dynamics of full data
augmentation. We first show that data augmentation, modeled as additive
perturbations, improves learning and generalization by relatively enlarging and
perturbing the smaller singular values of the network Jacobian, while
preserving its prominent directions. This prevents overfitting and enhances
learning the harder to learn information. Then, we propose a framework to
iteratively extract small subsets of training data that when augmented, closely
capture the alignment of the fully augmented Jacobian with labels/residuals. We
prove that stochastic gradient descent applied to the augmented subsets found
by our approach has similar training dynamics to that of fully augmented data.
Our experiments demonstrate that our method achieves 6.3x speedup on CIFAR10
and 2.2x speedup on SVHN, and outperforms the baselines by up to 10% across
various subset sizes. Similarly, on TinyImageNet and ImageNet, our method beats
the baselines by up to 8%, while achieving up to 3.3x speedup across various
subset sizes. Finally, training on and augmenting 50% subsets using our method
on a version of CIFAR10 corrupted with label noise even outperforms using the
full dataset. Our code is available at:
https://github.com/tianyu139/data-efficient-augmentation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1&quot;&gt;Baharan Mirzasoleiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16299">
<title>Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16299</link>
<description rdf:parseType="Literal">&lt;p&gt;A key challenge in solving the deterministic inverse reinforcement learning
(IRL) problem online and in real-time is the existence of multiple solutions.
Nonuniqueness necessitates the study of the notion of equivalent solutions,
i.e., solutions that result in a different cost functional but same feedback
matrix, and convergence to such solutions. While offline algorithms that result
in convergence to equivalent solutions have been developed in the literature,
online, real-time techniques that address nonuniqueness are not available. In
this paper, a regularized history stack observer that converges to
approximately equivalent solutions of the IRL problem is developed. Novel
data-richness conditions are developed to facilitate the analysis and
simulation results are provided to demonstrate the effectiveness of the
developed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Town_J/0/1/0/all/0/1&quot;&gt;Jared Town&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morrison_Z/0/1/0/all/0/1&quot;&gt;Zachary Morrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1&quot;&gt;Rushikesh Kamalapurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04974">
<title>Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04974</link>
<description rdf:parseType="Literal">&lt;p&gt;Two central paradigms have emerged in the reinforcement learning (RL)
community: online RL and offline RL. In the online RL setting, the agent has no
prior knowledge of the environment, and must interact with it in order to find
an $\epsilon$-optimal policy. In the offline RL setting, the learner instead
has access to a fixed dataset to learn from, but is unable to otherwise
interact with the environment, and must obtain the best policy it can from this
offline data. Practical scenarios often motivate an intermediate setting: if we
have some set of offline data and, in addition, may also interact with the
environment, how can we best use the offline data to minimize the number of
online interactions necessary to learn an $\epsilon$-optimal policy?
&lt;/p&gt;
&lt;p&gt;In this work, we consider this setting, which we call the \textsf{FineTuneRL}
setting, for MDPs with linear structure. We characterize the necessary number
of online samples needed in this setting given access to some offline dataset,
and develop an algorithm, \textsc{FTPedel}, which is provably optimal, up to
$H$ factors. We show through an explicit example that combining offline data
with online interactions can lead to a provable improvement over either purely
offline or purely online RL. Finally, our results illustrate the distinction
between \emph{verifiable} learning, the typical setting considered in online
RL, and \emph{unverifiable} learning, the setting often considered in offline
RL, and show that there is a formal separation between these regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1&quot;&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1&quot;&gt;Aldo Pacchiano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09100">
<title>Global Optimization with Parametric Function Approximation. (arXiv:2211.09100v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09100</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of global optimization with noisy zeroth order
oracles - a well-motivated problem useful for various applications ranging from
hyper-parameter tuning for deep learning to new material design. Existing work
relies on Gaussian processes or other non-parametric family, which suffers from
the curse of dimensionality. In this paper, we propose a new algorithm GO-UCB
that leverages a parametric family of functions (e.g., neural networks)
instead. Under a realizable assumption and a few other mild geometric
conditions, we show that GO-UCB achieves a cumulative regret of \~O$(\sqrt{T})$
where $T$ is the time horizon. At the core of GO-UCB is a carefully designed
uncertainty set over parameters based on gradients that allows optimistic
exploration. Synthetic and real-world experiments illustrate GO-UCB works
better than popular Bayesian optimization approaches, even if the model is
misspecified.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10515">
<title>Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10515</link>
<description rdf:parseType="Literal">&lt;p&gt;Consider the problem of exploration in sparse-reward or reward-free
environments, such as in Montezuma&apos;s Revenge. In the curiosity-driven paradigm,
the agent is rewarded for how much each realized outcome differs from their
predicted outcome. But using predictive error as intrinsic motivation is
fragile in stochastic environments, as the agent may become trapped by
high-entropy areas of the state-action space, such as a &quot;noisy TV&quot;. In this
work, we study a natural solution derived from structural causal models of the
world: Our key idea is to learn representations of the future that capture
precisely the unpredictable aspects of each outcome -- which we use as
additional input for predictions, such that intrinsic rewards only reflect the
predictable aspects of world dynamics. First, we propose incorporating such
hindsight representations into models to disentangle &quot;noise&quot; from &quot;novelty&quot;,
yielding Curiosity in Hindsight: a simple and scalable generalization of
curiosity that is robust to stochasticity. Second, we instantiate this
framework for the recently introduced BYOL-Explore algorithm as our prime
example, resulting in the noise-robust BYOL-Hindsight. Third, we illustrate its
behavior under a variety of different stochasticities in a grid world, and find
improvements over BYOL-Explore in hard-exploration Atari games with sticky
actions. Notably, we show state-of-the-art results in exploring Montezuma&apos;s
Revenge with sticky actions, while preserving performance in the non-sticky
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jarrett_D/0/1/0/all/0/1&quot;&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tallec_C/0/1/0/all/0/1&quot;&gt;Corentin Tallec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Altche_F/0/1/0/all/0/1&quot;&gt;Florent Altch&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1&quot;&gt;Thomas Mesnard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1&quot;&gt;Michal Valko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14085">
<title>Positive unlabeled learning with tensor networks. (arXiv:2211.14085v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14085</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive unlabeled learning is a binary classification problem with positive
and unlabeled data. It is common in domains where negative labels are costly or
impossible to obtain, e.g., medicine and personalized advertising. Most
approaches to positive unlabeled learning apply to specific data types (e.g.,
images, categorical data) and can not generate new positive and negative
samples. This work introduces a feature-space distance-based tensor network
approach to the positive unlabeled learning problem. The presented method is
not domain specific and significantly improves the state-of-the-art results on
the MNIST image and 15 categorical/mixed datasets. The trained tensor network
model is also a generative model and enables the generation of new positive and
negative instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zunkovic_B/0/1/0/all/0/1&quot;&gt;Bojan &amp;#x17d;unkovi&amp;#x10d;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15382">
<title>Neural Network Complexity of Chaos and Turbulence. (arXiv:2211.15382v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15382</link>
<description rdf:parseType="Literal">&lt;p&gt;Chaos and turbulence are complex physical phenomena, yet a precise definition
of the complexity measure that quantifies them is still lacking. In this work
we consider the relative complexity of chaos and turbulence from the
perspective of deep neural networks. We analyze a set of classification
problems, where the network has to distinguish images of fluid profiles in the
turbulent regime from other classes of images such as fluid profiles in the
chaotic regime, various constructions of noise and real world images. We
analyze incompressible as well as weakly compressible fluid flows. We quantify
the complexity of the computation performed by the network via the intrinsic
dimensionality of the internal feature representations, and calculate the
effective number of independent features which the network uses in order to
distinguish between classes. In addition to providing a numerical estimate of
the complexity of the computation, the measure also characterizes the neural
network processing at intermediate and final stages. We construct adversarial
examples and use them to identify the two point correlation spectra for the
chaotic and turbulent vorticity as the feature used by the network for
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whittaker_T/0/1/0/all/0/1&quot;&gt;Tim Whittaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janik_R/0/1/0/all/0/1&quot;&gt;Romuald A. Janik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oz_Y/0/1/0/all/0/1&quot;&gt;Yaron Oz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12658">
<title>Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12658</link>
<description rdf:parseType="Literal">&lt;p&gt;To improve the uncertainty quantification of variance networks, we propose a
novel tree-structured local neural network model that partitions the feature
space into multiple regions based on uncertainty heterogeneity. A tree is built
upon giving the training data, whose leaf nodes represent different regions
where region-specific neural networks are trained to predict both the mean and
the variance for quantifying uncertainty. The proposed Uncertainty-Splitting
Neural Regression Tree (USNRT) employs novel splitting criteria. At each node,
a neural network is trained on the full data first, and a statistical test for
the residuals is conducted to find the best split, corresponding to the two
sub-regions with the most significant uncertainty heterogeneity between them.
USNRT is computationally friendly because very few leaf nodes are sufficient
and pruning is unnecessary. Furthermore, an ensemble version can be easily
constructed to estimate the total uncertainty including the aleatory and
epistemic. On extensive UCI datasets, USNRT or its ensemble shows superior
performance compared to some recent popular methods for quantifying uncertainty
with variances. Through comprehensive visualization and analysis, we uncover
how USNRT works and show its merits, revealing that uncertainty heterogeneity
does exist in many datasets and can be learned by USNRT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wenxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xing Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.14319">
<title>Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2212.14319</link>
<description rdf:parseType="Literal">&lt;p&gt;Partial differential equations (PDEs) are important tools to model physical
systems and including them into machine learning models is an important way of
incorporating physical knowledge. Given any system of linear PDEs with constant
coefficients, we propose a family of Gaussian process (GP) priors, which we
call EPGP, such that all realizations are exact solutions of this system. We
apply the Ehrenpreis-Palamodov fundamental principle, which works as a
non-linear Fourier transform, to construct GP kernels mirroring standard
spectral methods for GPs. Our approach can infer probable solutions of linear
PDE systems from any data such as noisy measurements, or pointwise defined
initial and boundary conditions. Constructing EPGP-priors is algorithmic,
generally applicable, and comes with a sparse version (S-EPGP) that learns the
relevant spectral frequencies and works better for big data sets. We
demonstrate our approach on three families of systems of PDEs, the heat
equation, wave equation, and Maxwell&apos;s equations, where we improve upon the
state of the art in computation time and precision, in some experiments by
several orders of magnitude.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Harkonen_M/0/1/0/all/0/1&quot;&gt;Marc H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lange_Hegermann_M/0/1/0/all/0/1&quot;&gt;Markus Lange-Hegermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Raita_B/0/1/0/all/0/1&quot;&gt;Bogdan Rai&amp;#x163;&amp;#x103;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10074">
<title>Explainable Data-Driven Optimization: From Context to Decision and Back Again. (arXiv:2301.10074v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10074</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven optimization uses contextual information and machine learning
algorithms to find solutions to decision problems with uncertain parameters.
While a vast body of work is dedicated to interpreting machine learning models
in the classification setting, explaining decision pipelines involving learning
algorithms remains unaddressed. This lack of interpretability can block the
adoption of data-driven solutions as practitioners may not understand or trust
the recommended decisions. We bridge this gap by introducing a counterfactual
explanation methodology tailored to explain solutions to data-driven problems.
We introduce two classes of explanations and develop methods to find nearest
explanations of random forest and nearest-neighbor predictors. We demonstrate
our approach by explaining key problems in operations management such as
inventory management and routing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forel_A/0/1/0/all/0/1&quot;&gt;Alexandre Forel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmentier_A/0/1/0/all/0/1&quot;&gt;Axel Parmentier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_T/0/1/0/all/0/1&quot;&gt;Thibaut Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13816">
<title>Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13816</link>
<description rdf:parseType="Literal">&lt;p&gt;The utilization of programming language (PL) models, pre-trained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting unique sequence-level
characteristics of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that synergistically combines
pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely
used deep reinforcement learning technique. By utilizing non-differentiable
feedback from code execution and structure alignment, PPOCoder seamlessly
integrates external code-specific knowledge into the model optimization
process. It&apos;s important to note that PPOCoder is a task-agnostic and
model-agnostic framework that can be used across different code generation
tasks and PLs. Extensive experiments on three code generation tasks demonstrate
the effectiveness of our proposed approach compared to SOTA methods, achieving
significant improvements in compilation success rates and functional
correctness across different PLs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1&quot;&gt;Parshin Shojaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aneesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1&quot;&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13867">
<title>Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13867</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the mathematical capabilities of two iterations of ChatGPT
(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on
publicly available datasets, as well as hand-crafted ones, using a novel
methodology. In contrast to formal mathematics, where large databases of formal
proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, either cover
only elementary mathematics or are very small. We address this by publicly
releasing two new datasets: GHOSTS and miniGHOSTS. These are the first
natural-language datasets curated by working researchers in mathematics that
(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of
the mathematical capabilities of language models, and (3) distinguish multiple
dimensions of mathematical reasoning. These datasets also test whether ChatGPT
and GPT-4 can be helpful assistants to professional mathematicians by emulating
use cases that arise in the daily professional activities of mathematicians. We
benchmark the models on a range of fine-grained performance metrics. For
advanced mathematics, this is the most detailed evaluation effort to date. We
find that ChatGPT can be used most successfully as a mathematical assistant for
querying facts, acting as a mathematical search engine and knowledge base
interface. GPT-4 can additionally be used for undergraduate-level mathematics
but fails on graduate-level difficulty. Contrary to many positive reports in
the media about GPT-4 and ChatGPT&apos;s exam-solving abilities (a potential case of
selection bias), their overall mathematical performance is well below the level
of a graduate student. Hence, if your goal is to use ChatGPT to pass a
graduate-level math exam, you would be better off copying from your average
peer!
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frieder_S/0/1/0/all/0/1&quot;&gt;Simon Frieder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinchetti_L/0/1/0/all/0/1&quot;&gt;Luca Pinchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chevalier_A/0/1/0/all/0/1&quot;&gt;Alexis Chevalier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1&quot;&gt;Ryan-Rhys Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatori_T/0/1/0/all/0/1&quot;&gt;Tommaso Salvatori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1&quot;&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1&quot;&gt;Philipp Christian Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berner_J/0/1/0/all/0/1&quot;&gt;Julius Berner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06223">
<title>Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06223</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent approaches build on implicit neural representations (INRs) to propose
generative models over function spaces. However, they are computationally
costly when dealing with inference tasks, such as missing data imputation, or
directly cannot tackle them. In this work, we propose a novel deep generative
model, named VAMoH. VAMoH combines the capabilities of modeling continuous
functions using INRs and the inference capabilities of Variational Autoencoders
(VAEs). In addition, VAMoH relies on a normalizing flow to define the prior,
and a mixture of hypernetworks to parametrize the data log-likelihood. This
gives VAMoH a high expressive capability and interpretability. Through
experiments on a diverse range of data types, such as images, voxels, and
climate data, we show that VAMoH can effectively learn rich distributions over
continuous functions. Furthermore, it can perform inference-related tasks, such
as conditional super-resolution generation and in-painting, as well or better
than previous approaches, while being less computationally demanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyuncu_B/0/1/0/all/0/1&quot;&gt;Batuhan Koyuncu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Martin_P/0/1/0/all/0/1&quot;&gt;Pablo Sanchez-Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peis_I/0/1/0/all/0/1&quot;&gt;Ignacio Peis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olmos_P/0/1/0/all/0/1&quot;&gt;Pablo M. Olmos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1&quot;&gt;Isabel Valera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07989">
<title>From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07989</link>
<description rdf:parseType="Literal">&lt;p&gt;This note describes a new approach to classifying graphs that leverages graph
generative models (GGM). Assuming a GGM that defines a joint probability
distribution over graphs and their class labels, I derive classification
formulas for the probability of a class label given a graph. A new conditional
ELBO can be used to train a generative graph auto-encoder model for
discrimination. While leveraging generative models for classification has been
well explored for non-relational i.i.d. data, to our knowledge it is a novel
approach to graph classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1&quot;&gt;Oliver Schulte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08292">
<title>Navya3DSeg -- Navya 3D Semantic Segmentation Dataset &amp; split generation for autonomous vehicles. (arXiv:2302.08292v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08292</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving (AD) perception today relies heavily on deep learning
based architectures requiring large scale annotated datasets with their
associated costs for curation and annotation. The 3D semantic data are useful
for core perception tasks such as obstacle detection and ego-vehicle
localization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg),
with a diverse label space corresponding to a large scale production grade
operational domain, including rural, urban, industrial sites and universities
from 13 countries. It contains 23 labeled sequences and 25 supplementary
sequences without labels, designed to explore self-supervised and
semi-supervised semantic segmentation benchmarks on point clouds. We also
propose a novel method for sequential dataset split generation based on
iterative multi-label stratification, and demonstrated to achieve a +1.2% mIoU
improvement over the original split proposed by SemanticKITTI dataset. A
complete benchmark for semantic segmentation task was performed, with state of
the art methods. Finally, we demonstrate an Active Learning (AL) based dataset
distillation framework. We introduce a novel heuristic-free sampling method
called ego-pose distance based sampling in the context of AL. A detailed
presentation on the dataset is available here
https://www.youtube.com/watch?v=5m6ALIs-s20.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almin_A/0/1/0/all/0/1&quot;&gt;Alexandre Almin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemarie_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Lemari&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_A/0/1/0/all/0/1&quot;&gt;Anh Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1&quot;&gt;B Ravi Kiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10980">
<title>MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10980</link>
<description rdf:parseType="Literal">&lt;p&gt;The bulk of existing research in defending against adversarial examples
focuses on defending against a single (typically bounded Lp-norm) attack, but
for a practical setting, machine learning (ML) models should be robust to a
wide variety of attacks. In this paper, we present the first unified framework
for considering multiple attacks against ML models. Our framework is able to
model different levels of learner&apos;s knowledge about the test-time adversary,
allowing us to model robustness against unforeseen attacks and robustness
against unions of attacks. Using our framework, we present the first
leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which
captures performance across attack types and attack strengths. We evaluate the
performance of 16 defended models for robustness against a set of 9 different
attack types, including Lp-based threat models, spatial transformations, and
color changes, at 20 different attack strengths (180 attacks total).
Additionally, we analyze the state of current defenses against multiple
attacks. Our analysis shows that while existing defenses have made progress in
terms of average robustness across the set of attacks used, robustness against
the worst-case attack is still a big open problem as all existing models
perform worse than random guessing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1&quot;&gt;Sihui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1&quot;&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1&quot;&gt;Chong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1&quot;&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1&quot;&gt;Prateek Mittal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13252">
<title>No-Regret Linear Bandits beyond Realizability. (arXiv:2302.13252v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13252</link>
<description rdf:parseType="Literal">&lt;p&gt;We study linear bandits when the underlying reward function is not linear.
Existing work relies on a uniform misspecification parameter $\epsilon$ that
measures the sup-norm error of the best linear approximation. This results in
an unavoidable linear regret whenever $\epsilon &amp;gt; 0$. We describe a more
natural model of misspecification which only requires the approximation error
at each input $x$ to be proportional to the suboptimality gap at $x$. It
captures the intuition that, for optimization problems, near-optimal regions
should matter more and we can tolerate larger approximation errors in
suboptimal regions. Quite surprisingly, we show that the classical LinUCB
algorithm -- designed for the realizable case -- is automatically robust
against such gap-adjusted misspecification. It achieves a near-optimal
$\sqrt{T}$ regret for problems that the best-known regret is almost linear in
time horizon $T$. Technically, our proof relies on a novel self-bounding
argument that bounds the part of the regret due to misspecification by the
regret itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Ming Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09767">
<title>It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09767</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples are inputs to machine learning models that an attacker
has intentionally designed to confuse the model into making a mistake. Such
examples pose a serious threat to the applicability of machine-learning-based
systems, especially in life- and safety-critical domains. To address this
problem, the area of adversarial robustness investigates mechanisms behind
adversarial attacks and defenses against these attacks. This survey reviews a
particular subset of this literature that focuses on investigating properties
of training data in the context of model robustness under evasion attacks. It
first summarizes the main properties of data leading to adversarial
vulnerability. It then discusses guidelines and techniques for improving
adversarial robustness by enhancing the data representation and learning
procedures, as well as techniques for estimating robustness guarantees given
particular data. Finally, it discusses gaps of knowledge and promising future
research directions in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1&quot;&gt;Peiyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegegn_M/0/1/0/all/0/1&quot;&gt;Michael Tegegn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarin_J/0/1/0/all/0/1&quot;&gt;Jaskeerat Singh Sarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1&quot;&gt;Shubhraneel Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubin_J/0/1/0/all/0/1&quot;&gt;Julia Rubin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13501">
<title>Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13501</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new, provably-convergent algorithm for computing the
flag-mean and flag-median of a set of points on a flag manifold under the
chordal metric. The flag manifold is a mathematical space consisting of flags,
which are sequences of nested subspaces of a vector space that increase in
dimension. The flag manifold is a superset of a wide range of known matrix
spaces, including Stiefel and Grassmanians, making it a general object that is
useful in a wide variety computer vision problems.
&lt;/p&gt;
&lt;p&gt;To tackle the challenge of computing first order flag statistics, we first
transform the problem into one that involves auxiliary variables constrained to
the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and
leveraging the numerical stability and efficiency of Stiefel-manifold
optimization enables us to compute the flag-mean effectively. Through a series
of experiments, we show the competence of our method in Grassmann and rotation
averaging, as well as principal component analysis. We release our source code
under https://github.com/nmank/FlagAveraging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mankovich_N/0/1/0/all/0/1&quot;&gt;Nathan Mankovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1&quot;&gt;Tolga Birdal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16200">
<title>Natural Selection Favors AIs over Humans. (arXiv:2303.16200v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16200</link>
<description rdf:parseType="Literal">&lt;p&gt;For billions of years, evolution has been the driving force behind the
development of life, including humans. Evolution endowed humans with high
intelligence, which allowed us to become one of the most successful species on
the planet. Today, humans aim to create artificial intelligence systems that
surpass even our own intelligence. As artificial intelligences (AIs) evolve and
eventually surpass us in all domains, how might evolution shape our relations
with AIs? By analyzing the environment that is shaping the evolution of AIs, we
argue that the most successful AI agents will likely have undesirable traits.
Competitive pressures among corporations and militaries will give rise to AI
agents that automate human roles, deceive others, and gain power. If such
agents have intelligence that exceeds that of humans, this could lead to
humanity losing control of its future. More abstractly, we argue that natural
selection operates on systems that compete and vary, and that selfish species
typically have an advantage over species that are altruistic to other species.
This Darwinian logic could also apply to artificial agents, as agents may
eventually be better able to persist into the future if they behave selfishly
and pursue their own interests with little regard for humans, which could pose
catastrophic risks. To counteract these risks and evolutionary forces, we
consider interventions such as carefully designing AI agents&apos; intrinsic
motivations, introducing constraints on their actions, and institutions that
encourage cooperation. These steps, or others that resolve the problems we
pose, will be necessary in order to ensure the development of artificial
intelligence is a positive one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16716">
<title>Topological Point Cloud Clustering. (arXiv:2303.16716v2 [math.AT] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16716</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Topological Point Cloud Clustering (TPCC), a new method to cluster
points in an arbitrary point cloud based on their contribution to global
topological features. TPCC synthesizes desirable features from spectral
clustering and topological data analysis and is based on considering the
spectral properties of a simplicial complex associated to the considered point
cloud. As it is based on considering sparse eigenvector computations, TPCC is
similarly easy to interpret and implement as spectral clustering. However, by
focusing not just on a single matrix associated to a graph created from the
point cloud data, but on a whole set of Hodge-Laplacians associated to an
appropriately constructed simplicial complex, we can leverage a far richer set
of topological features to characterize the data points within the point cloud
and benefit from the relative robustness of topological techniques against
noise. We test the performance of TPCC on both synthetic and real-world data
and compare it with classical spectral clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grande_V/0/1/0/all/0/1&quot;&gt;Vincent P. Grande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schaub_M/0/1/0/all/0/1&quot;&gt;Michael T. Schaub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09826">
<title>Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09826</link>
<description rdf:parseType="Literal">&lt;p&gt;Successful deployment of artificial intelligence (AI) in various settings has
led to numerous positive outcomes for individuals and society. However, AI
systems have also been shown to harm parts of the population due to biased
predictions. AI fairness focuses on mitigating such biases to ensure AI
decision making is not discriminatory towards certain groups. We take a closer
look at AI fairness and analyze how lack of AI fairness can lead to deepening
of biases over time and act as a social stressor. More specifically, we discuss
how biased models can lead to more negative real-world outcomes for certain
groups, which may then become more prevalent by deploying new AI models trained
on increasingly biased data, resulting in a feedback loop. If the issues
persist, they could be reinforced by interactions with other risks and have
severe implications on society in the form of social unrest. We examine current
strategies for improving AI fairness, assess their limitations in terms of
real-world deployment, and explore potential paths forward to ensure we reap
AI&apos;s benefits without causing society&apos;s collapse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1&quot;&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1&quot;&gt;Timothy Hospedales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10159">
<title>Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems. (arXiv:2304.10159v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10159</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computing holds great potential for advancing the limitations of
machine learning algorithms to handle higher data dimensions and reduce overall
training parameters in deep neural network (DNN) models. This study uses a
parameterized quantum circuit (PQC) on a gate-based quantum computer to
investigate the potential for quantum advantage in a model-free reinforcement
learning problem. Through a comprehensive investigation and evaluation of the
current model and capabilities of quantum computers, we designed and trained a
novel hybrid Quantum neural network based on the latest Qiskit and PyTorch
framework. We compared its performance with a full-classical DNN with and
without an integrated PQC. Our research provides insights into the potential of
deep quantum learning to solve a maze problem and, potentially, other
reinforcement learning problems. We conclude that various reinforcement
learning problems can be effective with reasonable training epochs. Moreover, a
comparative discussion of the various quantum reinforcement learning model on
maze problems is discussed to evaluate our research&apos;s overall potential and
advantages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao-Yuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yen-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Ching-Ray Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13732">
<title>Lane Change Intention Recognition and Vehicle Status Prediction for Autonomous Vehicles. (arXiv:2304.13732v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13732</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately detecting and predicting lane change (LC)processes of human-driven
vehicles can help autonomous vehicles better understand their surrounding
environment, recognize potential safety hazards, and improve traffic safety.
This paper focuses on LC processes, first developing a temporal convolutional
network with an attention mechanism (TCN-ATM) model to recognize LC intention.
Considering the intrinsic relationship among output variables, the Multi-task
Learning (MTL)framework is employed to simultaneously predict multiple LC
vehicle status indicators. Furthermore, a unified modeling framework for LC
intention recognition and driving status prediction (LC-IR-SP) is developed.
The results indicate that the classification accuracy of LC intention was
improved from 96.14% to 98.20% when incorporating the attention mechanism into
the TCN model. For LC vehicle status prediction issues, three multi-tasking
learning models are constructed based on MTL framework. The results indicate
that the MTL-LSTM model outperforms the MTL-TCN and MTL-TCN-ATM models.
Compared to the corresponding single-task model, the MTL-LSTM model
demonstrates an average decrease of 26.04% in MAE and 25.19% in RMSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1&quot;&gt;Renteng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdel_Aty_M/0/1/0/all/0/1&quot;&gt;Mohamed Abdel-Aty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_O/0/1/0/all/0/1&quot;&gt;Ou Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Q/0/1/0/all/0/1&quot;&gt;Qiaojun Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00143">
<title>Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00143</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problems of sequential nonparametric two-sample and independence
testing. Sequential tests process data online and allow using observed data to
decide whether to stop and reject the null hypothesis or to collect more data,
while maintaining type I error control. We build upon the principle of
(nonparametric) testing by betting, where a gambler places bets on future
observations and their wealth measures evidence against the null hypothesis.
While recently developed kernel-based betting strategies often work well on
simple distributions, selecting a suitable kernel for high-dimensional or
structured data, such as images, is often nontrivial. To address this drawback,
we design prediction-based betting strategies that rely on the following fact:
if a sequentially updated predictor starts to consistently determine (a) which
distribution an instance is drawn from, or (b) whether an instance is drawn
from the joint distribution or the product of the marginal distributions (the
latter produced by external randomization), it provides evidence against the
two-sample or independence nulls respectively. We empirically demonstrate the
superiority of our tests over kernel-based approaches under structured
settings. Our tests can be applied beyond the case of independent and
identically distributed data, remaining valid and powerful even when the data
distribution drifts over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Podkopaev_A/0/1/0/all/0/1&quot;&gt;Aleksandr Podkopaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1&quot;&gt;Aaditya Ramdas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03017">
<title>Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03017</link>
<description rdf:parseType="Literal">&lt;p&gt;Our research investigates the recommendation of code examples to aid software
developers, a practice that saves developers significant time by providing
ready-to-use code snippets. The focus of our study is Stack Overflow, a
commonly used resource for coding discussions and solutions, particularly in
the context of the Java programming language. We applied BERT, a powerful Large
Language Model (LLM) that enables us to transform code examples into numerical
vectors by extracting their semantic information. Once these numerical
representations are prepared, we identify Approximate Nearest Neighbors (ANN)
using Locality-Sensitive Hashing (LSH). Our research employed two variants of
LSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared
these two approaches across four parameters: HitRate, Mean Reciprocal Rank
(MRR), Average Execution Time, and Relevance. Our study revealed that the
Query-Aware (QA) approach showed superior performance over the Random
Hyperplane-based (RH) method. Specifically, it exhibited a notable improvement
of 20% to 35% in HitRate for query pairs compared to the RH approach.
Furthermore, the QA approach proved significantly more time-efficient, with its
speed in creating hashing tables and assigning data samples to buckets being at
least four times faster. It can return code examples within milliseconds,
whereas the RH approach typically requires several seconds to recommend code
examples. Due to the superior performance of the QA approach, we tested it
against PostFinder and FaCoY, the state-of-the-art baselines. Our QA method
showed comparable efficiency proving its potential for effective code
recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1&quot;&gt;Sajjad Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1&quot;&gt;AmirHossein Naghshzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrouj_L/0/1/0/all/0/1&quot;&gt;Latifa Guerrouj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05610">
<title>Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05610</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical Shape Modeling (SSM) is a valuable tool for investigating and
quantifying anatomical variations within populations of anatomies. However,
traditional correspondence-based SSM generation methods have a prohibitive
inference process and require complete geometric proxies (e.g., high-resolution
binary volumes or surface meshes) as input shapes to construct the SSM.
Unordered 3D point cloud representations of shapes are more easily acquired
from various medical imaging practices (e.g., thresholded images and surface
scanning). Point cloud deep networks have recently achieved remarkable success
in learning permutation-invariant features for different point cloud tasks
(e.g., completion, semantic segmentation, classification). However, their
application to learning SSM from point clouds is to-date unexplored. In this
work, we demonstrate that existing point cloud encoder-decoder-based completion
networks can provide an untapped potential for SSM, capturing population-level
statistical representations of shapes while reducing the inference burden and
relaxing the input requirement. We discuss the limitations of these techniques
to the SSM application and suggest future improvements. Our work paves the way
for further exploration of point cloud deep learning for SSM, a promising
avenue for advancing shape analysis literature and broadening SSM to diverse
use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1&quot;&gt;Jadie Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08396">
<title>MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08396</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have made significant strides in medical
image analysis in recent years. However, the local nature of the convolution
operator may pose a limitation for capturing global and long-range interactions
in CNNs. Recently, Transformers have gained popularity in the computer vision
community and also medical image segmentation due to their ability to process
global features effectively. The scalability issues of self-attention mechanism
and lack of the CNN-like inductive bias may have limited their adoption.
Therefore, hybrid Vision transformers (CNN-Transformer), exploiting advantages
of both Convolution and Self-attention Mechanisms, have gained importance. In
this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision
transformer (CNN-Transformer) for medical image segmentation. The proposed
Hybrid Decoder, based on MaxViT-block, is designed to harness the power of both
the convolution and self-attention mechanisms at each decoding stage with
nominal computational burden. The inclusion of multi-axis self-attention,
within each decoder stage, significantly enhances the discriminating capacity
between the object and background regions, and thereby helps in improving the
segmentation efficiency. In the Hybrid Decoder block, the fusion process
commences by integrating the upsampled lower level decoder features, obtained
through transpose convolution, with the skip-connection features derived from
the hybrid encoder. Subsequently, the fused features undergo refinement through
the utilization of a multi-axis attention mechanism. The proposed decoder block
is repeated multiple times to progressively segment the nuclei regions.
Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates the
effectiveness of the proposed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Abdul Rehman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11408">
<title>AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11408</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention is the core mechanism of today&apos;s most used architectures for
natural language processing and has been analyzed from many perspectives,
including its effectiveness for machine translation-related tasks. Among these
studies, attention resulted to be a useful source of information to get
insights about word alignment also when the input text is substituted with
audio segments, as in the case of the speech translation (ST) task. In this
paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that
exploits the attention information to generate source-target alignments that
guide the model during inference. Through experiments on the 8 language pairs
of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art
SimulST policies applied to offline-trained models with gains in terms of BLEU
of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8
languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1&quot;&gt;Sara Papi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1&quot;&gt;Marco Turchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1&quot;&gt;Matteo Negri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13426">
<title>Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13426</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models deployed in healthcare systems must face data
drawn from continually evolving environments. However, researchers proposing
such models typically evaluate them in a time-agnostic manner, splitting
datasets according to patients sampled randomly throughout the entire study
time period. This work proposes the Evaluation on Medical Datasets Over Time
(EMDOT) framework, which evaluates the performance of a model class across
time. Inspired by the concept of backtesting, EMDOT simulates possible training
procedures that practitioners might have been able to execute at each point in
time and evaluates the resulting models on all future time points. Evaluating
both linear and more complex models on six distinct medical data sources
(tabular and imaging), we show how depending on the dataset, using all
historical data may be ideal in many cases, whereas using a window of the most
recent data could be advantageous in others. In datasets where models suffer
from sudden degradations in performance, we investigate plausible explanations
for these shocks. We release the EMDOT package to help facilitate further works
in deployment-oriented evaluation over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Helen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15776">
<title>AUC Optimization from Multiple Unlabeled Datasets. (arXiv:2305.15776v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15776</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised learning aims to empower machine learning when the perfect
supervision is unavailable, which has drawn great attention from researchers.
Among various types of weak supervision, one of the most challenging cases is
to learn from multiple unlabeled (U) datasets with only a little knowledge of
the class priors, or U$^m$ learning for short. In this paper, we study the
problem of building an AUC (area under ROC curve) optimization model from
multiple unlabeled datasets, which maximizes the pairwise ranking ability of
the classifier. We propose U$^m$-AUC, an AUC optimization approach that
converts the U$^m$ data into a multi-label AUC optimization problem, and can be
trained efficiently. We show that the proposed U$^m$-AUC is effective
theoretically and empirically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18088">
<title>Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v4 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18088</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic has created a global health crisis, driving the need
for the rapid identification of potential therapeutics. To meet this challenge,
drug repurposing is the only solution with saving cost, time, and labor. In
this study, we used the Zinc database to screen the world-approved including
FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments
targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking
and checked the efficacy of drug molecules. To enhance the efficiency of drug
repurposing approach, we modeled the binding affinities using several machine
learning regression approaches for QSAR modeling such as decision tree, extra
trees, MLP, KNN, XGBoost, and gradient boosting. The computational results
demonstrated that Decision Tree Regression (DTR) model has improved statistical
measures of R2 and RMSE. These simulated results helped to identify drugs with
high binding affinity. From the docking and other statistical analysis, we
shortlisted six promising drugs with their respective Zinc IDs (ZINC3873365,
ZINC85432544, ZINC203757351, ZINC85536956, ZINC8214470 and ZINC261494640)
within the range of -15 kcal/mol to -13 kcal/mol. In the study, the repurposed
drugs are novel except ZINC203757351 antiviral compound that has already
identified against COVID-19 in other studies. Further, we analyzed the
physiochemical and pharmacokinetic properties of these top-ranked selected
drugs with respect to their best binding interaction for specific target
protease 3CLpro. Our study has provided an efficient framework for drug
repurposing against COVID-19. This highlights the potential of combining
molecular docking with machine learning regression approaches to accelerate the
identification of potential therapeutic candidates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aqeel_I/0/1/0/all/0/1&quot;&gt;Imra Aqeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Majid_A/0/1/0/all/0/1&quot;&gt;Abdul Majid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03718">
<title>Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v4 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03718</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Objective experimental results show that our proposed model
outperforms other LSTM-based models. Through subjective evaluation, we conclude
that only altering the types of chords hardly changes the overall emotion of
the music. The qualitative analysis demonstrates the ability of our model to
generate variable harmonies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shulei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04777">
<title>Invariant Causal Set Covering Machines. (arXiv:2306.04777v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04777</link>
<description rdf:parseType="Literal">&lt;p&gt;Rule-based models, such as decision trees, appeal to practitioners due to
their interpretable nature. However, the learning algorithms that produce such
models are often vulnerable to spurious associations and thus, they are not
guaranteed to extract causally-relevant insights. In this work, we build on
ideas from the invariant causal prediction literature to propose Invariant
Causal Set Covering Machines, an extension of the classical Set Covering
Machine algorithm for conjunctions/disjunctions of binary-valued rules that
provably avoids spurious associations. We demonstrate both theoretically and
empirically that our method can identify the causal parents of a variable of
interest in polynomial time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godon_T/0/1/0/all/0/1&quot;&gt;Thibaud Godon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauvin_B/0/1/0/all/0/1&quot;&gt;Baptiste Bauvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1&quot;&gt;Pascal Germain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corbeil_J/0/1/0/all/0/1&quot;&gt;Jacques Corbeil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1&quot;&gt;Alexandre Drouin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10045">
<title>Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v6 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10045</link>
<description rdf:parseType="Literal">&lt;p&gt;We study property prediction for crystal materials. A crystal structure
consists of a minimal unit cell that is repeated infinitely in 3D space. How to
accurately represent such repetitive structures in machine learning models
remains unresolved. Current methods construct graphs by establishing edges only
between nearby nodes, thereby failing to faithfully capture infinite repeating
patterns and distant interatomic interactions. In this work, we propose several
innovations to overcome these limitations. First, we propose to model
physics-principled interatomic potentials directly instead of only using
distances as in many existing methods. These potentials include the Coulomb
potential, London dispersion potential, and Pauli repulsion potential. Second,
we model the complete set of potentials among all atoms, instead of only
between nearby atoms as in existing methods. This is enabled by our
approximations of infinite potential summations with provable error bounds. We
further develop efficient algorithms to compute the approximations. Finally, we
propose to incorporate our computations of complete interatomic potentials into
message passing neural networks for representation learning. We perform
experiments on the JARVIS and Materials Project benchmarks for evaluation.
Results show that the use of interatomic potentials and complete interatomic
potentials leads to consistent performance improvements with reasonable
computational costs. Our code is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuchao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Keqiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Youzhi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaoning Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shuiwang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11112">
<title>Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11112</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning from data corrupted by
underrepresentation bias, where positive examples are filtered from the data at
different, unknown rates for a fixed number of sensitive groups. We show that
with a small amount of unbiased data, we can efficiently estimate the
group-wise drop-out parameters, even in settings where intersectional group
membership makes learning each intersectional rate computationally infeasible.
Using this estimate for the group-wise drop-out rate, we construct a
re-weighting scheme that allows us to approximate the loss of any hypothesis on
the true distribution, even if we only observe the empirical error on a biased
sample. Finally, we present an algorithm encapsulating this learning and
re-weighting process, and we provide strong PAC-style guarantees that, with
high probability, our estimate of the risk of the hypothesis over the true
distribution will be arbitrarily close to the true risk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolbert_A/0/1/0/all/0/1&quot;&gt;Alexander Williams Tolbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diana_E/0/1/0/all/0/1&quot;&gt;Emily Diana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12619">
<title>Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12619</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the great success of pre-trained language models, it is still a
challenge to use these models for continual learning, especially for the
class-incremental learning (CIL) setting due to catastrophic forgetting (CF).
This paper reports our finding that if we formulate CIL as a continual label
generation problem, CF is drastically reduced and the generalizable
representations of pre-trained models can be better retained. We thus propose a
new CIL method (VAG) that also leverages the sparsity of vocabulary to focus
the generation and creates pseudo-replay samples by using label semantics.
Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yijia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiduo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13960">
<title>Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13960</link>
<description rdf:parseType="Literal">&lt;p&gt;Regular group convolutional neural networks (G-CNNs) have been shown to
increase model performance and improve equivariance to different geometrical
symmetries. This work addresses the problem of SE(3), i.e., roto-translation
equivariance, on volumetric data. Volumetric image data is prevalent in many
medical settings. Motivated by the recent work on separable group convolutions,
we devise a SE(3) group convolution kernel separated into a continuous SO(3)
(rotation) kernel and a spatial kernel. We approximate equivariance to the
continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel
is parameterized via RBF interpolation on similarly uniform grids. We
demonstrate the advantages of our approach in volumetric medical image
analysis. Our SE(3) equivariant models consistently outperform CNNs and regular
discrete G-CNNs on challenging medical classification tasks and show
significantly improved generalization capabilities. Our approach achieves up to
a 16.5% gain in accuracy over regular CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuipers_T/0/1/0/all/0/1&quot;&gt;Thijs P. Kuipers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1&quot;&gt;Erik J. Bekkers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14030">
<title>My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14030</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on code-mixed data is limited due to the unavailability of
dedicated code-mixed datasets and pre-trained language models. In this work, we
focus on the low-resource Indian language Marathi which lacks any prior work in
code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English
(Mr-En) corpus with 10 million social media sentences for pretraining. We also
release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models
pre-trained on MeCorpus. Furthermore, for benchmarking, we present three
supervised datasets MeHate, MeSent, and MeLID for downstream tasks like
code-mixed Mr-En hate speech detection, sentiment analysis, and language
identification respectively. These evaluation datasets individually consist of
manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations
show that the models trained on this novel corpus significantly outperform the
existing state-of-the-art BERT models. This is the first work that presents
artifacts for code-mixed Marathi research. All datasets and models are publicly
released at https://github.com/l3cube-pune/MarathiNLP .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1&quot;&gt;Tanmay Chavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1&quot;&gt;Omkar Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1&quot;&gt;Aditya Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shantanu Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1&quot;&gt;Raviraj Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17582">
<title>ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17582</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an experimental study regarding the use of OpenAI&apos;s
ChatGPT for robotics applications. We outline a strategy that combines design
principles for prompt engineering and the creation of a high-level function
library which allows ChatGPT to adapt to different robotics tasks, simulators,
and form factors. We focus our evaluations on the effectiveness of different
prompt engineering techniques and dialog strategies towards the execution of
various types of robotics tasks. We explore ChatGPT&apos;s ability to use free-form
dialog, parse XML tags, and to synthesize code, in addition to the use of
task-specific prompting functions and closed-loop reasoning through dialogues.
Our study encompasses a range of tasks within the robotics domain, from basic
logical, geometrical, and mathematical reasoning all the way to complex domains
such as aerial navigation, manipulation, and embodied agents. We show that
ChatGPT can be effective at solving several of such tasks, while allowing users
to interact with it primarily via natural language instructions. In addition to
these studies, we introduce an open-sourced research tool called PromptCraft,
which contains a platform where researchers can collaboratively upload and vote
on examples of good prompting schemes for robotics applications, as well as a
sample robotics simulator with ChatGPT integration, making it easier for users
to get started with using ChatGPT for robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1&quot;&gt;Sai Vemprala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1&quot;&gt;Rogerio Bonatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucker_A/0/1/0/all/0/1&quot;&gt;Arthur Bucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1&quot;&gt;Ashish Kapoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00405">
<title>Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00405</link>
<description rdf:parseType="Literal">&lt;p&gt;The general sequential decision-making problem, which includes Markov
decision processes (MDPs) and partially observable MDPs (POMDPs) as special
cases, aims at maximizing a cumulative reward by making a sequence of decisions
based on a history of observations and actions over time. Recent studies have
shown that the sequential decision-making problem is statistically learnable if
it admits a low-rank structure modeled by predictive state representations
(PSRs). Despite these advancements, existing approaches typically involve
oracles or steps that are not computationally efficient. On the other hand, the
upper confidence bound (UCB) based approaches, which have served successfully
as computationally efficient methods in bandits and MDPs, have not been
investigated for more general PSRs, due to the difficulty of optimistic bonus
design in these more challenging settings. This paper proposes the first known
UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the
total variation distance between the estimated and true models. We further
characterize the sample complexity bounds for our designed UCB-type algorithms
for both online and offline PSRs. In contrast to existing approaches for PSRs,
our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteed
near-optimal policy, and guaranteed model accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruiquan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02405">
<title>$\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows. (arXiv:2307.02405v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02405</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flows
method to final states containing multiple neutrinos. The architecture can
natively scale for all combinations of object types and multiplicities in the
final state for any desired neutrino multiplicities. In $t\bar{t}$ dilepton
events, the momenta of both neutrinos and correlations between them are
reconstructed more accurately than when using the most popular standard
analytical techniques, and solutions are found for all events. Inference time
is significantly faster than competing methods, and can be reduced further by
evaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to
$t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded
distributions is much closer to the limit of performance set by perfect
neutrino reconstruction than standard techniques. For the chosen double
differential observables $\nu^2$-Flows results in improved statistical
precision for each bin by a factor of 1.5 to 2 in comparison to the Neutrino
Weighting method and up to a factor of four in comparison to the Ellipse
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Raine_J/0/1/0/all/0/1&quot;&gt;John Andrew Raine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Leigh_M/0/1/0/all/0/1&quot;&gt;Matthew Leigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Zoch_K/0/1/0/all/0/1&quot;&gt;Knut Zoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Golling_T/0/1/0/all/0/1&quot;&gt;Tobias Golling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02719">
<title>Understanding Uncertainty Sampling. (arXiv:2307.02719v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02719</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty sampling is a prevalent active learning algorithm that queries
sequentially the annotations of data samples which the current prediction model
is uncertain about. However, the usage of uncertainty sampling has been largely
heuristic: (i) There is no consensus on the proper definition of &quot;uncertainty&quot;
for a specific task under a specific loss; (ii) There is no theoretical
guarantee that prescribes a standard protocol to implement the algorithm, for
example, how to handle the sequentially arrived annotated data under the
framework of optimization algorithms such as stochastic gradient descent. In
this work, we systematically examine uncertainty sampling algorithms under both
stream-based and pool-based active learning. We propose a notion of equivalent
loss which depends on the used uncertainty measure and the original loss
function and establish that an uncertainty sampling algorithm essentially
optimizes against such an equivalent loss. The perspective verifies the
properness of existing uncertainty measures from two aspects: surrogate
property and loss convexity. Furthermore, we propose a new notion for designing
uncertainty measures called \textit{loss as uncertainty}. The idea is to use
the conditional expected loss given the features as the uncertainty measure.
Such an uncertainty measure has nice analytical properties and generality to
cover both classification and regression problems, which enable us to provide
the first generalization bound for uncertainty sampling algorithms under both
stream-based and pool-based settings, in the full generality of the underlying
model and problem. Lastly, we establish connections between certain variants of
the uncertainty sampling algorithms with risk-sensitive objectives and
distributional robustness, which can partly explain the advantage of
uncertainty sampling algorithms when the sample size is small.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04001">
<title>Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04001</link>
<description rdf:parseType="Literal">&lt;p&gt;Set representation has become ubiquitous in deep learning for modeling the
inductive bias of neural networks that are insensitive to the input order.
DeepSets is the most widely used neural network architecture for set
representation. It involves embedding each set element into a latent space with
dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and
finally mapping the whole-set embedding to the output. In this work, we
investigate the impact of the dimension $L$ on the expressive power of
DeepSets. Previous analyses either oversimplified high-dimensional features to
be one-dimensional features or were limited to analytic activations, thereby
diverging from practical use or resulting in $L$ that grows exponentially with
the set size $N$ and feature dimension $D$. To investigate the minimal value of
$L$ that achieves sufficient expressive power, we present two set-element
embedding layers: (a) linear + power activation (LP) and (b) linear +
exponential activations (LE). We demonstrate that $L$ being poly$(N, D)$ is
sufficient for set representation using both embedding layers. We also provide
a lower bound of $L$ for the LP embedding layer. Furthermore, we extend our
results to permutation-equivariant set functions and the complex field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shenghao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04603">
<title>Solvent: A Framework for Protein Folding. (arXiv:2307.04603v4 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04603</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistency and reliability are crucial for conducting AI research. Many
famous research fields, such as object detection, have been compared and
validated with solid benchmark frameworks. After AlphaFold2, the protein
folding task has entered a new phase, and many methods are proposed based on
the component of AlphaFold2. The importance of a unified research framework in
protein folding contains implementations and benchmarks to consistently and
fairly compare various approaches. To achieve this, we present Solvent, an
protein folding framework that supports significant components of
state-of-the-art models in the manner of off-the-shelf interface Solvent
contains different models implemented in a unified codebase and supports
training and evaluation for defined models on the same dataset. We benchmark
well-known algorithms and their components and provide experiments that give
helpful insights into the protein structure modeling field. We hope that
Solvent will increase the reliability and consistency of proposed models and
gives efficiency in both speed and costs, resulting in acceleration on protein
folding modeling research. The code is available at
https://github.com/kakaobrain/solvent, and the project will continue to be
developed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaemyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kyeongtak Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hasun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Youhan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04668">
<title>Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach. (arXiv:2307.04668v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04668</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of social media platforms has facilitated the formation of echo
chambers, which are online spaces where users predominantly encounter
viewpoints that reinforce their existing beliefs while excluding dissenting
perspectives. This phenomenon significantly hinders information dissemination
across communities and fuels societal polarization. Therefore, it is crucial to
develop methods for quantifying echo chambers. In this paper, we present the
Echo Chamber Score (ECS), a novel metric that assesses the cohesion and
separation of user communities by measuring distances between users in the
embedding space. In contrast to existing approaches, ECS is able to function
without labels for user ideologies and makes no assumptions about the structure
of the interaction graph. To facilitate measuring distances between users, we
propose EchoGAE, a self-supervised graph autoencoder-based user embedding model
that leverages users&apos; posts and the interaction graph to embed them in a manner
that reflects their ideological similarity. To assess the effectiveness of ECS,
we use a Twitter dataset consisting of four topics - two polarizing and two
non-polarizing. Our results showcase ECS&apos;s effectiveness as a tool for
quantifying echo chambers and shedding light on the dynamics of online
discourse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alatawi_F/0/1/0/all/0/1&quot;&gt;Faisal Alatawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1&quot;&gt;Paras Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06092">
<title>Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06092</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma&amp;gt;0$, with the exponent
depending on the metric used to measure discrepancy. Our bounds are strictly
stronger in terms of their dependence on network width than any previously
available in the literature; in the one-dimensional case, we also prove that
they are optimal, i.e., we establish matching lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1&quot;&gt;Stefano Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1&quot;&gt;Domenico Marinucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1&quot;&gt;Ivan Nourdin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1&quot;&gt;Giovanni Peccati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07050">
<title>Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\&quot;odinger Equation. (arXiv:2307.07050v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07050</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving the quantum many-body Schr\&quot;odinger equation is a fundamental and
challenging problem in the fields of quantum physics, quantum chemistry, and
material sciences. One of the common computational approaches to this problem
is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are
obtained by minimizing the energy of the system within a restricted family of
parameterized wave functions. Deep learning methods partially address the
limitations of traditional QVMC by representing a rich family of wave functions
in terms of neural networks. However, the optimization objective in QVMC
remains notoriously hard to minimize and requires second-order optimization
methods such as natural gradient. In this paper, we first reformulate energy
functional minimization in the space of Born distributions corresponding to
particle-permutation (anti-)symmetric wave functions, rather than the space of
wave functions. We then interpret QVMC as the Fisher-Rao gradient flow in this
distributional space, followed by a projection step onto the variational
manifold. This perspective provides us with a principled framework to derive
new QMC algorithms, by endowing the distributional space with better metrics,
and following the projected gradient flow induced by those metrics. More
specifically, we propose &quot;Wasserstein Quantum Monte Carlo&quot; (WQMC), which uses
the gradient flow induced by the Wasserstein metric, rather than Fisher-Rao
metric, and corresponds to transporting the probability mass, rather than
teleporting it. We demonstrate empirically that the dynamics of WQMC results in
faster convergence to the ground state of molecular systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Neklyudov_K/0/1/0/all/0/1&quot;&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nys_J/0/1/0/all/0/1&quot;&gt;Jannes Nys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thiede_L/0/1/0/all/0/1&quot;&gt;Luca Thiede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carrasquilla_J/0/1/0/all/0/1&quot;&gt;Juan Carrasquilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Makhzani_A/0/1/0/all/0/1&quot;&gt;Alireza Makhzani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07269">
<title>Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07269</link>
<description rdf:parseType="Literal">&lt;p&gt;It is imperative to ensure the robustness of deep learning models in critical
applications such as, healthcare. While recent advances in deep learning have
improved the performance of volumetric medical image segmentation models, these
models cannot be deployed for real-world applications immediately due to their
vulnerability to adversarial attacks. We present a 3D frequency domain
adversarial attack for volumetric medical image segmentation models and
demonstrate its advantages over conventional input or voxel domain attacks.
Using our proposed attack, we introduce a novel frequency domain adversarial
training approach for optimizing a robust model against voxel and frequency
domain attacks. Moreover, we propose frequency consistency loss to regulate our
frequency domain adversarial training that achieves a better tradeoff between
model&apos;s performance on clean and adversarial samples. Code is publicly
available at https://github.com/asif-hanif/vafa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hanif_A/0/1/0/all/0/1&quot;&gt;Asif Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07666">
<title>Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07666</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust reinforcement learning (RL) aims to find a policy that optimizes the
worst-case performance in the face of uncertainties. In this paper, we focus on
action robust RL with the probabilistic policy execution uncertainty, in which,
instead of always carrying out the action specified by the policy, the agent
will take the action specified by the policy with probability $1-\rho$ and an
alternative adversarial action with probability $\rho$. We establish the
existence of an optimal policy on the action robust MDPs with probabilistic
policy execution uncertainty and provide the action robust Bellman optimality
equation for its solution. Furthermore, we develop Action Robust Reinforcement
Learning with Certificates (ARRLC) algorithm that achieves minimax optimal
regret and sample complexity. Furthermore, we conduct numerical experiments to
validate our approach&apos;s robustness, demonstrating that ARRLC outperforms
non-robust RL algorithms and converges faster than the robust TD algorithm in
the presence of action perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhihan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Lifeng Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08122">
<title>Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08122</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning
linearized transformers obtained by computing a First-order Taylor Expansion
around a pre-trained initialization. We show that the Jacobian-Vector Product
resulting from linearization can be computed efficiently in a single forward
pass, reducing training and inference cost to the same order of magnitude as
its original non-linear counterpart, while using the same number of parameters.
Furthermore, we show that, when applied to various downstream visual
classification tasks, the resulting Tangent Transformer fine-tuned with TAFT
can perform comparably with fine-tuning the original non-linear network. Since
Tangent Transformers are linear with respect to the new set of weights, and the
resulting fine-tuning loss is convex, we show that TAFT enjoys several
advantages compared to non-linear fine-tuning when it comes to model
composition, parallel training, machine unlearning, and differential privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1&quot;&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09018">
<title>Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09018</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation large language models (LLMs) have shown an impressive ability to
solve tasks across a wide range of fields including health. To effectively
solve personalized health tasks, LLMs need the ability to ingest a diversity of
data modalities that are relevant to an individual&apos;s health status. In this
paper, we take a step towards creating multimodal LLMs for health that are
grounded in individual-specific data by developing a framework (HeLM: Health
Large Language Model for Multimodal Understanding) that enables LLMs to use
high-dimensional clinical modalities to estimate underlying disease risk. HeLM
encodes complex data modalities by learning an encoder that maps them into the
LLM&apos;s token embedding space and for simple modalities like tabular data by
serializing the data into text. Using data from the UK Biobank, we show that
HeLM can effectively use demographic and clinical features in addition to
high-dimensional time-series data to estimate disease risk. For example, HeLM
achieves an AUROC of 0.75 for asthma prediction when combining tabular and
spirogram data modalities compared with 0.49 when only using tabular data.
Overall, we find that HeLM outperforms or performs at parity with classical
machine learning approaches across a selection of eight binary traits.
Furthermore, we investigate the downstream uses of this model such as its
generalizability to out-of-distribution traits and its ability to power
conversations around individual health and wellness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Belyaeva_A/0/1/0/all/0/1&quot;&gt;Anastasiya Belyaeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cosentino_J/0/1/0/all/0/1&quot;&gt;Justin Cosentino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hormozdiari_F/0/1/0/all/0/1&quot;&gt;Farhad Hormozdiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Eswaran_K/0/1/0/all/0/1&quot;&gt;Krish Eswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Shravya Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Greg Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Carroll_A/0/1/0/all/0/1&quot;&gt;Andrew Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+McLean_C/0/1/0/all/0/1&quot;&gt;Cory Y. McLean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Furlotte_N/0/1/0/all/0/1&quot;&gt;Nicholas A. Furlotte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09206">
<title>Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09206</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous navigation settings, several quantities can be subject to
variations. Terrain properties such as friction coefficients may vary over time
depending on the location of the robot. Also, the dynamics of the robot may
change due to, e.g., different payloads, changing the system&apos;s mass, or wear
and tear, changing actuator gains or joint friction. An autonomous agent should
thus be able to adapt to such variations. In this paper, we develop a novel
probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN,
which is able to adapt to the above-mentioned variations. It builds on recent
advances in meta-learning forward dynamics models based on Neural Processes. We
evaluate our method in a simulated 2D navigation setting with a unicycle-like
robot and different terrain layouts with spatially varying friction
coefficients. In our experiments, the proposed model exhibits lower prediction
error for the task of long-horizon trajectory prediction, compared to
non-adaptive ablation models. We also evaluate our model on the downstream task
of navigation planning, which demonstrates improved performance in planning
control-efficient paths by taking robot and terrain properties into account.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1&quot;&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achterhold_J/0/1/0/all/0/1&quot;&gt;Jan Achterhold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haolong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1&quot;&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1&quot;&gt;Joerg Stueckler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09614">
<title>Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09614</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeling of multivariate biomedical time series data is a laborious and
expensive process. Self-supervised contrastive learning alleviates the need for
large, labeled datasets through pretraining on unlabeled data. However, for
multivariate time series data, the set of input channels often varies between
applications, and most existing work does not allow for transfer between
datasets with different sets of input channels. We propose learning one encoder
to operate on all input channels individually. We then use a message passing
neural network to extract a single representation across channels. We
demonstrate the potential of this method by pretraining our model on a dataset
with six EEG channels and then fine-tuning it on a dataset with two different
EEG channels. We compare models with and without the message passing neural
network across different contrastive loss functions. We show that our method,
combined with the TS2Vec loss, outperforms all other methods in most settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brusch_T/0/1/0/all/0/1&quot;&gt;Thea Br&amp;#xfc;sch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1&quot;&gt;Mikkel N. Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Alstrom_T/0/1/0/all/0/1&quot;&gt;Tommy S. Alstr&amp;#xf8;m&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09702">
<title>Efficient Guided Generation for Large Language Models. (arXiv:2307.09702v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09702</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we describe an efficient approach to guiding language model
text generation with regular expressions and context-free grammars. Our
approach adds little to no overhead to the token sequence generation process,
and makes guided generation feasible in practice. An implementation is provided
in the open source Python library Outlines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willard_B/0/1/0/all/0/1&quot;&gt;Brandon T. Willard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louf_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Louf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09943">
<title>Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay. (arXiv:2307.09943v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09943</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems are a ubiquitous feature of online platforms.
Increasingly, they are explicitly tasked with increasing users&apos; long-term
satisfaction. In this context, we study a content exploration task, which we
formalize as a multi-armed bandit problem with delayed rewards. We observe that
there is an apparent trade-off in choosing the learning signal: Waiting for the
full reward to become available might take several weeks, hurting the rate at
which learning happens, whereas measuring short-term proxy rewards reflects the
actual long-term goal only imperfectly. We address this challenge in two steps.
First, we develop a predictive model of delayed rewards that incorporates all
information obtained to date. Full observations as well as partial (short or
medium-term) outcomes are combined through a Bayesian filter to obtain a
probabilistic belief. Second, we devise a bandit algorithm that takes advantage
of this new predictive model. The algorithm quickly learns to identify content
aligned with long-term success by carefully balancing exploration and
exploitation. We apply our approach to a podcast recommendation problem, where
we seek to identify shows that users engage with repeatedly over two months. We
empirically validate that our approach results in substantially better
performance compared to approaches that either optimize for short-term proxies,
or wait for the long-term outcome to be fully realized.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1&quot;&gt;Thomas M. McDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maystre_L/0/1/0/all/0/1&quot;&gt;Lucas Maystre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalmas_M/0/1/0/all/0/1&quot;&gt;Mounia Lalmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_D/0/1/0/all/0/1&quot;&gt;Daniel Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciosek_K/0/1/0/all/0/1&quot;&gt;Kamil Ciosek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08529">
<title>Synthetic Lagrangian Turbulence by Generative Diffusion Models. (arXiv:2307.08529v1 [physics.flu-dyn] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.08529</link>
<description rdf:parseType="Literal">&lt;p&gt;Lagrangian turbulence lies at the core of numerous applied and fundamental
problems related to the physics of dispersion and mixing in engineering,
bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional
theoretical, numerical, and experimental efforts conducted over the past thirty
years, no existing models are capable of faithfully reproducing statistical and
topological properties exhibited by particle trajectories in turbulence. We
propose a machine learning approach, based on a state-of-the-art Diffusion
Model, to generate single-particle trajectories in three-dimensional turbulence
at high Reynolds numbers, thereby bypassing the need for direct numerical
simulations or experiments to obtain reliable Lagrangian data. Our model
demonstrates the ability to quantitatively reproduce all relevant statistical
benchmarks over the entire range of time scales, including the presence of fat
tails distribution for the velocity increments, anomalous power law, and
enhancement of intermittency around the dissipative scale. The model exhibits
good generalizability for extreme events, achieving unprecedented intensity and
rarity. This paves the way for producing synthetic high-quality datasets for
pre-training various downstream applications of Lagrangian turbulence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1&quot;&gt;Luca Biferale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1&quot;&gt;Fabio Bonaccorso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Scarpolini_M/0/1/0/all/0/1&quot;&gt;Martino Andrea Scarpolini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1&quot;&gt;Michele Buzzicotti&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>