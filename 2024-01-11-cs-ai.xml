<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.09212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03301" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.04108">
<title>Working with Trouble and Failures in Conversation between Humans and Robots (WTF 2023) &amp; Is CUI Design Ready Yet?. (arXiv:2401.04108v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04108</link>
<description rdf:parseType="Literal">&lt;p&gt;Workshop proceedings of two co-located workshops &quot;Working with Troubles and
Failures in Conversation with Humans and Robots&quot; (WTF 2023) and &quot;Is CUI Design
Ready Yet?&quot;, both of which were part of the ACM conference on conversational
user interfaces 2023.
&lt;/p&gt;
&lt;p&gt;WTF 23 aimed at bringing together researchers from human-robot interaction,
dialogue systems, human-computer interaction, and conversation analysis.
Despite all progress, robotic speech interfaces continue to be brittle in a
number of ways and the experience of failure of such interfaces is commonplace
amongst roboticists. However, the technical literature is positively skewed
toward their good performance. The workshop aims to provide a platform for
discussing communicative troubles and failures in human-robot interactions and
related failures in non-robotic speech interfaces. Aims include a scrupulous
investigation into communicative failures, to begin working on a taxonomy of
such failures, and enable a preliminary discussion on possible mitigating
strategies. Workshop website: https://sites.google.com/view/wtf2023/overview
&lt;/p&gt;
&lt;p&gt;Is CUI Design Ready Yet? As CUIs become more prevalent in both academic
research and the commercial market, it becomes more essential to design usable
and adoptable CUIs. While research has been growing on the methods for
designing CUIs for commercial use, there has been little discussion on the
overall community practice of developing design resources to aid in practical
CUI design. The aim of this workshop, therefore, is to bring the CUI community
together to discuss the current practices for developing tools and resources
for practical CUI design, the adoption (or non-adoption) of these tools and
resources, and how these resources are utilized in the training and education
of new CUI designers entering the field. Workshop website:
https://speech-interaction.org/cui2023_design_workshop/index.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forster_F/0/1/0/all/0/1&quot;&gt;Frank F&amp;#xf6;rster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romeo_M/0/1/0/all/0/1&quot;&gt;Marta Romeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holthaus_P/0/1/0/all/0/1&quot;&gt;Patrick Holthaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigo_M/0/1/0/all/0/1&quot;&gt;Maria Jose Galvez Trigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1&quot;&gt;Joel E. Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nesset_B/0/1/0/all/0/1&quot;&gt;Birthe Nesset&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dondrup_C/0/1/0/all/0/1&quot;&gt;Christian Dondrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murad_C/0/1/0/all/0/1&quot;&gt;Christine Murad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munteanu_C/0/1/0/all/0/1&quot;&gt;Cosmin Munteanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cowan_B/0/1/0/all/0/1&quot;&gt;Benjamin R. Cowan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_L/0/1/0/all/0/1&quot;&gt;Leigh Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porcheron_M/0/1/0/all/0/1&quot;&gt;Martin Porcheron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candello_H/0/1/0/all/0/1&quot;&gt;Heloisa Candello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langevin_R/0/1/0/all/0/1&quot;&gt;Raina Langevin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04119">
<title>Why is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis. (arXiv:2401.04119v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04119</link>
<description rdf:parseType="Literal">&lt;p&gt;Dark patterns are deceptive user interface designs for online services that
make users behave in unintended ways. Dark patterns, such as privacy invasion,
financial loss, and emotional distress, can harm users. These issues have been
the subject of considerable debate in recent years. In this paper, we study
interpretable dark pattern auto-detection, that is, why a particular user
interface is detected as having dark patterns. First, we trained a model using
transformer-based pre-trained language models, BERT, on a text-based dataset
for the automatic detection of dark patterns in e-commerce. Then, we applied
post-hoc explanation techniques, including local interpretable model agnostic
explanation (LIME) and Shapley additive explanations (SHAP), to the trained
model, which revealed which terms influence each prediction as a dark pattern.
In addition, we extracted and analyzed terms that affected the dark patterns.
Our findings may prevent users from being manipulated by dark patterns, and aid
in the construction of more equitable internet services. Our code is available
at https://github.com/yamanalab/why-darkpattern.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yada_Y/0/1/0/all/0/1&quot;&gt;Yuki Yada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsumoto_T/0/1/0/all/0/1&quot;&gt;Tsuneo Matsumoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kido_F/0/1/0/all/0/1&quot;&gt;Fuyuko Kido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamana_H/0/1/0/all/0/1&quot;&gt;Hayato Yamana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04120">
<title>Generation Z&apos;s Ability to Discriminate Between AI-generated and Human-Authored Text on Discord. (arXiv:2401.04120v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04120</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing popularity of generative artificial intelligence (AI) chatbots
such as ChatGPT is having transformative effects on social media. As the
prevalence of AI-generated content grows, concerns have been raised regarding
privacy and misinformation online. Among social media platforms, Discord
enables AI integrations -- making their primarily &quot;Generation Z&quot; userbase
particularly exposed to AI-generated content. We surveyed Generation Z aged
individuals (n = 335) to evaluate their proficiency in discriminating between
AI-generated and human-authored text on Discord. The investigation employed
one-shot prompting of ChatGPT, disguised as a text message received on the
Discord.com platform. We explore the influence of demographic factors on
ability, as well as participants&apos; familiarity with Discord and artificial
intelligence technologies. We find that Generation Z individuals are unable to
discern between AI and human-authored text (p = 0.011), and that those with
lower self-reported familiarity with Discord demonstrated an improved ability
in identifying human-authored compared to those with self-reported experience
with AI (p &amp;lt;&amp;lt; 0.0001). Our results suggest that there is a nuanced relationship
between AI technology and popular modes of communication for Generation Z,
contributing valuable insights into human-computer interactions, digital
communication, and artificial intelligence literacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramu_D/0/1/0/all/0/1&quot;&gt;Dhruv Ramu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rishab Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aditya Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04122">
<title>From Prompt Engineering to Prompt Science With Human in the Loop. (arXiv:2401.04122v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04122</link>
<description rdf:parseType="Literal">&lt;p&gt;As LLMs make their way into many aspects of our lives, one place that
warrants increased scrutiny with LLM usage is scientific research. Using LLMs
for generating or analyzing data for research purposes is gaining popularity.
But when such application is marred with ad-hoc decisions and engineering
solutions, we need to be concerned about how it may affect that research, its
findings, or any future works based on that research. We need a more scientific
approach to using LLMs in our research. While there are several active efforts
to support more systematic construction of prompts, they are often focused more
on achieving desirable outcomes rather than producing replicable and
generalizable knowledge with sufficient transparency, objectivity, or rigor.
This article presents a new methodology inspired by codebook construction
through qualitative methods to address that. Using humans in the loop and a
multi-phase verification processes, this methodology lays a foundation for more
systematic, objective, and trustworthy way of applying LLMs for analyzing data.
Specifically, we show how a set of researchers can work through a rigorous
process of labeling, deliberating, and documenting to remove subjectivity and
bring transparency and replicability to prompt generation process. A set of
experiments are presented to show how this methodology can be put in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chirag Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04124">
<title>MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. (arXiv:2401.04124v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04124</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents centered around Large Language Models (LLMs) are now capable of
automating mobile device operations for users. After fine-tuning to learn a
user&apos;s mobile operations, these agents can adhere to high-level user
instructions online. They execute tasks such as goal decomposition, sequencing
of sub-goals, and interactive environmental exploration, until the final
objective is achieved. However, privacy concerns related to personalized user
data arise during mobile operations, requiring user confirmation. Moreover,
users&apos; real-world operations are exploratory, with action data being complex
and redundant, posing challenges for agent learning. To address these issues,
in our practical application, we have designed interactive tasks between agents
and humans to identify sensitive information and align with personalized user
needs. Additionally, we integrated Standard Operating Procedure (SOP)
information within the model&apos;s in-context learning to enhance the agent&apos;s
comprehension of complex task execution. Our approach is evaluated on the new
device control benchmark AitW, which encompasses 30K unique instructions across
multi-step tasks, including application operation, web searching, and web
shopping. Experimental results show that the SOP-based agent achieves
state-of-the-art performance without incurring additional inference costs,
boasting an overall action success rate of 66.92%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tinghe Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04125">
<title>DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling. (arXiv:2401.04125v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2401.04125</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate weather forecasting holds significant importance to human
activities. Currently, there are two paradigms for weather forecasting:
Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP).
NWP utilizes atmospheric physics for weather modeling but suffers from poor
data utilization and high computational costs, while DLP can learn weather
patterns from vast amounts of data directly but struggles to incorporate
physical laws. Both paradigms possess their respective strengths and
weaknesses, and are incompatible, because physical laws adopted in NWP describe
the relationship between coordinates and meteorological variables, while DLP
directly learns the relationships between meteorological variables without
consideration of coordinates. To address these problems, we introduce the
DeepPhysiNet framework, incorporating physical laws into deep learning models
for accurate and continuous weather system modeling. First, we construct
physics networks based on multilayer perceptrons (MLPs) for individual
meteorological variable, such as temperature, pressure, and wind speed. Physics
networks establish relationships between variables and coordinates by taking
coordinates as input and producing variable values as output. The physical laws
in the form of Partial Differential Equations (PDEs) can be incorporated as a
part of loss function. Next, we construct hyper-networks based on deep learning
methods to directly learn weather patterns from a large amount of
meteorological data. The output of hyper-networks constitutes a part of the
weights for the physics networks. Experimental results demonstrate that, upon
successful integration of physical laws, DeepPhysiNet can accomplish multiple
tasks simultaneously, not only enhancing forecast accuracy but also obtaining
continuous spatiotemporal resolution results, which is unattainable by either
the NWP or DLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Keyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shunlin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zhengxia Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04126">
<title>The Concept of the Tactile Signature System for Individuals with Visual Impairments. (arXiv:2401.04126v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04126</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of an accessible and effective system for blind individuals to
create handwritten signatures presents a significant barrier to their
independence and full participation in various aspects of life. This research
introduces the Tactile Signature System, a groundbreaking approach that
empowers individuals with visual impairments to form their unique handwritten
signatures. Key features of the system include: Personalized customization:
Through tactile interaction and voice algorithmic guidance, individuals create
signatures reflecting their preferences and natural writing style. Real-time
feedback: AI-powered voice prompts and analysis ensure accuracy and consistency
in signature formation. Accessibility: Installation in local service centers
provides a secure and supervised environment for signature creation. The
system&apos;s impact reaches beyond the individual level: Promotes inclusivity and
independence: Blind individuals can engage in legal and financial transactions
without relying on others. Empowers and fosters equal opportunities:
Participation in education, employment, and civic engagement becomes more
accessible. Aligns with international conventions: Upholds the right of persons
with disabilities to participate fully in society. The Tactile Signature System
represents a significant step towards an inclusive and accessible future for
individuals with visual impairments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kremenchutskiy_A/0/1/0/all/0/1&quot;&gt;Anatoliy Kremenchutskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabdreshov_G/0/1/0/all/0/1&quot;&gt;Galymzhan Gabdreshov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04130">
<title>Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04130</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual
Prompt Tuning (VPT) have found success in enabling adaptation to new domains by
tuning small modules within a transformer model. However, the number of domains
encountered during test time can be very large, and the data is usually
unlabeled. Thus, adaptation to new domains is challenging; it is also
impractical to generate customized tuned modules for each such domain. Toward
addressing these challenges, this work introduces PLUTO: a Plug-and-pLay
modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of
modules, each specialized for different source domains, effectively creating a
``module store&apos;&apos;. Given a target domain with few-shot unlabeled data, we
introduce an unsupervised test-time adaptation (TTA) method to (1) select a
sparse subset of relevant modules from this store and (2) create a weighted
combination of selected modules without tuning their weights. This
plug-and-play nature enables us to harness multiple most-relevant source
domains in a single inference call. Comprehensive evaluations demonstrate that
PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5
modules suffice to extract most of the benefit. At a high level, our method
equips pre-trained transformers with the capability to dynamically adapt to new
domains, motivating a new paradigm for efficient and scalable domain
adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sk Miraj Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guler_B/0/1/0/all/0/1&quot;&gt;Basak Guler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1&quot;&gt;Srikanth V. Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swami_A/0/1/0/all/0/1&quot;&gt;Ananthram Swami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1&quot;&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04133">
<title>SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04133</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) excel in various domains, from detecting
e-commerce spam to social network classification problems. However, the lack of
public graph datasets hampers research progress, particularly in heterogeneous
information networks (HIN). The demand for datasets for fair HIN comparisons is
growing due to advancements in GNN interpretation models. In response, we
propose SynHIN, a unique method for generating synthetic heterogeneous
information networks. SynHIN identifies motifs in real-world datasets,
summarizes graph statistics, and constructs a synthetic network. Our approach
utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN
from primary motif clusters. After In/Our-Cluster mergers and a post-pruning
process fitting the real dataset constraints, we ensure the synthetic graph
statistics align closely with the reference one. SynHIN generates a synthetic
heterogeneous graph dataset for node classification tasks, using the primary
motif as the explanation ground truth. It can adapt and address the lack of
heterogeneous graph datasets and motif ground truths, proving beneficial for
assessing heterogeneous graph neural network explainers. We further present a
benchmark dataset for future heterogeneous graph explainer model research. Our
work marks a significant step towards explainable AI in HGNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Ming-Yi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1&quot;&gt;You-Chen Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chih-Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Che Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04134">
<title>Web Neural Network with Complete DiGraphs. (arXiv:2401.04134v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.04134</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new neural network model that aims to mimic the
biological brain more closely by structuring the network as a complete directed
graph that processes continuous data for each timestep. Current neural networks
have structures that vaguely mimic the brain structure, such as neurons,
convolutions, and recurrence. The model proposed in this paper adds additional
structural properties by introducing cycles into the neuron connections and
removing the sequential nature commonly seen in other network layers.
Furthermore, the model has continuous input and output, inspired by spiking
neural networks, which allows the network to learn a process of classification,
rather than simply returning the final result.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Frank Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04135">
<title>Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction. (arXiv:2401.04135v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04135</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic flow prediction plays a crucial role in alleviating traffic
congestion and enhancing transport efficiency. While combining graph
convolution networks with recurrent neural networks for spatial-temporal
modeling is a common strategy in this realm, the restricted structure of
recurrent neural networks limits their ability to capture global information.
For spatial modeling, many prior studies learn a graph structure that is
assumed to be fixed and uniform at all time steps, which may not be true. This
paper introduces a novel traffic prediction framework, Global-Aware Enhanced
Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core
components: a spatial-temporal graph recurrent neural network and a global
awareness layer. Within this framework, three innovative prediction models are
formulated. A sequence-aware graph neural network is proposed and integrated
into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time
steps and capture local temporal relationships. To enhance the model&apos;s global
perception, three distinct global spatial-temporal transformer-like
architectures (GST^2) are devised for the global awareness layer. We conduct
extensive experiments on four real traffic datasets and the results demonstrate
the superiority of our framework and the three concrete models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chunjiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Detian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04136">
<title>The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. (arXiv:2401.04136v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.04136</link>
<description rdf:parseType="Literal">&lt;p&gt;The commercialization of diffusion models, renowned for their ability to
generate high-quality images that are often indistinguishable from real ones,
brings forth potential copyright concerns. Although attempts have been made to
impede unauthorized access to copyrighted material during training and to
subsequently prevent DMs from generating copyrighted images, the effectiveness
of these solutions remains unverified. This study explores the vulnerabilities
associated with copyright protection in DMs by introducing a backdoor data
poisoning attack (SilentBadDiffusion) against text-to-image diffusion models.
Our attack method operates without requiring access to or control over the
diffusion model&apos;s training or fine-tuning processes; it merely involves the
insertion of poisoning data into the clean training dataset. This data,
comprising poisoning images equipped with prompts, is generated by leveraging
the powerful capabilities of multimodal large language models and text-guided
image inpainting techniques. Our experimental results and analysis confirm the
method&apos;s effectiveness. By integrating a minor portion of
non-copyright-infringing stealthy poisoning data into the clean
dataset-rendering it free from suspicion-we can prompt the finetuned diffusion
models to produce copyrighted content when activated by specific trigger
prompts. These findings underline potential pitfalls in the prevailing
copyright protection strategies and underscore the necessity for increased
scrutiny and preventative measures against the misuse of DMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1&quot;&gt;Qianli Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1&quot;&gt;Yao Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04138">
<title>Expanding Horizons in HCI Research Through LLM-Driven Qualitative Analysis. (arXiv:2401.04138v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04138</link>
<description rdf:parseType="Literal">&lt;p&gt;How would research be like if we still needed to &quot;send&quot; papers typed with a
typewriter? Our life and research environment have continually evolved, often
accompanied by controversial opinions about new methodologies. In this paper,
we embrace this change by introducing a new approach to qualitative analysis in
HCI using Large Language Models (LLMs). We detail a method that uses LLMs for
qualitative data analysis and present a quantitative framework using SBART
cosine similarity for performance evaluation. Our findings indicate that LLMs
not only match the efficacy of traditional analysis methods but also offer
unique insights. Through a novel dataset and benchmark, we explore LLMs&apos;
characteristics in HCI research, suggesting potential avenues for further
exploration and application in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torii_M/0/1/0/all/0/1&quot;&gt;Maya Grace Torii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murakami_T/0/1/0/all/0/1&quot;&gt;Takahito Murakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ochiai_Y/0/1/0/all/0/1&quot;&gt;Yoichi Ochiai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04141">
<title>On The Potential of The Fractal Geometry and The CNNs Ability to Encode it. (arXiv:2401.04141v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04141</link>
<description rdf:parseType="Literal">&lt;p&gt;The fractal dimension provides a statistical index of object complexity by
studying how the pattern changes with the measuring scale. Although useful in
several classification tasks, the fractal dimension is under-explored in deep
learning applications. In this work, we investigate the features that are
learned by deep models and we study whether these deep networks are able to
encode features as complex and high-level as the fractal dimensions.
Specifically, we conduct a correlation analysis experiment to show that deep
networks are not able to extract such a feature in none of their layers. We
combine our analytical study with a human evaluation to investigate the
differences between deep learning networks and models that operate on the
fractal feature solely. Moreover, we show the effectiveness of fractal features
in applications where the object structure is crucial for the classification
task. We empirically show that training a shallow network on fractal features
achieves performance comparable, even superior in specific cases, to that of
deep networks trained on raw data while requiring less computational resources.
Fractals improved the accuracy of the classification by 30% on average while
requiring up to 84% less time to train. We couple our empirical study with a
complexity analysis of the computational cost of extracting the proposed
fractal features, and we study its limitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zini_J/0/1/0/all/0/1&quot;&gt;Julia El Zini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musharrafieh_B/0/1/0/all/0/1&quot;&gt;Bassel Musharrafieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1&quot;&gt;Mariette Awad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04144">
<title>Robust Calibration For Improved Weather Prediction Under Distributional Shift. (arXiv:2401.04144v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04144</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present results on improving out-of-domain weather
prediction and uncertainty estimation as part of the \texttt{Shifts Challenge
on Robustness and Uncertainty under Real-World Distributional Shift} challenge.
We find that by leveraging a mixture of experts in conjunction with an advanced
data augmentation technique borrowed from the computer vision domain, in
conjunction with robust \textit{post-hoc} calibration of predictive
uncertainties, we can potentially achieve more accurate and better-calibrated
results with deep neural networks than with boosted tree models for tabular
data. We quantify our predictions using several metrics and propose several
future lines of inquiry and experimentation to boost performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilda_S/0/1/0/all/0/1&quot;&gt;Sankalp Gilda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1&quot;&gt;Neel Bhandari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mak_W/0/1/0/all/0/1&quot;&gt;Wendy Mak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panizza_A/0/1/0/all/0/1&quot;&gt;Andrea Panizza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04145">
<title>Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning. (arXiv:2401.04145v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04145</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (DRL) methods have recently shown promise in path
planning tasks. However, when dealing with global planning tasks, these methods
face serious challenges such as poor convergence and generalization. To this
end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan
Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems
from the perspective of DRL&apos;s observation, revealing that the traditional
design causes DRL to be interfered by irrelevant map information. Secondly, we
develop the LOPA which utilizes a novel attention-enhanced mechanism to attain
an improved attention capability towards the key information of the
observation. Such a mechanism is realized by two steps: (1) an attention model
is built to transform the DRL&apos;s observation into two dynamic views: local and
global, significantly guiding the LOPA to focus on the key information on the
given maps; (2) a dual-channel network is constructed to process these two
views and integrate them to attain an improved reasoning capability. The LOPA
is validated via multi-objective global path planning experiments. The result
suggests the LOPA has improved convergence and generalization performance as
well as great path planning efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guoming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1&quot;&gt;Mingxin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaofang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shuqiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04148">
<title>Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting. (arXiv:2401.04148v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04148</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate spatial-temporal traffic flow forecasting is crucial in aiding
traffic managers in implementing control measures and assisting drivers in
selecting optimal travel routes. Traditional deep-learning based methods for
traffic flow forecasting typically rely on historical data to train their
models, which are then used to make predictions on future data. However, the
performance of the trained model usually degrades due to the temporal drift
between the historical and future data. To make the model trained on historical
data better adapt to future data in a fully online manner, this paper conducts
the first study of the online test-time adaptation techniques for
spatial-temporal traffic flow forecasting problems. To this end, we propose an
Adaptive Double Correction by Series Decomposition (ADCSD) method, which first
decomposes the output of the trained model into seasonal and trend-cyclical
parts and then corrects them by two separate modules during the testing phase
using the latest observed data entry by entry. In the proposed ADCSD method,
instead of fine-tuning the whole trained model during the testing phase, a lite
network is attached after the trained model, and only the lite network is
fine-tuned in the testing process each time a data entry is observed. Moreover,
to satisfy that different time series variables may have different levels of
temporal drift, two adaptive vectors are adopted to provide different weights
for different time series variables. Extensive experiments on four real-world
traffic flow forecasting datasets demonstrate the effectiveness of the proposed
ADCSD method. The code is available at https://github.com/Pengxin-Guo/ADCSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pengxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Pengrong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04152">
<title>Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.04152</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end multi-talker speech recognition has garnered great interest as an
effective approach to directly transcribe overlapped speech from multiple
speakers. Current methods typically adopt either 1) single-input
multiple-output (SIMO) models with a branched encoder, or 2) single-input
single-output (SISO) models based on attention-based encoder-decoder
architecture with serialized output training (SOT). In this work, we propose a
Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models
by aggregating cross-speaker representations. Furthermore, the CSE model is
integrated with SOT to leverage both the advantages of SIMO and SISO while
mitigating their drawbacks. To the best of our knowledge, this work represents
an early effort to integrate SIMO and SISO for multi-talker speech recognition.
Experiments on the two-speaker LibrispeechMix dataset show that the CES model
reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model
reduces WER by 10% overall and by 16% on high-overlap speech compared to the
SOT model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiawen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lingwei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Mingyu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Haohan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xixin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xunying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1&quot;&gt;Helen Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04154">
<title>Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification. (arXiv:2401.04154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04154</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio and video are two most common modalities in the mainstream media
platforms, e.g., YouTube. To learn from multimodal videos effectively, in this
work, we propose a novel audio-video recognition approach termed audio video
Transformer, AVT, leveraging the effective spatio-temporal representation by
the video Transformer to improve action recognition accuracy. For multimodal
fusion, simply concatenating multimodal tokens in a cross-modal Transformer
requires large computational and memory resources, instead we reduce the
cross-modality complexity through an audio-video bottleneck Transformer. To
improve the learning efficiency of multimodal Transformer, we integrate
self-supervised objectives, i.e., audio-video contrastive learning, audio-video
matching, and masked audio and video learning, into AVT training, which maps
diverse audio and video representations into a common multimodal representation
space. We further propose a masked audio segment loss to learn semantic audio
activities in AVT. Extensive experiments and ablation studies on three public
datasets and two in-house datasets consistently demonstrate the effectiveness
of the proposed AVT. Specifically, AVT outperforms its previous
state-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one
of the previous state-of-the-art video Transformers [25] by 10% on VGGSound by
leveraging the audio signal. Compared to one of the previous state-of-the-art
multimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and
improves the accuracy by 3.8% on Epic-Kitchens-100.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04192">
<title>Interactive Multi-Objective Evolutionary Optimization of Software Architectures. (arXiv:2401.04192v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.04192</link>
<description rdf:parseType="Literal">&lt;p&gt;While working on a software specification, designers usually need to evaluate
different architectural alternatives to be sure that quality criteria are met.
Even when these quality aspects could be expressed in terms of multiple
software metrics, other qualitative factors cannot be numerically measured, but
they are extracted from the engineer&apos;s know-how and prior experiences. In fact,
detecting not only strong but also weak points in the different solutions seems
to fit better with the way humans make their decisions. Putting the human in
the loop brings new challenges to the search-based software engineering field,
especially for those human-centered activities within the early analysis phase.
This paper explores how the interactive evolutionary computation can serve as a
basis for integrating the human&apos;s judgment into the search process. An
interactive approach is proposed to discover software architectures, in which
both quantitative and qualitative criteria are applied to guide a
multi-objective evolutionary algorithm. The obtained feedback is incorporated
into the fitness function using architectural preferences allowing the
algorithm to discern between promising and poor solutions. Experimentation with
real users has revealed that the proposed interaction mechanism can effectively
guide the search towards those regions of the search space that are of real
interest to the expert.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_A/0/1/0/all/0/1&quot;&gt;Aurora Ram&amp;#xed;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Ra&amp;#xfa;l Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventura_S/0/1/0/all/0/1&quot;&gt;Sebasti&amp;#xe1;n Ventura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04198">
<title>Curiosity &amp; Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04198</link>
<description rdf:parseType="Literal">&lt;p&gt;The authors of &apos;Unsupervised Reinforcement Learning in Multiple environments&apos;
propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple
environments. They pre-train a task-agnostic exploration policy using
interactions from an entire environment class and then fine-tune this policy
for various tasks using supervision. We expanded upon this work, with the goal
of improving performance. We primarily propose and experiment with five new
modifications to the original work: sampling trajectories using an
entropy-based probability distribution, dynamic alpha, higher KL Divergence
threshold, curiosity-driven exploration, and alpha-percentile sampling on
curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a
significant improvement over the baseline from the earlier work. PDF-sampling
failed to provide any improvement due to it being approximately equivalent to
the baseline method when the sample space is small. In high-dimensional
environments, the addition of curiosity-driven exploration enhances learning by
encouraging the agent to seek diverse experiences and explore the unknown more.
However, its benefits are limited in low-dimensional and simpler environments
where exploration possibilities are constrained and there is little that is
truly unknown to the agent. Overall, some of our experiments did boost
performance over the baseline and there are a few directions that seem
promising for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dewan_S/0/1/0/all/0/1&quot;&gt;Shaurya Dewan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anisha Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LaLena_Z/0/1/0/all/0/1&quot;&gt;Zoe LaLena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lifan Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04206">
<title>Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04206</link>
<description rdf:parseType="Literal">&lt;p&gt;In a pre-post experiment (n = 41), we test the impact of an AI Coach&apos;s
explanatory communications modeled after the instructions of human driving
experts. Participants were divided into four (4) groups to assess two (2)
dimensions of the AI coach&apos;s explanations: information type (&apos;what&apos; and
&apos;why&apos;-type explanations) and presentation modality (auditory and visual). We
directly compare how AI Coaching sessions employing these techniques impact
driving performance, cognitive load, confidence, expertise, and trust in an
observation learning context. Through interviews, we delineate the learning
process of our participants. Results show that an AI driving coach can be
useful for teaching performance driving skills to novices. Comparing between
groups, we find the type and modality of information influences performance
outcomes. We attribute differences to how information directed attention,
mitigated uncertainty, and influenced overload experienced by participants.
These, in turn, affected how successfully participants were able to learn.
Results suggest efficient, modality-appropriate explanations should be opted
for when designing effective HMI communications that can instruct without
overwhelming. Further, they support the need to align communications with human
learning and cognitive processes. Results are synthesized into eight design
implications for future autonomous vehicle HMI and AI coach design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufman_R/0/1/0/all/0/1&quot;&gt;Robert Kaufman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_J/0/1/0/all/0/1&quot;&gt;Jean Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimani_E/0/1/0/all/0/1&quot;&gt;Everlyne Kimani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04210">
<title>FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04210</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically understanding funny moments (i.e., the moments that make people
laugh) when watching comedy is challenging, as they relate to various features,
such as body language, dialogues and culture. In this paper, we propose
FunnyNet-W, a model that relies on cross- and self-attention for visual, audio
and text data to predict funny moments in videos. Unlike most methods that rely
on ground truth data in the form of subtitles, in this work we exploit
modalities that come naturally with videos: (a) video frames as they contain
visual information indispensable for scene understanding, (b) audio as it
contains higher-level cues associated with funny moments, such as intonation,
pitch and pauses and (c) text automatically extracted with a speech-to-text
model as it can provide rich information when processed by a Large Language
Model. To acquire labels for training, we propose an unsupervised approach that
spots and labels funny audio moments. We provide experiments on five datasets:
the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive
experiments and analysis show that FunnyNet-W successfully exploits visual,
auditory and textual cues to identify funny moments, while our findings reveal
FunnyNet-W&apos;s ability to predict funny moments in the wild. FunnyNet-W sets the
new state of the art for funny moment detection with multimodal cues on all
datasets with and without using ground truth information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi-Song Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courant_R/0/1/0/all/0/1&quot;&gt;Robin Courant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1&quot;&gt;Vicky Kalogeiton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04247">
<title>Robust Image Watermarking using Stable Diffusion. (arXiv:2401.04247v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;Watermarking images is critical for tracking image provenance and claiming
ownership. With the advent of generative models, such as stable diffusion, able
to create fake but realistic images, watermarking has become particularly
important, e.g., to make generated images reliably identifiable. Unfortunately,
the very same stable diffusion technology can remove watermarks injected using
existing methods. To address this problem, we present a ZoDiac, which uses a
pre-trained stable diffusion model to inject a watermark into the trainable
latent space, resulting in watermarks that can be reliably detected in the
latent vector, even when attacked. We evaluate ZoDiac on three benchmarks,
MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against
state-of-the-art watermark attacks, with a watermark detection rate over 98%
and a false positive rate below 6.4%, outperforming state-of-the-art
watermarking methods. Our research demonstrates that stable diffusion is a
promising approach to robust watermarking, able to withstand even
stable-diffusion-based attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1&quot;&gt;Antoni Viros Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bearfield_C/0/1/0/all/0/1&quot;&gt;Cindy Xiong Bearfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brun_Y/0/1/0/all/0/1&quot;&gt;Yuriy Brun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1&quot;&gt;Hui Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04290">
<title>StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments. (arXiv:2401.04290v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04290</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial reasoning tasks in multi-agent environments such as event prediction,
agent type identification, or missing data imputation are important for
multiple applications (e.g., autonomous surveillance over sensor networks and
subtasks for reinforcement learning (RL)). StarCraft II game replays encode
intelligent (and adversarial) multi-agent behavior and could provide a testbed
for these tasks; however, extracting simple and standardized representations
for prototyping these tasks is laborious and hinders reproducibility. In
contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled
rapid prototyping and reproducibility of ML methods. Following the simplicity
of these datasets, we construct a benchmark spatial reasoning dataset based on
StarCraft II replays that exhibit complex multi-agent behaviors, while still
being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize
a window of 255 consecutive game states to create 3.6 million summary images
from 60,000 replays, including all relevant metadata such as game outcome and
player races. We develop three formats of decreasing complexity: Hyperspectral
images that include one channel for every unit type (similar to multispectral
geospatial images), RGB images that mimic CIFAR10, and grayscale images that
mimic MNIST. We show how this dataset can be used for prototyping spatial
reasoning methods. All datasets, code for extraction, and code for dataset
loading can be found at https://starcraftdata.davidinouye.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1&quot;&gt;Sean Kulinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1&quot;&gt;James Z. Hare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1&quot;&gt;David I. Inouye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04319">
<title>Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04319</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore a new way for user targeting, where non-expert
marketers could select their target users solely given demands in natural
language form. The key to this issue is how to transform natural languages into
practical structured logical languages, i.e., the structured understanding of
marketer demands. Considering the impressive natural language processing
ability of large language models (LLMs), we try to leverage LLMs to solve this
issue. Past research indicates that the reasoning ability of LLMs can be
effectively enhanced through chain-of-thought (CoT) prompting. But existing
methods still have some limitations: (1) Previous methods either use simple
&quot;Let&apos;s think step by step&quot; spells or provide fixed examples in demonstrations
without considering compatibility between prompts and questions, making LLMs
ineffective in some complex reasoning tasks such as structured language
transformation. (2) Previous methods are often implemented in closed-source
models or excessively large models, which is not suitable in industrial
practical scenarios. Based on these, we propose ARALLM (i.e., Analogical
Reasoning Augmented Large Language Models) consisting of two modules:
Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model
Distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Binbin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yue Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04330">
<title>BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation. (arXiv:2401.04330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04330</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of remote sensing image change detection (RSCD) is to detect
differences between bi-temporal images taken at the same place. Deep learning
has been extensively used to RSCD tasks, yielding significant results in terms
of result recognition. However, due to the shooting angle of the satellite, the
impacts of thin clouds, and certain lighting conditions, the problem of fuzzy
edges in the change region in some remote sensing photographs cannot be
properly handled using current RSCD algorithms. To solve this issue, we
proposed a Body Decouple Multi-Scale by fearure Aggregation change detection
(BD-MSA), a novel model that collects both global and local feature map
information in the channel and space dimensions of the feature map during the
training and prediction phases. This approach allows us to successfully extract
the change region&apos;s boundary information while also divorcing the change
region&apos;s main body from its boundary. Numerous studies have shown that the
assessment metrics and evaluation effects of the model described in this paper
on the publicly available datasets DSIFN-CD and S2Looking are the best when
compared to other models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yonghui Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yishu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1&quot;&gt;Jinquan Ai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04331">
<title>Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04331</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we rigorously investigate the robustness of graph neural
fractional-order differential equation (FDE) models. This framework extends
beyond traditional graph neural (integer-order) ordinary differential equation
(ODE) models by implementing the time-fractional Caputo derivative. Utilizing
fractional calculus allows our model to consider long-term memory during the
feature updating process, diverging from the memoryless Markovian updates seen
in traditional graph neural ODE models. The superiority of graph neural FDE
models over graph neural ODE models has been established in environments free
from attacks or perturbations. While traditional graph neural ODE models have
been verified to possess a degree of stability and resilience in the presence
of adversarial attacks in existing literature, the robustness of graph neural
FDE models, especially under adversarial conditions, remains largely
unexplored. This paper undertakes a detailed assessment of the robustness of
graph neural FDE models. We establish a theoretical foundation outlining the
robustness characteristics of graph neural FDE models, highlighting that they
maintain more stringent output perturbation bounds in the face of input and
graph topology disturbances, compared to their integer-order counterparts. Our
empirical evaluations further confirm the enhanced robustness of graph neural
FDE models, highlighting their potential in adversarially robust applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qiyu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yihang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yanan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_R/0/1/0/all/0/1&quot;&gt;Rui She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04334">
<title>Large Language Models for Robotics: Opportunities, Challenges, and Perspectives. (arXiv:2401.04334v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.04334</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have undergone significant expansion and have
been increasingly integrated across various domains. Notably, in the realm of
robot task planning, LLMs harness their advanced reasoning and language
comprehension capabilities to formulate precise and efficient action plans
based on natural language instructions. However, for embodied tasks, where
robots interact with complex environments, text-only LLMs often face challenges
due to a lack of compatibility with robotic visual perception. This study
provides a comprehensive overview of the emerging integration of LLMs and
multimodal LLMs into various robotic tasks. Additionally, we propose a
framework that utilizes multimodal GPT-4V to enhance embodied task planning
through the combination of natural language instructions and robot visual
perceptions. Our results, based on diverse datasets, indicate that GPT-4V
effectively enhances robot performance in embodied tasks. This extensive survey
and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks
enriches the understanding of LLM-centric embodied intelligence and provides
forward-looking insights toward bridging the gap in Human-Robot-Environment
interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hanqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_P/0/1/0/all/0/1&quot;&gt;Peng Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_E/0/1/0/all/0/1&quot;&gt;Enze Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Huawen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yincheng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huaqin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Haixing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1&quot;&gt;Bao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04336">
<title>Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04336</link>
<description rdf:parseType="Literal">&lt;p&gt;Behemoth graphs are often fragmented and separately stored by multiple data
owners as distributed subgraphs in many realistic applications. Without harming
data privacy, it is natural to consider the subgraph federated learning
(subgraph FL) scenario, where each local client holds a subgraph of the entire
global graph, to obtain globally generalized graph mining models. To overcome
the unique challenge of incomplete information propagation on local subgraphs
due to missing cross-subgraph neighbors, previous works resort to the
augmentation of local neighborhoods through the joint FL of missing neighbor
generators and GNNs. Yet their technical designs have profound limitations
regarding the utility, efficiency, and privacy goals of FL. In this work, we
propose FedDEP to comprehensively tackle these challenges in subgraph FL.
FedDEP consists of a series of novel technical designs: (1) Deep neighbor
generation through leveraging the GNN embeddings of potential missing
neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding
prototyping; and (3) Privacy protection through noise-less
edge-local-differential-privacy.
&lt;/p&gt;
&lt;p&gt;We analyze the correctness and efficiency of FedDEP, and provide theoretical
guarantees on its privacy.
&lt;/p&gt;
&lt;p&gt;Empirical results on four real-world datasets justify the clear benefits of
proposed techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Ke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bolin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1&quot;&gt;Siu Ming Yiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Carl Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04339">
<title>Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04339</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of billion-parameter diffusion models like Stable Diffusion XL,
Imagen, and Dall-E3 markedly advances the field of generative AI. However,
their large-scale nature poses challenges in fine-tuning and deployment due to
high resource demands and slow inference speed. This paper ventures into the
relatively unexplored yet promising realm of fine-tuning quantized diffusion
models. We establish a strong baseline by customizing three models: PEQA for
fine-tuning quantization parameters, Q-Diffusion for post-training
quantization, and DreamBooth for personalization. Our analysis reveals a
notable trade-off between subject and prompt fidelity within the baseline
model. To address these issues, we introduce two strategies, inspired by the
distinct roles of different timesteps in diffusion models: S1 optimizing a
single set of fine-tuning parameters exclusively at selected intervals, and S2
creating multiple fine-tuning parameter sets, each specialized for different
timestep intervals. Our approach not only enhances personalization but also
upholds prompt fidelity and image quality, significantly outperforming the
baseline qualitatively and quantitatively. The code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyogon Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Seohyun Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04351">
<title>A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions. (arXiv:2401.04351v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04351</link>
<description rdf:parseType="Literal">&lt;p&gt;By informing the onset of the degradation process, health status evaluation
serves as a significant preliminary step for reliable remaining useful life
(RUL) estimation of complex equipment. This paper proposes a novel temporal
dynamics learning-based model for detecting change points of individual
devices, even under variable operating conditions, and utilises the learnt
change points to improve the RUL estimation accuracy. During offline model
development, the multivariate sensor data are decomposed to learn fused
temporal correlation features that are generalisable and representative of
normal operation dynamics across multiple operating conditions. Monitoring
statistics and control limit thresholds for normal behaviour are dynamically
constructed from these learnt temporal features for the unsupervised detection
of device-level change points. The detected change points then inform the
degradation data labelling for training a long short-term memory (LSTM)-based
RUL estimation model. During online monitoring, the temporal correlation
dynamics of a query device is monitored for breach of the control limit derived
in offline training. If a change point is detected, the device&apos;s RUL is
estimated with the well-trained offline model for early preventive action.
Using C-MAPSS turbofan engines as the case study, the proposed method improved
the accuracy by 5.6\% and 7.5\% for two scenarios with six operating
conditions, when compared to existing LSTM-based RUL estimation models that do
not consider heterogeneous change points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arunan_A/0/1/0/all/0/1&quot;&gt;Anushiya Arunan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_C/0/1/0/all/0/1&quot;&gt;Chau Yuen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04357">
<title>Iterative Feedback Network for Unsupervised Point Cloud Registration. (arXiv:2401.04357v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04357</link>
<description rdf:parseType="Literal">&lt;p&gt;As a fundamental problem in computer vision, point cloud registration aims to
seek the optimal transformation for aligning a pair of point clouds. In most
existing methods, the information flows are usually forward transferring, thus
lacking the guidance from high-level information to low-level information.
Besides, excessive high-level information may be overly redundant, and directly
using it may conflict with the original low-level information. In this paper,
we propose a novel Iterative Feedback Network (IFNet) for unsupervised point
cloud registration, in which the representation of low-level features is
efficiently enriched by rerouting subsequent high-level features. Specifically,
our IFNet is built upon a series of Feedback Registration Block (FRB) modules,
with each module responsible for generating the feedforward rigid
transformation and feedback high-level features. These FRB modules are cascaded
and recurrently unfolded over time. Further, the Feedback Transformer is
designed to efficiently select relevant information from feedback high-level
features, which is utilized to refine the low-level features. What&apos;s more, we
incorporate a geometry-awareness descriptor to empower the network for making
full use of most geometric information, which leads to more precise
registration results. Extensive experiments on various benchmark datasets
demonstrate the superior registration performance of our IFNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yifan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jihua Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04361">
<title>Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04361</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge-grounded dialogue (KGD) learns to generate an informative response
based on a given dialogue context and external knowledge (\emph{e.g.},
knowledge graphs; KGs). Recently, the emergence of large language models (LLMs)
and pre-training techniques has brought great success to knowledge-grounded
dialogue. However, when building KGD systems in real applications, there are
various real-world noises that are inevitable to face. For example, the
dialogue context might involve perturbations such as misspellings and
abbreviations. In addition, KGs typically suffer from incompletion and also
might contain erroneous and outdated facts. Such real-world noises pose a
challenge to the robustness of KGD systems and hinder their applications in the
real world. In this paper, we propose an entity-based contrastive learning
framework for improving the robustness of KGD. Specifically, we make use of the
entity information in a KGD sample to create both its positive and negative
samples which involve semantic-irrelevant and semantic-relevant perturbations,
respectively. The contrastive learning framework ensures the KGD model is aware
of these two types of perturbations, thus generating informative responses with
the potentially noisy inputs in real applications. Experimental results on
three benchmark datasets show that our method achieves new state-of-the-art
performance in terms of automatic evaluation scores, verifying its
effectiveness and potentiality. Furthermore, we show that our method can
generate better responses than comparison models in both the noisy and the
few-shot settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Jianfeng Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wen Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Ximing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;An Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04362">
<title>Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example. (arXiv:2401.04362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04362</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DiffSketch, a method for generating a variety of stylized
sketches from images. Our approach focuses on selecting representative features
from the rich semantics of deep features within a pretrained diffusion model.
This novel sketch generation method can be trained with one manual drawing.
Furthermore, efficient sketch extraction is ensured by distilling a trained
generator into a streamlined extractor. We select denoising diffusion features
through analysis and integrate these selected features with VAE features to
produce sketches. Additionally, we propose a sampling scheme for training
models using a conditional generative approach. Through a series of
comparisons, we verify that distilled DiffSketch not only outperforms existing
state-of-the-art sketch extraction methods but also surpasses diffusion-based
stylization methods in the task of extracting sketches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1&quot;&gt;Kwan Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngseo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_K/0/1/0/all/0/1&quot;&gt;Kwanggyoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1&quot;&gt;Chang Wook Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1&quot;&gt;Junyong Noh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04374">
<title>Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.04374</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the complexity and lack of transparency in deep neural networks (DNNs),
extensive efforts have been made to make these systems more interpretable or
explain their behaviors in accessible terms. Unlike most reviews, which focus
on algorithmic and model-centric perspectives, this work takes a &quot;data-centric&quot;
view, examining how data collection, processing, and analysis contribute to
explainable AI (XAI). We categorize existing work into three categories subject
to their purposes: interpretations of deep models, referring to feature
attributions and reasoning processes that correlate data points with model
outputs; influences of training data, examining the impact of training data
nuances, such as data valuation and sample anomalies, on decision-making
processes; and insights of domain knowledge, discovering latent patterns and
fostering new knowledge from data and models to advance social values and
scientific discovery. Specifically, we distill XAI methodologies into data
mining operations on training and testing data across modalities, such as
images, text, and tabular data, as well as on training logs, checkpoints,
models and other DNN behavior descriptors. In this way, our study offers a
comprehensive, data-centric examination of XAI from a lens of data mining
methods and applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+L_X/0/1/0/all/0/1&quot;&gt;Xuhong L&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiamin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinhao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zeyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04385">
<title>Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04385</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning techniques, which involve retracting data records and
reducing influence of said data on trained models, help with the user privacy
protection objective but incur significant computational costs. Weight
perturbation-based unlearning is a general approach, but it typically involves
globally modifying the parameters. We propose fine-grained Top-K and Random-k
parameters perturbed inexact machine unlearning strategies that address the
privacy needs while keeping the computational costs tractable.
&lt;/p&gt;
&lt;p&gt;In order to demonstrate the efficacy of our strategies we also tackle the
challenge of evaluating the effectiveness of machine unlearning by considering
the model&apos;s generalization performance across both unlearning and remaining
data. To better assess the unlearning effect and model generalization, we
propose novel metrics, namely, the forgetting rate and memory retention rate.
However, for inexact machine unlearning, current metrics are inadequate in
quantifying the degree of forgetting that occurs after unlearning strategies
are applied. To address this, we introduce SPD-GAN, which subtly perturbs the
distribution of data targeted for unlearning. Then, we evaluate the degree of
unlearning by measuring the performance difference of the models on the
perturbed unlearning data before and after the unlearning process. By
implementing these innovative techniques and metrics, we achieve
computationally efficacious privacy protection in machine learning applications
without significant sacrifice of model performance. Furthermore, this approach
provides a novel method for evaluating the degree of unlearning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhuo Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kenli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1&quot;&gt;Anwitaman Datta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04402">
<title>IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records. (arXiv:2401.04402v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04402</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic Health Records present a valuable modality for driving
personalized medicine, where treatment is tailored to fit individual-level
differences. For this purpose, many data-driven machine learning and
statistical models rely on the wealth of longitudinal EHRs to study patients&apos;
physiological and treatment effects. However, longitudinal EHRs tend to be
sparse and highly missing, where missingness could also be informative and
reflect the underlying patient&apos;s health status. Therefore, the success of
data-driven models for personalized medicine highly depends on how the EHR data
is represented from physiological data, treatments, and the missing values in
the data. To this end, we propose a novel deep-learning model that learns the
underlying patient dynamics over time across multivariate data to generate
personalized realistic values conditioning on an individual&apos;s demographic
characteristics and treatments. Our proposed model, IGNITE (Individualized
GeNeration of Imputations in Time-series Electronic health records), utilises a
conditional dual-variational autoencoder augmented with dual-stage attention to
generate missing values for an individual. In IGNITE, we further propose a
novel individualized missingness mask (IMM), which helps our model generate
values based on the individual&apos;s observed data and missingness patterns. We
further extend the use of IGNITE from imputing missingness to a personalized
data synthesizer, where it generates missing EHRs that were never observed
prior or even generates new patients for various applications. We validate our
model on three large publicly available datasets and show that IGNITE
outperforms state-of-the-art approaches in missing data reconstruction and task
prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosheh_G/0/1/0/all/0/1&quot;&gt;Ghadeer O. Ghosheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tingting Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04405">
<title>Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation. (arXiv:2401.04405v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2401.04405</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive video streaming requires efficient bitrate ladder construction to
meet heterogeneous network conditions and end-user demands. Per-title optimized
encoding typically traverses numerous encoding parameters to search the
Pareto-optimal operating points for each video. Recently, researchers have
attempted to predict the content-optimized bitrate ladder for pre-encoding
overhead reduction. However, existing methods commonly estimate the encoding
parameters on the Pareto front and still require subsequent pre-encodings. In
this paper, we propose to directly predict the optimal transcoding resolution
at each preset bitrate for efficient bitrate ladder construction. We adopt a
Temporal Attentive Gated Recurrent Network to capture spatial-temporal features
and predict transcoding resolutions as a multi-task classification problem. We
demonstrate that content-optimized bitrate ladders can thus be efficiently
determined without any pre-encoding. Our method well approximates the
ground-truth bitrate-resolution pairs with a slight Bj{\o}ntegaard Delta rate
loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinhai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mengxi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04422">
<title>Estimating Text Similarity based on Semantic Concept Embeddings. (arXiv:2401.04422v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04422</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings
enjoy great success in the semantic representation of words, sentences, and
whole documents as well as for semantic similarity estimation. However, they
have the shortcoming that they are directly extracted from a surface
representation, which does not adequately represent human thought processes and
also performs poorly for highly ambiguous words. Therefore, we propose Semantic
Concept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism,
which addresses both shortcomings. The evaluation on a marketing target group
distribution task showed that the accuracy of predicted target groups can be
increased by combining traditional word embeddings with semantic CEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruck_T/0/1/0/all/0/1&quot;&gt;Tim vor der Br&amp;#xfc;ck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouly_M/0/1/0/all/0/1&quot;&gt;Marc Pouly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04429">
<title>i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance. (arXiv:2401.04429v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.04429</link>
<description rdf:parseType="Literal">&lt;p&gt;Ride-hailing platforms have been facing the challenge of balancing demand and
supply. Existing vehicle reposition techniques often treat drivers as
homogeneous agents and relocate them deterministically, assuming compliance
with the reposition. In this paper, we consider a more realistic and
driver-centric scenario where drivers have unique cruising preferences and can
decide whether to take the recommendation or not on their own. We propose
i-Rebalance, a personalized vehicle reposition technique with deep
reinforcement learning (DRL). i-Rebalance estimates drivers&apos; decisions on
accepting reposition recommendations through an on-field user study involving
99 real drivers. To optimize supply-demand balance and enhance preference
satisfaction simultaneously, i-Rebalance has a sequential reposition strategy
with dual DRL agents: Grid Agent to determine the reposition order of idle
vehicles, and Vehicle Agent to provide personalized recommendations to each
vehicle in the pre-defined order. This sequential learning strategy facilitates
more effective policy training within a smaller action space compared to
traditional joint-action methods. Evaluation of real-world trajectory data
shows that i-Rebalance improves driver acceptance rate by 38.07% and total
driver income by 9.97%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peiyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qiyuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wanyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weiwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wencan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Guanyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yan Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04437">
<title>Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods. (arXiv:2401.04437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04437</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies try to use hyperspectral imaging (HSI) to detect foreign
matters in products because it enables to visualize the invisible wavelengths
including ultraviolet and infrared. Considering the enormous image channels of
the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be
considered to reduce but those cannot ease the fundamental limitations, as
follows: (1) latency of HSI capturing. (2) less explanation ability of the
important channels. In this paper, to circumvent the aforementioned methods,
one of the ways to channel reduction, on anomaly detection proposed HSI.
Different from feature extraction methods (i.e., PCA or UMAP), feature
selection can sort the feature by impact and show better explainability so we
might redesign the task-optimized and cost-effective spectroscopic camera. Via
the extensive experiment results with synthesized MVTec AD dataset, we confirm
that the feature selection method shows 6.90x faster at the inference phase
compared with feature extraction-based approaches while preserving anomaly
detection performance. Ultimately, we conclude the advantage of feature
selection which is effective yet fast.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;YeongHyeon Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04441">
<title>Image classification network enhancement methods based on knowledge injection. (arXiv:2401.04441v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04441</link>
<description rdf:parseType="Literal">&lt;p&gt;The current deep neural network algorithm still stays in the end-to-end
training supervision method like Image-Label pairs, which makes traditional
algorithm is difficult to explain the reason for the results, and the
prediction logic is difficult to understand and analyze. The current algorithm
does not use the existing human knowledge information, which makes the model
not in line with the human cognition model and makes the model not suitable for
human use. In order to solve the above problems, the present invention provides
a deep neural network training method based on the human knowledge, which uses
the human cognition model to construct the deep neural network training model,
and uses the existing human knowledge information to construct the deep neural
network training model. This paper proposes a multi-level hierarchical deep
learning algorithm, which is composed of multi-level hierarchical deep neural
network architecture and multi-level hierarchical deep learning framework. The
experimental results show that the proposed algorithm can effectively explain
the hidden information of the neural network. The goal of our study is to
improve the interpretability of deep neural networks (DNNs) by providing an
analysis of the impact of knowledge injection on the classification task. We
constructed a knowledge injection dataset with matching knowledge data and
image classification data. The knowledge injection dataset is the benchmark
dataset for the experiments in the paper. Our model expresses the improvement
in interpretability and classification task performance of hidden layers at
different scales.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yishuang Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Ning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04468">
<title>MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation. (arXiv:2401.04468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04468</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing demand for high-fidelity video generation from textual
descriptions has catalyzed significant research in this field. In this work, we
introduce MagicVideo-V2 that integrates the text-to-image model, video motion
generator, reference image embedding module and frame interpolation module into
an end-to-end video generation pipeline. Benefiting from these architecture
designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution
video with remarkable fidelity and smoothness. It demonstrates superior
performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,
Moon Valley and Stable Video Diffusion model via user evaluation at large
scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weimin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhijie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiangqiao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1&quot;&gt;Chetwin Low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Tuyen Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Daquan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04472">
<title>A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04472</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has become an established technique to facilitate
privacy-preserving collaborative training. However, new approaches to FL often
discuss their contributions involving small deep-learning models only. With the
tremendous success of transformer models, the following question arises: What
is necessary to operationalize foundation models in an FL application? Knowing
that computation and communication often take up similar amounts of time in FL,
we introduce a novel taxonomy focused on computational and communication
efficiency methods in FL applications. This said, these methods aim to optimize
the training time and reduce communication between clients and the server. We
also look at the current state of widely used FL frameworks and discuss future
research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woisetschlager_H/0/1/0/all/0/1&quot;&gt;Herbert Woisetschl&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isenko_A/0/1/0/all/0/1&quot;&gt;Alexander Isenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_R/0/1/0/all/0/1&quot;&gt;Ruben Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobsen_H/0/1/0/all/0/1&quot;&gt;Hans-Arno Jacobsen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04474">
<title>Combining Embedding-Based and Semantic-Based Models for Post-hoc Explanations in Recommender Systems. (arXiv:2401.04474v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.04474</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s data-rich environment, recommender systems play a crucial role in
decision support systems. They provide to users personalized recommendations
and explanations about these recommendations. Embedding-based models, despite
their widespread use, often suffer from a lack of interpretability, which can
undermine trust and user engagement. This paper presents an approach that
combines embedding-based and semantic-based models to generate post-hoc
explanations in recommender systems, leveraging ontology-based knowledge graphs
to improve interpretability and explainability. By organizing data within a
structured framework, ontologies enable the modeling of intricate relationships
between entities, which is essential for generating explanations. By combining
embedding-based and semantic based models for post-hoc explanations in
recommender systems, the framework we defined aims at producing meaningful and
easy-to-understand explanations, enhancing user trust and satisfaction, and
potentially promoting the adoption of recommender systems across the e-commerce
sector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngoc Luyen Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abel_M/0/1/0/all/0/1&quot;&gt;Marie-H&amp;#xe9;l&amp;#xe8;ne Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouspillou_P/0/1/0/all/0/1&quot;&gt;Philippe Gouspillou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04478">
<title>TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.04478</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of drug discovery and development relies on the precise
prediction of molecular activities and properties. While in silico molecular
property prediction has shown remarkable potential, its use has been limited so
far to assays for which large amounts of data are available. In this study, we
use a fine-tuned large language model to integrate biological assays based on
their textual information, coupled with Barlow Twins, a Siamese neural network
using a novel self-supervised learning approach. This architecture uses both
assay information and molecular fingerprints to extract the true molecular
information. TwinBooster enables the prediction of properties of unseen
bioassays and molecules by providing state-of-the-art zero-shot learning tasks.
Remarkably, our artificial intelligence pipeline shows excellent performance on
the FS-Mol benchmark. This breakthrough demonstrates the application of deep
learning to critical property prediction tasks where data is typically scarce.
By accelerating the early identification of active molecules in drug discovery
and development, this method has the potential to help streamline the
identification of novel therapeutics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Schuh_M/0/1/0/all/0/1&quot;&gt;Maximilian G. Schuh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boldini_D/0/1/0/all/0/1&quot;&gt;Davide Boldini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sieber_S/0/1/0/all/0/1&quot;&gt;Stephan A. Sieber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04481">
<title>Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04481</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success in language generation capabilities of large language
models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns
about their possible misuse in inducing mass agitation and communal hatred via
generating fake news and spreading misinformation. Traditional means of
developing a misinformation ground-truth dataset does not scale well because of
the extensive manual effort required to annotate the data. In this paper, we
propose an LLM-based approach of creating silver-standard ground-truth datasets
for identifying misinformation. Specifically speaking, given a trusted news
article, our proposed approach involves prompting LLMs to automatically
generate a summarised version of the original article. The prompts in our
proposed approach act as a controlling mechanism to generate specific types of
factual incorrectness in the generated summaries, e.g., incorrect quantities,
false attributions etc. To investigate the usefulness of this dataset, we
conduct a set of experiments where we train a range of supervised models for
the task of misinformation detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satapara_S/0/1/0/all/0/1&quot;&gt;Shrey Satapara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1&quot;&gt;Parth Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguly_D/0/1/0/all/0/1&quot;&gt;Debasis Ganguly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modha_S/0/1/0/all/0/1&quot;&gt;Sandip Modha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04489">
<title>Optimal Survival Trees: A Dynamic Programming Approach. (arXiv:2401.04489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04489</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis studies and predicts the time of death, or other singular
unrepeated events, based on historical data, while the true time of death for
some instances is unknown. Survival trees enable the discovery of complex
nonlinear relations in a compact human comprehensible model, by recursively
splitting the population and predicting a distinct survival distribution in
each leaf node. We use dynamic programming to provide the first survival tree
method with optimality guarantees, enabling the assessment of the optimality
gap of heuristics. We improve the scalability of our method through a special
algorithm for computing trees up to depth two. The experiments show that our
method&apos;s run time even outperforms some heuristics for realistic cases while
obtaining similar out-of-sample performance with the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huisman_T/0/1/0/all/0/1&quot;&gt;Tim Huisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linden_J/0/1/0/all/0/1&quot;&gt;Jacobus G. M. van der Linden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirovic_E/0/1/0/all/0/1&quot;&gt;Emir Demirovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04507">
<title>TechGPT-2.0: A large language model project to solve the task of knowledge graph construction. (arXiv:2401.04507v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04507</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have exhibited robust performance across diverse
natural language processing tasks. This report introduces TechGPT-2.0, a
project designed to enhance the capabilities of large language models
specifically in knowledge graph construction tasks, including named entity
recognition (NER) and relationship triple extraction (RTE) tasks in NLP
applications. Additionally, it serves as a LLM accessible for research within
the Chinese open-source model community. We offer two 7B large language model
weights and a QLoRA weight specialized for processing lengthy texts.Notably,
TechGPT-2.0 is trained on Huawei&apos;s Ascend server. Inheriting all
functionalities from TechGPT-1.0, it exhibits robust text processing
capabilities, particularly in the domains of medicine and law. Furthermore, we
introduce new capabilities to the model, enabling it to process texts in
various domains such as geographical areas, transportation, organizations,
literary works, biology, natural sciences, astronomical objects, and
architecture. These enhancements also fortified the model&apos;s adeptness in
handling hallucinations, unanswerable queries, and lengthy texts. This report
provides a comprehensive and detailed introduction to the full fine-tuning
process on Huawei&apos;s Ascend servers, encompassing experiences in Ascend server
debugging, instruction fine-tuning data processing, and model training. Our
code is available at https://github.com/neukg/TechGPT-2.0
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yuying Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_N/0/1/0/all/0/1&quot;&gt;Ning An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hei_L/0/1/0/all/0/1&quot;&gt;Lei Hei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Haibo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yifei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1&quot;&gt;Feiliang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04515">
<title>Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04515</link>
<description rdf:parseType="Literal">&lt;p&gt;This article investigates a zero-shot approach to hypernymy prediction using
large language models (LLMs). The study employs a method based on text
probability calculation, applying it to various generated prompts. The
experiments demonstrate a strong correlation between the effectiveness of
language model prompts and classic patterns, indicating that preliminary prompt
selection can be carried out using smaller models before moving to larger ones.
We also explore prompts for predicting co-hyponyms and improving hypernymy
predictions by augmenting prompts with additional information through
automatically identified co-hyponyms. An iterative approach is developed for
predicting higher-level concepts, which further improves the quality on the
BLESS dataset (MAP = 0.8).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhomirov_M/0/1/0/all/0/1&quot;&gt;Mikhail Tikhomirov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1&quot;&gt;Natalia Loukachevitch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04518">
<title>The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04518</link>
<description rdf:parseType="Literal">&lt;p&gt;Critique, as a natural language description for assessing the quality of
model-generated content, has been proven to play an essential role in the
training, evaluation, and refinement of Large Language Models (LLMs). However,
there is a lack of principled understanding in evaluating the quality of the
critique itself. In this paper, we pioneer the critique of critique, termed
MetaCritique, which is a framework to evaluate the critique from two aspects,
i.e., factuality as precision score and comprehensiveness as recall score. We
calculate the harmonic mean of precision and recall as the overall rating
called F1 score. To obtain a reliable evaluation outcome, we propose Atomic
Information Units (AIUs), which describe the critique in a more fine-grained
manner. MetaCritique takes each AIU into account and aggregates each AIU&apos;s
judgment for the overall score. Moreover, given the evaluation process involves
intricate reasoning, our MetaCritique provides a natural language rationale to
support each judgment. We construct a meta-evaluation dataset containing 300
critiques (2653 AIUs) across four tasks (question answering, reasoning,
entailment, and summarization), and we conduct a comparative study to
demonstrate the feasibility and effectiveness. Experiments also show superior
critique judged by MetaCritique leads to better refinement, indicating
generative artificial intelligence indeed has the potential to be significantly
advanced with our MetaCritique. We will release relevant code and
meta-evaluation datasets at https://github.com/GAIR-NLP/MetaCritique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1&quot;&gt;Ruifeng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengfei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04531">
<title>MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04531</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, one of the most notable advancements in AI research
has been in foundation models (FMs), headlined by the rise of language models
(LMs). As the models&apos; size increases, LMs demonstrate enhancements in
measurable aspects and the development of new qualitative features. However,
despite researchers&apos; attention and the rapid growth in LM application, the
capabilities, limitations, and associated risks still need to be better
understood. To address these issues, we introduce an open Multimodal Evaluation
of Russian-language Architectures (MERA), a new instruction benchmark for
evaluating foundation models oriented towards the Russian language. The
benchmark encompasses 21 evaluation tasks for generative models in 11 skill
domains and is designed as a black-box test to ensure the exclusion of data
leakage. The paper introduces a methodology to evaluate FMs and LMs in zero-
and few-shot fixed instruction settings that can be extended to other
modalities. We propose an evaluation methodology, an open-source code base for
the MERA assessment, and a leaderboard with a submission system. We evaluate
open LMs as baselines and find that they are still far behind the human level.
We publicly release MERA to guide forthcoming research, anticipate
groundbreaking model features, standardize the evaluation procedure, and
address potential societal drawbacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1&quot;&gt;Alena Fenogenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chervyakov_A/0/1/0/all/0/1&quot;&gt;Artem Chervyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martynov_N/0/1/0/all/0/1&quot;&gt;Nikita Martynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozlova_A/0/1/0/all/0/1&quot;&gt;Anastasia Kozlova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1&quot;&gt;Maria Tikhonova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhmetgareeva_A/0/1/0/all/0/1&quot;&gt;Albina Akhmetgareeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emelyanov_A/0/1/0/all/0/1&quot;&gt;Anton Emelyanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevelev_D/0/1/0/all/0/1&quot;&gt;Denis Shevelev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebedev_P/0/1/0/all/0/1&quot;&gt;Pavel Lebedev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinev_L/0/1/0/all/0/1&quot;&gt;Leonid Sinev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isaeva_U/0/1/0/all/0/1&quot;&gt;Ulyana Isaeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolomeytseva_K/0/1/0/all/0/1&quot;&gt;Katerina Kolomeytseva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskovskiy_D/0/1/0/all/0/1&quot;&gt;Daniil Moskovskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncharova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Goncharova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savushkin_N/0/1/0/all/0/1&quot;&gt;Nikita Savushkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhailova_P/0/1/0/all/0/1&quot;&gt;Polina Mikhailova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1&quot;&gt;Denis Dimitrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1&quot;&gt;Alexander Panchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markov_S/0/1/0/all/0/1&quot;&gt;Sergei Markov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04536">
<title>Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04536</link>
<description rdf:parseType="Literal">&lt;p&gt;Companies, organizations, and governments increasingly exploit Language
Models&apos; (LM) remarkable capability to display agent-like behavior. As LMs are
adopted to perform tasks with growing autonomy, there exists an urgent need for
reliable and scalable evaluation benchmarks. Current, predominantly static LM
benchmarks are ill-suited to evaluate such dynamic applications. Thus, we
propose jointly evaluating LM performance and alignment through the lenses of
negotiation games. We argue that this common task better reflects real-world
deployment conditions while offering insights into LMs&apos; decision-making
processes. Crucially, negotiation games allow us to study multi-turn, and
cross-model interactions, modulate complexity, and side-step accidental data
leakage in evaluation. We report results for six publicly accessible LMs from
several major providers on a variety of negotiation games, evaluating both
self-play and cross-play performance. Noteworthy findings include: (i)
open-source models are currently unable to complete these tasks; (ii)
cooperative bargaining games prove challenging; and (iii) the most powerful
models do not always &quot;win&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_T/0/1/0/all/0/1&quot;&gt;Tim R. Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veselovsky_V/0/1/0/all/0/1&quot;&gt;Veniamin Veselovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1&quot;&gt;Martin Josifoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1&quot;&gt;Maxime Peyrard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1&quot;&gt;Antoine Bosselut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1&quot;&gt;Michal Kosinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1&quot;&gt;Robert West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04575">
<title>Let&apos;s Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding. (arXiv:2401.04575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04575</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision and vision-language applications of neural networks, such as image
classification and captioning, rely on large-scale annotated datasets that
require non-trivial data-collecting processes. This time-consuming endeavor
hinders the emergence of large-scale datasets, limiting researchers and
practitioners to a small number of choices. Therefore, we seek more efficient
ways to collect and annotate images. Previous initiatives have gathered
captions from HTML alt-texts and crawled social media postings, but these data
sources suffer from noise, sparsity, or subjectivity. For this reason, we turn
to commercial shopping websites whose data meet three criteria: cleanliness,
informativeness, and fluency. We introduce the Let&apos;s Go Shopping (LGS) dataset,
a large-scale public dataset with 15 million image-caption pairs from publicly
available e-commerce websites. When compared with existing general-domain
datasets, the LGS images focus on the foreground object and have less complex
backgrounds. Our experiments on LGS show that the classifiers trained on
existing benchmark datasets do not readily generalize to e-commerce data, while
specific self-supervised visual feature extractors can better generalize.
Furthermore, LGS&apos;s high-quality e-commerce-focused images and bimodal nature
make it advantageous for vision-language bi-modal tasks: LGS enables
image-captioning models to generate richer captions and helps text-to-image
generation models achieve e-commerce style transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yatong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1&quot;&gt;Utsav Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanker_A/0/1/0/all/0/1&quot;&gt;Apaar Shanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1&quot;&gt;Samyak Parajuli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1&quot;&gt;Erhan Bas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filipovic_I/0/1/0/all/0/1&quot;&gt;Isidora Filipovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_A/0/1/0/all/0/1&quot;&gt;Amelia N. Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fomitcheva_E/0/1/0/all/0/1&quot;&gt;Eugenia D Fomitcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1&quot;&gt;Elliot Branson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Aerin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04577">
<title>Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.04577</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MAGNeT, a masked generative sequence modeling method that
operates directly over several streams of audio tokens. Unlike prior work,
MAGNeT is comprised of a single-stage, non-autoregressive transformer. During
training, we predict spans of masked tokens obtained from a masking scheduler,
while during inference we gradually construct the output sequence using several
decoding steps. To further enhance the quality of the generated audio, we
introduce a novel rescoring method in which, we leverage an external
pre-trained model to rescore and rank predictions from MAGNeT, which will be
then used for later decoding steps. Lastly, we explore a hybrid version of
MAGNeT, in which we fuse between autoregressive and non-autoregressive models
to generate the first few seconds in an autoregressive manner while the rest of
the sequence is being decoded in parallel. We demonstrate the efficiency of
MAGNeT for the task of text-to-music and text-to-audio generation and conduct
an extensive empirical evaluation, considering both objective metrics and human
studies. The proposed approach is comparable to the evaluated baselines, while
being significantly faster (x7 faster than the autoregressive baseline).
Through ablation studies and analysis, we shed light on the importance of each
of the components comprising MAGNeT, together with pointing to the trade-offs
between autoregressive and non-autoregressive modeling, considering latency,
throughput, and generation quality. Samples are available on our demo page
https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziv_A/0/1/0/all/0/1&quot;&gt;Alon Ziv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1&quot;&gt;Itai Gat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1&quot;&gt;Gael Le Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1&quot;&gt;Tal Remez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreuk_F/0/1/0/all/0/1&quot;&gt;Felix Kreuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defossez_A/0/1/0/all/0/1&quot;&gt;Alexandre D&amp;#xe9;fossez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1&quot;&gt;Jade Copet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1&quot;&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1&quot;&gt;Yossi Adi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04579">
<title>A Deep Network for Explainable Prediction of Non-Imaging Phenotypes using Anatomical Multi-View Data. (arXiv:2401.04579v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.04579</link>
<description rdf:parseType="Literal">&lt;p&gt;Large datasets often contain multiple distinct feature sets, or views, that
offer complementary information that can be exploited by multi-view learning
methods to improve results. We investigate anatomical multi-view data, where
each brain anatomical structure is described with multiple feature sets. In
particular, we focus on sets of white matter microstructure and connectivity
features from diffusion MRI, as well as sets of gray matter area and thickness
features from structural MRI. We investigate machine learning methodology that
applies multi-view approaches to improve the prediction of non-imaging
phenotypes, including demographics (age), motor (strength), and cognition
(picture vocabulary). We present an explainable multi-view network (EMV-Net)
that can use different anatomical views to improve prediction performance. In
this network, each individual anatomical view is processed by a view-specific
feature extractor and the extracted information from each view is fused using a
learnable weight. This is followed by a wavelet transform-based module to
obtain complementary information across views which is then applied to
calibrate the view-specific information. Additionally, the calibrator produces
an attention-based calibration score to indicate anatomical structures&apos;
importance for interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tengfei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zekelman_L/0/1/0/all/0/1&quot;&gt;Leo Zekelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Makris_N/0/1/0/all/0/1&quot;&gt;Nikos Makris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rathi_Y/0/1/0/all/0/1&quot;&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Donnell_L/0/1/0/all/0/1&quot;&gt;Lauren J. O&amp;#x27; Donnell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04620">
<title>Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04620</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents based on Large Language Models (LLMs) are increasingly permeating
various domains of human production and life, highlighting the importance of
aligning them with human values. The current alignment of AI systems primarily
focuses on passively aligning LLMs through human intervention. However, agents
possess characteristics like receiving environmental feedback and
self-evolution, rendering the LLM alignment methods inadequate. In response, we
propose an evolutionary framework for agent evolution and alignment, named
EvolutionaryAgent, which transforms agent alignment into a process of evolution
and selection under the principle of survival of the fittest. In an environment
where social norms continuously evolve, agents better adapted to the current
social norms will have a higher probability of survival and proliferation,
while those inadequately aligned dwindle over time. Experimental results
assessing the agents from multiple perspectives in aligning with social norms
demonstrate that EvolutionaryAgent possesses the capability to align
progressively better with the evolving social norms while maintaining its
proficiency in general tasks. Effectiveness tests conducted on various open and
closed-source LLMs as the foundation for agents also prove the applicability of
our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shimin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04621">
<title>DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.04621</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated exceptional coding capability.
However, as another critical component of programming proficiency, the
debugging capability of LLMs remains relatively unexplored. Previous
evaluations of LLMs&apos; debugging ability are significantly limited by the risk of
data leakage, the scale of the dataset, and the variety of tested bugs. To
overcome these deficiencies, we introduce `DebugBench&apos;, an LLM debugging
benchmark consisting of 4,253 instances. It covers four major bug categories
and 18 minor types in C++, Java, and Python. To construct DebugBench, we
collect code snippets from the LeetCode community, implant bugs into source
data with GPT-4, and assure rigorous quality checks. We evaluate two commercial
and three open-source models in a zero-shot scenario. We find that (1) while
closed-source models like GPT-4 exhibit inferior debugging performance compared
to humans, open-source models such as Code Llama fail to attain any pass rate
scores; (2) the complexity of debugging notably fluctuates depending on the bug
category; (3) incorporating runtime feedback has a clear impact on debugging
performance which is not always helpful. As an extension, we also compare LLM
debugging and code generation, revealing a strong correlation between them for
closed-source models. These findings will benefit the development of LLMs in
debugging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Runchu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yining Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04631">
<title>Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring. (arXiv:2401.04631v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.04631</link>
<description rdf:parseType="Literal">&lt;p&gt;The conservation of hydrological resources involves continuously monitoring
their contamination. A multi-agent system composed of autonomous surface
vehicles is proposed in this paper to efficiently monitor the water quality. To
achieve a safe control of the fleet, the fleet policy should be able to act
based on measurements and to the the fleet state. It is proposed to use Local
Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective
monitoring policies. Local Gaussian processes, unlike classical global Gaussian
processes, can accurately model the information in a dissimilar spatial
correlation which captures more accurately the water quality information. A
Deep convolutional policy is proposed, that bases the decisions on the
observation on the mean and variance of this model, by means of an information
gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to
minimize the estimation error in a safe manner thanks to a Consensus-based
heuristic. Simulation results indicate an improvement of up to 24% in terms of
the mean absolute error with the proposed models. Also, training results with
1-3 agents indicate that our proposed approach returns 20% and 24% smaller
average estimation errors for, respectively, monitoring water quality variables
and monitoring algae blooms, as compared to state-of-the-art approaches
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luis_S/0/1/0/all/0/1&quot;&gt;Samuel Yanes Luis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shutin_D/0/1/0/all/0/1&quot;&gt;Dmitriy Shutin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_J/0/1/0/all/0/1&quot;&gt;Juan Marchal G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reina_D/0/1/0/all/0/1&quot;&gt;Daniel Guti&amp;#xe9;rrez Reina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marin_S/0/1/0/all/0/1&quot;&gt;Sergio Toral Mar&amp;#xed;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04637">
<title>Applying Large Language Models API to Issue Classification Problem. (arXiv:2401.04637v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.04637</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective prioritization of issue reports is crucial in software engineering
to optimize resource allocation and address critical problems promptly.
However, the manual classification of issue reports for prioritization is
laborious and lacks scalability. Alternatively, many open source software (OSS)
projects employ automated processes for this task, albeit relying on
substantial datasets for adequate training. This research seeks to devise an
automated approach that ensures reliability in issue prioritization, even when
trained on smaller datasets. Our proposed methodology harnesses the power of
Generative Pre-trained Transformers (GPT), recognizing their potential to
efficiently handle this task. By leveraging the capabilities of such models, we
aim to develop a robust system for prioritizing issue reports accurately,
mitigating the necessity for extensive training data while maintaining
reliability. In our research, we have developed a reliable GPT-based approach
to accurately label and prioritize issue reports with a reduced training
dataset. By reducing reliance on massive data requirements and focusing on
few-shot fine-tuning, our methodology offers a more accessible and efficient
solution for issue prioritization in software engineering. Our model predicted
issue types in individual projects up to 93.2% in precision, 95% in recall, and
89.3% in F1-score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aracena_G/0/1/0/all/0/1&quot;&gt;Gabriel Aracena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luster_K/0/1/0/all/0/1&quot;&gt;Kyle Luster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1&quot;&gt;Fabio Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinmacher_I/0/1/0/all/0/1&quot;&gt;Igor Steinmacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerosa_M/0/1/0/all/0/1&quot;&gt;Marco A. Gerosa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04647">
<title>Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04647</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel concept learning framework for enhancing model
interpretability and performance in visual classification tasks. Our approach
appends an unsupervised explanation generator to the primary classifier network
and makes use of adversarial training. During training, the explanation module
is optimized to extract visual concepts from the classifier&apos;s latent
representations, while the GAN-based module aims to discriminate images
generated from concepts, from true images. This joint training scheme enables
the model to implicitly align its internally learned concepts with
human-interpretable visual properties. Comprehensive experiments demonstrate
the robustness of our approach, while producing coherent concept activations.
We analyse the learned concepts, showing their semantic concordance with object
parts and visual attributes. We also study how perturbations in the adversarial
training protocol impact both classification and concept acquisition. In
summary, this work presents a significant step towards building inherently
interpretable deep vision models with task-aligned concept representations - a
key enabler for developing trustworthy AI for real-world perception tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_T/0/1/0/all/0/1&quot;&gt;Tanmay Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemuri_D/0/1/0/all/0/1&quot;&gt;Deepika Vemuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04648">
<title>A novel framework for generalization of deep hidden physics models. (arXiv:2401.04648v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04648</link>
<description rdf:parseType="Literal">&lt;p&gt;Modelling of systems where the full system information is unknown is an oft
encountered problem for various engineering and industrial applications, as
it&apos;s either impossible to consider all the complex physics involved or simpler
models are considered to keep within the limits of the available resources.
Recent advances in greybox modelling like the deep hidden physics models
address this space by combining data and physics. However, for most real-life
applications, model generalizability is a key issue, as retraining a model for
every small change in system inputs and parameters or modification in domain
configuration can render the model economically unviable. In this work we
present a novel enhancement to the idea of hidden physics models which can
generalize for changes in system inputs, parameters and domains. We also show
that this approach holds promise in system discovery as well and helps learn
the hidden physics for the changed system inputs, parameters and domain
configuration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kag_V/0/1/0/all/0/1&quot;&gt;Vijay Kag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_B/0/1/0/all/0/1&quot;&gt;Birupaksha Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04658">
<title>Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04658</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear attention is an efficient attention mechanism that has recently
emerged as a promising alternative to conventional softmax attention. With its
ability to process tokens in linear computational complexities, linear
attention, in theory, can handle sequences of unlimited length without
sacrificing speed, i.e., maintaining a constant training speed for various
sequence lengths with a fixed memory consumption. However, due to the issue
with cumulative summation (cumsum), current linear attention algorithms cannot
demonstrate their theoretical advantage in a causal setting. In this paper, we
present Lightning Attention-2, the first linear attention implementation that
enables linear attention to realize its theoretical computational benefits. To
achieve this, we leverage the thought of tiling, separately handling the
intra-block and inter-block components in linear attention calculation.
Specifically, we utilize the conventional attention computation mechanism for
the intra-blocks and apply linear attention kernel tricks for the inter-blocks.
A tiling technique is adopted through both forward and backward procedures to
take full advantage of the GPU hardware. We implement our algorithm in Triton
to make it IO-aware and hardware-friendly. Various experiments are conducted on
different model sizes and sequence lengths. Lightning Attention-2 retains
consistent training and inference speed regardless of input sequence length and
is significantly faster than other attention mechanisms. The source code is
available at https://github.com/OpenNLPLab/lightning-attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weigao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xuyang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04666">
<title>Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset. (arXiv:2401.04666v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04666</link>
<description rdf:parseType="Literal">&lt;p&gt;As the most basic application and implementation of deep learning, image
classification has grown in popularity. Various datasets are provided by
renowned data science communities for benchmarking machine learning algorithms
and pre-trained models. The ASSIRA Cats &amp;amp; Dogs dataset is one of them and is
being used in this research for its overall acceptance and benchmark standards.
A comparison of various pre-trained models is demonstrated by using different
types of optimizers and loss functions. Hyper-parameters are changed to gain
the best result from a model. By applying this approach, we have got higher
accuracy without major changes in the training model. To run the experiment, we
used three different computer architectures: a laptop equipped with NVIDIA
GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a
desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate
supremacy in terms of accuracy over the previously done experiments on this
dataset. From this experiment, the highest accuracy which is 99.65% is gained
using the NASNet Large.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himel_G/0/1/0/all/0/1&quot;&gt;Galib Muhammad Shahriar Himel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Masudul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04679">
<title>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04679</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
https://github.com/IST-DASLab/RoSA}{\texttt{https://github.com/IST-DASLab/RoSA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1&quot;&gt;Mahdi Nikdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1&quot;&gt;Soroush Tabesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04728">
<title>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04728</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative diffusion models have enabled the previously
unfeasible capability of generating 3D assets from a single input image or a
text prompt. In this work, we aim to enhance the quality and functionality of
these models for the task of creating controllable, photorealistic human
avatars. We achieve this by integrating a 3D morphable model into the
state-of-the-art multiview-consistent diffusion approach. We demonstrate that
accurate conditioning of a generative pipeline on the articulated 3D model
enhances the baseline model performance on the task of novel view synthesis
from a single image. More importantly, this integration facilitates a seamless
and accurate incorporation of facial expression and body pose control into the
generation process. To the best of our knowledge, our proposed framework is the
first diffusion model to enable the creation of fully 3D-consistent,
animatable, and photorealistic human avatars from a single image of an unseen
subject; extensive quantitative and qualitative evaluations demonstrate the
advantages of our approach over existing state-of-the-art avatar creation
models on both novel view and novel expression synthesis tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1&quot;&gt;Marko Mihajlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prokudin_S/0/1/0/all/0/1&quot;&gt;Sergey Prokudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05206">
<title>PHPQ: Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05206</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to their high
computation and storage efficiency. Most existing hashing methods cannot
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes. Since deeper
layers tend to summarize visual clues, e.g., texture, into abstract semantics,
e.g., dogs and cats, the feature produced by the last CNN layer is less
effective in capturing subtle but discriminative visual details that mostly
exist in shallow layers. To improve fine-grained image hashing, we propose
Pyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid
Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic
information from multi-level features, which emphasizes the subtle
discrimination of different sub-categories. Besides, we propose a learnable
quantization module with a partial codebook attention mechanism, which helps to
optimize the most relevant codewords and improves the quantization.
Comprehensive experiments on two widely-used public benchmarks, i.e.,
CUB-200-2011 and Stanford Dogs, demonstrate that PHPQ outperforms
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.09212">
<title>Molecule Generation for Drug Design: a Graph Learning Perspective. (arXiv:2202.09212v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.09212</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning, particularly graph learning, is gaining increasing
recognition for its transformative impact across various fields. One such
promising application is in the realm of molecule design and discovery, notably
within the pharmaceutical industry. Our survey offers a comprehensive overview
of state-of-the-art methods in molecule design, particularly focusing on
\emph{de novo} drug design, which incorporates (deep) graph learning
techniques. We categorize these methods into three distinct groups: \emph{i)}
\emph{all-at-once}, \emph{ii)} \emph{fragment-based}, and \emph{iii)}
\emph{node-by-node}. Additionally, we introduce some key public datasets and
outline the commonly used evaluation metrics for both the generation and
optimization of molecules. In the end, we discuss the existing challenges in
this field and suggest potential directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Nianzu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huaijin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13131">
<title>On the Evolution of A.I. and Machine Learning: Towards a Meta-level Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences. (arXiv:2205.13131v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13131</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence is now recognized as a general-purpose technology
with ample impact on human life. This work aims at understanding the evolution
of AI and, in particular Machine learning, from the perspective of researchers&apos;
contributions to the field. In order to do so, we present several measures
allowing the analyses of AI and machine learning researchers&apos; impact,
influence, and leadership over the last decades. This work also contributes, to
a certain extent, to shed new light on the history and evolution of AI by
exploring the dynamics involved in the field&apos;s evolution by looking at papers
published at the flagship AI and machine learning conferences since the first
International Joint Conference on Artificial Intelligence (IJCAI) held in 1969.
AI development and evolution have led to increasing research output, reflected
in the number of articles published over the last sixty years. We construct
comprehensive citation collaboration and paper-author datasets and compute
corresponding centrality measures to carry out our analyses. These analyses
allow a better understanding of how AI has reached its current state of affairs
in research. Throughout the process, we correlate these datasets with the work
of the ACM Turing Award winners and the so-called two AI winters the field has
gone through. We also look at self-citation trends and new authors&apos; behaviors.
Finally, we present a novel way to infer the country of affiliation of a paper
from its organization. Therefore, this work provides a deep analysis of
Artificial Intelligence history from information gathered and analysed from
large technical venues datasets and suggests novel insights that can contribute
to understanding and measuring AI&apos;s evolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Audibert_R/0/1/0/all/0/1&quot;&gt;Rafael B. Audibert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemos_H/0/1/0/all/0/1&quot;&gt;Henrique Lemos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avelar_P/0/1/0/all/0/1&quot;&gt;Pedro Avelar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavares_A/0/1/0/all/0/1&quot;&gt;Anderson R. Tavares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s C. Lamb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08891">
<title>Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis. (arXiv:2209.08891v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08891</link>
<description rdf:parseType="Literal">&lt;p&gt;Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion,
have recently drawn a lot of interest from academia and the general public.
These models are capable of producing high-quality images that depict a variety
of concepts and styles when conditioned on textual descriptions. However, these
models adopt cultural characteristics associated with specific Unicode scripts
from their vast amount of training data, which may not be immediately apparent.
We show that by simply inserting single non-Latin characters in a textual
description, common models reflect cultural stereotypes and biases in their
generated images. We analyze this behavior both qualitatively and
quantitatively, and identify a model&apos;s text encoder as the root cause of the
phenomenon. Additionally, malicious users or service providers may try to
intentionally bias the image generation to create racist stereotypes by
replacing Latin characters with similarly-looking characters from non-Latin
scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we
propose a novel homoglyph unlearning method to fine-tune a text encoder, making
it robust against homoglyph manipulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07675">
<title>Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07675</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zingman_I/0/1/0/all/0/1&quot;&gt;Igor Zingman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stierstorfer_B/0/1/0/all/0/1&quot;&gt;Birgit Stierstorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lempp_C/0/1/0/all/0/1&quot;&gt;Charlotte Lempp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinemann_F/0/1/0/all/0/1&quot;&gt;Fabian Heinemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16906">
<title>DyG2Vec: Efficient Representation Learning for Dynamic Graphs. (arXiv:2210.16906v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16906</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal graph neural networks have shown promising results in learning
inductive representations by automatically extracting temporal patterns.
However, previous works often rely on complex memory modules or inefficient
random walk methods to construct temporal representations. To address these
limitations, we present an efficient yet effective attention-based encoder that
leverages temporal edge encodings and window-based subgraph sampling to
generate task-agnostic embeddings. Moreover, we propose a joint-embedding
architecture using non-contrastive SSL to learn rich temporal embeddings
without labels. Experimental results on 7 benchmark datasets indicate that on
average, our model outperforms SoTA baselines on the future link prediction
task by 4.23% for the transductive setting and 3.30% for the inductive setting
while only requiring 5-10x less training/inference time. Lastly, different
aspects of the proposed framework are investigated through experimental
analysis and ablation studies. The code is publicly available at
https://github.com/huawei-noah/noah-research/tree/master/graph_atlas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alomrani_M/0/1/0/all/0/1&quot;&gt;Mohammad Ali Alomrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biparva_M/0/1/0/all/0/1&quot;&gt;Mahdi Biparva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingxue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coates_M/0/1/0/all/0/1&quot;&gt;Mark Coates&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04458">
<title>General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04458</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern machine learning requires system designers to specify aspects of the
learning pipeline, such as losses, architectures, and optimizers.
Meta-learning, or learning-to-learn, instead aims to learn those aspects, and
promises to unlock greater capabilities with less manual effort. One
particularly ambitious goal of meta-learning is to train general-purpose
in-context learning algorithms from scratch, using only black-box models with
minimal inductive bias. Such a model takes in training data, and produces
test-set predictions across a wide range of problems, without any explicit
definition of an inference model, training loss, or optimization algorithm. In
this paper we show that Transformers and other black-box models can be
meta-trained to act as general-purpose in-context learners. We characterize
transitions between algorithms that generalize, algorithms that memorize, and
algorithms that fail to meta-train at all, induced by changes in model size,
number of tasks, and meta-optimization. We further show that the capabilities
of meta-trained algorithms are bottlenecked by the accessible state size
(memory) determining the next prediction, unlike standard models which are
thought to be bottlenecked by parameter count. Finally, we propose practical
interventions such as biasing the training distribution that improve the
meta-training and meta-generalization of general-purpose in-context learning
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirsch_L/0/1/0/all/0/1&quot;&gt;Louis Kirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1&quot;&gt;James Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1&quot;&gt;Luke Metz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00950">
<title>Class-Continuous Conditional Generative Neural Radiance Field. (arXiv:2301.00950v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00950</link>
<description rdf:parseType="Literal">&lt;p&gt;The 3D-aware image synthesis focuses on conserving spatial consistency
besides generating high-resolution images with fine details. Recently, Neural
Radiance Field (NeRF) has been introduced for synthesizing novel views with low
computational cost and superior performance. While several works investigate a
generative NeRF and show remarkable achievement, they cannot handle conditional
and continuous feature manipulation in the generation procedure. In this work,
we introduce a novel model, called Class-Continuous Conditional Generative NeRF
($\text{C}^{3}$G-NeRF), which can synthesize conditionally manipulated
photorealistic 3D-consistent images by projecting conditional features to the
generator and the discriminator. The proposed $\text{C}^{3}$G-NeRF is evaluated
with three image datasets, AFHQ, CelebA, and Cars. As a result, our model shows
strong 3D-consistency with fine details and smooth interpolation in conditional
feature manipulation. For instance, $\text{C}^{3}$G-NeRF exhibits a Fr\&apos;echet
Inception Distance (FID) of 7.64 in 3D-aware face image synthesis with a
$\text{128}^{2}$ resolution. Additionally, we provide FIDs of generated
3D-aware images of each class of the datasets as it is possible to synthesize
class-conditional images with $\text{C}^{3}$G-NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyeok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06188">
<title>Enhancing SMT-based Weighted Model Integration by Structure Awareness. (arXiv:2302.06188v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06188</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of efficient exact and approximate algorithms for
probabilistic inference is a long-standing goal of artificial intelligence
research. Whereas substantial progress has been made in dealing with purely
discrete or purely continuous domains, adapting the developed solutions to
tackle hybrid domains, characterised by discrete and continuous variables and
their relationships, is highly non-trivial. Weighted Model Integration (WMI)
recently emerged as a unifying formalism for probabilistic inference in hybrid
domains. Despite a considerable amount of recent work, allowing WMI algorithms
to scale with the complexity of the hybrid problem is still a challenge. In
this paper we highlight some substantial limitations of existing
state-of-the-art solutions, and develop an algorithm that combines SMT-based
enumeration, an efficient technique in formal verification, with an effective
encoding of the problem structure. This allows our algorithm to avoid
generating redundant models, resulting in drastic computational savings.
Additionally, we show how SMT-based approaches can seamlessly deal with
different integration techniques, both exact and approximate, significantly
expanding the set of problems that can be tackled by WMI technology. An
extensive experimental evaluation on both synthetic and real-world datasets
confirms the substantial advantage of the proposed solution over existing
alternatives. The application potential of this technology is further showcased
on a prototypical task aimed at verifying the fairness of probabilistic
programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spallitta_G/0/1/0/all/0/1&quot;&gt;Giuseppe Spallitta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masina_G/0/1/0/all/0/1&quot;&gt;Gabriele Masina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morettin_P/0/1/0/all/0/1&quot;&gt;Paolo Morettin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebastiani_R/0/1/0/all/0/1&quot;&gt;Roberto Sebastiani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09580">
<title>Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09580</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian processes provide a flexible, non-parametric framework for the
approximation of functions in high-dimensional spaces. The covariance kernel is
the main engine of Gaussian processes, incorporating correlations that underpin
the predictive distribution. For applications with spatiotemporal datasets,
suitable kernels should model joint spatial and temporal dependence. Separable
space-time covariance kernels offer simplicity and computational efficiency.
However, non-separable kernels include space-time interactions that better
capture observed correlations. Most non-separable kernels that admit explicit
expressions are based on mathematical considerations (admissibility conditions)
rather than first-principles derivations. We present a hybrid spectral approach
for generating covariance kernels which is based on physical arguments. We use
this approach to derive a new class of physically motivated, non-separable
covariance kernels which have their roots in the stochastic, linear, damped,
harmonic oscillator (LDHO). The new kernels incorporate functions with both
monotonic and oscillatory decay of space-time correlations. The LDHO covariance
kernels involve space-time interactions which are introduced by dispersion
relations that modulate the oscillator coefficients. We derive explicit
relations for the spatiotemporal covariance kernels in the three oscillator
regimes (underdamping, critical damping, overdamping) and investigate their
properties. We further illustrate the hybrid spectral method by deriving
covariance kernels that are based on the Ornstein-Uhlenbeck model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hristopulos_D/0/1/0/all/0/1&quot;&gt;Dionissios T.Hristopulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10639">
<title>Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning. (arXiv:2302.10639v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10639</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety in goal directed Reinforcement Learning (RL) settings has typically
been handled through constraints over trajectories and have demonstrated good
performance in primarily short horizon tasks. In this paper, we are
specifically interested in the problem of solving temporally extended decision
making problems such as robots cleaning different areas in a house while
avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge
to move to a charging dock; in the presence of complex safety constraints. Our
key contribution is a (safety) Constrained Search with Hierarchical
Reinforcement Learning (CoSHRL) mechanism that combines an upper level
constrained search agent (which computes a reward maximizing policy from a
given start to a far away goal state while satisfying cost constraints) with a
low-level goal conditioned RL agent (which estimates cost and reward values to
move between nearby states). A major advantage of CoSHRL is that it can handle
constraints on the cost value distribution (e.g., on Conditional Value at Risk,
CVaR) and can adjust to flexible constraint thresholds without retraining. We
perform extensive experiments with different types of safety constraints to
demonstrate the utility of our approach over leading approaches in constrained
and hierarchical RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Arunesh Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1&quot;&gt;Pradeep Varakantham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10819">
<title>Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10819</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world data often exhibits bias, imbalance, and privacy risks. Synthetic
datasets have emerged to address these issues. This paradigm relies on
generative AI models to generate unbiased, privacy-preserving data while
maintaining fidelity to the original data. However, assessing the
trustworthiness of synthetic datasets and models is a critical challenge. We
introduce a holistic auditing framework that comprehensively evaluates
synthetic datasets and AI models. It focuses on preventing bias and
discrimination, ensures fidelity to the source data, assesses utility,
robustness, and privacy preservation. We demonstrate the framework&apos;s
effectiveness by auditing various generative models across diverse use cases
like education, healthcare, banking, and human resources, spanning different
data modalities such as tabular, time-series, vision, and natural language.
This holistic assessment is essential for compliance with regulatory
safeguards. We introduce a trustworthiness index to rank synthetic datasets
based on their safeguards trade-offs. Furthermore, we present a
trustworthiness-driven model selection and cross-validation process during
training, exemplified with &quot;TrustFormers&quot; across various data types. This
approach allows for controllable trustworthiness trade-offs in synthetic data
creation. Our auditing framework fosters collaboration among stakeholders,
including data scientists, governance experts, internal reviewers, external
certifiers, and regulators. This transparent reporting should become a standard
practice to prevent bias, discrimination, and privacy violations, ensuring
compliance with policies and providing accountability, safety, and performance
guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1&quot;&gt;Brian Belgodere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1&quot;&gt;Pierre Dognin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivankay_A/0/1/0/all/0/1&quot;&gt;Adam Ivankay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1&quot;&gt;Igor Melnyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1&quot;&gt;Youssef Mroueh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mojsilovic_A/0/1/0/all/0/1&quot;&gt;Aleksandra Mojsilovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1&quot;&gt;Jiri Navratil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitsure_A/0/1/0/all/0/1&quot;&gt;Apoorva Nitsure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1&quot;&gt;Inkit Padhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1&quot;&gt;Mattia Rigotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1&quot;&gt;Jerret Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1&quot;&gt;Yair Schiff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedpathak_R/0/1/0/all/0/1&quot;&gt;Radhika Vedpathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1&quot;&gt;Richard A. Young&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12858">
<title>Unpaired Image Translation to Mitigate Domain Shift in Liquid Argon Time Projection Chamber Detector Responses. (arXiv:2304.12858v3 [hep-ex] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12858</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms often are trained and deployed on different
datasets. Any systematic difference between the training and a test dataset may
degrade the algorithm performance--what is known as the domain shift problem.
This issue is prevalent in many scientific domains where algorithms are trained
on simulated data but applied to real-world datasets. Typically, the domain
shift problem is solved through various domain adaptation methods. However,
these methods are often tailored for a specific downstream task and may not
easily generalize to different tasks. This work explores the feasibility of
using an alternative way to solve the domain shift problem that is not specific
to any downstream algorithm. The proposed approach relies on modern Unpaired
Image-to-Image translation techniques, designed to find translations between
different image domains in a fully unsupervised fashion. In this study, the
approach is applied to a domain shift problem commonly encountered in Liquid
Argon Time Projection Chamber (LArTPC) detector research when seeking a way to
translate samples between two differently distributed detector datasets
deterministically. This translation allows for mapping real-world data into the
simulated data domain where the downstream algorithms can be run with much less
domain-shift-related degradation. Conversely, using the translation from the
simulated data in a real-world domain can increase the realism of the simulated
dataset and reduce the magnitude of any systematic uncertainties. We adapted
several UI2I translation algorithms to work on scientific data and demonstrated
the viability of these techniques for solving the domain shift problem with
LArTPC detector data. To facilitate further development of domain adaptation
techniques for scientific datasets, the &quot;Simple Liquid-Argon Track Samples&quot;
dataset used in this study also is published.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Torbunov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Torbunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Viren_B/0/1/0/all/0/1&quot;&gt;Brett Viren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haiwang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meifeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yihui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable model for classification that provides a novel
probabilistic interpretation of neural network softmax classifiers. We derive a
variational objective to train the model, analogous to the evidence lower bound
(ELBO) used to train variational auto-encoders, that generalises the softmax
cross-entropy loss. Treating inputs to the softmax layer as samples of a latent
variable, our abstracted perspective reveals a potential inconsistency between
their anticipated distribution, required for accurate label predictions, and
their empirical distribution found in practice. We augment the variational
objective to mitigate such inconsistency and induce a chosen latent
distribution, instead of the implicit assumption found in a standard softmax
layer. Overall, we provide new theoretical insight into the inner workings of
widely-used softmax classifiers. Empirical evaluation on image and text
classification datasets demonstrates that our proposed approach, variational
classification, maintains classification accuracy while the reshaped latent
space improves other desirable properties of a classifier, such as calibration,
adversarial robustness, robustness to distribution shift and sample efficiency
useful in low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02053">
<title>The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02053</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have seen widespread deployment in various
real-world applications. Understanding these biases is crucial to comprehend
the potential downstream consequences when using LLMs to make decisions,
particularly for historically disadvantaged groups. In this work, we propose a
simple method for analyzing and comparing demographic bias in LLMs, through the
lens of job recommendations. We demonstrate the effectiveness of our method by
measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge
LLMs. Our experiments primarily focus on uncovering gender identity and
nationality bias; however, our method can be extended to examine biases
associated with any intersection of demographic identities. We identify
distinct biases in both models toward various demographic identities, such as
both models consistently suggesting low-paying jobs for Mexican workers or
preferring to recommend secretarial roles to women. Our study highlights the
importance of measuring the bias of LLMs in downstream applications to
understand the potential for harm and inequitable outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salinas_A/0/1/0/all/0/1&quot;&gt;Abel Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1&quot;&gt;Parth Vipul Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuzhong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCormack_R/0/1/0/all/0/1&quot;&gt;Robert McCormack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1&quot;&gt;Fred Morstatter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10393">
<title>Decomposition-based Hierarchical Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2308.10393v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10393</link>
<description rdf:parseType="Literal">&lt;p&gt;Past research into robotic planning with temporal logic specifications,
notably Linear Temporal Logic (LTL), was largely based on singular formulas for
individual or groups of robots. But with increasing task complexity, LTL
formulas unavoidably grow lengthy, complicating interpretation and
specification generation, and straining the computational capacities of the
planners. A recent development has been the hierarchical representation of LTL
[1] that contains multiple temporal logic specifications, providing a more
interpretable framework. However, the proposed planning algorithm assumes the
independence of robots within each specification, limiting their application to
multi-robot coordination with complex temporal constraints. In this work, we
formulated a decomposition-based hierarchical framework. At the high level,
each specification is first decomposed into a set of atomic sub-tasks. We
further infer the temporal relations among the sub-tasks of different
specifications to construct a task network. Subsequently, a Mixed Integer
Linear Program is utilized to assign sub-tasks to various robots. At the lower
level, domain-specific controllers are employed to execute sub-tasks. Our
approach was experimentally applied to domains of robotic navigation and
manipulation. The outcomes of thorough simulations, which included comparative
analyses, demonstrated the effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xusheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaojun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruixuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15053">
<title>Adapting Text-based Dialogue State Tracker for Spoken Dialogues. (arXiv:2308.15053v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15053</link>
<description rdf:parseType="Literal">&lt;p&gt;Although there have been remarkable advances in dialogue systems through the
dialogue systems technology competition (DSTC), it remains one of the key
challenges to building a robust task-oriented dialogue system with a speech
interface. Most of the progress has been made for text-based dialogue systems
since there are abundant datasets with written corpora while those with spoken
dialogues are very scarce. However, as can be seen from voice assistant systems
such as Siri and Alexa, it is of practical importance to transfer the success
to spoken dialogues. In this paper, we describe our engineering effort in
building a highly successful model that participated in the speech-aware
dialogue systems technology challenge track in DSTC11. Our model consists of
three major modules: (1) automatic speech recognition error correction to
bridge the gap between the spoken and the text utterances, (2) text-based
dialogue system (D3ST) for estimating the slots and values using slot
descriptions, and (3) post-processing for recovering the error of the estimated
slot value. Our experiments show that it is important to use an explicit
automatic speech recognition error correction module, post-processing, and data
augmentation to adapt a text-based dialogue state tracker for spoken dialogue
corpora.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaeseok Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Seunghyun Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1&quot;&gt;Ran Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1&quot;&gt;Jeonguk Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kee-Eung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15197">
<title>Where Would I Go Next? Large Language Models as Human Mobility Predictors. (arXiv:2308.15197v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15197</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate human mobility prediction underpins many important applications
across a variety of domains, including epidemic modelling, transport planning,
and emergency responses. Due to the sparsity of mobility data and the
stochastic nature of people&apos;s daily activities, achieving precise predictions
of people&apos;s locations remains a challenge. While recently developed large
language models (LLMs) have demonstrated superior performance across numerous
language-related tasks, their applicability to human mobility studies remains
unexplored. Addressing this gap, this article delves into the potential of LLMs
for human mobility prediction tasks. We introduce a novel method, LLM-Mob,
which leverages the language understanding and reasoning capabilities of LLMs
for analysing human mobility data. We present concepts of historical stays and
context stays to capture both long-term and short-term dependencies in human
movement and enable time-aware prediction by using time information of the
prediction target. Additionally, we design context-inclusive prompts that
enable LLMs to generate more accurate predictions. Comprehensive evaluations of
our method reveal that LLM-Mob excels in providing accurate and interpretable
predictions, highlighting the untapped potential of LLMs in advancing human
mobility prediction techniques. We posit that our research marks a significant
paradigm shift in human mobility modelling, transitioning from building complex
domain-specific models to harnessing general-purpose LLMs that yield accurate
predictions through language instructions. The code for this work is available
at https://github.com/xlwang233/LLM-Mob.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinglei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zichao Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tao Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02281">
<title>s-ID: Causal Effect Identification in a Sub-Population. (arXiv:2309.02281v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02281</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference in a sub-population involves identifying the causal effect
of an intervention on a specific subgroup, which is distinguished from the
whole population through the influence of systematic biases in the sampling
process. However, ignoring the subtleties introduced by sub-populations can
either lead to erroneous inference or limit the applicability of existing
methods. We introduce and advocate for a causal inference problem in
sub-populations (henceforth called s-ID), in which we merely have access to
observational data of the targeted sub-population (as opposed to the entire
population). Existing inference problems in sub-populations operate on the
premise that the given data distributions originate from the entire population,
thus, cannot tackle the s-ID problem. To address this gap, we provide necessary
and sufficient conditions that must hold in the causal graph for a causal
effect in a sub-population to be identifiable from the observational
distribution of that sub-population. Given these conditions, we present a sound
and complete algorithm for the s-ID problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abouei_A/0/1/0/all/0/1&quot;&gt;Amir Mohammad Abouei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1&quot;&gt;Ehsan Mokhtarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14345">
<title>Bias Testing and Mitigation in LLM-based Code Generation. (arXiv:2309.14345v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14345</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing state-of-the-art Large Language Models (LLMs), automatic code
generation models play a pivotal role in enhancing the productivity of software
development procedures. As the adoption of LLMs becomes more widespread in
software coding ecosystems, a pressing issue has emerged: does the generated
code contain social bias and unfairness, such as those related to age, gender,
and race? This issue concerns the integrity, fairness, and ethical foundation
of software applications that depend on the code generated by these models, yet
is under-explored in the literature. This paper presents a novel bias testing
framework that is specifically designed for code generation tasks. Based on
this framework, we conduct an extensive evaluation of the bias in code
generated by five state-of-the-art LLMs. Our findings reveal that 20.29% to
44.93% code functions generated by the models under study are biased when
handling bias sensitive tasks (i.e., tasks that involve sensitive attributes
such as age and gender). This indicates that the existing LLMs can be unfair in
code generation, posing risks of unintended and harmful software behaviors. To
mitigate bias for code generation models, we evaluate five bias mitigation
prompt strategies, i.e., utilizing bias testing results to refine the code
(zero-shot), one-, few-shot, and two Chain-of-Thought (CoT) prompts. Our
evaluation results illustrate that these strategies are all effective in
mitigating bias. Overall, one-shot and few-shot learning are the two most
effective. For GPT-4, 80% to 90% code bias can be removed with one-shot
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1&quot;&gt;Qingwen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Heming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06441">
<title>Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06441</link>
<description rdf:parseType="Literal">&lt;p&gt;Relational concept analysis (RCA) is an extension of formal concept analysis
allowing to deal with several related contexts simultaneously. It has been
designed for learning description logic theories from data and used within
various applications. A puzzling observation about RCA is that it returns a
single family of concept lattices although, when the data feature circular
dependencies, other solutions may be considered acceptable. The semantics of
RCA, provided in an operational way, does not shed light on this issue. In this
report, we define these acceptable solutions as those families of concept
lattices which belong to the space determined by the initial contexts
(well-formed), cannot scale new attributes (saturated), and refer only to
concepts of the family (self-supported). We adopt a functional view on the RCA
process by defining the space of well-formed solutions and two functions on
that space: one expansive and the other contractive. We show that the
acceptable solutions are the common fixed points of both functions. This is
achieved step-by-step by starting from a minimal version of RCA that considers
only one single context defined on a space of contexts and a space of lattices.
These spaces are then joined into a single space of context-lattice pairs,
which is further extended to a space of indexed families of context-lattice
pairs representing the objects manippulated by RCA. We show that RCA returns
the least element of the set of acceptable solutions. In addition, it is
possible to build dually an operation that generates its greatest element. The
set of acceptable solutions is a complete sublattice of the interval between
these two elements. Its structure and how the defined functions traverse it are
studied in detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Euzenat_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Euzenat&lt;/a&gt; (MOEX )</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06763">
<title>FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06763</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the interaction between proteins and ligands and accurately
predicting their binding structures is a critical yet challenging task in drug
discovery. Recent advancements in deep learning have shown promise in
addressing this challenge, with sampling-based and regression-based methods
emerging as two prominent approaches. However, these methods have notable
limitations. Sampling-based methods often suffer from low efficiency due to the
need for generating multiple candidate structures for selection. On the other
hand, regression-based methods offer fast predictions but may experience
decreased accuracy. Additionally, the variation in protein sizes often requires
external modules for selecting suitable binding pockets, further impacting
efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model
that combines pocket prediction and docking to achieve accurate and fast
protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed
pocket prediction module, which is also leveraged for docking pose estimation.
The model further enhances the docking process by incrementally integrating the
predicted pocket to optimize protein-ligand binding, reducing discrepancies
between training and inference. Through extensive experiments on benchmark
datasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages in
terms of effectiveness and efficiency compared to existing methods. Our code is
available at https://github.com/QizhiPei/FABind
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1&quot;&gt;Qizhi Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jinhua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingce Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shufang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12081">
<title>DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12081</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph matching is one of the most significant graph analytic tasks in
practice, which aims to find the node correspondence across different graphs.
Most existing approaches rely on adjacency matrices or node embeddings when
matching graphs, whose performances are often sub-optimal because of not fully
leveraging the multi-modal information hidden in graphs, such as node
attributes, subgraph structures, etc. In this study, we propose a novel and
effective graph matching method based on a differentiable hierarchical optimal
transport (HOT) framework, called DHOT-GM. Essentially, our method represents
each graph as a set of relational matrices corresponding to the information of
different modalities. Given two graphs, we enumerate all relational matrix
pairs and obtain their matching results, and accordingly, infer the node
correspondence by the weighted averaging of the matching results. This method
can be implemented as computing the HOT distance between the two graphs -- each
matching result is an optimal transport plan associated with the
Gromov-Wasserstein (GW) distance between two relational matrices, and the
weights of all matching results are the elements of an upper-level optimal
transport plan defined on the matrix sets. We propose a bi-level optimization
algorithm to compute the HOT distance in a differentiable way, making the
significance of the relational matrices adjustable. Experiments on various
graph matching tasks demonstrate the superiority and robustness of our method
compared to state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Haoran Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dixin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongteng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08516">
<title>LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08516</link>
<description rdf:parseType="Literal">&lt;p&gt;While self-correction has shown promise in improving LLM outputs in terms of
style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent
attempts to self-correct logical or reasoning errors often cause correct
answers to become incorrect, resulting in worse performances overall (Huang et
al., 2023). In this paper, we break down the self-correction process into two
core components: mistake finding and output correction. For mistake finding, we
release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought
reasoning traces. We provide benchmark numbers for several state-of-the-art
LLMs, and demonstrate that LLMs generally struggle with finding logical
mistakes. For output correction, we propose a backtracking method which
provides large improvements when given information on mistake location. We
construe backtracking as a lightweight alternative to reinforcement learning
methods, and show that it remains effective with a reward model at 60-70%
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyen_G/0/1/0/all/0/1&quot;&gt;Gladys Tyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1&quot;&gt;Hassan Mansoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbune_V/0/1/0/all/0/1&quot;&gt;Victor C&amp;#x103;rbune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peter Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mak_T/0/1/0/all/0/1&quot;&gt;Tony Mak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12241">
<title>InteraSSort: Interactive Assortment Planning Using Large Language Models. (arXiv:2311.12241v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12241</link>
<description rdf:parseType="Literal">&lt;p&gt;Assortment planning, integral to multiple commercial offerings, is a key
problem studied in e-commerce and retail settings. Numerous variants of the
problem along with their integration into business solutions have been
thoroughly investigated in the existing literature. However, the nuanced
complexities of in-store planning and a lack of optimization proficiency among
store planners with strong domain expertise remain largely overlooked. These
challenges frequently necessitate collaborative efforts with multiple
stakeholders which often lead to prolonged decision-making processes and
significant delays. To mitigate these challenges and capitalize on the
advancements of Large Language Models (LLMs), we propose an interactive
assortment planning framework, InteraSSort that augments LLMs with optimization
tools to assist store planners in making decisions through interactive
conversations. Specifically, we develop a solution featuring a user-friendly
interface that enables users to express their optimization objectives as input
text prompts to InteraSSort and receive tailored optimized solutions as output.
Our framework extends beyond basic functionality by enabling the inclusion of
additional constraints through interactive conversation, facilitating precise
and highly customized decision-making. Extensive experiments demonstrate the
effectiveness of our framework and potential extensions to a broad range of
operations management challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1&quot;&gt;Saketh Reddy Karra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulabandhula_T/0/1/0/all/0/1&quot;&gt;Theja Tulabandhula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13964">
<title>Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13964</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive segmentation is a crucial research area in medical image analysis
aiming to boost the efficiency of costly annotations by incorporating human
feedback. This feedback takes the form of clicks, scribbles, or masks and
allows for iterative refinement of the model output so as to efficiently guide
the system towards the desired behavior. In recent years, deep learning-based
approaches have propelled results to a new level causing a rapid growth in the
field with 121 methods proposed in the medical imaging domain alone. In this
review, we provide a structured overview of this emerging field featuring a
comprehensive taxonomy, a systematic review of existing methods, and an
in-depth analysis of current practices. Based on these contributions, we
discuss the challenges and opportunities in the field. For instance, we find
that there is a severe lack of comparison across methods which needs to be
tackled by standardized baselines and benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marinov_Z/0/1/0/all/0/1&quot;&gt;Zdravko Marinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1&quot;&gt;Paul F. J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01468">
<title>Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in Autonomous Driving. (arXiv:2312.01468v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01468</link>
<description rdf:parseType="Literal">&lt;p&gt;Our study assesses the adversarial robustness of LiDAR-camera fusion models
in 3D object detection. We introduce an attack technique that, by simply adding
a limited number of physically constrained adversarial points above a car, can
make the car undetectable by the fusion model. Experimental results reveal that
even without changes to the image data channel, the fusion model can be
deceived solely by manipulating the LiDAR data channel. This finding raises
safety concerns in the field of autonomous driving. Further, we explore how the
quantity of adversarial points, the distance between the front-near car and the
LiDAR-equipped car, and various angular factors affect the attack success rate.
We believe our research can contribute to the understanding of multi-sensor
robustness, offering insights and guidance to enhance the safety of autonomous
driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zizhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yushi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenyuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07302">
<title>From Knowledge Representation to Knowledge Organization and Back. (arXiv:2312.07302v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07302</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Representation (KR) and facet-analytical Knowledge Organization
(KO) have been the two most prominent methodologies of data and knowledge
modelling in the Artificial Intelligence community and the Information Science
community, respectively. KR boasts of a robust and scalable ecosystem of
technologies to support knowledge modelling while, often, underemphasizing the
quality of its models (and model-based data). KO, on the other hand, is less
technology-driven but has developed a robust framework of guiding principles
(canons) for ensuring modelling (and model-based data) quality. This paper
elucidates both the KR and facet-analytical KO methodologies in detail and
provides a functional mapping between them. Out of the mapping, the paper
proposes an integrated KO-enriched KR methodology with all the standard
components of a KR methodology plus the guiding canons of modelling quality
provided by KO. The practical benefits of the methodological integration has
been exemplified through a prominent case study of KR-based image annotation
exercise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1&quot;&gt;Mayukh Bagchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07476">
<title>Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07476</link>
<description rdf:parseType="Literal">&lt;p&gt;In-Context Learning (ICL) is an important paradigm for adapting Large
Language Models (LLMs) to downstream tasks through a few demonstrations.
Despite the great success of ICL, the limitation of the demonstration number
may lead to demonstration bias, i.e. the input-label mapping induced by LLMs
misunderstands the task&apos;s essence. Inspired by human experience, we attempt to
mitigate such bias through the perspective of the inter-demonstration
relationship. Specifically, we construct Comparable Demonstrations (CDs) by
minimally editing the texts to flip the corresponding labels, in order to
highlight the task&apos;s essence and eliminate potential spurious correlations
through the inter-demonstration comparison. Through a series of experiments on
CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can
significantly reduce such bias; (2) CDs exhibit good performance in ICL,
especially in out-of-distribution scenarios. In summary, this study explores
the ICL mechanisms from a novel perspective, providing a deeper insight into
the demonstration selection strategy for ICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Caoyun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jidong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yitian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09085">
<title>The Earth is Flat because...: Investigating LLMs&apos; Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09085</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) encapsulate vast amounts of knowledge but still
remain vulnerable to external misinformation. Existing research mainly studied
this susceptibility behavior in a single-turn setting. However, belief can
change during a multi-turn conversation, especially a persuasive one.
Therefore, in this study, we delve into LLMs&apos; susceptibility to persuasive
conversations, particularly on factual questions that they can answer
correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which
contains factual questions paired with systematically generated persuasive
misinformation. Then, we develop a testing framework to track LLMs&apos; belief
changes in a persuasive dialogue. Through extensive experiments, we find that
LLMs&apos; correct beliefs on factual knowledge can be easily manipulated by various
persuasive strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rongwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Brian S. Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shujian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weiyan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11535">
<title>Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior. (arXiv:2312.11535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11535</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel two-stage approach that fully utilizes the
information provided by the reference image to establish a customized knowledge
prior for image-to-3D generation. While previous approaches primarily rely on a
general diffusion prior, which struggles to yield consistent results with the
reference image, we propose a subject-specific and multi-modal diffusion model.
This model not only aids NeRF optimization by considering the shading mode for
improved geometry but also enhances texture from the coarse results to achieve
superior refinement. Both aspects contribute to faithfully aligning the 3D
content with the subject. Extensive experiments showcase the superiority of our
method, Customize-It-3D, outperforming previous works by a substantial margin.
It produces faithful 360-degree reconstructions with impressive visual quality,
making it well-suited for various applications, including text-to-3D creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Nan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Ting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11714">
<title>Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11714</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating time series data is a promising approach to address data
deficiency problems. However, it is also challenging due to the complex
temporal properties of time series data, including local correlations as well
as global dependencies. Most existing generative models have failed to
effectively learn both the local and global properties of time series data. To
address this open problem, we propose a novel time series generative model
named &apos;Time-Transformer AAE&apos;, which consists of an adversarial autoencoder
(AAE) and a newly designed architecture named &apos;Time-Transformer&apos; within the
decoder. The Time-Transformer first simultaneously learns local and global
features in a layer-wise parallel design, combining the abilities of Temporal
Convolutional Networks and Transformer in extracting local features and global
dependencies respectively. Second, a bidirectional cross attention is proposed
to provide complementary guidance across the two branches and achieve proper
fusion between local and global features. Experimental results demonstrate that
our model can outperform existing state-of-the-art models in 5 out of 6
datasets, specifically on those with data containing both global and local
properties. Furthermore, we highlight our model&apos;s advantage on handling this
kind of data via an artificial dataset. Finally, we show our model&apos;s ability to
address a real-world problem: data augmentation to support learning with small
datasets and imbalanced datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuansan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijewickrema_S/0/1/0/all/0/1&quot;&gt;Sudanthi Wijewickrema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bester_C/0/1/0/all/0/1&quot;&gt;Christofer Bester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OLeary_S/0/1/0/all/0/1&quot;&gt;Stephen O&amp;#x27;Leary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15224">
<title>LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination. (arXiv:2312.15224v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15224</link>
<description rdf:parseType="Literal">&lt;p&gt;AI agents powered by Large Language Models (LLMs) have made significant
advances, enabling them to assist humans in diverse complex tasks and leading
to a revolution in human-AI coordination. LLM-powered agents typically require
invoking LLM APIs and employing artificially designed complex prompts, which
results in high inference latency. While this paradigm works well in scenarios
with minimal interactive demands, such as code generation, it is unsuitable for
highly interactive and real-time applications, such as gaming. Traditional
gaming AI often employs small models or reactive policies, enabling fast
inference but offering limited task completion and interaction abilities. In
this work, we consider Overcooked as our testbed where players could
communicate with natural language and cooperate to serve orders. We propose a
Hierarchical Language Agent (HLA) for human-AI coordination that provides both
strong reasoning abilities while keeping real-time execution. In particular,
HLA adopts a hierarchical framework and comprises three modules: a proficient
LLM, referred to as Slow Mind, for intention reasoning and language
interaction, a lightweight LLM, referred to as Fast Mind, for generating macro
actions, and a reactive policy, referred to as Executor, for transforming macro
actions into atomic actions. Human studies show that HLA outperforms other
baseline agents, including slow-mind-only agents and fast-mind-only agents,
with stronger cooperation abilities, faster responses, and more consistent
language communications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuqing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qingmin Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17673">
<title>Jatmo: Prompt Injection Defense by Task-Specific Finetuning. (arXiv:2312.17673v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17673</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are attracting significant research attention
due to their instruction-following abilities, allowing users and developers to
leverage LLMs for a variety of tasks. However, LLMs are vulnerable to
prompt-injection attacks: a class of attacks that hijack the model&apos;s
instruction-following abilities, changing responses to prompts to undesired,
possibly malicious ones. In this work, we introduce Jatmo, a method for
generating task-specific models resilient to prompt-injection attacks. Jatmo
leverages the fact that LLMs can only follow instructions once they have
undergone instruction tuning. It harnesses a teacher instruction-tuned model to
generate a task-specific dataset, which is then used to fine-tune a base model
(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a
dataset of inputs for the task: it uses the teacher model to generate outputs.
For situations with no pre-existing datasets, Jatmo can use a single example,
or in some cases none at all, to produce a fully synthetic dataset. Our
experiments on seven tasks show that Jatmo models provide similar quality of
outputs on their specific task as standard LLMs, while being resilient to
prompt injections. The best attacks succeeded in less than 0.5% of cases
against our models, versus 87% success rate against GPT-3.5-Turbo. We release
Jatmo at https://github.com/wagner-group/prompt-injection-defense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piet_J/0/1/0/all/0/1&quot;&gt;Julien Piet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alrashed_M/0/1/0/all/0/1&quot;&gt;Maha Alrashed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1&quot;&gt;Chawin Sitawarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sizhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zeming Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1&quot;&gt;Elizabeth Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alomair_B/0/1/0/all/0/1&quot;&gt;Basel Alomair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00897">
<title>Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00897</link>
<description rdf:parseType="Literal">&lt;p&gt;As the deep learning revolution marches on, self-supervised learning has
garnered increasing attention in recent years thanks to its remarkable
representation learning ability and the low dependence on labeled data. Among
these varied self-supervised techniques, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training. This paradigm enables deep models to
learn robust representations and has demonstrated exceptional performance in
the context of computer vision, natural language processing, and other
modalities. In this survey, we present a comprehensive review of the masked
modeling framework and its methodology. We elaborate on the details of
techniques within masked modeling, including diverse masking strategies,
recovering targets, network architectures, and more. Then, we systematically
investigate its wide-ranging applications across domains. Furthermore, we also
explore the commonalities and differences between masked modeling methods in
different fields. Toward the end of this paper, we conclude by discussing the
limitations of current techniques and point out several potential avenues for
advancing masked modeling research. A paper list project with this survey is
available at \url{https://github.com/Lupin1998/Awesome-MIM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zedong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01286">
<title>A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01286</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs&apos; behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
give a deeper understanding of the knowledge structures inherent within LLMs.
Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jintian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01854">
<title>Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01854</link>
<description rdf:parseType="Literal">&lt;p&gt;As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1&quot;&gt;Uri Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1&quot;&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1&quot;&gt;Roee Aharoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1&quot;&gt;Reut Tsarfaty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eyal_M/0/1/0/all/0/1&quot;&gt;Matan Eyal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01990">
<title>GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01990</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_A/0/1/0/all/0/1&quot;&gt;Aarash Feizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1&quot;&gt;Adriana Romero-Soriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1&quot;&gt;Reihaneh Rabbany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02949">
<title>Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02949</link>
<description rdf:parseType="Literal">&lt;p&gt;Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users&apos; workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rute_J/0/1/0/all/0/1&quot;&gt;Jason Rute&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1&quot;&gt;Miroslav Ol&amp;#x161;&amp;#xe1;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaauwbroek_L/0/1/0/all/0/1&quot;&gt;Lasse Blaauwbroek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massolo_F/0/1/0/all/0/1&quot;&gt;Fidel Ivan Schaposnik Massolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piepenbrock_J/0/1/0/all/0/1&quot;&gt;Jelle Piepenbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pestun_V/0/1/0/all/0/1&quot;&gt;Vasily Pestun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02994">
<title>Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02994</link>
<description rdf:parseType="Literal">&lt;p&gt;In conversational AI research, there&apos;s a noticeable trend towards developing
models with a larger number of parameters, exemplified by models like ChatGPT.
While these expansive models tend to generate increasingly better chat
responses, they demand significant computational resources and memory. This
study explores a pertinent question: Can a combination of smaller models
collaboratively achieve comparable or enhanced performance relative to a
singular large model? We introduce an approach termed &quot;blending&quot;, a
straightforward yet effective method of integrating multiple chat AIs. Our
empirical evidence suggests that when specific smaller models are
synergistically blended, they can potentially outperform or match the
capabilities of much larger counterparts. For instance, integrating just three
models of moderate size (6B/13B paramaeters) can rival or even surpass the
performance metrics of a substantially larger model like ChatGPT (175B+
paramaters). This hypothesis is rigorously tested using A/B testing
methodologies with a large user base on the Chai research platform over a span
of thirty days. The findings underscore the potential of the &quot;blending&quot;
strategy as a viable approach for enhancing chat AI efficacy without a
corresponding surge in computational demands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaoding Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1&quot;&gt;Adian Liusie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1&quot;&gt;Vyas Raina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beauchamp_W/0/1/0/all/0/1&quot;&gt;William Beauchamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03512">
<title>Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03512</link>
<description rdf:parseType="Literal">&lt;p&gt;Finetuned large language models (such as ChatGPT and Qwen-chat) can generate
Chinese classical poetry following human&apos;s instructions. LLMs perform well in
content, but are usually lacking in format, with occasionally excess or
insufficient number of characters in each line. Since most SOTA LLMs are
token-based, we assume that the format inaccuracy is due to the difficulty of
the &quot;token planning&quot; task, which means that the LLM need to know exactly how
much characters are contained in each token and do length-control planning
based on that knowledge. In this paper, we first confirm our assumption by
showing that existing token-based large language models has limited knowledge
on token-character relationship. We use a spelling bee probing procedure, and
find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show
that a token-based model can be easily tailored into a token-free model (in
terms of Chinese), which can largely solve the format accuracy problem. Our
tailoring procedure removes long-tokens from the vocabulary and the language
model head, and keeps only character-level or byte-level tokens. As part of our
contribution, we release the finetuned token-free model (which is based on
Qwen-chat-7B), which can generate chinese classical poetry following complex
instructions like LLMs (such as story paraphrasing), and also perform well in
format. On the test set, our token-free model achives an format accuracy of
0.96, compared to 0.84 for token-based equivalents and 0.38 for GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chengyue Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_L/0/1/0/all/0/1&quot;&gt;Lei Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaotuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1&quot;&gt;Chenyi Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03729">
<title>The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance. (arXiv:2401.03729v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03729</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are regularly being used to label data across
many domains and for myriad tasks. By simply asking the LLM for an answer, or
``prompting,&apos;&apos; practitioners are able to use LLMs to quickly get a response for
an arbitrary task. This prompting is done through a series of decisions by the
practitioner, from simple wording of the prompt, to requesting the output in a
certain data format, to jailbreaking in the case of prompts that address more
sensitive topics. In this work, we ask: do variations in the way a prompt is
constructed change the ultimate decision of the LLM? We answer this using a
series of prompt variations across a variety of text classification tasks. We
find that even the smallest of perturbations, such as adding a space at the end
of a prompt, can cause the LLM to change its answer. Further, we find that
requesting responses in XML and commonly used jailbreaks can have cataclysmic
effects on the data labeled by LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salinas_A/0/1/0/all/0/1&quot;&gt;Abel Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1&quot;&gt;Fred Morstatter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03868">
<title>FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs. (arXiv:2401.03868v2 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03868</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based Large Language Models (LLMs) have made a significant impact
on various domains. However, LLMs&apos; efficiency suffers from both heavy
computation and memory overheads. Compression techniques like sparsification
and quantization are commonly used to mitigate the gap between LLM&apos;s
computation/memory overheads and hardware capacity. However, existing GPU and
transformer-based accelerators cannot efficiently process compressed LLMs, due
to the following unresolved challenges: low computational efficiency,
underutilized memory bandwidth, and large compilation overheads.
&lt;/p&gt;
&lt;p&gt;This paper proposes FlightLLM, enabling efficient LLMs inference with a
complete mapping flow on FPGAs. In FlightLLM, we highlight an innovative
solution that the computation and memory overhead of LLMs can be solved by
utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory
hierarchy). We propose a configurable sparse DSP chain to support different
sparsity patterns with high computation efficiency. Second, we propose an
always-on-chip decode scheme to boost memory bandwidth with mixed-precision
support. Finally, to make FlightLLM available for real-world LLMs, we propose a
length adaptive compilation method to reduce the compilation overhead.
Implemented on the Xilinx Alveo U280 FPGA, FlightLLM achieves 6.0$\times$
higher energy efficiency and 1.8$\times$ better cost efficiency against
commercial GPUs (e.g., NVIDIA V100S) on modern LLMs (e.g., LLaMA2-7B) using
vLLM and SmoothQuant under the batch size of one. FlightLLM beats NVIDIA A100
GPU with 1.2$\times$ higher throughput using the latest Versal VHK158 FPGA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shulin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guohao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tianyu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wenheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hanbo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yadong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jintao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1&quot;&gt;Kairui Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xuefei Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03988">
<title>A Primer on Temporal Graph Learning. (arXiv:2401.03988v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03988</link>
<description rdf:parseType="Literal">&lt;p&gt;This document aims to familiarize readers with temporal graph learning (TGL)
through a concept-first approach. We have systematically presented vital
concepts essential for understanding the workings of a TGL framework. In
addition to qualitative explanations, we have incorporated mathematical
formulations where applicable, enhancing the clarity of the text. Since TGL
involves temporal and spatial learning, we introduce relevant learning
architectures ranging from recurrent and convolutional neural networks to
transformers and graph neural networks. We also discuss classical time series
forecasting methods to inspire interpretable learning solutions for TGL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Aniq Ur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coon_J/0/1/0/all/0/1&quot;&gt;Justin P. Coon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03301">
<title>On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.03301</link>
<description rdf:parseType="Literal">&lt;p&gt;We seek to understand what facilitates sample-efficient learning from
historical datasets for sequential decision-making, a problem that is popularly
known as offline reinforcement learning (RL). Further, we are interested in
algorithms that enjoy sample efficiency while leveraging (value) function
approximation. In this paper, we address these fundamental questions by (i)
proposing a notion of data diversity that subsumes the previous notions of
coverage measures in offline RL and (ii) using this notion to {unify} three
distinct classes of offline RL algorithms based on version spaces (VS),
regularized optimization (RO), and posterior sampling (PS). We establish that
VS-based, RO-based, and PS-based algorithms, under standard assumptions,
achieve \emph{comparable} sample efficiency, which recovers the
state-of-the-art sub-optimality bounds for finite and linear model classes with
the standard assumptions. This result is surprising, given that the prior work
suggested an unfavorable sample complexity of the RO-based algorithm compared
to the VS-based algorithm, whereas posterior sampling is rarely considered in
offline RL due to its explorative nature. Notably, our proposed model-free
PS-based algorithm for offline RL is {novel}, with sub-optimality bounds that
are {frequentist} (i.e., worst-case) in nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Tang_T/0/1/0/all/0/1&quot;&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>