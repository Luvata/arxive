<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.13341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19069" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.05106">
<title>Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures. (arXiv:2308.05106v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05106</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an investigation into machine learning techniques for
violence detection in videos and their adaptation to a federated learning
context. The study includes experiments with spatio-temporal features extracted
from benchmark video datasets, comparison of different methods, and proposal of
a modified version of the &quot;Flow-Gated&quot; architecture called &quot;Diff-Gated.&quot;
Additionally, various machine learning techniques, including super-convergence
and transfer learning, are explored, and a method for adapting centralized
datasets to a federated learning context is developed. The research achieves
better accuracy results compared to state-of-the-art models by training the
best violence detection model in a federated learning context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quentin_P/0/1/0/all/0/1&quot;&gt;Pajon Quentin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swan_S/0/1/0/all/0/1&quot;&gt;Serre Swan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hugo_W/0/1/0/all/0/1&quot;&gt;Wissocq Hugo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leo_R/0/1/0/all/0/1&quot;&gt;Rabaud L&amp;#xe9;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siba_H/0/1/0/all/0/1&quot;&gt;Haidar Siba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoun_Y/0/1/0/all/0/1&quot;&gt;Yaacoub Antoun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05127">
<title>Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.05127</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant number of machine learning models are vulnerable to model
extraction attacks, which focus on stealing the models by using specially
curated queries against the target model. This task is well accomplished by
using part of the training data or a surrogate dataset to train a new model
that mimics a target model in a white-box environment. In pragmatic situations,
however, the target models are trained on private datasets that are
inaccessible to the adversary. The data-free model extraction technique
replaces this problem when it comes to using queries artificially curated by a
generator similar to that used in Generative Adversarial Nets. We propose for
the first time, to the best of our knowledge, an adversary black box attack
extending to a regression problem for predicting bounding box coordinates in
object detection. As part of our study, we found that defining a loss function
and using a novel generator setup is one of the key aspects in extracting the
target model. We find that the proposed model extraction method achieves
significant results by using reasonable queries. The discovery of this object
detection vulnerability will support future prospects for securing such models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1&quot;&gt;Harshit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+G_A/0/1/0/all/0/1&quot;&gt;Aravindhan G&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1&quot;&gt;Pavan Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govidarajulu_Y/0/1/0/all/0/1&quot;&gt;Yuvaraj Govidarajulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1&quot;&gt;Manojkumar Parmar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05170">
<title>FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. (arXiv:2308.05170v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2308.05170</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks achieve state-of-the-art performance in image classification,
speech recognition, scientific analysis and many more application areas. With
the ever-increasing need for faster computation and lower power consumption,
driven by real-time systems and Internet-of-Things (IoT) devices, FPGAs have
emerged as suitable devices for deep learning inference. Due to the high
computational complexity and memory footprint of neural networks, various
compression techniques, such as pruning, quantization and knowledge
distillation, have been proposed in literature. Pruning sparsifies a neural
network, reducing the number of multiplications and memory. However, pruning
often fails to capture properties of the underlying hardware, causing
unstructured sparsity and load-balance inefficiency, thus bottlenecking
resource improvements. We propose a hardware-centric formulation of pruning, by
formulating it as a knapsack problem with resource-aware tensor structures. The
primary emphasis is on real-time inference, with latencies in the order of
1$\mu$s, accelerated with hls4ml, an open-source framework for deep learning
inference on FPGAs. Evaluated on a range of tasks, including real-time particle
classification at CERN&apos;s Large Hadron Collider and fast image classification,
the proposed method achieves a reduction ranging between 55% and 92% in the
utilization of digital signal processing blocks (DSP) and up to 81% in block
memory (BRAM) utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramhorst_B/0/1/0/all/0/1&quot;&gt;Benjamin Ramhorst&lt;/a&gt; (Imperial College London), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constantinides_G/0/1/0/all/0/1&quot;&gt;George A. Constantinides&lt;/a&gt; (Imperial College London), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loncar_V/0/1/0/all/0/1&quot;&gt;Vladimir Loncar&lt;/a&gt; (Massachusetts Institute of Technology)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05176">
<title>Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models. (arXiv:2308.05176v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.05176</link>
<description rdf:parseType="Literal">&lt;p&gt;Epilepsy is a prevalent neurological disorder characterized by recurrent and
unpredictable seizures, necessitating accurate prediction for effective
management and patient care. Application of machine learning (ML) on
electroencephalogram (EEG) recordings, along with its ability to provide
valuable insights into brain activity during seizures, is able to make accurate
and robust seizure prediction an indispensable component in relevant studies.
In this research, we present a comprehensive comparative analysis of five
machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees
(ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction
of epileptic seizures using EEG data. The dataset underwent meticulous
preprocessing, including cleaning, normalization, outlier handling, and
oversampling, ensuring data quality and facilitating accurate model training.
These preprocessing techniques played a crucial role in enhancing the models&apos;
performance. The results of our analysis demonstrate the performance of each
model in terms of accuracy. The LR classifier achieved an accuracy of 56.95%,
while GB and DT both attained 97.17% accuracy. RT achieved a higher accuracy of
98.99%, while the ET model exhibited the best performance with an accuracy of
99.29%. Our findings reveal that the ET model outperformed not only the other
models in the comparative analysis but also surpassed the state-of-the-art
results from previous research. The superior performance of the ET model makes
it a compelling choice for accurate and robust epileptic seizure prediction
using EEG data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Talukder_M/0/1/0/all/0/1&quot;&gt;Md. Simul Hasan Talukder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sulaiman_R/0/1/0/all/0/1&quot;&gt;Rejwan Bin Sulaiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05184">
<title>PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions. (arXiv:2308.05184v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.05184</link>
<description rdf:parseType="Literal">&lt;p&gt;While diffusion-based text-to-image (T2I) models provide a simple and
powerful way to generate images, guiding this generation remains a challenge.
For concepts that are difficult to describe through language, users may
struggle to create prompts. Moreover, many of these models are built as
end-to-end systems, lacking support for iterative shaping of the image. In
response, we introduce PromptPaint, which combines T2I generation with
interactions that model how we use colored paints. PromptPaint allows users to
go beyond language to mix prompts that express challenging concepts. Just as we
iteratively tune colors through layered placements of paint on a physical
canvas, PromptPaint similarly allows users to apply different prompts to
different canvas areas and times of the generative process. Through a set of
studies, we characterize different approaches for mixing prompts, design
trade-offs, and socio-technical challenges for generative models. With
PromptPaint we provide insight into future steerable generative tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;John Joon Young Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adar_E/0/1/0/all/0/1&quot;&gt;Eytan Adar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05189">
<title>Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding. (arXiv:2308.05189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05189</link>
<description rdf:parseType="Literal">&lt;p&gt;This PhD. Thesis concerns the study and development of hierarchical
representations for spatio-temporal visual attention modeling and understanding
in video sequences. More specifically, we propose two computational models for
visual attention. First, we present a generative probabilistic model for
context-aware visual attention modeling and understanding. Secondly, we develop
a deep network architecture for visual attention modeling, which first
estimates top-down spatio-temporal visual attention, and ultimately serves for
modeling attention in the temporal domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Torres_M/0/1/0/all/0/1&quot;&gt;Miguel-&amp;#xc1;ngel Fern&amp;#xe1;ndez-Torres&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05201">
<title>&quot;Generate&quot; the Future of Work through AI: Empirical Evidence from Online Labor Markets. (arXiv:2308.05201v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05201</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of general-purpose Generative AI, the interest in discerning
its impact on the labor market escalates. In an attempt to bridge the extant
empirical void, we interpret the launch of ChatGPT as an exogenous shock, and
implement a Difference-in-Differences (DID) approach to quantify its influence
on text-related jobs and freelancers within an online labor marketplace. Our
results reveal a significant decrease in transaction volume for gigs and
freelancers directly exposed to ChatGPT. Additionally, this decline is
particularly marked in units of relatively higher past transaction volume or
lower quality standards. Yet, the negative effect is not universally
experienced among service providers. Subsequent analyses illustrate that
freelancers proficiently adapting to novel advancements and offering services
that augment AI technologies can yield substantial benefits amidst this
transformative period. Consequently, even though the advent of ChatGPT could
conceivably substitute existing occupations, it also unfolds immense
opportunities and carries the potential to reconfigure the future of work. This
research contributes to the limited empirical repository exploring the profound
influence of LLM-based generative AI on the labor market, furnishing invaluable
insights for workers, job intermediaries, and regulatory bodies navigating this
evolving landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jin Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xingchen Xu&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongjun Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yong Tan&lt;/a&gt; (2) ((1) University of Science and Technology of China, (2) University of Washington)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05221">
<title>Alexa, play with robot: Introducing the First Alexa Prize SimBot Challenge on Embodied AI. (arXiv:2308.05221v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.05221</link>
<description rdf:parseType="Literal">&lt;p&gt;The Alexa Prize program has empowered numerous university students to
explore, experiment, and showcase their talents in building conversational
agents through challenges like the SocialBot Grand Challenge and the TaskBot
Challenge. As conversational agents increasingly appear in multimodal and
embodied contexts, it is important to explore the affordances of conversational
interaction augmented with computer vision and physical embodiment. This paper
describes the SimBot Challenge, a new challenge in which university teams
compete to build robot assistants that complete tasks in a simulated physical
environment. This paper provides an overview of the SimBot Challenge, which
included both online and offline challenge phases. We describe the
infrastructure and support provided to the teams including Alexa Arena, the
simulated environment, and the ML toolkit provided to teams to accelerate their
building of vision and language models. We summarize the approaches the
participating teams took to overcome research challenges and extract key
lessons learned. Finally, we provide analysis of the performance of the
competing SimBots during the competition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hangjie Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_L/0/1/0/all/0/1&quot;&gt;Leslie Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1&quot;&gt;Govind Thattai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Desheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lucy Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qiaozi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakiah_S/0/1/0/all/0/1&quot;&gt;Suhaila Shakiah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1&quot;&gt;Aishwarya Padmakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bofei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1&quot;&gt;Cadence Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guthy_D/0/1/0/all/0/1&quot;&gt;Dinakar Guthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1&quot;&gt;Gaurav Sukhatme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arumugam_K/0/1/0/all/0/1&quot;&gt;Karthika Arumugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1&quot;&gt;Matthew Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ipek_O/0/1/0/all/0/1&quot;&gt;Osman Ipek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1&quot;&gt;Patrick Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rohan Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pansare_S/0/1/0/all/0/1&quot;&gt;Shreyas Pansare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vasu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flagg_C/0/1/0/all/0/1&quot;&gt;Cris Flagg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pressel_D/0/1/0/all/0/1&quot;&gt;Daniel Pressel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaz_L/0/1/0/all/0/1&quot;&gt;Lavina Vaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Luke Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahai_S/0/1/0/all/0/1&quot;&gt;Sattvik Sahai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaohua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottardi_A/0/1/0/all/0/1&quot;&gt;Anna Gottardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1&quot;&gt;Dilek Hakkani-Tur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bland_K/0/1/0/all/0/1&quot;&gt;Kate Bland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocker_H/0/1/0/all/0/1&quot;&gt;Heather Rocker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeun_J/0/1/0/all/0/1&quot;&gt;James Jeun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1&quot;&gt;Yadunandana Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnston_M/0/1/0/all/0/1&quot;&gt;Michael Johnston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyengar_A/0/1/0/all/0/1&quot;&gt;Akshaya Iyengar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1&quot;&gt;Arindam Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1&quot;&gt;Prem Natarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanadan_R/0/1/0/all/0/1&quot;&gt;Reza Ghanadan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05234">
<title>Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving. (arXiv:2308.05234v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05234</link>
<description rdf:parseType="Literal">&lt;p&gt;Environmental perception is a key element of autonomous driving because the
information received from the perception module influences core driving
decisions. An outstanding challenge in real-time perception for autonomous
driving lies in finding the best trade-off between detection quality and
latency. Major constraints on both computation and power have to be taken into
account for real-time perception in autonomous vehicles. Larger object
detection models tend to produce the best results, but are also slower at
runtime. Since the most accurate detectors cannot run in real-time locally, we
investigate the possibility of offloading computation to edge and cloud
platforms, which are less resource-constrained. We create a synthetic dataset
to train object detection models and evaluate different offloading strategies.
Using real hardware and network simulations, we compare different trade-offs
between prediction quality and end-to-end delay. Since sending raw frames over
the network implies additional transmission delays, we also explore the use of
JPEG and H.265 compression at varying qualities and measure their impact on
prediction metrics. We show that models with adequate compression can be run in
real-time on the cloud while outperforming local detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawlader_F/0/1/0/all/0/1&quot;&gt;Faisal Hawlader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinet_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Robinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Frank&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05242">
<title>Vector quantization loss analysis in VQGANs: a single-GPU ablation study for image-to-image synthesis. (arXiv:2308.05242v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05242</link>
<description rdf:parseType="Literal">&lt;p&gt;This study performs an ablation analysis of Vector Quantized Generative
Adversarial Networks (VQGANs), concentrating on image-to-image synthesis
utilizing a single NVIDIA A100 GPU. The current work explores the nuanced
effects of varying critical parameters including the number of epochs, image
count, and attributes of codebook vectors and latent dimensions, specifically
within the constraint of limited resources. Notably, our focus is pinpointed on
the vector quantization loss, keeping other hyperparameters and loss components
(GAN loss) fixed. This was done to delve into a deeper understanding of the
discrete latent space, and to explore how varying its size affects the
reconstruction. Though, our results do not surpass the existing benchmarks,
however, our findings shed significant light on VQGAN&apos;s behaviour for a smaller
dataset, particularly concerning artifacts, codebook size optimization, and
comparative analysis with Principal Component Analysis (PCA). The study also
uncovers the promising direction by introducing 2D positional encodings,
revealing a marked reduction in artifacts and insights into balancing clarity
and overfitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_L/0/1/0/all/0/1&quot;&gt;Luv Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1&quot;&gt;Varun Mohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05260">
<title>AI4GCC -- Track 3: Consumption and the Challenges of Multi-Agent RL. (arXiv:2308.05260v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05260</link>
<description rdf:parseType="Literal">&lt;p&gt;The AI4GCC competition presents a bold step forward in the direction of
integrating machine learning with traditional economic policy analysis. Below,
we highlight two potential areas for improvement that could enhance the
competition&apos;s ability to identify and evaluate proposed negotiation protocols.
Firstly, we suggest the inclusion of an additional index that accounts for
consumption/utility as part of the evaluation criteria. Secondly, we recommend
further investigation into the learning dynamics of agents in the simulator and
the game theoretic properties of outcomes from proposed negotiation protocols.
We hope that these suggestions can be of use for future iterations of the
competition/simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiralerspong_M/0/1/0/all/0/1&quot;&gt;Marco Jiralerspong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1&quot;&gt;Gauthier Gidel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05275">
<title>Cross-heterogeneity Graph Few-shot Learning. (arXiv:2308.05275v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.05275</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, heterogeneous graph few-shot learning has been proposed to
address the label sparsity issue in heterogeneous graphs (HGs), which contain
various types of nodes and edges. The existing methods have achieved good
performance by transferring generalized knowledge extracted from rich-labeled
classes in source HG(s) to few-labeled classes in a target HG. However, these
methods only consider the single-heterogeneity scenario where the source and
target HGs share a fixed set of node/edge types, ignoring the more general
scenario of cross-heterogeneity, where each HG can have a different and
non-fixed set of node/edge types. To this end, we focus on the unexplored
cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity
Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns
to capture heterogeneous information and propose a multi-view heterogeneous
graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose
a score module to measure the informativeness of labeled samples and determine
the transferability of each source HG. Finally, by integrating MHGN and the
score module into a meta-learning mechanism, CGFL can effectively transfer
generalized knowledge to predict new classes with few-labeled data. Extensive
experiments on four real-world datasets have demonstrated the superior
performance of CGFL over the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1&quot;&gt;Pengfei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanfeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05295">
<title>Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception. (arXiv:2308.05295v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05295</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently developed pretrained models can encode rich world knowledge
expressed in multiple modalities, such as text and images. However, the outputs
of these models cannot be integrated into algorithms to solve sequential
decision-making tasks. We develop an algorithm that utilizes the knowledge from
pretrained models to construct and verify controllers for sequential
decision-making tasks, and to ground these controllers to task environments
through visual observations. In particular, the algorithm queries a pretrained
model with a user-provided, text-based task description and uses the model&apos;s
output to construct an automaton-based controller that encodes the model&apos;s
task-relevant knowledge. It then verifies whether the knowledge encoded in the
controller is consistent with other independently available knowledge, which
may include abstract information on the environment or user-provided
specifications. If this verification step discovers any inconsistency, the
algorithm automatically refines the controller to resolve the inconsistency.
Next, the algorithm leverages the vision and language capabilities of
pretrained models to ground the controller to the task environment. It collects
image-based observations from the task environment and uses the pretrained
model to link these observations to the text-based control logic encoded in the
controller (e.g., actions and conditions that trigger the actions). We propose
a mechanism to ensure the controller satisfies the user-provided specification
even when perceptual uncertainties are present. We demonstrate the algorithm&apos;s
ability to construct, verify, and ground automaton-based controllers through a
suite of real-world tasks, including daily life and robot manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neary_C/0/1/0/all/0/1&quot;&gt;Cyrus Neary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1&quot;&gt;Ufuk Topcu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05298">
<title>Double-chain Constraints for 3D Human Pose Estimation in Images and Videos. (arXiv:2308.05298v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05298</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing 3D poses from 2D poses lacking depth information is
particularly challenging due to the complexity and diversity of human motion.
The key is to effectively model the spatial constraints between joints to
leverage their inherent dependencies. Thus, we propose a novel model, called
Double-chain Graph Convolutional Transformer (DC-GCT), to constrain the pose
through a double-chain design consisting of local-to-global and global-to-local
chains to obtain a complex representation more suitable for the current human
pose. Specifically, we combine the advantages of GCN and Transformer and design
a Local Constraint Module (LCM) based on GCN and a Global Constraint Module
(GCM) based on self-attention mechanism as well as a Feature Interaction Module
(FIM). The proposed method fully captures the multi-level dependencies between
human body joints to optimize the modeling capability of the model. Moreover,
we propose a method to use temporal information into the single-frame model by
guiding the video sequence embedding through the joint embedding of the target
frame, with negligible increase in computational cost. Experimental results
demonstrate that DC-GCT achieves state-of-the-art performance on two
challenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achieves
state-of-the-art performance on all action categories in the Human3.6M dataset
using detected 2D poses from CPN, and our code is available at:
https://github.com/KHB1698/DC-GCT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hongbo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Doudou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05309">
<title>Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.05309</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph clustering is a fundamental task in graph analysis, and recent advances
in utilizing graph neural networks (GNNs) have shown impressive results.
Despite the success of existing GNN-based graph clustering methods, they often
overlook the quality of graph structure, which is inherent in real-world graphs
due to their sparse and multifarious nature, leading to subpar performance.
Graph structure learning allows refining the input graph by adding missing
links and removing spurious connections. However, previous endeavors in graph
structure learning have predominantly centered around supervised settings, and
cannot be directly applied to our specific clustering tasks due to the absence
of ground-truth labels. To bridge the gap, we propose a novel method called
\textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering
(HoLe). Our motivation stems from the observation that subtly enhancing the
degree of homophily within the graph structure can significantly improve GNNs
and clustering outcomes. To realize this objective, we develop two
clustering-oriented structure learning modules, i.e., hierarchical correlation
estimation and cluster-aware sparsification. The former module enables a more
accurate estimation of pairwise node relationships by leveraging guidance from
latent and clustering spaces, while the latter one generates a sparsified
structure based on the similarity matrix and clustering assignments.
Additionally, we devise a joint optimization approach alternating between
training the homophily-enhanced structure learning and GNN-based clustering,
thereby enforcing their reciprocal effects. Extensive experiments on seven
benchmark datasets of various types and scales, across a range of clustering
metrics, demonstrate the superiority of HoLe against state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Ming Gu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Gaoming Yang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sheng Zhou&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Ning Ma&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qiaoyu Tan&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meihan Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt; (1) ((1) College of Computer Science and Technology, Zhejiang University, (2) School of Software Technology, Zhejiang University, (3) Zhejiang Provincial Key Laboratory of Service Robot, Zhejiang University, (4) Department of Computer Science, New York University Shanghai)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05320">
<title>Adv-Inpainting: Generating Natural and Transferable Adversarial Patch via Attention-guided Feature Fusion. (arXiv:2308.05320v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05320</link>
<description rdf:parseType="Literal">&lt;p&gt;The rudimentary adversarial attacks utilize additive noise to attack facial
recognition (FR) models. However, because manipulating the total face is
impractical in the physical setting, most real-world FR attacks are based on
adversarial patches, which limit perturbations to a small area. Previous
adversarial patch attacks often resulted in unnatural patterns and clear
boundaries that were easily noticeable. In this paper, we argue that generating
adversarial patches with plausible content can result in stronger
transferability than using additive noise or directly sampling from the latent
space. To generate natural-looking and highly transferable adversarial patches,
we propose an innovative two-stage coarse-to-fine attack framework called
Adv-Inpainting. In the first stage, we propose an attention-guided StyleGAN
(Att-StyleGAN) that adaptively combines texture and identity features based on
the attention map to generate high-transferable and natural adversarial
patches. In the second stage, we design a refinement network with a new
boundary variance loss to further improve the coherence between the patch and
its surrounding area. Experiment results demonstrate that Adv-Inpainting is
stealthy and can produce adversarial patches with stronger transferability and
improved visual quality than previous adversarial patch attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1&quot;&gt;Mingxing Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05341">
<title>Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT. (arXiv:2308.05341v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.05341</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, generative AIs like ChatGPT have become available to the wide
public. These tools can for instance be used by students to generate essays or
whole theses. But how does a teacher know whether a text is written by a
student or an AI? In our work, we explore traditional and new features to (1)
detect text generated by AI from scratch and (2) text rephrased by AI. Since we
found that classification is more difficult when the AI has been instructed to
create the text in a way that a human would not recognize that it was generated
by an AI, we also investigate this more advanced case. For our experiments, we
produced a new text corpus covering 10 school topics. Our best systems to
classify basic and advanced human-generated/AI-generated texts have F1-scores
of over 96%. Our best systems for classifying basic and advanced
human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems
use a combination of perplexity, semantic, list lookup, error-based,
readability, AI feedback, and text vector features. Our results show that the
new features substantially help to improve the performance of many classifiers.
Our best basic text rephrasing detection system even outperforms GPTZero by
183.8% relative in F1-score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindner_L/0/1/0/all/0/1&quot;&gt;Lorenz Mindner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlippe_T/0/1/0/all/0/1&quot;&gt;Tim Schlippe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaaff_K/0/1/0/all/0/1&quot;&gt;Kristina Schaaff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05342">
<title>Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.05342</link>
<description rdf:parseType="Literal">&lt;p&gt;In Large Language Models (LLMs), there have been consistent advancements in
task-specific performance, largely influenced by effective prompt design. While
recent research on prompting has enhanced the reasoning capabilities of LLMs, a
gap remains in further improving their understanding abilities. In this study,
we introduce metacognitive prompting (MP), a strategy inspired by human
introspective reasoning processes. Using MP, LLMs undergo a systematic series
of structured, self-aware evaluations, drawing on both their vast inherent
knowledge and new insights. Our experiments involve five prevalent LLMs:
Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general
natural language understanding (NLU) tasks from the GLUE and SuperGLUE
benchmarks. Results indicate that, although GPT-4 consistently excels in most
tasks, PaLM, when equipped with MP, approaches its performance level.
Furthermore, across models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought prompting. This
study underscores the potential to amplify the understanding abilities of LLMs
and highlights the benefits of mirroring human introspective reasoning in NLU
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05364">
<title>Machine Learning aided Computer Architecture Design for CNN Inferencing Systems. (arXiv:2308.05364v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2308.05364</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient and timely calculations of Machine Learning (ML) algorithms are
essential for emerging technologies like autonomous driving, the Internet of
Things (IoT), and edge computing. One of the primary ML algorithms used in such
systems is Convolutional Neural Networks (CNNs), which demand high
computational resources. This requirement has led to the use of ML accelerators
like GPGPUs to meet design constraints. However, selecting the most suitable
accelerator involves Design Space Exploration (DSE), a process that is usually
time-consuming and requires significant manual effort. Our work presents
approaches to expedite the DSE process by identifying the most appropriate
GPGPU for CNN inferencing systems. We have developed a quick and precise
technique for forecasting the power and performance of CNNs during inference,
with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer
architects to estimate power and performance in the early stages of
development, reducing the necessity for numerous prototypes. This saves time
and money while also improving the time-to-market period.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metz_C/0/1/0/all/0/1&quot;&gt;Christopher A. Metz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05374">
<title>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models&apos; Alignment. (arXiv:2308.05374v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05374</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring alignment, which refers to making models behave in accordance with
human intentions [1,2], has become a critical task before deploying large
language models (LLMs) in real-world applications. For instance, OpenAI devoted
six months to iteratively aligning GPT-4 before its release [3]. However, a
major challenge faced by practitioners is the lack of clear guidance on
evaluating whether LLM outputs align with social norms, values, and
regulations. This obstacle hinders systematic iteration and deployment of LLMs.
To address this issue, this paper presents a comprehensive survey of key
dimensions that are crucial to consider when assessing LLM trustworthiness. The
survey covers seven major categories of LLM trustworthiness: reliability,
safety, fairness, resistance to misuse, explainability and reasoning, adherence
to social norms, and robustness. Each major category is further divided into
several sub-categories, resulting in a total of 29 sub-categories.
Additionally, a subset of 8 sub-categories is selected for further
investigation, where corresponding measurement studies are designed and
conducted on several widely-used LLMs. The measurement results indicate that,
in general, more aligned models tend to perform better in terms of overall
trustworthiness. However, the effectiveness of alignment varies across the
different trustworthiness categories considered. This highlights the importance
of conducting more fine-grained analyses, testing, and making continuous
improvements on LLM alignment. By shedding light on these key dimensions of LLM
trustworthiness, this paper aims to provide valuable insights and guidance to
practitioners in the field. Understanding and addressing these concerns will be
crucial in achieving reliable and ethically sound deployment of LLMs in various
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuanshun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1&quot;&gt;Jean-Francois Ton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ruocheng Guo Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klochkov_Y/0/1/0/all/0/1&quot;&gt;Yegor Klochkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taufiq_M/0/1/0/all/0/1&quot;&gt;Muhammad Faaiz Taufiq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05379">
<title>Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning. (arXiv:2308.05379v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;Relevance modeling aims to locate desirable items for corresponding queries,
which is crucial for search engines to ensure user experience. Although most
conventional approaches address this problem by assessing the semantic
similarity between the query and item, pure semantic matching is not
everything. In reality, auxiliary query-item interactions extracted from user
historical behavior data of the search log could provide hints to reveal users&apos;
search intents further. Drawing inspiration from this, we devise a novel
Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that
leverages neighbor queries of target item and neighbor items of target query to
complement target query-item semantic matching. Specifically, our model builds
multi-level co-attention for distilling coarse-grained and fine-grained
semantic representations from both neighbor and target views. The model
subsequently employs neighbor-target self-supervised learning to improve the
accuracy and robustness of BARL-ASe by strengthening representation and logit
learning. Furthermore, we discuss how to deal with the long-tail query-item
matching of the mini apps search scenario of Alipay practically. Experiments on
real-world industry data and online A/B testing demonstrate our proposal
achieves promising performance with low latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05385">
<title>Adaptive Taxonomy Learning and Historical Patterns Modelling for Patent Classification. (arXiv:2308.05385v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05385</link>
<description rdf:parseType="Literal">&lt;p&gt;Patent classification aims to assign multiple International Patent
Classification (IPC) codes to a given patent. Recent methods for automatically
classifying patents mainly focus on analyzing the text descriptions of patents.
However, apart from the texts, each patent is also associated with some
assignees, and the knowledge of their applied patents is often valuable for
classification. Furthermore, the hierarchical taxonomy formulated by the IPC
system provides important contextual information and enables models to leverage
the correlations between IPC codes for more accurate classification. However,
existing methods fail to incorporate the above aspects. In this paper, we
propose an integrated framework that comprehensively considers the information
on patents for patent classification. To be specific, we first present an IPC
codes correlations learning module to derive their semantic representations via
adaptively passing and aggregating messages within the same level and across
different levels along the hierarchical taxonomy. Moreover, we design a
historical application patterns learning component to incorporate the
corresponding assignee&apos;s previous patents by a dual channel aggregation
mechanism. Finally, we combine the contextual information of patent texts that
contains the semantics of IPC codes, and assignees&apos; sequential preferences to
make predictions. Experiments on real-world datasets demonstrate the
superiority of our approach over the existing methods. Besides, we present the
model&apos;s ability to capture the temporal patterns of assignees and the semantic
dependencies among IPC codes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_T/0/1/0/all/0/1&quot;&gt;Tao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Le Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Leilei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bowen Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Deqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1&quot;&gt;Fuzhen Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05391">
<title>Enhancing Trust in LLM-Based AI Automation Agents: New Considerations and Future Challenges. (arXiv:2308.05391v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05391</link>
<description rdf:parseType="Literal">&lt;p&gt;Trust in AI agents has been extensively studied in the literature, resulting
in significant advancements in our understanding of this field. However, the
rapid advancements in Large Language Models (LLMs) and the emergence of
LLM-based AI agent frameworks pose new challenges and opportunities for further
research. In the field of process automation, a new generation of AI-based
agents has emerged, enabling the execution of complex tasks. At the same time,
the process of building automation has become more accessible to business users
via user-friendly no-code tools and training mechanisms. This paper explores
these new challenges and opportunities, analyzes the main aspects of trust in
AI agents discussed in existing literature, and identifies specific
considerations and challenges relevant to this new generation of automation
agents. We also evaluate how nascent products in this category address these
considerations. Finally, we highlight several challenges that the research
community should address in this evolving landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_S/0/1/0/all/0/1&quot;&gt;Sivan Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaeli_A/0/1/0/all/0/1&quot;&gt;Avi Yaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlomov_S/0/1/0/all/0/1&quot;&gt;Segev Shlomov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05407">
<title>A Comparative Assessment of Multi-view fusion learning for Crop Classification. (arXiv:2308.05407v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05407</link>
<description rdf:parseType="Literal">&lt;p&gt;With a rapidly increasing amount and diversity of remote sensing (RS) data
sources, there is a strong need for multi-view learning modeling. This is a
complex task when considering the differences in resolution, magnitude, and
noise of RS data. The typical approach for merging multiple RS sources has been
input-level fusion, but other - more advanced - fusion strategies may
outperform this traditional approach. This work assesses different fusion
strategies for crop classification in the CropHarvest dataset. The fusion
methods proposed in this work outperform models based on individual views and
previous fusion methods. We do not find one single fusion method that
consistently outperforms all other approaches. Instead, we present a comparison
of multi-view fusion methods for three different datasets and show that,
depending on the test region, different methods obtain the best performance.
Despite this, we suggest a preliminary criterion for the selection of fusion
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mena_F/0/1/0/all/0/1&quot;&gt;Francisco Mena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_D/0/1/0/all/0/1&quot;&gt;Diego Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuske_M/0/1/0/all/0/1&quot;&gt;Marlon Nuske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05411">
<title>Explainable AI applications in the Medical Domain: a systematic review. (arXiv:2308.05411v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05411</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence in Medicine has made significant progress with
emerging applications in medical imaging, patient care, and other areas. While
these applications have proven successful in retrospective studies, very few of
them were applied in practice.The field of Medical AI faces various challenges,
in terms of building user trust, complying with regulations, using data
ethically.Explainable AI (XAI) aims to enable humans understand AI and trust
its results. This paper presents a literature review on the recent developments
of XAI solutions for medical decision support, based on a representative sample
of 198 articles published in recent years. The systematic synthesis of the
relevant articles resulted in several findings. (1) model-agnostic XAI
techniques were mostly employed in these solutions, (2) deep learning models
are utilized more than other types of machine learning models, (3)
explainability was applied to promote trust, but very few works reported the
physicians participation in the loop, (4) visual and interactive user interface
is more useful in understanding the explanation and the recommendation of the
system. More research is needed in collaboration between medical and AI
experts, that could guide the development of suitable frameworks for the
design, implementation, and evaluation of XAI solutions in medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prentzas_N/0/1/0/all/0/1&quot;&gt;Nicoletta Prentzas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakas_A/0/1/0/all/0/1&quot;&gt;Antonis Kakas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pattichis_C/0/1/0/all/0/1&quot;&gt;Constantinos S. Pattichis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05478">
<title>Reviewing 3D Object Detectors in the Context of High-Resolution 3+1D Radar. (arXiv:2308.05478v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.05478</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments and the beginning market introduction of high-resolution
imaging 4D (3+1D) radar sensors have initialized deep learning-based radar
perception research. We investigate deep learning-based models operating on
radar point clouds for 3D object detection. 3D object detection on lidar point
cloud data is a mature area of 3D vision. Many different architectures have
been proposed, each with strengths and weaknesses. Due to similarities between
3D lidar point clouds and 3+1D radar point clouds, those existing 3D object
detectors are a natural basis to start deep learning-based 3D object detection
on radar data. Thus, the first step is to analyze the detection performance of
the existing models on the new data modality and evaluate them in depth. In
order to apply existing 3D point cloud object detectors developed for lidar
point clouds to the radar domain, they need to be adapted first. While some
detectors, such as PointPillars, have already been adapted to be applicable to
radar data, we have adapted others, e.g., Voxel R-CNN, SECOND, PointRCNN, and
PV-RCNN. To this end, we conduct a cross-model validation (evaluating a set of
models on one particular data set) as well as a cross-data set validation
(evaluating all models in the model set on several data sets). The
high-resolution radar data used are the View-of-Delft and Astyx data sets.
Finally, we evaluate several adaptations of the models and their training
procedures. We also discuss major factors influencing the detection performance
on radar data and propose possible solutions indicating potential future
research avenues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmer_P/0/1/0/all/0/1&quot;&gt;Patrick Palmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_M/0/1/0/all/0/1&quot;&gt;Martin Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altendorfer_R/0/1/0/all/0/1&quot;&gt;Richard Altendorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_G/0/1/0/all/0/1&quot;&gt;Ganesh Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1&quot;&gt;Torsten Bertram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05481">
<title>LLM As DBA. (arXiv:2308.05481v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2308.05481</link>
<description rdf:parseType="Literal">&lt;p&gt;Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuanhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05496">
<title>Exploring XAI for the Arts: Explaining Latent Space in Generative Music. (arXiv:2308.05496v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05496</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable AI has the potential to support more interactive and fluid
co-creative AI systems which can creatively collaborate with people. To do
this, creative AI models need to be amenable to debugging by offering
eXplainable AI (XAI) features which are inspectable, understandable, and
modifiable. However, currently there is very little XAI for the arts. In this
work, we demonstrate how a latent variable model for music generation can be
made more explainable; specifically we extend MeasureVAE which generates
measures of music. We increase the explainability of the model by: i) using
latent space regularisation to force some specific dimensions of the latent
space to map to meaningful musical attributes, ii) providing a user interface
feedback loop to allow people to adjust dimensions of the latent space and
observe the results of these changes in real-time, iii) providing a
visualisation of the musical attributes in the latent space to help people
understand and predict the effect of changes to latent space dimensions. We
suggest that in doing so we bridge the gap between the latent space and the
generated musical outcomes in a meaningful way which makes the model and its
outputs more explainable and more debuggable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bryan_Kinns_N/0/1/0/all/0/1&quot;&gt;Nick Bryan-Kinns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banar_B/0/1/0/all/0/1&quot;&gt;Berker Banar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ford_C/0/1/0/all/0/1&quot;&gt;Corey Ford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1&quot;&gt;Courtney N. Reed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colton_S/0/1/0/all/0/1&quot;&gt;Simon Colton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armitage_J/0/1/0/all/0/1&quot;&gt;Jack Armitage&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05501">
<title>More Than Meets the Eye: Analyzing Anesthesiologists&apos; Visual Attention in the Operating Room Using Deep Learning Models. (arXiv:2308.05501v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05501</link>
<description rdf:parseType="Literal">&lt;p&gt;Patient&apos;s vital signs, which are displayed on monitors, make the
anesthesiologist&apos;s visual attention (VA) a key component in the safe management
of patients under general anesthesia; moreover, the distribution of said VA and
the ability to acquire specific cues throughout the anesthetic, may have a
direct impact on patient&apos;s outcome. Currently, most studies employ wearable
eye-tracking technologies to analyze anesthesiologists&apos; visual patterns. Albeit
being able to produce meticulous data, wearable devices are not a sustainable
solution for large-scale or long-term use for data collection in the operating
room (OR). Thus, by utilizing a novel eye-tracking method in the form of deep
learning models that process monitor-mounted webcams, we collected continuous
behavioral data and gained insight into the anesthesiologist&apos;s VA distribution
with minimal disturbance to their natural workflow. In this study, we collected
OR video recordings using the proposed framework and compared different visual
behavioral patterns. We distinguished between baseline VA distribution during
uneventful periods to patterns associated with active phases or during
critical, unanticipated incidents. In the future, such a platform may serve as
a crucial component of context-aware assistive technologies in the OR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gershov_S/0/1/0/all/0/1&quot;&gt;Sapir Gershov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahameed_F/0/1/0/all/0/1&quot;&gt;Fadi Mahameed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raz_A/0/1/0/all/0/1&quot;&gt;Aeyal Raz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laufer_S/0/1/0/all/0/1&quot;&gt;Shlomi Laufer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05502">
<title>Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.05502</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based language models (TLMs) have widely been recognized to be a
cutting-edge technology for the successful development of deep-learning-based
solutions to problems and applications that require natural language processing
and understanding. Like for other textual domains, TLMs have indeed pushed the
state-of-the-art of AI approaches for many tasks of interest in the legal
domain. Despite the first Transformer model being proposed about six years ago,
there has been a rapid progress of this technology at an unprecedented rate,
whereby BERT and related models represent a major reference, also in the legal
domain. This article provides the first systematic overview of TLM-based
methods for AI-driven problems and tasks in the legal sphere. A major goal is
to highlight research advances in this field so as to understand, on the one
hand, how the Transformers have contributed to the success of AI in supporting
legal processes, and on the other hand, what are the current limitations and
opportunities for further research development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greco_C/0/1/0/all/0/1&quot;&gt;Candida M. Greco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagarelli_A/0/1/0/all/0/1&quot;&gt;Andrea Tagarelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05503">
<title>EFX Allocations Exist for Binary Valuations. (arXiv:2308.05503v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2308.05503</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the fair division problem and the existence of allocations
satisfying the fairness criterion envy-freeness up to any item (EFX). The
existence of EFX allocations is a major open problem in the fair division
literature. We consider binary valuations where the marginal gain of the value
by receiving an extra item is either $0$ or $1$. Babaioff et al. [2021] proved
that EFX allocations always exist for binary and submodular valuations. In this
paper, by using completely different techniques, we extend this existence
result to general binary valuations that are not necessarily submodular, and we
present a polynomial time algorithm for computing an EFX allocation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05508">
<title>Multi-domain Recommendation with Embedding Disentangling and Domain Alignment. (arXiv:2308.05508v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.05508</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-domain recommendation (MDR) aims to provide recommendations for
different domains (e.g., types of products) with overlapping users/items and is
common for platforms such as Amazon, Facebook, and LinkedIn that host multiple
services. Existing MDR models face two challenges: First, it is difficult to
disentangle knowledge that generalizes across domains (e.g., a user likes cheap
items) and knowledge specific to a single domain (e.g., a user likes blue
clothing but not blue cars). Second, they have limited ability to transfer
knowledge across domains with small overlaps. We propose a new MDR method named
EDDA with two key components, i.e., embedding disentangling recommender and
domain alignment, to tackle the two challenges respectively. In particular, the
embedding disentangling recommender separates both the model and embedding for
the inter-domain part and the intra-domain part, while most existing MDR
methods only focus on model-level disentangling. The domain alignment leverages
random walks from graph processing to identify similar user/item pairs from
different domains and encourages similar user/item pairs to have similar
embeddings, enhancing knowledge transfer. We compare EDDA with 12
state-of-the-art baselines on 3 real datasets. The results show that EDDA
consistently outperforms the baselines on all datasets and domains. All
datasets and codes are available at https://github.com/Stevenn9981/EDDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_W/0/1/0/all/0/1&quot;&gt;Wentao Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiwen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Reynold Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Bo Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05515">
<title>Mono-hydra: Real-time 3D scene graph construction from monocular camera input with IMU. (arXiv:2308.05515v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.05515</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability of robots to autonomously navigate through 3D environments
depends on their comprehension of spatial concepts, ranging from low-level
geometry to high-level semantics, such as objects, places, and buildings. To
enable such comprehension, 3D scene graphs have emerged as a robust tool for
representing the environment as a layered graph of concepts and their
relationships. However, building these representations using monocular vision
systems in real-time remains a difficult task that has not been explored in
depth. This paper puts forth a real-time spatial perception system Mono-Hydra,
combining a monocular camera and an IMU sensor setup, focusing on indoor
scenarios. However, the proposed approach is adaptable to outdoor applications,
offering flexibility in its potential uses. The system employs a suite of deep
learning algorithms to derive depth and semantics. It uses a robocentric
visual-inertial odometry (VIO) algorithm based on square-root information,
thereby ensuring consistent visual odometry with an IMU and a monocular camera.
This system achieves sub-20 cm error in real-time processing at 15 fps,
enabling real-time 3D scene graph construction using a laptop GPU (NVIDIA
3080). This enhances decision-making efficiency and effectiveness in simple
camera setups, augmenting robotic system agility. We make Mono-Hydra publicly
available at: https://github.com/UAV-Centre-ITC/Mono_Hydra
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udugama_U/0/1/0/all/0/1&quot;&gt;U.V.B.L. Udugama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosselman_G/0/1/0/all/0/1&quot;&gt;G. Vosselman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nex_F/0/1/0/all/0/1&quot;&gt;F. Nex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05522">
<title>Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning. (arXiv:2308.05522v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05522</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrosynthesis consists of breaking down a chemical compound recursively
step-by-step into molecular precursors until a set of commercially available
molecules is found with the goal to provide a synthesis route. Its two primary
research directions, single-step retrosynthesis prediction, which models the
chemical reaction logic, and multi-step synthesis planning, which tries to find
the correct sequence of reactions, are inherently intertwined. Still, this
connection is not reflected in contemporary research. In this work, we combine
these two major research directions by applying multiple single-step
retrosynthesis models within multi-step synthesis planning and analyzing their
impact using public and proprietary reaction data. We find a disconnection
between high single-step performance and potential route-finding success,
suggesting that single-step models must be evaluated within synthesis planning
in the future. Furthermore, we show that the commonly used single-step
retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation
task does not represent model performance and scalability on larger and more
diverse datasets. For multi-step synthesis planning, we show that the choice of
the single-step model can improve the overall success rate of synthesis
planning by up to +28% compared to the commonly used baseline model. Finally,
we show that each single-step model finds unique synthesis routes, and differs
in aspects such as route-finding success, the number of found synthesis routes,
and chemical validity, making the combination of single-step retrosynthesis
prediction and multi-step synthesis planning a crucial aspect when developing
future methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torren_Peraire_P/0/1/0/all/0/1&quot;&gt;Paula Torren-Peraire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassen_A/0/1/0/all/0/1&quot;&gt;Alan Kai Hassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genheden_S/0/1/0/all/0/1&quot;&gt;Samuel Genheden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1&quot;&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clevert_D/0/1/0/all/0/1&quot;&gt;Djork-Arne Clevert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1&quot;&gt;Mike Preuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tetko_I/0/1/0/all/0/1&quot;&gt;Igor Tetko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05547">
<title>Enhancing AUV Autonomy With Model Predictive Path Integral Control. (arXiv:2308.05547v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.05547</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous underwater vehicles (AUVs) play a crucial role in surveying marine
environments, carrying out underwater inspection tasks, and ocean exploration.
However, in order to ensure that the AUV is able to carry out its mission
successfully, a control system capable of adapting to changing environmental
conditions is required. Furthermore, to ensure the robotic platform&apos;s safe
operation, the onboard controller should be able to operate under certain
constraints. In this work, we investigate the feasibility of Model Predictive
Path Integral Control (MPPI) for the control of an AUV. We utilise a non-linear
model of the AUV to propagate the samples of the MPPI, which allow us to
compute the control action in real time. We provide a detailed evaluation of
the effect of the main hyperparameters on the performance of the MPPI
controller. Furthermore, we compared the performance of the proposed method
with a classical PID and Cascade PID approach, demonstrating the superiority of
our proposed controller. Finally, we present results where environmental
constraints are added and show how MPPI can handle them by simply incorporating
those constraints in the cost function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolay_P/0/1/0/all/0/1&quot;&gt;Pierre Nicolay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petillot_Y/0/1/0/all/0/1&quot;&gt;Yvan Petillot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marfeychuk_M/0/1/0/all/0/1&quot;&gt;Mykhaylo Marfeychuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlucho_I/0/1/0/all/0/1&quot;&gt;Ignacio Carlucho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05548">
<title>Learning (With) Distributed Optimization. (arXiv:2308.05548v1 [math.OC])</title>
<link>http://arxiv.org/abs/2308.05548</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides an overview of the historical progression of distributed
optimization techniques, tracing their development from early duality-based
methods pioneered by Dantzig, Wolfe, and Benders in the 1960s to the emergence
of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN)
algorithm. The initial focus on Lagrangian relaxation for convex problems and
decomposition strategies led to the refinement of methods like the Alternating
Direction Method of Multipliers (ADMM). The resurgence of interest in
distributed optimization in the late 2000s, particularly in machine learning
and imaging, demonstrated ADMM&apos;s practical efficacy and its unifying potential.
This overview also highlights the emergence of the proximal center method and
its applications in diverse domains. Furthermore, the paper underscores the
distinctive features of ALADIN, which offers convergence guarantees for
non-convex scenarios without introducing auxiliary variables, differentiating
it from traditional augmentation techniques. In essence, this work encapsulates
the historical trajectory of distributed optimization and underscores the
promising prospects of ALADIN in addressing non-convex optimization challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+A_A/0/1/0/all/0/1&quot;&gt;Aadharsh Aadhithya A&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+S_A/0/1/0/all/0/1&quot;&gt;Abinesh S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+J_A/0/1/0/all/0/1&quot;&gt;Akshaya J&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+M_J/0/1/0/all/0/1&quot;&gt;Jayanth M&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Radhakrishnan_V/0/1/0/all/0/1&quot;&gt;Vishnu Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+V_S/0/1/0/all/0/1&quot;&gt;Sowmya V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+P_S/0/1/0/all/0/1&quot;&gt;Soman K.P&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05563">
<title>Recent Advancements In The Field Of Deepfake Detection. (arXiv:2308.05563v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05563</link>
<description rdf:parseType="Literal">&lt;p&gt;A deepfake is a photo or video of a person whose image has been digitally
altered or partially replaced with an image of someone else. Deepfakes have the
potential to cause a variety of problems and are often used maliciously. A
common usage is altering videos of prominent political figures and celebrities.
These deepfakes can portray them making offensive, problematic, and/or untrue
statements. Current deepfakes can be very realistic, and when used in this way,
can spread panic and even influence elections and political opinions. There are
many deepfake detection strategies currently in use but finding the most
comprehensive and universal method is critical. So, in this survey we will
address the problems of malicious deepfake creation and the lack of universal
deepfake detection methods. Our objective is to survey and analyze a variety of
current methods and advances in the field of deepfake detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_N/0/1/0/all/0/1&quot;&gt;Natalie Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanamala_D/0/1/0/all/0/1&quot;&gt;Dr. Mounika Vanamala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1&quot;&gt;Dr. Rushit Dave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05567">
<title>C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT. (arXiv:2308.05567v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05567</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as ChatGPT, have demonstrated outstanding
performance in various fields, particularly in natural language understanding
and generation tasks. In complex application scenarios, users tend to engage in
multi-turn conversations with ChatGPT to keep contextual information and obtain
comprehensive responses. However, human forgetting and model contextual
forgetting remain prominent issues in multi-turn conversation scenarios, which
challenge the users&apos; conversation comprehension and contextual continuity for
ChatGPT. To address these challenges, we propose an interactive conversation
visualization system called C5, which includes Global View, Topic View, and
Context-associated Q\&amp;amp;A View. The Global View uses the GitLog diagram metaphor
to represent the conversation structure, presenting the trend of conversation
evolution and supporting the exploration of locally salient features. The Topic
View is designed to display all the question and answer nodes and their
relationships within a topic using the structure of a knowledge graph, thereby
display the relevance and evolution of conversations. The Context-associated
Q\&amp;amp;A View consists of three linked views, which allow users to explore
individual conversations deeply while providing specific contextual information
when posing questions. The usefulness and effectiveness of C5 were evaluated
through a case study and a user study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Pan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Danwei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wang Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Ronghua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guodao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05583">
<title>Generative Diffusion Models for Radio Wireless Channel Modelling and Sampling. (arXiv:2308.05583v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05583</link>
<description rdf:parseType="Literal">&lt;p&gt;Channel modelling is essential to designing modern wireless communication
systems. The increasing complexity of channel modelling and the cost of
collecting high-quality wireless channel data have become major challenges. In
this paper, we propose a diffusion model based channel sampling approach for
rapidly synthesizing channel realizations from limited data. We use a diffusion
model with a U Net based architecture operating in the frequency space domain.
To evaluate how well the proposed model reproduces the true distribution of
channels in the training dataset, two evaluation metrics are used: $i)$ the
approximate $2$-Wasserstein distance between real and generated distributions
of the normalized power spectrum in the antenna and frequency domains and $ii)$
precision and recall metric for distributions. We show that, compared to
existing GAN based approaches which suffer from mode collapse and unstable
training, our diffusion based approach trains stably and generates diverse and
high-fidelity samples from the true channel distribution. We also show that we
can pretrain the model on a simulated urban macro-cellular channel dataset and
fine-tune it on a smaller, out-of-distribution urban micro-cellular dataset,
therefore showing that it is feasible to model real world channels using
limited data with this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_U/0/1/0/all/0/1&quot;&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jao_C/0/1/0/all/0/1&quot;&gt;Chinkuo Jao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1&quot;&gt;Alberto Bernacchia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakili_S/0/1/0/all/0/1&quot;&gt;Sattar Vakili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1&quot;&gt;Da-shan Shiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05585">
<title>Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length. (arXiv:2308.05585v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05585</link>
<description rdf:parseType="Literal">&lt;p&gt;The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in
shaping the impact of large language models (LLMs), contributing significantly
to controlling output toxicity and selecting output styles, particularly as
LLMs often harbor misleading content, highlighting the urgency to align them
with human values for secure AI systems. The RLHF, characterized by complexity,
instability, and sensitivity to hyperparameters, makes the evaluation of the
reward model for complex tasks challenging, thereby further complicating the
use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple
task designed to employ Gloden as a reward model that validates the
effectiveness of PPO and inspires it, primarily explaining the task of
utilizing PPO to manipulate the tokenizer length of the output generated by the
model. Experiments confirm that PPO is not only effective in manipulating the
output tokenizer length to a certain extent in this type of task but also
exhibits facilitated training once the influence of the reward model effect is
excluded, making it an exciting development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Miao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuchang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05601">
<title>Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction. (arXiv:2308.05601v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.05601</link>
<description rdf:parseType="Literal">&lt;p&gt;Inter-city highway transportation is significant for urban life. As one of
the key functions in intelligent transportation system (ITS), traffic
evaluation always plays significant role nowadays, and daily traffic flow
prediction still faces challenges at network-wide toll stations. On the one
hand, the data imbalance in practice among various locations deteriorates the
performance of prediction. On the other hand, complex correlative
spatio-temporal factors cannot be comprehensively employed in long-term
duration. In this paper, a prediction method is proposed for daily traffic flow
in highway domain through spatio-temporal deep learning. In our method, data
normalization strategy is used to deal with data imbalance, due to long-tail
distribution of traffic flow at network-wide toll stations. And then, based on
graph convolutional network, we construct networks in distinct semantics to
capture spatio-temporal features. Beside that, meteorology and calendar
features are used by our model in the full connection stage to extra external
characteristics of traffic flow. By extensive experiments and case studies in
one Chinese provincial highway, our method shows clear improvement in
predictive accuracy than baselines and practical benefits in business.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Weilong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianpu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianwu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhuofeng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05612">
<title>A Smart Robotic System for Industrial Plant Supervision. (arXiv:2308.05612v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.05612</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s chemical production plants, human field operators perform frequent
checks on the plant&apos;s integrity to guarantee high safety standards, and thus
are possibly the first to encounter dangerous operating conditions. To
alleviate their tasks of failure detection and monitoring by audio, visual, and
olfactory perceptions, we present a robotic system that consists of an
autonomously navigating robot integrated with various sensors and data
processing. We aim to resemble the human sensing and interpretation
capabilities of sight, smell, and hearing, for providing automated inspection.
We evaluate our system extensively at a wastewater facility in full working
conditions. Our results demonstrate that the system is able to robustly
navigate a plant and to provide useful information about critical operating
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Rosal_D/0/1/0/all/0/1&quot;&gt;D. Adriana G&amp;#xf3;mez-Rosal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergau_M/0/1/0/all/0/1&quot;&gt;Max Bergau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_G/0/1/0/all/0/1&quot;&gt;Georg K.J. Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachaja_A/0/1/0/all/0/1&quot;&gt;Andreas Wachaja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grater_J/0/1/0/all/0/1&quot;&gt;Johannes Gr&amp;#xe4;ter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odenweller_M/0/1/0/all/0/1&quot;&gt;Matthias Odenweller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piechottka_U/0/1/0/all/0/1&quot;&gt;Uwe Piechottka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoeflinger_F/0/1/0/all/0/1&quot;&gt;Fabian Hoeflinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1&quot;&gt;Nikhil Gosala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzel_N/0/1/0/all/0/1&quot;&gt;Niklas Wetzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1&quot;&gt;Daniel B&amp;#xfc;scher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1&quot;&gt;Wolfram Burgard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05617">
<title>A Neural Network Based Choice Model for Assortment Optimization. (arXiv:2308.05617v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05617</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrete-choice models are used in economics, marketing and revenue
management to predict customer purchase probabilities, say as a function of
prices and other features of the offered assortment. While they have been shown
to be expressive, capturing customer heterogeneity and behaviour, they are also
hard to estimate, often based on many unobservables like utilities; and
moreover, they still fail to capture many salient features of customer
behaviour. A natural question then, given their success in other contexts, is
if neural networks can eliminate the necessity of carefully building a
context-dependent customer behaviour model and hand-coding and tuning the
estimation. It is unclear however how one would incorporate assortment effects
into such a neural network, and also how one would optimize the assortment with
such a black-box generative model of choice probabilities. In this paper we
investigate first whether a single neural network architecture can predict
purchase probabilities for datasets from various contexts and generated under
various models and assumptions. Next, we develop an assortment optimization
formulation that is solvable by off-the-shelf integer programming solvers. We
compare against a variety of benchmark discrete-choice models on simulated as
well as real-world datasets, developing training tricks along the way to make
the neural network prediction and subsequent optimization robust and comparable
in performance to the alternates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanzhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongze Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talluri_K/0/1/0/all/0/1&quot;&gt;Kalyan Talluri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05619">
<title>Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.05619</link>
<description rdf:parseType="Literal">&lt;p&gt;As data shift or new data become available, updating clinical machine
learning models may be necessary to maintain or improve performance over time.
However, updating a model can introduce compatibility issues when the behavior
of the updated model does not align with user expectations, resulting in poor
user-model team performance. Existing compatibility measures depend on model
decision thresholds, limiting their applicability in settings where models are
used to generate rankings based on estimated risk. To address this limitation,
we propose a novel rank-based compatibility measure, $C^R$, and a new loss
function that aims to optimize discriminative performance while encouraging
good compatibility. Applied to a case study in mortality risk stratification
leveraging data from MIMIC, our approach yields more compatible models while
maintaining discriminative performance compared to existing model selection
techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval:
$0.005$, $0.035$). This work provides new tools to analyze and update risk
stratification models used in clinical care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Otles_E/0/1/0/all/0/1&quot;&gt;Erkin &amp;#xd6;tle&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Denton_B/0/1/0/all/0/1&quot;&gt;Brian T. Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05658">
<title>Automatic Extraction of Relevant Road Infrastructure using Connected vehicle data and Deep Learning Model. (arXiv:2308.05658v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05658</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s rapidly evolving urban landscapes, efficient and accurate mapping
of road infrastructure is critical for optimizing transportation systems,
enhancing road safety, and improving the overall mobility experience for
drivers and commuters. Yet, a formidable bottleneck obstructs progress - the
laborious and time-intensive manual identification of intersections. Simply
considering the shear number of intersections that need to be identified, and
the labor hours required per intersection, the need for an automated solution
becomes undeniable. To address this challenge, we propose a novel approach that
leverages connected vehicle data and cutting-edge deep learning techniques. By
employing geohashing to segment vehicle trajectories and then generating image
representations of road segments, we utilize the YOLOv5 (You Only Look Once
version 5) algorithm for accurate classification of both straight road segments
and intersections. Experimental results demonstrate an impressive overall
classification accuracy of 95%, with straight roads achieving a remarkable 97%
F1 score and intersections reaching a 90% F1 score. This approach not only
saves time and resources but also enables more frequent updates and a
comprehensive understanding of the road network. Our research showcases the
potential impact on traffic management, urban planning, and autonomous vehicle
navigation systems. The fusion of connected vehicle data and deep learning
models holds promise for a transformative shift in road infrastructure mapping,
propelling us towards a smarter, safer, and more connected transportation
ecosystem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojo_A/0/1/0/all/0/1&quot;&gt;Adu-Gyamfi Kojo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghupathi_K/0/1/0/all/0/1&quot;&gt;Kandiboina Raghupathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varsha_R/0/1/0/all/0/1&quot;&gt;Ravichandra-Mouli Varsha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skylar_K/0/1/0/all/0/1&quot;&gt;Knickerbocker Skylar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+N_H/0/1/0/all/0/1&quot;&gt;Hans Zachary N&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawkins/0/1/0/all/0/1&quot;&gt;Hawkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_N/0/1/0/all/0/1&quot;&gt;Neal R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anuj_S/0/1/0/all/0/1&quot;&gt;Sharma Anuj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05665">
<title>Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data. (arXiv:2308.05665v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05665</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern transportation planning relies heavily on accurate predictions of
person and vehicle trips. However, traditional planning models often fail to
account for the intricacies and dynamics of travel behavior, leading to
less-than-optimal accuracy in these predictions. This study explores the
potential of deep learning techniques to transform the way we approach trip
predictions, and ultimately, transportation planning. Utilizing a comprehensive
dataset from the National Household Travel Survey (NHTS), we developed and
trained a deep learning model for predicting person and vehicle trips. The
proposed model leverages the vast amount of information in the NHTS data,
capturing complex, non-linear relationships that were previously overlooked by
traditional models. As a result, our deep learning model achieved an impressive
accuracy of 98% for person trip prediction and 96% for vehicle trip estimation.
This represents a significant improvement over the performances of traditional
transportation planning models, thereby demonstrating the power of deep
learning in this domain. The implications of this study extend beyond just more
accurate predictions. By enhancing the accuracy and reliability of trip
prediction models, planners can formulate more effective, data-driven
transportation policies, infrastructure, and services. As such, our research
underscores the need for the transportation planning field to embrace advanced
techniques like deep learning. The detailed methodology, along with a thorough
discussion of the results and their implications, are presented in the
subsequent sections of this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adu_Gyamfi_K/0/1/0/all/0/1&quot;&gt;Kojo Adu-Gyamfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anuj_S/0/1/0/all/0/1&quot;&gt;Sharma Anuj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05681">
<title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05681</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, methods for skeleton-based human activity recognition have been
shown to be vulnerable to adversarial attacks. However, these attack methods
require either the full knowledge of the victim (i.e. white-box attacks),
access to training data (i.e. transfer-based attacks) or frequent model queries
(i.e. black-box attacks). All their requirements are highly restrictive,
raising the question of how detrimental the vulnerability is. In this paper, we
show that the vulnerability indeed exists. To this end, we consider a new
attack task: the attacker has no access to the victim model or the training
data or labels, where we coin the term hard no-box attack. Specifically, we
first learn a motion manifold where we define an adversarial loss to compute a
new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our
gradient contains information of the motion dynamics, which is different from
existing gradient-based attack methods that compute the loss gradient assuming
each dimension in the data is independent. The SMI gradient can augment many
gradient-based attack methods, leading to a new family of no-box attack
methods. Extensive evaluation and comparison show that our method imposes a
real threat to existing classifiers. They also show that the SMI gradient
improves the transferability and imperceptibility of adversarial samples in
both no-box and transfer-based black-box settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhengzhi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guoan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P. H. Shum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05697">
<title>SSLRec: A Self-Supervised Learning Library for Recommendation. (arXiv:2308.05697v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has gained significant interest in recent
years as a solution to address the challenges posed by sparse and noisy data in
recommender systems. Despite the growing number of SSL algorithms designed to
provide state-of-the-art performance in various recommendation scenarios (e.g.,
graph collaborative filtering, sequential recommendation, social
recommendation, KG-enhanced recommendation), there is still a lack of unified
frameworks that integrate recommendation algorithms across different domains.
Such a framework could serve as the cornerstone for self-supervised
recommendation algorithms, unifying the validation of existing methods and
driving the design of new ones. To address this gap, we introduce SSLRec, a
novel benchmark platform that provides a standardized, flexible, and
comprehensive framework for evaluating various SSL-enhanced recommenders. The
SSLRec library features a modular architecture that allows users to easily
evaluate state-of-the-art models and a complete set of data augmentation and
self-supervised toolkits to help create SSL recommendation models with specific
needs. Furthermore, SSLRec simplifies the process of training and evaluating
different recommendation models with consistent and fair settings. Our SSLRec
platform covers a comprehensive set of state-of-the-art SSL-enhanced
recommendation models across different scenarios, enabling researchers to
evaluate these cutting-edge models and drive further innovation in the field.
Our implemented SSLRec framework is available at the source code repository
https://github.com/HKUDS/SSLRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xubin Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianle Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xuheng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05701">
<title>Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving. (arXiv:2308.05701v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05701</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years there have been remarkable advancements in autonomous
driving. While autonomous vehicles demonstrate high performance in closed-set
conditions, they encounter difficulties when confronted with unexpected
situations. At the same time, world models emerged in the field of model-based
reinforcement learning as a way to enable agents to predict the future
depending on potential actions. This led to outstanding results in sparse
reward and complex control tasks. This work provides an overview of how world
models can be leveraged to perform anomaly detection in the domain of
autonomous driving. We provide a characterization of world models and relate
individual components to previous works in anomaly detection to facilitate
further research in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1&quot;&gt;Daniel Bogdoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosch_L/0/1/0/all/0/1&quot;&gt;Lukas Bosch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_T/0/1/0/all/0/1&quot;&gt;Tim Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gremmelmaier_H/0/1/0/all/0/1&quot;&gt;Helen Gremmelmaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yitian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1&quot;&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05713">
<title>Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems. (arXiv:2308.05713v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.05713</link>
<description rdf:parseType="Literal">&lt;p&gt;This report describes a test of the large language model GPT-4 with the
Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in
science and math, at the high school and college levels, carried out in
June-August 2023. Our tests suggest that the plug-ins significantly enhance
GPT&apos;s ability to solve these problems. Having said that, there are still often
&quot;interface&quot; failures; that is, GPT often has trouble formulating problems in a
way that elicits useful answers from the plug-ins. Fixing these interface
failures seems like a central challenge in making GPT a reliable tool for
college-level calculation problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1&quot;&gt;Ernest Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aaronson_S/0/1/0/all/0/1&quot;&gt;Scott Aaronson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05731">
<title>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.05731</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated driving has the potential to revolutionize personal, public, and
freight mobility. Besides the enormous challenge of perception, i.e. accurately
perceiving the environment using available sensor data, automated driving
comprises planning a safe, comfortable, and efficient motion trajectory. To
promote safety and progress, many works rely on modules that predict the future
motion of surrounding traffic. Modular automated driving systems commonly
handle prediction and planning as sequential separate tasks. While this
accounts for the influence of surrounding traffic on the ego-vehicle, it fails
to anticipate the reactions of traffic participants to the ego-vehicle&apos;s
behavior. Recent works suggest that integrating prediction and planning in an
interdependent joint step is necessary to achieve safe, efficient, and
comfortable driving. While various models implement such integrated systems, a
comprehensive overview and theoretical understanding of different principles
are lacking. We systematically review state-of-the-art deep learning-based
prediction, planning, and integrated prediction and planning models. Different
facets of the integration ranging from model architecture and model design to
behavioral aspects are considered and related to each other. Moreover, we
discuss the implications, strengths, and limitations of different integration
methods. By pointing out research gaps, describing relevant future challenges,
and highlighting trends in the research field, we identify promising directions
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagedorn_S/0/1/0/all/0/1&quot;&gt;Steffen Hagedorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallgarten_M/0/1/0/all/0/1&quot;&gt;Marcel Hallgarten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1&quot;&gt;Martin Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1&quot;&gt;Alexandru Condurache&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05732">
<title>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. (arXiv:2308.05732v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.05732</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-dependent partial differential equations (PDEs) are ubiquitous in
science and engineering. Recently, mostly due to the high computational cost of
traditional solution techniques, deep neural network based surrogates have
gained increased interest. The practical utility of such neural PDE solvers
relies on their ability to provide accurate, stable predictions over long time
horizons, which is a notoriously hard problem. In this work, we present a
large-scale analysis of common temporal rollout strategies, identifying the
neglect of non-dominant spatial frequency information, often associated with
high frequencies in PDE solutions, as the primary pitfall limiting stable,
accurate rollout performance. Based on these insights, we draw inspiration from
recent advances in diffusion models to introduce PDE-Refiner; a novel model
class that enables more accurate modeling of all frequency components via a
multistep refinement process. We validate PDE-Refiner on challenging benchmarks
of complex fluid dynamics, demonstrating stable and accurate rollouts that
consistently outperform state-of-the-art models, including neural, numerical,
and hybrid neural-numerical architectures. We further demonstrate that
PDE-Refiner greatly enhances data efficiency, since the denoising objective
implicitly induces a novel form of spectral data augmentation. Finally,
PDE-Refiner&apos;s connection to diffusion models enables an accurate and efficient
assessment of the model&apos;s predictive uncertainty, allowing us to estimate when
the surrogate becomes inaccurate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1&quot;&gt;Phillip Lippe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1&quot;&gt;Bastiaan S. Veeling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1&quot;&gt;Paris Perdikaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1&quot;&gt;Johannes Brandstetter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05734">
<title>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.05734</link>
<description rdf:parseType="Literal">&lt;p&gt;Although audio generation shares commonalities across different types of
audio, such as speech, music, and sound effects, designing models for each type
requires careful consideration of specific objectives and biases that can
significantly differ from those of other types. To bring us closer to a unified
perspective of audio generation, this paper proposes a framework that utilizes
the same learning method for speech, music, and sound effect generation. Our
framework introduces a general representation of audio, called language of
audio (LOA). Any audio can be translated into LOA based on AudioMAE, a
self-supervised pre-trained representation learning model. In the generation
process, we translate any modalities into LOA by using a GPT-2 model, and we
perform self-supervised audio generation learning with a latent diffusion model
conditioned on LOA. The proposed framework naturally brings advantages such as
in-context learning abilities and reusable self-supervised pretrained AudioMAE
and latent diffusion models. Experiments on the major benchmarks of
text-to-audio, text-to-music, and text-to-speech demonstrate new
state-of-the-art or competitive performance to previous approaches. Our demo
and code are available at https://audioldm.github.io/audioldm2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haohe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qiao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xubo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_X/0/1/0/all/0/1&quot;&gt;Xinhao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qiuqiang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenwu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1&quot;&gt;Mark D. Plumbley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05741">
<title>Neural Progressive Meshes. (arXiv:2308.05741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05741</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent proliferation of 3D content that can be consumed on hand-held
devices necessitates efficient tools for transmitting large geometric data,
e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a
challenge to storage as well as transmission bandwidth, and level-of-detail
techniques are often used to transmit an asset using an appropriate bandwidth
budget. It is especially desirable for these methods to transmit data
progressively, improving the quality of the geometry with more data. Our key
insight is that the geometric details of 3D meshes often exhibit similar local
patterns even across different shapes, and thus can be effectively represented
with a shared learned generative space. We learn this space using a
subdivision-based encoder-decoder architecture trained in advance on a large
collection of surfaces. We further observe that additional residual features
can be transmitted progressively between intermediate levels of subdivision
that enable the client to control the tradeoff between bandwidth cost and
quality of reconstruction, providing a neural progressive mesh representation.
We evaluate our method on a diverse set of complex 3D shapes and demonstrate
that it outperforms baselines in terms of compression ratio and reconstruction
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1&quot;&gt;Vladimir G. Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1&quot;&gt;Noam Aigerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1&quot;&gt;Alec Jacobson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.13341">
<title>Overlooked Implications of the Reconstruction Loss for VAE Disentanglement. (arXiv:2202.13341v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.13341</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning disentangled representations with variational autoencoders (VAEs) is
often attributed to the regularisation component of the loss. In this work, we
highlight the interaction between data and the reconstruction term of the loss
as the main contributor to disentanglement in VAEs. We show that standard
benchmark datasets have unintended correlations between their subjective
ground-truth factors and perceived axes in the data according to typical VAE
reconstruction losses. Our work exploits this relationship to provide a theory
for what constitutes an adversarial dataset under a given reconstruction loss.
We verify this by constructing an example dataset that prevents disentanglement
in state-of-the-art frameworks while maintaining human-intuitive ground-truth
factors. Finally, we re-enable disentanglement by designing an example
reconstruction loss that is once again able to perceive the ground-truth
factors. Our findings demonstrate the subjective nature of disentanglement and
the importance of considering the interaction between the ground-truth factors,
data and notably, the reconstruction loss, which is under-recognised in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michlo_N/0/1/0/all/0/1&quot;&gt;Nathan Michlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1&quot;&gt;Richard Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Steven James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04278">
<title>Deep learning-based Crop Row Detection for Infield Navigation of Agri-Robots. (arXiv:2209.04278v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04278</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous navigation in agricultural environments is challenged by varying
field conditions that arise in arable fields. State-of-the-art solutions for
autonomous navigation in such environments require expensive hardware such as
RTK-GNSS. This paper presents a robust crop row detection algorithm that
withstands such field variations using inexpensive cameras. Existing datasets
for crop row detection does not represent all the possible field variations. A
dataset of sugar beet images was created representing 11 field variations
comprised of multiple grow stages, light levels, varying weed densities, curved
crop rows and discontinuous crop rows. The proposed pipeline segments the crop
rows using a deep learning-based method and employs the predicted segmentation
mask for extraction of the central crop using a novel central crop row
selection algorithm. The novel crop row detection algorithm was tested for crop
row detection performance and the capability of visual servoing along a crop
row. The visual servoing-based navigation was tested on a realistic simulation
scenario with the real ground and plant textures. Our algorithm demonstrated
robust vision-based crop row detection in challenging field conditions
outperforming the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Rajitha de Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1&quot;&gt;Grzegorz Cielniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10822">
<title>RobustPdM: Designing Robust Predictive Maintenance against Adversarial Attacks. (arXiv:2301.10822v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10822</link>
<description rdf:parseType="Literal">&lt;p&gt;The state-of-the-art predictive maintenance (PdM) techniques have shown great
success in reducing maintenance costs and downtime of complicated machines
while increasing overall productivity through extensive utilization of
Internet-of-Things (IoT) and Deep Learning (DL). Unfortunately, IoT sensors and
DL algorithms are both prone to cyber-attacks. For instance, DL algorithms are
known for their susceptibility to adversarial examples. Such adversarial
attacks are vastly under-explored in the PdM domain. This is because the
adversarial attacks in the computer vision domain for classification tasks
cannot be directly applied to the PdM domain for multivariate time series (MTS)
regression tasks. In this work, we propose an end-to-end methodology to design
adversarially robust PdM systems by extensively analyzing the effect of
different types of adversarial attacks and proposing a novel adversarial
defense technique for DL-enabled PdM models. First, we propose novel MTS
Projected Gradient Descent (PGD) and MTS PGD with random restarts (PGD_r)
attacks. Then, we evaluate the impact of MTS PGD and PGD_r along with MTS Fast
Gradient Sign Method (FGSM) and MTS Basic Iterative Method (BIM) on Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Convolutional Neural
Network (CNN), and Bi-directional LSTM based PdM system. Our results using
NASA&apos;s turbofan engine dataset show that adversarial attacks can cause a severe
defect (up to 11X) in the RUL prediction, outperforming the effectiveness of
the state-of-the-art PdM attacks by 3X. Furthermore, we present a novel
approximate adversarial training method to defend against adversarial attacks.
We observe that approximate adversarial training can significantly improve the
robustness of PdM models (up to 54X) and outperforms the state-of-the-art PdM
defense methods by offering 3X more robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1&quot;&gt;Ayesha Siddique&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_R/0/1/0/all/0/1&quot;&gt;Ripan Kumar Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mode_G/0/1/0/all/0/1&quot;&gt;Gautam Raj Mode&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoque_K/0/1/0/all/0/1&quot;&gt;Khaza Anuarul Hoque&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13817">
<title>Let&apos;s have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13817</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of an AI-powered chatbot that can generate human-like sentences
and write coherent essays has caught the world&apos;s attention. This paper
discusses the historical overview of chatbots and the technology behind Chat
Generative Pre-trained Transformer, better known as ChatGPT. Moreover,
potential applications of ChatGPT in various domains, including healthcare,
education, and research, are highlighted. Despite promising results, there are
several privacy and ethical concerns surrounding ChatGPT. In addition, we
highlight some of the important limitations of the current version of ChatGPT.
We also ask ChatGPT to provide its point of view and present its responses to
several questions we attempt to answer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1&quot;&gt;Sakib Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayawi_K/0/1/0/all/0/1&quot;&gt;Kadhim Hayawi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14961">
<title>Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14961</link>
<description rdf:parseType="Literal">&lt;p&gt;As the use of machine learning continues to expand, the importance of
ensuring its safety cannot be overstated. A key concern in this regard is the
ability to identify whether a given sample is from the training distribution,
or is an &quot;Out-Of-Distribution&quot; (OOD) sample. In addition, adversaries can
manipulate OOD samples in ways that lead a classifier to make a confident
prediction. In this study, we present a novel approach for certifying the
robustness of OOD detection within a $\ell_2$-norm around the input, regardless
of network architecture and without the need for specific components or
additional training. Further, we improve current techniques for detecting
adversarial attacks on OOD samples, while providing high levels of certified
and adversarial robustness on in-distribution samples. The average of all OOD
detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$
relative to previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_N/0/1/0/all/0/1&quot;&gt;Nicola Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korth_D/0/1/0/all/0/1&quot;&gt;Daniel Korth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1&quot;&gt;Jeanette Miriam Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roscher_K/0/1/0/all/0/1&quot;&gt;Karsten Roscher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guennemann_S/0/1/0/all/0/1&quot;&gt;Stephan Guennemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05874">
<title>Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer&apos;s Disease using EEG Data. (arXiv:2304.05874v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05874</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural network (GNN) models are increasingly being used for the
classification of electroencephalography (EEG) data. However, GNN-based
diagnosis of neurological disorders, such as Alzheimer&apos;s disease (AD), remains
a relatively unexplored area of research. Previous studies have relied on
functional connectivity methods to infer brain graph structures and used simple
GNN architectures for the diagnosis of AD. In this work, we propose a novel
adaptive gated graph convolutional network (AGGCN) that can provide explainable
predictions. AGGCN adaptively learns graph structures by combining
convolution-based node feature enhancement with a well-known correlation-based
measure of functional connectivity. Furthermore, the gated graph convolution
can dynamically weigh the contribution of various spatial scales. The proposed
model achieves high accuracy in both eyes-closed and eyes-open conditions,
indicating the stability of learned representations. Finally, we demonstrate
that the proposed AGGCN model generates consistent explanations of its
predictions that might be relevant for further study of AD-related alterations
of brain networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Klepl_D/0/1/0/all/0/1&quot;&gt;Dominik Klepl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Blackburn_D/0/1/0/all/0/1&quot;&gt;Daniel J. Blackburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sarrigiannis_P/0/1/0/all/0/1&quot;&gt;Ptolemaios G. Sarrigiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03216">
<title>Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution. (arXiv:2305.03216v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03216</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural network-based simulation super-resolution framework that
can efficiently and realistically enhance a facial performance produced by a
low-cost, realtime physics-based simulation to a level of detail that closely
approximates that of a reference-quality off-line simulator with much higher
resolution (26x element count in our examples) and accurate physical modeling.
Our approach is rooted in our ability to construct - via simulation - a
training set of paired frames, from the low- and high-resolution simulators
respectively, that are in semantic correspondence with each other. We use face
animation as an exemplar of such a simulation domain, where creating this
semantic congruence is achieved by simply dialing in the same muscle actuation
controls and skeletal pose in the two simulators. Our proposed neural network
super-resolution framework generalizes from this training set to unseen
expressions, compensates for modeling discrepancies between the two simulations
due to limited resolution or cost-cutting approximations in the real-time
variant, and does not require any semantic descriptors or parameters to be
provided as input, other than the result of the real-time simulation. We
evaluate the efficacy of our pipeline on a variety of expressive performances
and provide comparisons and ablation experiments for plausible variations and
alternatives to our proposed scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyojoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sangeetha Grama Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_M/0/1/0/all/0/1&quot;&gt;Matthew Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Doyub Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Byungsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1&quot;&gt;Jonathan Swartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Museth_K/0/1/0/all/0/1&quot;&gt;Ken Museth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1&quot;&gt;Eftychios Sifakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07304">
<title>CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07304</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in visual-language models have shown remarkable zero-shot
text-image matching ability that is transferable to downstream tasks such as
object detection and segmentation. Adapting these models for object counting,
however, remains a formidable challenge. In this study, we first investigate
transferring vision-language models (VLMs) for class-agnostic object counting.
Specifically, we propose CLIP-Count, the first end-to-end pipeline that
estimates density maps for open-vocabulary objects with text guidance in a
zero-shot manner. To align the text embedding with dense visual features, we
introduce a patch-text contrastive loss that guides the model to learn
informative patch-level visual representations for dense prediction. Moreover,
we design a hierarchical patch-text interaction module to propagate semantic
information across different resolution levels of visual features. Benefiting
from the full exploitation of the rich image-text alignment knowledge of
pretrained VLMs, our method effectively generates high-quality density maps for
objects-of-interest. Extensive experiments on FSC-147, CARPK, and ShanghaiTech
crowd counting datasets demonstrate state-of-the-art accuracy and
generalizability of the proposed method. Code is available:
https://github.com/songrise/CLIP-Count.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruixiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changwen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09378">
<title>Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09378</link>
<description rdf:parseType="Literal">&lt;p&gt;This research project investigates Lenia, an artificial life platform that
simulates ecosystems of digital creatures. Lenia&apos;s ecosystem consists of
simple, artificial organisms that can move, consume, grow, and reproduce. The
platform is important as a tool for studying artificial life and evolution, as
it provides a scalable and flexible environment for creating a diverse range of
organisms with varying abilities and behaviors. Measuring complexity in Lenia
is a key aspect of the study, which identifies the metrics for measuring
long-term complex emerging behavior of rules, with the aim of evolving better
Lenia behaviors which are yet not discovered. The Genetic Algorithm uses
neighborhoods or kernels as genotype while keeping the rest of the parameters
of Lenia as fixed, for example growth function, to produce different behaviors
respective to the population and then measures fitness value to decide the
complexity of the resulting behavior. First, we use Variation over Time as a
fitness function where higher variance between the frames are rewarded. Second,
we use Auto-encoder based fitness where variation of the list of reconstruction
loss for the frames is rewarded. Third, we perform combined fitness where
higher variation of the pixel density of reconstructed frames is rewarded. All
three experiments are tweaked with pixel alive threshold and frames used.
Finally, after performing nine experiments of each fitness for 500 generations,
we pick configurations from all experiments such that there is a scope of
further evolution, and run it for 2500 generations. Results show that the
kernel&apos;s center of mass increases with a specific set of pixels and together
with borders the kernel try to achieve a Gaussian distribution. Results are
available at https://s4nyam.github.io/evolenia/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanyam Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_A/0/1/0/all/0/1&quot;&gt;Aarati Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichele_S/0/1/0/all/0/1&quot;&gt;Stefano Nichele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16021">
<title>Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16021</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep
Neural Networks (DNNs) for function approximation, has demonstrated
considerable success in numerous applications. However, its practicality in
addressing various real-world scenarios, characterized by diverse and
unpredictable dynamics, noisy signals, and large state and action spaces,
remains limited. This limitation stems from issues such as poor data
efficiency, limited generalization capabilities, a lack of safety guarantees,
and the absence of interpretability, among other factors. To overcome these
challenges and improve performance across these crucial metrics, one promising
avenue is to incorporate additional structural information about the problem
into the RL learning process. Various sub-fields of RL have proposed methods
for incorporating such inductive biases. We amalgamate these diverse
methodologies under a unified framework, shedding light on the role of
structure in the learning problem, and classify these methods into distinct
patterns of incorporating structure. By leveraging this comprehensive
framework, we provide valuable insights into the challenges of structured RL
and lay the groundwork for a design pattern perspective on RL research. This
novel perspective paves the way for future advancements and aids in developing
more effective and efficient RL algorithms that can potentially handle
real-world scenarios better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1&quot;&gt;Aditya Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1&quot;&gt;Marius Lindauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07944">
<title>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (DA) with the aid of pseudo labeling
techniques has emerged as a crucial approach for domain-adaptive 3D object
detection. While effective, existing DA methods suffer from a substantial drop
in performance when applied to a multi-class training setting, due to the
co-existence of low-quality pseudo labels and class imbalance issues. In this
paper, we address this challenge by proposing a novel ReDB framework tailored
for learning to detect all classes at once. Our approach produces Reliable,
Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the
self-training on a distributionally different target domain. To alleviate
disruptions caused by the environmental discrepancy (e.g., beam numbers), the
proposed cross-domain examination (CDE) assesses the correctness of pseudo
labels by copy-pasting target instances into a source environment and measuring
the prediction consistency. To reduce computational overhead and mitigate the
object shift (e.g., scales and point densities), we design an overlapped boxes
counting (OBC) metric that allows to uniformly downsample pseudo-labeled
objects across different geometric characteristics. To confront the issue of
inter-class imbalance, we progressively augment the target point clouds with a
class-balanced set of pseudo-labeled target instances and source objects, which
boosts recognition accuracies on both frequently appearing and rare classes.
Experimental results on three benchmark datasets using both voxel-based (i.e.,
SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our
proposed ReDB approach outperforms existing 3D domain adaptation methods by a
large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.
The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14527">
<title>Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14527</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper details the challenges in applying two computer vision systems, an
EfficientDET supervised learning model and the unsupervised RX spectral
classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and
rescue (WSAR) effort in Japan and identifies 3 directions for future research.
There have been at least 19 proposed approaches and 3 datasets aimed at
locating missing persons in drone imagery, but only 3 approaches (2
unsupervised and 1 of an unknown structure) are referenced in the literature as
having been used in an actual WSAR operation. Of these proposed approaches, the
EfficientDET architecture and the unsupervised spectral RX classifier were
selected as the most appropriate for this setting. The EfficientDET model was
applied to the HERIDAL dataset and despite achieving performance that is
statistically equivalent to the state-of-the-art, the model fails to translate
to the real world in terms of false positives (e.g., identifying tree limbs and
rocks as people), and false negatives (e.g., failing to identify members of the
search team). The poor results in practice for algorithms that showed good
results on datasets suggest 3 areas of future research: more realistic datasets
for wilderness SAR, computer vision models that are capable of seamlessly
handling the variety of imagery that can be collected during actual WSAR
operations, and better alignment on performance measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1&quot;&gt;Thomas Manzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1&quot;&gt;Robin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15644">
<title>Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15644</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research in language-guided visual navigation has demonstrated a
significant demand for the diversity of traversable environments and the
quantity of supervision for training generalizable agents. To tackle the common
data scarcity issue in existing vision-and-language navigation datasets, we
propose an effective paradigm for generating large-scale data for learning,
which applies 1200+ photo-realistic environments from HM3D and Gibson datasets
and synthesizes 4.9 million instruction trajectory pairs using fully-accessible
resources on the web. Importantly, we investigate the influence of each
component in this paradigm on the agent&apos;s performance and study how to
adequately apply the augmented data to pre-train and fine-tune an agent. Thanks
to our large-scale dataset, the performance of an existing agent can be pushed
up (+11% absolute with regard to previous SoTA) to a significantly new best of
80% single-run success rate on the R2R test split by simple imitation learning.
The long-lasting generalization gap between navigating in seen and unseen
environments is also reduced to less than 1% (versus 8% in the previous best
method). Moreover, our paradigm also facilitates different models to achieve
new state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuous
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02312">
<title>Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. (arXiv:2308.02312v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, Q&amp;amp;A platforms have played a crucial role in how
programmers seek help online. The emergence of ChatGPT, however, is causing a
shift in this pattern. Despite ChatGPT&apos;s popularity, there hasn&apos;t been a
thorough investigation into the quality and usability of its responses to
software engineering queries. To address this gap, we undertook a comprehensive
analysis of ChatGPT&apos;s replies to 517 questions from Stack Overflow (SO). We
assessed the correctness, consistency, comprehensiveness, and conciseness of
these responses. Additionally, we conducted an extensive linguistic analysis
and a user study to gain insights into the linguistic and human aspects of
ChatGPT&apos;s answers. Our examination revealed that 52% of ChatGPT&apos;s answers
contain inaccuracies and 77% are verbose. Nevertheless, users still prefer
ChatGPT&apos;s responses 39.34% of the time due to their comprehensiveness and
articulate language style. These findings underscore the need for meticulous
error correction in ChatGPT while also raising awareness among users about the
potential risks associated with seemingly accurate answers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_S/0/1/0/all/0/1&quot;&gt;Samia Kabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udo_Imeh_D/0/1/0/all/0/1&quot;&gt;David N. Udo-Imeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_B/0/1/0/all/0/1&quot;&gt;Bonan Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03152">
<title>AI-GOMS: Large AI-Driven Global Ocean Modeling System. (arXiv:2308.03152v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03152</link>
<description rdf:parseType="Literal">&lt;p&gt;Ocean modeling is a powerful tool for simulating the physical, chemical, and
biological processes of the ocean, which is the foundation for marine science
research and operational oceanography. Modern numerical ocean modeling mainly
consists of governing equations and numerical algorithms. Nonlinear
instability, computational expense, low reusability efficiency and high
coupling costs have gradually become the main bottlenecks for the further
development of numerical ocean modeling. Recently, artificial
intelligence-based modeling in scientific computing has shown revolutionary
potential for digital twins and scientific simulations, but the bottlenecks of
numerical ocean modeling have not been further solved. Here, we present
AI-GOMS, a large AI-driven global ocean modeling system, for accurate and
efficient global ocean daily prediction. AI-GOMS consists of a backbone model
with the Fourier-based Masked Autoencoder structure for basic ocean variable
prediction and lightweight fine-tuning models incorporating regional
downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has
achieved the best performance in 30 days of prediction for the global ocean
basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond
the good performance in statistical metrics, AI-GOMS realizes the simulation of
mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and
ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new
backbone-downstream paradigm for Earth system modeling, which makes the system
transferable, scalable and reusable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yanfei Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuze Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Muyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03880">
<title>Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse. (arXiv:2308.03880v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03880</link>
<description rdf:parseType="Literal">&lt;p&gt;Online violence against children has increased globally recently, demanding
urgent attention. Competent authorities manually analyze abuse complaints to
comprehend crime dynamics and identify patterns. However, the manual analysis
of these complaints presents a challenge because it exposes analysts to harmful
content during the review process. Given these challenges, we present a novel
solution, an automated tool designed to analyze children&apos;s sexual abuse reports
comprehensively. By automating the analysis process, our tool significantly
reduces the risk of exposure to harmful content by categorizing the reports on
three dimensions: Subject, Degree of Criminality, and Damage. Furthermore,
leveraging our multidisciplinary team&apos;s expertise, we introduce a novel
approach to annotate the collected data, enabling a more in-depth analysis of
the reports. This approach improves the comprehension of fundamental patterns
and trends, enabling law enforcement agencies and policymakers to create
focused strategies in the fight against children&apos;s violence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puentes_J/0/1/0/all/0/1&quot;&gt;Juanita Puentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1&quot;&gt;Angela Castillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osejo_W/0/1/0/all/0/1&quot;&gt;Wilmar Osejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderon_Y/0/1/0/all/0/1&quot;&gt;Yuly Calder&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quintero_V/0/1/0/all/0/1&quot;&gt;Viviana Quintero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saldarriaga_L/0/1/0/all/0/1&quot;&gt;Lina Saldarriaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agudelo_D/0/1/0/all/0/1&quot;&gt;Diana Agudelo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04033">
<title>Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications. (arXiv:2308.04033v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04033</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing approaches to understanding, developing and researching modern
wireless communication technologies involves time-intensive and arduous process
of sifting through numerous webpages and technical specification documents,
gathering the required information and synthesizing it. This paper presents
NextGen Communications Copilot, a conversational artificial intelligence tool
for information synthesis of wireless communication specifications. The system
builds on top of recent advancements in foundation models and consists of three
key additional components: a domain-specific database, a context extractor, and
a feedback mechanism. The system appends user queries with concise and
query-dependent contextual information extracted from a database of wireless
technical specifications and incorporates tools for expert feedback and data
contributions. On evaluation using a benchmark dataset of queries and reference
responses created by subject matter experts, the system demonstrated more
relevant and accurate answers with an average BLEU score and BERTScore
F1-measure of 0.37 and 0.79 respectively compared to the corresponding values
of 0.07 and 0.59 achieved by state-of-the-art tools like ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotaru_M/0/1/0/all/0/1&quot;&gt;Manikanta Kotaru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04313">
<title>Apple Vision Pro for Healthcare: &quot;The Ultimate Display&quot;? -- Entering the Wonderland of Precision. (arXiv:2308.04313v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04313</link>
<description rdf:parseType="Literal">&lt;p&gt;At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user&apos;s eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers&apos; eyes to &quot;outsiders&quot; or a button on
the top, called &quot;Digital Crown&quot;, that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the &quot;Ultimate
Display&quot;, which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1&quot;&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puladi_B/0/1/0/all/0/1&quot;&gt;Behrus Puladi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04371">
<title>Cumulative Reasoning with Large Language Models. (arXiv:2308.04371v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04371</link>
<description rdf:parseType="Literal">&lt;p&gt;While language models are powerful and versatile, they often fail to address
highly complex problems. This is because solving complex problems requires
deliberate thinking, which has been only minimally guided during training. In
this paper, we propose a new method called Cumulative Reasoning (CR), which
employs language models in a cumulative and iterative manner to emulate human
thought processes. By decomposing tasks into smaller components, CR streamlines
the problem-solving process, rendering it both more manageable and effective.
For logical inference tasks, CR consistently outperforms existing methods with
an improvement up to 9.3%, and achieves the astonishing accuracy of 98.04% on
the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves
an accuracy of 94%, which signifies a substantial enhancement of 20% over the
previous state-of-the-art method (code is available at
https://github.com/iiis-ai/cumulative-reasoning).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingqin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Andrew Chi-Chih Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04952">
<title>Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04952</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic
Segmentation (FSS) to simultaneously segment unseen classes and seen classes
during evaluation. Previous works leverage additional branch or prototypical
aggregation to eliminate the constrained setting of FSS. However,
representation division and embedding prejudice, which heavily results in poor
performance of GFSS, have not been synthetical considered. We address the
aforementioned problems by jointing the prototypical kernel learning and
open-set foreground perception. Specifically, a group of learnable kernels is
proposed to perform segmentation with each kernel in charge of a stuff class.
Then, we explore to merge the prototypical learning to the update of base-class
kernels, which is consistent with the prototype knowledge aggregation of
few-shot novel classes. In addition, a foreground contextual perception module
cooperating with conditional bias based inference is adopted to perform
class-agnostic as well as open-set foreground detection, thus to mitigate the
embedding prejudice and prevent novel targets from being misclassified as
background. Moreover, we also adjust our method to the Class Incremental
Few-shot Semantic Segmentation (CIFSS) which takes the knowledge of novel
classes in a incremental stream. Extensive experiments on PASCAL-5i and
COCO-20i datasets demonstrate that our method performs better than previous
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feigege Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1&quot;&gt;Ye Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yutao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19069">
<title>Multi-source adversarial transfer learning for ultrasound image segmentation with limited similarity. (arXiv:2305.19069v1 [eess.IV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.19069</link>
<description rdf:parseType="Literal">&lt;p&gt;Lesion segmentation of ultrasound medical images based on deep learning
techniques is a widely used method for diagnosing diseases. Although there is a
large amount of ultrasound image data in medical centers and other places,
labeled ultrasound datasets are a scarce resource, and it is likely that no
datasets are available for new tissues/organs. Transfer learning provides the
possibility to solve this problem, but there are too many features in natural
images that are not related to the target domain. As a source domain, redundant
features that are not conducive to the task will be extracted. Migration
between ultrasound images can avoid this problem, but there are few types of
public datasets, and it is difficult to find sufficiently similar source
domains. Compared with natural images, ultrasound images have less information,
and there are fewer transferable features between different ultrasound images,
which may cause negative transfer. To this end, a multi-source adversarial
transfer learning network for ultrasound image segmentation is proposed.
Specifically, to address the lack of annotations, the idea of adversarial
transfer learning is used to adaptively extract common features between a
certain pair of source and target domains, which provides the possibility to
utilize unlabeled ultrasound data. To alleviate the lack of knowledge in a
single source domain, multi-source transfer learning is adopted to fuse
knowledge from multiple source domains. In order to ensure the effectiveness of
the fusion and maximize the use of precious data, a multi-source domain
independent strategy is also proposed to improve the estimation of the target
domain data distribution, which further increases the learning ability of the
multi-source adversarial migration learning network in multiple domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongru Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Rui Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shimeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiansong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Ning Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wujin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhanhu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>