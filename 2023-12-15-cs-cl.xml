<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08836" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.07550">
<title>Understanding (Un)Intended Memorization in Text-to-Image Generative Models. (arXiv:2312.07550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07550</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal machine learning, especially text-to-image models like Stable
Diffusion and DALL-E 3, has gained significance for transforming text into
detailed images.
&lt;/p&gt;
&lt;p&gt;Despite their growing use and remarkable generative capabilities, there is a
pressing need for a detailed examination of these models&apos; behavior,
particularly with respect to memorization. Historically, memorization in
machine learning has been context-dependent, with diverse definitions emerging
from classification tasks to complex models like Large Language Models (LLMs)
and Diffusion models. Yet, a definitive concept of memorization that aligns
with the intricacies of text-to-image synthesis remains elusive. This
understanding is vital as memorization poses privacy risks yet is essential for
meeting user expectations, especially when generating representations of
underrepresented entities. In this paper, we introduce a specialized definition
of memorization tailored to text-to-image models, categorizing it into three
distinct types according to user expectations. We closely examine the subtle
distinctions between intended and unintended memorization, emphasizing the
importance of balancing user privacy with the generative quality of the model
outputs. Using the Stable Diffusion model, we offer examples to validate our
memorization definitions and clarify their application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseh_A/0/1/0/all/0/1&quot;&gt;Ali Naseh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roh_J/0/1/0/all/0/1&quot;&gt;Jaechul Roh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houmansadr_A/0/1/0/all/0/1&quot;&gt;Amir Houmansadr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07551">
<title>Language Model Alignment with Elastic Reset. (arXiv:2312.07551v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07551</link>
<description rdf:parseType="Literal">&lt;p&gt;Finetuning language models with reinforcement learning (RL), e.g. from human
feedback (HF), is a prominent method for alignment. But optimizing against a
reward model can improve on reward while degrading performance in other areas,
a phenomenon known as reward hacking, alignment tax, or language drift. First,
we argue that commonly-used test metrics are insufficient and instead measure
how different algorithms tradeoff between reward and drift. The standard method
modified the reward with a Kullback-Lieber (KL) penalty between the online and
initial model. We propose Elastic Reset, a new algorithm that achieves higher
reward with less drift without explicitly modifying the training objective. We
periodically reset the online model to an exponentially moving average (EMA) of
itself, then reset the EMA model to the initial model. Through the use of an
EMA, our model recovers quickly after resets and achieves higher reward with
less drift in the same number of steps. We demonstrate that fine-tuning
language models with Elastic Reset leads to state-of-the-art performance on a
small scale pivot-translation benchmark, outperforms all baselines in a
medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant
and more aligned technical QA chatbot with LLaMA-7B. Code available at
github.com/mnoukhov/elastic-reset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noukhovitch_M/0/1/0/all/0/1&quot;&gt;Michael Noukhovitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavoie_S/0/1/0/all/0/1&quot;&gt;Samuel Lavoie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1&quot;&gt;Florian Strub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07552">
<title>Large Language Models for Intent-Driven Session Recommendations. (arXiv:2312.07552v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07552</link>
<description rdf:parseType="Literal">&lt;p&gt;Intent-aware session recommendation (ISR) is pivotal in discerning user
intents within sessions for precise predictions. Traditional approaches,
however, face limitations due to their presumption of a uniform number of
intents across all sessions. This assumption overlooks the dynamic nature of
user sessions, where the number and type of intentions can significantly vary.
In addition, these methods typically operate in latent spaces, thus hinder the
model&apos;s transparency.Addressing these challenges, we introduce a novel ISR
approach, utilizing the advanced reasoning capabilities of large language
models (LLMs). First, this approach begins by generating an initial prompt that
guides LLMs to predict the next item in a session, based on the varied intents
manifested in user sessions. Then, to refine this process, we introduce an
innovative prompt optimization mechanism that iteratively self-reflects and
adjusts prompts. Furthermore, our prompt selection module, built upon the LLMs&apos;
broad adaptability, swiftly selects the most optimized prompts across diverse
domains. This new paradigm empowers LLMs to discern diverse user intents at a
semantic level, leading to more accurate and interpretable session
recommendations. Our extensive experiments on three real-world datasets
demonstrate the effectiveness of our method, marking a significant advancement
in ISR systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xinghua Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kaidong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1&quot;&gt;Yew-Soon Ong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07553">
<title>Hijacking Context in Large Multi-modal Models. (arXiv:2312.07553v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07553</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to
understand the visual contents of images given the instructions regarding the
images. Built upon the Large Language Models (LLMs), LMMs also inherit their
abilities and characteristics such as in-context learning where a coherent
sequence of images and texts are given as the input prompt. However, we
identify a new limitation of off-the-shelf LMMs where a small fraction of
incoherent images or text descriptions mislead LMMs to only generate biased
output about the hijacked context, not the originally intended context. To
address this, we propose a pre-filtering method that removes irrelevant
contexts via GPT-4V, based on its robustness towards distribution shift within
the contexts. We further investigate whether replacing the hijacked visual and
textual contexts with the correlated ones via GPT-4V and text-to-image models
can help yield coherent responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Joonhyun Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07556">
<title>Federated Learning for Short Text Clustering. (arXiv:2312.07556v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07556</link>
<description rdf:parseType="Literal">&lt;p&gt;Short text clustering has been popularly studied for its significance in
mining valuable insights from many short texts. In this paper, we focus on the
federated short text clustering (FSTC) problem, i.e., clustering short texts
that are distributed in different clients, which is a realistic problem under
privacy requirements. Compared with the centralized short text clustering
problem that short texts are stored on a central server, the FSTC problem has
not been explored yet. To fill this gap, we propose a Federated Robust Short
Text Clustering (FSTC) framework. FSTC includes two main modules, i.e., robust
short text clustering module and federated cluster center aggregation module.
The robust short text clustering module aims to train an effective short text
clustering model with local data in each client. We innovatively combine
optimal transport to generate pseudo-labels with Gaussian-uniform mixture model
to ensure the reliability of the pseudo-supervised data. The federated cluster
center aggregation module aims to exchange knowledge across clients without
sharing local raw data in an efficient way. The server aggregates the local
cluster centers from different clients and then sends the global centers back
to all clients in each communication round. Our empirical studies on three
short text clustering datasets demonstrate that FSTC significantly outperforms
the federated short text clustering baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mengling Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1&quot;&gt;Xinting Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07559">
<title>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. (arXiv:2312.07559v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07559</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) generalize well across language tasks, but
suffer from hallucinations and uninterpretability, making it difficult to
assess their accuracy without ground-truth. Retrieval-Augmented Generation
(RAG) models have been proposed to reduce hallucinations and provide provenance
for how an answer was generated. Applying such models to the scientific
literature may enable large-scale, systematic processing of scientific
knowledge. We present PaperQA, a RAG agent for answering questions over the
scientific literature. PaperQA is an agent that performs information retrieval
across full-text scientific articles, assesses the relevance of sources and
passages, and uses RAG to provide answers. Viewing this agent as a question
answering model, we find it exceeds performance of existing LLMs and LLM agents
on current science QA benchmarks. To push the field closer to how humans
perform research on scientific literature, we also introduce LitQA, a more
complex benchmark that requires retrieval and synthesis of information from
full-text scientific papers across the literature. Finally, we demonstrate
PaperQA&apos;s matches expert human researchers on LitQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lala_J/0/1/0/all/0/1&quot;&gt;Jakub L&amp;#xe1;la&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonoghue_O/0/1/0/all/0/1&quot;&gt;Odhran O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cox_S/0/1/0/all/0/1&quot;&gt;Sam Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriques_S/0/1/0/all/0/1&quot;&gt;Samuel G. Rodriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Andrew D. White&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07573">
<title>Arabic Handwritten Text Line Dataset. (arXiv:2312.07573v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07573</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation of Arabic manuscripts into lines of text and words is an
important step to make recognition systems more efficient and accurate. The
problem of segmentation into text lines is solved since there are carefully
annotated dataset dedicated to this task. However, To the best of our
knowledge, there are no dataset annotating the word position of Arabic texts.
In this paper, we present a new dataset specifically designed for historical
Arabic script in which we annotate position in word level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchal_H/0/1/0/all/0/1&quot;&gt;Hakim Bouchal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belaid_A/0/1/0/all/0/1&quot;&gt;Ahror Belaid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07589">
<title>ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion. (arXiv:2312.07589v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07589</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs generally suffer from incompleteness, which can be
alleviated by completing the missing information. Deep knowledge convolutional
embedding models based on neural networks are currently popular methods for
knowledge graph completion. However, most existing methods use external
convolution kernels and traditional plain convolution processes, which limits
the feature interaction capability of the model. In this paper, we propose a
novel dynamic convolutional embedding model ConvD for knowledge graph
completion, which directly reshapes the relation embeddings into multiple
internal convolution kernels to improve the external convolution kernels of the
traditional convolutional embedding model. The internal convolution kernels can
effectively augment the feature interaction between the relation embeddings and
entity embeddings, thus enhancing the model embedding performance. Moreover, we
design a priori knowledge-optimized attention mechanism, which can assign
different contribution weight coefficients to multiple relation convolution
kernels for dynamic convolution to improve the expressiveness of the model
further. Extensive experiments on various datasets show that our proposed model
consistently outperforms the state-of-the-art baseline methods, with average
improvements ranging from 11.30\% to 16.92\% across all model evaluation
metrics. Ablation experiments verify the effectiveness of each component module
of the ConvD model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wenbin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zirui Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07592">
<title>Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models. (arXiv:2312.07592v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07592</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current era, a multitude of language models has emerged to cater to
user inquiries. Notably, the GPT-3.5 Turbo language model has gained
substantial attention as the underlying technology for ChatGPT. Leveraging
extensive parameters, this model adeptly responds to a wide range of questions.
However, due to its reliance on internal knowledge, the accuracy of responses
may not be absolute. This article scrutinizes ChatGPT as a Question Answering
System (QAS), comparing its performance to other existing QASs. The primary
focus is on evaluating ChatGPT&apos;s proficiency in extracting responses from
provided paragraphs, a core QAS capability. Additionally, performance
comparisons are made in scenarios without a surrounding passage. Multiple
experiments, exploring response hallucination and considering question
complexity, were conducted on ChatGPT. Evaluation employed well-known Question
Answering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, across
English and Persian languages. Metrics such as F-score, exact match, and
accuracy were employed in the assessment. The study reveals that, while ChatGPT
demonstrates competence as a generative model, it is less effective in question
answering compared to task-specific models. Providing context improves its
performance, and prompt engineering enhances precision, particularly for
questions lacking explicit answers in provided paragraphs. ChatGPT excels at
simpler factual questions compared to &quot;how&quot; and &quot;why&quot; question types. The
evaluation highlights occurrences of hallucinations, where ChatGPT provides
responses to questions without available answers in the provided context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahak_H/0/1/0/all/0/1&quot;&gt;Hossein Bahak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taheri_F/0/1/0/all/0/1&quot;&gt;Farzaneh Taheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zojaji_Z/0/1/0/all/0/1&quot;&gt;Zahra Zojaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1&quot;&gt;Arefeh Kazemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07599">
<title>Contrastive News and Social Media Linking using BERT for Articles and Tweets across Dual Platforms. (arXiv:2312.07599v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07599</link>
<description rdf:parseType="Literal">&lt;p&gt;X (formerly Twitter) has evolved into a contemporary agora, offering a
platform for individuals to express opinions and viewpoints on current events.
The majority of the topics discussed on Twitter are directly related to ongoing
events, making it an important source for monitoring public discourse. However,
linking tweets to specific news presents a significant challenge due to their
concise and informal nature. Previous approaches, including topic models,
graph-based models, and supervised classifiers, have fallen short in
effectively capturing the unique characteristics of tweets and articles.
&lt;/p&gt;
&lt;p&gt;Inspired by the success of the CLIP model in computer vision, which employs
contrastive learning to model similarities between images and captions, this
paper introduces a contrastive learning approach for training a representation
space where linked articles and tweets exhibit proximity. We present our
contrastive learning approach, CATBERT (Contrastive Articles Tweets BERT),
leveraging pre-trained BERT models. The model is trained and tested on a
dataset containing manually labeled English and Polish tweets and articles
related to the Russian-Ukrainian war. We evaluate CATBERT&apos;s performance against
traditional approaches like LDA, and the novel method based on OpenAI
embeddings, which has not been previously applied to this task. Our findings
indicate that CATBERT demonstrates superior performance in associating tweets
with relevant news articles. Furthermore, we demonstrate the performance of the
models when applied to finding the main topic -- represented by an article --
of the whole cascade of tweets. In this new task, we report the performance of
the different models in dependence on the cascade size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piotrowski_J/0/1/0/all/0/1&quot;&gt;Jan Piotrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachnicki_M/0/1/0/all/0/1&quot;&gt;Marek Wachnicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlik_M/0/1/0/all/0/1&quot;&gt;Mateusz Perlik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Podolak_J/0/1/0/all/0/1&quot;&gt;Jakub Podolak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rucki_G/0/1/0/all/0/1&quot;&gt;Grzegorz Rucki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brzozowski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Brzozowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olejnik_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Olejnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozlowski_J/0/1/0/all/0/1&quot;&gt;Julian Koz&amp;#x142;owski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nocon_T/0/1/0/all/0/1&quot;&gt;Tomasz Noco&amp;#x144;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koziel_J/0/1/0/all/0/1&quot;&gt;Jakub Kozie&amp;#x142;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1&quot;&gt;Stanis&amp;#x142;aw Gizi&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankowski_P/0/1/0/all/0/1&quot;&gt;Piotr Sankowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07622">
<title>Mathematical Language Models: A Survey. (arXiv:2312.07622v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07622</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been remarkable progress in leveraging Language
Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale
Language Models (LLMs), within the domain of mathematics. This paper conducts a
comprehensive survey of mathematical LMs, systematically categorizing pivotal
research endeavors from two distinct perspectives: tasks and methodologies. The
landscape reveals a large number of proposed mathematical LLMs, which are
further delineated into instruction learning, tool-based methods, fundamental
CoT techniques, and advanced CoT methodologies. In addition, our survey entails
the compilation of over 60 mathematical datasets, including training datasets,
benchmark datasets, and augmented datasets. Addressing the primary challenges
and delineating future trajectories within the field of mathematical LMs, this
survey is positioned as a valuable resource, poised to facilitate and inspire
future innovation among researchers invested in advancing this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wentao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hanglei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yuyang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jiayi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1&quot;&gt;Mengliang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aimin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07661">
<title>CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor. (arXiv:2312.07661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.07661</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing open-vocabulary image segmentation methods require a fine-tuning
step on mask annotations and/or image-text datasets. Mask labels are
labor-intensive, which limits the number of categories in segmentation
datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is
severely reduced after fine-tuning. However, without fine-tuning, VLMs trained
under weak image-text supervision tend to make suboptimal mask predictions when
there are text queries referring to non-existing concepts in the image. To
alleviate these issues, we introduce a novel recurrent framework that
progressively filters out irrelevant texts and enhances mask quality without
training efforts. The recurrent unit is a two-stage segmenter built upon a VLM
with frozen weights. Thus, our model retains the VLM&apos;s broad vocabulary space
and strengthens its segmentation capability. Experimental results show that our
method outperforms not only the training-free counterparts, but also those
fine-tuned with millions of additional data samples, and sets new
state-of-the-art records for both zero-shot semantic and referring image
segmentation tasks. Specifically, we improve the current record by 28.8, 16.0,
and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Runjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiuye Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07743">
<title>FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems. (arXiv:2312.07743v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07743</link>
<description rdf:parseType="Literal">&lt;p&gt;Word2Vec remains one of the highly-impactful innovations in the field of
Natural Language Processing (NLP) that represents latent grammatical and
syntactical information in human text with dense vectors in a low dimension.
Word2Vec has high computational cost due to the algorithm&apos;s inherent
sequentiality, intensive memory accesses, and the large vocabularies it
represents. While prior studies have investigated technologies to explore
parallelism and improve memory system performance, they struggle to effectively
gain throughput on powerful GPUs.
&lt;/p&gt;
&lt;p&gt;We identify memory data access and latency as the primary bottleneck in prior
works on GPUs, which prevents highly optimized kernels from attaining the
architecture&apos;s peak performance. We present a novel algorithm, FULL-W2V, which
maximally exploits the opportunities for data reuse in the W2V algorithm and
leverages GPU architecture and resources to reduce access to low memory levels
and improve temporal locality. FULL-W2V is capable of reducing accesses to GPU
global memory significantly, e.g., by more than 89\%, compared to prior
state-of-the-art GPU implementations, resulting in significant performance
improvement that scales across successive hardware generations. Our prototype
implementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to
Volta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards
with the same embedding quality. In-depth analysis indicates that the reduction
of memory accesses through register and shared memory caching and
high-throughput shared memory reduction leads to a significantly improved
arithmetic intensity. FULL-W2V can potentially benefit many applications in NLP
and other domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Randall_T/0/1/0/all/0/1&quot;&gt;Thomas Randall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_T/0/1/0/all/0/1&quot;&gt;Tyler Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rong Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07751">
<title>Large Human Language Models: A Need and the Challenges. (arXiv:2312.07751v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07751</link>
<description rdf:parseType="Literal">&lt;p&gt;As research in human-centered NLP advances, there is a growing recognition of
the importance of incorporating human and social factors into NLP models. At
the same time, our NLP systems have become heavily reliant on LLMs, most of
which do not model authors. To build NLP systems that can truly understand
human language, we must better integrate human contexts into LLMs. This brings
to the fore a range of design considerations and challenges in terms of what
human aspects to capture, how to represent them, and what modeling strategies
to pursue. To address these, we advocate for three positions toward creating
large human language models (LHLMs) using concepts from psychological and
behavioral sciences: First, LM training should include the human context.
Second, LHLMs should recognize that people are more than their group(s). Third,
LHLMs should be able to account for the dynamic and temporally-dependent nature
of the human context. We refer to relevant advances and present open challenges
that need to be addressed and their possible solutions in realizing these
goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1&quot;&gt;Nikita Soni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1&quot;&gt;H. Andrew Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Sedoc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1&quot;&gt;Niranjan Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07763">
<title>Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization. (arXiv:2312.07763v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07763</link>
<description rdf:parseType="Literal">&lt;p&gt;The meaning of complex phrases in natural language is composed of their
individual components. The task of compositional generalization evaluates a
model&apos;s ability to understand new combinations of components. Previous studies
trained smaller, task-specific models, which exhibited poor generalization.
While large language models (LLMs) exhibit impressive generalization abilities
on many tasks through in-context learning (ICL), their potential for
compositional generalization remains unexplored. In this paper, we first
empirically investigate prevailing ICL methods in compositional generalization.
We find that they struggle with complex compositional questions due to
cumulative errors in long reasoning steps and intricate logic required for
tool-making. Consequently, we propose a human-guided tool manipulation
framework (HTM) that generates tools for sub-questions and integrates multiple
tools. Our method enhances the effectiveness of tool creation and usage with
minimal human effort. Experiments show that our method achieves
state-of-the-art performance on two compositional generalization benchmarks and
outperforms existing methods on the most challenging test split by 70%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jianfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shuo Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1&quot;&gt;Murong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chang-Tien Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07796">
<title>Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge Gaps. (arXiv:2312.07796v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.07796</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper presents a methodology for uncovering knowledge gaps on the
internet using the Retrieval Augmented Generation (RAG) model. By simulating
user search behaviour, the RAG system identifies and addresses gaps in
information retrieval systems. The study demonstrates the effectiveness of the
RAG system in generating relevant suggestions with a consistent accuracy of
93%. The methodology can be applied in various fields such as scientific
discovery, educational enhancement, research development, market analysis,
search engine optimisation, and content development. The results highlight the
value of identifying and understanding knowledge gaps to guide future
endeavours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1&quot;&gt;Joan Figuerola Hurtado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07797">
<title>Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?. (arXiv:2312.07797v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07797</link>
<description rdf:parseType="Literal">&lt;p&gt;Undoubtedly that the Bidirectional Encoder representations from Transformers
is the most powerful technique in making Natural Language Processing tasks such
as Named Entity Recognition, Question &amp;amp; Answers or Sentiment Analysis, however,
the use of traditional techniques remains a major potential for the improvement
of recent models, in particular word tokenization techniques and embeddings,
but also the improvement of neural network architectures which are now the core
of each architecture. recent. In this paper, we conduct a comparative study
between Fine-Tuning the Bidirectional Encoder Representations from Transformers
and a method of concatenating two embeddings to boost the performance of a
stacked Bidirectional Long Short-Term Memory-Bidirectional Gated Recurrent
Units model; these two approaches are applied in the context of sentiment
analysis of shopping places in Morocco. A search for the best learning rate was
made at the level of the two approaches, and a comparison of the best
optimizers was made for each sentence embedding combination with regard to the
second approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouabdallaoui_I/0/1/0/all/0/1&quot;&gt;Ibrahim Bouabdallaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerouate_F/0/1/0/all/0/1&quot;&gt;Fatima Guerouate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouhaddour_S/0/1/0/all/0/1&quot;&gt;Samya Bouhaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadi_C/0/1/0/all/0/1&quot;&gt;Chaimae Saadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sbihi_M/0/1/0/all/0/1&quot;&gt;Mohammed Sbihi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07819">
<title>Native Language Identification with Large Language Models. (arXiv:2312.07819v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07819</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first experiments on Native Language Identification (NLI)
using LLMs such as GPT-4. NLI is the task of predicting a writer&apos;s first
language by analyzing their writings in a second language, and is used in
second language acquisition and forensic linguistics. Our results show that GPT
models are proficient at NLI classification, with GPT-4 setting a new
performance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot
setting. We also show that unlike previous fully-supervised settings, LLMs can
perform NLI without being limited to a set of known classes, which has
practical implications for real-world applications. Finally, we also show that
LLMs can provide justification for their choices, providing reasoning based on
spelling errors, syntactic patterns, and usage of directly translated
linguistic patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salle_A/0/1/0/all/0/1&quot;&gt;Alexandre Salle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07824">
<title>A Deep Learning-Based System for Automatic Case Summarization. (arXiv:2312.07824v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07824</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning-based system for efficient automatic case
summarization. Leveraging state-of-the-art natural language processing
techniques, the system offers both supervised and unsupervised methods to
generate concise and relevant summaries of lengthy legal case documents. The
user-friendly interface allows users to browse the system&apos;s database of legal
case documents, select their desired case, and choose their preferred
summarization method. The system generates comprehensive summaries for each
subsection of the legal text as well as an overall summary. This demo
streamlines legal case document analysis, potentially benefiting legal
professionals by reducing workload and increasing efficiency. Future work will
focus on refining summarization techniques and exploring the application of our
methods to other types of legal texts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_M/0/1/0/all/0/1&quot;&gt;Minh Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Long Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Y/0/1/0/all/0/1&quot;&gt;Yen Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trong Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Ha-Thanh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07831">
<title>Abusive Span Detection for Vietnamese Narrative Texts. (arXiv:2312.07831v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07831</link>
<description rdf:parseType="Literal">&lt;p&gt;Abuse in its various forms, including physical, psychological, verbal,
sexual, financial, and cultural, has a negative impact on mental health.
However, there are limited studies on applying natural language processing
(NLP) in this field in Vietnam. Therefore, we aim to contribute by building a
human-annotated Vietnamese dataset for detecting abusive content in Vietnamese
narrative texts. We sourced these texts from VnExpress, Vietnam&apos;s popular
online newspaper, where readers often share stories containing abusive content.
Identifying and categorizing abusive spans in these texts posed significant
challenges during dataset creation, but it also motivated our research. We
experimented with lightweight baseline models by freezing PhoBERT and
XLM-RoBERTa and using their hidden states in a BiLSTM to assess the complexity
of the dataset. According to our experimental results, PhoBERT outperforms
other models in both labeled and unlabeled abusive span detection tasks. These
results indicate that it has the potential for future improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Nhu-Thanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1&quot;&gt;Khoa Thi-Kim Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc-Vu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Ngan Luu-Thuy Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07848">
<title>Finetuning an LLM on Contextual Knowledge of Classics for Q&amp;A. (arXiv:2312.07848v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07848</link>
<description rdf:parseType="Literal">&lt;p&gt;The open-source publishing of large language models (LLMs) has created many
possibilities for how anyone who understands language and has access to a
computer can interact with significant tools of artificial intelligence,
particularly in the context of learning and knowledge dissemination. However,
the utility of these models in specialized fields like Classics is still
largely unexplored. This project is an attempt to merge the knowledge of
Classics with the capabilities of artificial intelligence by finetuning an LLM
to cater to the specific needs of learners and professionals. The goal of this
project is to develop an LLM that not only reproduces contextual knowledge
accurately but also exhibits a consistent &quot;personality&quot; - and, indeed, has
consistent propriety - to appeal to a diverse audience who possess differing
levels of knowledge. A significant portion of this project was dedicated to
refining the dataset, following the principle of &quot;garbage in, garbage out,&quot; to
ensure the model generates relevant, useful, and creative responses when given
a prompt (a statement, question, or single word). After training and
evaluation, my model&apos;s ability to handle a vast array of different types of
inputs and prompting exceeded expectations for a 355M parameter model, though
its occasional hallucinations (especially when set with a high temperature),
particularly in its assertions about historical events or its own identity,
make it seem somewhat capricious and more work in the form of continuous
finetuning will be undertaken.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strachan_S/0/1/0/all/0/1&quot;&gt;Shane Storm Strachan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07867">
<title>BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering. (arXiv:2312.07867v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical Visual Question Answering (Med-VQA) is a very important task in
healthcare industry, which answers a natural language question with a medical
image. Existing VQA techniques in information systems can be directly applied
to solving the task. However, they often suffer from (i) the data insufficient
problem, which makes it difficult to train the state of the arts (SOTAs) for
the domain-specific task, and (ii) the reproducibility problem, that many
existing models have not been thoroughly evaluated in a unified experimental
setup. To address these issues, this paper develops a Benchmark Evaluation
SysTem for Medical Visual Question Answering, denoted by BESTMVQA. Given
self-collected clinical data, our system provides a useful tool for users to
automatically build Med-VQA datasets, which helps overcoming the data
insufficient problem. Users also can conveniently select a wide spectrum of
SOTA models from our model library to perform a comprehensive empirical study.
With simple configurations, our system automatically trains and evaluates the
selected models over a benchmark dataset, and reports the comprehensive results
for users to develop new techniques or perform medical practice. Limitations of
existing work are overcome (i) by the data generation tool, which automatically
constructs new datasets from unstructured clinical data, and (ii) by evaluating
SOTAs on benchmark datasets in a unified experimental setup. The demonstration
video of our system can be found at https://youtu.be/QkEeFlu1x4A. Our code and
data will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1&quot;&gt;Xiaojie Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zixin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feiyan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07868">
<title>Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue. (arXiv:2312.07868v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07868</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge-grounded dialogue is a task of generating an informative response
based on both the dialogue history and external knowledge source. In general,
there are two forms of knowledge: manually annotated knowledge graphs and
knowledge text from website. From various evaluation viewpoints, each type of
knowledge has advantages and downsides. To further distinguish the principles
and determinants from the intricate factors, we conduct a thorough experiment
and study on the task to answer three essential questions. The questions
involve the choice of appropriate knowledge form, the degree of mutual effects
between knowledge and the model selection, and the few-shot performance of
knowledge. Supported by statistical shreds of evidence, we offer conclusive
solutions and sensible suggestions for directions and standards of future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yizhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07886">
<title>Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI. (arXiv:2312.07886v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07886</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are capable of reasoning over diverse input data
modalities through pre-trained encoders. However, the growing diversity of
input data modalities prevents incorporating all modalities into LLMs,
especially when LLMs are deployed on resource-constrained edge devices for
embodied AI applications. Instead, a better option is to adaptively involve
only the useful modalities at runtime, depending on the current environmental
contexts and task requirements. For such modality adaptation, existing work
adopts fixed connections between encoders and the LLM&apos;s input layer, leading to
high training cost at runtime and ineffective cross-modal interaction. In this
paper, we address these limitations by presenting mPnP-LLM, a new technique
that allows fully elastic, automated and prompt runtime modality adaptation, by
connecting unimodal encoders to a flexible set of last LLM blocks and making
such latent connections fully trainable at runtime. Experiments over the
nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction
and 30% GPU memory usage reduction, while retaining on-par accuracy with the
existing schemes. Under the same compute budget, mPnP-LLM improves the task
accuracy by up to 4% compared to the best existing scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Boyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wei Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07887">
<title>Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models. (arXiv:2312.07887v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental Learning (IL) has been a long-standing problem in both vision and
Natural Language Processing (NLP) communities. In recent years, as Pre-trained
Language Models (PLMs) have achieved remarkable progress in various NLP
downstream tasks, utilizing PLMs as backbones has become a common practice in
recent research of IL in NLP. Most assume that catastrophic forgetting is the
biggest obstacle to achieving superior IL performance and propose various
techniques to overcome this issue. However, we find that this assumption is
problematic. Specifically, we revisit more than 20 methods on four
classification tasks (Text Classification, Intent Classification, Relation
Extraction, and Named Entity Recognition) under the two most popular IL
settings (Class-Incremental and Task-Incremental) and reveal that most of them
severely underestimate the inherent anti-forgetting ability of PLMs. Based on
the observation, we propose a frustratingly easy method called SEQ* for IL with
PLMs. The results show that SEQ* has competitive or superior performance
compared to state-of-the-art (SOTA) IL methods and requires considerably less
trainable parameters and training time. These findings urge us to revisit the
IL with PLMs and encourage future studies to have a fundamental understanding
of the catastrophic forgetting in PLMs. The data, code and scripts are publicly
available at
https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Junhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shengjie Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qianli Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07910">
<title>PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.07910</link>
<description rdf:parseType="Literal">&lt;p&gt;The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qinlin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07913">
<title>A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07913</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, significant advancements have been made in the text
generation capabilities of Large Language Models (LLMs), demonstrating
exceptional performance in downstream tasks such as abstract summarization,
dialogue generation, and data-to-text conversion. However, their generative
abilities also pose risks such as the rapid spread of fake news, infringement
of datasets/LLM copyrights, and challenges to academic integrity. Text
watermarking technology emerges as a potential solution. By embedding invisible
yet detectable patterns in generated texts, it helps in tracking and verifying
text origins, thus preventing misuse and piracy.
&lt;/p&gt;
&lt;p&gt;This survey aims to comprehensively summarize current text watermarking
technologies, covering three main aspects: (1) an overview and comparison of
different text watermarking techniques; (2) evaluation methods for text
watermarking algorithms, including their success rate, impact on text quality,
robustness, and unforgeability; (3) potential applications of text watermarking
technologys. This survey aims to help researchers thoroughly understanding the
text watermarking technologies, thereby fostering further development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Leyi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yijian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingjing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xuming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Lijie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07930">
<title>Towards Optimal Statistical Watermarking. (arXiv:2312.07930v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07930</link>
<description rdf:parseType="Literal">&lt;p&gt;We study statistical watermarking by formulating it as a hypothesis testing
problem, a general framework which subsumes all previous statistical
watermarking methods. Key to our formulation is a coupling of the output tokens
and the rejection region, realized by pseudo-random generators in practice,
that allows non-trivial trade-off between the Type I error and Type II error.
We characterize the Uniformly Most Powerful (UMP) watermark in this context. In
the most common scenario where the output is a sequence of $n$ tokens, we
establish matching upper and lower bounds on the number of i.i.d. tokens
required to guarantee small Type I and Type II errors. Our rate scales as
$\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$
and thus greatly improves the $O(h^{-2})$ rate in the previous works. For
scenarios where the detector lacks knowledge of the model&apos;s distribution, we
introduce the concept of model-agnostic watermarking and establish the minimax
bounds for the resultant increase in Type II error. Moreover, we formulate the
robust watermarking problem where user is allowed to perform a class of
perturbation on the generated texts, and characterize the optimal type II error
of robust UMP tests via a linear programming problem. To the best of our
knowledge, this is the first systematic statistical treatment on the
watermarking problem with near-optimal rates in the i.i.d. setting, and might
be of interest for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Baihe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanlin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jason D. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07950">
<title>CBQ: Cross-Block Quantization for Large Language Models. (arXiv:2312.07950v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07950</link>
<description rdf:parseType="Literal">&lt;p&gt;Post-training quantization (PTQ) has driven attention to producing efficient
large language models (LLMs) with ultra-low costs. Since hand-craft
quantization parameters lead to low performance in low-bit quantization, recent
methods optimize the quantization parameters through block-wise reconstruction
between the floating-point and quantized models. However, these methods suffer
from two challenges: accumulated errors from independent one-by-one block
quantization and reconstruction difficulties from extreme weight and activation
outliers. To address these two challenges, we propose CBQ, a cross-block
reconstruction-based PTQ method for LLMs. To reduce error accumulation, we
introduce a cross-block dependency with the aid of a homologous reconstruction
scheme to build the long-range dependency between adjacent multi-blocks with
overlapping. To reduce reconstruction difficulty, we design a coarse-to-fine
pre-processing (CFP) to truncate weight outliers and dynamically scale
activation outliers before optimization, and an adaptive rounding scheme,
called LoRA-Rounding, with two low-rank learnable matrixes to further rectify
weight quantization errors. Extensive experiments demonstrate that: (1) CBQ
pushes both activation and weight quantization to low-bit settings W4A4, W4A8,
and W2A16. (2) CBQ achieves better performance than the existing
state-of-the-art methods on various LLMs and benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhijun Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yehui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Baoqun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07961">
<title>Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification. (arXiv:2312.07961v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07961</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot named entity recognition (NER) aims to recognize novel named
entities in low-resource domains utilizing existing knowledge. However, the
present few-shot NER models assume that the labeled data are all clean without
noise or outliers, and there are few works focusing on the robustness of the
cross-domain transfer learning ability to textual adversarial attacks in
Few-shot NER. In this work, we comprehensively explore and assess the
robustness of few-shot NER models under textual adversarial attack scenario,
and found the vulnerability of existing few-shot NER models. Furthermore, we
propose a robust two-stage few-shot NER method with Boundary Discrimination and
Correlation Purification (BDCP). Specifically, in the span detection stage, the
entity boundary discriminative module is introduced to provide a highly
distinguishing boundary representation space to detect entity spans. In the
entity typing stage, the correlations between entities and contexts are
purified by minimizing the interference information and facilitating
correlation generalization to alleviate the perturbations caused by textual
adversarial attacks. In addition, we construct adversarial examples for
few-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive
evaluations on those two groups of few-shot NER datasets containing adversarial
examples demonstrate the robustness and superiority of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiaojun Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunxia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianxiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Zhendong Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07979">
<title>SLJP: Semantic Extraction based Legal Judgment Prediction. (arXiv:2312.07979v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.07979</link>
<description rdf:parseType="Literal">&lt;p&gt;Legal Judgment Prediction (LJP) is a judicial assistance system that
recommends the legal components such as applicable statues, prison term and
penalty term by analyzing the given input case document. Indian legal system is
in the need of technical assistance such as artificial intelligence to solve
the crores of pending cases in various courts for years and its being increased
day to day. Most of the existing Indian models did not adequately concentrate
on the semantics embedded in the fact description (FD) that impacts the
decision. The proposed semantic extraction based LJP (SLJP) model provides the
advantages of pretrained transformers for complex unstructured legal case
document understanding and to generate embeddings. The model draws the in-depth
semantics of the given FD at multiple levels i.e., chunk and case document
level by following the divide and conquer approach. It creates the concise view
of the given fact description using the extracted semantics as per the original
court case document structure and predicts judgment using attention mechanism.
We tested the model performance on two available Indian datasets Indian Legal
Documents corpus (ILDC) and Indian Legal Statue Identification (ILSI) and got
promising results. Also shown the highest performance and less performance
degradation for increased epochs than base models on ILDC dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madambakam_P/0/1/0/all/0/1&quot;&gt;Prameela Madambakam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1&quot;&gt;Shathanaa Rajmohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Himangshu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1&quot;&gt;Tummepalli Anka Chandrahas Purushotham Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07987">
<title>SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention. (arXiv:2312.07987v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.07987</link>
<description rdf:parseType="Literal">&lt;p&gt;The costly self-attention layers in modern Transformers require memory and
compute quadratic in sequence length. Existing approximation methods usually
underperform and fail to obtain significant speedups in practice. Here we
present SwitchHead - a novel method that reduces both compute and memory
requirements and achieves wall-clock speedup, while matching the language
modeling performance of baseline Transformers with the same parameter budget.
SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output
projections and requires 4 to 8 times fewer attention matrices than standard
Transformers. Our novel attention can also be combined with MoE MLP layers,
resulting in an efficient fully-MoE &quot;SwitchHead&quot; Transformer model. Our code is
public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1&quot;&gt;R&amp;#xf3;bert Csord&amp;#xe1;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piekos_P/0/1/0/all/0/1&quot;&gt;Piotr Pi&amp;#x119;kos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1&quot;&gt;Kazuki Irie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08027">
<title>Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning. (arXiv:2312.08027v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08027</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can be used as accessible and intelligent
chatbots by constructing natural language queries and directly inputting the
prompt into the large language model. However, different prompt&apos; constructions
often lead to uncertainty in the answers and thus make it hard to utilize the
specific knowledge of LLMs (like ChatGPT). To alleviate this, we use an
interpretable structure to explain the prompt learning principle in LLMs, which
certificates that the effectiveness of language models is determined by
position changes of the task&apos;s related tokens. Therefore, we propose MTPrompt,
a multi-dimensional task prompt learning method consisting based on
task-related object, summary, and task description information. By
automatically building and searching for appropriate prompts, our proposed
MTPrompt achieves the best results on few-shot samples setting and five
different datasets. In addition, we demonstrate the effectiveness and stability
of our method in different experimental settings and ablation experiments. In
interaction with large language models, embedding more task-related information
into prompts will make it easier to stimulate knowledge embedded in large
language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1&quot;&gt;Jinta Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiarui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fa_D/0/1/0/all/0/1&quot;&gt;Daidong Fa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuand_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Xuand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08036">
<title>CoRTEx: Contrastive Learning for Representing Terms via Explanations with Applications on Constructing Biomedical Knowledge Graphs. (arXiv:2312.08036v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08036</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Biomedical Knowledge Graphs play a pivotal role in various
biomedical research domains. Concurrently, term clustering emerges as a crucial
step in constructing these knowledge graphs, aiming to identify synonymous
terms. Due to a lack of knowledge, previous contrastive learning models trained
with Unified Medical Language System (UMLS) synonyms struggle at clustering
difficult terms and do not generalize well beyond UMLS terms. In this work, we
leverage the world knowledge from Large Language Models (LLMs) and propose
Contrastive Learning for Representing Terms via Explanations (CoRTEx) to
enhance term representation and significantly improves term clustering.
Materials and Methods: The model training involves generating explanations for
a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning,
considering term and explanation embeddings simultaneously, and progressively
introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH
algorithm is designed for efficient clustering of a new ontology. Results: We
established a clustering test set and a hard negative test set, where our model
consistently achieves the highest F1 score. With CoRTEx embeddings and the
modified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical
Informatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries
to ChatGPT. Case studies highlight the model&apos;s efficacy in handling challenging
samples, aided by information from explanations. Conclusion: By aligning terms
to their explanations, CoRTEx demonstrates superior accuracy over benchmark
models and robustness beyond its training set, and it is suitable for
clustering terms for large-scale biomedical ontologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1&quot;&gt;Huaiyuan Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhengyun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Sihang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08063">
<title>Estimation of Concept Explanations Should be Uncertainty Aware. (arXiv:2312.08063v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08063</link>
<description rdf:parseType="Literal">&lt;p&gt;Model explanations are very valuable for interpreting and debugging
prediction models. We study a specific kind of global explanations called
Concept Explanations, where the goal is to interpret a model using
human-understandable concepts. Recent advances in multi-modal learning
rekindled interest in concept explanations and led to several label-efficient
proposals for estimation. However, existing estimation methods are unstable to
the choice of concepts or dataset that is used for computing explanations. We
observe that instability in explanations is due to high variance in point
estimation of importance scores. We propose an uncertainty aware Bayesian
estimation method, which readily improved reliability of the concept
explanations. We demonstrate with theoretical analysis and empirical evaluation
that explanations computed by our method are more reliable while also being
label-efficient and faithful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1&quot;&gt;Vihari Piratla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Juyeon Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sukriti Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08078">
<title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches&apos; for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08079">
<title>Extending Whisper with prompt tuning to target-speaker ASR. (arXiv:2312.08079v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08079</link>
<description rdf:parseType="Literal">&lt;p&gt;Target-speaker automatic speech recognition (ASR) aims to transcribe the
desired speech of a target speaker from multi-talker overlapped utterances.
Most of the existing target-speaker ASR (TS-ASR) methods involve either
training from scratch or fully fine-tuning a pre-trained model, leading to
significant training costs and becoming inapplicable to large foundation
models. This work leverages prompt tuning, a parameter-efficient fine-tuning
approach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR.
Experimental results show that prompt tuning can achieve performance comparable
to state-of-the-art full fine-tuning approaches while only requiring about 1%
of task-specific model parameters. Notably, the original Whisper&apos;s features,
such as inverse text normalization and timestamp prediction, are retained in
target-speaker ASR, keeping the generated transcriptions natural and
informative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1&quot;&gt;Mingjie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ju Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08198">
<title>Towards Model-Based Data Acquisition for Subjective Multi-Task NLP Problems. (arXiv:2312.08198v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08198</link>
<description rdf:parseType="Literal">&lt;p&gt;Data annotated by humans is a source of knowledge by describing the
peculiarities of the problem and therefore fueling the decision process of the
trained model. Unfortunately, the annotation process for subjective natural
language processing (NLP) problems like offensiveness or emotion detection is
often very expensive and time-consuming. One of the inevitable risks is to
spend some of the funds and annotator effort on annotations that do not provide
any additional knowledge about the specific task. To minimize these costs, we
propose a new model-based approach that allows the selection of tasks annotated
individually for each text in a multi-task scenario. The experiments carried
out on three datasets, dozens of NLP tasks, and thousands of annotations show
that our method allows up to 40% reduction in the number of annotations with
negligible loss of knowledge. The results also emphasize the need to collect a
diverse amount of data required to efficiently train a model, depending on the
subjectivity of the annotation task. We also focused on measuring the relation
between subjective tasks by evaluating the model in single-task and multi-task
scenarios. Moreover, for some datasets, training only on the labels predicted
by our model improved the efficiency of task selection as a self-supervised
learning regularization technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanclerz_K/0/1/0/all/0/1&quot;&gt;Kamil Kanclerz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bielaniewicz_J/0/1/0/all/0/1&quot;&gt;Julita Bielaniewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruza_M/0/1/0/all/0/1&quot;&gt;Marcin Gruza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1&quot;&gt;Jan Kocon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1&quot;&gt;Stanis&amp;#x142;aw Wo&amp;#x17a;niak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazienko_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Kazienko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08274">
<title>High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models. (arXiv:2312.08274v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08274</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: To develop a high-throughput biomedical relation extraction system
that takes advantage of the large language models&apos; (LLMs) reading comprehension
ability and biomedical world knowledge in a scalable and evidential manner.
Methods: We formulate the relation extraction task as a simple binary
classification problem for large language models such as ChatGPT. Specifically,
LLMs make the decision based on the external corpus and its world knowledge,
giving the reason for the judgment to factual verification. This method is
tailored for semi-structured web articles, wherein we designate the main title
as the tail entity and explicitly incorporate it into the context, and the
potential head entities are matched based on a biomedical thesaurus. Moreover,
lengthy contents are sliced into text chunks, embedded, and retrieved with
additional embedding models, ensuring compatibility with the context window
size constraints of available open-source LLMs. Results: Using an open-source
LLM, we extracted 304315 relation triplets of three distinct relation types
from four reputable biomedical websites. To assess the efficacy of the basic
pipeline employed for biomedical relation extraction, we curated a benchmark
dataset annotated by a medical expert. Evaluation results indicate that the
pipeline exhibits performance comparable to that of GPT-4. Case studies further
illuminate challenges faced by contemporary LLMs in the context of biomedical
relation extraction for semi-structured web articles. Conclusion: The proposed
method has demonstrated its effectiveness in leveraging the strengths of LLMs
for high-throughput biomedical relation extraction. Its adaptability is
evident, as it can be seamlessly extended to diverse semi-structured biomedical
websites, facilitating the extraction of various types of biomedical relations
with ease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Songchi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08282">
<title>Prompting LLMs with content plans to enhance the summarization of scientific articles. (arXiv:2312.08282v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08282</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents novel prompting techniques to improve the performance of
automatic summarization systems for scientific articles. Scientific article
summarization is highly challenging due to the length and complexity of these
documents. We conceive, implement, and evaluate prompting techniques that
provide additional contextual information to guide summarization systems.
Specifically, we feed summarizers with lists of key terms extracted from
articles, such as author keywords or automatically generated keywords. Our
techniques are tested with various summarization models and input texts.
Results show performance gains, especially for smaller models summarizing
sections separately. This evidences that prompting is a promising approach to
overcoming the limitations of less powerful systems. Our findings introduce a
new research direction of using prompts to aid smaller models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creo_A/0/1/0/all/0/1&quot;&gt;Aldan Creo&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lama_M/0/1/0/all/0/1&quot;&gt;Manuel Lama&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1&quot;&gt;Juan C. Vidal&lt;/a&gt; (1) ((1) Singular Research Center on Intelligent Technologies (CiTIUS), Universidade de Santiago de Compostela, Santiago de Compostela, Spain)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08299">
<title>Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data. (arXiv:2312.08299v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08299</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic has escalated mental health crises worldwide, with
social isolation and economic instability contributing to a rise in suicidal
behavior. Suicide can result from social factors such as shame, abuse,
abandonment, and mental health conditions like depression, Post-Traumatic
Stress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),
anxiety disorders, and bipolar disorders. As these conditions develop, signs of
suicidal ideation may manifest in social media interactions. Analyzing social
media data using artificial intelligence (AI) techniques can help identify
patterns of suicidal behavior, providing invaluable insights for suicide
prevention agencies, professionals, and broader community awareness
initiatives. Machine learning algorithms for this purpose require large volumes
of accurately labeled data. Previous research has not fully explored the
potential of incorporating explanations in analyzing and labeling longitudinal
social media data. In this study, we employed a model explanation method, Layer
Integrated Gradients, on top of a fine-tuned state-of-the-art language model,
to assign each token from Reddit users&apos; posts an attribution score for
predicting suicidal ideation. By extracting and analyzing attributions of
tokens from the data, we propose a methodology for preliminary screening of
social media posts for suicidal ideation without using large language models
during inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nur_N/0/1/0/all/0/1&quot;&gt;Nasheen Nur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stern_W/0/1/0/all/0/1&quot;&gt;William Stern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercer_T/0/1/0/all/0/1&quot;&gt;Thomas Mercer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_C/0/1/0/all/0/1&quot;&gt;Chiradeep Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1&quot;&gt;Siddhartha Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumbiolo_V/0/1/0/all/0/1&quot;&gt;Victor Tumbiolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1&quot;&gt;Seng Jhing Goh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08303">
<title>Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models. (arXiv:2312.08303v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08303</link>
<description rdf:parseType="Literal">&lt;p&gt;Toxic content detection is crucial for online services to remove
inappropriate content that violates community standards. To automate the
detection process, prior works have proposed varieties of machine learning (ML)
approaches to train Language Models (LMs) for toxic content detection. However,
both their accuracy and transferability across datasets are limited. Recently,
Large Language Models (LLMs) have shown promise in toxic content detection due
to their superior zero-shot and few-shot in-context learning ability as well as
broad transferability on ML tasks. However, efficiently designing prompts for
LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder
their deployments in production. To address these challenges, in this work, we
propose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling
LLMs for toxic content detection. Specifically, we design a novel prompting
method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs&apos; detection
performance and extract high-quality rationales. DToT can automatically select
more fine-grained context to re-prompt LLMs when their responses lack
confidence. Additionally, we use the rationales extracted via DToT to fine-tune
student LMs. Our experimental results on various datasets demonstrate that DToT
can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs
fine-tuned with rationales extracted via DToT outperform baselines on all
datasets with up to 16.9\% accuracy improvement, while being more than 60x
smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned
with rationales exhibit better cross-dataset transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yiming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Cheng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zheng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psounis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Psounis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11871">
<title>Cem Mil Podcasts: A Spoken Portuguese Document Corpus For Multi-modal, Multi-lingual and Multi-Dialect Information Access Research. (arXiv:2209.11871v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11871</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we describe the Portuguese-language podcast dataset we have
released for academic research purposes. We give an overview of how the data
was sampled, descriptive statistics over the collection, as well as information
about the distribution over Brazilian and Portuguese dialects. We give results
from experiments on multi-lingual summarization, showing that summarizing
podcast transcripts can be performed well by a system supporting both English
and Portuguese. We also show experiments on Portuguese podcast genre
classification using text metadata. Combining this collection with previously
released English-language collection opens up the potential for multi-modal,
multi-lingual and multi-dialect podcast information access research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garmash_E/0/1/0/all/0/1&quot;&gt;Ekaterina Garmash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_E/0/1/0/all/0/1&quot;&gt;Edgar Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifton_A/0/1/0/all/0/1&quot;&gt;Ann Clifton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1&quot;&gt;Joana Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jat_S/0/1/0/all/0/1&quot;&gt;Sharmistha Jat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Winstead Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1&quot;&gt;Rosie Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlgren_J/0/1/0/all/0/1&quot;&gt;Jussi Karlgren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04118">
<title>ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning. (arXiv:2211.04118v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt recently have become an effective linguistic tool on utilizing the
pre-trained language models. However, in few-shot scenarios, subtle changes of
prompt&apos;s design always make the result widely different, and the prompt design
is also easy to overfit the current limited samples. To alleviate this, we
explore how to utilize suitable contrastive samples and multiple contrastive
learning methods to realize a more robust prompt&apos;s representation. Therefore,
the contrastive prompt model ConsPrompt combining with prompt encoding network,
contrastive sampling modules, and contrastive scoring modules are introduced to
realize differential contrastive learning. Our results exhibit the
state-of-the-art performance in different few-shot settings, and the ablation
experiments also certificate the effectiveness in utilizing multi-degree
contrastive learning in prompt-based fine-tuning process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1&quot;&gt;Jinta Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yifan Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_d/0/1/0/all/0/1&quot;&gt;d Donghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1&quot;&gt;Hao You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09512">
<title>Rethinking Label Smoothing on Multi-hop Question Answering. (arXiv:2212.09512v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09512</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Hop Question Answering (MHQA) is a significant area in question
answering, requiring multiple reasoning components, including document
retrieval, supporting sentence prediction, and answer span extraction. In this
work, we analyze the primary factors limiting the performance of multi-hop
reasoning and introduce label smoothing into the MHQA task. This is aimed at
enhancing the generalization capabilities of MHQA systems and mitigating
overfitting of answer spans and reasoning paths in training set. We propose a
novel label smoothing technique, F1 Smoothing, which incorporates uncertainty
into the learning process and is specifically tailored for Machine Reading
Comprehension (MRC) tasks. Inspired by the principles of curriculum learning,
we introduce the Linear Decay Label Smoothing Algorithm (LDLA), which
progressively reduces uncertainty throughout the training process. Experiment
on the HotpotQA dataset demonstrates the effectiveness of our methods in
enhancing performance and generalizability in multi-hop reasoning, achieving
new state-of-the-art results on the leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiannian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yiguang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10915">
<title>Conformers are All You Need for Visual Speech Recognition. (arXiv:2302.10915v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10915</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual speech recognition models extract visual features in a hierarchical
manner. At the lower level, there is a visual front-end with a limited temporal
receptive field that processes the raw pixels depicting the lips or faces. At
the higher level, there is an encoder that attends to the embeddings produced
by the front-end over a large temporal receptive field. Previous work has
focused on improving the visual front-end of the model to extract more useful
features for speech recognition. Surprisingly, our work shows that complex
visual front-ends are not necessary. Instead of allocating resources to a
sophisticated visual front-end, we find that a linear visual front-end paired
with a larger Conformer encoder results in lower latency, more efficient memory
usage, and improved WER performance. We achieve a new state-of-the-art of 12.8%
WER for visual speech recognition on the TED LRS3 dataset, which rivals the
performance of audio-only models from just four years ago.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_O/0/1/0/all/0/1&quot;&gt;Oscar Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hank Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1&quot;&gt;Dmitriy Serdyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ankit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1&quot;&gt;Olivier Siohan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17728">
<title>Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text. (arXiv:2303.17728v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17728</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting protein-protein interactions (PPIs) is crucial for understanding
genetic mechanisms, disease pathogenesis, and drug design. However, with the
fast-paced growth of biomedical literature, there is a growing need for
automated and accurate extraction of PPIs to facilitate scientific knowledge
discovery. Pre-trained language models, such as generative pre-trained
transformers (GPT) and bidirectional encoder representations from transformers
(BERT), have shown promising results in natural language processing (NLP)
tasks. We evaluated the performance of PPI identification of multiple GPT and
BERT models using three manually curated gold-standard corpora: Learning
Language in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference
Database with 163 PPIs in 145 sentences, and Interaction Extraction Performance
Assessment with 335 PPIs in 486 sentences. BERT-based models achieved the best
overall performance, with BioBERT achieving the highest recall (91.95%) and
F1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%).
Interestingly, despite not being explicitly trained for biomedical texts, GPT-4
achieved commendable performance, comparable to the top-performing BERT models.
It achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of
86.49% on the LLL dataset. These results suggest that GPT models can
effectively detect PPIs from text data, offering promising avenues for
application in biomedical literature mining. Further research could explore how
these models might be fine-tuned for even more specialized tasks within the
biomedical domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehana_H/0/1/0/all/0/1&quot;&gt;Hasin Rehana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cam_N/0/1/0/all/0/1&quot;&gt;Nur Bengisu &amp;#xc7;am&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basmaci_M/0/1/0/all/0/1&quot;&gt;Mert Basmaci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jemiyo_C/0/1/0/all/0/1&quot;&gt;Christianah Jemiyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongqun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1&quot;&gt;Arzucan &amp;#xd6;zg&amp;#xfc;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1&quot;&gt;Junguk Hur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07699">
<title>A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07699</link>
<description rdf:parseType="Literal">&lt;p&gt;New intent discovery is of great value to natural language processing,
allowing for a better understanding of user needs and providing friendly
services. However, most existing methods struggle to capture the complicated
semantics of discrete text representations when limited or no prior knowledge
of labeled data is available. To tackle this problem, we propose a novel
clustering framework, USNID, for unsupervised and semi-supervised new intent
discovery, which has three key technologies. First, it fully utilizes
unsupervised or semi-supervised data to mine shallow semantic similarity
relations and provide well-initialized representations for clustering. Second,
it designs a centroid-guided clustering mechanism to address the issue of
cluster allocation inconsistency and provide high-quality self-supervised
targets for representation learning. Third, it captures high-level semantics in
unsupervised or semi-supervised data to discover fine-grained intent-wise
clusters by optimizing both cluster-level and instance-level objectives. We
also propose an effective method for estimating the cluster number in
open-world scenarios without knowing the number of new intents beforehand.
USNID performs exceptionally well on several benchmark intent datasets,
achieving new state-of-the-art results in unsupervised and semi-supervised new
intent discovery and demonstrating robust performance with different cluster
numbers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanlei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1&quot;&gt;Fei Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kai Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08485">
<title>Visual Instruction Tuning. (arXiv:2304.08485v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08485</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning large language models (LLMs) using machine-generated
instruction-following data has improved zero-shot capabilities on new tasks,
but the idea is less explored in the multimodal field. In this paper, we
present the first attempt to use language-only GPT-4 to generate multimodal
language-image instruction-following data. By instruction tuning on such
generated data, we introduce LLaVA: Large Language and Vision Assistant, an
end-to-end trained large multimodal model that connects a vision encoder and
LLM for general-purpose visual and language understanding.Our early experiments
show that LLaVA demonstrates impressive multimodel chat abilities, sometimes
exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and
yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
instruction-following dataset. When fine-tuned on Science QA, the synergy of
LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make
GPT-4 generated visual instruction tuning data, our model and code base
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haotian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10464">
<title>Learning to Plan with Natural Language. (arXiv:2304.10464v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10464</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown remarkable performance in various
basic natural language tasks. For completing the complex task, we still need a
plan for the task to guide LLMs to generate the specific solutions step by
step. LLMs can directly generate task plans, but these plans may still contain
factual errors or are incomplete. A high-quality task plan contains correct
step-by-step solutions for solving all situations and behavioral instructions
for avoiding mistakes. To obtain it, we propose the Learning to Plan method,
which involves two phases: (1) In the first learning task plan phase, it
iteratively updates the task plan with new step-by-step solutions and
behavioral instructions, which are obtained by prompting LLMs to derive from
training error feedback. (2) In the subsequent test phase, the LLM uses the
learned task plan to guide the inference of LLM on the test set. We demonstrate
the effectiveness of our method on the five different reasoning type tasks (8
datasets). Further, our analysis experiment shows that the task plan learned by
one LLM can directly guide another LLM to improve its performance, which
reveals a new transfer learning paradigm. We release the code at
\url{https://github.com/Eureka6174/LearnNLPlan}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiduo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yaobo Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03520">
<title>Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03520</link>
<description rdf:parseType="Literal">&lt;p&gt;The issue of word sense ambiguity poses a significant challenge in natural
language processing due to the scarcity of annotated data to feed machine
learning models to face the challenge. Therefore, unsupervised word sense
disambiguation methods have been developed to overcome that challenge without
relying on annotated data. This research proposes a new context-aware approach
to unsupervised word sense disambiguation, which provides a flexible mechanism
for incorporating contextual information into the similarity measurement
process. We experiment with a popular benchmark dataset to evaluate the
proposed strategy and compare its performance with state-of-the-art
unsupervised word sense disambiguation techniques. The experimental results
indicate that our approach substantially enhances disambiguation accuracy and
surpasses the performance of several existing techniques. Our findings
underscore the significance of integrating contextual information in semantic
similarity measurements to manage word sense ambiguity in unsupervised
scenarios effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1&quot;&gt;Jorge Martinez-Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06152">
<title>Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations. (arXiv:2305.06152v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06152</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale vision-language pre-training has achieved significant performance
in multi-modal understanding and generation tasks. However, existing methods
often perform poorly on image-text matching tasks that require structured
representations, i.e., representations of objects, attributes, and relations.
As illustrated in Fig.~reffig:case (a), the models cannot make a distinction
between ``An astronaut rides a horse&quot; and ``A horse rides an astronaut&quot;. This
is because they fail to fully leverage structured knowledge when learning
representations in multi-modal scenarios. In this paper, we present an
end-to-end framework Structure-CLIP, which integrates Scene Graph Knowledge
(SGK) to enhance multi-modal structured representations. Firstly, we use scene
graphs to guide the construction of semantic negative examples, which results
in an increased emphasis on learning structured representations. Moreover, a
Knowledge-Enhance Encoder (KEE) is proposed to leverage SGK as input to further
enhance structured representations. To verify the effectiveness of the proposed
framework, we pre-train our model with the aforementioned approaches and
conduct experiments on downstream tasks. Experimental results demonstrate that
Structure-CLIP achieves state-of-the-art (SOTA) performance on VG-Attribution
and VG-Relation datasets, with 12.5% and 4.1% ahead of the multi-modal SOTA
model respectively. Meanwhile, the results on MSCOCO indicate that
Structure-CLIP significantly enhances the structured representations while
maintaining the ability of general representations. Our code is available at
https://github.com/zjukg/Structure-CLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiji Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1&quot;&gt;Tangjie Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06161">
<title>StarCoder: may the source be with you!. (arXiv:2305.06161v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06161</link>
<description rdf:parseType="Literal">&lt;p&gt;The BigCode community, an open-scientific collaboration working on the
responsible development of Large Language Models for Code (Code LLMs),
introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context
length, infilling capabilities and fast large-batch inference enabled by
multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced
from The Stack, a large collection of permissively licensed GitHub repositories
with inspection tools and an opt-out process. We fine-tuned StarCoderBase on
35B Python tokens, resulting in the creation of StarCoder. We perform the most
comprehensive evaluation of Code LLMs to date and show that StarCoderBase
outperforms every open Code LLM that supports multiple programming languages
and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,
StarCoder outperforms every model that is fine-tuned on Python, can be prompted
to achieve 40\% pass@1 on HumanEval, and still retains its performance on other
programming languages. We take several important steps towards a safe
open-access model release, including an improved PII redaction pipeline and a
novel attribution tracing tool, and make the StarCoder models publicly
available under a more commercially viable version of the Open Responsible AI
Model license.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Raymond Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allal_L/0/1/0/all/0/1&quot;&gt;Loubna Ben Allal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zi_Y/0/1/0/all/0/1&quot;&gt;Yangtian Zi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocetkov_D/0/1/0/all/0/1&quot;&gt;Denis Kocetkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1&quot;&gt;Chenghao Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marone_M/0/1/0/all/0/1&quot;&gt;Marc Marone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1&quot;&gt;Christopher Akiki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1&quot;&gt;Jenny Chim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheltonozhskii_E/0/1/0/all/0/1&quot;&gt;Evgenii Zheltonozhskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1&quot;&gt;Terry Yue Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Thomas Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehaene_O/0/1/0/all/0/1&quot;&gt;Olivier Dehaene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davaadorj_M/0/1/0/all/0/1&quot;&gt;Mishig Davaadorj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1&quot;&gt;Joel Lamy-Poirier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Monteiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1&quot;&gt;Oleh Shliazhko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1&quot;&gt;Nicolas Gontier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1&quot;&gt;Nicholas Meade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zebaze_A/0/1/0/all/0/1&quot;&gt;Armel Zebaze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yee_M/0/1/0/all/0/1&quot;&gt;Ming-Ho Yee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umapathi_L/0/1/0/all/0/1&quot;&gt;Logesh Kumar Umapathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipkin_B/0/1/0/all/0/1&quot;&gt;Benjamin Lipkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oblokulov_M/0/1/0/all/0/1&quot;&gt;Muhtasham Oblokulov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiruo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1&quot;&gt;Rudra Murthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stillerman_J/0/1/0/all/0/1&quot;&gt;Jason Stillerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Siva Sankalp Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abulkhanov_D/0/1/0/all/0/1&quot;&gt;Dmitry Abulkhanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zocca_M/0/1/0/all/0/1&quot;&gt;Marco Zocca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1&quot;&gt;Manan Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_N/0/1/0/all/0/1&quot;&gt;Nour Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_U/0/1/0/all/0/1&quot;&gt;Urvashi Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Swayam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1&quot;&gt;Sasha Luccioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villegas_P/0/1/0/all/0/1&quot;&gt;Paulo Villegas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunakov_M/0/1/0/all/0/1&quot;&gt;Maxim Kunakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhdanov_F/0/1/0/all/0/1&quot;&gt;Fedor Zhdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_M/0/1/0/all/0/1&quot;&gt;Manuel Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tony Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timor_N/0/1/0/all/0/1&quot;&gt;Nadav Timor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jennifer Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlesinger_C/0/1/0/all/0/1&quot;&gt;Claire Schlesinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1&quot;&gt;Hailey Schoelkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebert_J/0/1/0/all/0/1&quot;&gt;Jan Ebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_T/0/1/0/all/0/1&quot;&gt;Tri Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1&quot;&gt;Mayank Mishra&lt;/a&gt;, et al. (15 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09515">
<title>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09515</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have gained significant attention in the realm of image
generation due to their exceptional performance. Their success has been
recently expanded to text generation via generating all tokens within a
sequence concurrently. However, natural language exhibits a far more pronounced
sequential dependency in comparison to images, and the majority of existing
language models are trained with a left-to-right auto-regressive approach. To
account for the inherent sequential characteristic of natural language, we
introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that
the generation of tokens on the right depends on the generated ones on the
left, a mechanism achieved through employing a dynamic number of denoising
steps that vary based on token position. This results in tokens on the left
undergoing fewer denoising steps than those on the right, thereby enabling them
to generate earlier and subsequently influence the generation of tokens on the
right. In a series of experiments on various text generation tasks, including
text summarization, machine translation, and common sense generation,
AR-Diffusion clearly demonstrated its superiority over existing diffusion
language models and that it can be $100\times\sim600\times$ faster when
achieving comparable results. Our code is available at
https://github.com/microsoft/ProphetNet/tree/master/AR-diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhihao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yeyun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yelong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jian Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juntao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weizhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18290">
<title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model. (arXiv:2305.18290v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18290</link>
<description rdf:parseType="Literal">&lt;p&gt;While large-scale unsupervised language models (LMs) learn broad world
knowledge and some reasoning skills, achieving precise control of their
behavior is difficult due to the completely unsupervised nature of their
training. Existing methods for gaining such steerability collect human labels
of the relative quality of model generations and fine-tune the unsupervised LM
to align with these preferences, often with reinforcement learning from human
feedback (RLHF). However, RLHF is a complex and often unstable procedure, first
fitting a reward model that reflects the human preferences, and then
fine-tuning the large unsupervised LM using reinforcement learning to maximize
this estimated reward without drifting too far from the original model. In this
paper we introduce a new parameterization of the reward model in RLHF that
enables extraction of the corresponding optimal policy in closed form, allowing
us to solve the standard RLHF problem with only a simple classification loss.
The resulting algorithm, which we call Direct Preference Optimization (DPO), is
stable, performant, and computationally lightweight, eliminating the need for
sampling from the LM during fine-tuning or performing significant
hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align
with human preferences as well as or better than existing methods. Notably,
fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of
generations, and matches or improves response quality in summarization and
single-turn dialogue while being substantially simpler to implement and train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1&quot;&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Archit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1&quot;&gt;Eric Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1&quot;&gt;Christopher D. Manning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03061">
<title>Structured Voronoi Sampling. (arXiv:2306.03061v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03061</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient-based sampling algorithms have demonstrated their effectiveness in
text generation, especially in the context of controlled text generation.
However, there exists a lack of theoretically grounded and principled
approaches for this task. In this paper, we take an important step toward
building a principled approach for sampling from language models with
gradient-based methods. We use discrete distributions given by language models
to define densities and develop an algorithm based on Hamiltonian Monte Carlo
to sample from them. We name our gradient-based technique Structured Voronoi
Sampling (SVS). In an experimental setup where the reference distribution is
known, we show that the empirical distribution of SVS samples is closer to the
reference distribution compared to alternative sampling schemes. Furthermore,
in a controlled generation task, SVS is able to generate fluent and diverse
samples while following the control targets significantly better than other
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1&quot;&gt;Afra Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Li Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08742">
<title>PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08742</link>
<description rdf:parseType="Literal">&lt;p&gt;Model editing techniques modify a minor proportion of knowledge in Large
Language Models (LLMs) at a relatively low cost, which have demonstrated
notable success. Existing methods assume Transformer Layer (TL) hidden states
are values of key-value memories of the Feed-Forward Network (FFN). They
usually optimize the TL hidden states to memorize target knowledge and use it
to update the weights of the FFN in LLMs. However, the information flow of TL
hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,
and residual connections. Existing methods neglect the fact that the TL hidden
states contains information not specifically required for FFN. Consequently,
the performance of model editing decreases. To achieve more precise model
editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes
certain general knowledge extraction patterns. This implies that MHSA weights
do not require updating when new knowledge is introduced. Based on above
findings, we introduce PMET, which simultaneously optimizes Transformer
Component (TC, namely MHSA and FFN) hidden states, while only using the
optimized TC hidden states of FFN to precisely update FFN weights. Our
experiments demonstrate that PMET exhibits state-of-the-art performance on both
the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the
effectiveness of our enhancements, further reinforcing the finding that the
MHSA encodes certain general knowledge extraction patterns and indicating its
storage of a small amount of factual knowledge. Our code is available at
https://github.com/xpq-tech/PMET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shasha Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shezheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jie Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02784">
<title>Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02784</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of large language models (LLMs) continues to grow, model
compression without sacrificing accuracy has become a crucial challenge for
deployment. While some quantization methods, such as GPTQ, have made progress
in achieving acceptable 4-bit weight-only quantization, attempts at lower-bit
quantization often result in severe performance degradation. In this paper, we
introduce a technique called norm tweaking, which can be used as a plugin in
current PTQ methods to achieve high precision while being cost-efficient. Our
approach is inspired by the observation that rectifying the quantized
activation distribution to match its float counterpart can readily restore
accuracy for LLMs. To achieve this, we carefully design a tweaking strategy
that includes calibration data generation and channel-wise distance constraint
to update the weights of normalization layers for better generalization. We
conduct extensive experiments on various datasets using several open-sourced
LLMs. Our method demonstrates significant improvements in both weight-only
quantization and joint quantization of weights and activations, surpassing
existing PTQ methods. On GLM-130B and OPT-66B, our method even achieves the
same level of accuracy at 2-bit quantization as their float ones. Our simple
and effective approach makes it more practical for real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01217">
<title>ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale. (arXiv:2310.01217v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01217</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) has shown considerable practical benefits,
particularly when using pre-trained language models (PLMs). While this is
commonly achieved by simultaneously learning $n$ tasks under a joint
optimization procedure, recent methods such as AdapterFusion structure the
problem into two distinct stages: (i) task learning, where knowledge specific
to a task is encapsulated within sets of parameters (e.g., adapters), and (ii)
transfer, where this already learned knowledge is leveraged for a target task.
This separation of concerns provides numerous benefits, such as promoting
reusability, and addressing cases involving data privacy and societal concerns;
on the flip side, current two-stage MTL methods come with the cost of
introducing a substantial number of additional parameters. In this work, we
address this issue by leveraging the usefulness of linearly scaling the output
representations of source adapters for transfer learning. We introduce
ScaLearn, a simple and highly parameter-efficient two-stage MTL method that
capitalizes on the knowledge of the source tasks by learning a minimal set of
scaling parameters that enable effective knowledge transfer to a target task.
Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our
ScaLearn, in addition to facilitating the benefits of two-stage MTL,
consistently outperforms strong baselines with only a small number of transfer
parameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe
that ScaLearn maintains its strong abilities even when further reducing
parameters through uniform scaling and layer-sharing, achieving similarly
competitive results with only $8$ transfer parameters for each target task. Our
proposed approach thus demonstrates the power of simple scaling as a promise
for more efficient task transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frohmann_M/0/1/0/all/0/1&quot;&gt;Markus Frohmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holtermann_C/0/1/0/all/0/1&quot;&gt;Carolin Holtermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masoudian_S/0/1/0/all/0/1&quot;&gt;Shahed Masoudian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1&quot;&gt;Anne Lauscher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1&quot;&gt;Navid Rekabsaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08320">
<title>Defending Our Privacy With Backdoors. (arXiv:2310.08320v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08320</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-&quot;a person&quot; instead of the person&apos;s name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new &quot;dual-use&quot;
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1&quot;&gt;Daniel Neider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10594">
<title>Motion2Language, unsupervised learning of synchronized semantic motion segmentation. (arXiv:2310.10594v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10594</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate building a sequence to sequence architecture
for motion to language translation and synchronization. The aim is to translate
motion capture inputs into English natural-language descriptions, such that the
descriptions are generated synchronously with the actions performed, enabling
semantic segmentation as a byproduct, but without requiring synchronized
training data. We propose a new recurrent formulation of local attention that
is suited for synchronous/live text generation, as well as an improved motion
encoder architecture better suited to smaller data and for synchronous
generation. We evaluate both contributions in individual experiments, using the
standard BLEU4 metric, as well as a simple semantic equivalence measure, on the
KIT motion language dataset. In a follow-up experiment, we assess the quality
of the synchronization of generated text in our proposed approaches through
multiple evaluation metrics. We find that both contributions to the attention
mechanism and the encoder architecture additively improve the quality of
generated text (BLEU and semantic equivalence), but also of synchronization.
Our code is available at
https://github.com/rd20karim/M2T-Segmentation/tree/main
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radouane_K/0/1/0/all/0/1&quot;&gt;Karim Radouane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tchechmedjiev_A/0/1/0/all/0/1&quot;&gt;Andon Tchechmedjiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagarde_J/0/1/0/all/0/1&quot;&gt;Julien Lagarde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranwez_S/0/1/0/all/0/1&quot;&gt;Sylvie Ranwez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05608">
<title>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts. (arXiv:2311.05608v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05608</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the safety of artificial intelligence-generated content (AIGC) is a
longstanding topic in the artificial intelligence (AI) community, and the
safety concerns associated with Large Language Models (LLMs) have been widely
investigated. Recently, large vision-language models (VLMs) represent an
unprecedented revolution, as they are built upon LLMs but can incorporate
additional modalities (e.g., images). However, the safety of VLMs lacks
systematic evaluation, and there may be an overconfidence in the safety
guarantees provided by their underlying LLMs. In this paper, to demonstrate
that introducing additional modality modules leads to unforeseen AI safety
issues, we propose FigStep, a straightforward yet effective jailbreaking
algorithm against VLMs. Instead of feeding textual harmful instructions
directly, FigStep converts the harmful content into images through typography
to bypass the safety alignment within the textual module of the VLMs, inducing
VLMs to output unsafe responses that violate common AI safety policies. In our
evaluation, we manually review 46,500 model responses generated by 3 families
of the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a total
of 6 VLMs). The experimental results show that FigStep can achieve an average
attack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, we
demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which
already leverages an OCR detector to filter harmful queries. Above all, our
work reveals that VLMs are vulnerable to jailbreaking attacks, which highlights
the necessity of novel safety alignments between visual and textual modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yichen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_D/0/1/0/all/0/1&quot;&gt;Delong Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Conglei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_T/0/1/0/all/0/1&quot;&gt;Tianshuo Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Anyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Sisi Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08836">
<title>Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English. (arXiv:2311.08836v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08836</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Translation (MT) continues to improve in quality and adoption, yet
the inadvertent perpetuation of gender bias remains a significant concern.
Despite numerous studies into gender bias in translations from gender-neutral
languages such as Turkish into more strongly gendered languages like English,
there are no benchmarks for evaluating this phenomenon or for assessing
mitigation strategies. To address this gap, we introduce GATE X-E, an extension
to the GATE (Rarrick et al., 2023) corpus, that consists of human translations
from Turkish, Hungarian, Finnish, and Persian into English. Each translation is
accompanied by feminine, masculine, and neutral variants for each possible
gender interpretation. The dataset, which contains between 1250 and 1850
instances for each of the four language pairs, features natural sentences with
a wide range of sentence lengths and domains, challenging translation rewriters
on various linguistic phenomena. Additionally, we present an English gender
rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We
open source our contributions to encourage further research on gender
debiasing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rarrick_S/0/1/0/all/0/1&quot;&gt;Spencer Rarrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_R/0/1/0/all/0/1&quot;&gt;Ranjita Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poudel_S/0/1/0/all/0/1&quot;&gt;Sundar Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhary_V/0/1/0/all/0/1&quot;&gt;Vishal Chowdhary&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>