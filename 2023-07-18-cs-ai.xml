<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1909.07750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.12689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.04486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.10998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.07533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.07433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07417" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.07513">
<title>An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07513</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: The predictive Intensive Care Unit (ICU) scoring system plays an
important role in ICU management because it predicts important outcomes,
especially mortality. Many scoring systems have been developed and used in the
ICU. These scoring systems are primarily based on the structured clinical data
in the electronic health record (EHR), which may suffer the loss of important
clinical information in the narratives and images. Methods: In this work, we
build a deep learning based survival prediction model with multi-modality data
to predict ICU mortality. Four sets of features are investigated: (1)
physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)
common thorax diseases pre-defined by radiologists, (3) BERT-based text
representations, and (4) chest X-ray image features. We use the Medical
Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the
proposed model. Results: Our model achieves the average C-index of 0.7829 (95%
confidence interval, 0.7620-0.8038), which substantially exceeds that of the
baseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies
further demonstrate the contributions of pre-defined labels (2.00%), text
features (2.44%), and image features (2.82%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingquan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Ying Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lihui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07514">
<title>Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07514</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable artificial intelligence (XAI) aims to help human decision-makers
in understanding complex machine learning (ML) models. One of the hallmarks of
XAI are measures of relative feature importance, which are theoretically
justified through the use of Shapley values. This paper builds on recent work
and offers a simple argument for why Shapley values can provide misleading
measures of relative feature importance, by assigning more importance to
features that are irrelevant for a prediction, and assigning less importance to
features that are relevant for a prediction. The significance of these results
is that they effectively challenge the many proposed uses of measures of
relative feature importance in a fast-growing range of high-stakes application
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1&quot;&gt;Joao Marques-Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanxiang Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07515">
<title>Artificial intelligence is algorithmic mimicry: why artificial &quot;agents&quot; are not (and won&apos;t be) proper agents. (arXiv:2307.07515v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07515</link>
<description rdf:parseType="Literal">&lt;p&gt;What is the prospect of developing artificial general intelligence (AGI)? I
investigate this question by systematically comparing living and algorithmic
systems, with a special focus on the notion of &quot;agency.&quot; There are three
fundamental differences to consider: (1) Living systems are autopoietic, that
is, self-manufacturing, and therefore able to set their own intrinsic goals,
while algorithms exist in a computational environment with target functions
that are both provided by an external agent. (2) Living systems are embodied in
the sense that there is no separation between their symbolic and physical
aspects, while algorithms run on computational architectures that maximally
isolate software from hardware. (3) Living systems experience a large world, in
which most problems are ill-defined (and not all definable), while algorithms
exist in a small world, in which all problems are well-defined. These three
differences imply that living and algorithmic systems have very different
capabilities and limitations. In particular, it is extremely unlikely that true
AGI (beyond mere mimicry) can be developed in the current algorithmic framework
of AI research. Consequently, discussions about the proper development and
deployment of algorithmic tools should be shaped around the dangers and
opportunities of current narrow AI, not the extremely unlikely prospect of the
emergence of true agency in artificial systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_J/0/1/0/all/0/1&quot;&gt;Johannes Jaeger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07517">
<title>Causing is Achieving -- A solution to the problem of causation. (arXiv:2307.07517v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07517</link>
<description rdf:parseType="Literal">&lt;p&gt;From the standpoint of applied ontology, the problem of understanding and
modeling causation has been recently challenged on the premise that causation
is real. As a consequence, the following three results were obtained: (1)
causation can be understood via the notion of systemic function; (2) any cause
can be decomposed using only four subfunctions, namely Achieves, Prevents,
Allows, and Disallows; and (3) the last three subfunctions can be defined in
terms of Achieves alone. It follows that the essence of causation lies in a
single function, namely Achieves. It remains to elucidate the nature of the
Achieves function, which has been elaborated only partially in the previous
work. In this paper, we first discuss a couple of underlying policies in the
above-mentioned causal theory since these are useful in the discussion, then
summarize the results obtained in the former paper, and finally reveal the
nature of Achieves giving a complete solution to the problem of what causation
is.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mizoguchi_R/0/1/0/all/0/1&quot;&gt;Riichiro Mizoguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07518">
<title>CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model. (arXiv:2307.07518v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07518</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale multimodal language models (LMMs) have achieved remarkable
success in general domains. However, the exploration of diagnostic language
models based on multimodal cephalometric medical data remains limited. In this
paper, we propose a novel multimodal cephalometric analysis and diagnostic
dialogue model. Firstly, a multimodal orthodontic medical dataset is
constructed, comprising cephalometric images and doctor-patient dialogue data,
with automatic analysis of cephalometric landmarks using U-net and generation
of diagnostic reports. Then, the cephalometric dataset and generated diagnostic
reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results
demonstrate that the CephGPT-4 model exhibits excellent performance and has the
potential to revolutionize orthodontic measurement and diagnostic applications.
These innovations hold revolutionary application potential in the field of
orthodontics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jincong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07522">
<title>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07522</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning and AI, including Generative AI and LLMs,
are disrupting technological innovation, product development, and society as a
whole. AI&apos;s contribution to technology can come from multiple approaches that
require access to large training data sets and clear performance evaluation
criteria, ranging from pattern recognition and classification to generative
models. Yet, AI has contributed less to fundamental science in part because
large data sets of high-quality data for scientific practice and model
discovery are more difficult to access. Generative AI, in general, and Large
Language Models in particular, may represent an opportunity to augment and
accelerate the scientific discovery of fundamental deep science with
quantitative models. Here we explore and investigate aspects of an AI-driven,
automated, closed-loop approach to scientific discovery, including self-driven
hypothesis generation and open-ended autonomous exploration of the hypothesis
space. Integrating AI-driven automation into the practice of science would
mitigate current problems, including the replication of findings, systematic
production of data, and ultimately democratisation of the scientific process.
Realising these possibilities requires a vision for augmented AI coupled with a
diversity of AI approaches able to deal with fundamental aspects of causality
analysis and model discovery while enabling unbiased search across the space of
putative explanations. These advances hold the promise to unleash AI&apos;s
potential for searching and discovering the fundamental structure of our world
beyond what human scientists have been able to achieve. Such a vision would
push the boundaries of new fundamental science rather than automatize current
workflows and instead open doors for technological innovation to tackle some of
the greatest challenges facing humanity today.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrahao_F/0/1/0/all/0/1&quot;&gt;Felipe S. Abrah&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1&quot;&gt;Alexander Lavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vipin Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frey_J/0/1/0/all/0/1&quot;&gt;Jeremy G. Frey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soldatova_L/0/1/0/all/0/1&quot;&gt;Larisa Soldatova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bundy_A/0/1/0/all/0/1&quot;&gt;Alan R. Bundy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jennings_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Jennings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1&quot;&gt;Koichi Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_L/0/1/0/all/0/1&quot;&gt;Lawrence Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1&quot;&gt;Saso Dzeroski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briggs_A/0/1/0/all/0/1&quot;&gt;Andrew Briggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregory_F/0/1/0/all/0/1&quot;&gt;Frederick D. Gregory&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1&quot;&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher K. I. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowe_J/0/1/0/all/0/1&quot;&gt;Jon Rowe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_J/0/1/0/all/0/1&quot;&gt;James Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitano_H/0/1/0/all/0/1&quot;&gt;Hiroaki Kitano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1&quot;&gt;Ross King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07523">
<title>PapagAI:Automated Feedback for Reflective Essays. (arXiv:2307.07523v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07523</link>
<description rdf:parseType="Literal">&lt;p&gt;Written reflective practice is a regular exercise pre-service teachers
perform during their higher education. Usually, their lecturers are expected to
provide individual feedback, which can be a challenging task to perform on a
regular basis. In this paper, we present the first open-source automated
feedback tool based on didactic theory and implemented as a hybrid AI system.
We describe the components and discuss the advantages and disadvantages of our
system compared to the state-of-art generative large language models. The main
objective of our work is to enable better learning outcomes for students and to
complement the teaching activities of lecturers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solopova_V/0/1/0/all/0/1&quot;&gt;Veronika Solopova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruszczynski_A/0/1/0/all/0/1&quot;&gt;Adrian Gruszczynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostom_E/0/1/0/all/0/1&quot;&gt;Eiad Rostom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremer_F/0/1/0/all/0/1&quot;&gt;Fritz Cremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witte_S/0/1/0/all/0/1&quot;&gt;Sascha Witte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plossl_F/0/1/0/all/0/1&quot;&gt;Fernando Ramos L&amp;#xf3;pez Lea Pl&amp;#xf6;&amp;#xdf;l&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1&quot;&gt;Florian Hofmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romeike_R/0/1/0/all/0/1&quot;&gt;Ralf Romeike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glaser_Zikuda_M/0/1/0/all/0/1&quot;&gt;Michaela Gl&amp;#xe4;ser-Zikuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1&quot;&gt;Christoph Benzm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landgraf_T/0/1/0/all/0/1&quot;&gt;Tim Landgraf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07524">
<title>Reducing Causality to Functions with Structural Models. (arXiv:2307.07524v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07524</link>
<description rdf:parseType="Literal">&lt;p&gt;The precise definition of causality is currently an open problem in
philosophy and statistics. We believe causality should be defined as functions
(in mathematics) that map causes to effects. We propose a reductive definition
of causality based on Structural Functional Model (SFM). Using delta
compression and contrastive forward inference, SFM can produce causal
utterances like &quot;X causes Y&quot; and &quot;X is the cause of Y&quot; that match our
intuitions. We compile a dataset of causal scenarios and use SFM in all of
them. SFM is compatible with but not reducible to probability theory. We also
compare SFM with other theories of causation and apply SFM to downstream
problems like free will, causal explanation, and mental causation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_T/0/1/0/all/0/1&quot;&gt;Tianyi Miao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07526">
<title>Can I say, now machines can think?. (arXiv:2307.07526v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07526</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI techniques have opened the path for new generations of machines
in diverse domains. These machines have various capabilities for example, they
can produce images, generate answers or stories, and write codes based on the
&quot;prompts&quot; only provided by users. These machines are considered &apos;thinking
minds&apos; because they have the ability to generate human-like responses. In this
study, we have analyzed and explored the capabilities of artificial
intelligence-enabled machines. We have revisited on Turing&apos;s concept of
thinking machines and compared it with recent technological advancements. The
objections and consequences of the thinking machines are also discussed in this
study, along with available techniques to evaluate machines&apos; cognitive
capabilities. We have concluded that Turing Test is a critical aspect of
evaluating machines&apos; ability. However, there are other aspects of intelligence
too, and AI machines exhibit most of these aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_N/0/1/0/all/0/1&quot;&gt;Nitisha Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_G/0/1/0/all/0/1&quot;&gt;Geetika Jain Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sanjeev Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pundir_A/0/1/0/all/0/1&quot;&gt;Amit Pundir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07527">
<title>Machine Learning for Autonomous Vehicle&apos;s Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions. (arXiv:2307.07527v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07527</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicles (AVs) have emerged as a promising solution by replacing
human drivers with advanced computer-aided decision-making systems. However,
for AVs to effectively navigate the road, they must possess the capability to
predict the future behavior of nearby traffic participants, similar to the
predictive driving abilities of human drivers. Building upon existing
literature is crucial to advance the field and develop a comprehensive
understanding of trajectory prediction methods in the context of automated
driving. To address this need, we have undertaken a comprehensive review that
focuses on trajectory prediction methods for AVs, with a particular emphasis on
machine learning techniques including deep learning and reinforcement
learning-based approaches. We have extensively examined over two hundred
studies related to trajectory prediction in the context of AVs. The paper
begins with an introduction to the general problem of predicting vehicle
trajectories and provides an overview of the key concepts and terminology used
throughout. After providing a brief overview of conventional methods, this
review conducts a comprehensive evaluation of several deep learning-based
techniques. Each method is summarized briefly, accompanied by a detailed
analysis of its strengths and weaknesses. The discussion further extends to
reinforcement learning-based methods. This article also examines the various
datasets and evaluation metrics that are commonly used in trajectory prediction
tasks. Encouraging an unbiased and objective discussion, we compare two major
learning processes, considering specific functional features. By identifying
challenges in the existing literature and outlining potential research
directions, this review significantly contributes to the advancement of
knowledge in the domain of AV trajectory prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharilya_V/0/1/0/all/0/1&quot;&gt;Vibha Bharilya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Neetesh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07528">
<title>PatchSorter: A High Throughput Deep Learning Digital Pathology Tool for Object Labeling. (arXiv:2307.07528v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of patterns associated with diagnosis, prognosis, and therapy
response in digital pathology images often requires intractable labeling of
large quantities of histological objects. Here we release an open-source
labeling tool, PatchSorter, which integrates deep learning with an intuitive
web interface. Using &amp;gt;100,000 objects, we demonstrate a &amp;gt;7x improvement in
labels per second over unaided labeling, with minimal impact on labeling
accuracy, thus enabling high-throughput labeling of large datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Walker_C/0/1/0/all/0/1&quot;&gt;Cedric Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Talawalla_T/0/1/0/all/0/1&quot;&gt;Tasneem Talawalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Toth_R/0/1/0/all/0/1&quot;&gt;Robert Toth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ambekar_A/0/1/0/all/0/1&quot;&gt;Akhil Ambekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rea_K/0/1/0/all/0/1&quot;&gt;Kien Rea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chamian_O/0/1/0/all/0/1&quot;&gt;Oswin Chamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Berezowska_S/0/1/0/all/0/1&quot;&gt;Sabina Berezowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rottenberg_S/0/1/0/all/0/1&quot;&gt;Sven Rottenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Madabhushi_A/0/1/0/all/0/1&quot;&gt;Anant Madabhushi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maillard_M/0/1/0/all/0/1&quot;&gt;Marie Maillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Barisoni_L/0/1/0/all/0/1&quot;&gt;Laura Barisoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Horlings_H/0/1/0/all/0/1&quot;&gt;Hugo Mark Horlings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Janowczyk_A/0/1/0/all/0/1&quot;&gt;Andrew Janowczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07529">
<title>Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07529</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel multi-agent reinforcement learning (MARL) method
to learn multiple coordinated agents under directed acyclic graph (DAG)
constraints. Unlike existing MARL approaches, our method explicitly exploits
the DAG structure between agents to achieve more effective learning
performance. Theoretically, we propose a novel surrogate value function based
on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a
lower bound of the optimal value function. Computationally, we propose a
practical training algorithm that exploits new notion of leader agent and
reward generator and distributor agent to guide the decomposed follower agents
to better explore the parameter space in environments with DAG constraints.
Empirically, we exploit four DAG environments including a real-world scheduling
for one of Intel&apos;s high volume packaging and test factory to benchmark our
methods and show it outperforms the other non-DAG approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jaeyeon Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Nital S. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiuqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ananthanarayanan_B/0/1/0/all/0/1&quot;&gt;Balakrishnan Ananthanarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauod_H/0/1/0/all/0/1&quot;&gt;Husam Dauod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juang_T/0/1/0/all/0/1&quot;&gt;Tzung-Han Juang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07542">
<title>Source-Free Domain Adaptation with Temporal Imputation for Time Series Data. (arXiv:2307.07542v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.07542</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a
labeled source domain to an unlabeled target domain without access to the
source domain data, preserving source domain privacy. Despite its prevalence in
visual applications, SFDA is largely unexplored in time series applications.
The existing SFDA methods that are mainly designed for visual applications may
fail to handle the temporal dynamics in time series, leading to impaired
adaptation performance. To address this challenge, this paper presents a simple
yet effective approach for source-free domain adaptation on time series data,
namely MAsk and imPUte (MAPU). First, to capture temporal information of the
source domain, our method performs random masking on the time series signals
while leveraging a novel temporal imputer to recover the original signal from a
masked version in the embedding space. Second, in the adaptation step, the
imputer network is leveraged to guide the target model to produce target
features that are temporally consistent with the source features. To this end,
our MAPU can explicitly account for temporal dependency during the adaptation
while avoiding the imputation in the noisy input space. Our method is the first
to handle temporal consistency in SFDA for time series data and can be
seamlessly equipped with other existing SFDA methods. Extensive experiments
conducted on three real-world time series datasets demonstrate that our MAPU
achieves significant performance gain over existing methods. Our code is
available at \url{https://github.com/mohamedr002/MAPU_SFDA_TS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ragab_M/0/1/0/all/0/1&quot;&gt;Mohamed Ragab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eldele_E/0/1/0/all/0/1&quot;&gt;Emadeldeen Eldele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07544">
<title>A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge. (arXiv:2307.07544v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07544</link>
<description rdf:parseType="Literal">&lt;p&gt;In healthcare, the ability to care for oneself is reflected in the
&quot;Activities of Daily Living (ADL),&quot; which serve as a measure of functional
ability (functioning). A lack of functioning may lead to poor living conditions
requiring personal care and assistance. To accurately identify those in need of
support, assistance programs continuously evaluate participants&apos; functioning
across various domains. However, the assessment process may encounter
consistency issues when multiple assessors with varying levels of expertise are
involved. Novice assessors, in particular, may lack the necessary preparation
for real-world interactions with participants. To address this issue, we
developed a dialogue system that simulates interactions between assessors and
individuals of varying functioning in a natural and reproducible way. The
dialogue system consists of two major modules, one for natural language
understanding (NLU) and one for natural language generation (NLG),
respectively. In order to generate responses consistent with the underlying
knowledge base, the dialogue system requires both an understanding of the
user&apos;s query and of biographical details of an individual being simulated. To
fulfill this requirement, we experimented with query classification and
generated responses based on those biographical details using some recently
released InstructGPT-like models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1&quot;&gt;Zhecheng Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finzel_R/0/1/0/all/0/1&quot;&gt;Raymond Finzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucke_M/0/1/0/all/0/1&quot;&gt;Michael Lucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dufresne_S/0/1/0/all/0/1&quot;&gt;Sheena Dufresne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gini_M/0/1/0/all/0/1&quot;&gt;Maria Gini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1&quot;&gt;Serguei Pakhomov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07628">
<title>Value-based Fast and Slow AI Nudging. (arXiv:2307.07628v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07628</link>
<description rdf:parseType="Literal">&lt;p&gt;Nudging is a behavioral strategy aimed at influencing people&apos;s thoughts and
actions. Nudging techniques can be found in many situations in our daily lives,
and these nudging techniques can targeted at human fast and unconscious
thinking, e.g., by using images to generate fear or the more careful and
effortful slow thinking, e.g., by releasing information that makes us reflect
on our choices. In this paper, we propose and discuss a value-based AI-human
collaborative framework where AI systems nudge humans by proposing decision
recommendations. Three different nudging modalities, based on when
recommendations are presented to the human, are intended to stimulate human
fast thinking, slow thinking, or meta-cognition. Values that are relevant to a
specific decision scenario are used to decide when and how to use each of these
nudging modalities. Examples of values are decision quality, speed, human
upskilling and learning, human agency, and privacy. Several values can be
present at the same time, and their priorities can vary over time. The
framework treats values as parameters to be instantiated in a specific decision
environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganapini_M/0/1/0/all/0/1&quot;&gt;Marianna B. Ganapini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabiano_F/0/1/0/all/0/1&quot;&gt;Francesco Fabiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horesh_L/0/1/0/all/0/1&quot;&gt;Lior Horesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1&quot;&gt;Andrea Loreggia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattei_N/0/1/0/all/0/1&quot;&gt;Nicholas Mattei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1&quot;&gt;Keerthiram Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pallagani_V/0/1/0/all/0/1&quot;&gt;Vishal Pallagani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1&quot;&gt;Francesca Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venable_B/0/1/0/all/0/1&quot;&gt;Brent Venable&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07636">
<title>Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance. (arXiv:2307.07636v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07636</link>
<description rdf:parseType="Literal">&lt;p&gt;While explainability is a desirable characteristic of increasingly complex
black-box models, modern explanation methods have been shown to be inconsistent
and contradictory. The semantics of explanations is not always fully understood
- to what extent do explanations &quot;explain&quot; a decision and to what extent do
they merely advocate for a decision? Can we help humans gain insights from
explanations accompanying correct predictions and not over-rely on incorrect
predictions advocated for by explanations? With this perspective in mind, we
introduce the notion of dissenting explanations: conflicting predictions with
accompanying explanations. We first explore the advantage of dissenting
explanations in the setting of model multiplicity, where multiple models with
similar performance may have different predictions. In such cases, providing
dissenting explanations could be done by invoking the explanations of
disagreeing models. Through a pilot study, we demonstrate that dissenting
explanations reduce overreliance on model predictions, without reducing overall
accuracy. Motivated by the utility of dissenting explanations we present both
global and local methods for their generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1&quot;&gt;Omer Reingold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Judy Hanwen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talati_A/0/1/0/all/0/1&quot;&gt;Aditi Talati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07645">
<title>Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07645</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying and understanding implicit attitudes toward food can help efforts
to mitigate social prejudice due to food&apos;s pervasive role as a marker of
cultural and ethnic identity. Stereotypes about food are a form of
microaggression that contribute to harmful public discourse that may in turn
perpetuate prejudice toward ethnic groups and negatively impact economic
outcomes for restaurants. Through careful linguistic analyses, we evaluate
social theories about attitudes toward immigrant cuisine in a large-scale study
of framing differences in 2.1M English language Yelp reviews of restaurants in
14 US states. Controlling for factors such as restaurant price and neighborhood
racial diversity, we find that immigrant cuisines are more likely to be framed
in objectifying and othering terms of authenticity (e.g., authentic,
traditional), exoticism (e.g., exotic, different), and prototypicality (e.g.,
typical, usual), but that non-Western immigrant cuisines (e.g., Indian,
Mexican) receive more othering than European cuisines (e.g., French, Italian).
We further find that non-Western immigrant cuisines are framed less positively
and as lower status, being evaluated in terms of affordability and hygiene.
Finally, we show that reviews generated by large language models (LLMs)
reproduce many of the same framing tendencies. Our results empirically
corroborate social theories of taste and gastronomic stereotyping, and reveal
linguistic processes by which such attitudes are reified.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yiwei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1&quot;&gt;Kristina Gligori&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1&quot;&gt;Dan Jurafsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07650">
<title>SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization. (arXiv:2307.07650v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07650</link>
<description rdf:parseType="Literal">&lt;p&gt;Wireless indoor localization has attracted significant amount of attention in
recent years. Using received signal strength (RSS) obtained from WiFi access
points (APs) for establishing fingerprinting database is a widely utilized
method in indoor localization. However, the time-variant problem for indoor
positioning systems is not well-investigated in existing literature. Compared
to conventional static fingerprinting, the dynamicallyreconstructed database
can adapt to a highly-changing environment, which achieves sustainability of
localization accuracy. To deal with the time-varying issue, we propose a
skeleton-assisted learning-based clustering localization (SALC) system,
including RSS-oriented map-assisted clustering (ROMAC), cluster-based online
database establishment (CODE), and cluster-scaled location estimation (CsLE).
The SALC scheme jointly considers similarities from the skeleton-based shortest
path (SSP) and the time-varying RSS measurements across the reference points
(RPs). ROMAC clusters RPs into different feature sets and therefore selects
suitable monitor points (MPs) for enhancing location estimation. Moreover, the
CODE algorithm aims for establishing adaptive fingerprint database to alleviate
the timevarying problem. Finally, CsLE is adopted to acquire the target
position by leveraging the benefits of clustering information and estimated
signal variations in order to rescale the weights fromweighted k-nearest
neighbors (WkNN) method. Both simulation and experimental results demonstrate
that the proposed SALC system can effectively reconstruct the fingerprint
database with an enhanced location estimation accuracy, which outperforms the
other existing schemes in the open literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_A/0/1/0/all/0/1&quot;&gt;An-Hung Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li-Hsiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chen-Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Chun-Jie Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kai-Ten Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07662">
<title>MPDIoU: A Loss for Efficient and Accurate Bounding Box Regression. (arXiv:2307.07662v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07662</link>
<description rdf:parseType="Literal">&lt;p&gt;Bounding box regression (BBR) has been widely used in object detection and
instance segmentation, which is an important step in object localization.
However, most of the existing loss functions for bounding box regression cannot
be optimized when the predicted box has the same aspect ratio as the
groundtruth box, but the width and height values are exactly different. In
order to tackle the issues mentioned above, we fully explore the geometric
features of horizontal rectangle and propose a novel bounding box similarity
comparison metric MPDIoU based on minimum point distance, which contains all of
the relevant factors considered in the existing loss functions, namely
overlapping or non-overlapping area, central points distance, and deviation of
width and height, while simplifying the calculation process. On this basis, we
propose a bounding box regression loss function based on MPDIoU, called LMPDIoU
. Experimental results show that the MPDIoU loss function is applied to
state-of-the-art instance segmentation (e.g., YOLACT) and object detection
(e.g., YOLOv7) model trained on PASCAL VOC, MS COCO, and IIIT5k outperforms
existing loss functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siliang_M/0/1/0/all/0/1&quot;&gt;Ma Siliang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_X/0/1/0/all/0/1&quot;&gt;Xu Yong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07666">
<title>Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07666</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust reinforcement learning (RL) aims to find a policy that optimizes the
worst-case performance in the face of uncertainties. In this paper, we focus on
action robust RL with the probabilistic policy execution uncertainty, in which,
instead of always carrying out the action specified by the policy, the agent
will take the action specified by the policy with probability $1-\rho$ and an
alternative adversarial action with probability $\rho$. We establish the
existence of an optimal policy on the action robust MDPs with probabilistic
policy execution uncertainty and provide the action robust Bellman optimality
equation for its solution. Furthermore, we develop Action Robust Reinforcement
Learning with Certificates (ARRLC) algorithm that achieves minimax optimal
regret and sample complexity. Furthermore, we conduct numerical experiments to
validate our approach&apos;s robustness, demonstrating that ARRLC outperforms
non-robust RL algorithms and converges faster than the robust TD algorithm in
the presence of action perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhihan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Lifeng Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07670">
<title>Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning. (arXiv:2307.07670v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07670</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the broad range of applications of multi-agent reinforcement learning
(MARL), understanding the effects of adversarial attacks against MARL model is
essential for the safe applications of this model. Motivated by this, we
investigate the impact of adversarial attacks on MARL. In the considered setup,
there is an exogenous attacker who is able to modify the rewards before the
agents receive them or manipulate the actions before the environment receives
them. The attacker aims to guide each agent into a target policy or maximize
the cumulative rewards under some specific reward function chosen by the
attacker, while minimizing the amount of manipulation on feedback and action.
We first show the limitations of the action poisoning only attacks and the
reward poisoning only attacks. We then introduce a mixed attack strategy with
both the action poisoning and the reward poisoning. We show that the mixed
attack strategy can efficiently attack MARL agents even if the attacker has no
prior information about the underlying environment and the agents&apos; algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Lifeng Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07686">
<title>Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.07686</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a novel dataset for training machine learning
models translating between OpenMP Fortran and C++ code. To ensure reliability
and applicability, the dataset is initially refined using a meticulous code
similarity test. The effectiveness of our dataset is assessed using both
quantitative (CodeBLEU) and qualitative (human evaluation) methods. We
demonstrate how this dataset can significantly improve the translation
capabilities of large-scale language models, with improvements of \times 5.1
for models with no prior coding knowledge and \times 9.9 for models with some
coding familiarity. Our work highlights the potential of this dataset to
advance the field of code translation for high-performance computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1&quot;&gt;Bin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Le Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Pei-Hung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chunhua Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07691">
<title>A Survey on Change Detection Techniques in Document Images. (arXiv:2307.07691v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07691</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of change detection in images finds application in different
domains like diagnosis of diseases in the medical field, detecting growth
patterns of cities through remote sensing, and finding changes in legal
documents and contracts. However, this paper presents a survey on core
techniques and rules to detect changes in different versions of a document
image. Our discussions on change detection focus on two categories --
content-based and layout-based. The content-based techniques intelligently
extract and analyze the image contents (text or non-text) to show the possible
differences, whereas the layout-based techniques use structural information to
predict document changes. We also summarize the existing datasets and
evaluation metrics used in change detection experiments. The shortcomings and
challenges the existing methods face are reported, along with some pointers for
future research work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_A/0/1/0/all/0/1&quot;&gt;Abhinandan Kumar Pun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David S. Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07696">
<title>Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07696</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs), such as GPT-3, appear to be robust and
general, their reasoning ability is not at a level to compete with the best
models trained for specific natural language reasoning problems. In this study,
we observe that a large language model can serve as a highly effective few-shot
semantic parser. It can convert natural language sentences into a logical form
that serves as input for answer set programs, a logic-based declarative
knowledge representation formalism. The combination results in a robust and
general system that can handle multiple question-answering tasks without
requiring retraining for each new task. It only needs a few examples to guide
the LLM&apos;s adaptation to a specific task, along with reusable ASP knowledge
modules that can be applied to multiple tasks. We demonstrate that this method
achieves state-of-the-art performance on several NLP benchmarks, including
bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot
planning tasks that an LLM alone fails to solve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1&quot;&gt;Adam Ishay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07699">
<title>Leveraging Large Language Models to Generate Answer Set Programs. (arXiv:2307.07699v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07699</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1&quot;&gt;Adam Ishay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07700">
<title>NeurASP: Embracing Neural Networks into Answer Set Programming. (arXiv:2307.07700v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07700</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NeurASP, a simple extension of answer set programs by embracing
neural networks. By treating the neural network output as the probability
distribution over atomic facts in answer set programs, NeurASP provides a
simple and effective way to integrate sub-symbolic and symbolic computation. We
demonstrate how NeurASP can make use of a pre-trained neural network in
symbolic computation and how it can improve the neural network&apos;s perception
result by applying symbolic reasoning in answer set programming. Also, NeurASP
can be used to train a neural network better by training with ASP rules so that
a neural network not only learns from implicit correlations from the data but
also from the explicit complex semantic constraints expressed by the rules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1&quot;&gt;Adam Ishay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07734">
<title>Abstracting Concept-Changing Rules for Solving Raven&apos;s Progressive Matrix Problems. (arXiv:2307.07734v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07734</link>
<description rdf:parseType="Literal">&lt;p&gt;The abstract visual reasoning ability in human intelligence benefits
discovering underlying rules in the novel environment. Raven&apos;s Progressive
Matrix (RPM) is a classic test to realize such ability in machine intelligence
by selecting from candidates. Recent studies suggest that solving RPM in an
answer-generation way boosts a more in-depth understanding of rules. However,
existing generative solvers cannot discover the global concept-changing rules
without auxiliary supervision (e.g., rule annotations and distractors in
candidate sets). To this end, we propose a deep latent variable model for
Concept-changing Rule ABstraction (CRAB) by learning interpretable concepts and
parsing concept-changing rules in the latent space. With the iterative learning
process, CRAB can automatically abstract global rules shared on the dataset on
each concept and form the learnable prior knowledge of global rules. CRAB
outperforms the baselines trained without auxiliary supervision in the
arbitrary-position answer generation task and achieves comparable and even
higher accuracy than the compared models trained with auxiliary supervision.
Finally, we conduct experiments to illustrate the interpretability of CRAB in
concept learning, answer selection, and global rule abstraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Fan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07742">
<title>SINC: Self-Supervised In-Context Learning for Vision-Language Tasks. (arXiv:2307.07742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07742</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Pre-trained Transformers exhibit an intriguing capacity for in-context
learning. Without gradient updates, these models can rapidly construct new
predictors from demonstrations presented in the inputs. Recent works promote
this ability in the vision-language domain by incorporating visual information
into large language models that can already make in-context predictions.
However, these methods could inherit issues in the language domain, such as
template sensitivity and hallucination. Also, the scale of these language
models raises a significant demand for computations, making learning and
operating these models resource-intensive. To this end, we raise a question:
``How can we enable in-context learning for general models without being
constrained on large language models?&quot;. To answer it, we propose a succinct and
general framework, Self-supervised IN-Context learning (SINC), that introduces
a meta-model to learn on self-supervised prompts consisting of tailored
demonstrations. The learned models can be transferred to downstream tasks for
making in-context predictions on-the-fly. Extensive experiments show that SINC
outperforms gradient-based methods in various vision-language tasks under
few-shot settings. Furthermore, the designs of SINC help us investigate the
benefits of in-context learning across different tasks, and the analysis
further reveals the essential components for the emergence of in-context
learning in the vision-language domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Syuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yun-Zhu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1&quot;&gt;Cheng Yu Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jianlong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1&quot;&gt;Hong-Han Shuai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07752">
<title>Combining model-predictive control and predictive reinforcement learning for stable quadrupedal robot locomotion. (arXiv:2307.07752v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07752</link>
<description rdf:parseType="Literal">&lt;p&gt;Stable gait generation is a crucial problem for legged robot locomotion as
this impacts other critical performance factors such as, e.g. mobility over an
uneven terrain and power consumption. Gait generation stability results from
the efficient control of the interaction between the legged robot&apos;s body and
the environment where it moves. Here, we study how this can be achieved by a
combination of model-predictive and predictive reinforcement learning
controllers. Model-predictive control (MPC) is a well-established method that
does not utilize any online learning (except for some adaptive variations) as
it provides a convenient interface for state constraints management.
Reinforcement learning (RL), in contrast, relies on adaptation based on pure
experience. In its bare-bone variants, RL is not always suitable for robots due
to their high complexity and expensive simulation/experimentation. In this
work, we combine both control methods to address the quadrupedal robot stable
gate generation problem. The hybrid approach that we develop and apply uses a
cost roll-out algorithm with a tail cost in the form of a Q-function modeled by
a neural network; this allows to alleviate the computational complexity, which
grows exponentially with the prediction horizon in a purely MPC approach. We
demonstrate that our RL gait controller achieves stable locomotion at short
horizons, where a nominal MP controller fails. Further, our controller is
capable of live operation, meaning that it does not require previous training.
Our results suggest that the hybridization of MPC with RL, as presented here,
is beneficial to achieve a good balance between online control capabilities and
computational complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalev_V/0/1/0/all/0/1&quot;&gt;Vyacheslav Kovalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shkromada_A/0/1/0/all/0/1&quot;&gt;Anna Shkromada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouerdane_H/0/1/0/all/0/1&quot;&gt;Henni Ouerdane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osinenko_P/0/1/0/all/0/1&quot;&gt;Pavel Osinenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07753">
<title>Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks. (arXiv:2307.07753v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07753</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel prior learning method for advancing
generalization and uncertainty estimation in deep neural networks. The key idea
is to exploit scalable and structured posteriors of neural networks as
informative priors with generalization guarantees. Our learned priors provide
expressive probabilistic representations at large scale, like Bayesian
counterparts of pre-trained models on ImageNet, and further produce non-vacuous
generalization bounds. We also extend this idea to a continual learning
framework, where the favorable properties of our priors are desirable. Major
enablers are our technical contributions: (1) the sums-of-Kronecker-product
computations, and (2) the derivations and optimizations of tractable objectives
that lead to improved generalization bounds. Empirically, we exhaustively show
the effectiveness of this method for uncertainty estimation and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnaus_D/0/1/0/all/0/1&quot;&gt;Dominik Schnaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jongseok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1&quot;&gt;Rudolph Triebel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07754">
<title>Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07754</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-based human pose transfer is a video-to-video generation task that
animates a plain source human image based on a series of target human poses.
Considering the difficulties in transferring highly structural patterns on the
garments and discontinuous poses, existing methods often generate
unsatisfactory results such as distorted textures and flickering artifacts. To
address these issues, we propose a novel Deformable Motion Modulation (DMM)
that utilizes geometric kernel offset with adaptive weight modulation to
simultaneously perform feature alignment and style transfer. Different from
normal style modulation used in style transfer, the proposed modulation
mechanism adaptively reconstructs smoothed frames from style codes according to
the object shape through an irregular receptive field of view. To enhance the
spatio-temporal consistency, we leverage bidirectional propagation to extract
the hidden motion information from a warped image sequence generated by noisy
poses. The proposed feature propagation significantly enhances the motion
prediction ability by forward and backward propagation. Both quantitative and
qualitative experimental results demonstrate superiority over the
state-of-the-arts in terms of image fidelity and visual continuity. The source
code is publicly available at github.com/rocketappslab/bdmm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wing-Yin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1&quot;&gt;Lai-Man Po&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_R/0/1/0/all/0/1&quot;&gt;Ray Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yu Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07764">
<title>Explainable AI with counterfactual paths. (arXiv:2307.07764v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07764</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable AI (XAI) is an increasingly important area of research in machine
learning, which in principle aims to make black-box models transparent and
interpretable. In this paper, we propose a novel approach to XAI that uses
counterfactual paths generated by conditional permutations. Our method provides
counterfactual explanations by identifying alternative paths that could have
led to different outcomes. The proposed method is particularly suitable for
generating explanations based on counterfactual paths in knowledge graphs. By
examining hypothetical changes to the input data in the knowledge graph, we can
systematically validate the behaviour of the model and examine the features or
combination of features that are most important to the model&apos;s predictions. Our
approach provides a more intuitive and interpretable explanation for the
model&apos;s behaviour than traditional feature weighting methods and can help
identify and mitigate biases in the model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeifer_B/0/1/0/all/0/1&quot;&gt;Bastian Pfeifer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krzyzinski_M/0/1/0/all/0/1&quot;&gt;Mateusz Krzyzinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baniecki_H/0/1/0/all/0/1&quot;&gt;Hubert Baniecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saranti_A/0/1/0/all/0/1&quot;&gt;Anna Saranti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzinger_A/0/1/0/all/0/1&quot;&gt;Andreas Holzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07781">
<title>Improving Trace Link Recommendation by Using Non-Isotropic Distances and Combinations. (arXiv:2307.07781v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.07781</link>
<description rdf:parseType="Literal">&lt;p&gt;The existence of trace links between artifacts of the software development
life cycle can improve the efficiency of many activities during software
development, maintenance and operations. Unfortunately, the creation and
maintenance of trace links is time-consuming and error-prone. Research efforts
have been spent to automatically compute trace links and lately gained
momentum, e.g., due to the availability of powerful tools in the area of
natural language processing. In this paper, we report on some observations that
we made during studying non-linear similarity measures for computing trace
links. We argue, that taking a geometric viewpoint on semantic similarity can
be helpful for future traceability research. We evaluated our observations on a
dataset of four open source projects and two industrial projects. We
furthermore point out that our findings are more general and can build the
basis for other information retrieval problems as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tinnes_C/0/1/0/all/0/1&quot;&gt;Christof Tinnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07832">
<title>MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07832</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have received increasing attention due to their
ability to learn from graph-structured data. However, their predictions are
often not interpretable. Post-hoc instance-level explanation methods have been
proposed to understand GNN predictions. These methods seek to discover
substructures that explain the prediction behavior of a trained GNN. In this
paper, we shed light on the existence of the distribution shifting issue in
existing methods, which affects explanation quality, particularly in
applications on real-life datasets with tight decision boundaries. To address
this issue, we introduce a generalized Graph Information Bottleneck (GIB) form
that includes a label-independent graph variable, which is equivalent to the
vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method,
MixupExplainer, with a theoretical guarantee to resolve the distribution
shifting issue. We conduct extensive experiments on both synthetic and
real-world datasets to validate the effectiveness of our proposed mixup
approach over existing approaches. We also provide a detailed analysis of how
our proposed approach alleviates the distribution shifting issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongsheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07840">
<title>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07840</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph regression is a fundamental task and has received increasing attention
in a wide range of graph learning tasks. However, the inference process is
often not interpretable. Most existing explanation techniques are limited to
understanding GNN behaviors in classification tasks. In this work, we seek an
explanation to interpret the graph regression models (XAIG-R). We show that
existing methods overlook the distribution shifting and continuously ordered
decision boundary, which hinders them away from being applied in the regression
tasks. To address these challenges, we propose a novel objective based on the
information bottleneck theory and introduce a new mix-up framework, which could
support various GNNs in a model-agnostic manner. We further present a
contrastive learning strategy to tackle the continuously ordered labels in
regression task. To empirically verify the effectiveness of the proposed
method, we introduce three benchmark datasets and a real-life dataset for
evaluation. Extensive experiments show the effectiveness of the proposed method
in interpreting GNN models in regression tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuomin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongsheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07846">
<title>AIOptimizer -- A reinforcement learning-based software performance optimisation prototype for cost minimisation. (arXiv:2307.07846v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.07846</link>
<description rdf:parseType="Literal">&lt;p&gt;This research article introduces AIOptimizer, a prototype for a software
performance optimisation tool based on cost reduction. AIOptimizer uses a
recommendation system driven by reinforcement learning to improve software
system efficiency and affordability. The paper highlights AIOptimizer&apos;s design
factors, such as accuracy, adaptability, scalability, and user-friendliness. To
provide effective and user-centric performance optimisation solutions, it
emphasises the use of a modular design, data gathering techniques, continuous
learning, and resilient integration. The article also investigates AIOptimizer
features such as fault identification, cost optimisation recommendations,
efficiency prediction, and cooperation. Furthermore, it explores several
software development life cycle models and introduces AIOptimizer uses a
reinforcement learning-based recommendation engine for cost optimisation. The
purpose of this research study is to highlight AIOptimizer as a prototype that
uses advanced optimisation techniques and smart recommendation systems to
continually enhance software performance and save expenses. The research
focuses on various software development life cycle models, such as the
Waterfall model, Iterative model, Spiral model, V-Model, Big Bang model and
Agile Model. Each model has advantages and disadvantages, and their usefulness
is determined by the project&apos;s specifications and characteristics. The
AIOptimizer tool is a theoretical prototype for such software performance
optimizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambare_N/0/1/0/all/0/1&quot;&gt;Noopur Zambare&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07851">
<title>AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07851</link>
<description rdf:parseType="Literal">&lt;p&gt;Generic sentence embeddings provide a coarse-grained approximation of
semantic textual similarity but ignore specific aspects that make texts
similar. Conversely, aspect-based sentence embeddings provide similarities
between texts based on certain predefined aspects. Thus, similarity predictions
of texts are more targeted to specific requirements and more easily
explainable. In this paper, we present AspectCSE, an approach for aspect-based
contrastive learning of sentence embeddings. Results indicate that AspectCSE
achieves an average improvement of 3.97% on information retrieval tasks across
multiple aspects compared to the previous best results. We also propose using
Wikidata knowledge graph properties to train models of multi-aspect sentence
embeddings in which multiple specific aspects are simultaneously considered
during similarity predictions. We demonstrate that multi-aspect embeddings
outperform single-aspect embeddings on aspect-specific information retrieval
tasks. Finally, we examine the aspect-based sentence embedding space and
demonstrate that embeddings of semantically similar aspect labels are often
close, even without explicit similarity training between different aspect
labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1&quot;&gt;Tim Schopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerber_E/0/1/0/all/0/1&quot;&gt;Emanuel Gerber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostendorff_M/0/1/0/all/0/1&quot;&gt;Malte Ostendorff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1&quot;&gt;Florian Matthes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07857">
<title>A Multi-Heuristic Search-based Motion Planning for Automated Parking. (arXiv:2307.07857v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07857</link>
<description rdf:parseType="Literal">&lt;p&gt;In unstructured environments like parking lots or construction sites, due to
the large search-space and kinodynamic constraints of the vehicle, it is
challenging to achieve real-time planning. Several state-of-the-art planners
utilize heuristic search-based algorithms. However, they heavily rely on the
quality of the single heuristic function, used to guide the search. Therefore,
they are not capable to achieve reasonable computational performance, resulting
in unnecessary delays in the response of the vehicle. In this work, we are
adopting a Multi-Heuristic Search approach, that enables the use of multiple
heuristic functions and their individual advantages to capture different
complexities of a given search space. Based on our knowledge, this approach was
not used previously for this problem. For this purpose, multiple admissible and
non-admissible heuristic functions are defined, the original Multi-Heuristic A*
Search was extended for bidirectional use and dealing with hybrid
continuous-discrete search space, and a mechanism for adapting scale of motion
primitives is introduced. To demonstrate the advantage, the Multi-Heuristic A*
algorithm is benchmarked against a very popular heuristic search-based
algorithm, Hybrid A*. The Multi-Heuristic A* algorithm outperformed baseline in
both terms, computation efficiency and motion plan (path) quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adabala_B/0/1/0/all/0/1&quot;&gt;Bhargav Adabala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajanovic_Z/0/1/0/all/0/1&quot;&gt;Zlatan Ajanovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07863">
<title>Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans. (arXiv:2307.07863v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;Plant breeders and agricultural researchers can increase crop productivity by
identifying desirable features, disease resistance, and nutritional content by
analysing the Dry Bean dataset. This study analyses and compares different
Support Vector Machine (SVM) classification algorithms, namely linear,
polynomial, and radial basis function (RBF), along with other popular
classification algorithms. The analysis is performed on the Dry Bean Dataset,
with PCA (Principal Component Analysis) conducted as a preprocessing step for
dimensionality reduction. The primary evaluation metric used is accuracy, and
the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision
of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept
visualization and empirical analysis, this study offers valuable guidance by
emphasizing the importance of considering different SVM algorithms for complex
and non-linear structured datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Anant Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_P/0/1/0/all/0/1&quot;&gt;Prajit Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1&quot;&gt;Divisha Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1&quot;&gt;Harpreet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diamand_Y/0/1/0/all/0/1&quot;&gt;Yosi Shacham Diamand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07870">
<title>Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07870</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model&apos;s affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models&apos; drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1&quot;&gt;Grgur Kova&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1&quot;&gt;Masataka Sawayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Portelas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1&quot;&gt;Peter Ford Dominey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07871">
<title>The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;Developmental psychologists have long-established the importance of
socio-cognitive abilities in human intelligence. These abilities enable us to
enter, participate and benefit from human culture. AI research on social
interactive agents mostly concerns the emergence of culture in a multi-agent
setting (often without a strong grounding in developmental psychology). We
argue that AI research should be informed by psychology and study
socio-cognitive abilities enabling to enter a culture too. We discuss the
theories of Michael Tomasello and Jerome Bruner to introduce some of their
concepts to AI and outline key concepts and socio-cognitive abilities. We
present The SocialAI school - a tool including a customizable parameterized
uite of procedurally generated environments, which simplifies conducting
experiments regarding those concepts. We show examples of such experiments with
RL agents and Large Language Models. The main motivation of this work is to
engage the AI community around the problem of social intelligence informed by
developmental psychology, and to provide a tool to simplify first steps in this
direction. Refer to the project website for code and additional information:
https://sites.google.com/view/socialai-school.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1&quot;&gt;Grgur Kova&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Portelas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1&quot;&gt;Peter Ford Dominey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07872">
<title>Does Double Descent Occur in Self-Supervised Learning?. (arXiv:2307.07872v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07872</link>
<description rdf:parseType="Literal">&lt;p&gt;Most investigations into double descent have focused on supervised models
while the few works studying self-supervised settings find a surprising lack of
the phenomenon. These results imply that double descent may not exist in
self-supervised models. We show this empirically using a standard and linear
autoencoder, two previously unstudied settings. The test loss is found to have
either a classical U-shape or to monotonically decrease instead of exhibiting a
double-descent curve. We hope that further work on this will help elucidate the
theoretical underpinnings of this phenomenon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupidi_A/0/1/0/all/0/1&quot;&gt;Alisia Lupidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gideoni_Y/0/1/0/all/0/1&quot;&gt;Yonatan Gideoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayalath_D/0/1/0/all/0/1&quot;&gt;Dulhan Jayalath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07876">
<title>Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation. (arXiv:2307.07876v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07876</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent work on online goal recognition efficiently infers goals under
low observability, comparatively less work focuses on online goal recognition
that works in both discrete and continuous domains. Online goal recognition
approaches often rely on repeated calls to the planner at each new observation,
incurring high computational costs. Recognizing goals online in continuous
space quickly and reliably is critical for any trajectory planning problem
since the real physical world is fast-moving, e.g. robot applications. We
develop an efficient method for goal recognition that relies either on a single
call to the planner for each possible goal in discrete domains or a simplified
motion model that reduces the computational burden in continuous ones. The
resulting approach performs the online component of recognition orders of
magnitude faster than the current state of the art, making it the first online
method effectively usable for robotics applications that require sub-second
recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tesch_D/0/1/0/all/0/1&quot;&gt;Douglas Tesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amado_L/0/1/0/all/0/1&quot;&gt;Leonardo Rosa Amado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneguzzi_F/0/1/0/all/0/1&quot;&gt;Felipe Meneguzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07887">
<title>Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;While analyzing scanned documents, handwritten text can overlay printed text.
This causes difficulties during the optical character recognition (OCR) and
digitization process of documents, and subsequently, hurts downstream NLP
tasks. Prior research either focuses only on the binary classification of
handwritten text, or performs a three-class segmentation of the document, i.e.,
recognition of handwritten, printed, and background pixels. This results in the
assignment of the handwritten and printed overlapping pixels to only one of the
classes, and thus, they are not accounted for in the other class. Thus, in this
research, we develop novel approaches for addressing the challenges of
handwritten and printed text segmentation with the goal of recovering text in
different classes in whole, especially improving the segmentation performance
on the overlapping parts. As such, to facilitate with this task, we introduce a
new dataset, SignaTR6K, collected from real legal documents, as well as a new
model architecture for handwritten and printed text segmentation task. Our best
configuration outperforms the prior work on two different datasets by 17.9% and
7.3% on IoU scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholamian_S/0/1/0/all/0/1&quot;&gt;Sina Gholamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1&quot;&gt;Ali Vahdat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07893">
<title>Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07893</link>
<description rdf:parseType="Literal">&lt;p&gt;Current defect detection systems for Automated Fibre Placement (AFP) are
mostly based on end-to-end supervised learning methods requiring abundant
labelled defective samples, which are not easily generated in sufficient
numbers. To address this data scarcity problem, we introduce an
autoencoder-based approach compatible with small datasets. Fortunately, the
problem from a foundational point of view can be simplified as a binary
classification between normal and abnormal samples. The proposed approach uses
a depth map of the fibre layup surface, split into small windows aligned to
each composite strip (tow). A subset of these windows that do not contain
anomalies is passed to an autoencoder to reconstruct the input. Because the
autoencoder is trained with normal samples, it produces more accurate
reconstructions for these samples than for abnormal ones. Therefore, the value
of reconstruction error is used as a quantitative metric for whether there are
potential anomalies. These values are combined to produce an anomaly map, which
can localize the manufacturing defects in the depth map. The results show that
although the autoencoder is trained with a very limited number of scans, the
proposed approach can produce sufficient binary classification accuracy and
specify the location of the defects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_A/0/1/0/all/0/1&quot;&gt;Assef Ghamisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charter_T/0/1/0/all/0/1&quot;&gt;Todd Charter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Li Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivard_M/0/1/0/all/0/1&quot;&gt;Maxime Rivard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lund_G/0/1/0/all/0/1&quot;&gt;Gil Lund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07909">
<title>Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training. (arXiv:2307.07909v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07909</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DualMind, a generalist agent designed to tackle various
decision-making tasks that addresses challenges posed by current methods, such
as overfitting behaviors and dependence on task-specific fine-tuning. DualMind
uses a novel &quot;Dual-phase&quot; training strategy that emulates how humans learn to
act in the world. The model first learns fundamental common knowledge through a
self-supervised objective tailored for control tasks and then learns how to
make decisions based on different contexts through imitating behaviors
conditioned on given prompts. DualMind can handle tasks across domains, scenes,
and embodiments using just a single set of model weights and can execute
zero-shot prompting without requiring task-specific fine-tuning. We evaluate
DualMind on MetaWorld and Habitat through extensive experiments and demonstrate
its superior generalizability compared to previous techniques, outperforming
other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld,
respectively. On the 45 tasks in MetaWorld, DualMind achieves over 30 tasks at
a 90$\%$ success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanchao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Ruijie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1&quot;&gt;Sai Vemprala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1&quot;&gt;Rogerio Bonatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaan_R/0/1/0/all/0/1&quot;&gt;Ratnesh Madaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1&quot;&gt;Ashish Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07919">
<title>Neural Architecture Retrieval. (arXiv:2307.07919v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07919</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing number of new neural architecture designs and substantial
existing neural architectures, it becomes difficult for the researchers to
situate their contributions compared with existing neural architectures or
establish the connections between their designs and other relevant ones. To
discover similar neural architectures in an efficient and automatic manner, we
define a new problem Neural Architecture Retrieval which retrieves a set of
existing neural architectures which have similar designs to the query neural
architecture. Existing graph pre-training strategies cannot address the
computational graph in neural architectures due to the graph size and motifs.
To fulfill this potential, we propose to divide the graph into motifs which are
used to rebuild the macro graph to tackle these issues, and introduce
multi-level contrastive learning to achieve accurate graph representation
learning. Extensive evaluations on both human-designed and synthesized neural
architectures demonstrate the superiority of our algorithm. Such a dataset
which contains 12k real-world network architectures, as well as their
embedding, is built for neural architecture retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1&quot;&gt;Xiaohuan Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Minjing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07930">
<title>GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT. (arXiv:2307.07930v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07930</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-makers in GIS need to combine a series of spatial algorithms and
operations to solve geospatial tasks. For example, in the task of facility
siting, the Buffer tool is usually first used to locate areas close or away
from some specific entities; then, the Intersect or Erase tool is used to
select candidate areas satisfied multiple requirements. Though professionals
can easily understand and solve these geospatial tasks by sequentially
utilizing relevant tools, it is difficult for non-professionals to handle these
problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents
strong performance in semantic understanding and reasoning. Especially, AutoGPT
can further extend the capabilities of large language models (LLMs) by
automatically reasoning and calling externally defined tools. Inspired by these
studies, we attempt to lower the threshold of non-professional users to solve
geospatial tasks by integrating the semantic understanding ability inherent in
LLMs with mature tools within the GIS community. Specifically, we develop a new
framework called GeoGPT that can conduct geospatial data collection,
processing, and analysis in an autonomous manner with the instruction of only
natural language. In other words, GeoGPT is used to understand the demands of
non-professional users merely based on input natural language descriptions, and
then think, plan, and execute defined GIS tools to output final effective
results. Several cases including geospatial data crawling, spatial query,
facility siting, and mapping validate the effectiveness of our framework.
Though limited cases are presented in this paper, GeoGPT can be further
extended to various tasks by equipping with more GIS tools, and we think the
paradigm of &quot;foundational plus professional&quot; implied in GeoGPT provides an
effective way to develop next-generation GIS in this era of large foundation
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Cheng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shangyou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhengting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07942">
<title>KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection. (arXiv:2307.07942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07942</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving a reliable LiDAR-based object detector in autonomous driving is
paramount, but its success hinges on obtaining large amounts of precise 3D
annotations. Active learning (AL) seeks to mitigate the annotation burden
through algorithms that use fewer labels and can attain performance comparable
to fully supervised learning. Although AL has shown promise, current approaches
prioritize the selection of unlabeled point clouds with high uncertainty and/or
diversity, leading to the selection of more instances for labeling and reduced
computational efficiency. In this paper, we resort to a novel kernel coding
rate maximization (KECOR) strategy which aims to identify the most informative
point clouds to acquire labels through the lens of information theory. Greedy
search is applied to seek desired point clouds that can maximize the minimal
number of bits required to encode the latent features. To determine the
uniqueness and informativeness of the selected samples from the model
perspective, we construct a proxy network of the 3D detector head and compute
the outer product of Jacobians from all proxy layers to form the empirical
neural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e.,
SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the
classification entropy maximization and well trade-off between detection
performance and the total number of bounding boxes selected for annotation.
Extensive experiments conducted on two 3D benchmarks and a 2D detection dataset
evidence the superiority and versatility of the proposed approach. Our results
show that approximately 44% box-level annotation costs and 26% computational
time are reduced compared to the state-of-the-art AL method, without
compromising detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07944">
<title>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (DA) with the aid of pseudo labeling
techniques has emerged as a crucial approach for domain-adaptive 3D object
detection. While effective, existing DA methods suffer from a substantial drop
in performance when applied to a multi-class training setting, due to the
co-existence of low-quality pseudo labels and class imbalance issues. In this
paper, we address this challenge by proposing a novel ReDB framework tailored
for learning to detect all classes at once. Our approach produces Reliable,
Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the
self-training on a distributionally different target domain. To alleviate
disruptions caused by the environmental discrepancy (e.g., beam numbers), the
proposed cross-domain examination (CDE) assesses the correctness of pseudo
labels by copy-pasting target instances into a source environment and measuring
the prediction consistency. To reduce computational overhead and mitigate the
object shift (e.g., scales and point densities), we design an overlapped boxes
counting (OBC) metric that allows to uniformly downsample pseudo-labeled
objects across different geometric characteristics. To confront the issue of
inter-class imbalance, we progressively augment the target point clouds with a
class-balanced set of pseudo-labeled target instances and source objects, which
boosts recognition accuracies on both frequently appearing and rare classes.
Experimental results on three benchmark datasets using both voxel-based (i.e.,
SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our
proposed ReDB approach outperforms existing 3D domain adaptation methods by a
large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07951">
<title>MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07951</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning in mathematical domains remains a significant challenge for
relatively small language models (LMs). Many current methods focus on
specializing LMs in mathematical reasoning and rely heavily on knowledge
distillation from powerful but inefficient large LMs (LLMs). In this work, we
explore a new direction that avoids over-reliance on LLM teachers, introducing
a multi-view fine-tuning method that efficiently exploits existing mathematical
problem datasets with diverse annotation styles. Our approach uniquely
considers the various annotation formats as different &quot;views&quot; and leverages
them in training the model. By postpending distinct instructions to input
questions, models can learn to generate solutions in diverse formats in a
flexible manner. Experimental results show that our strategy enables a LLaMA-7B
model to outperform prior approaches that utilize knowledge distillation, as
well as carefully established baselines. Additionally, the proposed method
grants the models promising generalization ability across various views and
datasets, and the capability to learn from inaccurate or incomplete noisy data.
We hope our multi-view training paradigm could inspire future studies in other
machine reasoning domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhenwen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaoman Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Qingkai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07956">
<title>Automated Polynomial Filter Learning for Graph Neural Networks. (arXiv:2307.07956v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07956</link>
<description rdf:parseType="Literal">&lt;p&gt;Polynomial graph filters have been widely used as guiding principles in the
design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the
polynomial graph filters has demonstrated promising performance for modeling
graph signals on both homophilic and heterophilic graphs, owning to their
flexibility and expressiveness. In this work, we conduct a novel preliminary
study to explore the potential and limitations of polynomial graph filter
learning approaches, revealing a severe overfitting issue. To improve the
effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel
and general automated polynomial graph filter learning framework that
efficiently learns better filters capable of adapting to various complex graph
signals. Comprehensive experiments and ablation studies demonstrate significant
and consistent performance improvements on both homophilic and heterophilic
graphs across multiple learning settings considering various labeling ratios,
which unleashes the potential of polynomial filter learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wendi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhichao Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaorui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07997">
<title>MargCTGAN: A &quot;Marginally&apos;&apos; Better CTGAN for the Low Sample Regime. (arXiv:2307.07997v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07997</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential of realistic and useful synthetic data is significant. However,
current evaluation methods for synthetic tabular data generation predominantly
focus on downstream task usefulness, often neglecting the importance of
statistical properties. This oversight becomes particularly prominent in low
sample scenarios, accompanied by a swift deterioration of these statistical
measures. In this paper, we address this issue by conducting an evaluation of
three state-of-the-art synthetic tabular data generators based on their
marginal distribution, column-pair correlation, joint distribution and
downstream task utility performance across high to low sample regimes. The
popular CTGAN model shows strong utility, but underperforms in low sample
settings in terms of utility. To overcome this limitation, we propose MargCTGAN
that adds feature matching of de-correlated marginals, which results in a
consistent improvement in downstream utility as well as statistical properties
of the synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afonja_T/0/1/0/all/0/1&quot;&gt;Tejumade Afonja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dingfan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08003">
<title>SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods. (arXiv:2307.08003v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;The interpretability of deep neural networks has become a subject of great
interest within the medical and healthcare domain. This attention stems from
concerns regarding transparency, legal and ethical considerations, and the
medical significance of predictions generated by these deep neural networks in
clinical decision support systems. To address this matter, our study delves
into the application of four well-established interpretability methods: Local
Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations
(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise
Relevance Propagation (LRP). Leveraging the approach of transfer learning with
a multi-label-multi-class chest radiography dataset, we aim to interpret
predictions pertaining to specific pathology classes. Our analysis encompasses
both single-label and multi-label predictions, providing a comprehensive and
unbiased assessment through quantitative and qualitative investigations, which
are compared against human expert annotation. Notably, Grad-CAM demonstrates
the most favorable performance in quantitative evaluation, while the LIME
heatmap segmentation visualization exhibits the highest level of medical
significance. Our research highlights the strengths and limitations of these
interpretability methods and suggests that a multimodal-based approach,
incorporating diverse sources of information beyond chest radiography images,
could offer additional insights for enhancing interpretability in the medical
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mahbub Ul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollmen_J/0/1/0/all/0/1&quot;&gt;Jaakko Hollm&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baldvinsson_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf3;n R&amp;#xfa;nar Baldvinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahmani_R/0/1/0/all/0/1&quot;&gt;Rahim Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08016">
<title>Breaking Down the Task: A Unit-Grained Hybrid Training Framework for Vision and Language Decision Making. (arXiv:2307.08016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08016</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision language decision making (VLDM) is a challenging multimodal task. The
agent have to understand complex human instructions and complete compositional
tasks involving environment navigation and object manipulation. However, the
long action sequences involved in VLDM make the task difficult to learn. From
an environment perspective, we find that task episodes can be divided into
fine-grained \textit{units}, each containing a navigation phase and an
interaction phase. Since the environment within a unit stays unchanged, we
propose a novel hybrid-training framework that enables active exploration in
the environment and reduces the exposure bias. Such framework leverages the
unit-grained configurations and is model-agnostic. Specifically, we design a
Unit-Transformer (UT) with an intrinsic recurrent state that maintains a
unit-scale cross-modal memory. Through extensive experiments on the TEACH
benchmark, we demonstrate that our proposed framework outperforms existing
state-of-the-art methods in terms of all evaluation metrics. Overall, our work
introduces a novel approach to tackling the VLDM task by breaking it down into
smaller, manageable units and utilizing a hybrid-training framework. By doing
so, we provide a more flexible and effective solution for multimodal decision
making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Ruipu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08024">
<title>Bayesian inference for data-efficient, explainable, and safe robotic motion planning: A review. (arXiv:2307.08024v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.08024</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian inference has many advantages in robotic motion planning over four
perspectives: The uncertainty quantification of the policy, safety (risk-aware)
and optimum guarantees of robot motions, data-efficiency in training of
reinforcement learning, and reducing the sim2real gap when the robot is applied
to real-world tasks. However, the application of Bayesian inference in robotic
motion planning is lagging behind the comprehensive theory of Bayesian
inference. Further, there are no comprehensive reviews to summarize the
progress of Bayesian inference to give researchers a systematic understanding
in robotic motion planning. This paper first provides the probabilistic
theories of Bayesian inference which are the preliminary of Bayesian inference
for complex cases. Second, the Bayesian estimation is given to estimate the
posterior of policies or unknown functions which are used to compute the
policy. Third, the classical model-based Bayesian RL and model-free Bayesian RL
algorithms for robotic motion planning are summarized, while these algorithms
in complex cases are also analyzed. Fourth, the analysis of Bayesian inference
in inverse RL is given to infer the reward functions in a data-efficient
manner. Fifth, we systematically present the hybridization of Bayesian
inference and RL which is a promising direction to improve the convergence of
RL for better motion planning. Sixth, given the Bayesian inference, we present
the interpretable and safe robotic motion plannings which are the hot research
topic recently. Finally, all algorithms reviewed in this paper are summarized
analytically as the knowledge graphs, and the future of Bayesian inference for
robotic motion planning is also discussed, to pave the way for data-efficient,
explainable, and safe robotic motion planning strategies for practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengmin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1&quot;&gt;Haseeb Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1&quot;&gt;Himat Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bingding Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franti_P/0/1/0/all/0/1&quot;&gt;Pasi Fr&amp;#xe4;nti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08036">
<title>A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences. (arXiv:2307.08036v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08036</link>
<description rdf:parseType="Literal">&lt;p&gt;Textual content around us is growing on a daily basis. Numerous articles are
being written as we speak on online newspapers, blogs, or social media.
Similarly, recent advances in the AI field, like language models or traditional
classic AI approaches, are utilizing all the above to improve their learned
representation to tackle NLP challenges with human-like accuracy. It is
commonly accepted that it is crucial to have access to well-written text from
valid sources to tackle challenges like text summarization, question-answering,
machine translation, or even pronoun resolution. For instance, to summarize
well, one needs to select the most important sentences in order to concatenate
them to form the summary. However, what happens if we do not have access to
well-formed English sentences or even non-valid sentences? Despite the
importance of having access to well-written sentences, figuring out ways to
validate them is still an open area of research. To address this problem, we
present a simplified way to validate English sentences through a novel
neural-symbolic approach. Lately, neural-symbolic approaches have triggered an
increasing interest towards tackling various NLP challenges, as they are
demonstrating their effectiveness as a central component in various AI systems.
Through combining Classic with Modern AI, which involves the blending of
grammatical and syntactical rules with language models, we effectively tackle
the Corpus of Linguistic Acceptability (COLA), a task that shows whether or not
a sequence of words is an English grammatical sentence. Among others,
undertaken experiments effectively show that blending symbolic and non-symbolic
systems helps the former provide insights about the latter&apos;s accuracy results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isaak_N/0/1/0/all/0/1&quot;&gt;Nicos Isaak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08044">
<title>Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08044</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-to-event analysis, also known as survival analysis, aims to predict the
time of occurrence of an event, given a set of features. One of the major
challenges in this area is dealing with censored data, which can make learning
algorithms more complex. Traditional methods such as Cox&apos;s proportional hazards
model and the accelerated failure time (AFT) model have been popular in this
field, but they often require assumptions such as proportional hazards and
linearity. In particular, the AFT models often require pre-specified parametric
distributional assumptions. To improve predictive performance and alleviate
strict assumptions, there have been many deep learning approaches for
hazard-based models in recent years. However, representation learning for AFT
has not been widely explored in the neural network literature, despite its
simplicity and interpretability in comparison to hazard-focused methods. In
this work, we introduce the Deep AFT Rank-regression model for Time-to-event
prediction (DART). This model uses an objective function based on Gehan&apos;s rank
statistic, which is efficient and reliable for representation learning. On top
of eliminating the requirement to establish a baseline event time distribution,
DART retains the advantages of directly predicting event time in standard AFT
models. The proposed method is a semiparametric approach to AFT modeling that
does not impose any distributional assumptions on the survival time
distribution. This also eliminates the need for additional hyperparameters or
complex model architectures, unlike existing neural network-based AFT models.
Through quantitative analysis on various benchmark datasets, we have shown that
DART has significant potential for modeling high-throughput censored
time-to-event data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunjun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junhyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1&quot;&gt;Taehwa Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sangbum Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1909.07750">
<title>MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1909.07750</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MDP Playground, a testbed for Reinforcement Learning (RL) agents
with dimensions of hardness that can be controlled independently to challenge
agents in different ways and obtain varying degrees of hardness in toy and
complex RL environments. We consider and allow control over a wide variety of
dimensions, including delayed rewards, sequence lengths, reward density,
stochasticity, image representations, irrelevant features, time unit, action
range and more. We define a parameterised collection of fast-to-run toy
environments in OpenAI Gym by varying these dimensions and propose to use these
to understand agents better. We then show how to design experiments using MDP
Playground to gain insights on the toy environments. We also provide wrappers
that can inject many of these dimensions into any Gym environment. We
experiment with these wrappers on Atari and Mujoco to allow for understanding
the effects of these dimensions on environments that are more complex than the
toy environments. We also compare the effect of the dimensions on the toy and
complex environments. Finally, we show how to use MDP Playground to debug
agents, to study the interaction of multiple dimensions and describe further
use-cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1&quot;&gt;Raghu Rajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1&quot;&gt;Jessica Lizeth Borja Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1&quot;&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Biedenkapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1&quot;&gt;Jan Ole von Hartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.12689">
<title>The Less Intelligent the Elements, the More Intelligent the Whole. Or, Possibly Not?. (arXiv:2012.12689v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2012.12689</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore a Leviathan analogy between neurons in a brain and human beings in
society, asking ourselves whether individual intelligence is necessary for
collective intelligence to emerge and, most importantly, what sort of
individual intelligence is conducive of greater collective intelligence. We
first review disparate insights from connectionist cognitive science,
agent-based modeling, group psychology, economics and physics. Subsequently, we
apply these insights to the sort and degrees of intelligence that in the
Lotka-Volterra model lead to either co-existence or global extinction of
predators and preys.
&lt;/p&gt;
&lt;p&gt;We find several individual behaviors -- particularly of predators -- that are
conducive to co-existence, eventually with oscillations around an equilibrium.
However, we also find that if both preys and predators are sufficiently
intelligent to extrapolate one other&apos;s behavior, co-existence comes along with
indefinite growth of both populations. Since the Lotka-Volterra model is also
interpreted to represent the business cycle, we understand this finding as a
condition for economic growth around oscillations. Specifically, we hypothesize
that pre-modern societies may not have exhibited limitless growth also because
capitalistic future-oriented thinking based on saving and investing concerned
at most a fraction of the population.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fioretti_G/0/1/0/all/0/1&quot;&gt;Guido Fioretti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Policarpi_A/0/1/0/all/0/1&quot;&gt;Andrea Policarpi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.04486">
<title>Sketch-Based Anomaly Detection in Streaming Graphs. (arXiv:2106.04486v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2106.04486</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a stream of graph edges from a dynamic graph, how can we assign anomaly
scores to edges and subgraphs in an online manner, for the purpose of detecting
unusual behavior, using constant time and memory? For example, in intrusion
detection, existing work seeks to detect either anomalous edges or anomalous
subgraphs, but not both. In this paper, we first extend the count-min sketch
data structure to a higher-order sketch. This higher-order sketch has the
useful property of preserving the dense subgraph structure (dense subgraphs in
the input turn into dense submatrices in the data structure). We then propose 4
online algorithms that utilize this enhanced data structure, which (a) detect
both edge and graph anomalies; (b) process each edge and graph in constant
memory and constant update time per newly arriving edge, and; (c) outperform
state-of-the-art baselines on 4 real-world datasets. Our method is the first
streaming approach that incorporates dense subgraph search to detect graph
anomalies in constant memory and time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1&quot;&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1&quot;&gt;Mohit Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.10998">
<title>Pruning Ternary Quantization. (arXiv:2107.10998v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.10998</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference time, model size, and accuracy are three key factors in deep model
compression. Most of the existing work addresses these three key factors
separately as it is difficult to optimize them all at the same time. For
example, low-bit quantization aims at obtaining a faster model; weight sharing
quantization aims at improving compression ratio and accuracy; and
mixed-precision quantization aims at balancing accuracy and inference time. To
simultaneously optimize bit-width, model size, and accuracy, we propose pruning
ternary quantization (PTQ): a simple, effective, symmetric ternary quantization
method. We integrate L2 normalization, pruning, and the weight decay term to
reduce the weight discrepancy in the gradient estimator during quantization,
thus producing highly compressed ternary weights. Our method brings the highest
test accuracy and the highest compression ratio. For example, it produces a
939kb (49$\times$) 2bit ternary ResNet-18 model with only 4\% accuracy drop on
the ImageNet dataset. It compresses 170MB Mask R-CNN to 5MB (34$\times$) with
only 2.8\% average precision drop. Our method is verified on image
classification, object detection/segmentation tasks with different network
structures such as ResNet-18, ResNet-50, and MobileNetV2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xue Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.07533">
<title>Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2111.07533</link>
<description rdf:parseType="Literal">&lt;p&gt;Peer review is a widely accepted mechanism for research evaluation, playing a
pivotal role in academic publishing. However, criticisms have long been leveled
at this mechanism, mostly because of its poor efficiency and low
reproducibility. Recent years have seen the application of artificial
intelligence (AI) in assisting the peer review process. Nonetheless, with the
involvement of humans, such limitations remain inevitable. In this paper, we
propose the concept and pipeline of automated scholarly paper review (ASPR) and
review the relevant literature and technologies of achieving a full-scale
computerized review process. On the basis of the review and discussion, we
conclude that there is already corresponding research and preliminary
implementation at each stage of ASPR. We further look into the challenges in
ASPR with the existing technologies. The major difficulties lie in inadequate
data, imperfect document parsing and representation, defective
human$\unicode{x2013}$computer interaction, and flawed deep logical reasoning.
Moreover, we point out the future directions and discuss the possible moral and
ethical issues of ASPR. In the foreseeable future, ASPR and peer review will
coexist in a reinforcing manner before ASPR is able to fully undertake the
reviewing workload from humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jialiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhangping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yidong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaodong Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16329">
<title>Parameter-efficient Model Adaptation for Vision Transformers. (arXiv:2203.16329v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16329</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision, it has achieved great transfer learning performance via
adapting large-scale pretrained vision models (e.g., vision transformers) to
downstream tasks. Common approaches for model adaptation either update all
model parameters or leverage linear probes. In this paper, we aim to study
parameter-efficient model adaptation strategies for vision transformers on the
image classification task. We formulate efficient model adaptation as a
subspace training problem and perform a comprehensive benchmarking over
different efficient adaptation methods. We conduct an empirical study on each
efficient model adaptation method focusing on its performance alongside
parameter cost. Furthermore, we propose a parameter-efficient model adaptation
framework, which first selects submodules by measuring local intrinsic
dimensions and then projects them into subspace for further decomposition via a
novel Kronecker Adaptation (KAdaptation) method. We analyze and compare our
method with a diverse set of baseline model adaptation methods (including
state-of-the-art methods for pretrained language models). Our method performs
the best in terms of the tradeoff between accuracy and parameter efficiency
across 20 image classification datasets under the few-shot setting and 7 image
classification datasets under the full-shot setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuehai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07028">
<title>Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07028</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a privacy-preserving machine learning paradigm in
which the server periodically aggregates local model parameters from clients
without assembling their private data.
&lt;/p&gt;
&lt;p&gt;Constrained communication and personalization requirements pose severe
challenges to FL. Federated distillation (FD) is proposed to simultaneously
address the above two problems, which exchanges knowledge between the server
and clients, supporting heterogeneous local models while significantly reducing
communication overhead. However, most existing FD methods require a proxy
dataset, which is often unavailable in reality.
&lt;/p&gt;
&lt;p&gt;A few recent proxy-data-free FD approaches can eliminate the need for
additional public data, but suffer from remarkable discrepancy among local
knowledge due to client-side model heterogeneity, leading to ambiguous
representation on the server and inevitable accuracy degradation.
&lt;/p&gt;
&lt;p&gt;To tackle this issue, we propose a proxy-data-free FD algorithm based on
distributed knowledge congruence (FedDKC). FedDKC leverages well-designed
refinement strategies to narrow local knowledge differences into an acceptable
upper bound, so as to mitigate the negative effects of knowledge incongruence.
&lt;/p&gt;
&lt;p&gt;Specifically, from perspectives of peak probability and Shannon entropy of
local knowledge, we design kernel-based knowledge refinement (KKR) and
searching-based knowledge refinement (SKR) respectively, and theoretically
guarantee that the refined-local knowledge can satisfy an approximately-similar
distribution and be regarded as congruent.
&lt;/p&gt;
&lt;p&gt;Extensive experiments conducted on three common datasets demonstrate that our
proposed FedDKC significantly outperforms the state-of-the-art on various
heterogeneous settings while evidently improving the convergence speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Quyang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingxiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03923">
<title>Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03923</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Cameron Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1&quot;&gt;Sergey Zakharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1&quot;&gt;Fredo Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1&quot;&gt;Vincent Sitzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.07433">
<title>Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary neural networks leverage $\mathrm{Sign}$ function to binarize weights
and activations, which require gradient estimators to overcome its
non-differentiability and will inevitably bring gradient errors during
backpropagation. Although many hand-designed soft functions have been proposed
as gradient estimators to better approximate gradients, their mechanism is not
clear and there are still huge performance gaps between binary models and their
full-precision counterparts. To address these issues and reduce gradient error,
we propose to tackle network binarization as a binary classification problem
and use a multi-layer perceptron (MLP) as the classifier in the forward pass
and gradient estimator in the backward pass. Benefiting from the MLP&apos;s
theoretical capability to fit any continuous function, it can be adaptively
learned to binarize networks and backpropagate gradients without any prior
knowledge of soft functions. From this perspective, we further empirically
justify that even a simple linear function can outperform previous complex soft
functions. Extensive experiments demonstrate that the proposed method yields
surprising performance both in image classification and human pose estimation
tasks. Specifically, we achieve $65.7\%$ top-1 accuracy of ResNet-34 on
ImageNet dataset, with an absolute improvement of $2.6\%$. Moreover, we take
binarization as a lightweighting approach for pose estimation models and
propose well-designed binary pose estimation networks SBPN and BHRNet. When
evaluating on the challenging Microsoft COCO keypoint dataset, the proposed
method enables binary networks to achieve a mAP of up to $60.6$ for the first
time. Experiments conducted on real platforms demonstrate that BNN achieves a
better balance between performance and computational complexity, especially
when computational resources are extremely low.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yefei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luoming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weijia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06767">
<title>Semi-supervised cross-lingual speech emotion recognition. (arXiv:2207.06767v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06767</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance in Speech Emotion Recognition (SER) on a single language has
increased greatly in the last few years thanks to the use of deep learning
techniques. However, cross-lingual SER remains a challenge in real-world
applications due to two main factors: the first is the big gap among the source
and the target domain distributions; the second factor is the major
availability of unlabeled utterances in contrast to the labeled ones for the
new language. Taking into account previous aspects, we propose a
Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition
when only few labeled examples in the target domain (i.e. the new language) are
available. Our method is based on a Transformer and it adapts to the new domain
by exploiting a pseudo-labeling strategy on the unlabeled utterances. In
particular, the use of a hard and soft pseudo-labels approach is investigated.
We thoroughly evaluate the performance of the proposed method in a
speaker-independent setup on both the source and the new language and show its
robustness across five languages belonging to different linguistic strains. The
experimental findings indicate that the unweighted accuracy is increased by an
average of 40% compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarla_M/0/1/0/all/0/1&quot;&gt;Mirko Agarla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianco_S/0/1/0/all/0/1&quot;&gt;Simone Bianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1&quot;&gt;Luigi Celona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Napoletano_P/0/1/0/all/0/1&quot;&gt;Paolo Napoletano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrovsky_A/0/1/0/all/0/1&quot;&gt;Alexey Petrovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1&quot;&gt;Flavio Piccoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schettini_R/0/1/0/all/0/1&quot;&gt;Raimondo Schettini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanin_I/0/1/0/all/0/1&quot;&gt;Ivan Shanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09418">
<title>SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09418</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.
Despite great efforts made by the Explainable AI (XAI) community, explanations
lack robustness -- indistinguishable input perturbations may lead to different
XAI results. Thus, it is vital to assess how robust DL interpretability is,
given an XAI method. In this paper, we identify several challenges that the
state-of-the-art is unable to cope with collectively: i) existing metrics are
not comprehensive; ii) XAI techniques are highly heterogeneous; iii)
misinterpretations are normally rare events. To tackle these challenges, we
introduce two black-box evaluation methods, concerning the worst-case
interpretation discrepancy and a probabilistic notion of how robust in general,
respectively. Genetic Algorithm (GA) with bespoke fitness function is used to
solve constrained optimisation for efficient worst-case evaluation. Subset
Simulation (SS), dedicated to estimate rare event probabilities, is used for
evaluating overall robustness. Experiments show that the accuracy, sensitivity,
and efficiency of our methods outperform the state-of-the-arts. Finally, we
demonstrate two applications of our methods: ranking robust XAI methods and
selecting training schemes to improve both classification and interpretation
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1&quot;&gt;Gaojie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00465">
<title>On Grounded Planning for Embodied Tasks with Language Models. (arXiv:2209.00465v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00465</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models (LMs) have demonstrated their capability in possessing
commonsense knowledge of the physical world, a crucial aspect of performing
tasks in everyday life. However, it remains unclear **whether LMs have the
capacity to generate grounded, executable plans for embodied tasks.** This is a
challenging task as LMs lack the ability to perceive the environment through
vision and feedback from the physical environment. In this paper, we address
this important research question and present the first investigation into the
topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level
goal and a data table about objects in a specific environment, and then outputs
a step-by-step actionable plan for a robotic agent to follow. To facilitate the
study, we establish an **evaluation protocol** and design a dedicated metric to
assess the quality of the plans. Our experiments demonstrate that the use of
tables for encoding the environment and an iterative decoding strategy can
significantly enhance the LMs&apos; ability in grounded planning. Our analysis also
reveals interesting and non-trivial findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Wenda Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommerer_S/0/1/0/all/0/1&quot;&gt;Sam Sommerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10818">
<title>Memory-Augmented Graph Neural Networks: A Brain-Inspired Review. (arXiv:2209.10818v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10818</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a comprehensive review of the existing literature on
memory-augmented GNNs. We review these works through the lens of psychology and
neuroscience, which has several established theories on how multiple memory
systems and mechanisms operate in biological brains. We propose a taxonomy of
memory-augmented GNNs and a set of criteria for comparing their memory
mechanisms. We also provide critical discussions on the limitations of these
works. Finally, we discuss the challenges and future directions for this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guixiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1&quot;&gt;Vy A. Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1&quot;&gt;Theodore Willke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11134">
<title>Neural Networks Based on Power Method and Inverse Power Method for Solving Linear Eigenvalue Problems. (arXiv:2209.11134v5 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11134</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose two kinds of neural networks inspired by power
method and inverse power method to solve linear eigenvalue problems. These
neural networks share similar ideas with traditional methods, in which the
differential operator is realized by automatic differentiation. The
eigenfunction of the eigenvalue problem is learned by the neural network and
the iterative algorithms are implemented by optimizing the specially defined
loss function. The largest positive eigenvalue, smallest eigenvalue and
interior eigenvalues with the given prior knowledge can be solved efficiently.
We examine the applicability and accuracy of our methods in the numerical
experiments in one dimension, two dimensions and higher dimensions. Numerical
results show that accurate eigenvalue and eigenfunction approximations can be
obtained by our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yangtao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiaolin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiquan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13220">
<title>Exploiting Transformer in Sparse Reward Reinforcement Learning for Interpretable Temporal Logic Motion Planning. (arXiv:2209.13220v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13220</link>
<description rdf:parseType="Literal">&lt;p&gt;Automaton based approaches have enabled robots to perform various complex
tasks. However, most existing automaton based algorithms highly rely on the
manually customized representation of states for the considered task, limiting
its applicability in deep reinforcement learning algorithms. To address this
issue, by incorporating Transformer into reinforcement learning, we develop a
Double-Transformer-guided Temporal Logic framework (T2TL) that exploits the
structural feature of Transformer twice, i.e., first encoding the LTL
instruction via the Transformer module for efficient understanding of task
instructions during the training and then encoding the context variable via the
Transformer again for improved task performance. Particularly, the LTL
instruction is specified by co-safe LTL. As a semantics-preserving rewriting
operation, LTL progression is exploited to decompose the complex task into
learnable sub-goals, which not only converts non-Markovian reward decision
processes to Markovian ones, but also improves the sampling efficiency by
simultaneous learning of multiple sub-tasks. An environment-agnostic LTL
pre-training scheme is further incorporated to facilitate the learning of the
Transformer module resulting in an improved representation of LTL. The
simulation results demonstrate the effectiveness of the T2TL framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1&quot;&gt;Zhen Kan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00131">
<title>Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00131</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern language modeling tasks are often underspecified: for a given token
prediction, many words may satisfy the user&apos;s intent of producing natural
language at inference time, however only one word would minimize the task&apos;s
loss function at training time. We provide a simple yet plausible causal
mechanism describing the role underspecification plays in the generation of
spurious correlations. Despite its simplicity, our causal model directly
informs the development of two lightweight black-box evaluation methods, that
we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid
in the detection of inference-time task underspecification by exploiting 2)
previously unreported gender vs. time and gender vs. location spurious
correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B)
pre-training objectives: from masked &amp;amp; autoregressive language modeling to a
mixture of these objectives, and C) training stages: from pre-training only to
reinforcement learning from human feedback (RLHF). Code and open-source demos
available at https: //github.com/2dot71mily/sib_paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMilin_E/0/1/0/all/0/1&quot;&gt;Emily McMilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10886">
<title>Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research and
augment medical datasets. Training generative adversarial neural networks
(GANs) usually require large amounts of training data. Federated learning (FL)
provides a way of training a central model using distributed data while keeping
raw data locally. However, given that the FL server cannot access the raw data,
it is vulnerable to backdoor attacks, an adversarial by poisoning training
data. Most backdoor attack strategies focus on classification models and
centralized domains. It is still an open question if the existing backdoor
attacks can affect GAN training and, if so, how to defend against the attack in
the FL setting. In this work, we investigate the overlooked issue of backdoor
attacks in federated GANs (FedGANs). The success of this attack is subsequently
determined to be the result of some local discriminators overfitting the
poisoned data and corrupting the local GAN equilibrium, which then further
contaminates other clients when averaging the generator&apos;s parameters and yields
high generator loss. Therefore, we proposed FedDetect, an efficient and
effective way of defending against the backdoor attack in the FL setting, which
allows the server to detect the client&apos;s adversarial behavior based on their
losses and block the malicious clients. Our extensive experiments on two
medical datasets with different modalities demonstrate the backdoor attack on
FedGANs can result in synthetic images with low fidelity. After detecting and
suppressing the detected malicious clients using the proposed defense strategy,
we show that FedGANs can synthesize high-quality medical datasets (with labels)
for data augmentation to improve classification models&apos; performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruinan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02641">
<title>Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02641</link>
<description rdf:parseType="Literal">&lt;p&gt;The motor imagery (MI) classification has been a prominent research topic in
brain-computer interfaces based on electroencephalography (EEG). Over the past
few decades, the performance of MI-EEG classifiers has gradually improved. In
this study, we enhance the geometric deep learning classifier for MI-EEG
classification from the perspective of time-frequency analysis, introducing a
new architecture called Graph-CSPNet. We refer to this category of classifiers
as geometric methods, emphasizing their rich background in differential
geometry induced by signal covariance matrices. Graph-CSPNet utilizes a novel
SPD matrix-valued graph convolutional techniques to capture the EEG features in
the time-frequency domain, providing greater flexibility in signal segmentation
and capturing localized fluctuations. To evaluate the effectiveness of
Graph-CSPNet, we employ five commonly-used publicly available MI-EEG datasets,
achieving near-optimal classification accuracies in nine out of eleven
scenarios. The Python repository can be found at
https://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Ce Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Cuntai Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07137">
<title>DroneNet: Crowd Density Estimation using Self-ONNs for Drones. (arXiv:2211.07137v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07137</link>
<description rdf:parseType="Literal">&lt;p&gt;Video surveillance using drones is both convenient and efficient due to the
ease of deployment and unobstructed movement of drones in many scenarios. An
interesting application of drone-based video surveillance is to estimate crowd
densities (both pedestrians and vehicles) in public places. Deep learning using
convolution neural networks (CNNs) is employed for automatic crowd counting and
density estimation using images and videos. However, the performance and
accuracy of such models typically depend upon the model architecture i.e.,
deeper CNN models improve accuracy at the cost of increased inference time. In
this paper, we propose a novel crowd density estimation model for drones
(DroneNet) using Self-organized Operational Neural Networks (Self-ONN).
Self-ONN provides efficient learning capabilities with lower computational
complexity as compared to CNN-based models. We tested our algorithm on two
drone-view public datasets. Our evaluation shows that the proposed DroneNet
shows superior performance on an equivalent CNN-based model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13435">
<title>A Benchmark of Long-tailed Instance Segmentation with Noisy Labels. (arXiv:2211.13435v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13435</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the instance segmentation task on a long-tailed
dataset, which contains label noise, i.e., some of the annotations are
incorrect. There are two main reasons making this case realistic. First,
datasets collected from real world usually obey a long-tailed distribution.
Second, for instance segmentation datasets, as there are many instances in one
image and some of them are tiny, it is easier to introduce noise into the
annotations. Specifically, we propose a new dataset, which is a large
vocabulary long-tailed dataset containing label noise for instance
segmentation. Furthermore, we evaluate previous proposed instance segmentation
algorithms on this dataset. The results indicate that the noise in the training
dataset will hamper the model in learning rare categories and decrease the
overall performance, and inspire us to explore more effective approaches to
address this practical challenge. The code and dataset are available in
https://github.com/GuanlinLee/Noisy-LVIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13709">
<title>Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13709</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models and Natural Language Processing (NLP) technology
rapidly develops and spreads into daily life, it becomes crucial to anticipate
how its use could harm people. One problem that has received a lot of attention
in recent years is that this technology has displayed harmful biases in its
behavior. Although a lot of effort has been invested in assessing and
mitigating these biases, our methods of measuring the biases of NLP models have
serious problems (e.g., it is often unclear what they actually measure). In
this paper, we provide an interdisciplinary approach to discussing the issue of
NLP model bias by adopting the lens of psychometrics -- a field specialized in
the measurement of concepts like bias that are not directly observable. In
particular, we will explore two central notions from psychometrics, the
construct validity and the reliability of measurement tools, and discuss how
they can be applied in the context of measuring model bias. Our goal is to
provide NLP practitioners with methodological tools for designing better bias
measures, and to inspire them more generally to explore tools from
psychometrics when working on bias measurement tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1&quot;&gt;Oskar van der Wal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1&quot;&gt;Dominik Bachmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1&quot;&gt;Alina Leidinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1&quot;&gt;Leendert van Maanen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1&quot;&gt;Willem Zuidema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1&quot;&gt;Katrin Schulz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14646">
<title>Towards Improved Input Masking for Convolutional Neural Networks. (arXiv:2211.14646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14646</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to remove features from the input of machine learning models is
very important to understand and interpret model predictions. However, this is
non-trivial for vision models since masking out parts of the input image
typically causes large distribution shifts. This is because the baseline color
used for masking (typically grey or black) is out of distribution. Furthermore,
the shape of the mask itself can contain unwanted signals which can be used by
the model for its predictions. Recently, there has been some progress in
mitigating this issue (called missingness bias) in image masking for vision
transformers. In this work, we propose a new masking method for CNNs we call
layer masking in which the missingness bias caused by masking is reduced to a
large extent. Intuitively, layer masking applies a mask to intermediate
activation maps so that the model only processes the unmasked input. We show
that our method (i) is able to eliminate or minimize the influence of the mask
shape or color on the output of the model, and (ii) is much better than
replacing the masked region by black or grey for input perturbation based
interpretability techniques like LIME. Thus, layer masking is much less
affected by missingness bias than other masking strategies. We also demonstrate
how the shape of the mask may leak information about the class, thus affecting
estimates of model reliance on class-relevant features derived from input
masking. Furthermore, we discuss the role of data augmentation techniques for
tackling this problem, and argue that they are not sufficient for preventing
model reliance on mask shape. The code for this project is publicly available
at https://github.com/SriramB-98/layer_masking
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1&quot;&gt;Sriram Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1&quot;&gt;Soheil Feizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16162">
<title>Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a communication-efficient hierarchical federated
learning algorithm for distributed setups including core servers and multiple
edge servers with clusters of devices. Assuming different learning tasks,
clusters with a same task collaborate. To implement the algorithm over wireless
links, we propose a scalable clustered over-the-air aggregation scheme for the
uplink with a bandwidth-limited broadcast scheme for the downlink that requires
only a single resource block for each algorithm iteration, independent of the
number of edge servers and devices. This setup is faced with interference of
devices in the uplink and interference of edge servers in the downlink that are
to be modeled rigorously. We first develop a spatial model for the setup by
modeling devices as a Poisson cluster process over the edge servers and
quantify uplink and downlink error terms due to the interference. Accordingly,
we present a comprehensive mathematical approach to derive the convergence
bound for the proposed algorithm including any number of collaborating clusters
and provide special cases and design remarks. Finally, we show that despite the
interference and data heterogeneity, the proposed algorithm not only achieves
high learning accuracy for a variety of parameters but also significantly
outperforms the conventional hierarchical learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azimi_Abarghouyi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Azimi-Abarghouyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fodor_V/0/1/0/all/0/1&quot;&gt;Viktoria Fodor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01450">
<title>Crowd Density Estimation using Imperfect Labels. (arXiv:2212.01450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01450</link>
<description rdf:parseType="Literal">&lt;p&gt;Density estimation is one of the most widely used methods for crowd counting
in which a deep learning model learns from head-annotated crowd images to
estimate crowd density in unseen images. Typically, the learning performance of
the model is highly impacted by the accuracy of the annotations and inaccurate
annotations may lead to localization and counting errors during prediction. A
significant amount of works exist on crowd counting using perfectly labelled
datasets but none of these explore the impact of annotation errors on the model
accuracy. In this paper, we investigate the impact of imperfect labels (both
noisy and missing labels) on crowd counting accuracy. We propose a system that
automatically generates imperfect labels using a deep learning model (called
annotator) which are then used to train a new crowd counting model (target
model). Our analysis on two crowd counting models and two benchmark datasets
shows that the proposed scheme achieves accuracy closer to that of the model
trained with perfect labels showing the robustness of crowd models to
annotation errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01452">
<title>CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01452</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models require an enormous amount of data for training.
However, recently there is a shift in machine learning from model-centric to
data-centric approaches. In data-centric approaches, the focus is to refine and
improve the quality of the data to improve the learning performance of the
models rather than redesigning model architectures. In this paper, we propose
CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two
data-centric approaches i.e., curriculum learning and dataset pruning to
improve the model learning accuracy and convergence speed. The proposed scheme
applies loss-aware dataset pruning to iteratively remove the least significant
samples and progressively reduces the size of the effective dataset in the
curriculum learning training. Extensive experiments performed on crowd density
estimation models validate the notion behind combining the two approaches by
reducing the convergence time and improving generalization. To our knowledge,
the idea of data pruning as an embedded process in curriculum learning is
novel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamila_R/0/1/0/all/0/1&quot;&gt;Ridha Hamila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menouar_H/0/1/0/all/0/1&quot;&gt;Hamid Menouar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01793">
<title>kHGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01793</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of tree-like structures, encompassing hierarchical structures
and power law distributions, exists extensively in real-world applications,
including recommendation systems, ecosystems, financial networks, social
networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness
modeling has garnered considerable attention owing to its exponential growth
volume. Compared to the flat Euclidean space, the curved hyperbolic space
provides a more amenable and embeddable room, especially for datasets
exhibiting implicit tree-like architectures. However, the intricate nature of
real-world tree-like data presents a considerable challenge, as it frequently
displays a heterogeneous composition of tree-like, flat, and circular regions.
The direct embedding of such heterogeneous structures into a homogeneous
embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions.
To mitigate the aforementioned shortage, this study endeavors to explore the
curvature between discrete structure and continuous learning space, aiming at
encoding the message conveyed by the network topology in the learning process,
thereby improving tree-likeness modeling. To the end, a curvature-aware
hyperbolic graph convolutional neural network, \{kappa}HGCN, is proposed, which
utilizes the curvature to guide message passing and improve long-range
propagation. Extensive experiments on node classification and link prediction
tasks verify the superiority of the proposal as it consistently outperforms
various competitive models by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Menglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lujia Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03530">
<title>Curiosity creates Diversity in Policy Search. (arXiv:2212.03530v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03530</link>
<description rdf:parseType="Literal">&lt;p&gt;When searching for policies, reward-sparse environments often lack sufficient
information about which behaviors to improve upon or avoid. In such
environments, the policy search process is bound to blindly search for
reward-yielding transitions and no early reward can bias this search in one
direction or another. A way to overcome this is to use intrinsic motivation in
order to explore new transitions until a reward is found. In this work, we use
a recently proposed definition of intrinsic motivation, Curiosity, in an
evolutionary policy search method. We propose Curiosity-ES, an evolutionary
strategy adapted to use Curiosity as a fitness metric. We compare Curiosity
with Novelty, a commonly used diversity metric, and find that Curiosity can
generate higher diversity over full episodes without the need for an explicit
diversity criterion and lead to multiple policies which find reward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolguenec_P/0/1/0/all/0/1&quot;&gt;Paul-Antoine Le Tolguenec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachelson_E/0/1/0/all/0/1&quot;&gt;Emmanuel Rachelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besse_Y/0/1/0/all/0/1&quot;&gt;Yann Besse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;Dennis G. Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12887">
<title>Closed-form control with spike coding networks. (arXiv:2212.12887v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12887</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient and robust control using spiking neural networks (SNNs) is still an
open problem. Whilst behaviour of biological agents is produced through sparse
and irregular spiking patterns, which provide both robust and efficient
control, the activity patterns in most artificial spiking neural networks used
for control are dense and regular -- resulting in potentially less efficient
codes. Additionally, for most existing control solutions network training or
optimization is necessary, even for fully identified systems, complicating
their implementation in on-chip low-power solutions. The neuroscience theory of
Spike Coding Networks (SCNs) offers a fully analytical solution for
implementing dynamical systems in recurrent spiking neural networks -- while
maintaining irregular, sparse, and robust spiking activity -- but it&apos;s not
clear how to directly apply it to control problems. Here, we extend SCN theory
by incorporating closed-form optimal estimation and control. The resulting
networks work as a spiking equivalent of a linear-quadratic-Gaussian
controller. We demonstrate robust spiking control of simulated
spring-mass-damper and cart-pole systems, in the face of several perturbations,
including input- and system-noise, system disturbances, and neural silencing.
As our approach does not need learning or optimization, it offers opportunities
for deploying fast and efficient task-specific on-chip spiking controllers with
biologically realistic activity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slijkhuis_F/0/1/0/all/0/1&quot;&gt;Filip S. Slijkhuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keemink_S/0/1/0/all/0/1&quot;&gt;Sander W. Keemink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanillos_P/0/1/0/all/0/1&quot;&gt;Pablo Lanillos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01947">
<title>StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose StitchNet, a novel neural network creation paradigm that stitches
together fragments (one or more consecutive network layers) from multiple
pre-trained neural networks. StitchNet allows the creation of high-performing
neural networks without the large compute and data requirements needed under
traditional model creation processes via backpropagation training. We leverage
Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide
the selection of these fragments in composing a network for a given task
tailored to specific accuracy needs and computing resource constraints. We then
show that these fragments can be stitched together to create neural networks
with comparable accuracy to traditionally trained networks at a fraction of
computing resource and data requirements. Finally, we explore a novel
on-the-fly personalized model creation and inference application enabled by
this new paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teerapittayanon_S/0/1/0/all/0/1&quot;&gt;Surat Teerapittayanon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comiter_M/0/1/0/all/0/1&quot;&gt;Marcus Comiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDanel_B/0/1/0/all/0/1&quot;&gt;Brad McDanel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1&quot;&gt;H.T. Kung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08125">
<title>Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2301.08125v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08125</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple Instance Learning (MIL) and transformers are increasingly popular in
histopathology Whole Slide Image (WSI) classification. However, unlike human
pathologists who selectively observe specific regions of histopathology tissues
under different magnifications, most methods do not incorporate multiple
resolutions of the WSIs, hierarchically and attentively, thereby leading to a
loss of focus on the WSIs and information from other resolutions. To resolve
this issue, we propose a Hierarchical Attention-Guided Multiple Instance
Learning framework to fully exploit the WSIs. This framework can dynamically
and attentively discover the discriminative regions across multiple resolutions
of the WSIs. Within this framework, an Integrated Attention Transformer is
proposed to further enhance the performance of the transformer and obtain a
more holistic WSI (bag) representation. This transformer consists of multiple
Integrated Attention Modules, which is the combination of a transformer layer
and an aggregation module that produces a bag representation based on every
instance representation in that bag. The experimental results show that our
method achieved state-of-the-art performances on multiple datasets, including
Camelyon16, TCGA-RCC, TCGA-NSCLC, and an in-house IMGC dataset. The code is
available at https://github.com/BearCleverProud/HAG-MIL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Conghao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1&quot;&gt;Joseph J.Y. Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03665">
<title>HumanMAC: Masked Motion Completion for Human Motion Prediction. (arXiv:2302.03665v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03665</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion prediction is a classical problem in computer vision and
computer graphics, which has a wide range of practical applications. Previous
effects achieve great empirical performance based on an encoding-decoding
style. The methods of this style work by first encoding previous motions to
latent representations and then decoding the latent representations into
predicted motions. However, in practice, they are still unsatisfactory due to
several issues, including complicated loss constraints, cumbersome training
processes, and scarce switch of different categories of motions in prediction.
In this paper, to address the above issues, we jump out of the foregoing style
and propose a novel framework from a new perspective. Specifically, our
framework works in a masked completion fashion. In the training stage, we learn
a motion diffusion model that generates motions from random noise. In the
inference stage, with a denoising procedure, we make motion prediction
conditioning on observed motions to output more continuous and controllable
predictions. The proposed framework enjoys promising algorithmic properties,
which only needs one loss in optimization and is trained in an end-to-end
manner. Additionally, it accomplishes the switch of different categories of
motions effectively, which is significant in realistic tasks, e.g., the
animation task. Comprehensive experiments on benchmarks confirm the superiority
of the proposed framework. The project page is available at
https://lhchen.top/Human-MAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling-Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yewen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yiren Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1&quot;&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08942">
<title>PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08942</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend PAC-Bayesian theory to generative models and develop generalization
bounds for models based on the Wasserstein distance and the total variation
distance. Our first result on the Wasserstein distance assumes the instance
space is bounded, while our second result takes advantage of dimensionality
reduction. Our results naturally apply to Wasserstein GANs and Energy-Based
GANs, and our bounds provide new training objectives for these two. Although
our work is mainly theoretical, we perform numerical experiments showing
non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mbacke_S/0/1/0/all/0/1&quot;&gt;Sokhna Diarra Mbacke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clerc_F/0/1/0/all/0/1&quot;&gt;Florence Clerc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1&quot;&gt;Pascal Germain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10893">
<title>Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10893</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI models have recently achieved astonishing results in quality
and are consequently employed in a fast-growing number of applications.
However, since they are highly data-driven, relying on billion-sized datasets
randomly scraped from the internet, they also suffer from degenerated and
biased human behavior, as we demonstrate. In fact, they may even reinforce such
biases. To not only uncover but also combat these undesired effects, we present
a novel strategy, called Fair Diffusion, to attenuate biases after the
deployment of generative text-to-image models. Specifically, we demonstrate
shifting a bias, based on human instructions, in any direction yielding
arbitrarily new proportions for, e.g., identity groups. As our empirical
evaluation demonstrates, this introduced control enables instructing generative
image models on fairness, with no data filtering and additional training
required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1&quot;&gt;Sasha Luccioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11351">
<title>Regularised neural networks mimic human insight. (arXiv:2302.11351v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11351</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans sometimes show sudden improvements in task performance that have been
linked to moments of insight. Such insight-related performance improvements
appear special because they are preceded by an extended period of impasse, are
unusually abrupt, and occur only in some, but not all, learners. Here, we ask
whether insight-like behaviour also occurs in artificial neural networks
trained with gradient descent algorithms. We compared learning dynamics in
humans and regularised neural networks in a perceptual decision task that
provided a hidden opportunity which allowed to solve the task more efficiently.
We show that humans tend to discover this regularity through insight, rather
than gradually. Notably, neural networks with regularised gate modulation
closely mimicked behavioural characteristics of human insights, exhibiting
delay of insight, suddenness and selective occurrence. Analyses of network
learning dynamics revealed that insight-like behaviour crucially depended on
noise added to gradient updates, and was preceded by ``silent knowledge&apos;&apos; that
is initially suppressed by regularised (attentional) gating. This suggests that
insights can arise naturally from gradual learning, where they reflect the
combined influences of noise, attentional gating and regularisation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lowe_A/0/1/0/all/0/1&quot;&gt;Anika T. L&amp;#xf6;we&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touzo_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Touzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhle_Karbe_P/0/1/0/all/0/1&quot;&gt;Paul S. Muhle-Karbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1&quot;&gt;Andrew M. Saxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Summerfield_C/0/1/0/all/0/1&quot;&gt;Christopher Summerfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuck_N/0/1/0/all/0/1&quot;&gt;Nicolas W. Schuck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12200">
<title>A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12200</link>
<description rdf:parseType="Literal">&lt;p&gt;Named Entity Recognition (NER) models capable of Continual Learning (CL) are
realistically valuable in areas where entity types continuously increase (e.g.,
personal assistants). Meanwhile the learning paradigm of NER advances to new
patterns such as the span-based methods. However, its potential to CL has not
been fully explored. In this paper, we propose SpanKL, a simple yet effective
Span-based model with Knowledge distillation (KD) to preserve memories and
multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence
labeling approaches, the inherently independent modeling in span and entity
level with the designed coherent optimization on SpanKL promotes its learning
at each incremental step and mitigates the forgetting. Experiments on synthetic
CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly
outperforms previous SoTA in many aspects, and obtains the smallest gap from CL
to the upper bound revealing its high practiced value. The code is available at
https://github.com/Qznan/SpanKL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingcai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03323">
<title>CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03323</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal contrastive pretraining has been used to train multimodal
representation models, such as CLIP, on large amounts of paired image-text
data. However, previous studies have revealed that such models are vulnerable
to backdoor attacks. Specifically, when trained on backdoored examples, CLIP
learns spurious correlations between the embedded backdoor trigger and the
target label, aligning their representations in the joint embedding space.
Injecting even a small number of poisoned examples, such as 75 examples in 3
million pretraining data, can significantly manipulate the model&apos;s behavior,
making it difficult to detect or unlearn such correlations. To address this
issue, we propose CleanCLIP, a finetuning framework that weakens the learned
spurious associations introduced by backdoor attacks by independently
re-aligning the representations for individual modalities. We demonstrate that
unsupervised finetuning using a combination of multimodal contrastive and
unimodal self-supervised objectives for individual modalities can significantly
reduce the impact of the backdoor attack. Additionally, we show that supervised
finetuning on task-specific labeled image data removes the backdoor trigger
from the CLIP vision encoder. We show empirically that CleanCLIP maintains
model performance on benign examples while erasing a range of backdoor attacks
on multimodal contrastive learning. The code and checkpoints are available at
https://github.com/nishadsinghi/CleanCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhi_N/0/1/0/all/0/1&quot;&gt;Nishad Singhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04185">
<title>Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04185</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1&quot;&gt;Azade Nova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hanjun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11369">
<title>Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11369</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the following problem: Given an offline
demonstration dataset from an imperfect expert, what is the best way to
leverage it to bootstrap online learning performance in MDPs. We first propose
an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline
dataset, and information about the expert&apos;s behavioral policy used to generate
the offline dataset. Its cumulative Bayesian regret goes down to zero
exponentially fast in N, the offline dataset size if the expert is competent
enough. Since this algorithm is computationally impractical, we then propose
the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm
for online RL, and imitation learning. Our empirical results show that the
proposed iRLSVI algorithm is able to achieve significant reduction in regret as
compared to two baselines: no offline data, and offline dataset but used
without information about the generative policy. Our algorithm bridges online
RL and imitation learning for the first time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1&quot;&gt;Botao Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rahul Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dengwang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zheng Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02312">
<title>How to choose your best allies for a transferable attack?. (arXiv:2304.02312v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02312</link>
<description rdf:parseType="Literal">&lt;p&gt;The transferability of adversarial examples is a key issue in the security of
deep neural networks. The possibility of an adversarial example crafted for a
source model fooling another targeted model makes the threat of adversarial
attacks more realistic. Measuring transferability is a crucial problem, but the
Attack Success Rate alone does not provide a sound evaluation. This paper
proposes a new methodology for evaluating transferability by putting distortion
in a central position. This new tool shows that transferable attacks may
perform far worse than a black box attack if the attacker randomly picks the
source model. To address this issue, we propose a new selection mechanism,
called FiT, which aims at choosing the best source model with only a few
preliminary queries to the target. Our experimental results show that FiT is
highly effective at selecting the best source model for multiple scenarios such
as single-model attacks, ensemble-model attacks and multiple attacks (Code
available at: https://github.com/t-maho/transferability_measure_fit).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maho_T/0/1/0/all/0/1&quot;&gt;Thibault Maho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1&quot;&gt;Seyed-Mohsen Moosavi-Dezfooli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1&quot;&gt;Teddy Furon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04391">
<title>CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Representation Learning on graphs is gaining traction due to the
increasing abundance of unlabelled network data and the compactness, richness,
and usefulness of the representations generated. In this context, the need to
consider fairness and bias constraints while generating the representations has
been well-motivated and studied to some extent in prior works. One major
limitation of most of the prior works in this setting is that they do not aim
to address the bias generated due to connectivity patterns in the graphs, such
as varied node centrality, which leads to a disproportionate performance across
nodes. In our work, we aim to address this issue of mitigating bias due to
inherent graph structure in an unsupervised setting. To this end, we propose
CAFIN, a centrality-aware fairness-inducing framework that leverages the
structural information of graphs to tune the representations generated by
existing frameworks. We deploy it on GraphSAGE (a popular framework in this
domain) and showcase its efficacy on two downstream tasks - Node Classification
and Link Prediction. Empirically, CAFIN consistently reduces the performance
disparity across popular datasets (varying from 18 to 80% reduction in
performance disparity) from various domains while incurring only a minimal cost
of fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1&quot;&gt;Arvindh Arun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aanegola_A/0/1/0/all/0/1&quot;&gt;Aakash Aanegola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Amul Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanam_R/0/1/0/all/0/1&quot;&gt;Ramasuri Narayanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1&quot;&gt;Ponnurangam Kumaraguru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11235">
<title>Spatial-Language Attention Policies for Efficient Robot Learning. (arXiv:2304.11235v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11235</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite great strides in language-guided manipulation, existing work has been
constrained to table-top settings. Table-tops allow for perfect and consistent
camera angles, properties are that do not hold in mobile manipulation. Task
plans that involve moving around the environment must be robust to egocentric
views and changes in the plane and angle of grasp. A further challenge is
ensuring this is all true while still being able to learn skills efficiently
from limited data. We propose Spatial-Language Attention Policies (SLAP) as a
solution. SLAP uses three-dimensional tokens as the input representation to
train a single multi-task, language-conditioned action prediction policy. Our
method shows an 80% success rate in the real world across eight tasks with a
single model, and a 47.5% success rate when unseen clutter and unseen object
configurations are introduced, even with only a handful of examples per task.
This represents an improvement of 30% over prior work (20% given unseen
distractors and configurations). We see a 4x improvement over baseline in
mobile manipulation setting. In addition, we show how SLAPs robustness allows
us to execute Task Plans from open-vocabulary instructions using a large
language model for multi-step mobile manipulation. For videos, see the website:
https://robotslap.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parashar_P/0/1/0/all/0/1&quot;&gt;Priyam Parashar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakil_J/0/1/0/all/0/1&quot;&gt;Jay Vakil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Powers_S/0/1/0/all/0/1&quot;&gt;Sam Powers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03403">
<title>LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03403</link>
<description rdf:parseType="Literal">&lt;p&gt;As the field of automated machine learning (AutoML) advances, it becomes
increasingly important to incorporate domain knowledge into these systems. We
present an approach for doing so by harnessing the power of large language
models (LLMs). Specifically, we introduce Context-Aware Automated Feature
Engineering (CAAFE), a feature engineering method for tabular datasets that
utilizes an LLM to iteratively generate additional semantically meaningful
features for tabular datasets based on the description of the dataset. The
method produces both Python code for creating new features and explanations for
the utility of the generated features.
&lt;/p&gt;
&lt;p&gt;Despite being methodologically simple, CAAFE improves performance on 11 out
of 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 across
all dataset - similar to the improvement achieved by using a random forest
instead of logistic regression on our datasets.
&lt;/p&gt;
&lt;p&gt;Furthermore, CAAFE is interpretable by providing a textual explanation for
each generated feature. CAAFE paves the way for more extensive semi-automation
in data science tasks and emphasizes the significance of context-aware
solutions that can extend the scope of AutoML systems to semantic AutoML. We
release our $\href{https://github.com/automl/CAAFE}{code}$, a simple
$\href{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB_alZvyARTMjhl6RZf0a}{demo}$
and a $\href{https://pypi.org/project/caafe/}{python\ package}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollmann_N/0/1/0/all/0/1&quot;&gt;Noah Hollmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1&quot;&gt;Samuel M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14094">
<title>Sustainable Edge Intelligence Through Energy-Aware Early Exiting. (arXiv:2305.14094v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14094</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models have emerged as a promising solution for the
Internet of Things (IoT). However, due to their computational complexity, DL
models consume significant amounts of energy, which can rapidly drain the
battery and compromise the performance of IoT devices. For sustainable
operation, we consider an edge device with a rechargeable battery and energy
harvesting (EH) capabilities. In addition to the stochastic nature of the
ambient energy source, the harvesting rate is often insufficient to meet the
inference energy requirements, leading to drastic performance degradation in
energy-agnostic devices. To mitigate this problem, we propose energy-adaptive
dynamic early exiting (EE) to enable efficient and accurate inference in an EH
edge intelligence system. Our approach derives an energy-aware EE policy that
determines the optimal amount of computational processing on a per-sample
basis. The proposed policy balances the energy consumption to match the limited
incoming energy and achieves continuous availability. Numerical results show
that accuracy and service rate are improved up to 25% and 35%, respectively, in
comparison with an energy-agnostic policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bullo_M/0/1/0/all/0/1&quot;&gt;Marcello Bullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jardak_S/0/1/0/all/0/1&quot;&gt;Seifallah Jardak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Carnelli_P/0/1/0/all/0/1&quot;&gt;Pietro Carnelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16960">
<title>Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16960</link>
<description rdf:parseType="Literal">&lt;p&gt;Social alignment in AI systems aims to ensure that these models behave
according to established societal values. However, unlike humans, who derive
consensus on value judgments through social interaction, current language
models (LMs) are trained to rigidly replicate their training corpus in
isolation, leading to subpar generalization in unfamiliar scenarios and
vulnerability to adversarial attacks. This work presents a novel training
paradigm that permits LMs to learn from simulated social interactions. In
comparison to existing methodologies, our approach is considerably more
scalable and efficient, demonstrating superior performance in alignment
benchmarks and human evaluations. This paradigm shift in the training of LMs
brings us a step closer to developing AI systems that can robustly and
accurately reflect societal norms and values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruixin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chenyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Denny Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1&quot;&gt;Soroush Vosoughi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00932">
<title>Cross Modal Data Discovery over Structured and Unstructured Data Lakes. (arXiv:2306.00932v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00932</link>
<description rdf:parseType="Literal">&lt;p&gt;Organizations are collecting increasingly large amounts of data for data
driven decision making. These data are often dumped into a centralized
repository, e.g., a data lake, consisting of thousands of structured and
unstructured datasets. Perversely, such mixture of datasets makes the problem
of discovering elements (e.g., tables or documents) that are relevant to a
user&apos;s query or an analytical task very challenging. Despite the recent efforts
in data discovery, the problem remains widely open especially in the two fronts
of (1) discovering relationships and relatedness across structured and
unstructured datasets where existing techniques suffer from either scalability,
being customized for a specific problem type (e.g., entity matching or data
integration), or demolishing the structural properties on its way, and (2)
developing a holistic system for integrating various similarity measurements
and sketches in an effective way to boost the discovery accuracy. In this
paper, we propose a new data discovery system, named CMDL, for addressing these
two limitations. CMDL supports the data discovery process over both structured
and unstructured data while retaining the structural properties of tables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eltabakh_M/0/1/0/all/0/1&quot;&gt;Mohamed Y. Eltabakh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunjir_M/0/1/0/all/0/1&quot;&gt;Mayuresh Kunjir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmagarmid_A/0/1/0/all/0/1&quot;&gt;Ahmed Elmagarmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1&quot;&gt;Mohammad Shahmeer Ahmad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01007">
<title>Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01007</link>
<description rdf:parseType="Literal">&lt;p&gt;In the problem of online learning for changing environments, data are
sequentially received one after another over time, and their distribution
assumptions may vary frequently. Although existing methods demonstrate the
effectiveness of their learning algorithms by providing a tight bound on either
dynamic regret or adaptive regret, most of them completely ignore learning with
model fairness, defined as the statistical parity across different
sub-population (e.g., race and gender). Another drawback is that when adapting
to a new environment, an online learner needs to update model parameters with a
global change, which is costly and inefficient. Inspired by the sparse
mechanism shift hypothesis, we claim that changing environments in online
learning can be attributed to partial changes in learned parameters that are
specific to environments and the rest remain invariant to changing
environments. To this end, in this paper, we propose a novel algorithm under
the assumption that data collected at each time can be disentangled with two
representations, an environment-invariant semantic factor and an
environment-specific variation factor. The semantic factor is further used for
fair prediction under a group fairness constraint. To evaluate the sequence of
model parameters generated by the learner, a novel regret is proposed in which
it takes a mixed form of dynamic and static regret metrics followed by a
fairness-aware long-term constraint. The detailed analysis provides theoretical
guarantees for loss regret and violation of cumulative fairness constraints.
Empirical evaluations on real-world datasets demonstrate our proposed method
sequentially outperforms baseline methods in model accuracy and fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Feng Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_L/0/1/0/all/0/1&quot;&gt;Latifur Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1&quot;&gt;Christan Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01097">
<title>Fast Matrix Multiplication Without Tears: A Constraint Programming Approach. (arXiv:2306.01097v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;It is known that the multiplication of an $N \times M$ matrix with an $M
\times P$ matrix can be performed using fewer multiplications than what the
naive $NMP$ approach suggests. The most famous instance of this is Strassen&apos;s
algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8
multiplications. This gives rise to the constraint satisfaction problem of fast
matrix multiplication, where a set of $R &amp;lt; NMP$ multiplication terms must be
chosen and combined such that they satisfy correctness constraints on the
output matrix. Despite its highly combinatorial nature, this problem has not
been exhaustively examined from that perspective, as evidenced for example by
the recent deep reinforcement learning approach of AlphaTensor. In this work,
we propose a simple yet novel Constraint Programming approach to find
non-commutative algorithms for fast matrix multiplication or provide proof of
infeasibility otherwise. We propose a set of symmetry-breaking constraints and
valid inequalities that are particularly helpful in proving infeasibility. On
the feasible side, we find that exploiting solver performance variability in
conjunction with a sparsity-based problem decomposition enables finding
solutions for larger (feasible) instances of fast matrix multiplication. Our
experimental results using CP Optimizer demonstrate that we can find fast
matrix multiplication algorithms for matrices up to $3\times 3$ in a short
amount of time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1&quot;&gt;Arnaud Deza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaezipoor_P/0/1/0/all/0/1&quot;&gt;Pashootan Vaezipoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalil_E/0/1/0/all/0/1&quot;&gt;Elias B. Khalil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02520">
<title>A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02520</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent Traffic Monitoring (ITMo) technologies hold the potential for
improving road safety/security and for enabling smart city infrastructure.
Understanding traffic situations requires a complex fusion of perceptual
information with domain-specific and causal commonsense knowledge. Whereas
prior work has provided benchmarks and methods for traffic monitoring, it
remains unclear whether models can effectively align these information sources
and reason in novel scenarios. To address this assessment gap, we devise three
novel text-based tasks for situational reasoning in the traffic domain: i)
BDD-QA, which evaluates the ability of Language Models (LMs) to perform
situational decision-making, ii) TV-QA, which assesses LMs&apos; abilities to reason
about complex event causality, and iii) HDT-QA, which evaluates the ability of
models to solve human driving exams. We adopt four knowledge-enhanced methods
that have shown generalization capability across language reasoning tasks in
prior work, based on natural language inference, commonsense knowledge-graph
self-supervision, multi-QA joint training, and dense retrieval of domain
information. We associate each method with a relevant knowledge source,
including knowledge graphs, relevant benchmarks, and driving manuals. In
extensive experiments, we benchmark various knowledge-aware methods against the
three datasets, under zero-shot evaluation; we provide in-depth analyses of
model performance on data partitions and examine model predictions
categorically, to yield useful insights on traffic understanding, given
different background knowledge and reasoning strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiarui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1&quot;&gt;Filip Ilievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kaixin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollaa_A/0/1/0/all/0/1&quot;&gt;Aravinda Kollaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1&quot;&gt;Alessandro Oltramari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02786">
<title>Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02786</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual explanations are the de facto standard when tasked with
interpreting decisions of (opaque) predictive models. Their generation is often
subject to algorithmic and domain-specific constraints -- such as density-based
feasibility and attribute (im)mutability or directionality of change -- that
aim to maximise their real-life utility. In addition to desiderata with respect
to the counterfactual instance itself, existence of a viable path connecting it
with the factual data point, known as algorithmic recourse, has become an
important technical consideration. While both of these requirements ensure that
the steps of the journey as well as its destination are admissible, current
literature neglects the multiplicity of such counterfactual paths. To address
this shortcoming we introduce the novel concept of explanatory multiverse that
encompasses all the possible counterfactual journeys; we then show how to
navigate, reason about and compare the geometry of these trajectories -- their
affinity, branching, divergence and possible future convergence -- with two
methods: vector spaces and graphs. Implementing this (interactive) explanatory
process grants explainees agency by allowing them to select counterfactuals
based on the properties of the journey leading to them in addition to their
absolute differences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1&quot;&gt;Kacper Sokol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Small_E/0/1/0/all/0/1&quot;&gt;Edward Small&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1&quot;&gt;Yueqing Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02797">
<title>Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02797</link>
<description rdf:parseType="Literal">&lt;p&gt;We model learning of abstract symbolic concepts by performing Bayesian
inference over utterances in natural language. For efficient inference, we use
a large language model as a proposal distribution. We fit a prior to human data
to better model human learners, and evaluate on both generative and logical
concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1&quot;&gt;Kevin Ellis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08044">
<title>Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08044</link>
<description rdf:parseType="Literal">&lt;p&gt;Most medical treatment decisions are sequential in nature. Hence, there is
substantial hope that reinforcement learning may make it possible to formulate
precise data-driven treatment plans. However, a key challenge for most
applications in this field is the sparse nature of primarily mortality-based
reward functions, leading to decreased stability of offline estimates. In this
work, we introduce a deep Q-learning approach able to obtain more reliable
critical care policies. This method integrates relevant but noisy intermediate
biomarker signals into the reward specification, without compromising the
optimization of the main outcome of interest (e.g. patient survival). We
achieve this by first pruning the action set based on all available rewards,
and second training a final model based on the sparse main reward but with a
restricted action set. By disentangling accurate and approximated rewards
through action pruning, potential distortions of the main objective are
minimized, all while enabling the extraction of valuable information from
intermediate signals that can guide the learning process. We evaluate our
method in both off-policy and offline settings using simulated environments and
real health records of patients in intensive care units. Our empirical results
indicate that pruning significantly reduces the size of the action space while
staying mostly consistent with the actions taken by physicians, outperforming
the current state-of-the-art offline reinforcement learning method conservative
Q-learning. Our work is a step towards developing reliable policies by
effectively harnessing the wealth of available information in data-intensive
critical care environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirali_A/0/1/0/all/0/1&quot;&gt;Ali Shirali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schubert_A/0/1/0/all/0/1&quot;&gt;Alexander Schubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaa_A/0/1/0/all/0/1&quot;&gt;Ahmed Alaa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09862">
<title>DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09862</link>
<description rdf:parseType="Literal">&lt;p&gt;Stock trend forecasting is a fundamental task of quantitative investment
where precise predictions of price trends are indispensable. As an online
service, stock data continuously arrive over time. It is practical and
efficient to incrementally update the forecast model with the latest data which
may reveal some new patterns recurring in the future stock market. However,
incremental learning for stock trend forecasting still remains under-explored
due to the challenge of distribution shifts (a.k.a. concept drifts). With the
stock market dynamically evolving, the distribution of future data can slightly
or significantly differ from incremental data, hindering the effectiveness of
incremental updates. To address this challenge, we propose DoubleAdapt, an
end-to-end framework with two adapters, which can effectively adapt the data
and the model to mitigate the effects of distribution shifts. Our key insight
is to automatically learn how to adapt stock data into a locally stationary
distribution in favor of profitable updates. Complemented by data adaptation,
we can confidently adapt the model parameters under mitigated distribution
shifts. We cast each incremental learning task as a meta-learning task and
automatically optimize the adapters for desirable data adaptation and parameter
initialization. Experiments on real-world stock datasets demonstrate that
DoubleAdapt achieves state-of-the-art predictive performance and shows
considerable efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lifan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shuming Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanyan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13935">
<title>Are Good Explainers Secretly Human-in-the-Loop Active Learners?. (arXiv:2306.13935v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13935</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable AI (XAI) techniques have become popular for multiple use-cases in
the past few years. Here we consider its use in studying model predictions to
gather additional training data. We argue that this is equivalent to Active
Learning, where the query strategy involves a human-in-the-loop. We provide a
mathematical approximation for the role of the human, and present a general
formalization of the end-to-end workflow. This enables us to rigorously compare
this use with standard Active Learning algorithms, while allowing for
extensions to the workflow. An added benefit is that their utility can be
assessed via simulation instead of conducting expensive user-studies. We also
present some initial promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1&quot;&gt;Emma Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghose_A/0/1/0/all/0/1&quot;&gt;Abhishek Ghose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of complex human interactions and group activities has
garnered attention in human-centric computer vision. However, the advancement
of the related tasks is hindered due to the difficulty of obtaining large-scale
labeled real-world datasets. To mitigate the issue, we propose M3Act, a
multi-view multi-group multi-person human atomic action and group activity data
generator. Powered by the Unity engine, M3Act contains simulation-ready 3D
scenes and human assets, configurable lighting and camera systems, highly
parameterized modular group activities, and a large degree of domain
randomization during the data generation process. Our data generator is capable
of generating large-scale datasets of human activities with multiple
viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality
annotations for individual persons and multi-person groups (2D bounding boxes,
instance segmentation masks, individual actions and group activity categories).
Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group
activity recognition and RGB-based multi-person pose tracking. The results
indicate that learning from our synthetic datasets largely improves the model
performances on real-world datasets, with the highest gain of 5.59% and 7.32%
respectively in group and person recognition accuracy on CAD2, as well as an
improvement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data also
leads to faster model convergence on downstream tasks (up to 6.8% faster).
Moreover, M3Act opens new research problems for 3D group activity generation.
We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with
larger group sizes and higher complexity of inter-person interactions than
previous multi-person datasets. We define multiple metrics and propose a
competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1&quot;&gt;Aditya Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00259">
<title>InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00259</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) performs tasks by prompting a large language model
(LLM) using an instruction and a small set of annotated examples called
demonstrations. Recent work has shown that precise details of the inputs used
in the ICL prompt significantly impact performance, which has incentivized
instruction selection algorithms. The effect of instruction-choice however is
severely underexplored, with existing analyses restricted to shallow subsets of
models and tasks, limiting the generalizability of their insights. We develop
InstructEval, an ICL evaluation suite to conduct a thorough assessment of these
techniques. The suite includes 13 open-sourced LLMs of varying scales from four
model families, and covers nine tasks across three categories. Using the suite,
we evaluate the relative performance of seven popular instruction selection
methods over five metrics relevant to ICL. Our experiments reveal that using
curated manually-written instructions or simple instructions without any
task-specific descriptions often elicits superior ICL performance overall than
that of automatic instruction-induction methods, pointing to a lack of
generalizability among the latter. We release our evaluation suite for
benchmarking instruction selection approaches and enabling more generalizable
methods in this space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1&quot;&gt;Anirudh Ajith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chris Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Mengzhou Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Ameet Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1&quot;&gt;Karthik Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02054">
<title>Emoji Prediction using Transformer Models. (arXiv:2307.02054v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02054</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the use of emojis in social media has increased
dramatically, making them an important element in understanding online
communication. However, predicting the meaning of emojis in a given text is a
challenging task due to their ambiguous nature. In this study, we propose a
transformer-based approach for emoji prediction using BERT, a widely-used
pre-trained language model. We fine-tuned BERT on a large corpus of text
containing both text and emojis to predict the most appropriate emoji for a
given text. Our experimental results demonstrate that our approach outperforms
several state-of-the-art models in predicting emojis with an accuracy of over
75 percent. This work has potential applications in natural language
processing, sentiment analysis, and social media marketing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nusrat_M/0/1/0/all/0/1&quot;&gt;Muhammad Osama Nusrat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habib_Z/0/1/0/all/0/1&quot;&gt;Zeeshan Habib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mehreen Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1&quot;&gt;Saad Ahmed Jamal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02631">
<title>An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02631</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Myeloid Leukemia (AML) is one of the most aggressive types of
hematological neoplasm. To support the specialists&apos; decision about the
appropriate therapy, patients with AML receive a prognostic of outcomes
according to their cytogenetic and molecular characteristics, often divided
into three risk categories: favorable, intermediate, and adverse. However, the
current risk classification has known problems, such as the heterogeneity
between patients of the same risk group and no clear definition of the
intermediate risk category. Moreover, as most patients with AML receive an
intermediate-risk classification, specialists often demand other tests and
analyses, leading to delayed treatment and worsening of the patient&apos;s clinical
condition. This paper presents the data analysis and an explainable
machine-learning model to support the decision about the most appropriate
therapy protocol according to the patient&apos;s survival prediction. In addition to
the prediction model being explainable, the results obtained are promising and
indicate that it is possible to use it to support the specialists&apos; decisions
safely. Most importantly, the findings offered in this study have the potential
to open new avenues of research toward better treatments and prognostic
markers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1&quot;&gt;Jade M. Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_G/0/1/0/all/0/1&quot;&gt;Giovanna A. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_Neto_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o A. Machado-Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1&quot;&gt;Tiago A. Almeida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03913">
<title>Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03913</link>
<description rdf:parseType="Literal">&lt;p&gt;Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the &quot;teaming&quot; paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
&lt;/p&gt;
&lt;p&gt;Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zaifeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04251">
<title>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04251</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1&quot;&gt;Salman Mohamadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1&quot;&gt;Ghulam Mujtaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1&quot;&gt;Gianfranco Doretto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1&quot;&gt;Donald A. Adjeroh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04368">
<title>ECS -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.04368v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04368</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing capabilities of machine learning systems and their
potential use in safety-critical systems, ensuring high-quality data is
becoming increasingly important. In this paper we present a novel approach for
the assurance of data quality. For this purpose, the mathematical basics are
first discussed and the approach is presented using multiple examples. This
results in the detection of data points with potentially harmful properties for
the use in safety-critical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sieberichs_C/0/1/0/all/0/1&quot;&gt;Christian Sieberichs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geerkens_S/0/1/0/all/0/1&quot;&gt;Simon Geerkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1&quot;&gt;Thomas Waschulzik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05358">
<title>Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05358</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning has become a popular method to learn from decentralized
heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train
models from a small fraction of labeled data due to label scarcity on
decentralized clients. Existing FSSL methods assume independent and identically
distributed (IID) labeled data across clients and consistent class distribution
between labeled and unlabeled data within a client. This work studies a more
practical and challenging scenario of FSSL, where data distribution is
different not only across clients but also within a client between labeled and
unlabeled data. To address this challenge, we propose a novel FSSL framework
with dual regulators, FedDure.} FedDure lifts the previous assumption with a
coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg
regularizes the updating of the local model by tracking the learning effect on
labeled data distribution; F-reg learns an adaptive weighting scheme tailored
for unlabeled instances in each client. We further formulate the client model
training as bi-level optimization that adaptively optimizes the model in the
client with two regulators. Theoretically, we show the convergence guarantee of
the dual regulators. Empirically, we demonstrate that FedDure is superior to
the existing methods across a wide range of settings, notably by more than 11%
on CIFAR-10 and CINIC-10 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Sikai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kunlin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1&quot;&gt;Shuai Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05360">
<title>Unmasking the giant: A comprehensive evaluation of ChatGPT&apos;s proficiency in coding algorithms and data structures. (arXiv:2307.05360v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05360</link>
<description rdf:parseType="Literal">&lt;p&gt;The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT&apos;s
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arefin_S/0/1/0/all/0/1&quot;&gt;Sayed Erfan Arefin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heya_T/0/1/0/all/0/1&quot;&gt;Tasnia Ashrafi Heya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Qudah_H/0/1/0/all/0/1&quot;&gt;Hasan Al-Qudah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ineza_Y/0/1/0/all/0/1&quot;&gt;Ynes Ineza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serwadda_A/0/1/0/all/0/1&quot;&gt;Abdul Serwadda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05508">
<title>Human in the AI loop via xAI and Active Learning for Visual Inspection. (arXiv:2307.05508v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05508</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial revolutions have historically disrupted manufacturing by
introducing automation into production. Increasing automation reshapes the role
of the human worker. Advances in robotics and artificial intelligence open new
frontiers of human-machine collaboration. Such collaboration can be realized
considering two sub-fields of artificial intelligence: active learning and
explainable artificial intelligence. Active learning aims to devise strategies
that help obtain data that allows machine learning algorithms to learn better.
On the other hand, explainable artificial intelligence aims to make the machine
learning models intelligible to the human person. The present work first
describes Industry 5.0, human-machine collaboration, and state-of-the-art
regarding quality inspection, emphasizing visual inspection. Then it outlines
how human-machine collaboration could be realized and enhanced in visual
inspection. Finally, some of the results obtained in the EU H2020 STAR project
regarding visual inspection are shared, considering artificial intelligence,
human digital twins, and cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozanec_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#x17e;e M. Ro&amp;#x17e;anec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montini_E/0/1/0/all/0/1&quot;&gt;Elias Montini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutrona_V/0/1/0/all/0/1&quot;&gt;Vincenzo Cutrona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papamartzivanos_D/0/1/0/all/0/1&quot;&gt;Dimitrios Papamartzivanos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemencic_T/0/1/0/all/0/1&quot;&gt;Timotej Klemen&amp;#x10d;i&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuna_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; Fortuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1&quot;&gt;Dunja Mladeni&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veliou_E/0/1/0/all/0/1&quot;&gt;Entso Veliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannetsos_T/0/1/0/all/0/1&quot;&gt;Thanassis Giannetsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emmanouilidis_C/0/1/0/all/0/1&quot;&gt;Christos Emmanouilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06947">
<title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on three large-scale datasets
(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our
code/models are released at https://github.com/TalalWasim/Video-FocalNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1&quot;&gt;Muhammad Uzair Khattak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07119">
<title>DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07119</link>
<description rdf:parseType="Literal">&lt;p&gt;Current automated machine learning (ML) tools are model-centric, focusing on
model selection and parameter optimization. However, the majority of the time
in data analysis is devoted to data cleaning and wrangling, for which limited
tools are available. Here we present DataAssist, an automated data preparation
and cleaning platform that enhances dataset quality using ML-informed methods.
We show that DataAssist provides a pipeline for exploratory data analysis and
data cleaning, including generating visualization for user-selected variables,
unifying data annotation, suggesting anomaly removal, and preprocessing data.
The exported dataset can be readily integrated with other autoML tools or
user-specified model for downstream analysis. Our data-centric tool is
applicable to a variety of fields, including economics, business, and
forecasting applications saving over 50% time of the time spent on data
cleansing and preparation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1&quot;&gt;Kartikay Goyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1&quot;&gt;Vakul Goyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07417">
<title>RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07417</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation has been widely used in low-resource NER tasks to tackle
the problem of data sparsity. However, previous data augmentation methods have
the disadvantages of disrupted syntactic structures, token-label mismatch, and
requirement for external knowledge or manual effort. To address these issues,
we propose Robust Prompt-based Data Augmentation (RoPDA) for low-resource NER.
Based on pre-trained language models (PLMs) with continuous prompt, RoPDA
performs entity augmentation and context augmentation through five fundamental
augmentation operations to generate label-flipping and label-preserving
examples. To optimize the utilization of the augmented samples, we present two
techniques: Self-Consistency Filtering and mixup. The former effectively
eliminates low-quality samples, while the latter prevents performance
degradation arising from the direct utilization of label-flipping samples.
Extensive experiments on three benchmarks from different domains demonstrate
that RoPDA significantly improves upon strong baselines, and also outperforms
state-of-the-art semi-supervised learning methods when unlabeled data is
included.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sihan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Furao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>