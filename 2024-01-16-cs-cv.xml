<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.03180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02877" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.06133">
<title>The possibility of making $\$138,000$ from shredded banknote pieces using computer vision. (arXiv:2401.06133v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06133</link>
<description rdf:parseType="Literal">&lt;p&gt;Every country must dispose of old banknotes. At the Hong Kong Monetary
Authority visitor center, visitors can buy a paperweight souvenir full of
shredded banknotes. Even though the shredded banknotes are small, by using
computer vision, it is possible to reconstruct the whole banknote like a jigsaw
puzzle. Each paperweight souvenir costs $\$100$ HKD, and it is claimed to
contain shredded banknotes equivalent to 138 complete $\$1000$ HKD banknotes.
In theory, $\$138,000$ HKD can be recovered by using computer vision. This
paper discusses the technique of collecting shredded banknote pieces and
applying a computer vision program.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1&quot;&gt;Chung To Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06143">
<title>Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and Neural Radiance Fields. (arXiv:2401.06143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06143</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of digital situational awareness during disaster situations,
accurate digital representations, like 3D models, play an indispensable role.
To ensure the safety of rescue teams, robotic platforms are often deployed to
generate these models. In this paper, we introduce an innovative approach that
synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller
than 30 cm, equipped with 360 degree cameras and the advances of Neural
Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D
representation of any scene using 2D images and then synthesize it from various
angles upon request. This method is especially tailored for urban environments
which have experienced significant destruction, where the structural integrity
of buildings is compromised to the point of barring entry-commonly observed
post-earthquakes and after severe fires. We have tested our approach through
recent post-fire scenario, underlining the efficacy of NeRFs even in
challenging outdoor environments characterized by water, snow, varying light
conditions, and reflective surfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surmann_H/0/1/0/all/0/1&quot;&gt;Hartmut Surmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Digakis_N/0/1/0/all/0/1&quot;&gt;Niklas Digakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kremer_J/0/1/0/all/0/1&quot;&gt;Jan-Nicklas Kremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meine_J/0/1/0/all/0/1&quot;&gt;Julien Meine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_M/0/1/0/all/0/1&quot;&gt;Max Schulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voigt_N/0/1/0/all/0/1&quot;&gt;Niklas Voigt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06144">
<title>DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06144</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion generative models have achieved remarkable success in generating
images with a fixed resolution. However, existing models have limited ability
to generalize to different resolutions when training data at those resolutions
are not available. Leveraging techniques from operator learning, we present a
novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the
score operator by combining both spatial and spectral information at multiple
resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1)
simultaneously training on multiple resolutions improves FID over training at
any single fixed resolution; 2) DFU generalizes beyond its training
resolutions, allowing for coherent, high-fidelity generation at
higher-resolutions with the same model, i.e. zero-shot super-resolution
image-generation; 3) we propose a fine-tuning strategy to further enhance the
zero-shot super-resolution image-generation capability of our model, leading to
a FID of 11.3 at 1.66 times the maximum training resolution on FFHQ, which no
other method can come close to achieving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havrilla_A/0/1/0/all/0/1&quot;&gt;Alex Havrilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojas_K/0/1/0/all/0/1&quot;&gt;Kevin Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1&quot;&gt;Wenjing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Molei Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06145">
<title>Minuet: Accelerating 3D Sparse Convolutions on GPUs. (arXiv:2401.06145v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.06145</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Convolution (SC) is widely used for processing 3D point clouds that
are inherently sparse. Different from dense convolution, SC preserves the
sparsity of the input point cloud by only allowing outputs to specific
locations. To efficiently compute SC, prior SC engines first use hash tables to
build a kernel map that stores the necessary General Matrix Multiplication
(GEMM) operations to be executed (Map step), and then use a Gather-GEMM-Scatter
process to execute these GEMM operations (GMaS step). In this work, we analyze
the shortcomings of prior state-of-the-art SC engines, and propose Minuet, a
novel memory-efficient SC engine tailored for modern GPUs. Minuet proposes to
(i) replace the hash tables used in the Map step with a novel segmented sorting
double-traversed binary search algorithm that highly utilizes the on-chip
memory hierarchy of GPUs, (ii) use a lightweight scheme to autotune the tile
size in the Gather and Scatter operations of the GMaS step, such that to adapt
the execution to the particular characteristics of each SC layer, dataset, and
GPU architecture, and (iii) employ a padding-efficient GEMM grouping approach
that reduces both memory padding and kernel launching overheads. Our
evaluations show that Minuet significantly outperforms prior SC engines by on
average $1.74\times$ (up to $2.22\times$) for end-to-end point cloud network
executions. Our novel segmented sorting double-traversed binary search
algorithm achieves superior speedups by $15.8\times$ on average (up to
$26.8\times$) over prior SC engines in the Map step. The source code of Minuet
is publicly available at https://github.com/UofT-EcoSystem/Minuet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannoula_C/0/1/0/all/0/1&quot;&gt;Christina Giannoula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1&quot;&gt;Mostafa Elhoushi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleeson_J/0/1/0/all/0/1&quot;&gt;James Gleeson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1&quot;&gt;Gennady Pekhimenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06146">
<title>AAMDM: Accelerated Auto-regressive Motion Diffusion Model. (arXiv:2401.06146v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive motion synthesis is essential in creating immersive experiences
in entertainment applications, such as video games and virtual reality.
However, generating animations that are both high-quality and contextually
responsive remains a challenge. Traditional techniques in the game industry can
produce high-fidelity animations but suffer from high computational costs and
poor scalability. Trained neural network models alleviate the memory and speed
issues, yet fall short on generating diverse motions. Diffusion models offer
diverse motion synthesis with low memory usage, but require expensive reverse
diffusion processes. This paper introduces the Accelerated Auto-regressive
Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to
achieve quality, diversity, and efficiency all together. AAMDM integrates
Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive
Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a
lower-dimensional embedded space rather than the full-dimensional pose space,
which reduces the training complexity as well as further improves the
performance. We show that AAMDM outperforms existing methods in motion quality,
diversity, and runtime efficiency, through comprehensive quantitative analyses
and visual comparisons. We also demonstrate the effectiveness of each
algorithmic component through ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1&quot;&gt;Calvin Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1&quot;&gt;Guanqiao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1&quot;&gt;KangKang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Sehoon Ha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06148">
<title>Artificial Intelligence for Digital and Computational Pathology. (arXiv:2401.06148v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06148</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in digitizing tissue slides and the fast-paced progress in
artificial intelligence, including deep learning, have boosted the field of
computational pathology. This field holds tremendous potential to automate
clinical diagnosis, predict patient prognosis and response to therapy, and
discover new morphological biomarkers from tissue images. Some of these
artificial intelligence-based systems are now getting approved to assist
clinical diagnosis; however, technical barriers remain for their widespread
clinical adoption and integration as a research tool. This Review consolidates
recent methodological advances in computational pathology for predicting
clinical end points in whole-slide images and highlights how these developments
enable the automation of clinical practice and the discovery of new biomarkers.
We then provide future perspectives as the field expands into a broader range
of clinical and research tasks with increasingly diverse modalities of clinical
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1&quot;&gt;Andrew H. Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaume_G/0/1/0/all/0/1&quot;&gt;Guillaume Jaume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1&quot;&gt;Drew F.K. Williamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vaidya_A/0/1/0/all/0/1&quot;&gt;Anurag Vaidya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miller_T/0/1/0/all/0/1&quot;&gt;Tiffany R. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06149">
<title>Image Classifier Based Generative Method for Planar Antenna Design. (arXiv:2401.06149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06149</link>
<description rdf:parseType="Literal">&lt;p&gt;To extend the antenna design on printed circuit boards (PCBs) for more
engineers of interest, we propose a simple method that models PCB antennas with
a few basic components. By taking two separate steps to decide their geometric
dimensions and positions, antenna prototypes can be facilitated with no
experience required. Random sampling statistics relate to the quality of
dimensions are used in selecting among dimension candidates. A novel
image-based classifier using a convolutional neural network (CNN) is introduced
to further determine the positions of these fixed-dimension components. Two
examples from wearable products have been chosen to examine the entire
workflow. Their final designs are realistic and their performance metrics are
not inferior to the ones designed by experienced engineers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_W/0/1/0/all/0/1&quot;&gt;Weiping Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Andrew Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisharat_D/0/1/0/all/0/1&quot;&gt;Dia&amp;#x27;a Bisharat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qing Huo Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06150">
<title>D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06150</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the challenge of automatically assessing physical
rehabilitation exercises for patients who perform the exercises without
clinician supervision. The objective is to provide a quality score to ensure
correct performance and achieve desired results. To achieve this goal, a new
graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with
Transformer, is introduced. This model combines a modified version of STGCN and
transformer architectures for efficient handling of spatio-temporal data. The
key idea is to consider skeleton data respecting its non-linear structure as a
graph and detecting joints playing the main role in each rehabilitation
exercise. Dense connections and GRU mechanisms are used to rapidly process
large 3D skeleton inputs and effectively model temporal dynamics. The
transformer encoder&apos;s attention mechanism focuses on relevant parts of the
input sequence, making it useful for evaluating rehabilitation exercises. The
evaluation of our proposed approach on the KIMORE and UI-PRMD datasets
highlighted its potential, surpassing state-of-the-art methods in terms of
accuracy and computational time. This resulted in faster and more accurate
learning and assessment of rehabilitation exercises. Additionally, our model
provides valuable feedback through qualitative illustrations, effectively
highlighting the significance of joints in specific exercises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mourchid_Y/0/1/0/all/0/1&quot;&gt;Youssef Mourchid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Slama_R/0/1/0/all/0/1&quot;&gt;Rim Slama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06156">
<title>A Stochastic Approach to Classification Error Estimates in Convolutional Neural Networks. (arXiv:2401.06156v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06156</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report presents research results achieved in the field of
verification of trained Convolutional Neural Network (CNN) used for image
classification in safety-critical applications. As running example, we use the
obstacle detection function needed in future autonomous freight trains with
Grade of Automation (GoA) 4. It is shown that systems like GoA 4 freight trains
are indeed certifiable today with new standards like ANSI/UL 4600 and ISO 21448
used in addition to the long-existing standards EN 50128 and EN 50129.
Moreover, we present a quantitative analysis of the system-level hazard rate to
be expected from an obstacle detection function. It is shown that using
sensor/perceptor fusion, the fused detection system can meet the tolerable
hazard rate deemed to be acceptable for the safety integrity level to be
applied (SIL-3). A mathematical analysis of CNN models is performed which
results in the identification of classification clusters and equivalence
classes partitioning the image input space of the CNN. These clusters and
classes are used to introduce a novel statistical testing method for
determining the residual error probability of a trained CNN and an associated
upper confidence limit. We argue that this greybox approach to CNN
verification, taking into account the CNN model&apos;s internal structure, is
essential for justifying that the statistical tests have covered the trained
CNN with its neurons and inter-layer mappings in a comprehensive way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peleska_J/0/1/0/all/0/1&quot;&gt;Jan Peleska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruning_F/0/1/0/all/0/1&quot;&gt;Felix Br&amp;#xfc;ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gleirscher_M/0/1/0/all/0/1&quot;&gt;Mario Gleirscher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wen-ling Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06157">
<title>UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection. (arXiv:2401.06157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06157</link>
<description rdf:parseType="Literal">&lt;p&gt;Invasive signal crayfish have a detrimental impact on ecosystems. They spread
the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to
the native white clawed crayfish, the only native crayfish species in Britain.
Invasive signal crayfish extensively burrow, causing habitat destruction,
erosion of river banks and adverse changes in water quality, while also
competing with native species for resources and leading to declines in native
populations. Moreover, pollution exacerbates the vulnerability of White-clawed
crayfish, with their populations declining by over 90% in certain English
counties, making them highly susceptible to extinction. To safeguard aquatic
ecosystems, it is imperative to address the challenges posed by invasive
species and discarded plastics in the United Kingdom&apos;s river ecosystem&apos;s. The
UDEEP platform can play a crucial role in environmental monitoring by
performing on-the-fly classification of Signal crayfish and plastic debris
while leveraging the efficacy of AI, IoT devices and the power of edge
computing (i.e., NJN). By providing accurate data on the presence, spread and
abundance of these species, the UDEEP platform can contribute to monitoring
efforts and aid in mitigating the spread of invasive species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monari_D/0/1/0/all/0/1&quot;&gt;Dennis Monari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larkin_J/0/1/0/all/0/1&quot;&gt;Jack Larkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1&quot;&gt;Pedro Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1&quot;&gt;Jordan J. Bird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1&quot;&gt;Isibor Kennedy Ihianle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahaya_S/0/1/0/all/0/1&quot;&gt;Salisu Wada Yahaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tash_F/0/1/0/all/0/1&quot;&gt;Farhad Fassihi Tash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mahmudul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotfi_A/0/1/0/all/0/1&quot;&gt;Ahmad Lotfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06159">
<title>FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection. (arXiv:2401.06159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06159</link>
<description rdf:parseType="Literal">&lt;p&gt;Rotation-equivariance is an essential yet challenging property in oriented
object detection. While general object detectors naturally leverage robustness
to spatial shifts due to the translation-equivariance of the conventional CNNs,
achieving rotation-equivariance remains an elusive goal. Current detectors
deploy various alignment techniques to derive rotation-invariant features, but
still rely on high capacity models and heavy data augmentation with all
possible rotations. In this paper, we introduce a Fully Rotation-Equivariant
Oriented Object Detector (FRED), whose entire process from the image to the
bounding box prediction is strictly equivariant. Specifically, we decouple the
invariant task (object classification) and the equivariant task (object
localization) to achieve end-to-end equivariance. We represent the bounding box
as a set of rotation-equivariant vectors to implement rotation-equivariant
localization. Moreover, we utilized these rotation-equivariant vectors as
offsets in the deformable convolution, thereby enhancing the existing
advantages of spatial adaptation. Leveraging full rotation-equivariance, our
FRED demonstrates higher robustness to image-level rotation compared to
existing methods. Furthermore, we show that FRED is one step closer to non-axis
aligned learning through our experiments. Compared to state-of-the-art methods,
our proposed method delivers comparable performance on DOTA-v1.0 and
outperforms by 1.5 mAP on DOTA-v1.5, all while significantly reducing the model
parameters to 16%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chanho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1&quot;&gt;Jinsu Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shon_H/0/1/0/all/0/1&quot;&gt;Hyounguk Shon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_Y/0/1/0/all/0/1&quot;&gt;Yunho Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06167">
<title>Enhancing Multimodal Understanding with CLIP-Based Image-to-Text Transformation. (arXiv:2401.06167v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06167</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of transforming input images into corresponding textual
explanations stands as a crucial and complex endeavor within the domains of
computer vision and natural language processing. In this paper, we propose an
innovative ensemble approach that harnesses the capabilities of Contrastive
Language-Image Pretraining models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1&quot;&gt;Chang Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qunwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Liqiang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06169">
<title>Deep Learning model predicts the c-Kit-11 mutational status of canine cutaneous mast cell tumors by HE stained histological slides. (arXiv:2401.06169v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.06169</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous prognostic factors are currently assessed histopathologically in
biopsies of canine mast cell tumors to evaluate clinical behavior. In addition,
PCR analysis of the c-Kit exon 11 mutational status is often performed to
evaluate the potential success of a tyrosine kinase inhibitor therapy. This
project aimed at training deep learning models (DLMs) to identify the c-Kit-11
mutational status of MCTs solely based on morphology without additional
molecular analysis. HE slides of 195 mutated and 173 non-mutated tumors were
stained consecutively in two different laboratories and scanned with three
different slide scanners. This resulted in six different datasets
(stain-scanner variations) of whole slide images. DLMs were trained with single
and mixed datasets and their performances was assessed under scanner and
staining domain shifts. The DLMs correctly classified HE slides according to
their c-Kit 11 mutation status in, on average, 87% of cases for the best-suited
stain-scanner variant. A relevant performance drop could be observed when the
stain-scanner combination of the training and test dataset differed.
Multi-variant datasets improved the average accuracy but did not reach the
maximum accuracy of algorithms trained and tested on the same stain-scanner
variant. In summary, DLM-assisted morphological examination of MCTs can predict
c-Kit-exon 11 mutational status of MCTs with high accuracy. However, the
recognition performance is impeded by a change of scanner or staining protocol.
Larger data sets with higher numbers of scans originating from different
laboratories and scanners may lead to more robust DLMs to identify c-Kit
mutations in HE slides.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Puget_C/0/1/0/all/0/1&quot;&gt;Chlo&amp;#xe9; Puget&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ganz_J/0/1/0/all/0/1&quot;&gt;Jonathan Ganz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ostermaier_J/0/1/0/all/0/1&quot;&gt;Julian Ostermaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Konrad_T/0/1/0/all/0/1&quot;&gt;Thomas Konrad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Parlak_E/0/1/0/all/0/1&quot;&gt;Eda Parlak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bertram_C/0/1/0/all/0/1&quot;&gt;Christof Albert Bertram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kiupel_M/0/1/0/all/0/1&quot;&gt;Matti Kiupel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Breininger_K/0/1/0/all/0/1&quot;&gt;Katharina Breininger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aubreville_M/0/1/0/all/0/1&quot;&gt;Marc Aubreville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Klopfleisch_R/0/1/0/all/0/1&quot;&gt;Robert Klopfleisch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06182">
<title>Prediction of Cellular Identities from Trajectory and Cell Fate Information. (arXiv:2401.06182v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.06182</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining cell identities in imaging sequences is an important yet
challenging task. The conventional method for cell identification is via cell
tracking, which is complex and can be time-consuming. In this study, we propose
an innovative approach to cell identification during early C. elegans
embryogenesis using machine learning. We employed random forest, MLP, and LSTM
models, and tested cell classification accuracy on 3D time-lapse confocal
datasets spanning the first 4 hours of embryogenesis. By leveraging a small
number of spatial-temporal features of individual cells, including cell
trajectory and cell fate information, our models achieve an accuracy of over
90%, even with limited data. We also determine the most important feature
contributions and can interpret these features in the context of biological
knowledge. Our research demonstrates the success of predicting cell identities
in 4D imaging sequences directly from simple spatio-temporal features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Baiyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiamin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shroff_H/0/1/0/all/0/1&quot;&gt;Hari Shroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Riviere_P/0/1/0/all/0/1&quot;&gt;Patrick La Riviere&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06187">
<title>Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06187</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning has become a pivotal task to erase the influence of data
from a trained model. It adheres to recent data regulation standards and
enhances the privacy and security of machine learning applications. Most
existing machine unlearning methods perform well, however, they typically
necessitate access to the entirety of the remaining data, which might not be
feasible in certain scenarios. In this work, we present a new machine
unlearning approach Scissorhands, which operates effectively with only a subset
of the training data. Initially, Scissorhands identifies the most pertinent
parameters in the given model relative to the forgetting data via connection
sensitivity. This process involves reinitializing the most influential top-$k$
percent of these parameters, resulting in a trimmed model for erasing the
influence of the forgetting data. Subsequently, Scissorhands retrains the
trimmed model through a min-max optimization process, seeking parameters that
preserve information on the remaining data while discarding information related
to the forgetting data. Our experimental results, conducted across five
distinct datasets and utilizing both CNN and ViT, demonstrate that
Scissorhands, despite utilizing only a limited portion of the training data,
showcases competitive performance when compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06191">
<title>TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation. (arXiv:2401.06191v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06191</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the neural radiance field (NeRF) model has gained popularity
due to its ability to recover complex 3D scenes. Following its success, many
approaches proposed different NeRF representations in order to further improve
both runtime and performance. One such example is Triplane, in which NeRF is
represented using three 2D feature planes. This enables easily using existing
2D neural networks in this framework, e.g., to generate the three planes.
Despite its advantage, the triplane representation lagged behind in its 3D
recovery quality compared to NeRF solutions. In this work, we propose
TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF,
which closes the 3D recovery performance gap and is competitive with current
state-of-the-art methods. Building upon the triplane framework, we also propose
a novel super-resolution (SR) technique that combines a diffusion model with
TriNeRFLet for improving NeRF resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatib_R/0/1/0/all/0/1&quot;&gt;Rajaei Khatib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06197">
<title>Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications. (arXiv:2401.06197v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06197</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Deformable Convolution v4 (DCNv4), a highly efficient and
effective operator designed for a broad spectrum of vision applications. DCNv4
addresses the limitations of its predecessor, DCNv3, with two key enhancements:
1. removing softmax normalization in spatial aggregation to enhance its dynamic
property and expressive power and 2. optimizing memory access to minimize
redundant operations for speedup. These improvements result in a significantly
faster convergence compared to DCNv3 and a substantial increase in processing
speed, with DCNv4 achieving more than three times the forward speed. DCNv4
demonstrates exceptional performance across various tasks, including image
classification, instance and semantic segmentation, and notably, image
generation. When integrated into generative models like U-Net in the latent
diffusion model, DCNv4 outperforms its baseline, underscoring its possibility
to enhance generative models. In practical applications, replacing DCNv3 with
DCNv4 in the InternImage model to create FlashInternImage results in up to 80%
speed increase and further performance improvement without further
modifications. The advancements in speed and efficiency of DCNv4, combined with
its robust performance across diverse vision tasks, show its potential as a
foundational building block for future vision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiapeng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06209">
<title>Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. (arXiv:2401.06209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06209</link>
<description rdf:parseType="Literal">&lt;p&gt;Is vision good enough for language? Recent advancements in multimodal models
primarily stem from the powerful reasoning abilities of large language models
(LLMs). However, the visual component typically depends only on the
instance-level contrastive language-image pre-training (CLIP). Our research
reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still
exhibit systematic shortcomings. To understand the roots of these errors, we
explore the gap between the visual embedding space of CLIP and vision-only
self-supervised learning. We identify &apos;&apos;CLIP-blind pairs&apos;&apos; - images that CLIP
perceives as similar despite their clear visual differences. With these pairs,
we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes
areas where state-of-the-art systems, including GPT-4V, struggle with
straightforward questions across nine basic visual patterns, often providing
incorrect answers and hallucinated explanations. We further evaluate various
CLIP-based vision-and-language models and found a notable correlation between
visual patterns that challenge CLIP models and those problematic for multimodal
LLMs. As an initial effort to address these issues, we propose a Mixture of
Features (MoF) approach, demonstrating that integrating vision self-supervised
learning features with MLLMs can significantly enhance their visual grounding
capabilities. Together, our research suggests visual representation learning
remains an open challenge, and accurate visual grounding is crucial for future
successful multimodal systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Shengbang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1&quot;&gt;Yann LeCun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06224">
<title>Leveraging Frequency Domain Learning in 3D Vessel Segmentation. (arXiv:2401.06224v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06224</link>
<description rdf:parseType="Literal">&lt;p&gt;Coronary microvascular disease constitutes a substantial risk to human
health. Employing computer-aided analysis and diagnostic systems, medical
professionals can intervene early in disease progression, with 3D vessel
segmentation serving as a crucial component. Nevertheless, conventional U-Net
architectures tend to yield incoherent and imprecise segmentation outcomes,
particularly for small vessel structures. While models with attention
mechanisms, such as Transformers and large convolutional kernels, demonstrate
superior performance, their extensive computational demands during training and
inference lead to increased time complexity. In this study, we leverage Fourier
domain learning as a substitute for multi-scale convolutional kernels in 3D
hierarchical segmentation models, which can reduce computational expenses while
preserving global receptive fields within the network. Furthermore, a
zero-parameter frequency domain fusion method is designed to improve the skip
connections in U-Net architecture. Experimental results on a public dataset and
an in-house dataset indicate that our novel Fourier transformation-based
network achieves remarkable dice performance (84.37\% on ASACA500 and 80.32\%
on ImageCAS) in tubular vessel segmentation tasks and substantially reduces
computational requirements without compromising global receptive fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chengwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hongming Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Gangming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06244">
<title>YOLO-Former: YOLO Shakes Hand With ViT. (arXiv:2401.06244v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;The proposed YOLO-Former method seamlessly integrates the ideas of
transformer and YOLOv4 to create a highly accurate and efficient object
detection system. The method leverages the fast inference speed of YOLOv4 and
incorporates the advantages of the transformer architecture through the
integration of convolutional attention and transformer modules. The results
demonstrate the effectiveness of the proposed approach, with a mean average
precision (mAP) of 85.76\% on the Pascal VOC dataset, while maintaining high
prediction speed with a frame rate of 10.85 frames per second. The contribution
of this work lies in the demonstration of how the innovative combination of
these two state-of-the-art techniques can lead to further improvements in the
field of object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoramdel_J/0/1/0/all/0/1&quot;&gt;Javad Khoramdel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moori_A/0/1/0/all/0/1&quot;&gt;Ahmad Moori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borhani_Y/0/1/0/all/0/1&quot;&gt;Yasamin Borhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanbarzadeh_A/0/1/0/all/0/1&quot;&gt;Armin Ghanbarzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafi_E/0/1/0/all/0/1&quot;&gt;Esmaeil Najafi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06252">
<title>AGSPNet: A framework for parcel-scale crop fine-grained semantic change detection from UAV high-resolution imagery with agricultural geographic scene constraints. (arXiv:2401.06252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06252</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time and accurate information on fine-grained changes in crop
cultivation is of great significance for crop growth monitoring, yield
prediction and agricultural structure adjustment. Aiming at the problems of
serious spectral confusion in visible high-resolution unmanned aerial vehicle
(UAV) images of different phases, interference of large complex background and
salt-and-pepper noise by existing semantic change detection (SCD) algorithms,
in order to effectively extract deep image features of crops and meet the
demand of agricultural practical engineering applications, this paper designs
and proposes an agricultural geographic scene and parcel-scale constrained SCD
framework for crops (AGSPNet). AGSPNet framework contains three parts:
agricultural geographic scene (AGS) division module, parcel edge extraction
module and crop SCD module. Meanwhile, we produce and introduce an UAV image
SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple
semantic variation types of crops in complex geographical scene. We conduct
comparative experiments and accuracy evaluations in two test areas of this
dataset, and the results show that the crop SCD results of AGSPNet consistently
outperform other deep learning SCD models in terms of quantity and quality,
with the evaluation metrics F1-score, kappa, OA, and mIoU obtaining
improvements of 0.038, 0.021, 0.011 and 0.062, respectively, on average over
the sub-optimal method. The method proposed in this paper can clearly detect
the fine-grained change information of crop types in complex scenes, which can
provide scientific and technical support for smart agriculture monitoring and
management, food policy formulation and food security assurance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shaochun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengfan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Lina Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yunhao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06272">
<title>Segmentation of Mediastinal Lymph Nodes in CT with Anatomical Priors. (arXiv:2401.06272v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06272</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Lymph nodes (LNs) in the chest have a tendency to enlarge due to
various pathologies, such as lung cancer or pneumonia. Clinicians routinely
measure nodal size to monitor disease progression, confirm metastatic cancer,
and assess treatment response. However, variations in their shapes and
appearances make it cumbersome to identify LNs, which reside outside of most
organs. Methods: We propose to segment LNs in the mediastinum by leveraging the
anatomical priors of 28 different structures (e.g., lung, trachea etc.)
generated by the public TotalSegmentator tool. The CT volumes from 89 patients
available in the public NIH CT Lymph Node dataset were used to train three 3D
nnUNet models to segment LNs. The public St. Olavs dataset containing 15
patients (out-of-training-distribution) was used to evaluate the segmentation
performance. Results: For the 15 test patients, the 3D cascade nnUNet model
obtained the highest Dice score of 72.2 +- 22.3 for mediastinal LNs with short
axis diameter $\geq$ 8mm and 54.8 +- 23.8 for all LNs respectively. These
results represent an improvement of 10 points over a current approach that was
evaluated on the same test dataset. Conclusion: To our knowledge, we are the
first to harness 28 distinct anatomical priors to segment mediastinal LNs, and
our work can be extended to other nodal zones in the body. The proposed method
has immense potential for improved patient outcomes through the identification
of enlarged nodes in initial staging CT scans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mathai_T/0/1/0/all/0/1&quot;&gt;Tejas Sudharshan Mathai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bohan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1&quot;&gt;Ronald M. Summers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06278">
<title>A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy. (arXiv:2401.06278v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06278</link>
<description rdf:parseType="Literal">&lt;p&gt;Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally
use image encoders pretrained in a supervised manner with ImageNet-1k as
backbones. However, the use of modern self-supervised pretraining algorithms
and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may
allow for improvements. In this work, we study the fine-tuned performance of
models with ResNet50 and ViT-B backbones pretrained in self-supervised and
supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised
only) in a range of GIE vision tasks. In addition to identifying the most
suitable pretraining pipeline and backbone architecture for each task, out of
those considered, our results suggest: that self-supervised pretraining
generally produces more suitable backbones for GIE vision tasks than supervised
pretraining; that self-supervised pretraining with ImageNet-1k is typically
more suitable than pretraining with Hyperkvasir-unlabelled, with the notable
exception of monocular depth estimation in colonoscopy; and that ViT-Bs are
more suitable in polyp segmentation and monocular depth estimation in
colonoscopy, ResNet50s are more suitable in polyp detection, and both
architectures perform similarly in anatomical landmark recognition and
pathological finding characterisation. We hope this work draws attention to the
complexity of pretraining for GIE vision tasks, informs this development of
more suitable approaches than the convention, and inspires further research on
this topic to help advance this development. Code available:
\underline{github.com/ESandML/SSL4GIE}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanderson_E/0/1/0/all/0/1&quot;&gt;Edward Sanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matuszewski_B/0/1/0/all/0/1&quot;&gt;Bogdan J. Matuszewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06281">
<title>Demystifying Variational Diffusion Models. (arXiv:2401.06281v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06281</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the growing popularity of diffusion models, gaining a deep
understanding of the model class remains somewhat elusive for the uninitiated
in non-equilibrium statistical physics. With that in mind, we present what we
believe is a more straightforward introduction to diffusion models using
directed graphical modelling and variational Bayesian principles, which imposes
relatively fewer prerequisites on the average reader. Our exposition
constitutes a comprehensive technical review spanning from foundational
concepts like deep latent variable models to recent advances in continuous-time
diffusion-based modelling, highlighting theoretical connections between model
classes along the way. We provide additional mathematical insights that were
omitted in the seminal works whenever possible to aid in understanding, while
avoiding the introduction of new notation. We envision this article serving as
a useful educational supplement for both researchers and practitioners in the
area, and we welcome feedback and contributions from the community at
https://github.com/biomedia-mira/demystifying-diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06287">
<title>Hierarchical Augmentation and Distillation for Class Incremental Audio-Visual Video Recognition. (arXiv:2401.06287v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06287</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual video recognition (AVVR) aims to integrate audio and visual
clues to categorize videos accurately. While existing methods train AVVR models
using provided datasets and achieve satisfactory results, they struggle to
retain historical class knowledge when confronted with new classes in
real-world situations. Currently, there are no dedicated methods for addressing
this problem, so this paper concentrates on exploring Class Incremental
Audio-Visual Video Recognition (CIAVVR). For CIAVVR, since both stored data and
learned model of past classes contain historical knowledge, the core challenge
is how to capture past data knowledge and past model knowledge to prevent
catastrophic forgetting. We introduce Hierarchical Augmentation and
Distillation (HAD), which comprises the Hierarchical Augmentation Module (HAM)
and Hierarchical Distillation Module (HDM) to efficiently utilize the
hierarchical structure of data and models, respectively. Specifically, HAM
implements a novel augmentation strategy, segmental feature augmentation, to
preserve hierarchical model knowledge. Meanwhile, HDM introduces newly designed
hierarchical (video-distribution) logical distillation and hierarchical
(snippet-video) correlative distillation to capture and maintain the
hierarchical intra-sample knowledge of each data and the hierarchical
inter-sample knowledge between data, respectively. Evaluations on four
benchmarks (AVE, AVK-100, AVK-200, and AVK-400) demonstrate that the proposed
HAD effectively captures hierarchical information in both data and models,
resulting in better preservation of historical class knowledge and improved
performance. Furthermore, we provide a theoretical analysis to support the
necessity of the segmental feature augmentation strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1&quot;&gt;Yukun Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hantao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_L/0/1/0/all/0/1&quot;&gt;Liansheng Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06291">
<title>Frequency-Time Diffusion with Neural Cellular Automata. (arXiv:2401.06291v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06291</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising Diffusion Models (DDMs) have become the leading generative
technique for synthesizing high-quality images but are often constrained by
their UNet-based architectures that impose certain limitations. In particular,
the considerable size of often hundreds of millions of parameters makes them
impractical when hardware resources are limited. However, even with powerful
hardware, processing images in the gigapixel range is difficult. This is
especially true in fields such as microscopy or satellite imaging, where such
challenges arise from the limitation to a predefined generative size and the
inefficient scaling to larger images. We present two variations of Neural
Cellular Automata (NCA)-based DDM methods to address these challenges and
jumpstart NCA-based DDMs: Diff-NCA and FourierDiff-NCA. Diff-NCA performs
diffusion by using only local features of the underlying distribution, making
it suitable for applications where local features are critical. To communicate
global knowledge in image space, naive NCA setups require timesteps that
increase with the image scale. We solve this bottleneck of current NCA
architectures by introducing FourierDiff-NCA, which advances Diff-NCA by adding
a Fourier-based diffusion process and combines the frequency-organized Fourier
space with the image space. By initiating diffusion in the Fourier domain and
finalizing it in the image space, FourierDiff-NCA accelerates global
communication. We validate our techniques by using Diff-NCA (208k parameters)
to generate high-resolution digital pathology scans at 576x576 resolution and
FourierDiff-NCA (887k parameters) to synthesize CelebA images at 64x64,
outperforming VNCA and five times bigger UNet-based DDMs. In addition, we
demonstrate FourierDiff-NCA&apos;s capabilities in super-resolution, OOD image
synthesis, and inpainting without additional training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalkhof_J/0/1/0/all/0/1&quot;&gt;John Kalkhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_A/0/1/0/all/0/1&quot;&gt;Arlene K&amp;#xfc;hn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frisch_Y/0/1/0/all/0/1&quot;&gt;Yannik Frisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Anirban Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06310">
<title>Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06310</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have highlighted the issue of stereotypical depictions for
people of different identity groups in Text-to-Image (T2I) model generations.
However, these existing approaches have several key limitations, including a
noticeable lack of coverage of global identity groups in their evaluation, and
the range of their associated stereotypes. Additionally, they often lack a
critical distinction between inherently visual stereotypes, such as
`underweight&apos; or `sombrero&apos;, and culturally dependent stereotypes like
`attractive&apos; or `terrorist&apos;. In this work, we address these limitations with a
multifaceted approach that leverages existing textual resources to ground our
evaluation of geo-cultural stereotypes in the generated images from T2I models.
We employ existing stereotype benchmarks to identify and evaluate visual
stereotypes at a global scale, spanning 135 nationality-based identity groups.
We demonstrate that stereotypical attributes are thrice as likely to be present
in images of these identities as compared to other attributes. We further
investigate how disparately offensive the depictions of generated images are
for different nationalities. Finally, through a detailed case study, we reveal
how the &apos;default&apos; representations of all identity groups have a stereotypical
appearance. Moreover, for the Global South, images across different attributes
are visually similar, even when explicitly prompted otherwise. CONTENT WARNING:
Some examples may contain offensive stereotypes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1&quot;&gt;Akshita Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1&quot;&gt;Vinodkumar Prabhakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denton_R/0/1/0/all/0/1&quot;&gt;Remi Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszlo_S/0/1/0/all/0/1&quot;&gt;Sarah Laszlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1&quot;&gt;Shachi Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadri_R/0/1/0/all/0/1&quot;&gt;Rida Qadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1&quot;&gt;Sunipa Dev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06312">
<title>Video Super-Resolution Transformer with Masked Inter&amp;Intra-Frame Attention. (arXiv:2401.06312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06312</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Vision Transformer has achieved great success in recovering missing
details in low-resolution sequences, i.e., the video super-resolution (VSR)
task.Despite its superiority in VSR accuracy, the heavy computational burden as
well as the large memory footprint hinder the deployment of Transformer-based
VSR models on constrained devices.In this paper, we address the above issue by
proposing a novel feature-level masked processing framework: VSR with Masked
Intra and inter frame Attention (MIA-VSR).The core of MIA-VSR is leveraging
feature-level temporal continuity between adjacent frames to reduce redundant
computations and make more rational use of previously enhanced SR features.
Concretely, we propose an intra-frame and inter-frame attention block which
takes the respective roles of past features and input features into
consideration and only exploits previously enhanced features to provide
supplementary information. In addition, an adaptive block-wise mask prediction
module is developed to skip unimportant computations according to feature
similarity between adjacent frames. We conduct detailed ablation studies to
validate our contributions and compare the proposed method with recent
state-of-the-art VSR approaches. The experimental results demonstrate that
MIA-VSR improves the memory and computation efficiency over state-of-the-art
methods, without trading off PSNR accuracy. The code is available at
https://github.com/LabShuHangGU/MIA-VSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiaorui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Keze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Leida Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shuhang Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06331">
<title>Application Of Vision-Language Models For Assessing Osteoarthritis Disease Severity. (arXiv:2401.06331v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06331</link>
<description rdf:parseType="Literal">&lt;p&gt;Osteoarthritis (OA) poses a global health challenge, demanding precise
diagnostic methods. Current radiographic assessments are time consuming and
prone to variability, prompting the need for automated solutions. The existing
deep learning models for OA assessment are unimodal single task systems and
they don&apos;t incorporate relevant text information such as patient demographics,
disease history, or physician reports. This study investigates employing Vision
Language Processing (VLP) models to predict OA severity using Xray images and
corresponding reports. Our method leverages Xray images of the knee and diverse
report templates generated from tabular OA scoring values to train a CLIP
(Contrastive Language Image PreTraining) style VLP model. Furthermore, we
incorporate additional contrasting captions to enforce the model to
discriminate between positive and negative reports. Results demonstrate the
efficacy of these models in learning text image representations and their
contextual relationships, showcase potential advancement in OA assessment, and
establish a foundation for specialized vision language models in medical
contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felfeliyan_B/0/1/0/all/0/1&quot;&gt;Banafshe Felfeliyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shrimanti Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupper_J/0/1/0/all/0/1&quot;&gt;Jessica Kupper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaobo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hareendranathan_A/0/1/0/all/0/1&quot;&gt;Abhilash Hareendranathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaremko_J/0/1/0/all/0/1&quot;&gt;Jacob L. Jaremko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06341">
<title>AffordanceLLM: Grounding Affordance from Vision Language Models. (arXiv:2401.06341v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06341</link>
<description rdf:parseType="Literal">&lt;p&gt;Affordance grounding refers to the task of finding the area of an object with
which one can interact. It is a fundamental but challenging task, as a
successful solution requires the comprehensive understanding of a scene in
multiple aspects including detection, localization, and recognition of objects
with their parts, of geo-spatial configuration/layout of the scene, of 3D
shapes and physics, as well as of the functionality and potential interaction
of the objects and humans. Much of the knowledge is hidden and beyond the image
content with the supervised labels from a limited training set. In this paper,
we make an attempt to improve the generalization capability of the current
affordance grounding by taking the advantage of the rich world, abstract, and
human-object-interaction knowledge from pretrained large-scale vision language
models. Under the AGD20K benchmark, our proposed model demonstrates a
significant performance gain over the competing methods for in-the-wild object
affordance grounding. We further demonstrate it can ground affordance for
objects from random Internet images, even if both objects and actions are
unseen during training. Project site: https://jasonqsy.github.io/AffordanceLLM/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengyi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1&quot;&gt;Min Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Erran Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06344">
<title>Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06344</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting crowded intents and trajectories is crucial in varouls real-world
applications, including service robots and autonomous vehicles. Understanding
environmental dynamics is challenging, not only due to the complexities of
modeling pair-wise spatial and temporal interactions but also the diverse
influence of group-wise interactions. To decode the comprehensive pair-wise and
group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a
Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory
prediction. In Hyper-STTN, crowded group-wise correlations are constructed
using a set of multi-scale hypergraphs with varying group sizes, captured
through random-walk robability-based hypergraph spectral convolution.
Additionally, a spatial-temporal transformer is adapted to capture pedestrians&apos;
pair-wise latent interactions in spatial-temporal dimensions. These
heterogeneous group-wise and pair-wise are then fused and aligned though a
multimodal transformer network. Hyper-STTN outperformes other state-of-the-art
baselines and ablation models on 5 real-world pedestrian motion datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weizheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Le Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Baijian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guohua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1&quot;&gt;Byung-Cheol Min&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06345">
<title>Seek for Incantations: Towards Accurate Text-to-Image Diffusion Synthesis through Prompt Engineering. (arXiv:2401.06345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06345</link>
<description rdf:parseType="Literal">&lt;p&gt;The text-to-image synthesis by diffusion models has recently shown remarkable
performance in generating high-quality images. Although performs well for
simple texts, the models may get confused when faced with complex texts that
contain multiple objects or spatial relationships. To get the desired images, a
feasible way is to manually adjust the textual descriptions, i.e., narrating
the texts or adding some words, which is labor-consuming. In this paper, we
propose a framework to learn the proper textual descriptions for diffusion
models through prompt learning. By utilizing the quality guidance and the
semantic guidance derived from the pre-trained diffusion model, our method can
effectively learn the prompts to improve the matches between the input text and
the generated images. Extensive experiments and analyses have validated the
effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Junran Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06349">
<title>MedTransformer: Accurate AD Diagnosis for 3D MRI Images through 2D Vision Transformers. (arXiv:2401.06349v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06349</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated diagnosis of AD in brain images is becoming a clinically important
technique to support precision and efficient diagnosis and treatment planning.
A few efforts have been made to automatically diagnose AD in magnetic resonance
imaging (MRI) using three-dimensional CNNs. However, due to the complexity of
3D models, the performance is still unsatisfactory, both in terms of accuracy
and efficiency. To overcome the complexities of 3D images and 3D models, in
this study, we aim to attack this problem with 2D vision Transformers. We
propose a 2D transformer-based medical image model with various transformer
attention encoders to diagnose AD in 3D MRI images, by cutting the 3D images
into multiple 2D slices.The model consists of four main components: shared
encoders across three dimensions, dimension-specific encoders, attention across
images from the same dimension, and attention across three dimensions. It is
used to obtain attention relationships among multiple sequences from different
dimensions (axial, coronal, and sagittal) and multiple slices. We also propose
morphology augmentation, an erosion and dilation based method to increase the
structural difference between AD and normal images. In this experiment, we use
multiple datasets from ADNI, AIBL, MIRAID, OASIS to show the performance of our
model. Our proposed MedTransformer demonstrates a strong ability in diagnosing
AD. These results demonstrate the effectiveness of MedTransformer in learning
from 3D data using a much smaller model and its capability to generalize among
different medical tasks, which provides a possibility to help doctors diagnose
AD in a simpler way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06370">
<title>Graph Relation Distillation for Efficient Biomedical Instance Segmentation. (arXiv:2401.06370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06370</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance-aware embeddings predicted by deep neural networks have
revolutionized biomedical instance segmentation, but its resource requirements
are substantial. Knowledge distillation offers a solution by transferring
distilled knowledge from heavy teacher networks to lightweight yet
high-performance student networks. However, existing knowledge distillation
methods struggle to extract knowledge for distinguishing instances and overlook
global relation information. To address these challenges, we propose a graph
relation distillation approach for efficient biomedical instance segmentation,
which considers three essential types of knowledge: instance-level features,
instance relations, and pixel-level boundaries. We introduce two graph
distillation schemes deployed at both the intra-image level and the inter-image
level: instance graph distillation (IGD) and affinity graph distillation (AGD).
IGD constructs a graph representing instance features and relations,
transferring these two types of knowledge by enforcing instance graph
consistency. AGD constructs an affinity graph representing pixel relations to
capture structured knowledge of instance boundaries, transferring
boundary-related knowledge by ensuring pixel affinity consistency. Experimental
results on a number of biomedical datasets validate the effectiveness of our
approach, enabling student models with less than $ 1\%$ parameters and less
than $10\%$ inference time while achieving promising performance compared to
teacher models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yueyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06374">
<title>SamLP: A Customized Segment Anything Model for License Plate Detection. (arXiv:2401.06374v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06374</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of foundation model, this novel paradigm of deep learning
has encouraged many powerful achievements in natural language processing and
computer vision. There are many advantages of foundation model, such as
excellent feature extraction power, mighty generalization ability, great
few-shot and zero-shot learning capacity, etc. which are beneficial to vision
tasks. As the unique identity of vehicle, different countries and regions have
diverse license plate (LP) styles and appearances, and even different types of
vehicles have different LPs. However, recent deep learning based license plate
detectors are mainly trained on specific datasets, and these limited datasets
constrain the effectiveness and robustness of LP detectors. To alleviate the
negative impact of limited data, an attempt to exploit the advantages of
foundation model is implement in this paper. We customize a vision foundation
model, i.e. Segment Anything Model (SAM), for LP detection task and propose the
first LP detector based on vision foundation model, named SamLP. Specifically,
we design a Low-Rank Adaptation (LoRA) fine-tuning strategy to inject extra
parameters into SAM and transfer SAM into LP detection task. And then, we
further propose a promptable fine-tuning step to provide SamLP with prompatable
segmentation capacity. The experiments show that our proposed SamLP achieves
promising detection performance compared to other LP detectors. Meanwhile, the
proposed SamLP has great few-shot and zero-shot learning ability, which shows
the potential of transferring vision foundation model. The code is available at
https://github.com/Dinghaoxuan/SamLP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Haoxuan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06385">
<title>SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical Refinement and EM optimization. (arXiv:2401.06385v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06385</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Segmentation-Driven Deformation Multi-View Stereo
(SD-MVS), a method that can effectively tackle challenges in 3D reconstruction
of textureless areas. We are the first to adopt the Segment Anything Model
(SAM) to distinguish semantic instances in scenes and further leverage these
constraints for pixelwise patch deformation on both matching cost and
propagation. Concurrently, we propose a unique refinement strategy that
combines spherical coordinates and gradient descent on normals and pixelwise
search interval on depths, significantly improving the completeness of
reconstructed 3D model. Furthermore, we adopt the Expectation-Maximization (EM)
algorithm to alternately optimize the aggregate matching cost and
hyperparameters, effectively mitigating the problem of parameters being
excessively dependent on empirical tuning. Evaluations on the ETH3D
high-resolution multi-view stereo benchmark and the Tanks and Temples dataset
demonstrate that our method can achieve state-of-the-art results with less time
consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhenlong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiakai Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06395">
<title>ModaVerse: Efficiently Transforming Modalities with LLMs. (arXiv:2401.06395v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06395</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans possess the capability to comprehend diverse modalities and seamlessly
transfer information between them. In this work, we introduce ModaVerse, a
Multi-modal Large Language Model (MLLM) capable of comprehending and
transforming content across various modalities including images, videos, and
audio. Predominant MLLM frameworks have largely relied on the alignment of
latent spaces of textual and non-textual features. This alignment process,
which synchronizes a language model trained on textual data with encoders and
decoders trained on multi-modal data, often necessitates extensive training of
several projection layers in multiple stages. Inspired by LLM-as-agent
methodologies, we propose a novel Input/Output (I/O) alignment mechanism that
operates directly at the level of natural language. It aligns the LLM&apos;s output
with the input of generative models, avoiding the complexities associated with
latent feature alignments, and simplifying the multiple training stages of
existing MLLMs into a single, efficient process. This conceptual advancement
leads to significant reductions in both data and computational costs. By
conducting experiments on several benchmarks, we demonstrate that our approach
attains comparable performance with the state of the art while achieving
considerable efficiencies in data usage and training duration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06397">
<title>UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding. (arXiv:2401.06397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06397</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models, represented by Contrastive language-image
pre-training (CLIP), have gained increasing attention for jointly understanding
both vision and textual tasks. However, existing approaches primarily focus on
training models to match global image representations with textual
descriptions, thereby overlooking the critical alignment between local regions
and corresponding text tokens. This paper extends CLIP with multi-granularity
alignment. Notably, we deliberately construct a new dataset comprising pseudo
annotations at various levels of granularities, encompassing image-level,
region-level, and pixel-level captions/tags. Accordingly, we develop a unified
multi-granularity learning framework, named UMG-CLIP, that simultaneously
empowers the model with versatile perception abilities across different levels
of detail. Equipped with parameter efficient tuning, UMG-CLIP surpasses current
widely used CLIP models and achieves state-of-the-art performance on diverse
image understanding benchmarks, including open-world recognition, retrieval,
semantic segmentation, and panoptic segmentation tasks. We hope UMG-CLIP can
serve as a valuable option for advancing vision-language foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bowen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peisen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaoming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wenrui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Junni Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06400">
<title>Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06400</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering (VQA) is a task where an image is given, and a
series of questions are asked about the image. To build an efficient VQA
algorithm, a large amount of QA data is required which is very expensive.
Generating synthetic QA pairs based on templates is a practical way to obtain
data. However, VQA models trained on those data do not perform well on complex,
human-written questions. To address this issue, we propose a new method called
{\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a
sequence of QA interactions between a large language model and a VQA model
trained on synthetic data to reason and derive logical answers for
human-written questions. We tested the effectiveness of CoQAH on two types of
human-written VQA datasets for 3D-rendered and chest X-ray images and found
that it achieved state-of-the-art accuracy in both types of data. Notably,
CoQAH outperformed general vision-language models, VQA models, and medical
foundation models with no finetuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yeongjae Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1&quot;&gt;Heejun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yohan Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1&quot;&gt;Dongmyung Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06407">
<title>UAV-borne Mapping Algorithms for Canopy-Level and High-Speed Drone Applications. (arXiv:2401.06407v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.06407</link>
<description rdf:parseType="Literal">&lt;p&gt;This article presents a comprehensive review of and analysis of
state-of-the-art mapping algorithms for UAV (Unmanned Aerial Vehicle)
applications, focusing on canopy-level and high-speed scenarios. This article
presents a comprehensive exploration of sensor technologies suitable for UAV
mapping, assessing their capabilities to provide measurements that meet the
requirements of fast UAV mapping. Furthermore, the study conducts extensive
experiments in a simulated environment to evaluate the performance of three
distinct mapping algorithms: Direct Sparse Odometry (DSO), Stereo DSO (SDSO),
and DSO Lite (DSOL). The experiments delve into mapping accuracy and mapping
speed, providing valuable insights into the strengths and limitations of each
algorithm. The results highlight the versatility and shortcomings of these
algorithms in meeting the demands of modern UAV applications. The findings
contribute to a nuanced understanding of UAV mapping dynamics, emphasizing
their applicability in complex environments and high-speed scenarios. This
research not only serves as a benchmark for mapping algorithm comparisons but
also offers practical guidance for selecting sensors tailored to specific UAV
mapping applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jincheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolek_A/0/1/0/all/0/1&quot;&gt;Artur Wolek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_A/0/1/0/all/0/1&quot;&gt;Andrew R. Willis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06415">
<title>3D Reconstruction of Interacting Multi-Person in Clothing from a Single Image. (arXiv:2401.06415v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06415</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel pipeline to reconstruct the geometry of
interacting multi-person in clothing on a globally coherent scene space from a
single image. The main challenge arises from the occlusion: a part of a human
body is not visible from a single view due to the occlusion by others or the
self, which introduces missing geometry and physical implausibility (e.g.,
penetration). We overcome this challenge by utilizing two human priors for
complete 3D geometry and surface contacts. For the geometry prior, an encoder
learns to regress the image of a person with missing body parts to the latent
vectors; a decoder decodes these vectors to produce 3D features of the
associated geometry; and an implicit network combines these features with a
surface normal map to reconstruct a complete and detailed 3D humans. For the
contact prior, we develop an image-space contact detector that outputs a
probability distribution of surface contacts between people in 3D. We use these
priors to globally refine the body poses, enabling the penetration-free and
accurate reconstruction of interacting multi-person in clothing on the scene
space. The results demonstrate that our method is complete, globally coherent,
and physically plausible compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1&quot;&gt;Junuk Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hansol Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaewon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_N/0/1/0/all/0/1&quot;&gt;Nhat Nguyen Bao Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jae Shin Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seungryul Baek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06426">
<title>UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer. (arXiv:2401.06426v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06426</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional channel-wise pruning methods by reducing network channels
struggle to effectively prune efficient CNN models with depth-wise
convolutional layers and certain efficient modules, such as popular inverted
residual blocks. Prior depth pruning methods by reducing network depths are not
suitable for pruning some efficient models due to the existence of some
normalization layers. Moreover, finetuning subnet by directly removing
activation layers would corrupt the original model weights, hindering the
pruned model from achieving high performance. To address these issues, we
propose a novel depth pruning method for efficient models. Our approach
proposes a novel block pruning strategy and progressive training method for the
subnet. Additionally, we extend our pruning method to vision transformer
models. Experimental results demonstrate that our method consistently
outperforms existing depth pruning methods across various pruning
configurations. We obtained three pruned ConvNeXtV1 models with our method
applying on ConvNeXtV1, which surpass most SOTA efficient models with
comparable inference performance. Our method also achieves state-of-the-art
pruning performance on the vision transformer model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dehua Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanxian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mingjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinzhang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Fan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirasao_A/0/1/0/all/0/1&quot;&gt;Ashish Sirasao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06430">
<title>Mutual Distillation Learning For Person Re-Identification. (arXiv:2401.06430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06430</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid advancements in deep learning technologies, person
re-identification (ReID) has witnessed remarkable performance improvements.
However, the majority of prior works have traditionally focused on solving the
problem via extracting features solely from a single perspective, such as
uniform partitioning, hard attention mechanisms, or semantic masks. While these
approaches have demonstrated efficacy within specific contexts, they fall short
in diverse situations. In this paper, we propose a novel approach, Mutual
Distillation Learning For Person Re-identification (termed as MDPR), which
addresses the challenging problem from multiple perspectives within a single
unified model, leveraging the power of mutual distillation to enhance the
feature representations collectively. Specifically, our approach encompasses
two branches: a hard content branch to extract local features via a uniform
horizontal partitioning strategy and a Soft Content Branch to dynamically
distinguish between foreground and background and facilitate the extraction of
multi-granularity features via a carefully designed attention mechanism. To
facilitate knowledge exchange between these two branches, a mutual distillation
and fusion process is employed, promoting the capability of the outputs of each
branch. Extensive experiments are conducted on widely used person ReID datasets
to validate the effectiveness and superiority of our approach. Notably, our
method achieves an impressive $88.7\%/94.4\%$ in mAP/Rank-1 on the
DukeMTMC-reID dataset, surpassing the current state-of-the-art results. Our
source code is available at https://github.com/KuilongCui/MDPR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huiyuan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kuilong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuanming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06438">
<title>Improving Low-Light Image Recognition Performance Based on Image-adaptive Learnable Module. (arXiv:2401.06438v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06438</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, significant progress has been made in image recognition
technology based on deep neural networks. However, improving recognition
performance under low-light conditions remains a significant challenge. This
study addresses the enhancement of recognition model performance in low-light
conditions. We propose an image-adaptive learnable module which apply
appropriate image processing on input images and a hyperparameter predictor to
forecast optimal parameters used in the module. Our proposed approach allows
for the enhancement of recognition performance under low-light conditions by
easily integrating as a front-end filter without the need to retrain existing
recognition models designed for low-light conditions. Through experiments, our
proposed method demonstrates its contribution to enhancing image recognition
performance under low-light conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ono_S/0/1/0/all/0/1&quot;&gt;Seitaro Ono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogino_Y/0/1/0/all/0/1&quot;&gt;Yuka Ogino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toizumi_T/0/1/0/all/0/1&quot;&gt;Takahiro Toizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_A/0/1/0/all/0/1&quot;&gt;Atsushi Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1&quot;&gt;Masato Tsukada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06442">
<title>RotationDrag: Point-based Image Editing with Rotated Diffusion Features. (arXiv:2401.06442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06442</link>
<description rdf:parseType="Literal">&lt;p&gt;A precise and user-friendly manipulation of image content while preserving
image fidelity has always been crucial to the field of image editing. Thanks to
the power of generative models, recent point-based image editing methods allow
users to interactively change the image content with high generalizability by
clicking several control points. But the above mentioned editing process is
usually based on the assumption that features stay constant in the motion
supervision step from initial to target points. In this work, we conduct a
comprehensive investigation in the feature space of diffusion models, and find
that features change acutely under in-plane rotation. Based on this, we propose
a novel approach named RotationDrag, which significantly improves point-based
image editing performance when users intend to in-plane rotate the image
content. Our method tracks handle points more precisely by utilizing the
feature map of the rotated images, thus ensuring precise optimization and high
image fidelity. Furthermore, we build a in-plane rotation focused benchmark
called RotateBench, the first benchmark to evaluate the performance of
point-based image editing method under in-plane rotation scenario on both real
images and generated images. A thorough user study demonstrates the superior
capability in accomplishing in-plane rotation that users intend to achieve,
comparing the DragDiffusion baseline and other existing diffusion-based
methods. See the project page https://github.com/Tony-Lowe/RotationDrag for
code and experiment results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Minxing Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wentao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06462">
<title>AttributionScanner: A Visual Analytics System for Metadata-Free Data-Slicing Based Model Validation. (arXiv:2401.06462v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06462</link>
<description rdf:parseType="Literal">&lt;p&gt;Data slice-finding is an emerging technique for evaluating machine learning
models. It works by identifying subgroups within a specified dataset that
exhibit poor performance, often defined by distinct feature sets or
meta-information. However, in the context of unstructured image data, data
slice-finding poses two notable challenges: it requires additional metadata --
a laborious and costly requirement, and also demands non-trivial efforts for
interpreting the root causes of the underperformance within data slices. To
address these challenges, we introduce AttributionScanner, an innovative
human-in-the-loop Visual Analytics (VA) system, designed for data-slicing-based
machine learning (ML) model validation. Our approach excels in identifying
interpretable data slices, employing explainable features extracted through the
lens of Explainable AI (XAI) techniques, and removing the necessity for
additional metadata of textual annotations or cross-model embeddings.
AttributionScanner demonstrates proficiency in pinpointing critical model
issues, including spurious correlations and mislabeled data. Our novel VA
interface visually summarizes data slices, enabling users to gather insights
into model behavior patterns effortlessly. Furthermore, our framework closes
the ML Development Cycle by empowering domain experts to address model issues
by using a cutting-edge neural network regularization technique. The efficacy
of AttributionScanner is underscored through two prototype use cases,
elucidating its substantial effectiveness in model validation for
vision-centric tasks. Our approach paves the way for ML researchers and
practitioners to drive interpretable model validation in a data-efficient way,
ultimately leading to more reliable and accurate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_X/0/1/0/all/0/1&quot;&gt;Xiwei Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ono_J/0/1/0/all/0/1&quot;&gt;Jorge Piazentin Ono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1&quot;&gt;Liang Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kwan-Liu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liu Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06473">
<title>Self-supervised Learning of Dense Hierarchical Representations for Medical Image Segmentation. (arXiv:2401.06473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06473</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper demonstrates a self-supervised framework for learning voxel-wise
coarse-to-fine representations tailored for dense downstream tasks. Our
approach stems from the observation that existing methods for hierarchical
representation learning tend to prioritize global features over local features
due to inherent architectural bias. To address this challenge, we devise a
training strategy that balances the contributions of features from multiple
scales, ensuring that the learned representations capture both coarse and
fine-grained details. Our strategy incorporates 3-fold improvements: (1) local
data augmentations, (2) a hierarchically balanced architecture, and (3) a
hybrid contrastive-restorative loss function. We evaluate our method on CT and
MRI data and demonstrate that our new approach particularly beneficial for
fine-tuning with limited annotated data and consistently outperforms the
baseline counterpart in linear evaluation settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kats_E/0/1/0/all/0/1&quot;&gt;Eytan Kats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirsch_J/0/1/0/all/0/1&quot;&gt;Jochen G. Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1&quot;&gt;Mattias P. Heinrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06499">
<title>Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner UNet. (arXiv:2401.06499v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06499</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated segmentation of distinct tumor regions is critical for accurate
diagnosis and treatment planning in pediatric brain tumors. This study
evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in
segmenting different tumor subregions across three challenging datasets:
Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and
Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse
scenarios and anatomical variations, making them suitable for assessing the
robustness and generalization capabilities of the MPUnet model. By utilizing
multi-planar information, the MPUnet architecture aims to enhance segmentation
accuracy. Our results show varying performance levels across the evaluated
challenges, with the tumor core (TC) class demonstrating relatively higher
segmentation accuracy. However, variability is observed in the segmentation of
other classes, such as the edema and enhancing tumor (ET) regions. These
findings emphasize the complexity of brain tumor segmentation and highlight the
potential for further refinement of the MPUnet approach and inclusion of MRI
more data and preprocessing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pandey_S/0/1/0/all/0/1&quot;&gt;Sumit Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Changdar_S/0/1/0/all/0/1&quot;&gt;Satyasaran Changdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Perslev_M/0/1/0/all/0/1&quot;&gt;Mathias Perslev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dam_E/0/1/0/all/0/1&quot;&gt;Erik B Dam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06503">
<title>Improving the Detection of Small Oriented Objects in Aerial Images. (arXiv:2401.06503v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06503</link>
<description rdf:parseType="Literal">&lt;p&gt;Small oriented objects that represent tiny pixel-area in large-scale aerial
images are difficult to detect due to their size and orientation. Existing
oriented aerial detectors have shown promising results but are mainly focused
on orientation modeling with less regard to the size of the objects. In this
work, we proposed a method to accurately detect small oriented objects in
aerial images by enhancing the classification and regression tasks of the
oriented object detection model. We designed the Attention-Points Network
consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss
(BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn
the attention features needed to improve the detection of small objects. These
attention features are then used to predict box points for BPLoss, which
determines the points&apos; position relative to the target oriented bounding box.
Experimental results show the effectiveness of our Attention-Points Network on
a standard oriented aerial dataset with small object instances (DOTA-v1.5) and
on a maritime-related dataset (HRSC2016). The code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doloriel_C/0/1/0/all/0/1&quot;&gt;Chandler Timm C. Doloriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cajote_R/0/1/0/all/0/1&quot;&gt;Rhandley D. Cajote&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06506">
<title>Frequency Masking for Universal Deepfake Detection. (arXiv:2401.06506v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06506</link>
<description rdf:parseType="Literal">&lt;p&gt;We study universal deepfake detection. Our goal is to detect synthetic images
from a range of generative AI approaches, particularly from emerging ones which
are unseen during training of the deepfake detector. Universal deepfake
detection requires outstanding generalization capability. Motivated by recently
proposed masked image modeling which has demonstrated excellent generalization
in self-supervised pre-training, we make the first attempt to explore masked
image modeling for universal deepfake detection. We study spatial and frequency
domain masking in training deepfake detectors. Based on empirical analysis, we
propose a novel deepfake detector via frequency masking. Our focus on frequency
domain is different from the majority, which primarily target spatial domain
detection. Our comparative analyses reveal substantial performance gains over
existing methods. Code and models are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doloriel_C/0/1/0/all/0/1&quot;&gt;Chandler Timm Doloriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06521">
<title>Exploring Diverse Representations for Open Set Recognition. (arXiv:2401.06521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06521</link>
<description rdf:parseType="Literal">&lt;p&gt;Open set recognition (OSR) requires the model to classify samples that belong
to closed sets while rejecting unknown samples during test. Currently,
generative models often perform better than discriminative models in OSR, but
recent studies show that generative models may be computationally infeasible or
unstable on complex tasks. In this paper, we provide insights into OSR and find
that learning supplementary representations can theoretically reduce the open
space risk. Based on the analysis, we propose a new model, namely Multi-Expert
Diverse Attention Fusion (MEDAF), that learns diverse representations in a
discriminative way. MEDAF consists of multiple experts that are learned with an
attention diversity regularization term to ensure the attention maps are
mutually different. The logits learned by each expert are adaptively fused and
used to identify the unknowns through the score function. We show that the
differences in attention maps can lead to diverse representations so that the
fused representations can well handle the open space. Extensive experiments are
conducted on standard and OSR large-scale benchmarks. Results show that the
proposed discriminative method can outperform existing generative models by up
to 9.5% on AUROC and achieve new state-of-the-art performance with little
computational cost. Our method can also seamlessly integrate existing
classification models. Code is available at https://github.com/Vanixxz/MEDAF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1&quot;&gt;Junxian Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06528">
<title>PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed Circuit Boards. (arXiv:2401.06528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06528</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the critical theme of recycling electronic waste (E-waste), this
contribution is dedicated to developing advanced automated data processing
pipelines as a basis for decision-making and process control. Aligning with the
broader goals of the circular economy and the United Nations (UN) Sustainable
Development Goals (SDG), our work leverages non-invasive analysis methods
utilizing RGB and hyperspectral imaging data to provide both quantitative and
qualitative insights into the E-waste stream composition for optimizing
recycling efficiency. In this paper, we introduce &apos;PCB-Vision&apos;; a pioneering
RGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53
RGB images of high spatial resolution paired with their corresponding high
spectral resolution hyperspectral data cubes in the visible and near-infrared
(VNIR) range. Grounded in open science principles, our dataset provides a
comprehensive resource for researchers through high-quality ground truths,
focusing on three primary PCB components: integrated circuits (IC), capacitors,
and connectors. We provide extensive statistical investigations on the proposed
dataset together with the performance of several state-of-the-art (SOTA)
models, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and
DeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the
baseline codes, we hope to foster transparent, traceable, and comparable
developments of advanced data processing across various scientific communities,
including, but not limited to, computer vision and remote sensing. Emphasizing
our commitment to supporting a collaborative and inclusive scientific
community, all materials, including code, data, ground truth, and masks, will
be accessible at https://github.com/hifexplo/PCBVision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbash_E/0/1/0/all/0/1&quot;&gt;Elias Arbash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_M/0/1/0/all/0/1&quot;&gt;Margret Fuchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1&quot;&gt;Behnood Rasti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_S/0/1/0/all/0/1&quot;&gt;Sandra Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1&quot;&gt;Pedram Ghamisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1&quot;&gt;Richard Gloaguen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06542">
<title>Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook. (arXiv:2401.06542v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06542</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of modern autonomous driving, the perception system is
indispensable for accurately assessing the state of the surrounding
environment, thereby enabling informed prediction and planning. Key to this
system is 3D object detection methods, that utilize vehicle-mounted sensors
such as LiDAR and cameras to identify the size, category, and location of
nearby objects. Despite the surge in 3D object detection methods aimed at
enhancing detection precision and efficiency, there is a gap in the literature
that systematically examines their resilience against environmental variations,
noise, and weather changes. This study emphasizes the importance of robustness,
alongside accuracy and latency, in evaluating perception systems under
practical scenarios. Our work presents an extensive survey of camera-based,
LiDAR-based, and multimodal 3D object detection algorithms, thoroughly
evaluating their trade-off between accuracy, latency, and robustness,
particularly on datasets like KITTI-C and nuScenes-C to ensure fair
comparisons. Among these,multimodal 3D detection approaches exhibit superior
robustness and a novel taxonomy is introduced to reorganize its literature for
enhanced clarity. This survey aims to offer a more practical perspective on the
current capabilities and constraints of 3D object detection algorithms in
real-world applications, thus steering future research towards
robustness-centric advancements
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Ziying Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Feiyang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Caiyan Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06546">
<title>Optimizing Feature Selection for Binary Classification with Noisy Labels: A Genetic Algorithm Approach. (arXiv:2401.06546v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06546</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection in noisy label scenarios remains an understudied topic. We
propose a novel genetic algorithm-based approach, the Noise-Aware
Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting
optimal feature subsets in binary classification with noisy labels. NMFS-GA
offers a unified framework for selecting feature subsets that are both accurate
and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise,
a Breast Cancer dataset enriched with noisy features, and a real-world ADNI
dataset for dementia conversion prediction. Our results indicate that NMFS-GA
can effectively select feature subsets that improve the accuracy and
interpretability of binary classifiers in scenarios with noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imani_V/0/1/0/all/0/1&quot;&gt;Vandad Imani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_E/0/1/0/all/0/1&quot;&gt;Elaheh Moradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sevilla_Salcedo_C/0/1/0/all/0/1&quot;&gt;Carlos Sevilla-Salcedo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortino_V/0/1/0/all/0/1&quot;&gt;Vittorio Fortino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tohka_J/0/1/0/all/0/1&quot;&gt;Jussi Tohka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06548">
<title>Enhancing Consistency and Mitigating Bias: A Data Replay Approach for Incremental Learning. (arXiv:2401.06548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06548</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning systems are prone to catastrophic forgetting when learning from
a sequence of tasks, where old data from experienced tasks is unavailable when
learning from a new task. To mitigate the problem, a line of methods propose to
replay the data of experienced tasks when learning new tasks. These methods
usually adopt an extra memory to store the data for replay. However, it is not
expected in practice considering the memory constraint or data privacy issue.
As a replacement, data-free data replay methods are proposed by inverting
samples from the classification model. Though achieving good results, these
methods still suffer from the inconsistency of the inverted and real training
data, which is neglected in the inversion stage in recent works. To that
effect, we propose to measure the data consistency quantitatively by some
simplification and assumptions. Using the measurement, we analyze existing
techniques for inverting samples and get some insightful information that
inspires a novel loss function to reduce the inconsistency. Specifically, the
loss minimizes the KL divergence of the distributions of inverted and real data
under the tied multivariate Gaussian assumption, which is easy to implement in
continual learning. In addition, we observe that the norms of old class weights
turn to decrease continually as learning progresses. We thus analyze the
underlying reasons and propose a simple regularization term to balance the
class weights so that the samples of old classes are more distinguishable. To
conclude, we propose the Consistency enhanced data replay with debiased
classifier for Class Incremental Learning (CCIL). Extensive experiments on
CIFAR-100, Tiny-ImageNet, and ImageNet100 show consistently improved
performance of CCIL compared to previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xingyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06550">
<title>Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information. (arXiv:2401.06550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06550</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban area-of-interest (AOI) refers to an integrated urban functional zone
with defined boundaries. The rapid development of urban commerce has resulted
in an increased demand for more precise requirements in defining AOIs. However,
existing research primarily concentrates on broad AOI mining for urban planning
or regional economic analysis, failing to cater to the precise requirements of
mobile Internet online-to-offline businesses. These businesses necessitate
accuracy down to a specific community, school, or hospital. In this paper, we
propose an end-to-end multimodal deep learning algorithm for detecting AOI
fence polygon using remote sensing images and multi-semantics reference
information. We then evaluate its timeliness through a cascaded module that
incorporates dynamic human mobility and logistics address information.
Specifically, we begin by selecting a point-of-interest (POI) of specific
category, and use it to recall corresponding remote sensing images, nearby
POIs, road nodes, human mobility, and logistics addresses to build a multimodal
detection model based on transformer encoder-decoder architecture, titled
AOITR. In the model, in addition to the remote sensing images, multi-semantic
information including core POI and road nodes is embedded and reorganized as
the query content part for the transformer decoder to generate the AOI polygon.
Meanwhile, relatively dynamic distribution features of human mobility, nearby
POIs, and logistics addresses are used for AOI reliability evaluation through a
cascaded feedforward network. The experimental results demonstrate that our
algorithm significantly outperforms two existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chuanji Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaotuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qiqi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06563">
<title>Resource-Efficient Gesture Recognition using Low-Resolution Thermal Camera via Spiking Neural Networks and Sparse Segmentation. (arXiv:2401.06563v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06563</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a novel approach for hand gesture recognition using an
inexpensive, low-resolution (24 x 32) thermal sensor processed by a Spiking
Neural Network (SNN) followed by Sparse Segmentation and feature-based gesture
classification via Robust Principal Component Analysis (R-PCA). Compared to the
use of standard RGB cameras, the proposed system is insensitive to lighting
variations while being significantly less expensive compared to high-frequency
radars, time-of-flight cameras and high-resolution thermal sensors previously
used in literature. Crucially, this paper shows that the innovative use of the
recently proposed Monostable Multivibrator (MMV) neural networks as a new class
of SNN achieves more than one order of magnitude smaller memory and compute
complexity compared to deep learning approaches, while reaching a top gesture
recognition accuracy of 93.9% using a 5-class thermal camera dataset acquired
in a car cabin, within an automotive context. Our dataset is released for
helping future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1&quot;&gt;Ali Safa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mommen_W/0/1/0/all/0/1&quot;&gt;Wout Mommen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuninckx_L/0/1/0/all/0/1&quot;&gt;Lars Keuninckx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06578">
<title>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model. (arXiv:2401.06578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06578</link>
<description rdf:parseType="Literal">&lt;p&gt;360-degree panoramic videos recently attract more interest in both studies
and applications, courtesy of the heightened immersive experiences they
engender. Due to the expensive cost of capturing 360-degree panoramic videos,
generating desirable panoramic videos by given prompts is urgently required.
Recently, the emerging text-to-video (T2V) diffusion methods demonstrate
notable effectiveness in standard video generation. However, due to the
significant gap in content and motion patterns between panoramic and standard
videos, these methods encounter challenges in yielding satisfactory 360-degree
panoramic videos. In this paper, we propose a controllable panorama video
generation pipeline named 360-Degree Video Diffusion model (360DVD) for
generating panoramic videos based on the given prompts and motion conditions.
Concretely, we introduce a lightweight module dubbed 360-Adapter and assisted
360 Enhancement Techniques to transform pre-trained T2V models for 360-degree
video generation. We further propose a new panorama dataset named WEB360
consisting of 360-degree video-text pairs for training 360DVD, addressing the
absence of captioned panoramic video datasets. Extensive experiments
demonstrate the superiority and effectiveness of 360DVD for panorama video
generation. The code and dataset will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1&quot;&gt;Chong Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xinhua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06588">
<title>Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints. (arXiv:2401.06588v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.06588</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the use of connectionist techniques in phonetic speech
recognition with strong latency constraints. The constraints are imposed by the
task of deriving the lip movements of a synthetic face in real time from the
speech signal, by feeding the phonetic string into an articulatory synthesiser.
Particular attention has been paid to analysing the interaction between the
time evolution model learnt by the multi-layer perceptrons and the transition
model imposed by the Viterbi decoder, in different latency conditions. Two
experiments were conducted in which the time dependencies in the language model
(LM) were controlled by a parameter. The results show a strong interaction
between the three factors involved, namely the neural network topology, the
length of time dependencies in the LM and the decoder latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salvi_G/0/1/0/all/0/1&quot;&gt;Giampiero Salvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06614">
<title>Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking. (arXiv:2401.06614v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06614</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Motion2VecSets, a 4D diffusion model for dynamic surface
reconstruction from point cloud sequences. While existing state-of-the-art
methods have demonstrated success in reconstructing non-rigid objects using
neural field representations, conventional feed-forward networks encounter
challenges with ambiguous observations from noisy, partial, or sparse point
clouds. To address these challenges, we introduce a diffusion model that
explicitly learns the shape and motion distribution of non-rigid objects
through an iterative denoising process of compressed latent representations.
The diffusion-based prior enables more plausible and probabilistic
reconstructions when handling ambiguous inputs. We parameterize 4D dynamics
with latent vector sets instead of using a global latent. This novel 4D
representation allows us to learn local surface shape and deformation patterns,
leading to more accurate non-linear motion capture and significantly improving
generalizability to unseen motions and identities. For more temporal-coherent
object tracking, we synchronously denoise deformation latent sets and exchange
information across multiple frames. To avoid the computational overhead, we
design an interleaved space and time attention block to alternately aggregate
deformation latents along spatial and temporal domains. Extensive comparisons
against the state-of-the-art methods demonstrate the superiority of our
Motion2VecSets in 4D reconstruction from various imperfect observations,
notably achieving a 19% improvement in Intersection over Union (IoU) compared
to CaDex for reconstructing unseen individuals from sparse point clouds on the
DeformingThings4D-Animals dataset. More detailed information can be found at
https://vveicao.github.io/projects/Motion2VecSets/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Biao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiapeng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06637">
<title>Adversarial Examples are Misaligned in Diffusion Model Manifolds. (arXiv:2401.06637v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06637</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, diffusion models (DMs) have drawn significant attention for
their success in approximating data distributions, yielding state-of-the-art
generative results. Nevertheless, the versatility of these models extends
beyond their generative capabilities to encompass various vision applications,
such as image inpainting, segmentation, adversarial robustness, among others.
This study is dedicated to the investigation of adversarial attacks through the
lens of diffusion models. However, our objective does not involve enhancing the
adversarial robustness of image classifiers. Instead, our focus lies in
utilizing the diffusion model to detect and analyze the anomalies introduced by
these attacks on images. To that end, we systematically examine the alignment
of the distributions of adversarial examples when subjected to the process of
transformation using diffusion models. The efficacy of this approach is
assessed across CIFAR-10 and ImageNet datasets, including varying image sizes
in the latter. The results demonstrate a notable capacity to discriminate
effectively between benign and attacked images, providing compelling evidence
that adversarial instances do not align with the learned manifold of the DMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1&quot;&gt;Ricard Durall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Jansi Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06654">
<title>Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks. (arXiv:2401.06654v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06654</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature removal is a central building block for eXplainable AI (XAI), both
for occlusion-based explanations (Shapley values) as well as their evaluation
(pixel flipping, PF). However, occlusion strategies can vary significantly from
simple mean replacement up to inpainting with state-of-the-art diffusion
models. This ambiguity limits the usefulness of occlusion-based approaches. For
example, PF benchmarks lead to contradicting rankings. This is amplified by
competing PF measures: Features are either removed starting with most
influential first (MIF) or least influential first (LIF). This study proposes
two complementary perspectives to resolve this disagreement problem. Firstly,
we address the common criticism of occlusion-based XAI, that artificial samples
lead to unreliable model evaluations. We propose to measure the reliability by
the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a
systematic comparison of occlusion strategies and resolves the disagreement
problem by grouping consistent PF rankings. Secondly, we show that the
insightfulness of MIF and LIF is conversely dependent on the R-OMS score. To
leverage this, we combine the MIF and LIF measures into the symmetric relevance
gain (SRG) measure. This breaks the inherent connection to the underlying
occlusion strategy and leads to consistent rankings. This resolves the
disagreement problem, which we verify for a set of 40 different occlusion
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blucher_S/0/1/0/all/0/1&quot;&gt;Stefan Bl&amp;#xfc;cher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielhaben_J/0/1/0/all/0/1&quot;&gt;Johanna Vielhaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1&quot;&gt;Nils Strodthoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06690">
<title>Embedded Planogram Compliance Control System. (arXiv:2401.06690v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06690</link>
<description rdf:parseType="Literal">&lt;p&gt;The retail sector presents several open and challenging problems that could
benefit from advanced pattern recognition and computer vision techniques. One
such critical challenge is planogram compliance control. In this study, we
propose a complete embedded system to tackle this issue. Our system consists of
four key components as image acquisition and transfer via stand-alone embedded
camera module, object detection via computer vision and deep learning methods
working on single board computers, planogram compliance control method again
working on single board computers, and energy harvesting and power management
block to accompany the embedded camera modules. The image acquisition and
transfer block is implemented on the ESP-EYE camera module. The object
detection block is based on YOLOv5 as the deep learning method and local
feature extraction. We implement these methods on Raspberry Pi 4, NVIDIA Jetson
Orin Nano, and NVIDIA Jetson AGX Orin as single board computers. The planogram
compliance control block utilizes sequence alignment through a modified
Needleman-Wunsch algorithm. This block is also working along with the object
detection block on the same single board computers. The energy harvesting and
power management block consists of solar and RF energy harvesting modules with
suitable battery pack for operation. We tested the proposed embedded planogram
compliance control system on two different datasets to provide valuable
insights on its strengths and weaknesses. The results show that our method
achieves F1 scores of 0.997 and 1.0 in object detection and planogram
compliance control blocks, respectively. Furthermore, we calculated that the
complete embedded system can work in stand-alone form up to two years based on
battery. This duration can be further extended with the integration of the
proposed solar and RF energy harvesting options.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yucel_M/0/1/0/all/0/1&quot;&gt;M. Erkin Y&amp;#xfc;cel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topaloglu_S/0/1/0/all/0/1&quot;&gt;Serkan Topalo&amp;#x11f;lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unsalan_C/0/1/0/all/0/1&quot;&gt;Cem &amp;#xdc;nsalan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06704">
<title>Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering. (arXiv:2401.06704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06704</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a highly efficient method for panoptic segmentation of large 3D
point clouds by redefining this task as a scalable graph clustering problem.
This approach can be trained using only local auxiliary tasks, thereby
eliminating the resource-intensive instance-matching step during training.
Moreover, our formulation can easily be adapted to the superpoint paradigm,
further increasing its efficiency. This allows our model to process scenes with
millions of points and thousands of objects in a single inference. Our method,
called SuperCluster, achieves a new state-of-the-art panoptic segmentation
performance for two indoor scanning datasets: $50.1$ PQ ($+7.8$) for S3DIS
Area~5, and $58.7$ PQ ($+25.2$) for ScanNetV2. We also set the first
state-of-the-art for two large-scale mobile mapping benchmarks: KITTI-360 and
DALES. With only $209$k parameters, our model is over $30$ times smaller than
the best-competing method and trains up to $15$ times faster. Our code and
pretrained models are available at
https://github.com/drprojects/superpoint_transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robert_D/0/1/0/all/0/1&quot;&gt;Damien Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raguet_H/0/1/0/all/0/1&quot;&gt;Hugo Raguet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1&quot;&gt;Loic Landrieu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06757">
<title>Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06757</link>
<description rdf:parseType="Literal">&lt;p&gt;Pedestrian intention prediction is crucial for autonomous driving. In
particular, knowing if pedestrians are going to cross in front of the
ego-vehicle is core to performing safe and comfortable maneuvers. Creating
accurate and fast models that predict such intentions from sequential images is
challenging. A factor contributing to this is the lack of datasets with diverse
crossing and non-crossing (C/NC) scenarios. We address this scarceness by
introducing a framework, named ARCANE, which allows programmatically generating
synthetic datasets consisting of C/NC video clip samples. As an example, we use
ARCANE to generate a large and diverse dataset named PedSynth. We will show how
PedSynth complements widely used real-world datasets such as JAAD and PIE, so
enabling more accurate models for C/NC prediction. Considering the onboard
deployment of C/NC prediction models, we also propose a deep model named
PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a
GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to
predict crossing intentions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Naveed Riaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielgosz_M/0/1/0/all/0/1&quot;&gt;Maciej Wielgosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romera_A/0/1/0/all/0/1&quot;&gt;Abel Garcia Romera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1&quot;&gt;Antonio M. Lopez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06762">
<title>Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery. (arXiv:2401.06762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06762</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully understanding a complex high-resolution satellite or aerial imagery
scene often requires spatial reasoning over a broad relevant context. The human
object recognition system is able to understand object in a scene over a
long-range relevant context. For example, if a human observes an aerial scene
that shows sections of road broken up by tree canopy, then they will be
unlikely to conclude that the road has actually been broken up into disjoint
pieces by trees and instead think that the canopy of nearby trees is occluding
the road. However, there is limited research being conducted to understand
long-range context understanding of modern machine learning models. In this
work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial
Context (RSC), for evaluating the spatial long-range context understanding of
geospatial machine learning models and show how commonly used semantic
segmentation models can fail at this task. For example, we show that a U-Net
trained to segment roads from background in aerial imagery achieves an 84%
recall on unoccluded roads, but just 63.5% recall on roads covered by tree
canopy despite being trained to model both the same way. We further analyze how
the performance of models changes as the relevant context for a decision
(unoccluded roads in our case) varies in distance. We release the code to
reproduce our experiments and dataset of imagery and masks to encourage future
research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1&quot;&gt;Caleb Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corley_I/0/1/0/all/0/1&quot;&gt;Isaac Corley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1&quot;&gt;Anthony Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1&quot;&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1&quot;&gt;Juan M. Lavista Ferres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1&quot;&gt;Peyman Najafirad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.03180">
<title>Vision Transformers with Hierarchical Attention. (arXiv:2106.03180v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.03180</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the high computational/space complexity associated with
Multi-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,
we propose Hierarchical MHSA (H-MHSA), a novel approach that computes
self-attention in a hierarchical fashion. Specifically, we first divide the
input image into patches as commonly done, and each patch is viewed as a token.
Then, the proposed H-MHSA learns token relationships within local patches,
serving as local relationship modeling. Then, the small patches are merged into
larger ones, and H-MHSA models the global dependencies for the small number of
the merged tokens. At last, the local and global attentive features are
aggregated to obtain features with powerful representation capacity. Since we
only calculate attention for a limited number of tokens at each step, the
computational load is reduced dramatically. Hence, H-MHSA can efficiently model
global relationships among tokens without sacrificing fine-grained information.
With the H-MHSA module incorporated, we build a family of
Hierarchical-Attention-based Transformer Networks, namely HAT-Net. To
demonstrate the superiority of HAT-Net in scene understanding, we conduct
extensive experiments on fundamental vision tasks, including image
classification, semantic segmentation, object detection, and instance
segmentation. Therefore, HAT-Net provides a new perspective for vision
transformers. Code and pretrained models are available at
https://github.com/yun-liu/HAT-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guolei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1&quot;&gt;Ajad Chhatkuli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11892">
<title>DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection. (arXiv:2206.11892v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11892</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing change detection is crucial for understanding the dynamics of
our planet&apos;s surface, facilitating the monitoring of environmental changes,
evaluating human impact, predicting future trends, and supporting
decision-making. In this work, we introduce a novel approach for change
detection that can leverage off-the-shelf, unlabeled remote sensing images in
the training process by pre-training a Denoising Diffusion Probabilistic Model
(DDPM) - a class of generative models used in image synthesis. DDPMs learn the
training data distribution by gradually converting training images into a
Gaussian distribution using a Markov chain. During inference (i.e., sampling),
they can generate a diverse set of samples closer to the training distribution,
starting from Gaussian noise, achieving state-of-the-art image synthesis
results. However, in this work, our focus is not on image synthesis but on
utilizing it as a pre-trained feature extractor for the downstream application
of change detection. Specifically, we fine-tune a lightweight change classifier
utilizing the feature representations produced by the pre-trained DDPM
alongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD,
DSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD method
significantly outperforms the existing state-of-the-art change detection
methods in terms of F1 score, IoU, and overall accuracy, highlighting the
pivotal role of pre-trained DDPM as a feature extractor for downstream
applications. We have made both the code and pre-trained models available at
https://github.com/wgcban/ddpm-cd
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1&quot;&gt;Wele Gedara Chaminda Bandara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1&quot;&gt;Nithin Gopalakrishnan Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1&quot;&gt;Vishal M. Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06544">
<title>Correcting Faulty Road Maps by Image Inpainting. (arXiv:2211.06544v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06544</link>
<description rdf:parseType="Literal">&lt;p&gt;As maintaining road networks is labor-intensive, many automatic road
extraction approaches have been introduced to solve this real-world problem,
fueled by the abundance of large-scale high-resolution satellite imagery and
advances in computer vision. However, their performance is limited for fully
automating the road map extraction in real-world services. Hence, many services
employ the two-step human-in-the-loop system to post-process the extracted road
maps: error localization and automatic mending for faulty road maps. Our paper
exclusively focuses on the latter step, introducing a novel image inpainting
approach for fixing road maps with complex road geometries without custom-made
heuristics, yielding a method that is readily applicable to any road geometry
extraction model. We demonstrate the effectiveness of our method on various
real-world road geometries, such as straight and curvy roads, T-junctions, and
intersections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Soojung Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1&quot;&gt;Kwanghee Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08332">
<title>Versatile Diffusion: Text, Images and Variations All in One Diffusion Model. (arXiv:2211.08332v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08332</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in diffusion models have set an impressive milestone in many
generation tasks, and trending works such as DALL-E2, Imagen, and Stable
Diffusion have attracted great interest. Despite the rapid landscape changes,
recent new approaches focus on extensions and performance rather than capacity,
thus requiring separate models for separate tasks. In this work, we expand the
existing single-flow diffusion pipeline into a multi-task multimodal network,
dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image,
image-to-text, and variations in one unified model. The pipeline design of VD
instantiates a unified multi-flow diffusion framework, consisting of sharable
and swappable layer modules that enable the crossmodal generality beyond images
and text. Through extensive experiments, we demonstrate that VD successfully
achieves the following: a) VD outperforms the baseline approaches and handles
all its base tasks with competitive quality; b) VD enables novel extensions
such as disentanglement of style and semantics, dual- and multi-context
blending, etc.; c) The success of our multi-flow multimodal framework over
images and text may inspire further diffusion-based universal AI research. Our
code and models are open-sourced at
https://github.com/SHI-Labs/Versatile-Diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xingqian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Eric Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04384">
<title>SEMv2: Table Separation Line Detection Based on Instance Segmentation. (arXiv:2303.04384v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04384</link>
<description rdf:parseType="Literal">&lt;p&gt;Table structure recognition is an indispensable element for enabling machines
to comprehend tables. Its primary purpose is to identify the internal structure
of a table. Nevertheless, due to the complexity and diversity of their
structure and style, it is highly challenging to parse the tabular data into a
structured format that machines can comprehend. In this work, we adhere to the
principle of the split-and-merge based methods and propose an accurate table
structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the
previous works in the ``split&apos;&apos; stage, we aim to address the table separation
line instance-level discrimination problem and introduce a table separation
line detection strategy based on conditional convolution. Specifically, we
design the ``split&apos;&apos; in a top-down manner that detects the table separation
line instance first and then dynamically predicts the table separation line
mask for each instance. The final table separation line shape can be accurately
obtained by processing the table separation line mask in a row-wise/column-wise
manner. To comprehensively evaluate the SEMv2, we also present a more
challenging dataset for table structure recognition, dubbed iFLYTAB, which
encompasses multiple style tables in various scenarios such as photos, scanned
documents, etc. Extensive experiments on publicly available datasets (e.g.
SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed
approach. The code and iFLYTAB dataset are available at
https://github.com/ZZR8066/SEMv2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Pengfei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiefeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianshu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huihui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Baocai Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13199">
<title>First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning. (arXiv:2303.13199v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13199</link>
<description rdf:parseType="Literal">&lt;p&gt;In Class-Incremental Learning (CIL) an image classification system is exposed
to new classes in each learning session and must be updated incrementally.
Methods approaching this problem have updated both the classification head and
the feature extractor body at each session of CIL. In this work, we develop a
baseline method, First Session Adaptation (FSA), that sheds light on the
efficacy of existing CIL approaches and allows us to assess the relative
performance contributions from head and body adaption. FSA adapts a pre-trained
neural network body only on the first learning session and fixes it thereafter;
a head based on linear discriminant analysis (LDA), is then placed on top of
the adapted body, allowing exact updates through CIL. FSA is replay-free
i.e.~it does not memorize examples from previous sessions of continual
learning. To empirically motivate FSA, we first consider a diverse selection of
22 image-classification datasets, evaluating different heads and body
adaptation techniques in high/low-shot offline settings. We find that the LDA
head performs well and supports CIL out-of-the-box. We also find that
Featurewise Layer Modulation (FiLM) adapters are highly effective in the
few-shot setting, and full-body adaption in the high-shot setting. Second, we
empirically investigate various CIL settings including high-shot CIL and
few-shot CIL, including settings that have previously been used in the
literature. We show that FSA significantly improves over the state-of-the-art
in 15 of the 16 settings considered. FSA with FiLM adapters is especially
performant in the few-shot setting. These results indicate that current
approaches to continuous body adaptation are not working as expected. Finally,
we propose a measure that can be applied to a set of unlabelled inputs which is
predictive of the benefits of body adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panos_A/0/1/0/all/0/1&quot;&gt;Aristeidis Panos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobe_Y/0/1/0/all/0/1&quot;&gt;Yuriko Kobe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1&quot;&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1&quot;&gt;Rahaf Aljundi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13959">
<title>Bridging Stereo Geometry and BEV Representation with Reliable Mutual Interaction for Semantic Scene Completion. (arXiv:2303.13959v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13959</link>
<description rdf:parseType="Literal">&lt;p&gt;3D semantic scene completion (SSC) is an ill-posed perception task that
requires inferring a dense 3D scene from limited observations. Previous
camera-based methods struggle to predict accurate semantic scenes due to
inherent geometric ambiguity and incomplete observations. In this paper, we
resort to stereo matching technique and bird&apos;s-eye-view (BEV) representation
learning to address such issues in SSC. Complementary to each other, stereo
matching mitigates geometric ambiguity with epipolar constraint while BEV
representation enhances the hallucination ability for invisible regions with
global semantic context. However, due to the inherent representation gap
between stereo geometry and BEV features, it is non-trivial to bridge them for
dense prediction task of SSC. Therefore, we further develop a unified
occupancy-based framework dubbed BRGScene, which effectively bridges these two
representations with dense 3D volumes for reliable semantic scene completion.
Specifically, we design a novel Mutual Interactive Ensemble (MIE) block for
pixel-level reliable aggregation of stereo geometry and BEV features. Within
the MIE block, a Bi-directional Reliable Interaction (BRI) module, enhanced
with confidence re-weighting, is employed to encourage fine-grained interaction
through mutual guidance. Besides, a Dual Volume Ensemble (DVE) module is
introduced to facilitate complementary aggregation through channel-wise
recalibration and multi-group voting. Our method outperforms all published
camera-based methods on SemanticKITTI for semantic scene completion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yasheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhujin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1&quot;&gt;Dalong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuanghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunnan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00917">
<title>Vocabulary-free Image Classification. (arXiv:2306.00917v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large vision-language models have revolutionized the image
classification paradigm. Despite showing impressive zero-shot capabilities, a
pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time
for composing the textual prompts. However, such assumption can be impractical
when the semantic context is unknown and evolving. We thus formalize a novel
task, termed as Vocabulary-free Image Classification (VIC), where we aim to
assign to an input image a class that resides in an unconstrained
language-induced semantic space, without the prerequisite of a known
vocabulary. VIC is a challenging task as the semantic space is extremely large,
containing millions of concepts, with hard-to-discriminate fine-grained
categories. In this work, we first empirically verify that representing this
semantic space by means of an external vision-language database is the most
effective way to obtain semantically relevant content for classifying the
image. We then propose Category Search from External Databases (CaSED), a
method that exploits a pre-trained vision-language model and an external
vision-language database to address VIC in a training-free manner. CaSED first
extracts a set of candidate categories from captions retrieved from the
database based on their semantic similarity to the image, and then assigns to
the image the best matching candidate category according to the same
vision-language model. Experiments on benchmark datasets validate that CaSED
outperforms other complex vision-language frameworks, while being efficient
with much fewer parameters, paving the way for future research in this
direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_A/0/1/0/all/0/1&quot;&gt;Alessandro Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1&quot;&gt;Enrico Fini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1&quot;&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1&quot;&gt;Paolo Rota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09910">
<title>LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning. (arXiv:2306.09910v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09910</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench&apos;s modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
https://github.com/EfficientTraining/LabelBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canal_G/0/1/0/all/0/1&quot;&gt;Gregory Canal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav M. Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeffrey Bilmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon Shaolei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert D Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17010">
<title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning method for scene flow
estimation as a complementary motion information for mmWave point cloud,
serving as an intermediate level of features and directly benefiting downstream
human motion sensing tasks. Experimental results demonstrate the superior
performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we will provide our codebase and dataset
for open access upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peijun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01838">
<title>EdgeFace: Efficient Face Recognition Model for Edge Devices. (arXiv:2307.01838v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01838</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1&quot;&gt;Anjith George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecabert_C/0/1/0/all/0/1&quot;&gt;Christophe Ecabert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahreza_H/0/1/0/all/0/1&quot;&gt;Hatef Otroshi Shahreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotwal_K/0/1/0/all/0/1&quot;&gt;Ketan Kotwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1&quot;&gt;Sebastien Marcel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14288">
<title>US \&amp; MRI Image Fusion Based on Markerless Skin Registration. (arXiv:2307.14288v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14288</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an innovative automatic fusion imaging system that
combines 3D CT/MR images with real-time ultrasound (US) acquisition. The system
eliminates the need for external physical markers and complex training, making
image fusion feasible for physicians with different experience levels. The
integrated system involves a portable 3D camera for patient-specific surface
acquisition, an electromagnetic tracking system, and US components. The fusion
algorithm comprises two main parts: skin segmentation and rigid
co-registration, both integrated into the US machine. The co-registration
software aligns the surface extracted from CT/MR images with patient-specific
coordinates, facilitating rapid and effective fusion. Experimental testing in
different settings, including the clinical environment, validates the system&apos;s
accuracy, computational efficiency, noise robustness, and operator
independence. The co-registration error remains under the acceptable range
of~$1$ cm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paccini_M/0/1/0/all/0/1&quot;&gt;Martina Paccini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschina_G/0/1/0/all/0/1&quot;&gt;Giacomo Paschina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beni_S/0/1/0/all/0/1&quot;&gt;Stefano De Beni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patane_G/0/1/0/all/0/1&quot;&gt;Giuseppe Patan&amp;#xe8;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07593">
<title>AKVSR: Audio Knowledge Empowered Visual Speech Recognition by Compressing Audio Knowledge of a Pretrained Model. (arXiv:2308.07593v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07593</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Speech Recognition (VSR) is the task of predicting spoken words from
silent lip movements. VSR is regarded as a challenging task because of the
insufficient information on lip movements. In this paper, we propose an Audio
Knowledge empowered Visual Speech Recognition framework (AKVSR) to complement
the insufficient speech information of visual modality by using audio modality.
Different from the previous methods, the proposed AKVSR 1) utilizes rich audio
knowledge encoded by a large-scale pretrained audio model, 2) saves the
linguistic information of audio knowledge in compact audio memory by discarding
the non-linguistic information from the audio through quantization, and 3)
includes Audio Bridging Module which can find the best-matched audio features
from the compact audio memory, which makes our training possible without audio
inputs, once after the compact audio memory is composed. We validate the
effectiveness of the proposed method through extensive experiments, and achieve
new state-of-the-art performances on the widely-used LRS3 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jeong Hun Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dae Hoe Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09311">
<title>Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge. (arXiv:2308.09311v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09311</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel lip reading framework, especially for
low-resource languages, which has not been well addressed in the previous
literature. Since low-resource languages do not have enough video-text paired
data to train the model to have sufficient power to model lip movements and
language, it is regarded as challenging to develop lip reading models for
low-resource languages. In order to mitigate the challenge, we try to learn
general speech knowledge, the ability to model lip movements, from a
high-resource language through the prediction of speech units. It is known that
different languages partially share common phonemes, thus general speech
knowledge learned from one language can be extended to other languages. Then,
we try to learn language-specific knowledge, the ability to model language, by
proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder
saves language-specific audio features into memory banks and can be trained on
audio-text paired data which is more easily accessible than video-text paired
data. Therefore, with LMDecoder, we can transform the input speech units into
language-specific audio features and translate them into texts by utilizing the
learned rich language knowledge. Finally, by combining general speech knowledge
and language-specific knowledge, we can efficiently develop lip reading models
even for low-resource languages. Through extensive experiments using five
languages, English, Spanish, French, Italian, and Portuguese, the effectiveness
of the proposed method is evaluated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jeong Hun Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongsoo Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12604">
<title>PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12604</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic medical report generation (MRG) is of great research value as it
has the potential to relieve radiologists from the heavy burden of report
writing. Despite recent advancements, accurate MRG remains challenging due to
the need for precise clinical understanding and disease identification.
Moreover, the imbalanced distribution of diseases makes the challenge even more
pronounced, as rare diseases are underrepresented in training data, making
their diagnostic performance unreliable. To address these challenges, we
propose diagnosis-driven prompts for medical report generation (PromptMRG), a
novel framework that aims to improve the diagnostic accuracy of MRG with the
guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on
encoder-decoder architecture with an extra disease classification branch. When
generating reports, the diagnostic results from the classification branch are
converted into token prompts to explicitly guide the generation process. To
further improve the diagnostic accuracy, we design cross-modal feature
enhancement, which retrieves similar reports from the database to assist the
diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP.
Moreover, the disease imbalanced issue is addressed by applying an adaptive
logit-adjusted loss to the classification branch based on the individual
learning status of each disease, which overcomes the barrier of text decoder&apos;s
inability to manipulate disease distributions. Experiments on two MRG
benchmarks show the effectiveness of the proposed method, where it obtains
state-of-the-art clinical efficacy performance on both datasets. The code is
available at https://github.com/jhb86253817/PromptMRG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haibo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1&quot;&gt;Haoxuan Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05499">
<title>Zero-Shot Co-salient Object Detection Framework. (arXiv:2309.05499v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05499</link>
<description rdf:parseType="Literal">&lt;p&gt;Co-salient Object Detection (CoSOD) endeavors to replicate the human visual
system&apos;s capacity to recognize common and salient objects within a collection
of images. Despite recent advancements in deep learning models, these models
still rely on training with well-annotated CoSOD datasets. The exploration of
training-free zero-shot CoSOD frameworks has been limited. In this paper,
taking inspiration from the zero-shot transfer capabilities of foundational
computer vision models, we introduce the first zero-shot CoSOD framework that
harnesses these models without any training process. To achieve this, we
introduce two novel components in our proposed framework: the group prompt
generation (GPG) module and the co-saliency map generation (CMP) module. We
evaluate the framework&apos;s performance on widely-used datasets and observe
impressive results. Our approach surpasses existing unsupervised methods and
even outperforms fully supervised methods developed before 2020, while
remaining competitive with some fully supervised methods developed before 2022.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Haoke Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Lv Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhiming Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shaozi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07846">
<title>MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image Acquisition Systems. (arXiv:2309.07846v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07846</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) employ multi-view images for 3D scene
representation and have shown remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods often assume a global unique camera and seldom consider
scenarios with multiple cameras. Besides, some pose-robust methods still remain
susceptible to suboptimal solutions when poses are poor initialized. In this
paper, we propose MC-NeRF, a method can jointly optimize both intrinsic and
extrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, we
conduct a theoretical analysis to tackle the degenerate case and coupling issue
that arise from the joint optimization between intrinsic and extrinsic
parameters. Secondly, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Lastly, we present a global
end-to-end network with training sequence that enables the regression of
intrinsic and extrinsic parameters, along with the rendering network. Moreover,
most existing datasets are designed for unique camera, we create a new dataset
that includes four different styles of multi-camera acquisition systems,
allowing readers to generate custom datasets. Experiments confirm the
effectiveness of our method when each image corresponds to different camera
parameters. Specifically, we adopt up to 110 images with 110 different
intrinsic and extrinsic parameters, to achieve 3D scene representation without
providing initial poses. The Code and supplementary materials are available at
https://in2-viaun.github.io/MC-NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Lutong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yufeng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1&quot;&gt;Mengyin Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08535">
<title>Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper. (arXiv:2309.08535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08535</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a powerful Visual Speech Recognition (VSR) method for
multiple languages, especially for low-resource languages that have a limited
number of labeled data. Different from previous methods that tried to improve
the VSR performance for the target language by using knowledge learned from
other languages, we explore whether we can increase the amount of training data
itself for the different languages without human intervention. To this end, we
employ a Whisper model which can conduct both language identification and
audio-based speech recognition. It serves to filter data of the desired
languages and transcribe labels from the unannotated, multilingual audio-visual
data pool. By comparing the performances of VSR models trained on automatic
labels and the human-annotated labels, we show that we can achieve similar VSR
performance to that of human-annotated labels even without utilizing human
annotations. Through the automated labeling process, we label large-scale
unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours
of data for four low VSR resource languages, French, Italian, Spanish, and
Portuguese. With the automatic labels, we achieve new state-of-the-art
performance on mTEDx in four languages, significantly surpassing the previous
methods. The automatic labels are available online:
https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jeong Hun Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13745">
<title>Overview of Computer Vision Techniques in Robotized Wire Harness Assembly: Current State and Future Opportunities. (arXiv:2309.13745v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13745</link>
<description rdf:parseType="Literal">&lt;p&gt;Wire harnesses are essential hardware for electronic systems in modern
automotive vehicles. With a shift in the automotive industry towards
electrification and autonomous driving, more and more automotive electronics
are responsible for energy transmission and safety-critical functions such as
maneuvering, driver assistance, and safety system. This paradigm shift places
more demand on automotive wire harnesses from the safety perspective and
stresses the greater importance of high-quality wire harness assembly in
vehicles. However, most of the current operations of wire harness assembly are
still performed manually by skilled workers, and some of the manual processes
are problematic in terms of quality control and ergonomics. There is also a
persistent demand in the industry to increase competitiveness and gain market
share. Hence, assuring assembly quality while improving ergonomics and
optimizing labor costs is desired. Robotized assembly, accomplished by robots
or in human-robot collaboration, is a key enabler for fulfilling the
increasingly demanding quality and safety as it enables more replicable,
transparent, and comprehensible processes than completely manual operations.
However, robotized assembly of wire harnesses is challenging in practical
environments due to the flexibility of the deformable objects, though many
preliminary automation solutions have been proposed under simplified industrial
configurations. Previous research efforts have proposed the use of computer
vision technology to facilitate robotized automation of wire harness assembly,
enabling the robots to better perceive and manipulate the flexible wire
harness. This article presents an overview of computer vision technology
proposed for robotized wire harness assembly and derives research gaps that
require further study to facilitate a more practical robotized assembly of wire
harnesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salunkhe_O/0/1/0/all/0/1&quot;&gt;Omkar Salunkhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quadrini_W/0/1/0/all/0/1&quot;&gt;Walter Quadrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamkull_D/0/1/0/all/0/1&quot;&gt;Dan L&amp;#xe4;mkull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ore_F/0/1/0/all/0/1&quot;&gt;Fredrik Ore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahre_J/0/1/0/all/0/1&quot;&gt;Johan Stahre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14062">
<title>FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14062</link>
<description rdf:parseType="Literal">&lt;p&gt;Exemplar-free class-incremental learning (CIL) poses several challenges since
it prohibits the rehearsal of data from previous tasks and thus suffers from
catastrophic forgetting. Recent approaches to incrementally learning the
classifier by freezing the feature extractor after the first task have gained
much attention. In this paper, we explore prototypical networks for CIL, which
generate new class prototypes using the frozen feature extractor and classify
the features based on the Euclidean distance to the prototypes. In an analysis
of the feature distributions of classes, we show that classification based on
Euclidean metrics is successful for jointly trained features. However, when
learning from non-stationary data, we observe that the Euclidean metric is
suboptimal and that feature distributions are heterogeneous. To address this
challenge, we revisit the anisotropic Mahalanobis distance for CIL. In
addition, we empirically show that modeling the feature covariance relations is
better than previous attempts at sampling features from normal distributions
and training a linear classifier. Unlike existing methods, our approach
generalizes to both many- and few-shot CIL settings, as well as to
domain-incremental settings. Interestingly, without updating the backbone
network, our method obtains state-of-the-art results on several standard
continual learning benchmarks. Code is available at
https://github.com/dipamgoswami/FeCAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1&quot;&gt;Dipam Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05392">
<title>Lightweight Full-Convolutional Siamese Tracker. (arXiv:2310.05392v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05392</link>
<description rdf:parseType="Literal">&lt;p&gt;Although single object trackers have achieved advanced performance, their
large-scale models hinder their application on limited resources platforms.
Moreover, existing lightweight trackers only achieve a balance between 2-3
points in terms of parameters, performance, Flops and FPS. To achieve the
optimal balance among these points, this paper proposes a lightweight
full-convolutional Siamese tracker called LightFC. LightFC employs a novel
efficient cross-correlation module (ECM) and a novel efficient rep-center head
(ERH) to improve the feature representation of the convolutional tracking
pipeline. The ECM uses an attention-like module design, which conducts spatial
and channel linear fusion of fused features and enhances the nonlinearity of
the fused features. Additionally, it refers to successful factors of current
lightweight trackers and introduces skip-connections and reuse of search area
features. The ERH reparameterizes the feature dimensional stage in the standard
center-head and introduces channel attention to optimize the bottleneck of key
feature flows. Comprehensive experiments show that LightFC achieves the optimal
balance between performance, parameters, Flops and FPS. The precision score of
LightFC outperforms MixFormerV2-S on LaSOT and TNL2K by 3.7 % and 6.5 %,
respectively, while using 5x fewer parameters and 4.6x fewer Flops. Besides,
LightFC runs 2x faster than MixFormerV2-S on CPUs. In addition, a
higher-performance version named LightFC-vit is proposed by replacing a more
powerful backbone network. The code and raw results can be found at
https://github.com/LiYunfengLYF/LightFC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xueyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuoyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Ye Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01811">
<title>DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating high-quality and person-generic visual dubbing remains a
challenge. Recent innovation has seen the advent of a two-stage paradigm,
decoupling the rendering and lip synchronization process facilitated by
intermediate representation as a conduit. Still, previous methodologies rely on
rough landmarks or are confined to a single speaker, thus limiting their
performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We
first craft the Diffusion auto-encoder by an inpainting renderer incorporating
a mask to delineate editable zones and unaltered regions. This allows for
seamless filling of the lower-face region while preserving the remaining parts.
Throughout our experiments, we encountered several challenges. Primarily, the
semantic encoder lacks robustness, constricting its ability to capture
high-level features. Besides, the modeling ignored facial positioning, causing
mouth or nose jitters across frames. To tackle these issues, we employ
versatile strategies, including data augmentation and supplementary eye
guidance. Moreover, we encapsulated a conformer-based reference encoder and
motion generator fortified by a cross-attention mechanism. This enables our
model to learn person-specific textures with varying references and reduces
reliance on paired audio-visual data. Our rigorous experiments comprehensively
highlight that our ground-breaking approach outpaces existing methods with
considerable margins and delivers seamless, intelligible videos in
person-generic and multilingual scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chenpeng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Shuai Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03209">
<title>Cache Me if You Can: Accelerating Diffusion Models through Block Caching. (arXiv:2312.03209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03209</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently revolutionized the field of image synthesis
due to their ability to generate photorealistic images. However, one of the
major drawbacks of diffusion models is that the image generation process is
costly. A large image-to-image network has to be applied many times to
iteratively refine an image from random noise. While many recent works propose
techniques to reduce the number of required steps, they generally treat the
underlying denoising network as a black box. In this work, we investigate the
behavior of the layers within the network and find that 1) the layers&apos; output
changes smoothly over time, 2) the layers show distinct patterns of change, and
3) the change from step to step is often very small. We hypothesize that many
layer computations in the denoising network are redundant. Leveraging this, we
introduce block caching, in which we reuse outputs from layer blocks of
previous steps to speed up inference. Furthermore, we propose a technique to
automatically determine caching schedules based on each block&apos;s changes over
timesteps. In our experiments, we show through FID, human evaluation and
qualitative analysis that Block Caching allows to generate images with higher
visual quality at the same computational cost. We demonstrate this for
different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wimbauer_F/0/1/0/all/0/1&quot;&gt;Felix Wimbauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenfeld_E/0/1/0/all/0/1&quot;&gt;Edgar Schoenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Ji Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zijian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1&quot;&gt;Artsiom Sanakoyeu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peizhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1&quot;&gt;Sam Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jonas Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1&quot;&gt;Christian Rupprecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17431">
<title>MVPatch: More Vivid Patch for Adversarial Camouflaged Attacks on Object Detectors in the Physical World. (arXiv:2312.17431v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent investigations demonstrate that adversarial patches can be utilized to
manipulate the result of object detection models. However, the conspicuous
patterns on these patches may draw more attention and raise suspicions among
humans. Moreover, existing works have primarily focused on enhancing the
efficacy of attacks in the physical domain, rather than seeking to optimize
their stealth attributes and transferability potential. To address these
issues, we introduce a dual-perception-based attack framework that generates an
adversarial patch known as the More Vivid Patch (MVPatch). The framework
consists of a model-perception degradation method and a human-perception
improvement method. To derive the MVPatch, we formulate an iterative process
that simultaneously constrains the efficacy of multiple object detectors and
refines the visual correlation between the generated adversarial patch and a
realistic image. Our method employs a model-perception-based approach that
reduces the object confidence scores of several object detectors to boost the
transferability of adversarial patches. Further, within the
human-perception-based framework, we put forward a lightweight technique for
visual similarity measurement that facilitates the development of inconspicuous
and natural adversarial patches and eliminates the reliance on additional
generative models. Additionally, we introduce the naturalness score and
transferability score as metrics for an unbiased assessment of various
adversarial patches&apos; natural appearance and transferability capacity. Extensive
experiments demonstrate that the proposed MVPatch algorithm achieves superior
attack transferability compared to similar algorithms in both digital and
physical domains while also exhibiting a more natural appearance. These
findings emphasize the remarkable stealthiness and transferability of the
proposed MVPatch attack algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hongbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ju Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_L/0/1/0/all/0/1&quot;&gt;Liwei Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Shuchang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wenquan Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17663">
<title>Shape-IoU: More Accurate Metric considering Bounding Box Shape and Scale. (arXiv:2312.17663v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17663</link>
<description rdf:parseType="Literal">&lt;p&gt;As an important component of the detector localization branch, bounding box
regression loss plays a significant role in object detection tasks. The
existing bounding box regression methods usually consider the geometric
relationship between the GT box and the predicted box, and calculate the loss
by using the relative position and shape of the bounding boxes, while ignoring
the influence of inherent properties such as the shape and scale of the
bounding boxes on bounding box regression. In order to make up for the
shortcomings of existing research, this article proposes a bounding box
regression method that focuses on the shape and scale of the bounding box
itself. Firstly, we analyzed the regression characteristics of the bounding
boxes and found that the shape and scale factors of the bounding boxes
themselves will have an impact on the regression results. Based on the above
conclusions, we propose the Shape IoU method, which can calculate the loss by
focusing on the shape and scale of the bounding box itself, thereby making the
bounding box regression more accurate. Finally, we validated our method through
a large number of comparative experiments, which showed that our method can
effectively improve detection performance and outperform existing methods,
achieving state-of-the-art performance in different detection tasks.Code is
available at https://github.com/malagoutou/Shape-IoU
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaijie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05379">
<title>AutoVisual Fusion Suite: A Comprehensive Evaluation of Image Segmentation and Voice Conversion Tools on HuggingFace Platform. (arXiv:2401.05379v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a comprehensive evaluation of tools available on the
HuggingFace platform for two pivotal applications in artificial intelligence:
image segmentation and voice conversion. The primary objective was to identify
the top three tools within each category and subsequently install and configure
these tools on Linux systems. We leveraged the power of pre-trained
segmentation models such as SAM and DETR Model with ResNet-50 backbone for
image segmentation, and the so-vits-svc-fork model for voice conversion. This
paper delves into the methodologies and challenges encountered during the
implementation process, and showcases the successful combination of video
segmentation and voice conversion in a unified project named AutoVisual Fusion
Suite.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1&quot;&gt;Amirreza Hashemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05594">
<title>Wasserstein Distance-based Expansion of Low-Density Latent Regions for Unknown Class Detection. (arXiv:2401.05594v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05594</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the significant challenge in open-set object detection
(OSOD): the tendency of state-of-the-art detectors to erroneously classify
unknown objects as known categories with high confidence. We present a novel
approach that effectively identifies unknown objects by distinguishing between
high and low-density regions in latent space. Our method builds upon the
Open-Det (OD) framework, introducing two new elements to the loss function.
These elements enhance the known embedding space&apos;s clustering and expand the
unknown space&apos;s low-density regions. The first addition is the Class
Wasserstein Anchor (CWA), a new function that refines the classification
boundaries. The second is a spectral normalisation step, improving the
robustness of the model. Together, these augmentations to the existing
Contrastive Feature Learner (CFL) and Unknown Probability Learner (UPL) loss
functions significantly improve OSOD performance. Our proposed OpenDet-CWA
(OD-CWA) method demonstrates: a) a reduction in open-set errors by
approximately 17%-22%, b) an enhancement in novelty detection capability by
1.5%-16%, and c) a decrease in the wilderness index by 2%-20% across various
open-set scenarios. These results represent a substantial advancement in the
field, showcasing the potential of our approach in managing the complexities of
open-set object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_P/0/1/0/all/0/1&quot;&gt;Prakash Mallick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1&quot;&gt;Feras Dayoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherrah_J/0/1/0/all/0/1&quot;&gt;Jamie Sherrah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05646">
<title>Masked Attribute Description Embedding for Cloth-Changing Person Re-identification. (arXiv:2401.05646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05646</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloth-changing person re-identification (CC-ReID) aims to match persons who
change clothes over long periods. The key challenge in CC-ReID is to extract
clothing-independent features, such as face, hairstyle, body shape, and gait.
Current research mainly focuses on modeling body shape using multi-modal
biological features (such as silhouettes and sketches). However, it does not
fully leverage the personal description information hidden in the original RGB
image. Considering that there are certain attribute descriptions which remain
unchanged after the changing of cloth, we propose a Masked Attribute
Description Embedding (MADE) method that unifies personal visual appearance and
attribute description for CC-ReID. Specifically, handling variable
clothing-sensitive information, such as color and type, is challenging for
effective modeling. To address this, we mask the clothing and color information
in the personal attribute description extracted through an attribute detection
model. The masked attribute description is then connected and embedded into
Transformer blocks at various levels, fusing it with the low-level to
high-level features of the image. This approach compels the model to discard
clothing information. Experiments are conducted on several CC-ReID benchmarks,
including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE
effectively utilizes attribute description, enhancing cloth-changing person
re-identification performance, and compares favorably with state-of-the-art
methods. The code is available at https://github.com/moon-wh/MADE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chunlei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Ruimin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05806">
<title>CLIP-Driven Semantic Discovery Network for Visible-Infrared Person Re-Identification. (arXiv:2401.05806v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05806</link>
<description rdf:parseType="Literal">&lt;p&gt;Visible-infrared person re-identification (VIReID) primarily deals with
matching identities across person images from different modalities. Due to the
modality gap between visible and infrared images, cross-modality identity
matching poses significant challenges. Recognizing that high-level semantics of
pedestrian appearance, such as gender, shape, and clothing style, remain
consistent across modalities, this paper intends to bridge the modality gap by
infusing visual features with high-level semantics. Given the capability of
CLIP to sense high-level semantic information corresponding to visual
representations, we explore the application of CLIP within the domain of
VIReID. Consequently, we propose a CLIP-Driven Semantic Discovery Network
(CSDN) that consists of Modality-specific Prompt Learner, Semantic Information
Integration (SII), and High-level Semantic Embedding (HSE). Specifically,
considering the diversity stemming from modality discrepancies in language
descriptions, we devise bimodal learnable text tokens to capture
modality-private semantic information for visible and infrared images,
respectively. Additionally, acknowledging the complementary nature of semantic
details across different modalities, we integrate text features from the
bimodal language descriptions to achieve comprehensive semantics. Finally, we
establish a connection between the integrated text features and the visual
features across modalities. This process embed rich high-level semantic
information into visual representations, thereby promoting the modality
invariance of visual representations. The effectiveness and superiority of our
proposed CSDN over existing methods have been substantiated through
experimental evaluations on multiple widely used benchmarks. The code will be
released at \url{https://github.com/nengdong96/CSDN}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Neng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liehuang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dapeng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06013">
<title>Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06013</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,
surgical navigation and augmented reality visualization. Although the
foundation model exhibits outstanding performance in many vision tasks,
including depth estimation (e.g., DINOv2), recent works observed its
limitations in medical and surgical domain-specific applications. This work
presents a low-ranked adaptation (LoRA) of the foundation model for surgical
depth estimation. Methods: We design a foundation model-based depth estimation
method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for
depth estimation in endoscopic surgery. We build LoRA layers and integrate them
into DINO to adapt with surgery-specific domain knowledge instead of
conventional fine-tuning. During training, we freeze the DINO image encoder,
which shows excellent visual representation capacity, and only optimize the
LoRA layers and depth decoder to integrate features from the surgical scene.
Results: Our model is extensively validated on a MICCAI challenge dataset of
SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically
show that Surgical-DINO significantly outperforms all the state-of-the-art
models in endoscopic depth estimation tasks. The analysis with ablation studies
has shown evidence of the remarkable effect of our LoRA layers and adaptation.
Conclusion: Surgical-DINO shed some light on the successful adaptation of the
foundation models into the surgical domain for depth estimation. There is clear
evidence in the results that zero-shot prediction on pre-trained weights in
computer vision datasets or naive fine-tuning is not sufficient to use the
foundation model in the surgical domain directly. Code is available at
https://github.com/BeileiCui/SurgicalDINO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Beilei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Mobarakol Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06071">
<title>LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06071</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal large language models have demonstrated impressive performance
across various tasks in different modalities. However, existing multi-modal
models primarily emphasize capturing global information within each modality
while neglecting the importance of perceiving local information across
modalities. Consequently, these models lack the ability to effectively
understand the fine-grained details of input data, limiting their performance
in tasks that require a more nuanced understanding. To address this limitation,
there is a compelling need to develop models that enable fine-grained
understanding across multiple modalities, thereby enhancing their applicability
to a wide range of tasks. In this paper, we propose LEGO, a language enhanced
multi-modal grounding model. Beyond capturing global information like other
multi-modal models, our proposed model excels at tasks demanding a detailed
understanding of local information within the input. It demonstrates precise
identification and localization of specific regions in images or moments in
videos. To achieve this objective, we design a diversified dataset construction
pipeline, resulting in a multi-modal, multi-granularity dataset for model
training. The code, dataset, and demo of our model can be found at https:
//github.com/lzw-lzw/LEGO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yiqing Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1&quot;&gt;Qi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Ran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Junting Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zefeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_V/0/1/0/all/0/1&quot;&gt;Van Tu Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhida Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02877">
<title>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. (arXiv:2311.02877v4 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of detectors, Bounding Box Regression (BBR) loss
function has constantly updated and optimized. However, the existing IoU-based
BBR still focus on accelerating convergence by adding new loss terms, ignoring
the limitations of IoU loss term itself. Although theoretically IoU loss can
effectively describe the state of bounding box regression,in practical
applications, it cannot adjust itself according to different detectors and
detection tasks, and does not have strong generalization. Based on the above,
we first analyzed the BBR model and concluded that distinguishing different
regression samples and using different scales of auxiliary bounding boxes to
calculate losses can effectively accelerate the bounding box regression
process. For high IoU samples, using smaller auxiliary bounding boxes to
calculate losses can accelerate convergence, while larger auxiliary bounding
boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which
calculates IoU loss through auxiliary bounding boxes. For different datasets
and detectors, we introduce a scaling factor ratio to control the scale size of
the auxiliary bounding boxes for calculating losses. Finally, integrate
Inner-IoU into the existing IoU-based loss functions for simulation and
comparative experiments. The experiment result demonstrate a further
enhancement in detection performance with the utilization of the method
proposed in this paper, verifying the effectiveness and generalization ability
of Inner-IoU loss. Code is available at
https://github.com/malagoutou/Inner-IoU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaijie Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>