<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.14053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.01660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10887" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.17066">
<title>Cluster trajectory of SOFA score in predicting mortality in sepsis. (arXiv:2311.17066v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.17066</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Sepsis is a life-threatening condition. Sequential Organ Failure
Assessment (SOFA) score is commonly used to assess organ dysfunction and
predict ICU mortality, but it is taken as a static measurement and fails to
capture dynamic changes. This study aims to investigate the relationship
between dynamic changes in SOFA scores over the first 72 hours of ICU admission
and patient outcomes.
&lt;/p&gt;
&lt;p&gt;Design, setting, and participants: 3,253 patients in the Medical Information
Mart for Intensive Care IV database who met the sepsis-3 criteria and were
admitted from the emergency department with at least 72 hours of ICU admission
and full-active resuscitation status were analysed. Group-based trajectory
modelling with dynamic time warping and k-means clustering identified distinct
trajectory patterns in dynamic SOFA scores. They were subsequently compared
using Python.
&lt;/p&gt;
&lt;p&gt;Main outcome measures: Outcomes including hospital and ICU mortality, length
of stay in hospital and ICU, and readmission during hospital stay, were
collected. Discharge time from ICU to wards and cut-offs at 7-day and 14-day
were taken.
&lt;/p&gt;
&lt;p&gt;Results: Four clusters were identified: A (consistently low SOFA scores), B
(rapid increase followed by a decline in SOFA scores), C (higher baseline
scores with gradual improvement), and D (persistently elevated scores). Cluster
D had the longest ICU and hospital stays, highest ICU and hospital mortality.
Discharge rates from ICU were similar for Clusters A and B, while Cluster C had
initially comparable rates but a slower transition to ward.
&lt;/p&gt;
&lt;p&gt;Conclusion: Monitoring dynamic changes in SOFA score is valuable for
assessing sepsis severity and treatment responsiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ke_Y/0/1/0/all/0/1&quot;&gt;Yuhe Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Matilda Swee Sun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Loh_C/0/1/0/all/0/1&quot;&gt;Celestine Jia Ling Loh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Abdullah_H/0/1/0/all/0/1&quot;&gt;Hairil Rizal Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shannon_N/0/1/0/all/0/1&quot;&gt;Nicholas Brian Shannon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17072">
<title>IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers. (arXiv:2311.17072v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17072</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative training has been demonstrated to be powerful for building
visual-language models. However, on zero-shot discriminative benchmarks, there
is still a performance gap between models trained with generative and
discriminative objectives. In this paper, we aim to narrow this gap by
improving the efficacy of generative training on classification tasks, without
any finetuning processes or additional modules.
&lt;/p&gt;
&lt;p&gt;Specifically, we focus on narrowing the gap between the generative captioner
and the CLIP classifier. We begin by analysing the predictions made by the
captioner and classifier and observe that the caption generation inherits the
distribution bias from the language model trained with pure text modality,
making it less grounded on the visual signal. To tackle this problem, we
redesign the scoring objective for the captioner to alleviate the
distributional bias and focus on measuring the gain of information brought by
the visual inputs. We further design a generative training objective to match
the evaluation objective. We name our model trained and evaluated from the
novel procedures as Information Gain (IG) captioner. We pretrain the models on
the public Laion-5B dataset and perform a series of discriminative evaluations.
For the zero-shot classification on ImageNet, IG captioner achieves $&amp;gt; 18\%$
improvements over the standard captioner, achieving comparable performances
with the CLIP classifier. IG captioner also demonstrated strong performance on
zero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this
paper inspires further research towards unifying generative and discriminative
training procedures for visual-language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiahui Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17076">
<title>Compositional Chain-of-Thought Prompting for Large Multimodal Models. (arXiv:2311.17076v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17076</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of strong visual backbones and Large Language Model (LLM)
reasoning has led to Large Multimodal Models (LMMs) becoming the current
standard for a wide range of vision and language (VL) tasks. However, recent
research has shown that even the most advanced LMMs still struggle to capture
aspects of compositional visual reasoning, such as attributes and relationships
between objects. One solution is to utilize scene graphs (SGs)--a formalization
of objects and their relations and attributes that has been extensively used as
a bridge between the visual and textual domains. Yet, scene graph data requires
scene graph annotations, which are expensive to collect and thus not easily
scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic
forgetting of the pretraining objective. To overcome this, inspired by
chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a
novel zero-shot Chain-of-Thought prompting method that utilizes SG
representations in order to extract compositional knowledge from an LMM.
Specifically, we first generate an SG using the LMM, and then use that SG in
the prompt to produce a response. Through extensive experiments, we find that
the proposed CCoT approach not only improves LMM performance on several vision
and language VL compositional benchmarks but also improves the performance of
several popular LMMs on general multimodal benchmarks, without the need for
fine-tuning or annotated ground-truth SGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1&quot;&gt;Chancharik Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Brandon Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1&quot;&gt;Roei Herzig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17080">
<title>Combating the &quot;Sameness&quot; in AI Art: Reflections on the Interactive AI Installation Fencing Hallucination. (arXiv:2311.17080v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17080</link>
<description rdf:parseType="Literal">&lt;p&gt;The article summarizes three types of &quot;sameness&quot; issues in Artificial
Intelligence(AI) art, each occurring at different stages of development in AI
image creation tools. Through the Fencing Hallucination project, the article
reflects on the design of AI art production in alleviating the sense of
uniformity, maintaining the uniqueness of images from an AI image synthesizer,
and enhancing the connection between the artworks and the audience. This paper
endeavors to stimulate the creation of distinctive AI art by recounting the
efforts and insights derived from the Fencing Hallucination project, all
dedicated to addressing the issue of &quot;sameness&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1&quot;&gt;Weihao Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legrady_G/0/1/0/all/0/1&quot;&gt;George Legrady&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17095">
<title>Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models. (arXiv:2311.17095v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17095</link>
<description rdf:parseType="Literal">&lt;p&gt;From an enormous amount of image-text pairs, large-scale vision-language
models (VLMs) learn to implicitly associate image regions with words, which is
vital for tasks such as image captioning and visual question answering.
However, leveraging such pre-trained models for open-vocabulary semantic
segmentation remains a challenge. In this paper, we propose a simple, yet
extremely effective, training-free technique, Plug-and-Play Open-Vocabulary
Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with
direct text-to-image cross-attention and an image-text matching loss to produce
semantic segmentation. However, cross-attention alone tends to over-segment,
whereas cross-attention plus GradCAM tend to under-segment. To alleviate this
issue, we introduce Salience Dropout; by iteratively dropping patches that the
model is most attentive to, we are able to better resolve the entire extent of
the segmentation mask. Compared to existing techniques, the proposed method
does not require any neural network training and performs hyperparameter tuning
without the need for any segmentation annotations, even for a validation set.
PnP-OVSS demonstrates substantial improvements over a comparable baseline
(+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS
COCO, +2.4% mIoU on COCO Stuff) and even outperforms most baselines that
conduct additional network training on top of pretrained VLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiayun_L/0/1/0/all/0/1&quot;&gt;Luo Jiayun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_S/0/1/0/all/0/1&quot;&gt;Siddhesh Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1&quot;&gt;Leonid Sigal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17097">
<title>Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis. (arXiv:2311.17097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17097</link>
<description rdf:parseType="Literal">&lt;p&gt;Jamming and intrusion detection are critical in 5G research, aiming to
maintain reliability, prevent user experience degradation, and avoid
infrastructure failure. This paper introduces an anonymous jamming detection
model for 5G based on signal parameters from the protocol stacks. The system
uses supervised and unsupervised learning for real-time, high-accuracy
detection of jamming, including unknown types. Supervised models reach an AUC
of 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the
need for data annotation limits the supervised approach. To address this, an
unsupervised auto-encoder-based anomaly detection is presented with an AUC of
0.987. The approach is resistant to adversarial training samples. For
transparency and domain knowledge injection, a Bayesian network-based causation
analysis is introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jere_S/0/1/0/all/0/1&quot;&gt;Shashank Jere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Soumya Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Sachin Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayekh_S/0/1/0/all/0/1&quot;&gt;Shehadi Dayekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17098">
<title>DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection. (arXiv:2311.17098v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17098</link>
<description rdf:parseType="Literal">&lt;p&gt;In object detection, achieving constant accuracy is challenging due to the
variability of object sizes. One possible solution to this problem is to
optimize the input resolution, known as a multi-resolution strategy. Previous
approaches for optimizing resolution are often based on pre-defined resolutions
or a dynamic neural network, but there is a lack of study for run-time
resolution optimization for existing architecture. In this paper, we propose an
adaptive resolution scaling network called DyRA, which comprises convolutions
and transformer encoder blocks, for existing detectors. Our DyRA returns a
scale factor from an input image, which enables instance-specific scaling. This
network is jointly trained with detectors with specially designed loss
functions, namely ParetoScaleLoss and BalanceLoss. The ParetoScaleLoss produces
an adaptive scale factor from the image, while the BalanceLoss optimizes the
scale factor according to localization power for the dataset. The loss function
is designed to minimize accuracy drop about the contrasting objective of small
and large objects. Our experiments on COCO, RetinaNet, Faster-RCNN, FCOS, and
Mask-RCNN achieved 1.3%, 1.1%, 1.3%, and 0.8% accuracy improvement than a
multi-resolution baseline with solely resolution adjustment. The code is
available at https://github.com/DaEunFullGrace/DyRA.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1&quot;&gt;Daeun Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hoeseok Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungshin Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17099">
<title>StreamFlow: Streamlined Multi-Frame Optical Flow Estimation for Video Sequences. (arXiv:2311.17099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17099</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusions between consecutive frames have long posed a significant challenge
in optical flow estimation. The inherent ambiguity introduced by occlusions
directly violates the brightness constancy constraint and considerably hinders
pixel-to-pixel matching. To address this issue, multi-frame optical flow
methods leverage adjacent frames to mitigate the local ambiguity. Nevertheless,
prior multi-frame methods predominantly adopt recursive flow estimation,
resulting in a considerable computational overlap. In contrast, we propose a
streamlined in-batch framework that eliminates the need for extensive redundant
recursive computations while concurrently developing effective spatio-temporal
modeling approaches under in-batch estimation constraints. Specifically, we
present a Streamlined In-batch Multi-frame (SIM) pipeline tailored to video
input, attaining a similar level of time efficiency to two-frame networks.
Furthermore, we introduce an efficient Integrative Spatio-temporal Coherence
(ISC) modeling method for effective spatio-temporal modeling during the
encoding phase, which introduces no additional parameter overhead.
Additionally, we devise a Global Temporal Regressor (GTR) that effectively
explores temporal relations during decoding. Benefiting from the efficient SIM
pipeline and effective modules, StreamFlow not only excels in terms of
performance on the challenging KITTI and Sintel datasets, with particular
improvement in occluded areas but also attains a remarkable $63.82\%$
enhancement in speed compared with previous multi-frame methods. The code will
be available soon at https://github.com/littlespray/StreamFlow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shangkun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Thomas H. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huaxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guoqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wei Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17103">
<title>Single-cell Multi-view Clustering via Community Detection with Unknown Number of Clusters. (arXiv:2311.17103v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2311.17103</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-cell multi-view clustering enables the exploration of cellular
heterogeneity within the same cell from different views. Despite the
development of several multi-view clustering methods, two primary challenges
persist. Firstly, most existing methods treat the information from both
single-cell RNA (scRNA) and single-cell Assay of Transposase Accessible
Chromatin (scATAC) views as equally significant, overlooking the substantial
disparity in data richness between the two views. This oversight frequently
leads to a degradation in overall performance. Additionally, the majority of
clustering methods necessitate manual specification of the number of clusters
by users. However, for biologists dealing with cell data, precisely determining
the number of distinct cell types poses a formidable challenge. To this end, we
introduce scUNC, an innovative multi-view clustering approach tailored for
single-cell data, which seamlessly integrates information from different views
without the need for a predefined number of clusters. The scUNC method
comprises several steps: initially, it employs a cross-view fusion network to
create an effective embedding, which is then utilized to generate initial
clusters via community detection. Subsequently, the clusters are automatically
merged and optimized until no further clusters can be merged. We conducted a
comprehensive evaluation of scUNC using three distinct single-cell datasets.
The results underscored that scUNC outperforms the other baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhibin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17104">
<title>Single-Cell Clustering via Dual-Graph Alignment. (arXiv:2311.17104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17104</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of single-cell RNA sequencing has seen a surge in
the development of clustering methods. These methods enable the identification
of cell subpopulations, thereby facilitating the understanding of tumor
microenvironments. Despite their utility, most existing clustering algorithms
primarily focus on the attribute information provided by the cell matrix or the
network structure between cells, often neglecting the network between genes.
This oversight could lead to loss of information and clustering results that
lack clinical significance. To address this limitation, we develop an advanced
single-cell clustering model incorporating dual-graph alignment, which
integrates gene network information into the clustering process based on
self-supervised and unsupervised optimization. Specifically, we designed a
graph-based autoencoder enhanced by an attention mechanism to effectively
capture relationships between cells. Moreover, we performed the node2vec method
on Protein-Protein Interaction (PPI) networks to derive the gene network
structure and maintained this structure throughout the clustering process. Our
proposed method has been demonstrated to be effective through experimental
results, showcasing its ability to optimize clustering outcomes while
preserving the original associations between cells and genes. This research
contributes to obtaining accurate cell subpopulations and generates clustering
results that more closely resemble real-world biological scenarios. It provides
better insights into the characteristics and distribution of diseased cells,
ultimately building a foundation for early disease diagnosis and treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17107">
<title>ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?. (arXiv:2311.17107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17107</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the accuracy of outputs generated by Large Language Models (LLMs)
is especially important in the climate science and policy domain. We introduce
the Expert Confidence in Climate Statements (ClimateX) dataset, a novel,
curated, expert-labeled dataset consisting of 8094 climate statements collected
from the latest Intergovernmental Panel on Climate Change (IPCC) reports,
labeled with their associated confidence levels. Using this dataset, we show
that recent LLMs can classify human expert confidence in climate-related
statements, especially in a few-shot learning setting, but with limited (up to
47%) accuracy. Overall, models exhibit consistent and significant
over-confidence on low and medium confidence statements. We highlight
implications of our results for climate communication, LLMs evaluation
strategies, and the use of LLMs in information retrieval systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacombe_R/0/1/0/all/0/1&quot;&gt;Romain Lacombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kerrie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilworth_E/0/1/0/all/0/1&quot;&gt;Eddie Dilworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17110">
<title>XAI for time-series classification leveraging image highlight methods. (arXiv:2311.17110v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17110</link>
<description rdf:parseType="Literal">&lt;p&gt;Although much work has been done on explainability in the computer vision and
natural language processing (NLP) fields, there is still much work to be done
to explain methods applied to time series as time series by nature can not be
understood at first sight. In this paper, we present a Deep Neural Network
(DNN) in a teacher-student architecture (distillation model) that offers
interpretability in time-series classification tasks. The explainability of our
approach is based on transforming the time series to 2D plots and applying
image highlight methods (such as LIME and GradCam), making the predictions
interpretable. At the same time, the proposed approach offers increased
accuracy competing with the baseline model with the trade-off of increasing the
training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makridis_G/0/1/0/all/0/1&quot;&gt;Georgios Makridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatouros_G/0/1/0/all/0/1&quot;&gt;Georgios Fatouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koukos_V/0/1/0/all/0/1&quot;&gt;Vasileios Koukos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Kotios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyriazis_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Kyriazis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soldatos_I/0/1/0/all/0/1&quot;&gt;Ioannis Soldatos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17123">
<title>ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis. (arXiv:2311.17123v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17123</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a method to address the challenge of rendering a 3D
human from a single image in a free-view manner. Some existing approaches could
achieve this by using generalizable pixel-aligned implicit fields to
reconstruct a textured mesh of a human or by employing a 2D diffusion model as
guidance with the Score Distillation Sampling (SDS) method, to lift the 2D
image into 3D space. However, a generalizable implicit field often results in
an over-smooth texture field, while the SDS method tends to lead to a
texture-inconsistent novel view with the input image. In this paper, we
introduce a texture-consistent back view synthesis module that could transfer
the reference image content to the back view through depth and text-guided
attention injection. Moreover, to alleviate the color distortion that occurs in
the side region, we propose a visibility-aware patch consistency regularization
for texture mapping and refinement combined with the synthesized back view
texture. With the above techniques, we could achieve high-fidelity and
texture-consistent human rendering from a single image. Experiments conducted
on both real and synthetic data demonstrate the effectiveness of our method and
show that our approach outperforms previous baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiangjun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yanpei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1&quot;&gt;Long Quan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17124">
<title>A knowledge-driven AutoML architecture. (arXiv:2311.17124v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17124</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a knowledge-driven AutoML architecture for pipeline and
deep feature synthesis. The main goal is to render the AutoML process
explainable and to leverage domain knowledge in the synthesis of pipelines and
features. The architecture explores several novel ideas: first, the
construction of pipelines and deep features is approached in an unified way.
Next, synthesis is driven by a shared knowledge system, interactively queried
as to what pipeline operations to use or features to compute. Lastly, the
synthesis processes takes decisions at runtime using partial solutions and
results of their application on data. Two experiments are conducted to
demonstrate the functionality of a na\&quot;{\i}ve implementation of the proposed
architecture and to discuss its advantages, trade-offs as well as future
potential for AutoML.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cofaru_C/0/1/0/all/0/1&quot;&gt;Corneliu Cofaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loeckx_J/0/1/0/all/0/1&quot;&gt;Johan Loeckx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17128">
<title>Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks. (arXiv:2311.17128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17128</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Optical Character Recognition (OCR) have been driven
by transformer-based models. OCR systems are critical in numerous high-stakes
domains, yet their vulnerability to adversarial attack remains largely
uncharted territory, raising concerns about security and compliance with
emerging AI regulations. In this work we present a novel framework to assess
the resilience of Transformer-based OCR (TrOCR) models. We develop and assess
algorithms for both targeted and untargeted attacks. For the untargeted case,
we measure the Character Error Rate (CER), while for the targeted case we use
the success ratio. We find that TrOCR is highly vulnerable to untargeted
attacks and somewhat less vulnerable to targeted attacks. On a benchmark
handwriting data set, untargeted attacks can cause a CER of more than 1 without
being noticeable to the eye. With a similar perturbation size, targeted attacks
can lead to success rates of around $25\%$ -- here we attacked single tokens,
requiring TrOCR to output the tenth most likely token from a large vocabulary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beerens_L/0/1/0/all/0/1&quot;&gt;Lucas Beerens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1&quot;&gt;Desmond J. Higham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17132">
<title>TransNeXt: Robust Foveal Visual Perception for Vision Transformers. (arXiv:2311.17132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17132</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the depth degradation effect in residual connections, many efficient
Vision Transformers models that rely on stacking layers for information
exchange often fail to form sufficient information mixing, leading to unnatural
visual perception. To address this issue, in this paper, we propose Aggregated
Attention, a biomimetic design-based token mixer that simulates biological
foveal vision and continuous eye movement while enabling each token on the
feature map to have a global perception. Furthermore, we incorporate learnable
tokens that interact with conventional queries and keys, which further
diversifies the generation of affinity matrices beyond merely relying on the
similarity between queries and keys. Our approach does not rely on stacking for
information exchange, thus effectively avoiding depth degradation and achieving
natural visual perception. Additionally, we propose Convolutional GLU, a
channel mixer that bridges the gap between GLU and SE mechanism, which empowers
each token to have channel attention based on its nearest neighbor image
features, enhancing local modeling capability and model robustness. We combine
aggregated attention and convolutional GLU to create a new visual backbone
called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves
state-of-the-art performance across multiple model sizes. At a resolution of
$224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0%, surpassing
ConvNeXt-B with 69% fewer parameters. Our TransNeXt-Base achieves an ImageNet
accuracy of 86.2% and an ImageNet-A accuracy of 61.6% at a resolution of
$384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic
segmentation mIoU of 54.7.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Dai Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17133">
<title>Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond. (arXiv:2311.17133v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17133</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epifano_J/0/1/0/all/0/1&quot;&gt;Jacob R. Epifano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_S/0/1/0/all/0/1&quot;&gt;Stephen Glass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1&quot;&gt;Ravi P. Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Sharad Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masino_A/0/1/0/all/0/1&quot;&gt;Aaron J. Masino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1&quot;&gt;Ghulam Rasool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17136">
<title>UniIR: Training and Benchmarking Universal Multimodal Information Retrievers. (arXiv:2311.17136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17136</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing information retrieval (IR) models often assume a homogeneous format,
limiting their applicability to diverse user needs, such as searching for
images with text descriptions, searching for a news article with a headline
image, or finding a similar photo with a query image. To approach such
different information-seeking demands, we introduce UniIR, a unified
instruction-guided multimodal retriever capable of handling eight distinct
retrieval tasks across modalities. UniIR, a single retrieval system jointly
trained on ten diverse multimodal-IR datasets, interprets user instructions to
execute various retrieval tasks, demonstrating robust performance across
existing datasets and zero-shot generalization to new tasks. Our experiments
highlight that multi-task training and instruction tuning are keys to UniIR&apos;s
generalization ability. Additionally, we construct the M-BEIR, a multimodal
retrieval benchmark with comprehensive results, to standardize the evaluation
of universal multimodal information retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Cong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haonan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hexiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1&quot;&gt;Alan Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17137">
<title>Generative Models: What do they know? Do they know things? Let&apos;s find out!. (arXiv:2311.17137v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17137</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models have been shown to be capable of synthesizing highly
detailed and realistic images. It is natural to suspect that they implicitly
learn to model some image intrinsics such as surface normals, depth, or
shadows. In this paper, we present compelling evidence that generative models
indeed internally produce high-quality scene intrinsic maps. We introduce
Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms
any generative model into a scene intrinsic predictor, capable of extracting
intrinsic scene maps directly from the original generator network without
needing additional decoders or fully fine-tuning the original network. Our
method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly
learned parameters that make up less than 0.6% of the total parameters in the
generative model. Optimized with a small set of labeled images, our
model-agnostic approach adapts to various generative architectures, including
Diffusion models, GANs, and Autoregressive models. We show that the scene
intrinsic maps produced by our method compare well with, and in some cases
surpass those generated by leading supervised techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaodan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1&quot;&gt;Nicholas Kolkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1&quot;&gt;Greg Shakhnarovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17138">
<title>Shadows Don&apos;t Lie and Lines Can&apos;t Bend! Generative Models don&apos;t know Projective Geometry...for now. (arXiv:2311.17138v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17138</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models can produce impressively realistic images. This paper
demonstrates that generated images have geometric features different from those
of real images. We build a set of collections of generated images, prequalified
to fool simple, signal-based classifiers into believing they are real. We then
show that prequalified generated images can be identified reliably by
classifiers that only look at geometric properties. We use three such
classifiers. All three classifiers are denied access to image pixels, and look
only at derived geometric features. The first classifier looks at the
perspective field of the image, the second looks at lines detected in the
image, and the third looks at relations between detected objects and shadows.
Our procedure detects generated images more reliably than SOTA local signal
based detectors, for images from a number of distinct generators. Saliency maps
suggest that the classifiers can identify geometric problems reliably. We
conclude that current generators cannot reliably reproduce geometric properties
of real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Ayush Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1&quot;&gt;Hanlin Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1&quot;&gt;Amitabh Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1&quot;&gt;Svetlana Lazebnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;D.A. Forsyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17154">
<title>Pragmatic Radiology Report Generation. (arXiv:2311.17154v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17154</link>
<description rdf:parseType="Literal">&lt;p&gt;When pneumonia is not found on a chest X-ray, should the report describe this
negative observation or omit it? We argue that this question cannot be answered
from the X-ray alone and requires a pragmatic perspective, which captures the
communicative goal that radiology reports serve between radiologists and
patients. However, the standard image-to-text formulation for radiology report
generation fails to incorporate such pragmatic intents. Following this
pragmatic perspective, we demonstrate that the indication, which describes why
a patient comes for an X-ray, drives the mentions of negative observations and
introduce indications as additional input to report generation. With respect to
the output, we develop a framework to identify uninferable information from the
image as a source of model hallucinations, and limit them by cleaning
groundtruth reports. Finally, we use indications and cleaned groundtruth
reports to develop pragmatic models, and show that they outperform existing
methods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also
in standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chacha Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenhao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17165">
<title>(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions. (arXiv:2311.17165v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17165</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macmillan_Scott_O/0/1/0/all/0/1&quot;&gt;Olivia Macmillan-Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1&quot;&gt;Mirco Musolesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17179">
<title>SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery. (arXiv:2311.17179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17179</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic location is essential for modeling tasks in fields ranging from
ecology to epidemiology to the Earth system sciences. However, extracting
relevant and meaningful characteristics of a location can be challenging, often
entailing expensive data fusion or data distillation from global imagery
datasets. To address this challenge, we introduce Satellite Contrastive
Location-Image Pretraining (SatCLIP), a global, general-purpose geographic
location encoder that learns an implicit representation of locations from
openly available satellite imagery. Trained location encoders provide vector
embeddings summarizing the characteristics of any given location for convenient
usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained
on globally sampled multi-spectral Sentinel-2 satellite data, can be used in
various predictive tasks that depend on location information but not
necessarily satellite imagery, including temperature prediction, animal
recognition in imagery, and population density estimation. Across tasks,
SatCLIP embeddings consistently outperform embeddings from existing pretrained
location encoders, ranging from models trained on natural images to models
trained on semantic context. SatCLIP embeddings also help to improve geographic
generalization. This demonstrates the potential of general-purpose location
encoders and opens the door to learning meaningful representations of our
planet from the vast, varied, and largely untapped modalities of geospatial
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1&quot;&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1&quot;&gt;Esther Rolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1&quot;&gt;Caleb Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1&quot;&gt;Marc Ru&amp;#xdf;wurm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17190">
<title>Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play. (arXiv:2311.17190v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17190</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Competitive Self-Play (CSP) have achieved, or even
surpassed, human level performance in complex game environments such as Dota 2
and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL).
One core component of these methods relies on creating a pool of learning
agents -- consisting of the Main Agent, past versions of this agent, and
Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main
Agents. A key drawback of these approaches is the large computational cost and
physical time that is required to train the system, making them impractical to
deploy in highly iterative real-life settings such as video game productions.
In this paper, we propose the Minimax Exploiter, a game theoretic approach to
exploiting Main Agents that leverages knowledge of its opponents, leading to
significant increases in data efficiency. We validate our approach in a
diversity of settings, including simple turn based games, the arcade learning
environment, and For Honor, a modern video game. The Minimax Exploiter
consistently outperforms strong baselines, demonstrating improved stability and
data efficiency, leading to a robust CSP-MARL method that is both flexible and
easy to deploy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bairamian_D/0/1/0/all/0/1&quot;&gt;Daniel Bairamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcotte_P/0/1/0/all/0/1&quot;&gt;Philippe Marcotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romoff_J/0/1/0/all/0/1&quot;&gt;Joshua Romoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robert_G/0/1/0/all/0/1&quot;&gt;Gabriel Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17227">
<title>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. (arXiv:2311.17227v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17227</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we avoid wars at the crossroads of history? This question has been
pursued by individuals, scholars, policymakers, and organizations throughout
human history. In this research, we attempt to answer the question based on the
recent advances of Artificial Intelligence (AI) and Large Language Models
(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to
simulate the participating countries, their decisions, and the consequences, in
historical international conflicts, including the World War I (WWI), the World
War II (WWII), and the Warring States Period (WSP) in Ancient China. By
evaluating the simulation effectiveness, we examine the advancements and
limitations of cutting-edge AI systems&apos; abilities in studying complex
collective human behaviors such as international conflicts under diverse
settings. In these simulations, the emergent interactions among agents also
offer a novel perspective for examining the triggers and conditions that lead
to war. Our findings offer data-driven and AI-augmented insights that can
redefine how we approach conflict resolution and peacekeeping strategies. The
implications stretch beyond historical analysis, offering a blueprint for using
AI to understand human history and possibly prevent future international
conflicts. Code and data are available at
\url{https://github.com/agiresearch/WarAgent}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lizhou Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1&quot;&gt;Kai Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1&quot;&gt;Libby Hemphill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17228">
<title>Survey on AI Ethics: A Socio-technical Perspective. (arXiv:2311.17228v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.17228</link>
<description rdf:parseType="Literal">&lt;p&gt;The past decade has observed a great advancement in AI with deep
learning-based models being deployed in diverse scenarios including
safety-critical applications. As these AI systems become deeply embedded in our
societal infrastructure, the repercussions of their decisions and actions have
significant consequences, making the ethical implications of AI deployment
highly relevant and important. The ethical concerns associated with AI are
multifaceted, including challenging issues of fairness, privacy and data
protection, responsibility and accountability, safety and robustness,
transparency and explainability, and environmental impact. These principles
together form the foundations of ethical AI considerations that concern every
stakeholder in the AI system lifecycle. In light of the present ethical and
future x-risk concerns, governments have shown increasing interest in
establishing guidelines for the ethical deployment of AI. This work unifies the
current and future ethical concerns of deploying AI into society. While we
acknowledge and appreciate the technical surveys for each of the ethical
principles concerned, in this paper, we aim to provide a comprehensive overview
that not only addresses each principle from a technical point of view but also
discusses them from a social perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mbiazi_D/0/1/0/all/0/1&quot;&gt;Dave Mbiazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhange_M/0/1/0/all/0/1&quot;&gt;Meghana Bhange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaei_M/0/1/0/all/0/1&quot;&gt;Maryam Babaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_I/0/1/0/all/0/1&quot;&gt;Ivaxi Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenfack_P/0/1/0/all/0/1&quot;&gt;Patrik Joslin Kenfack&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17232">
<title>ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks Mimicking Real Prenatal Development. (arXiv:2311.17232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17232</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational models trained on a large amount of natural images are the
state-of-the-art to study human vision - usually adult vision. Computational
models of infant vision and its further development are gaining more and more
attention in the community. In this work we aim at the very beginning of our
visual experience - pre- and post-natal retinal waves which suggest to be a
pre-training mechanism for the primate visual system at a very early stage of
development. We see this approach as an instance of biologically plausible data
driven inductive bias through pre-training. We built a computational model that
mimics this development mechanism by pre-training different artificial
convolutional neural networks with simulated retinal wave images. The resulting
features of this biologically plausible pre-training closely match the V1
features of the primate visual system. We show that the performance gain by
pre-training with retinal waves is similar to a state-of-the art pre-training
pipeline. Our framework contains the retinal wave generator, as well as a
training strategy, which can be a first step in a curriculum learning based
training diet for various models of development. We release code, data and
trained networks to build the basis for future work on visual development and
based on a curriculum learning approach including prenatal development to
support studies of innate vs. learned properties of the primate visual system.
An additional benefit of our pre-trained networks for neuroscience or computer
vision applications is the absence of biases inherited from datasets like
ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cappell_B/0/1/0/all/0/1&quot;&gt;Benjamin Cappell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_A/0/1/0/all/0/1&quot;&gt;Andreas Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umah_W/0/1/0/all/0/1&quot;&gt;Williams Chukwudi Umah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1&quot;&gt;Bernhard Egger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17233">
<title>Quantifying the redundancy between prosody and text. (arXiv:2311.17233v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17233</link>
<description rdf:parseType="Literal">&lt;p&gt;Prosody -- the suprasegmental component of speech, including pitch, loudness,
and tempo -- carries critical aspects of meaning. However, the relationship
between the information conveyed by prosody vs. by the words themselves remains
poorly understood. We use large language models (LLMs) to estimate how much
information is redundant between prosody and the words themselves. Using a
large spoken corpus of English audiobooks, we extract prosodic features aligned
to individual words and test how well they can be predicted from LLM
embeddings, compared to non-contextual word embeddings. We find a high degree
of redundancy between the information carried by the words and prosodic
information across several prosodic features, including intensity, duration,
pauses, and pitch contours. Furthermore, a word&apos;s prosodic information is
redundant with both the word itself and the context preceding as well as
following it. Still, we observe that prosodic features can not be fully
predicted from text, suggesting that prosody carries information above and
beyond the words. Along with this paper, we release a general-purpose data
processing pipeline for quantifying the relationship between linguistic
information and extra-linguistic features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lukas Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1&quot;&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1&quot;&gt;Alex Warstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1&quot;&gt;Ethan Wilcox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regev_T/0/1/0/all/0/1&quot;&gt;Tamar Regev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17248">
<title>Deep Regularized Compound Gaussian Network for Solving Linear Inverse Problems. (arXiv:2311.17248v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.17248</link>
<description rdf:parseType="Literal">&lt;p&gt;Incorporating prior information into inverse problems, e.g. via
maximum-a-posteriori estimation, is an important technique for facilitating
robust inverse problem solutions. In this paper, we devise two novel approaches
for linear inverse problems that permit problem-specific statistical prior
selections within the compound Gaussian (CG) class of distributions. The CG
class subsumes many commonly used priors in signal and image reconstruction
methods including those of sparsity-based approaches. The first method
developed is an iterative algorithm, called generalized compound Gaussian least
squares (G-CG-LS), that minimizes a regularized least squares objective
function where the regularization enforces a CG prior. G-CG-LS is then
unrolled, or unfolded, to furnish our second method, which is a novel deep
regularized (DR) neural network, called DR-CG-Net, that learns the prior
information. A detailed computational theory on convergence properties of
G-CG-LS and thorough numerical experiments for DR-CG-Net are provided. Due to
the comprehensive nature of the CG prior, these experiments show that our
unrolled DR-CG-Net outperforms competitive prior art methods in tomographic
imaging and compressive sensing, especially in challenging low-training
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyons_C/0/1/0/all/0/1&quot;&gt;Carter Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raj_R/0/1/0/all/0/1&quot;&gt;Raghu G. Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheney_M/0/1/0/all/0/1&quot;&gt;Margaret Cheney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17295">
<title>Elo Uncovered: Robustness and Best Practices in Language Model Evaluation. (arXiv:2311.17295v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17295</link>
<description rdf:parseType="Literal">&lt;p&gt;In Natural Language Processing (NLP), the Elo rating system, originally
designed for ranking players in dynamic games such as chess, is increasingly
being used to evaluate Large Language Models (LLMs) through &quot;A vs B&quot; paired
comparisons. However, while popular, the system&apos;s suitability for assessing
entities with constant skill levels, such as LLMs, remains relatively
unexplored. We study two fundamental axioms that evaluation methods should
adhere to: reliability and transitivity. We conduct extensive evaluation of Elo
behaviour, illustrating that individual Elo computations exhibit volatility and
delving into the impact of varying the Elo rating system&apos;s hyperparameters. We
show that these axioms are not always satisfied raising questions about the
reliability of current comparative evaluations of LLMs. If the current use of
Elo scores is intended to substitute the costly head-to-head comparison of
LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by
the axioms, our findings offer concrete guidelines for enhancing the
reliability of LLM evaluation methods, suggesting a need for reassessment of
existing comparative approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boubdir_M/0/1/0/all/0/1&quot;&gt;Meriem Boubdir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Edward Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermis_B/0/1/0/all/0/1&quot;&gt;Beyza Ermis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1&quot;&gt;Sara Hooker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1&quot;&gt;Marzieh Fadaee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17301">
<title>Language Models: A Guide for the Perplexed. (arXiv:2311.17301v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17301</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the growing importance of AI literacy, we decided to write this
tutorial to help narrow the gap between the discourse among those who study
language models -- the core technology underlying ChatGPT and similar products
-- and those who are intrigued and want to learn more about them. In short, we
believe the perspective of researchers and educators can add some clarity to
the public&apos;s understanding of the technologies beyond what&apos;s currently
available, which tends to be either extremely technical or promotional material
generated about products by their purveyors.
&lt;/p&gt;
&lt;p&gt;Our approach teases apart the concept of a language model from products built
on them, from the behaviors attributed to or desired from those products, and
from claims about similarity to human cognition. As a starting point, we (1)
offer a scientific viewpoint that focuses on questions amenable to study
through experimentation; (2) situate language models as they are today in the
context of the research that led to their development; and (3) describe the
boundaries of what is known about the models at this writing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serrano_S/0/1/0/all/0/1&quot;&gt;Sofia Serrano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brumbaugh_Z/0/1/0/all/0/1&quot;&gt;Zander Brumbaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17303">
<title>Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge. (arXiv:2311.17303v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17303</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a generic methodology to encode hierarchical
causality structure among observed variables into a neural network in order to
improve its predictive performance. The proposed methodology, called
causality-informed neural network (CINN), leverages three coherent steps to
systematically map the structural causal knowledge into the layer-to-layer
design of neural network while strictly preserving the orientation of every
causal relationship. In the first step, CINN discovers causal relationships
from observational data via directed acyclic graph (DAG) learning, where causal
discovery is recast as a continuous optimization problem to avoid the
combinatorial nature. In the second step, the discovered hierarchical causality
structure among observed variables is systematically encoded into neural
network through a dedicated architecture and customized loss function. By
categorizing variables in the causal DAG as root, intermediate, and leaf nodes,
the hierarchical causal DAG is translated into CINN with a one-to-one
correspondence between nodes in the causal DAG and units in the CINN while
maintaining the relative order among these nodes. Regarding the loss function,
both intermediate and leaf nodes in the DAG graph are treated as target outputs
during CINN training so as to drive co-learning of causal relationships among
different types of nodes. As multiple loss components emerge in CINN, we
leverage the projection of conflicting gradients to mitigate gradient
interference among the multiple learning tasks. Computational experiments
across a broad spectrum of UCI data sets demonstrate substantial advantages of
CINN in predictive performance over other state-of-the-art methods. In
addition, an ablation study underscores the value of integrating structural and
quantitative causal knowledge in enhancing the neural network&apos;s predictive
performance incrementally.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao-Lin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fenglei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1&quot;&gt;Yiu-Ming Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_I/0/1/0/all/0/1&quot;&gt;Indranil Bose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17305">
<title>Two-Step Reinforcement Learning for Multistage Strategy Card Game. (arXiv:2311.17305v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17305</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of artificial intelligence and card games, this study introduces
a two-step reinforcement learning (RL) strategy tailored for &quot;The Lord of the
Rings: The Card Game (LOTRCG),&quot; a complex multistage strategy card game. This
research diverges from conventional RL methods by adopting a phased learning
approach, beginning with a foundational learning stage in a simplified version
of the game and subsequently progressing to the complete, intricate game
environment. This methodology notably enhances the AI agent&apos;s adaptability and
performance in the face of LOTRCG&apos;s unpredictable and challenging nature. The
paper also explores a multi-agent system, where distinct RL agents are employed
for various decision-making aspects of the game. This approach has demonstrated
a remarkable improvement in game outcomes, with the RL agents achieving a
winrate of 78.5% across a set of 10,000 random games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godlewski_K/0/1/0/all/0/1&quot;&gt;Konrad Godlewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawicki_B/0/1/0/all/0/1&quot;&gt;Bartosz Sawicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17307">
<title>RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of Traditional Chinese Medicine. (arXiv:2311.17307v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17307</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Chinese medicine (TCM) prescription is the most critical form of
TCM treatment, and uncovering the complex nonlinear relationship between
symptoms and TCM is of great significance for clinical practice and assisting
physicians in diagnosis and treatment. Although there have been some studies on
TCM prescription generation, these studies consider a single factor and
directly model the symptom-prescription generation problem mainly based on
symptom descriptions, lacking guidance from TCM knowledge. To this end, we
propose a RoBERTa and Knowledge Enhancement model for Prescription Generation
of Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our
constructed TCM corpus, followed by fine-tuning the pre-trained model, and the
model is guided to generate TCM prescriptions by introducing four classes of
knowledge of TCM through the attention mask matrix. Experimental results on the
publicly available TCM prescription dataset show that RoKEPG improves the F1
metric by about 2% over the baseline model with the best results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1&quot;&gt;Hua Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_J/0/1/0/all/0/1&quot;&gt;Jiacong Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jieyue He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17311">
<title>Universal Self-Consistency for Large Language Model Generation. (arXiv:2311.17311v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17311</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-consistency with chain-of-thought prompting (CoT) has demonstrated
remarkable performance gains on various challenging tasks, by utilizing
multiple reasoning paths sampled from large language models (LLMs). However,
self-consistency relies on the answer extraction process to aggregate multiple
solutions, which is not applicable to free-form answers. In this work, we
propose Universal Self-Consistency (USC), which leverages LLMs themselves to
select the most consistent answer among multiple candidates. We evaluate USC on
a variety of benchmarks, including mathematical reasoning, code generation,
long-context summarization, and open-ended question answering. On open-ended
generation tasks where the original self-consistency method is not applicable,
USC effectively utilizes multiple samples and improves the performance. For
mathematical reasoning, USC matches the standard self-consistency performance
without requiring the answer formats to be similar. Finally, without access to
execution results, USC also matches the execution-based voting performance on
code generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksitov_R/0/1/0/all/0/1&quot;&gt;Renat Aksitov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1&quot;&gt;Kefan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1&quot;&gt;Pengcheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1&quot;&gt;Sushant Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1&quot;&gt;Charles Sutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Denny Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17323">
<title>Accelerating DNN Training With Photonics: A Residue Number System-Based Design. (arXiv:2311.17323v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.17323</link>
<description rdf:parseType="Literal">&lt;p&gt;Photonic computing is a compelling avenue for performing highly efficient
matrix multiplication, a crucial operation in Deep Neural Networks (DNNs).
While this method has shown great success in DNN inference, meeting the high
precision demands of DNN training proves challenging due to the precision
limitations imposed by costly data converters and the analog noise inherent in
photonic hardware. This paper proposes Mirage, a photonic DNN training
accelerator that overcomes the precision challenges in photonic hardware using
the Residue Number System (RNS). RNS is a numeral system based on modular
arithmetic$\unicode{x2014}$allowing us to perform high-precision operations via
multiple low-precision modular operations. In this work, we present a novel
micro-architecture and dataflow for an RNS-based photonic tensor core
performing modular arithmetic in the analog domain. By combining RNS and
photonics, Mirage provides high energy efficiency without compromising
precision and can successfully train state-of-the-art DNNs achieving accuracy
comparable to FP32 training. Our study shows that on average across several
DNNs when compared to systolic arrays, Mirage achieves more than $23.8\times$
faster training and $32.1\times$ lower EDP in an iso-energy scenario and
consumes $42.8\times$ lower power with comparable or better EDP in an iso-area
scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirkiran_C/0/1/0/all/0/1&quot;&gt;Cansu Demirkiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guowei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunandar_D/0/1/0/all/0/1&quot;&gt;Darius Bunandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ajay Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17329">
<title>Cascade: A Platform for Delay-Sensitive Edge Intelligence. (arXiv:2311.17329v1 [cs.OS])</title>
<link>http://arxiv.org/abs/2311.17329</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive intelligent computing applications are increasingly prevalent,
creating a need for AI/ML platforms optimized to reduce per-event latency while
maintaining high throughput and efficient resource management. Yet many
intelligent applications run on AI/ML platforms that optimize for high
throughput even at the cost of high tail-latency. Cascade is a new AI/ML
hosting platform intended to untangle this puzzle. Innovations include a
legacy-friendly storage layer that moves data with minimal copying and a &quot;fast
path&quot; that collocates data and computation to maximize responsiveness. Our
evaluation shows that Cascade reduces latency by orders of magnitude with no
loss of throughput.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Weijia Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrett_T/0/1/0/all/0/1&quot;&gt;Thiago Garrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuting Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingzhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tremel_E/0/1/0/all/0/1&quot;&gt;Edward Tremel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merlina_A/0/1/0/all/0/1&quot;&gt;Andrea Merlina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vitenberg_R/0/1/0/all/0/1&quot;&gt;Roman Vitenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birman_K/0/1/0/all/0/1&quot;&gt;Ken Birman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17338">
<title>VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model. (arXiv:2311.17338v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17338</link>
<description rdf:parseType="Literal">&lt;p&gt;Identity-consistent video generation seeks to synthesize videos that are
guided by both textual prompts and reference images of entities. Current
approaches typically utilize cross-attention layers to integrate the appearance
of the entity, which predominantly captures semantic attributes, resulting in
compromised fidelity of entities. Moreover, these methods necessitate iterative
fine-tuning for each new entity encountered, thereby limiting their
applicability. To address these challenges, we introduce VideoAssembler, a
novel end-to-end framework for identity-consistent video generation that can
conduct inference directly when encountering new entities. VideoAssembler is
adept at producing videos that are not only flexible with respect to the input
reference entities but also responsive to textual conditions. Additionally, by
modulating the quantity of input images for the entity, VideoAssembler enables
the execution of tasks ranging from image-to-video generation to sophisticated
video editing. VideoAssembler comprises two principal components: the Reference
Entity Pyramid (REP) encoder and the Entity-Prompt Attention Fusion (EPAF)
module. The REP encoder is designed to infuse comprehensive appearance details
into the denoising stages of the stable diffusion model. Concurrently, the EPAF
module is utilized to integrate text-aligned features effectively. Furthermore,
to mitigate the challenge of scarce data, we present a methodology for the
preprocessing of training data. Our evaluation of the VideoAssembler framework
on the UCF-101, MSR-VTT, and DAVIS datasets indicates that it achieves good
performances in both quantitative and qualitative analyses (346.84 in FVD and
48.01 in IS on UCF-101). Our project page is at
https://videoassembler.github.io/videoassembler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tianyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17351">
<title>Exploring Large Language Models for Human Mobility Prediction under Public Events. (arXiv:2311.17351v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17351</link>
<description rdf:parseType="Literal">&lt;p&gt;Public events, such as concerts and sports games, can be major attractors for
large crowds, leading to irregular surges in travel demand. Accurate human
mobility prediction for public events is thus crucial for event planning as
well as traffic or crowd management. While rich textual descriptions about
public events are commonly available from online sources, it is challenging to
encode such information in statistical or machine learning models. Existing
methods are generally limited in incorporating textual information, handling
data sparsity, or providing rationales for their predictions. To address these
challenges, we introduce a framework for human mobility prediction under public
events (LLM-MPE) based on Large Language Models (LLMs), leveraging their
unprecedented ability to process textual data, learn from minimal examples, and
generate human-readable explanations. Specifically, LLM-MPE first transforms
raw, unstructured event descriptions from online sources into a standardized
format, and then segments historical mobility data into regular and
event-related components. A prompting strategy is designed to direct LLMs in
making and rationalizing demand predictions considering historical mobility and
event features. A case study is conducted for Barclays Center in New York City,
based on publicly available event information and taxi trip data. Results show
that LLM-MPE surpasses traditional models, particularly on event days, with
textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers
interpretable insights into its predictions. Despite the great potential of
LLMs, we also identify key challenges including misinformation and high costs
that remain barriers to their broader adoption in large-scale human mobility
analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuebing Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yichao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhan Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17368">
<title>Two Scalable Approaches for Burned-Area Mapping Using U-Net and Landsat Imagery. (arXiv:2311.17368v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17368</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring wildfires is an essential step in minimizing their impact on the
planet, understanding the many negative environmental, economic, and social
consequences. Recent advances in remote sensing technology combined with the
increasing application of artificial intelligence methods have improved
real-time, high-resolution fire monitoring. This study explores two proposed
approaches based on the U-Net model for automating and optimizing the
burned-area mapping process. Denoted 128 and AllSizes (AS), they are trained on
datasets with a different class balance by cropping input images to different
sizes. They are then applied to Landsat imagery and time-series data from two
fire-prone regions in Chile. The results obtained after enhancement of model
performance by hyperparameter optimization demonstrate the effectiveness of
both approaches. Tests based on 195 representative images of the study area
show that increasing dataset balance using the AS model yields better
performance. More specifically, AS exhibited a Dice Coefficient (DC) of 0.93,
an Omission Error (OE) of 0.086, and a Commission Error (CE) of 0.045, while
the 128 model achieved a DC of 0.86, an OE of 0.12, and a CE of 0.12. These
findings should provide a basis for further development of scalable automatic
burned-area mapping tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancilla_Wulff_I/0/1/0/all/0/1&quot;&gt;Ian Mancilla-Wulff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrasco_J/0/1/0/all/0/1&quot;&gt;Jaime Carrasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pais_C/0/1/0/all/0/1&quot;&gt;Cristobal Pais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miranda_A/0/1/0/all/0/1&quot;&gt;Alejandro Miranda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weintraub_A/0/1/0/all/0/1&quot;&gt;Andres Weintraub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17371">
<title>Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&amp;A. (arXiv:2311.17371v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17371</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) underscore their
potential for responding to medical inquiries. However, ensuring that
generative agents provide accurate and reliable answers remains an ongoing
challenge. In this context, multi-agent debate (MAD) has emerged as a prominent
strategy for enhancing the truthfulness of LLMs. In this work, we provide a
comprehensive benchmark of MAD strategies for medical Q&amp;amp;A, along with
open-source implementations. This explores the effective utilization of various
strategies including the trade-offs between cost, time, and accuracy. We build
upon these insights to provide a novel debate-prompting strategy based on agent
agreement that outperforms previously published strategies on medical Q&amp;amp;A
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smit_A/0/1/0/all/0/1&quot;&gt;Andries Smit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duckworth_P/0/1/0/all/0/1&quot;&gt;Paul Duckworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1&quot;&gt;Nathan Grinsztajn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1&quot;&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1&quot;&gt;Thomas D. Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17393">
<title>Comparison of metaheuristics for the firebreak placement problem: a simulation-based optimization approach. (arXiv:2311.17393v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17393</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of firebreak placement is crucial for fire prevention, and its
effectiveness at landscape scale will depend on their ability to impede the
progress of future wildfires. To provide an adequate response, it is therefore
necessary to consider the stochastic nature of fires, which are highly
unpredictable from ignition to extinction. Thus, the placement of firebreaks
can be considered a stochastic optimization problem where: (1) the objective
function is to minimize the expected cells burnt of the landscape; (2) the
decision variables being the location of firebreaks; and (3) the random
variable being the spatial propagation/behavior of fires. In this paper, we
propose a solution approach for the problem from the perspective of
simulation-based optimization (SbO), where the objective function is not
available (a black-box function), but can be computed (and/or approximated) by
wildfire simulations. For this purpose, Genetic Algorithm and GRASP are
implemented. The final implementation yielded favorable results for the Genetic
Algorithm, demonstrating strong performance in scenarios with medium to high
operational capacity, as well as medium levels of stochasticity
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palacios_Meneses_D/0/1/0/all/0/1&quot;&gt;David Palacios-Meneses&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrasco_J/0/1/0/all/0/1&quot;&gt;Jaime Carrasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davila_S/0/1/0/all/0/1&quot;&gt;Sebasti&amp;#xe1;n D&amp;#xe1;vila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Maximiliano Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahaluf_R/0/1/0/all/0/1&quot;&gt;Rodrigo Mahaluf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weintraub_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Weintraub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17401">
<title>Gene-MOE: A Sparsely-gated Framework for Pan-Cancer Genomic Analysis. (arXiv:2311.17401v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17401</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing the genomic information from the Pan-Cancer database can help us
understand cancer-related factors and contribute to the cancer diagnosis and
prognosis. However, existing computational methods and deep learning methods
can not effectively find the deep correlations between tens of thousands of
genes, which leads to precision loss. In this paper, we proposed a novel
pretrained model called Gene-MOE to learn the general feature representations
of the Pan-Cancer dataset and transfer the pretrained weights to the downstream
tasks. The Gene-MOE fully exploits the mixture of expert (MOE) layers to learn
rich feature representations of high-dimensional genes. At the same time, we
build a mixture of attention expert (MOAE) model to learn the deep semantic
relationships within genetic features. Finally, we proposed a new
self-supervised pretraining strategy including loss function design, data
enhancement, and optimization strategy to train the Gene-MOE and further
improve the performance for the downstream analysis. We carried out cancer
classification and survival analysis experiments based on the Gene-MOE.
According to the survival analysis results on 14 cancer types, using Gene-MOE
outperformed state-of-the-art models on 12 cancer types. According to the
classification results, the total accuracy of the classification model for 33
cancer classifications reached 95.2\%. Through detailed feature analysis, we
found the Gene-MOE model can learn rich feature representations of
high-dimensional genes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangyu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Huanhuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Lian Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongzhen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1&quot;&gt;Long Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17404">
<title>VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models. (arXiv:2311.17404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17404</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to perceive how objects change over time is a crucial ingredient
in human intelligence. However, current benchmarks cannot faithfully reflect
the temporal understanding abilities of video-language models (VidLMs) due to
the existence of static visual shortcuts. To remedy this issue, we present
VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal
Concept underStanding. Specifically, we first introduce a fine-grained taxonomy
of temporal concepts in natural language in order to diagnose the capability of
VidLMs to comprehend different temporal aspects. Furthermore, to disentangle
the correlation between static and temporal information, we generate
counterfactual video descriptions that differ from the original one only in the
specified temporal aspect. We employ a semi-automatic data collection framework
using large language models and human-in-the-loop annotation to obtain
high-quality counterfactual descriptions efficiently. Evaluation of
representative video-language understanding models confirms their deficiency in
temporal understanding, revealing the need for greater emphasis on the temporal
elements in video-language research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rundong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lu Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17406">
<title>LLM-State: Expandable State Representation for Long-horizon Task Planning in the Open World. (arXiv:2311.17406v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.17406</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses the problem of long-horizon task planning with the Large
Language Model (LLM) in an open-world household environment. Existing works
fail to explicitly track key objects and attributes, leading to erroneous
decisions in long-horizon tasks, or rely on highly engineered state features
and feedback, which is not generalizable. We propose a novel, expandable state
representation that provides continuous expansion and updating of object
attributes from the LLM&apos;s inherent capabilities for context understanding and
historical action reasoning. Our proposed representation maintains a
comprehensive record of an object&apos;s attributes and changes, enabling robust
retrospective summary of the sequence of actions leading to the current state.
This allows enhanced context understanding for decision-making in task
planning. We validate our model through experiments across simulated and
real-world task planning scenarios, demonstrating significant improvements over
baseline methods in a variety of tasks requiring long-horizon state tracking
and reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1&quot;&gt;Anxing Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17428">
<title>SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation. (arXiv:2311.17428v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17428</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal human action segmentation is a critical and challenging task with
a wide range of applications. Nowadays, the majority of approaches concentrate
on the fusion of dense signals (i.e., RGB, optical flow, and depth maps).
However, the potential contributions of sparse IoT sensor signals, which can be
crucial for achieving accurate recognition, have not been fully explored. To
make up for this, we introduce a Sparse signalguided Transformer (SigFormer) to
combine both dense and sparse signals. We employ mask attention to fuse
localized features by constraining cross-attention within the regions where
sparse signals are valid. However, since sparse signals are discrete, they lack
sufficient information about the temporal action boundaries. Therefore, in
SigFormer, we propose to emphasize the boundary information at two stages to
alleviate this problem. In the first feature extraction stage, we introduce an
intermediate bottleneck module to jointly learn both category and boundary
features of each dense modality through the inner loss functions. After the
fusion of dense modalities and sparse signals, we then devise a two-branch
architecture that explicitly models the interrelationship between action
category and temporal boundary. Experimental results demonstrate that SigFormer
outperforms the state-of-the-art approaches on a multi-modal action
segmentation dataset from real industrial environments, reaching an outstanding
F1 score of 0.958. The codes and pre-trained models have been available at
https://github.com/LIUQI-creat/SigFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17429">
<title>TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4. (arXiv:2311.17429v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17429</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has been widely applied in many low-resource NLP tasks
such as few-shot scenarios. However, this paradigm has been shown to be
vulnerable to backdoor attacks. Most of the existing attack methods focus on
inserting manually predefined templates as triggers in the pre-training phase
to train the victim model and utilize the same triggers in the downstream task
to perform inference, which tends to ignore the transferability and
stealthiness of the templates. In this work, we propose a novel approach of
TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models
via GPT4), which is a data-independent attack method. Specifically, we first
utilize GPT4 to reformulate manual templates to generate tone-strong and normal
templates, and the former are injected into the model as a backdoor trigger in
the pre-training phase. Then, we not only directly employ the above templates
in the downstream task, but also use GPT4 to generate templates with similar
tone to the above templates to carry out transferable attacks. Finally we have
conducted extensive experiments on five NLP datasets and three BERT series
models, with experimental results justifying that our TARGET method has better
attack performance and stealthiness compared to the two-external baseline
methods on direct attacks, and in addition achieves satisfactory attack
capability in the unseen tone-similar templates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zihao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17431">
<title>Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanlin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17435">
<title>MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning. (arXiv:2311.17435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17435</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MM-Narrator, a novel system leveraging GPT-4 with multimodal
in-context learning for the generation of audio descriptions (AD). Unlike
previous methods that primarily focused on downstream fine-tuning with short
video clips, MM-Narrator excels in generating precise audio descriptions for
videos of extensive lengths, even beyond hours, in an autoregressive manner.
This capability is made possible by the proposed memory-augmented generation
process, which effectively utilizes both the short-term textual context and
long-term visual memory through an efficient register-and-recall mechanism.
These contextual memories compile pertinent past information, including
storylines and character identities, ensuring an accurate tracking and
depicting of story-coherent and character-centric audio descriptions.
Maintaining the training-free design of MM-Narrator, we further propose a
complexity-based demonstration selection strategy to largely enhance its
multi-step reasoning capability via few-shot multimodal in-context learning
(MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator
consistently outperforms both the existing fine-tuning-based approaches and
LLM-based approaches in most scenarios, as measured by standard evaluation
metrics. Additionally, we introduce the first segment-based evaluator for
recurrent text generation. Empowered by GPT-4, this evaluator comprehensively
reasons and marks AD generation performance in various extendable dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chung-Ching Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17438">
<title>CLOMO: Counterfactual Logical Modification with Large Language Models. (arXiv:2311.17438v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17438</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we delve into the realm of counterfactual reasoning
capabilities of large language models (LLMs). Our primary objective is to
cultivate the counterfactual thought processes within LLMs and rigorously
assess these processes for their validity. Specifically, we introduce a novel
task, Counterfactual Logical Modification (CLOMO), and a high-quality
human-annotated benchmark. In this task, LLMs must adeptly alter a given
argumentative text to uphold a predetermined logical relationship. To
effectively evaluate a generation model&apos;s counterfactual capabilities, we
propose an innovative evaluation metric, the LogicAware Counterfactual Score to
directly evaluate the natural language output of LLMs instead of modeling the
task as a multiple-choice problem. Analysis shows that the proposed automatic
metric aligns well with human preference. Our experimental results show that
while LLMs demonstrate a notable capacity for logical counterfactual thinking,
there remains a discernible gap between their current abilities and human
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yinya Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Ruixin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linqi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17446">
<title>Uncertainty in Additive Feature Attribution methods. (arXiv:2311.17446v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17446</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we explore various topics that fall under the umbrella of
Uncertainty in post-hoc Explainable AI (XAI) methods. We in particular focus on
the class of additive feature attribution explanation methods. We first
describe our specifications of uncertainty and compare various statistical and
recent methods to quantify the same. Next, for a particular instance, we study
the relationship between a feature&apos;s attribution and its uncertainty and
observe little correlation. As a result, we propose a modification in the
distribution from which perturbations are sampled in LIME-based algorithms such
that the important features have minimal uncertainty without an increase in
computational cost. Next, while studying how the uncertainty in explanations
varies across the feature space of a classifier, we observe that a fraction of
instances show near-zero uncertainty. We coin the term &quot;stable instances&quot; for
such instances and diagnose factors that make an instance stable. Next, we
study how an XAI algorithm&apos;s uncertainty varies with the size and complexity of
the underlying model. We observe that the more complex the model, the more
inherent uncertainty is exhibited by it. As a result, we propose a measure to
quantify the relative complexity of a blackbox classifier. This could be
incorporated, for example, in LIME-based algorithms&apos; sampling densities, to
help different explanation algorithms achieve tighter confidence levels.
Together, the above measures would have a strong impact on making XAI models
relatively trustworthy for the end-user as well as aiding scientific discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1&quot;&gt;Abhishek Madaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1&quot;&gt;Tanya Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_N/0/1/0/all/0/1&quot;&gt;Neha Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1&quot;&gt;James Allan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17447">
<title>Learning-driven Zero Trust in Distributed Computing Continuum Systems. (arXiv:2311.17447v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.17447</link>
<description rdf:parseType="Literal">&lt;p&gt;Converging Zero Trust (ZT) with learning techniques can solve various
operational and security challenges in Distributed Computing Continuum Systems
(DCCS). Implementing centralized ZT architecture is seen as unsuitable for the
computing continuum (e.g., computing entities with limited connectivity and
visibility, etc.). At the same time, implementing decentralized ZT in the
computing continuum requires understanding infrastructure limitations and novel
approaches to enhance resource access management decisions. To overcome such
challenges, we present a novel learning-driven ZT conceptual architecture
designed for DCCS. We aim to enhance ZT architecture service quality by
incorporating lightweight learning strategies such as Representation Learning
(ReL) and distributing ZT components across the computing continuum. The ReL
helps to improve the decision-making process by predicting threats or untrusted
requests. Through an illustrative example, we show how the learning process
detects and blocks the requests, enhances resource access control, and reduces
network and computation overheads. Lastly, we discuss the conceptual
architecture, processes, and provide a research agenda.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murturi_I/0/1/0/all/0/1&quot;&gt;Ilir Murturi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1&quot;&gt;Praveen Kumar Donta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujol_V/0/1/0/all/0/1&quot;&gt;Victor Casamayor Pujol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morichetta_A/0/1/0/all/0/1&quot;&gt;Andrea Morichetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1&quot;&gt;Schahram Dustdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17453">
<title>Privacy Measurement in Tabular Synthetic Data: State of the Art and Future Research Directions. (arXiv:2311.17453v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17453</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data (SD) have garnered attention as a privacy enhancing
technology. Unfortunately, there is no standard for quantifying their degree of
privacy protection. In this paper, we discuss proposed quantification
approaches. This contributes to the development of SD privacy standards;
stimulates multi-disciplinary discussion; and helps SD researchers make
informed modeling and evaluation decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boudewijn_A/0/1/0/all/0/1&quot;&gt;Alexander Boudewijn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraris_A/0/1/0/all/0/1&quot;&gt;Andrea Filippo Ferraris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panfilo_D/0/1/0/all/0/1&quot;&gt;Daniele Panfilo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cocca_V/0/1/0/all/0/1&quot;&gt;Vanessa Cocca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zinutti_S/0/1/0/all/0/1&quot;&gt;Sabrina Zinutti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schepper_K/0/1/0/all/0/1&quot;&gt;Karel De Schepper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauvenet_C/0/1/0/all/0/1&quot;&gt;Carlo Rossi Chauvenet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17458">
<title>Quantum Neural Networks under Depolarization Noise: Exploring White-Box Attacks and Defenses. (arXiv:2311.17458v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.17458</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging the unique properties of quantum mechanics, Quantum Machine
Learning (QML) promises computational breakthroughs and enriched perspectives
where traditional systems reach their boundaries. However, similarly to
classical machine learning, QML is not immune to adversarial attacks. Quantum
adversarial machine learning has become instrumental in highlighting the weak
points of QML models when faced with adversarial crafted feature vectors.
Diving deep into this domain, our exploration shines light on the interplay
between depolarization noise and adversarial robustness. While previous results
enhanced robustness from adversarial threats through depolarization noise, our
findings paint a different picture. Interestingly, adding depolarization noise
discontinued the effect of providing further robustness for a multi-class
classification scenario. Consolidating our findings, we conducted experiments
with a multi-class classifier adversarially trained on gate-based quantum
simulators, further elucidating this unexpected behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Winderl_D/0/1/0/all/0/1&quot;&gt;David Winderl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Franco_N/0/1/0/all/0/1&quot;&gt;Nicola Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lorenz_J/0/1/0/all/0/1&quot;&gt;Jeanette Miriam Lorenz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17466">
<title>Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification. (arXiv:2311.17466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17466</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole slide image (WSI) classification requires repetitive zoom-in and out
for pathologists, as only small portions of the slide may be relevant to
detecting cancer. Due to the lack of patch-level labels, multiple instance
learning (MIL) is a common practice for training a WSI classifier. One of the
challenges in MIL for WSIs is the weak supervision coming only from the
slide-level labels, often resulting in severe overfitting. In response,
researchers have considered adopting patch-level augmentation or applying mixup
augmentation, but their applicability remains unverified. Our approach augments
the training dataset by sampling a subset of patches in the WSI without
significantly altering the underlying semantics of the original slides.
Additionally, we introduce an efficient model (Slot-MIL) that organizes patches
into a fixed number of slots, the abstract representation of patches, using an
attention mechanism. We empirically demonstrate that the subsampling
augmentation helps to make more informative slots by restricting the
over-concentration of attention and to improve interpretability. Finally, we
illustrate that combining our attention-based aggregation model with
subsampling and mixup, which has shown limited compatibility in existing MIL
methods, can enhance both generalization and calibration. Our proposed methods
achieve the state-of-the-art performance across various benchmark datasets
including class imbalance and distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keum_S/0/1/0/all/0/1&quot;&gt;Seongho Keum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soojeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17471">
<title>Distributed AI in Zero-touch Provisioning for Edge Networks: Challenges and Research Directions. (arXiv:2311.17471v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17471</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-touch network is anticipated to inaugurate the generation of intelligent
and highly flexible resource provisioning strategies where multiple service
providers collaboratively offer computation and storage resources. This
transformation presents substantial challenges to network administration and
service providers regarding sustainability and scalability. This article
combines Distributed Artificial Intelligence (DAI) with Zero-touch Provisioning
(ZTP) for edge networks. This combination helps to manage network devices
seamlessly and intelligently by minimizing human intervention. In addition,
several advantages are also highlighted that come with incorporating
Distributed AI into ZTP in the context of edge networks. Further, we draw
potential research directions to foster novel studies in this field and
overcome the current limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazra_A/0/1/0/all/0/1&quot;&gt;Abhishek Hazra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morichetta_A/0/1/0/all/0/1&quot;&gt;Andrea Morichetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murturi_I/0/1/0/all/0/1&quot;&gt;Ilir Murturi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loven_L/0/1/0/all/0/1&quot;&gt;Lauri Lov&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehury_C/0/1/0/all/0/1&quot;&gt;Chinmaya Kumar Dehury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujol_V/0/1/0/all/0/1&quot;&gt;Victor Casamayor Pujol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donta_P/0/1/0/all/0/1&quot;&gt;Praveen Kumar Donta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dustdar_S/0/1/0/all/0/1&quot;&gt;Schahram Dustdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17487">
<title>Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model. (arXiv:2311.17487v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17487</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of language models, the nuanced linguistic and cultural
intricacies of Traditional Chinese, as spoken in Taiwan, have been largely
overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model
that specifically caters to the Traditional Chinese language, with a focus on
the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and
instruction-finetuning datasets, we have developed a model that not only
understands the complexities of Traditional Chinese but also embodies the
cultural context of Taiwan. Taiwan LLM represents the first of its kind, a
model that is not only linguistically accurate but also culturally resonant
with its user base. Our evaluations demonstrate that Taiwan LLM achieves
superior performance in understanding and generating Traditional Chinese text,
outperforming existing models that are predominantly trained on Simplified
Chinese or English. The open-source release of Taiwan LLM invites collaboration
and further innovation, ensuring that the linguistic diversity of Chinese
speakers is embraced and well-served. The model, datasets, and further
resources are made publicly available to foster ongoing research and
development in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen-Ting Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun-Nung Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17514">
<title>Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning. (arXiv:2311.17514v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17514</link>
<description rdf:parseType="Literal">&lt;p&gt;Query-focused Summarization (QfS) deals with systems that generate summaries
from document(s) based on a query. Motivated by the insight that Reinforcement
Learning (RL) provides a generalization to Supervised Learning (SL) for Natural
Language Generation, and thereby performs better (empirically) than SL, we use
an RL-based approach for this task of QfS. Additionally, we also resolve the
conflict of employing RL in Transformers with Teacher Forcing. We develop
multiple Policy Gradient networks, trained on various reward signals: ROUGE,
BLEU, and Semantic Similarity, which lead to a 10-point improvement over the
State-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5).
We also show performance of our approach in zero-shot setting for another
benchmark dataset (DebatePedia) -- our approach leads to results comparable to
baselines, which were specifically trained on DebatePedia. To aid the RL
training, we propose a better semantic similarity reward, enabled by a novel
Passage Embedding scheme developed using Cluster Hypothesis. Lastly, we
contribute a gold-standard test dataset to further research in QfS and
Long-form Question Answering (LfQA).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1&quot;&gt;Swaroop Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1&quot;&gt;Harshad Khadilkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1&quot;&gt;Pushpak Bhattacharyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17518">
<title>The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding. (arXiv:2311.17518v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17518</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large vision-language models enabled visual object
detection in open-vocabulary scenarios, where object classes are defined in
free-text formats during inference. In this paper, we aim to probe the
state-of-the-art methods for open-vocabulary object detection to determine to
what extent they understand fine-grained properties of objects and their parts.
To this end, we introduce an evaluation protocol based on dynamic vocabulary
generation to test whether models detect, discern, and assign the correct
fine-grained description to objects in the presence of hard-negative classes.
We contribute with a benchmark suite of increasing difficulty and probing
different properties like color, pattern, and material. We further enhance our
investigation by evaluating several state-of-the-art open-vocabulary object
detectors using the proposed protocol and find that most existing solutions,
which shine in standard open-vocabulary benchmarks, struggle to accurately
capture and distinguish finer object details. We conclude the paper by
highlighting the limitations of current methodologies and exploring promising
research directions to overcome the discovered drawbacks. Data and code are
available at https://github.com/lorebianchi98/FG-OVD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrara_F/0/1/0/all/0/1&quot;&gt;Fabio Carrara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1&quot;&gt;Nicola Messina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1&quot;&gt;Claudio Gennaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Falchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17521">
<title>Spinal Muscle Atrophy Disease Modelling as Bayesian Network. (arXiv:2311.17521v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2311.17521</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the molecular gene expressions studies and public databases
for disease modelling using Probabilistic Graphical Models and Bayesian
Inference. A case study on Spinal Muscle Atrophy Genome-Wide Association Study
results is modelled and analyzed. The genes up and down-regulated in two stages
of the disease development are linked to prior knowledge published in the
public domain and co-expressions network is created and analyzed. The Molecular
Pathways triggered by these genes are identified. The Bayesian inference
posteriors distributions are estimated using a variational analytical algorithm
and a Markov chain Monte Carlo sampling algorithm. Assumptions, limitations and
possible future work are concluded.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Helal_M/0/1/0/all/0/1&quot;&gt;Mohammed Ezzat Helal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Helal_M/0/1/0/all/0/1&quot;&gt;Manal Ezzat Helal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fahmy_S/0/1/0/all/0/1&quot;&gt;Sherif Fadel Fahmy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17541">
<title>TaskWeaver: A Code-First Agent Framework. (arXiv:2311.17541v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17541</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Language Models (LLMs) have shown impressive abilities in natural
language understanding and generation, leading to their use in applications
such as chatbots and virtual assistants. However, existing LLM frameworks face
limitations in handling domain-specific data analytics tasks with rich data
structures. Moreover, they struggle with flexibility to meet diverse user
requirements. To address these issues, TaskWeaver is proposed as a code-first
framework for building LLM-powered autonomous agents. It converts user requests
into executable code and treats user-defined plugins as callable functions.
TaskWeaver provides support for rich data structures, flexible plugin usage,
and dynamic plugin selection, and leverages LLM coding capabilities for complex
logic. It also incorporates domain-specific knowledge through examples and
ensures the secure execution of generated code. TaskWeaver offers a powerful
and flexible framework for creating intelligent conversational agents that can
handle complex tasks and adapt to domain-specific scenarios. The code is
open-sourced at https://github.com/microsoft/TaskWeaver/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_B/0/1/0/all/0/1&quot;&gt;Bo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liqun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shilin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fangkai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Minghua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Pu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Si Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qingwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1&quot;&gt;Saravan Rajmohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17565">
<title>Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning. (arXiv:2311.17565v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17565</link>
<description rdf:parseType="Literal">&lt;p&gt;In goal-conditioned reinforcement learning (GCRL), sparse rewards present
significant challenges, often obstructing efficient learning. Although
multi-step GCRL can boost this efficiency, it can also lead to off-policy
biases in target values. This paper dives deep into these biases, categorizing
them into two distinct categories: &quot;shooting&quot; and &quot;shifting&quot;. Recognizing that
certain behavior policies can hasten policy refinement, we present solutions
designed to capitalize on the positive aspects of these biases while minimizing
their drawbacks, enabling the use of larger step sizes to speed up GCRL. An
empirical study demonstrates that our approach ensures a resilient and robust
improvement, even in ten-step learning scenarios, leading to superior learning
efficiency and performance that generally surpass the baseline and several
state-of-the-art multi-step GCRL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lisheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17593">
<title>LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17593</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep reinforcement learning have showcased its potential
in tackling complex tasks. However, experiments on visual control tasks have
revealed that state-of-the-art reinforcement learning models struggle with
out-of-distribution generalization. Conversely, expressing higher-level
concepts and global contexts is relatively easy using language.
&lt;/p&gt;
&lt;p&gt;Building upon recent success of the large language models, our main objective
is to improve the state abstraction technique in reinforcement learning by
leveraging language for robust action selection. Specifically, we focus on
learning language-grounded visual features to enhance the world model learning,
a model-based reinforcement learning technique.
&lt;/p&gt;
&lt;p&gt;To enforce our hypothesis explicitly, we mask out the bounding boxes of a few
objects in the image observation and provide the text prompt as descriptions
for these masked objects. Subsequently, we predict the masked objects along
with the surrounding regions as pixel reconstruction, similar to the
transformer-based masked autoencoder approach.
&lt;/p&gt;
&lt;p&gt;Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art
performance in out-of-distribution test at the 100K interaction steps
benchmarks of iGibson point navigation tasks. Furthermore, our proposed
technique of explicit language-grounded visual representation learning has the
potential to improve models for human-robot interaction because our extracted
visual features are language grounded.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poudel_R/0/1/0/all/0/1&quot;&gt;Rudra P.K. Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandya_H/0/1/0/all/0/1&quot;&gt;Harit Pandya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1&quot;&gt;Roberto Cipolla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17598">
<title>Improving embedding of graphs with missing data by soft manifolds. (arXiv:2311.17598v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17598</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding graphs in continous spaces is a key factor in designing and
developing algorithms for automatic information extraction to be applied in
diverse tasks (e.g., learning, inferring, predicting). The reliability of graph
embeddings directly depends on how much the geometry of the continuous space
matches the graph structure. Manifolds are mathematical structure that can
enable to incorporate in their topological spaces the graph characteristics,
and in particular nodes distances. State-of-the-art of manifold-based graph
embedding algorithms take advantage of the assumption that the projection on a
tangential space of each point in the manifold (corresponding to a node in the
graph) would locally resemble a Euclidean space. Although this condition helps
in achieving efficient analytical solutions to the embedding problem, it does
not represent an adequate set-up to work with modern real life graphs, that are
characterized by weighted connections across nodes often computed over sparse
datasets with missing records. In this work, we introduce a new class of
manifold, named soft manifold, that can solve this situation. In particular,
soft manifolds are mathematical structures with spherical symmetry where the
tangent spaces to each point are hypocycloids whose shape is defined according
to the velocity of information propagation across the data points. Using soft
manifolds for graph embedding, we can provide continuous spaces to pursue any
task in data analysis over complex datasets. Experimental results on
reconstruction tasks on synthetic and real datasets show how the proposed
approach enable more accurate and reliable characterization of graphs in
continuous spaces with respect to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinoni_A/0/1/0/all/0/1&quot;&gt;Andrea Marinoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Lio&amp;#x27;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barp_A/0/1/0/all/0/1&quot;&gt;Alessandro Barp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jutten_C/0/1/0/all/0/1&quot;&gt;Christian Jutten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17601">
<title>Continual Learning with Low Rank Adaptation. (arXiv:2311.17601v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17601</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work using pretrained transformers has shown impressive performance
when fine-tuned with data from the downstream problem of interest. However,
they struggle to retain that performance when the data characteristics changes.
In this paper, we focus on continual learning, where a pre-trained transformer
is updated to perform well on new data, while retaining its performance on data
it was previously trained on. Earlier works have tackled this primarily through
methods inspired from prompt tuning. We question this choice, and investigate
the applicability of Low Rank Adaptation (LoRA) to continual learning. On a
range of domain-incremental learning benchmarks, our LoRA-based solution,
CoLoR, yields state-of-the-art performance, while still being as parameter
efficient as the prompt tuning based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1&quot;&gt;Martin Wistuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivaprasad_P/0/1/0/all/0/1&quot;&gt;Prabhu Teja Sivaprasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balles_L/0/1/0/all/0/1&quot;&gt;Lukas Balles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1&quot;&gt;Giovanni Zappella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17633">
<title>Introduction to Transformers: an NLP Perspective. (arXiv:2311.17633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17633</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have dominated empirical machine learning models of natural
language processing. In this paper, we introduce basic concepts of Transformers
and present key techniques that form the recent advances of these models. This
includes a description of the standard Transformer architecture, a series of
model refinements, and common applications. Given that Transformers and related
deep learning techniques might be evolving in ways we have never seen, we
cannot dive into all the model details or cover all the technical areas.
Instead, we focus on just those concepts that are helpful for gaining a good
understanding of Transformers and their variants. We also summarize the key
ideas that impact this field, thereby yielding some insights into the strengths
and limitations of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17647">
<title>VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following. (arXiv:2311.17647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17647</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to
evaluate the visual instruction following capability of Multimodal Large
Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs
by embedding the instructions into the visual scenes, demanding strong visual
interpretative skills for instruction following. We adapt VIM to various
benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM
bench, and probe diverse MLLMs across three distinct in-context learning
settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a
significant performance disparity between the open-source MLLMs and GPT-4V,
implying that their proficiency in visual instruction comprehension is not up
to par. Our results highlight a promising direction for the enhancement of
MLLMs capabilities on instruction following. We aim VIM to serve as a useful
norm for advancing the state of the art and driving further progress in the
field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17655">
<title>Vulnerability of Automatic Identity Recognition to Audio-Visual Deepfakes. (arXiv:2311.17655v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17655</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of deepfakes detection is far from being solved by speech or vision
researchers. Several publicly available databases of fake synthetic video and
speech were built to aid the development of detection methods. However,
existing databases typically focus on visual or voice modalities and provide no
proof that their deepfakes can in fact impersonate any real person. In this
paper, we present the first realistic audio-visual database of deepfakes
SWAN-DF, where lips and speech are well synchronized and video have high visual
and audio qualities. We took the publicly available SWAN dataset of real videos
with different identities to create audio-visual deepfakes using several models
from DeepFaceLab and blending techniques for face swapping and HiFiVC, DiffVC,
YourTTS, and FreeVC models for voice conversion. From the publicly available
speech dataset LibriTTS, we also created a separate database of only audio
deepfakes LibriTTS-DF using several latest text to speech methods: YourTTS,
Adaspeech, and TorToiSe. We demonstrate the vulnerability of a state of the art
speaker recognition system, such as ECAPA-TDNN-based model from SpeechBrain, to
the synthetic voices. Similarly, we tested face recognition system based on the
MobileFaceNet architecture to several variants of our visual deepfakes. The
vulnerability assessment show that by tuning the existing pretrained deepfake
models to specific identities, one can successfully spoof the face and speaker
recognition systems in more than 90% of the time and achieve a very realistic
looking and sounding fake video of a given person.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korshunov_P/0/1/0/all/0/1&quot;&gt;Pavel Korshunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haolin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garner_P/0/1/0/all/0/1&quot;&gt;Philip N. Garner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1&quot;&gt;Sebastien Marcel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17667">
<title>TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models. (arXiv:2311.17667v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17667</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding time is a pivotal aspect of human cognition, crucial in the
broader framework of grasping the intricacies of the world. Previous studies
typically focus on specific aspects of time, lacking a comprehensive temporal
reasoning benchmark. To address this issue, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena, which provides a thorough evaluation
for investigating the temporal reasoning capabilities of large language models.
We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and
Mistral, incorporating chain-of-thought prompting. Our experimental results
indicate a significant performance gap between the state-of-the-art LLMs and
humans, highlighting that there is still a considerable distance to cover in
temporal reasoning. We aspire for TimeBench to serve as a comprehensive
benchmark, fostering research in temporal reasoning for LLMs. Our resource is
available at https://github.com/zchuz/TimeBench
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zheng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingchang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qianglong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weijiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haotian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17673">
<title>Using Ornstein-Uhlenbeck Process to understand Denoising Diffusion Probabilistic Model and its Noise Schedules. (arXiv:2311.17673v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2311.17673</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this short note is to show that Denoising Diffusion Probabilistic
Model DDPM, a non-homogeneous discrete-time Markov process, can be represented
by a time-homogeneous continuous-time Markov process observed at non-uniformly
sampled discrete times. Surprisingly, this continuous-time Markov process is
the well-known and well-studied Ornstein-Ohlenbeck (OU) process, which was
developed in 1930&apos;s for studying Brownian particles in Harmonic potentials. We
establish the formal equivalence between DDPM and the OU process using its
analytical solution. We further demonstrate that the design problem of the
noise scheduler for non-homogeneous DDPM is equivalent to designing observation
times for the OU process. We present several heuristic designs for observation
times based on principled quantities such as auto-variance and Fisher
Information and connect them to ad hoc noise schedules for DDPM. Interestingly,
we show that the Fisher-Information-motivated schedule corresponds exactly the
cosine schedule, which was developed without any theoretical foundation but is
the current state-of-the-art noise schedule.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Javier E. Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen Ting Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17676">
<title>Improving Minority Stress Detection with Emotions. (arXiv:2311.17676v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17676</link>
<description rdf:parseType="Literal">&lt;p&gt;Psychological stress detection is an important task for mental healthcare
research, but there has been little prior work investigating the effectiveness
of psychological stress models on minority individuals, who are especially
vulnerable to poor mental health outcomes. In this work, we use the related
task of minority stress detection to evaluate the ability of psychological
stress models to understand the language of sexual and gender minorities. We
find that traditional psychological stress models underperform on minority
stress detection, and we propose using emotion-infused models to reduce that
performance disparity. We further demonstrate that multi-task psychological
stress models outperform the current state-of-the-art for minority stress
detection without directly training on minority stress data. We provide
explanatory analysis showing that minority communities have different
distributions of emotions than the general population and that emotion-infused
models improve the performance of stress models on underrepresented groups
because of their effectiveness in low-data environments, and we propose that
integrating emotions may benefit underrepresented groups in other mental health
detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivey_J/0/1/0/all/0/1&quot;&gt;Jonathan Ivey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_S/0/1/0/all/0/1&quot;&gt;Susan Gauch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17686">
<title>AviationGPT: A Large Language Model for the Aviation Domain. (arXiv:2311.17686v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17686</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of ChatGPT and GPT-4 has captivated the world with large language
models (LLMs), demonstrating exceptional performance in question-answering,
summarization, and content generation. The aviation industry is characterized
by an abundance of complex, unstructured text data, replete with technical
jargon and specialized terminology. Moreover, labeled data for model building
are scarce in this domain, resulting in low usage of aviation text data. The
emergence of LLMs presents an opportunity to transform this situation, but
there is a lack of LLMs specifically designed for the aviation domain. To
address this gap, we propose AviationGPT, which is built on open-source LLaMA-2
and Mistral architectures and continuously trained on a wealth of carefully
curated aviation datasets. Experimental results reveal that AviationGPT offers
users multiple advantages, including the versatility to tackle diverse natural
language processing (NLP) problems (e.g., question-answering, summarization,
document writing, information extraction, report querying, data cleaning, and
interactive data exploration). It also provides accurate and contextually
relevant responses within the aviation domain and significantly improves
performance (e.g., over a 40% performance gain in tested cases). With
AviationGPT, the aviation industry is better equipped to address more complex
research problems and enhance the efficiency and safety of National Airspace
System (NAS) operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1&quot;&gt;Jason Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tien_A/0/1/0/all/0/1&quot;&gt;Alex Tien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumgartner_D/0/1/0/all/0/1&quot;&gt;Diane M Baumgartner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17695">
<title>Fair Text-to-Image Diffusion via Fair Mapping. (arXiv:2311.17695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17695</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the limitations of existing text-to-image diffusion
models in generating demographically fair results when given human-related
descriptions. These models often struggle to disentangle the target language
context from sociocultural biases, resulting in biased image generation. To
overcome this challenge, we propose Fair Mapping, a general, model-agnostic,
and lightweight approach that modifies a pre-trained text-to-image model by
controlling the prompt to achieve fair image generation. One key advantage of
our approach is its high efficiency. The training process only requires
updating a small number of parameters in an additional linear mapping network.
This not only reduces the computational cost but also accelerates the
optimization process. We first demonstrate the issue of bias in generated
results caused by language biases in text-guided diffusion models. By
developing a mapping network that projects language embeddings into an unbiased
space, we enable the generation of relatively balanced demographic results
based on a keyword specified in the prompt. With comprehensive experiments on
face image generation, we show that our method significantly improves image
generation performance when prompted with descriptions related to human faces.
By effectively addressing the issue of bias, we produce more fair and diverse
image outputs. This work contributes to the field of text-to-image generation
by enhancing the ability to generate images that accurately reflect the
intended demographic characteristics specified in the text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lijie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianhang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17743">
<title>Mukhyansh: A Headline Generation Dataset for Indic Languages. (arXiv:2311.17743v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17743</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of headline generation within the realm of Natural Language
Processing (NLP) holds immense significance, as it strives to distill the true
essence of textual content into concise and attention-grabbing summaries. While
noteworthy progress has been made in headline generation for widely spoken
languages like English, there persist numerous challenges when it comes to
generating headlines in low-resource languages, such as the rich and diverse
Indian languages. A prominent obstacle that specifically hinders headline
generation in Indian languages is the scarcity of high-quality annotated data.
To address this crucial gap, we proudly present Mukhyansh, an extensive
multilingual dataset, tailored for Indian language headline generation.
Comprising an impressive collection of over 3.39 million article-headline
pairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu,
Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a
comprehensive evaluation of several state-of-the-art baseline models.
Additionally, through an empirical analysis of existing works, we demonstrate
that Mukhyansh outperforms all other models, achieving an impressive average
ROUGE-L score of 31.43 across all 8 languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madasu_L/0/1/0/all/0/1&quot;&gt;Lokesh Madasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanumolu_G/0/1/0/all/0/1&quot;&gt;Gopichand Kanumolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surange_N/0/1/0/all/0/1&quot;&gt;Nirmal Surange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1&quot;&gt;Manish Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17750">
<title>Addressing Membership Inference Attack in Federated Learning with Model Compression. (arXiv:2311.17750v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17750</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has been proposed as a privacy-preserving solution
for machine learning. However, recent works have shown that Federated Learning
can leak private client data through membership attacks. In this paper, we show
that the effectiveness of these attacks on the clients negatively correlates
with the size of the client datasets and model complexity. Based on this
finding, we propose model-agnostic Federated Learning as a privacy-enhancing
solution because it enables the use of models of varying complexity in the
clients. To this end, we present $\texttt{MaPP-FL}$, a novel privacy-aware FL
approach that leverages model compression on the clients while keeping a full
model on the server. We compare the performance of $\texttt{MaPP-FL}$ against
state-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and
FEMNIST vision datasets. Our experiments show the effectiveness of
$\texttt{MaPP-FL}$ in preserving the clients&apos; and the server&apos;s privacy while
achieving competitive classification accuracies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nemeth_G/0/1/0/all/0/1&quot;&gt;Gergely D&amp;#xe1;niel N&amp;#xe9;meth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_M/0/1/0/all/0/1&quot;&gt;Miguel &amp;#xc1;ngel Lozano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quadrianto_N/0/1/0/all/0/1&quot;&gt;Novi Quadrianto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliver_N/0/1/0/all/0/1&quot;&gt;Nuria Oliver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17766">
<title>Robustness Approaches for the Examination Timetabling Problem under Data Uncertainty. (arXiv:2311.17766v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17766</link>
<description rdf:parseType="Literal">&lt;p&gt;In the literature the examination timetabling problem (ETTP) is often
considered a post-enrollment problem (PE-ETTP). In the real world, universities
often schedule their exams before students register using information from
previous terms. A direct consequence of this approach is the uncertainty
present in the resulting models. In this work we discuss several approaches
available in the robust optimization literature. We consider the implications
of each approach in respect to the examination timetabling problem and present
how the most favorable approaches can be applied to the ETTP. Afterwards we
analyze the impact of some possible implementations of the given robustness
approaches on two real world instances and several random instances generated
by our instance generation framework which we introduce in this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassimir_B/0/1/0/all/0/1&quot;&gt;Bernd Bassimir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wanka_R/0/1/0/all/0/1&quot;&gt;Rolf Wanka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17781">
<title>Propagate &amp; Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs. (arXiv:2311.17781v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17781</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve
semisupervised node classification on graphs, by training a student MLP by
knowledge distillation from a teacher graph neural network (GNN). While
previous studies have focused mostly on training the student MLP by matching
the output probability distributions between the teacher and student models
during distillation, it has not been systematically studied how to inject the
structural information in an explicit and interpretable manner. Inspired by
GNNs that separate feature transformation $T$ and propagation $\Pi$, we
re-frame the distillation process as making the student MLP learn both $T$ and
$\Pi$. Although this can be achieved by applying the inverse propagation
$\Pi^{-1}$ before distillation from the teacher, it still comes with a high
computational cost from large matrix multiplications during training. To solve
this problem, we propose Propagate &amp;amp; Distill (P&amp;amp;D), which propagates the output
of the teacher before distillation, which can be interpreted as an approximate
process of the inverse propagation. We demonstrate that P&amp;amp;D can readily improve
the performance of the student MLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yong-Min Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1&quot;&gt;Won-Yong Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17815">
<title>A Survey on Design Methodologies for Accelerating Deep Learning on Heterogeneous Architectures. (arXiv:2311.17815v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.17815</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of Deep Learning has seen many disruptive and
impactful advancements. Given the increasing complexity of deep neural
networks, the need for efficient hardware accelerators has become more and more
pressing to design heterogeneous HPC platforms. The design of Deep Learning
accelerators requires a multidisciplinary approach, combining expertise from
several areas, spanning from computer architecture to approximate computing,
computational models, and machine learning algorithms. Several methodologies
and tools have been proposed to design accelerators for Deep Learning,
including hardware-software co-design approaches, high-level synthesis methods,
specific customized compilers, and methodologies for design space exploration,
modeling, and simulation. These methodologies aim to maximize the exploitable
parallelism and minimize data movement to achieve high performance and energy
efficiency. This survey provides a holistic review of the most influential
design methodologies and EDA tools proposed in recent years to implement Deep
Learning accelerators, offering the reader a wide perspective in this rapidly
evolving field. In particular, this work complements the previous survey
proposed by the same authors in [203], which focuses on Deep Learning hardware
accelerators for heterogeneous HPC platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrandi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Ferrandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curzel_S/0/1/0/all/0/1&quot;&gt;Serena Curzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorin_L/0/1/0/all/0/1&quot;&gt;Leandro Fiorin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ielmini_D/0/1/0/all/0/1&quot;&gt;Daniele Ielmini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvano_C/0/1/0/all/0/1&quot;&gt;Cristina Silvano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_F/0/1/0/all/0/1&quot;&gt;Francesco Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burrello_A/0/1/0/all/0/1&quot;&gt;Alessio Burrello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barchi_F/0/1/0/all/0/1&quot;&gt;Francesco Barchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavagno_L/0/1/0/all/0/1&quot;&gt;Luciano Lavagno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urso_T/0/1/0/all/0/1&quot;&gt;Teodoro Urso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calore_E/0/1/0/all/0/1&quot;&gt;Enrico Calore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schifano_S/0/1/0/all/0/1&quot;&gt;Sebastiano Fabio Schifano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambelli_C/0/1/0/all/0/1&quot;&gt;Cristian Zambelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palesi_M/0/1/0/all/0/1&quot;&gt;Maurizio Palesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ascia_G/0/1/0/all/0/1&quot;&gt;Giuseppe Ascia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_E/0/1/0/all/0/1&quot;&gt;Enrico Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petra_N/0/1/0/all/0/1&quot;&gt;Nicola Petra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caro_D/0/1/0/all/0/1&quot;&gt;Davide De Caro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meo_G/0/1/0/all/0/1&quot;&gt;Gennaro Di Meo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardellini_V/0/1/0/all/0/1&quot;&gt;Valeria Cardellini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filippone_S/0/1/0/all/0/1&quot;&gt;Salvatore Filippone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Presti_F/0/1/0/all/0/1&quot;&gt;Francesco Lo Presti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1&quot;&gt;Francesco Silvestri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palazzari_P/0/1/0/all/0/1&quot;&gt;Paolo Palazzari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perri_S/0/1/0/all/0/1&quot;&gt;Stefania Perri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17822">
<title>Anomalous Behavior Detection in Trajectory Data of Older Drivers. (arXiv:2311.17822v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17822</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a road network and a set of trajectory data, the anomalous behavior
detection (ABD) problem is to identify drivers that show significant
directional deviations, hardbrakings, and accelerations in their trips. The ABD
problem is important in many societal applications, including Mild Cognitive
Impairment (MCI) detection and safe route recommendations for older drivers.
The ABD problem is computationally challenging due to the large size of
temporally-detailed trajectories dataset. In this paper, we propose an
Edge-Attributed Matrix that can represent the key properties of
temporally-detailed trajectory datasets and identify abnormal driving
behaviors. Experiments using real-world datasets demonstrated that our approach
identifies abnormal driving behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghoreishi_S/0/1/0/all/0/1&quot;&gt;Seyedeh Gol Ara Ghoreishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moshfeghi_S/0/1/0/all/0/1&quot;&gt;Sonia Moshfeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jan_M/0/1/0/all/0/1&quot;&gt;Muhammad Tanveer Jan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conniff_J/0/1/0/all/0/1&quot;&gt;Joshua Conniff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;KwangSoo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jinwoo Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furht_B/0/1/0/all/0/1&quot;&gt;Borko Furht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tappen_R/0/1/0/all/0/1&quot;&gt;Ruth Tappen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1&quot;&gt;David Newman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosselli_M/0/1/0/all/0/1&quot;&gt;Monica Rosselli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1&quot;&gt;Jiannan Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17833">
<title>Analyzing and Explaining Image Classifiers via Diffusion Guidance. (arXiv:2311.17833v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17833</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning has led to huge progress in complex image classification
tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call
into question how reliably these classifiers work in the wild. Furthermore, for
safety-critical tasks the black-box nature of their decisions is problematic,
and explanations or at least methods which make decisions plausible are needed
urgently. In this paper, we address these problems by generating images that
optimize a classifier-derived objective using a framework for guided image
generation. We analyze the behavior and decisions of image classifiers by
visual counterfactual explanations (VCEs), detection of systematic mistakes by
analyzing images where classifiers maximally disagree, and visualization of
neurons to verify potential spurious features. In this way, we validate
existing observations, e.g. the shape bias of adversarially robust models, as
well as novel failure modes, e.g. systematic errors of zero-shot CLIP
classifiers, or identify harmful spurious features. Moreover, our VCEs
outperform previous work while being more versatile.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1&quot;&gt;Maximilian Augustin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuhaus_Y/0/1/0/all/0/1&quot;&gt;Yannic Neuhaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17842">
<title>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa&apos;s superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fanqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17855">
<title>Maximum Entropy Model Correction in Reinforcement Learning. (arXiv:2311.17855v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17855</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and theoretically analyze an approach for planning with an
approximate model in reinforcement learning that can reduce the adverse impact
of model error. If the model is accurate enough, it accelerates the convergence
to the true value function too. One of its key components is the MaxEnt Model
Correction (MoCo) procedure that corrects the model&apos;s next-state distributions
based on a Maximum Entropy density estimation formulation. Based on MoCo, we
introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its
sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna&apos;s convergence
can be much faster than the conventional model-free algorithms. Unlike
traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an
approximate model and still converge to the correct value function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhsha_A/0/1/0/all/0/1&quot;&gt;Amin Rakhsha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemertas_M/0/1/0/all/0/1&quot;&gt;Mete Kemertas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1&quot;&gt;Amir-massoud Farahmand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.14053">
<title>NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2110.14053</link>
<description rdf:parseType="Literal">&lt;p&gt;Propositional satisfiability (SAT) is an NP-complete problem that impacts
many research fields, such as planning, verification, and security. Mainstream
modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL)
algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural
Networks (GNNs). However, so far this approach either has not made solving more
effective, or required substantial GPU resources for frequent online model
inferences. Aiming to make GNN improvements practical, this paper proposes an
approach called NeuroBack, which builds on two insights: (1) predicting phases
(i.e., values) of variables appearing in the majority (or even all) of the
satisfying assignments are essential for CDCL SAT solving, and (2) it is
sufficient to query the neural model only once for the predictions before the
SAT solving starts. Once trained, the offline model inference allows NeuroBack
to execute exclusively on the CPU, removing its reliance on GPU resources. To
train NeuroBack, a new dataset called DataBack containing 120,286 data samples
is created. Finally, NeuroBack is implemented as an enhancement to a
state-of-the-art SAT solver called Kissat. As a result, it allowed Kissat to
solve 5.2% more problems on the recent SAT competition problem set,
SATCOMP-2022. NeuroBack therefore shows how machine learning can be harnessed
to improve SAT solving in an effective and practical manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1&quot;&gt;Mohit Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurshid_S/0/1/0/all/0/1&quot;&gt;Sarfraz Khurshid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMillan_K/0/1/0/all/0/1&quot;&gt;Kenneth McMillan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03865">
<title>Universalizing Weak Supervision. (arXiv:2112.03865v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03865</link>
<description rdf:parseType="Literal">&lt;p&gt;Weak supervision (WS) frameworks are a popular way to bypass hand-labeling
large datasets for training data-hungry models. These approaches synthesize
multiple noisy but cheaply-acquired estimates of labels into a set of
high-quality pseudolabels for downstream training. However, the synthesis
technique is specific to a particular kind of label, such as binary labels or
sequences, and each new label type requires manually designing a new synthesis
algorithm. Instead, we propose a universal technique that enables weak
supervision over any label type while still offering desirable properties,
including practical flexibility, computational efficiency, and theoretical
guarantees. We apply this technique to important problems previously not
tackled by WS frameworks including learning to rank, regression, and learning
in hyperbolic space. Theoretically, our synthesis approach produces a
consistent estimators for learning some challenging but important
generalizations of the exponential family model. Experimentally, we validate
our framework and show improvement over baselines in diverse settings including
real-world learning-to-rank and regression problems along with learning on
hyperbolic manifolds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Changho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Winfred Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwakarma_H/0/1/0/all/0/1&quot;&gt;Harit Vishwakarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1&quot;&gt;Nicholas Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.01660">
<title>On the Complexity of Winner Determination and Strategic Control in Conditional Approval Voting. (arXiv:2202.01660v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2202.01660</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on a generalization of the classic Minisum approval voting rule,
introduced by Barrot and Lang (2016), and referred to as Conditional Minisum
(CMS), for multi-issue elections with preferential dependencies. Under this
rule, voters are allowed to declare dependencies between different issues, but
the price we have to pay for this higher level of expressiveness is that we end
up with a computationally hard rule. Motivated by this, we first focus on
finding special cases that admit efficient algorithms for CMS. Our main result
in this direction is that we identify the condition of bounded treewidth (of an
appropriate graph, emerging from the provided ballots) as the necessary and
sufficient condition for exact polynomial algorithms, under common complexity
assumptions. We then move to the design of approximation algorithms. For the
(still hard) case of binary issues, we identify natural restrictions on the
voters&apos; ballots, under which we provide the first multiplicative approximation
algorithms for the problem. The restrictions involve upper bounds on the number
of dependencies an issue can have on the others and on the number of
alternatives per issue that a voter can approve. Finally, we also investigate
the complexity of problems related to the strategic control of conditional
approval elections by adding or deleting either voters or alternatives and we
show that in most variants of these problems, CMS is computationally resistant
against control. Overall, we conclude that CMS can be viewed as a solution that
achieves a satisfactory tradeoff between expressiveness and computational
efficiency, when we have a limited number of dependencies among issues, while
at the same time exhibiting sufficient resistance to control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markakis_E/0/1/0/all/0/1&quot;&gt;Evangelos Markakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papasotiropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Papasotiropoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11952">
<title>3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual Architecture. (arXiv:2205.11952v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11952</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based computed tomography (CT) reconstruction has demonstrated
outstanding performance on simulated 2D low-dose CT data. This applies in
particular to domain adapted neural networks, which incorporate a handcrafted
physics model for CT imaging. Empirical evidence shows that employing such
architectures reduces the demand for training data and improves upon
generalisation. However, their training requires large computational resources
that quickly become prohibitive in 3D helical CT, which is the most common
acquisition geometry used for medical imaging. Furthermore, clinical data also
comes with other challenges not accounted for in simulations, like errors in
flux measurement, resolution mismatch and, most importantly, the absence of the
real ground truth. The necessity to have a computationally feasible training
combined with the need to address these issues has made it difficult to
evaluate deep learning based reconstruction on clinical 3D helical CT. This
paper modifies a domain adapted neural network architecture, the Learned
Primal-Dual (LPD), so that it can be trained and applied to reconstruction in
this setting. We achieve this by splitting the helical trajectory into sections
and applying the unrolled LPD iterations to those sections sequentially. To the
best of our knowledge, this work is the first to apply an unrolled deep
learning architecture for reconstruction on full-sized clinical data, like
those in the Low dose CT image and projection data set (LDCT). Moreover,
training and testing is done on a single GPU card with 24GB of memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1&quot;&gt;Jevgenija Rudzusika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1&quot;&gt;Buda Baji&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Koehler_T/0/1/0/all/0/1&quot;&gt;Thomas Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1&quot;&gt;Ozan &amp;#xd6;ktem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04502">
<title>Building Open Knowledge Graph for Metal-Organic Frameworks (MOF-KG): Challenges and Case Studies. (arXiv:2207.04502v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04502</link>
<description rdf:parseType="Literal">&lt;p&gt;Metal-Organic Frameworks (MOFs) are a class of modular, porous crystalline
materials that have great potential to revolutionize applications such as gas
storage, molecular separations, chemical sensing, catalysis, and drug delivery.
The Cambridge Structural Database (CSD) reports 10,636 synthesized MOF crystals
which in addition contains ca. 114,373 MOF-like structures. The sheer number of
synthesized (plus potentially synthesizable) MOF structures requires
researchers pursue computational techniques to screen and isolate MOF
candidates. In this demo paper, we describe our effort on leveraging knowledge
graph methods to facilitate MOF prediction, discovery, and synthesis. We
present challenges and case studies about (1) construction of a MOF knowledge
graph (MOF-KG) from structured and unstructured sources and (2) leveraging the
MOF-KG for discovery of new or missing knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1&quot;&gt;Yuan An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenberg_J/0/1/0/all/0/1&quot;&gt;Jane Greenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xintong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaohua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCLellan_S/0/1/0/all/0/1&quot;&gt;Scott McCLellan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalinowski_A/0/1/0/all/0/1&quot;&gt;Alex Kalinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uribe_Romo_F/0/1/0/all/0/1&quot;&gt;Fernando J. Uribe-Romo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlois_K/0/1/0/all/0/1&quot;&gt;Kyle Langlois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furst_J/0/1/0/all/0/1&quot;&gt;Jacob Furst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Gualdron_D/0/1/0/all/0/1&quot;&gt;Diego A. G&amp;#xf3;mez-Gualdr&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fajardo_Rojas_F/0/1/0/all/0/1&quot;&gt;Fernando Fajardo-Rojas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardila_K/0/1/0/all/0/1&quot;&gt;Katherine Ardila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12337">
<title>Quality-diversity in dissimilarity spaces. (arXiv:2211.12337v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12337</link>
<description rdf:parseType="Literal">&lt;p&gt;The theory of magnitude provides a mathematical framework for quantifying and
maximizing diversity. We apply this framework to formulate quality-diversity
algorithms in generic dissimilarity spaces. In particular, we instantiate and
demonstrate a very general version of Go-Explore with promising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huntsman_S/0/1/0/all/0/1&quot;&gt;Steve Huntsman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14545">
<title>Modern Bayesian Experimental Design. (arXiv:2302.14545v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14545</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian experimental design (BED) provides a powerful and general framework
for optimizing the design of experiments. However, its deployment often poses
substantial computational challenges that can undermine its practical use. In
this review, we outline how recent advances have transformed our ability to
overcome these challenges and thus utilize BED effectively, before discussing
some key areas for future development in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1&quot;&gt;Adam Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ivanova_D/0/1/0/all/0/1&quot;&gt;Desi R Ivanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_F/0/1/0/all/0/1&quot;&gt;Freddie Bickford Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01552">
<title>Meta-Learning with a Geometry-Adaptive Preconditioner. (arXiv:2304.01552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01552</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-agnostic meta-learning (MAML) is one of the most successful
meta-learning algorithms. It has a bi-level optimization structure where the
outer-loop process learns a shared initialization and the inner-loop process
optimizes task-specific weights. Although MAML relies on the standard gradient
descent in the inner-loop, recent studies have shown that controlling the
inner-loop&apos;s gradient descent with a meta-learned preconditioner can be
beneficial. Existing preconditioners, however, cannot simultaneously adapt in a
task-specific and path-dependent way. Additionally, they do not satisfy the
Riemannian metric condition, which can enable the steepest descent learning
with preconditioned gradient. In this study, we propose Geometry-Adaptive
Preconditioned gradient descent (GAP) that can overcome the limitations in
MAML; GAP can efficiently meta-learn a preconditioner that is dependent on
task-specific parameters, and its preconditioner can be shown to be a
Riemannian metric. Thanks to the two properties, the geometry-adaptive
preconditioner is effective for improving the inner-loop optimization.
Experiment results show that GAP outperforms the state-of-the-art MAML family
and preconditioned gradient descent-MAML (PGD-MAML) family in a variety of
few-shot learning tasks. Code is available at:
https://github.com/Suhyun777/CVPR23-GAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Suhyun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1&quot;&gt;Duhun Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eo_M/0/1/0/all/0/1&quot;&gt;Moonjung Eo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesup Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1&quot;&gt;Wonjong Rhee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05062">
<title>A Feasibility Study on Indoor Localization and Multi-person Tracking Using Sparsely Distributed Camera Network with Edge Computing. (arXiv:2305.05062v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05062</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera-based activity monitoring systems are becoming an attractive solution
for smart building applications with the advances in computer vision and edge
computing technologies. In this paper, we present a feasibility study and
systematic analysis of a camera-based indoor localization and multi-person
tracking system implemented on edge computing devices within a large indoor
space. To this end, we deployed an end-to-end edge computing pipeline that
utilizes multiple cameras to achieve localization, body orientation estimation
and tracking of multiple individuals within a large therapeutic space spanning
$1700m^2$, all while maintaining a strong focus on preserving privacy. Our
pipeline consists of 39 edge computing camera systems equipped with Tensor
Processing Units (TPUs) placed in the indoor space&apos;s ceiling. To ensure the
privacy of individuals, a real-time multi-person pose estimation algorithm runs
on the TPU of the computing camera system. This algorithm extracts poses and
bounding boxes, which are utilized for indoor localization, body orientation
estimation, and multi-person tracking. Our pipeline demonstrated an average
localization error of 1.41 meters, a multiple-object tracking accuracy score of
88.6\%, and a mean absolute body orientation error of 29\degree. These results
shows that localization and tracking of individuals in a large indoor space is
feasible even with the privacy constrains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Hyeokhyen Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chaitra Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiarashi_Y/0/1/0/all/0/1&quot;&gt;Yashar Kiarashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madala_V/0/1/0/all/0/1&quot;&gt;Venkata Siva Krishna Madala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Ratan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakum_A/0/1/0/all/0/1&quot;&gt;ArjunSinh Nakum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tweedy_R/0/1/0/all/0/1&quot;&gt;Robert Tweedy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonetto_L/0/1/0/all/0/1&quot;&gt;Leandro Miletto Tonetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimring_C/0/1/0/all/0/1&quot;&gt;Craig M. Zimring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doiron_M/0/1/0/all/0/1&quot;&gt;Matthew Doiron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Amy D. Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levey_A/0/1/0/all/0/1&quot;&gt;Allan I. Levey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_G/0/1/0/all/0/1&quot;&gt;Gari D. Clifford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10361">
<title>Human Choice Prediction in Language-based Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10361</link>
<description rdf:parseType="Literal">&lt;p&gt;Persuasion games have been fundamental in economics and AI research, and have
significant practical applications. Recent works in this area have started to
incorporate natural language, moving beyond the traditional stylized message
setting. However, previous research has focused on on-policy prediction, where
the train and test data have the same distribution, which is not representative
of real-life scenarios. In this paper, we tackle the challenging problem of
off-policy evaluation (OPE) in language-based persuasion games. To address the
inherent difficulty of human data collection in this setup, we propose a novel
approach which combines real and simulated human-bot interaction data. Our
simulated data is created by an exogenous model assuming decision makers (DMs)
start with a mixture of random and decision-theoretic based behaviors and
improve over time. We present a deep learning training algorithm that
effectively integrates real interaction and simulated data, substantially
improving over models that train only with interaction data. Our results
demonstrate the potential of real interaction and simulation mixtures as a
cost-effective and scalable solution for OPE in language-based persuasion
games. Our code and the large dataset we collected and generated are submitted
as supplementary material and publicly available in our GitHub repository:
https://github.com/eilamshapira/HumanChoicePrediction
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapira_E/0/1/0/all/0/1&quot;&gt;Eilam Shapira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1&quot;&gt;Reut Apel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1&quot;&gt;Moshe Tennenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10665">
<title>Content-based Unrestricted Adversarial Attack. (arXiv:2305.10665v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10665</link>
<description rdf:parseType="Literal">&lt;p&gt;Unrestricted adversarial attacks typically manipulate the semantic content of
an image (e.g., color or texture) to create adversarial examples that are both
effective and photorealistic, demonstrating their ability to deceive human
perception and deep neural networks with stealth and success. However, current
works usually sacrifice unrestricted degrees and subjectively select some image
content to guarantee the photorealism of unrestricted adversarial examples,
which limits its attack performance. To ensure the photorealism of adversarial
examples and boost attack performance, we propose a novel unrestricted attack
framework called Content-based Unrestricted Adversarial Attack. By leveraging a
low-dimensional manifold that represents natural images, we map the images onto
the manifold and optimize them along its adversarial direction. Therefore,
within this framework, we implement Adversarial Content Attack based on Stable
Diffusion and can generate high transferable unrestricted adversarial examples
with various adversarial contents. Extensive experimentation and visualization
demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art
attacks by an average of 13.3-50.4% and 16.8-48.0% in normally trained models
and defense methods, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kaixun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11120">
<title>A Compound Gaussian Least Squares Algorithm and Unrolled Network for Linear Inverse Problems. (arXiv:2305.11120v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11120</link>
<description rdf:parseType="Literal">&lt;p&gt;For solving linear inverse problems, particularly of the type that appears in
tomographic imaging and compressive sensing, this paper develops two new
approaches. The first approach is an iterative algorithm that minimizes a
regularized least squares objective function where the regularization is based
on a compound Gaussian prior distribution. The compound Gaussian prior subsumes
many of the commonly used priors in image reconstruction, including those of
sparsity-based approaches. The developed iterative algorithm gives rise to the
paper&apos;s second new approach, which is a deep neural network that corresponds to
an &quot;unrolling&quot; or &quot;unfolding&quot; of the iterative algorithm. Unrolled deep neural
networks have interpretable layers and outperform standard deep learning
methods. This paper includes a detailed computational theory that provides
insight into the construction and performance of both algorithms. The
conclusion is that both algorithms outperform other state-of-the-art approaches
to tomographic image formation and compressive sensing, especially in the
difficult regime of low training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyons_C/0/1/0/all/0/1&quot;&gt;Carter Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raj_R/0/1/0/all/0/1&quot;&gt;Raghu G. Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheney_M/0/1/0/all/0/1&quot;&gt;Margaret Cheney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14991">
<title>MuLER: Detailed and Scalable Reference-based Evaluation. (arXiv:2305.14991v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14991</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel methodology (namely, MuLER) that transforms any
reference-based evaluation metric for text generation, such as machine
translation (MT) into a fine-grained analysis tool. Given a system and a
metric, MuLER quantifies how much the chosen metric penalizes specific error
types (e.g., errors in translating names of locations). MuLER thus enables a
detailed error analysis which can lead to targeted improvement efforts for
specific phenomena. We perform experiments in both synthetic and naturalistic
settings to support MuLER&apos;s validity and showcase its usability in MT
evaluation, and other tasks, such as summarization. Analyzing all submissions
to WMT in 2014-2020, we find consistent trends. For example, nouns and verbs
are among the most frequent POS tags. However, they are among the hardest to
translate. Performance on most POS tags improves with overall system
performance, but a few are not thus correlated (their identity changes from
language to language). Preliminary experiments with summarization reveal
similar trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1&quot;&gt;Taelin Karidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1&quot;&gt;Gal Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1&quot;&gt;Omri Abend&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18381">
<title>Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18381</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-efficient learning has garnered significant attention, especially given
the current trend of large multi-modal models. Recently, dataset distillation
becomes an effective approach for data-efficiency; however, the distillation
process itself can still be inefficient. In this work, we model the dataset
distillation task within the context of information transport. By observing the
substantial data redundancy inherent in the distillation, we argue to put more
emphasis on the samples&apos; utility for the distillation task. We introduce and
validate a family of data utility estimators and optimal data selection methods
to exploit the most valuable samples. This strategy significantly reduces the
training costs and extends various existing distillation algorithms to larger
and more diversified datasets, e.g., in some cases only 0.04% training data is
sufficient for comparable distillation performance. Our method consistently
enhances the distillation algorithms, even on much larger-scale and more
heterogeneous datasets, e.g. ImageNet-1K and Kinetics-400. This paradigm opens
up new avenues in the dynamics of distillation and paves the way for efficient
dataset distillation. Our code is available on
https://github.com/silicx/GoldFromOres .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kaitong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19891">
<title>Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19891</link>
<description rdf:parseType="Literal">&lt;p&gt;Large discrete action spaces (LDAS) remain a central challenge in
reinforcement learning. Existing solution approaches can handle unstructured
LDAS with up to a few million actions. However, many real-world applications in
logistics, production, and transportation systems have combinatorial action
spaces, whose size grows well beyond millions of actions, even on small
instances. Fortunately, such action spaces exhibit structure, e.g., equally
spaced discrete resource units. With this work, we focus on handling structured
LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we
propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm
for SLDAS. We present a scalable neighborhood exploration heuristic that
utilizes this paradigm and efficiently explores the discrete neighborhood
around the continuous proxy action in structured action spaces with up to
$10^{73}$ actions. We demonstrate the performance of our method by benchmarking
it against three state-of-the-art approaches designed for large discrete action
spaces across two distinct environments. Our results show that DNC matches or
outperforms state-of-the-art approaches while being computationally more
efficient. Furthermore, our method scales to action spaces that so far remained
computationally intractable for existing methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkerman_F/0/1/0/all/0/1&quot;&gt;Fabian Akkerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luy_J/0/1/0/all/0/1&quot;&gt;Julius Luy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heeswijk_W/0/1/0/all/0/1&quot;&gt;Wouter van Heeswijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiffer_M/0/1/0/all/0/1&quot;&gt;Maximilian Schiffer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15832">
<title>Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15832</link>
<description rdf:parseType="Literal">&lt;p&gt;Generated images of score-based models can suffer from errors in their
spatial means, an effect, referred to as a color shift, which grows for larger
images. This paper investigates a previously-introduced approach to mitigate
color shifts in score-based diffusion models. We quantify the performance of a
nonlinear bypass connection in the score network, designed to process the
spatial mean of the input and to predict the mean of the score function. We
show that this network architecture substantially improves the resulting
quality of the generated images, and that this improvement is approximately
independent of the size of the generated images. As a result, this modified
architecture offers a simple solution for the color shift problem across image
sizes. We additionally discuss the origin of color shifts in an idealized
setting in order to motivate the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deck_K/0/1/0/all/0/1&quot;&gt;Katherine Deck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischoff_T/0/1/0/all/0/1&quot;&gt;Tobias Bischoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by the Unity engine, M3Act
features multiple semantic groups, highly diverse and photorealistic images,
and a comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments using various input modalities. First, adding our synthetic data
significantly improves the performance of MOTRv2 on DanceTrack, leading to a
hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking
results on par with MOTRv2*, which is trained with 62.5% more real-world data.
Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on
group activity and atomic action accuracy respectively. Moreover, M3Act opens
new research for controllable 3D group activity generation. We define multiple
metrics and propose a competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03705">
<title>Intelligent Robotic Sonographer: Mutual Information-based Disentangled Reward Learning from Few Demonstrations. (arXiv:2307.03705v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03705</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound (US) imaging is widely used for biometric measurement and
diagnosis of internal organs due to the advantages of being real-time and
radiation-free. However, due to inter-operator variations, resulting images
highly depend on the experience of sonographers. This work proposes an
intelligent robotic sonographer to autonomously &quot;explore&quot; target anatomies and
navigate a US probe to a relevant 2D plane by learning from the expert. The
underlying high-level physiological knowledge from experts is inferred by a
neural reward function, using a ranked pairwise image comparisons approach in a
self-supervised fashion. This process can be referred to as understanding the
&quot;language of sonography&quot;. Considering the generalization capability to overcome
inter-patient variations, mutual information is estimated by a network to
explicitly disentangle the task-related and domain features in latent space.
The robotic localization is carried out in coarse-to-fine mode based on the
predicted reward associated with B-mode images. To validate the effectiveness
of the proposed reward inference network, representative experiments were
performed on vascular phantoms (&quot;line&quot; target), two types of ex-vivo animal
organs (chicken heart and lamb kidney) phantoms (&quot;point&quot; target) and in-vivo
human carotids, respectively. To further validate the performance of the
autonomous acquisition framework, physical robotic acquisitions were performed
on three phantoms (vascular, chicken heart, and lamb kidney). The results
demonstrated that the proposed advanced framework can robustly work on a
variety of seen and unseen phantoms as well as in-vivo human carotid data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1&quot;&gt;Yuan Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingchuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Ying Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1&quot;&gt;Michael Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05052">
<title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05052</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it&apos;s more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Paiheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongxia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyemi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00688">
<title>AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00688</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Place Recognition (VPR) is vital for robot localization. To date, the
most performant VPR approaches are environment- and task-specific: while they
exhibit strong performance in structured environments (predominantly urban
driving), their performance degrades severely in unstructured environments,
rendering most approaches brittle to robust real-world deployment. In this
work, we develop a universal solution to VPR -- a technique that works across a
broad range of structured and unstructured environments (urban, outdoors,
indoors, aerial, underwater, and subterranean environments) without any
re-training or fine-tuning. We demonstrate that general-purpose feature
representations derived from off-the-shelf self-supervised models with no
VPR-specific training are the right substrate upon which to build such a
universal VPR solution. Combining these derived features with unsupervised
feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X
significantly higher performance than existing approaches. We further obtain a
6% improvement in performance by characterizing the semantic properties of
these features, uncovering unique domains which encapsulate datasets from
similar environments. Our detailed experiments and analysis lay a foundation
for building VPR solutions that may be deployed anywhere, anytime, and across
anyview. We encourage the readers to explore our project page and interactive
demos: https://anyloc.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1&quot;&gt;Nikhil Keetha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Avneesh Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karhade_J/0/1/0/all/0/1&quot;&gt;Jay Karhade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1&quot;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_M/0/1/0/all/0/1&quot;&gt;Madhava Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sourav Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01050">
<title>A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles&apos; Riskiness. (arXiv:2308.01050v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01050</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicles (AVs) promise a range of societal advantages, including
broader access to mobility, reduced road accidents, and enhanced transportation
efficiency. However, evaluating the risks linked to AVs is complex due to
limited historical data and the swift progression of technology. This paper
presents a data-driven framework for assessing the risk of different AVs&apos;
behaviors in various operational design domains (ODDs), based on counterfactual
simulations of &quot;misbehaving&quot; road users. We propose the notion of
counterfactual safety margin, which represents the minimum deviation from
nominal behavior that could cause a collision. This methodology not only
pinpoints the most critical scenarios but also quantifies the (relative) risk&apos;s
frequency and severity concerning AVs. Importantly, we show that our approach
is applicable even when the AV&apos;s behavioral policy remains undisclosed, through
worst- and best-case analyses, benefiting external entities like regulators and
risk evaluators. Our experimental outcomes demonstrate the correlation between
the safety margin, the quality of the driving policy, and the ODD, shedding
light on the relative risks of different AV providers. Overall, this work
contributes to the safety assessment of AVs and addresses legislative and
insurance concerns surrounding this burgeoning technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanardi_A/0/1/0/all/0/1&quot;&gt;Alessandro Zanardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Censi_A/0/1/0/all/0/1&quot;&gt;Andrea Censi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzei_M/0/1/0/all/0/1&quot;&gt;Margherita Atzei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillo_L/0/1/0/all/0/1&quot;&gt;Luigi Di Lillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frazzoli_E/0/1/0/all/0/1&quot;&gt;Emilio Frazzoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15068">
<title>A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15068</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation methods are commonly integrated into the training of
anomaly detection models. Previous approaches have primarily focused on
replicating real-world anomalies or enhancing diversity, without considering
that the standard of anomaly varies across different classes, potentially
leading to a biased training distribution.This paper analyzes crucial traits of
simulated anomalies that contribute to the training of reconstructive networks
and condenses them into several methods, thus creating a comprehensive
framework by selectively utilizing appropriate combinations.Furthermore, we
integrate this framework with a reconstruction-based approach and concurrently
propose a split training strategy that alleviates the issue of overfitting
while avoiding introducing interference to the reconstruction process. The
evaluations conducted on the MVTec anomaly detection dataset demonstrate that
our method outperforms the previous state-of-the-art approach, particularly in
terms of object classes. To evaluate generalizability, we generate a simulated
dataset comprising anomalies with diverse characteristics since the original
test samples only include specific types of anomalies and may lead to biased
evaluations. Experimental results demonstrate that our approach exhibits
promising potential for generalizing effectively to various unforeseen
anomalies encountered in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yaping Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01029">
<title>Explainability for Large Language Models: A Survey. (arXiv:2309.01029v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01029</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated impressive capabilities in
natural language processing. However, their internal mechanisms are still
unclear and this lack of transparency poses unwanted risks for downstream
applications. Therefore, understanding and explaining these models is crucial
for elucidating their behaviors, limitations, and social impacts. In this
paper, we introduce a taxonomy of explainability techniques and provide a
structured overview of methods for explaining Transformer-based language
models. We categorize techniques based on the training paradigms of LLMs:
traditional fine-tuning-based paradigm and prompting-based paradigm. For each
paradigm, we summarize the goals and dominant approaches for generating local
explanations of individual predictions and global explanations of overall model
knowledge. We also discuss metrics for evaluating generated explanations, and
discuss how explanations can be leveraged to debug models and improve
performance. Lastly, we examine key challenges and emerging opportunities for
explanation techniques in the era of LLMs in comparison to conventional machine
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Huiqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02685">
<title>Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02685</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion generative modeling has become a promising approach for learning
robotic manipulation tasks from stochastic human demonstrations. In this paper,
we present Diffusion-EDFs, a novel SE(3)-equivariant diffusion-based approach
for visual robotic manipulation tasks. We show that our proposed method
achieves remarkable data efficiency, requiring only 5 to 10 human
demonstrations for effective end-to-end training in less than an hour.
Furthermore, our benchmark experiments demonstrate that our approach has
superior generalizability and robustness compared to state-of-the-art methods.
Lastly, we validate our methods with real hardware experiments. Project
Website: https://sites.google.com/view/diffusion-edfs/home
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1&quot;&gt;Hyunseok An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Junwoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yubin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1&quot;&gt;Chaewon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03579">
<title>DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03579</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring distance or similarity between time-series data is a fundamental
aspect of many applications including classification, clustering, and
ensembling/alignment. Existing measures may fail to capture similarities among
local trends (shapes) and may even produce misleading results. Our goal is to
develop a measure that looks for similar trends occurring around similar times
and is easily interpretable for researchers in applied domains. This is
particularly useful for applications where time-series have a sequence of
meaningful local trends that are ordered, such as in epidemics (a surge to an
increase to a peak to a decrease). We propose a novel measure, DTW+S, which
creates an interpretable &quot;closeness-preserving&quot; matrix representation of the
time-series, where each column represents local trends, and then it applies
Dynamic Time Warping to compute distances between these matrices. We present a
theoretical analysis that supports the choice of this representation. We
demonstrate the utility of DTW+S in several tasks. For the clustering of
epidemic curves, we show that DTW+S is the only measure able to produce good
clustering compared to the baselines. For ensemble building, we propose a
combination of DTW+S and barycenter averaging that results in the best
preservation of characteristics of the underlying trajectories. We also
demonstrate that our approach results in better classification compared to
Dynamic Time Warping for a class of datasets, particularly when local trends
rather than scale play a decisive role.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Ajitesh Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06800">
<title>Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06800</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction is a crucial topic because of its broad scope of
applications in the transportation domain. Recently, various studies have
achieved promising results. However, most studies assume the prediction
locations have complete or at least partial historical records and cannot be
extended to non-historical recorded locations. In real-life scenarios, the
deployment of sensors could be limited due to budget limitations and
installation availability, which makes most current models not applicable.
Though few pieces of literature tried to impute traffic states at the missing
locations, these methods need the data simultaneously observed at the locations
with sensors, making them not applicable to prediction tasks. Another drawback
is the lack of measurement of uncertainty in prediction, making prior works
unsuitable for risk-sensitive tasks or involving decision-making. To fill the
gap, inspired by the previous inductive graph neural network, this work
proposed an uncertainty-aware framework with the ability to 1) extend
prediction to missing locations with no historical records and significantly
extend spatial coverage of prediction locations while reducing deployment of
sensors and 2) generate probabilistic prediction with uncertainty
quantification to help the management of risk and decision making in the
down-stream tasks. Through extensive experiments on real-life datasets, the
result shows our method achieved promising results on prediction tasks, and the
uncertainty quantification gives consistent results which highly correlated
with the locations with and without historical data. We also show that our
model could help support sensor deployment tasks in the transportation field to
achieve higher accuracy with a limited sensor deployment budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junxian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhiming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guanjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12931">
<title>On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12931</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised training methods for transformers have demonstrated
remarkable performance across various domains. Previous transformer-based
models, such as masked autoencoders (MAE), typically utilize a single
normalization layer for both the [CLS] symbol and the tokens. We propose in
this paper a simple modification that employs separate normalization layers for
the tokens and the [CLS] symbol to better capture their distinct
characteristics and enhance downstream task performance. Our method aims to
alleviate the potential negative effects of using the same normalization
statistics for both token types, which may not be optimally aligned with their
individual roles. We empirically show that by utilizing a separate
normalization layer, the [CLS] embeddings can better encode the global
contextual information and are distributed more uniformly in its anisotropic
space. When replacing the conventional normalization layer with the two
separate layers, we observe an average 2.7% performance improvement over the
image, natural language, and graph domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinkai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuanqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1&quot;&gt;Soha Hassoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li-Ping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13731">
<title>Arabic Sentiment Analysis with Noisy Deep Explainable Model. (arXiv:2309.13731v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13731</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentiment Analysis (SA) is an indispensable task for many real-world
applications. Compared to limited resourced languages (i.e., Arabic, Bengali),
most of the research on SA are conducted for high resourced languages (i.e.,
English, Chinese). Moreover, the reasons behind any prediction of the Arabic
sentiment analysis methods exploiting advanced artificial intelligence
(AI)-based approaches are like black-box - quite difficult to understand. This
paper proposes an explainable sentiment classification framework for the Arabic
language by introducing a noise layer on Bi-Directional Long Short-Term Memory
(BiLSTM) and Convolutional Neural Networks (CNN)-BiLSTM models that overcome
over-fitting problem. The proposed framework can explain specific predictions
by training a local surrogate explainable model to understand why a particular
sentiment (positive or negative) is being predicted. We carried out experiments
on public benchmark Arabic SA datasets. The results concluded that adding noise
layers improves the performance in sentiment analysis for the Arabic language
by reducing overfitting and our method outperformed some known state-of-the-art
methods. In addition, the introduced explainability with noise layer could make
the model more transparent and accountable and hence help adopting AI-enabled
system in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atabuzzaman_M/0/1/0/all/0/1&quot;&gt;Md. Atabuzzaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1&quot;&gt;Md Shajalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baby_M/0/1/0/all/0/1&quot;&gt;Maksuda Bilkis Baby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boden_A/0/1/0/all/0/1&quot;&gt;Alexander Boden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01929">
<title>Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have
demonstrated remarkable prompt-based image generation capabilities.
Multilingual encoders may have a substantial impact on the cultural agency of
these models, as language is a conduit of culture. In this study, we explore
the cultural perception embedded in TTI models by characterizing culture across
three hierarchical tiers: cultural dimensions, cultural domains, and cultural
concepts. Based on this ontology, we derive prompt templates to unlock the
cultural knowledge in TTI models, and propose a comprehensive suite of
evaluation techniques, including intrinsic evaluations using the CLIP space,
extrinsic evaluations with a Visual-Question-Answer (VQA) model and human
assessments, to evaluate the cultural content of TTI-generated images. To
bolster our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments provide insights
regarding Do, What, Which and How research questions about the nature of
cultural encoding in TTI models, paving the way for cross-cultural applications
of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventura_M/0/1/0/all/0/1&quot;&gt;Mor Ventura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1&quot;&gt;Eyal Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03605">
<title>FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03605</link>
<description rdf:parseType="Literal">&lt;p&gt;Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collyer_J/0/1/0/all/0/1&quot;&gt;Josh Collyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1&quot;&gt;Tim Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_I/0/1/0/all/0/1&quot;&gt;Iain Phillips&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03684">
<title>SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03684</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM. Our code is publicly available at the following link:
https://github.com/arobey1/smooth-llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1&quot;&gt;Alexander Robey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05703">
<title>An Attribution Method for Siamese Encoders. (arXiv:2310.05703v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05703</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of Siamese encoder models such as sentence transformers
(ST), little is known about the aspects of inputs they pay attention to. A
barrier is that their predictions cannot be attributed to individual features,
as they compare two inputs rather than processing a single one. This paper
derives a local attribution method for Siamese encoders by generalizing the
principle of integrated gradients to models with multiple inputs. The solution
takes the form of feature-pair attributions, and can be reduced to a
token-token matrix for STs. Our method involves the introduction of integrated
Jacobians and inherits the advantageous formal properties of integrated
gradients: it accounts for the model&apos;s full computation graph and is guaranteed
to converge to the actual prediction. A pilot study shows that in an ST few
token-pairs can often explain large fractions of predictions, and it focuses on
nouns and verbs. For accurate predictions, it however needs to attend to the
majority of tokens and parts of speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_L/0/1/0/all/0/1&quot;&gt;Lucas M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1&quot;&gt;Dmitry Nikolaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1&quot;&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05866">
<title>Generative quantum machine learning via denoising diffusion probabilistic models. (arXiv:2310.05866v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05866</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models are key-enabling technology to computer vision, text
generation and large language models. Denoising diffusion probabilistic models
(DDPMs) have recently gained much attention due to their ability to generate
diverse and high-quality samples in many computer vision tasks, as well as to
incorporate flexible model architectures and relatively simple training scheme.
Quantum generative models, empowered by entanglement and superposition, have
brought new insight to learning classical and quantum data. Inspired by the
classical counterpart, we propose the quantum denoising diffusion probabilistic
models (QuDDPM) to enable efficiently trainable generative learning of quantum
data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,
while introduces multiple intermediate training tasks as interpolation between
the target distribution and noise to avoid barren plateau and guarantee
efficient training. We provide bounds on the learning error and demonstrate
QuDDPM&apos;s capability in learning correlated quantum noise model, quantum
many-body phases and topological structure of quantum data. The results provide
a paradigm for versatile and efficient quantum generative learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bingzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhuang_Q/0/1/0/all/0/1&quot;&gt;Quntao Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08235">
<title>GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shaofei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yitao Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09932">
<title>&quot;Reading Between the Heat&quot;: Co-Teaching Body Thermal Signatures for Non-intrusive Stress Detection. (arXiv:2310.09932v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09932</link>
<description rdf:parseType="Literal">&lt;p&gt;Stress impacts our physical and mental health as well as our social life. A
passive and contactless indoor stress monitoring system can unlock numerous
important applications such as workplace productivity assessment, smart homes,
and personalized mental health monitoring. While the thermal signatures from a
user&apos;s body captured by a thermal camera can provide important information
about the &quot;fight-flight&quot; response of the sympathetic and parasympathetic
nervous system, relying solely on thermal imaging for training a stress
prediction model often lead to overfitting and consequently a suboptimal
performance. This paper addresses this challenge by introducing ThermaStrain, a
novel co-teaching framework that achieves high-stress prediction performance by
transferring knowledge from the wearable modality to the contactless thermal
modality. During training, ThermaStrain incorporates a wearable electrodermal
activity (EDA) sensor to generate stress-indicative representations from
thermal videos, emulating stress-indicative representations from a wearable EDA
sensor. During testing, only thermal sensing is used, and stress-indicative
patterns from thermal data and emulated EDA representations are extracted to
improve stress assessment. The study collected a comprehensive dataset with
thermal video and EDA data under various stress conditions and distances.
ThermaStrain achieves an F1 score of 0.8293 in binary stress classification,
outperforming the thermal-only baseline approach by over 9%. Extensive
evaluations highlight ThermaStrain&apos;s effectiveness in recognizing
stress-indicative attributes, its adaptability across distances and stress
scenarios, real-time executability on edge platforms, its applicability to
multi-individual sensing, ability to function on limited visibility and
unfamiliar conditions, and the advantages of its co-teaching approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harshit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergen_Cico_D/0/1/0/all/0/1&quot;&gt;Dessa Bergen-Cico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1&quot;&gt;Tauhidur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salekin_A/0/1/0/all/0/1&quot;&gt;Asif Salekin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09949">
<title>Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09949</link>
<description rdf:parseType="Literal">&lt;p&gt;A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1&quot;&gt;Marco Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1&quot;&gt;Roger Waleffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1&quot;&gt;Gustavo Alonso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11518">
<title>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11518</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-play is a technique for machine learning in multi-agent systems where a
learning algorithm learns by interacting with copies of itself. Self-play is
useful for generating large quantities of data for learning, but has the
drawback that the agents the learner will face post-training may have
dramatically different behavior than the learner came to expect by interacting
with itself. For the special case of two-player constant-sum games, self-play
that reaches Nash equilibrium is guaranteed to produce strategies that perform
well against any post-training opponent; however, no such guarantee exists for
multiplayer games. We show that in games that approximately decompose into a
set of two-player constant-sum games (called constant-sum polymatrix games)
where global $\epsilon$-Nash equilibria are boundedly far from Nash equilibria
in each subgame (called subgame stability), any no-external-regret algorithm
that learns by self-play will produce a strategy with bounded vulnerability.
For the first time, our results identify a structural property of multiplayer
games that enable performance guarantees for the strategies produced by a broad
class of self-play algorithms. We demonstrate our findings through experiments
on Leduc poker.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacQueen_R/0/1/0/all/0/1&quot;&gt;Revan MacQueen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1&quot;&gt;James R. Wright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12819">
<title>Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12819</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving complex planning problems has been a long-standing challenge in
computer science. Learning-based subgoal search methods have shown promise in
tackling these problems, but they often suffer from a lack of completeness
guarantees, meaning that they may fail to find a solution even if one exists.
In this paper, we propose an efficient approach to augment a subgoal search
method to achieve completeness in discrete action spaces. Specifically, we
augment the high-level search with low-level actions to execute a multi-level
(hybrid) search, which we call complete subgoal search. This solution achieves
the best of both worlds: the practical efficiency of high-level search and the
completeness of low-level search. We apply the proposed search method to a
recently proposed subgoal search algorithm and evaluate the algorithm trained
on offline data on complex planning problems. We demonstrate that our complete
subgoal search not only guarantees completeness but can even improve
performance in terms of search expansions for instances that the high-level
could solve without low-level augmentations. Our approach makes it possible to
apply subgoal-level planning for systems where completeness is a critical
requirement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujanpaa_K/0/1/0/all/0/1&quot;&gt;Kalle Kujanp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1&quot;&gt;Alexander Ilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17462">
<title>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17462</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object&apos;s
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method&apos;s potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1&quot;&gt;Daniel Kienzle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1&quot;&gt;Julian Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1&quot;&gt;Katja Ludwig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1&quot;&gt;Rainer Lienhart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18297">
<title>Image Clustering Conditioned on Text Criteria. (arXiv:2310.18297v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18297</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical clustering methods do not provide users with direct control of the
clustering results, and the clustering results may not be consistent with the
relevant criterion that a user has in mind. In this work, we present a new
methodology for performing image clustering based on user-specified text
criteria by leveraging modern vision-language models and large language models.
We call our method Image Clustering Conditioned on Text Criteria (IC|TC), and
it represents a different paradigm of image clustering. IC|TC requires a
minimal and practical degree of human intervention and grants the user
significant control over the clustering results in return. Our experiments show
that IC|TC can effectively cluster images with various criteria, such as human
action, physical location, or the person&apos;s mood, while significantly
outperforming baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1&quot;&gt;Sehyun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jaeseung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minkyu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_E/0/1/0/all/0/1&quot;&gt;Ernest K. Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kangwook Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18348">
<title>Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18348</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models. Our code is available at:
https://github.com/tianyu139/meaning-as-trajectories
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1&quot;&gt;Matthew Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1&quot;&gt;Pramuditha Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1&quot;&gt;Luca Zancato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00213">
<title>Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00213</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel and efficient approach for text-based video-to-video
editing that eliminates the need for resource-intensive per-video-per-model
finetuning. At the core of our approach is a synthetic paired video dataset
tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix&apos;s
image transfer via editing instruction, we adapt this paradigm to the video
domain. Extending the Prompt-to-Prompt to videos, we efficiently generate
paired samples, each with an input video and its edited counterpart. Alongside
this, we introduce the Long Video Sampling Correction during sampling, ensuring
consistent long videos across batches. Our method surpasses current methods
like Tune-A-Video, heralding substantial progress in text-based video-to-video
editing and suggesting exciting avenues for further exploration and deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiaxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tianjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00290">
<title>Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v2 [cs.CE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00290</link>
<description rdf:parseType="Literal">&lt;p&gt;As the global deployment of carbon capture and sequestration (CCS) technology
intensifies in the fight against climate change, it becomes increasingly
imperative to establish robust monitoring and detection mechanisms for
potential underground CO2 leakage, particularly through pre-existing or induced
faults in the storage reservoir&apos;s seals. While techniques such as history
matching and time-lapse seismic monitoring of CO2 storage have been used
successfully in tracking the evolution of CO2 plumes in the subsurface, these
methods lack principled approaches to characterize uncertainties related to the
CO2 plumes&apos; behavior. Inclusion of systematic assessment of uncertainties is
essential for risk mitigation for the following reasons: (i) CO2 plume-induced
changes are small and seismic data is noisy; (ii) changes between regular and
irregular (e.g., caused by leakage) flow patterns are small; and (iii) the
reservoir properties that control the flow are strongly heterogeneous and
typically only available as distributions. To arrive at a formulation capable
of inferring flow patterns for regular and irregular flow from well and seismic
data, the performance of conditional normalizing flow will be analyzed on a
series of carefully designed numerical experiments. While the inferences
presented are preliminary in the context of an early CO2 leakage detection
system, the results do indicate that inferences with conditional normalizing
flows can produce high-fidelity estimates for CO2 plumes with or without
leakage. We are also confident that the inferred uncertainty is reasonable
because it correlates well with the observed errors. This uncertainty stems
from noise in the seismic data and from the lack of precise knowledge of the
reservoir&apos;s fluid flow properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gahlot_A/0/1/0/all/0/1&quot;&gt;Abhinav Prakash Gahlot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdinc_H/0/1/0/all/0/1&quot;&gt;Huseyin Tuna Erdinc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orozco_R/0/1/0/all/0/1&quot;&gt;Rafael Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_F/0/1/0/all/0/1&quot;&gt;Felix J. Herrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07184">
<title>Cross-Axis Transformer with 2D Rotary Embeddings. (arXiv:2311.07184v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07184</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite lagging behind their modal cousins in many respects, Vision
Transformers have provided an interesting opportunity to bridge the gap between
sequence modeling and image modeling. Up until now however, vision transformers
have largely been held back, due to both computational inefficiency, and lack
of proper handling of spatial dimensions. In this paper, we introduce the
Cross-Axis Transformer. CAT is a model inspired by both Axial Transformers, and
Microsoft&apos;s recent Retentive Network, that drastically reduces the required
number of floating point operations required to process an image, while
simultaneously converging faster and more accurately than the Vision
Transformers it replaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erickson_L/0/1/0/all/0/1&quot;&gt;Lily Erickson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10793">
<title>Traffic Sign Interpretation in Real Road Scene. (arXiv:2311.10793v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10793</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing traffic sign-related works are dedicated to detecting and
recognizing part of traffic signs individually, which fails to analyze the
global semantic logic among signs and may convey inaccurate traffic
instruction. Following the above issues, we propose a traffic sign
interpretation (TSI) task, which aims to interpret global semantic interrelated
traffic signs (e.g.,~driving instruction-related texts, symbols, and guide
panels) into a natural language for providing accurate instruction support to
autonomous or assistant driving. Meanwhile, we design a multi-task learning
architecture for TSI, which is responsible for detecting and recognizing
various traffic signs and interpreting them into a natural language like a
human. Furthermore, the absence of a public TSI available dataset prompts us to
build a traffic sign interpretation dataset, namely TSI-CN. The dataset
consists of real road scene images, which are captured from the highway and the
urban way in China from a driver&apos;s perspective. It contains rich location
labels of texts, symbols, and guide panels, and the corresponding natural
language description labels. Experiments on TSI-CN demonstrate that the TSI
task is achievable and the TSI architecture can interpret traffic signs from
scenes successfully even if there is a complex semantic logic among signs. The
TSI-CN dataset and the source code of the TSI architecture will be publicly
available after the revision process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chuang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_K/0/1/0/all/0/1&quot;&gt;Kai Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haozhao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Changxing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Han Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingxuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10813">
<title>A Language Agent for Autonomous Driving. (arXiv:2311.10813v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiageng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuxi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12754">
<title>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction. (arXiv:2311.12754v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12754</link>
<description rdf:parseType="Literal">&lt;p&gt;3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird&apos;s eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on nuScenes. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13373">
<title>Large Language Model is a Good Policy Teacher for Training Reinforcement Learning Agents. (arXiv:2311.13373v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13373</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have shown that Large Language Models (LLMs) can be utilized
for solving complex sequential decision-making tasks by providing high-level
instructions. However, LLM-based agents face limitations in real-time dynamic
environments due to their lack of specialization in solving specific target
problems. Moreover, the deployment of such LLM-based agents is both costly and
time-consuming in practical scenarios. In this paper, we introduce a novel
framework that addresses these challenges by training a smaller scale
specialized student agent using instructions from an LLM-based teacher agent.
By leveraging guided actions provided by the teachers, the prior knowledge of
the LLM is distilled into the local student model. Consequently, the student
agent can be trained with significantly less data. Furthermore, subsequent
training with environment feedback empowers the student agents to surpass the
capabilities of their teachers. We conducted experiments on three challenging
MiniGrid environments to evaluate the effectiveness of our framework. The
results demonstrate that our approach enhances sample efficiency and achieves
superior performance compared to baseline methods. Our code is available at
https://github.com/ZJLAB-AMMI/LLM4Teach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13534">
<title>LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13534</link>
<description rdf:parseType="Literal">&lt;p&gt;The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose LM-Cocktail which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging, where the fine-tuned language model is merged with the
pre-trained base model or the peer models from other domains through weighted
average. Despite simplicity, LM-Cocktail is surprisingly effective: the
resulted model is able to achieve a strong empirical performance in the whole
scope of general tasks while preserving a superior capacity in its targeted
domain. We conduct comprehensive experiments with LLama and BGE model on
popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shitao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peitian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xingrun Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13562">
<title>Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object. (arXiv:2311.13562v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13562</link>
<description rdf:parseType="Literal">&lt;p&gt;Image style transfer occupies an important place in both computer graphics
and computer vision. However, most current methods require reference to
stylized images and cannot individually stylize specific objects. To overcome
this limitation, we propose the &quot;Soulstyler&quot; framework, which allows users to
guide the stylization of specific objects in an image through simple textual
descriptions. We introduce a large language model to parse the text and
identify stylization goals and specific styles. Combined with a CLIP-based
semantic visual embedding encoder, the model understands and matches text and
image content. We also introduce a novel localized text-image block matching
loss that ensures that style transfer is performed only on specified target
objects, while non-target regions remain in their original style. Experimental
results demonstrate that our model is able to accurately perform style transfer
on target objects according to textual descriptions without affecting the style
of background regions. Our code will be available at
https://github.com/yisuanwang/Soulstyler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1&quot;&gt;Peng Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingbo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1&quot;&gt;Hongwu Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14713">
<title>The Rise of the AI Co-Pilot: Lessons for Design from Aviation and Beyond. (arXiv:2311.14713v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14713</link>
<description rdf:parseType="Literal">&lt;p&gt;The fast pace of advances in AI promises to revolutionize various aspects of
knowledge work, extending its influence to daily life and professional fields
alike. We advocate for a paradigm where AI is seen as a collaborative co-pilot,
working under human guidance rather than as a mere tool. Drawing from relevant
research and literature in the disciplines of Human-Computer Interaction and
Human Factors Engineering, we highlight the criticality of maintaining human
oversight in AI interactions. Reflecting on lessons from aviation, we address
the dangers of over-relying on automation, such as diminished human vigilance
and skill erosion. Our paper proposes a design approach that emphasizes active
human engagement, control, and skill enhancement in the AI partnership, aiming
to foster a harmonious, effective, and empowering human-AI relationship. We
particularly call out the critical need to design AI interaction capabilities
and software applications to enable and celebrate the primacy of human agency.
This calls for designs for human-AI partnership that cede ultimate control and
responsibility to the human user as pilot, with the AI co-pilot acting in a
well-defined supporting role.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellen_A/0/1/0/all/0/1&quot;&gt;Abigail Sellen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1&quot;&gt;Eric Horvitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16027">
<title>An HCAI Methodological Framework: Putting It Into Action to Enable Human-Centered AI. (arXiv:2311.16027v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16027</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centered AI (HCAI), as a design philosophy, advocates prioritizing
humans in designing, developing, and deploying intelligent systems, aiming to
maximize the benefits of AI technology to humans and avoid its potential
adverse effects. While HCAI has gained momentum, the lack of guidance on
methodology in its implementation makes its adoption challenging. After
assessing the needs for a methodological framework for HCAI, this paper first
proposes a comprehensive and interdisciplinary HCAI methodological framework
integrated with seven components, including design goals, design principles,
implementation approaches, design paradigms, interdisciplinary teams, methods,
and processes. THe implications of the framework are also discussed. This paper
also presents a &quot;three-layer&quot; approach to facilitate the implementation of the
framework. We believe the proposed framework is systematic and executable,
which can overcome the weaknesses in current frameworks and the challenges
currently faced in implementing HCAI. Thus, the framework can help put it into
action to develop, transfer, and implement HCAI in practice, eventually
enabling the design, development, and deployment of HCAI-based intelligent
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zaifeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dainoff_M/0/1/0/all/0/1&quot;&gt;Marvin Dainoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16133">
<title>Effective Quantization for Diffusion Models on CPUs. (arXiv:2311.16133v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16133</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have gained popularity for generating images from textual
descriptions. Nonetheless, the substantial need for computational resources
continues to present a noteworthy challenge, contributing to time-consuming
processes. Quantization, a technique employed to compress deep learning models
for enhanced efficiency, presents challenges when applied to diffusion models.
These models are notably more sensitive to quantization compared to other model
types, potentially resulting in a degradation of image quality. In this paper,
we introduce a novel approach to quantize the diffusion models by leveraging
both quantization-aware training and distillation. Our results show the
quantized models can maintain the high image quality while demonstrating the
inference efficiency on CPUs. The code is publicly available at:
https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hanwen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haihao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yiyang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xinyu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wenhua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1&quot;&gt;Kaokao Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yintong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Heng Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16153">
<title>Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications. (arXiv:2311.16153v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16153</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are increasingly deployed as the service backend
for LLM-integrated applications such as code completion and AI-powered search.
LLM-integrated applications serve as middleware to refine users&apos; queries with
domain-specific knowledge to better inform LLMs and enhance the responses.
Despite numerous opportunities and benefits, LLM-integrated applications also
introduce new attack surfaces. Understanding, minimizing, and eliminating these
emerging attack surfaces is a new area of research. In this work, we consider a
setup where the user and LLM interact via an LLM-integrated application in the
middle. We focus on the communication rounds that begin with user&apos;s queries and
end with LLM-integrated application returning responses to the queries, powered
by LLMs at the service backend. For this query-response protocol, we identify
potential vulnerabilities that can originate from the malicious application
developer or from an outsider threat initiator that is able to control the
database access, manipulate and poison data that are high-risk for the user.
Successful exploits of the identified vulnerabilities result in the users
receiving responses tailored to the intent of a threat initiator. We assess
such threats against LLM-integrated applications empowered by OpenAI GPT-3.5
and GPT-4. Our empirical results show that the threats can effectively bypass
the restrictions and moderation policies of OpenAI, resulting in users
receiving responses that contain bias, toxic content, privacy risk, and
disinformation. To mitigate those threats, we identify and define four key
properties, namely integrity, source identification, attack detectability, and
utility preservation, that need to be satisfied by a safe LLM-integrated
application. Based on these properties, we develop a lightweight,
threat-agnostic defense that mitigates both insider and outsider threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Fengqing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhangchen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Luyao Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poovendran_R/0/1/0/all/0/1&quot;&gt;Radha Poovendran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16203">
<title>ChatTraffic: Text-to-Traffic Generation via Diffusion Model. (arXiv:2311.16203v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16203</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction is one of the most significant foundations in Intelligent
Transportation Systems (ITS). Traditional traffic prediction methods rely only
on historical traffic data to predict traffic trends and face two main
challenges. 1) insensitivity to unusual events. 2) poor performance in
long-term prediction. In this work, we explore how generative models combined
with text describing the traffic system can be applied for traffic generation
and name the task Text-to-Traffic Generation (TTG). The key challenge of the
TTG task is how to associate text with the spatial structure of the road
network and traffic data for generating traffic situations. To this end, we
propose ChatTraffic, the first diffusion model for text-to-traffic generation.
To guarantee the consistency between synthetic and real data, we augment a
diffusion model with the Graph Convolutional Network (GCN) to extract spatial
correlations of traffic data. In addition, we construct a large dataset
containing text-traffic pairs for the TTG task. We benchmarked our model
qualitatively and quantitatively on the released dataset. The experimental
results indicate that ChatTraffic can generate realistic traffic situations
from the text. Our code and dataset are available at
https://github.com/ChyaZhang/ChatTraffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1&quot;&gt;Qitan Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yisheng Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piao_X/0/1/0/all/0/1&quot;&gt;Xinglin Piao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Baocai Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16680">
<title>ROSO: Improving Robotic Policy Inference via Synthetic Observations. (arXiv:2311.16680v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16680</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose the use of generative artificial intelligence (AI)
to improve zero-shot performance of a pre-trained policy by altering
observations during inference. Modern robotic systems, powered by advanced
neural networks, have demonstrated remarkable capabilities on pre-trained
tasks. However, generalizing and adapting to new objects and environments is
challenging, and fine-tuning visuomotor policies is time-consuming. To overcome
these issues we propose Robotic Policy Inference via Synthetic Observations
(ROSO). ROSO uses stable diffusion to pre-process a robot&apos;s observation of
novel objects during inference time to fit within its distribution of
observations of the pre-trained policies. This novel paradigm allows us to
transfer learned knowledge from known tasks to previously unseen scenarios,
enhancing the robot&apos;s adaptability without requiring lengthy fine-tuning. Our
experiments show that incorporating generative AI into robotic inference
significantly improves successful outcomes, finishing up to 57% of tasks
otherwise unsuccessful with the pre-trained policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyashita_Y/0/1/0/all/0/1&quot;&gt;Yusuke Miyashita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gahtidis_D/0/1/0/all/0/1&quot;&gt;Dimitris Gahtidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+La_C/0/1/0/all/0/1&quot;&gt;Colin La&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabinowicz_J/0/1/0/all/0/1&quot;&gt;Jeremy Rabinowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitner_J/0/1/0/all/0/1&quot;&gt;Jurgen Leitner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16834">
<title>Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention. (arXiv:2311.16834v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16834</link>
<description rdf:parseType="Literal">&lt;p&gt;Multivariate time series have many applications, from healthcare and
meteorology to life science. Although deep learning models have shown excellent
predictive performance for time series, they have been criticised for being
&quot;black-boxes&quot; or non-interpretable. This paper proposes a novel modular neural
network model for multivariate time series prediction that is interpretable by
construction. A recurrent neural network learns the temporal dependencies in
the data while an attention-based feature selection component selects the most
relevant features and suppresses redundant features used in the learning of the
temporal dependencies. A modular deep network is trained from the selected
features independently to show the users how features influence outcomes,
making the model interpretable. Experimental results show that this approach
can outperform state-of-the-art interpretable Neural Additive Models (NAM) and
variations thereof in both regression and classification of time series tasks,
achieving a predictive performance that is comparable to the top
non-interpretable methods for time series, LSTM and XGBoost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qiqi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloukinas_C/0/1/0/all/0/1&quot;&gt;Christos Kloukinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1&quot;&gt;Artur d&amp;#x27;Avila Garcez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17041">
<title>Efficient In-Context Learning in Vision-Language Models for Egocentric Videos. (arXiv:2311.17041v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17041</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in text-only large language models (LLMs) have
highlighted the benefit of in-context learning for adapting to new tasks with a
few demonstrations. However, extending in-context learning to large
vision-language models (VLMs) using a huge amount of naturalistic
vision-language data has shown limited success, particularly for egocentric
videos, due to high data collection costs. We propose a novel training method
$\mathbb{E}$fficient $\mathbb{I}$n-context $\mathbb{L}$earning on
$\mathbb{E}$gocentric $\mathbb{V}$ideos ($\mathbb{EILEV}$), which elicits
in-context learning in VLMs for egocentric videos without requiring massive,
naturalistic egocentric video datasets. $\mathbb{EILEV}$ involves architectural
and training data adaptations to allow the model to process contexts
interleaved with video clips and narrations, sampling of in-context examples
with clusters of similar verbs and nouns, use of data with skewed marginal
distributions with a long tail of infrequent verbs and nouns, as well as
homonyms and synonyms. Our evaluations show that $\mathbb{EILEV}$-trained
models outperform larger VLMs trained on a huge amount of naturalistic data in
in-context learning. Furthermore, they can generalize to not only
out-of-distribution, but also novel, rare egocentric videos and texts via
in-context learning, demonstrating potential for applications requiring
cost-effective training, and rapid post-deployment adaptability. Our code and
demo are available at \url{https://github.com/yukw777/EILEV}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Keunwoo Peter Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1&quot;&gt;Fengyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10887">
<title>Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder. (arXiv:2311.10887v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.10887</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of 3D self-supervised learning has witnessed
significant progress, resulting in the emergence of Multi-Modality Masked
AutoEncoders (MAE) methods that leverage both 2D images and 3D point clouds for
pre-training. However, a notable limitation of these approaches is that they do
not fully utilize the multi-view attributes inherent in 3D point clouds, which
is crucial for a deeper understanding of 3D structures. Building upon this
insight, we introduce a novel approach employing a 3D to multi-view masked
autoencoder to fully harness the multi-modal attributes of 3D point clouds. To
be specific, our method uses the encoded tokens from 3D masked point clouds to
generate original point clouds and multi-view depth images across various
poses. This approach not only enriches the model&apos;s comprehension of geometric
structures but also leverages the inherent multi-modal properties of point
clouds. Our experiments illustrate the effectiveness of the proposed method for
different tasks and under different settings. Remarkably, our method
outperforms state-of-the-art counterparts by a large margin in a variety of
downstream tasks, including 3D object classification, few-shot learning, part
segmentation, and 3D object detection. Code will be available at:
https://github.com/Zhimin-C/Multiview-MAE
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhimin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Longlong Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>