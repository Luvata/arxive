<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.04251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.13648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.09930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09994" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.12220">
<title>Automatic Recognition of Learning Resource Category in a Digital Library. (arXiv:2401.12220v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2401.12220</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital libraries often face the challenge of processing a large volume of
diverse document types. The manual collection and tagging of metadata can be a
time-consuming and error-prone task. To address this, we aim to develop an
automatic metadata extractor for digital libraries. In this work, we introduce
the Heterogeneous Learning Resources (HLR) dataset designed for document image
classification. The approach involves decomposing individual learning resources
into constituent document images (sheets). These images are then processed
through an OCR tool to extract textual representation. State-of-the-art
classifiers are employed to classify both the document image and its textual
content. Subsequently, the labels of the constituent document images are
utilized to predict the label of the overall document.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Soumya Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_D/0/1/0/all/0/1&quot;&gt;Debarshi Kumar Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Samiran Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhowmick_P/0/1/0/all/0/1&quot;&gt;Plaban Kumar Bhowmick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1&quot;&gt;Partha Pratim Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12225">
<title>Multimodal Data Curation via Object Detection and Filter Ensembles. (arXiv:2401.12225v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12225</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach for curating multimodal data that we used for our
entry in the 2023 DataComp competition filtering track. Our technique combines
object detection and weak supervision-based ensembling. In the first of two
steps in our approach, we employ an out-of-the-box zero-shot object detection
model to extract granular information and produce a variety of filter designs.
In the second step, we employ weak supervision to ensemble filtering rules.
This approach results in a 4% performance improvement when compared to the
best-performing baseline, producing the top-ranking position in the small scale
track at the time of writing. Furthermore, in the medium scale track, we
achieve a noteworthy 4.2% improvement over the baseline by simply ensembling
existing baselines with weak supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzu-Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Changho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_S/0/1/0/all/0/1&quot;&gt;Sui Jiet Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1&quot;&gt;Dyah Adila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12244">
<title>Large-scale Reinforcement Learning for Diffusion Models. (arXiv:2401.12244v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models are a class of deep generative models that
have demonstrated an impressive capacity for high-quality image generation.
However, these models are susceptible to implicit biases that arise from
web-scale text-image training pairs and may inaccurately model aspects of
images we care about. This can result in suboptimal samples, model bias, and
images that do not align with human ethics and preferences. In this paper, we
present an effective scalable algorithm to improve diffusion models using
Reinforcement Learning (RL) across a diverse set of reward functions, such as
human preference, compositionality, and fairness over millions of images. We
illustrate how our approach substantially outperforms existing methods for
aligning diffusion models with human preferences. We further illustrate how
this substantially improves pretrained Stable Diffusion (SD) models, generating
samples that are preferred by humans 80.3% of the time over those from the base
SD model while simultaneously improving both the composition and diversity of
generated samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzeng_E/0/1/0/all/0/1&quot;&gt;Eric Tzeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kislyuk_D/0/1/0/all/0/1&quot;&gt;Dmitry Kislyuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12275">
<title>Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12275</link>
<description rdf:parseType="Literal">&lt;p&gt;Social robot navigation can be helpful in various contexts of daily life but
requires safe human-robot interactions and efficient trajectory planning. While
modeling pairwise relations has been widely studied in multi-agent interacting
systems, the ability to capture larger-scale group-wise activities is limited.
In this paper, we propose a systematic relational reasoning approach with
explicit inference of the underlying dynamically evolving relational
structures, and we demonstrate its effectiveness for multi-agent trajectory
prediction and social robot navigation. In addition to the edges between pairs
of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect
multiple nodes to enable group-wise reasoning in an unsupervised manner. Our
approach infers dynamically evolving relation graphs and hypergraphs to capture
the evolution of relations, which the trajectory predictor employs to generate
future states. Meanwhile, we propose to regularize the sharpness and sparsity
of the learned relations and the smoothness of the relation evolution, which
proves to enhance training stability and model performance. The proposed
approach is validated on synthetic crowd simulations and real-world benchmark
datasets. Experiments demonstrate that the approach infers reasonable relations
and achieves state-of-the-art prediction performance. In addition, we present a
deep reinforcement learning (DRL) framework for social robot navigation, which
incorporates relational reasoning and trajectory prediction systematically. In
a group-based crowd simulation, our method outperforms the strongest baseline
by a significant margin in terms of safety, efficiency, and social compliance
in dense, interactive scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1&quot;&gt;Chuanbo Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hengbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dax_V/0/1/0/all/0/1&quot;&gt;Victoria Dax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12340">
<title>Contrastive Learning and Cycle Consistency-based Transductive Transfer Learning for Target Annotation. (arXiv:2401.12340v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12340</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating automatic target recognition (ATR) is a highly challenging task,
primarily due to the unavailability of labeled data in the target domain.
Hence, it is essential to construct an optimal target domain classifier by
utilizing the labeled information of the source domain images. The transductive
transfer learning (TTL) method that incorporates a CycleGAN-based unpaired
domain translation network has been previously proposed in the literature for
effective ATR annotation. Although this method demonstrates great potential for
ATR, it severely suffers from lower annotation performance, higher Fr\&apos;echet
Inception Distance (FID) score, and the presence of visual artifacts in the
synthetic images. To address these issues, we propose a hybrid contrastive
learning base unpaired domain translation (H-CUT) network that achieves a
significantly lower FID score. It incorporates both attention and entropy to
emphasize the domain-specific region, a noisy feature mixup module to generate
high variational synthetic negative patches, and a modulated noise contrastive
estimation (MoNCE) loss to reweight all negative patches using optimal
transport for better performance. Our proposed contrastive learning and
cycle-consistency-based TTL (C3TTL) framework consists of two H-CUT networks
and two classifiers. It simultaneously optimizes cycle-consistency, MoNCE, and
identity losses. In C3TTL, two H-CUT networks have been employed through a
bijection mapping to feed the reconstructed source domain images into a
pretrained classifier to guide the optimal target domain classifier. Extensive
experimental analysis conducted on three ATR datasets demonstrates that the
proposed C3TTL method is effective in annotating civilian and military
vehicles, as well as ship targets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sami_S/0/1/0/all/0/1&quot;&gt;Shoaib Meraj Sami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mahedi Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1&quot;&gt;Nasser M. Nasrabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1&quot;&gt;Raghuveer Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12344">
<title>OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection. (arXiv:2401.12344v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12344</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the revolutionary impact of AI and the development of locally trained
algorithms, achieving widespread generalized learning from multi-modal data in
medical AI remains a significant challenge. This gap hinders the practical
deployment of scalable medical AI solutions. Addressing this challenge, our
research contributes a self-supervised robust machine learning framework,
OCT-SelfNet, for detecting eye diseases using optical coherence tomography
(OCT) images. In this work, various data sets from various institutions are
combined enabling a more comprehensive range of representation. Our method
addresses the issue using a two-phase training approach that combines
self-supervised pretraining and supervised fine-tuning with a mask autoencoder
based on the SwinV2 backbone by providing a solution for real-world clinical
deployment. Extensive experiments on three datasets with different encoder
backbones, low data settings, unseen data settings, and the effect of
augmentation show that our method outperforms the baseline model, Resnet-50 by
consistently attaining AUC-ROC performance surpassing 77% across all tests,
whereas the baseline model exceeds 54%. Moreover, in terms of the AUC-PR
metric, our proposed method exceeded 42%, showcasing a substantial increase of
at least 10% in performance compared to the baseline, which exceeded only 33%.
This contributes to our understanding of our approach&apos;s potential and
emphasizes its usefulness in clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannat_F/0/1/0/all/0/1&quot;&gt;Fatema-E Jannat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_S/0/1/0/all/0/1&quot;&gt;Sina Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Minhaj Nur Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1&quot;&gt;Hamed Tabkhi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12350">
<title>Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge. (arXiv:2401.12350v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12350</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) has become the de-facto approach for
designing accurate and efficient networks for edge devices. Since models are
typically quantized for edge deployment, recent work has investigated
quantization-aware NAS (QA-NAS) to search for highly accurate and efficient
quantized models. However, existing QA-NAS approaches, particularly few-bit
mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently,
QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this
work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale
tasks by leveraging the block-wise formulation introduced by block-wise NAS. We
demonstrate strong results for the semantic segmentation task on the Cityscapes
dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than
DeepLabV3 (INT8) without compromising task performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_H/0/1/0/all/0/1&quot;&gt;Hiram Rayo Torres Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogel_S/0/1/0/all/0/1&quot;&gt;Sebastian Vogel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waterlaat_N/0/1/0/all/0/1&quot;&gt;Nick van de Waterlaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jancura_P/0/1/0/all/0/1&quot;&gt;Pavol Jancura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12414">
<title>Icy Moon Surface Simulation and Stereo Depth Estimation for Sampling Autonomy. (arXiv:2401.12414v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12414</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling autonomy for icy moon lander missions requires understanding of
topographic and photometric properties of the sampling terrain. Unavailability
of high resolution visual datasets (either bird-eye view or point-of-view from
a lander) is an obstacle for selection, verification or development of
perception systems. We attempt to alleviate this problem by: 1) proposing
Graphical Utility for Icy moon Surface Simulations (GUISS) framework, for
versatile stereo dataset generation that spans the spectrum of bulk photometric
properties, and 2) focusing on a stereo-based visual perception system and
evaluating both traditional and deep learning-based algorithms for depth
estimation from stereo matching. The surface reflectance properties of icy moon
terrains (Enceladus and Europa) are inferred from multispectral datasets of
previous missions. With procedural terrain generation and physically valid
illumination sources, our framework can fit a wide range of hypotheses with
respect to visual representations of icy moon terrains. This is followed by a
study over the performance of stereo matching algorithms under different visual
hypotheses. Finally, we emphasize the standing challenges to be addressed for
simulating perception data assets for icy moons such as Enceladus and Europa.
Our code can be found here: https://github.com/nasa-jpl/guiss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhaskara_R/0/1/0/all/0/1&quot;&gt;Ramchander Bhaskara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1&quot;&gt;Georgios Georgakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nash_J/0/1/0/all/0/1&quot;&gt;Jeremy Nash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cameron_M/0/1/0/all/0/1&quot;&gt;Marissa Cameron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowkett_J/0/1/0/all/0/1&quot;&gt;Joseph Bowkett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansar_A/0/1/0/all/0/1&quot;&gt;Adnan Ansar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majji_M/0/1/0/all/0/1&quot;&gt;Manoranjan Majji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_P/0/1/0/all/0/1&quot;&gt;Paul Backes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12419">
<title>Multi-modal News Understanding with Professionally Labelled Videos (ReutersViLNews). (arXiv:2401.12419v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12419</link>
<description rdf:parseType="Literal">&lt;p&gt;While progress has been made in the domain of video-language understanding,
current state-of-the-art algorithms are still limited in their ability to
understand videos at high levels of abstraction, such as news-oriented videos.
Alternatively, humans easily amalgamate information from video and language to
infer information beyond what is visually observable in the pixels. An example
of this is watching a news story, where the context of the event can play as
big of a role in understanding the story as the event itself. Towards a
solution for designing this ability in algorithms, we present a large-scale
analysis on an in-house dataset collected by the Reuters News Agency, called
Reuters Video-Language News (ReutersViLNews) dataset which focuses on
high-level video-language understanding with an emphasis on long-form news. The
ReutersViLNews Dataset consists of long-form news videos collected and labeled
by news industry professionals over several years and contains prominent news
reporting from around the world. Each video involves a single story and
contains action shots of the actual event, interviews with people associated
with the event, footage from nearby areas, and more. ReutersViLNews dataset
contains videos from seven subject categories: disaster, finance,
entertainment, health, politics, sports, and miscellaneous with annotations
from high-level to low-level, title caption, visual video description,
high-level story description, keywords, and location. We first present an
analysis of the dataset statistics of ReutersViLNews compared to previous
datasets. Then we benchmark state-of-the-art approaches for four different
video-language tasks. The results suggest that news-oriented videos are a
substantial challenge for current video-language understanding algorithms and
we conclude by providing future directions in designing approaches to solve the
ReutersViLNews dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1&quot;&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1&quot;&gt;Matthew Kowal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niknam_Y/0/1/0/all/0/1&quot;&gt;Yasmin Niknam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moyano_D/0/1/0/all/0/1&quot;&gt;Diana Moyano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehdi_S/0/1/0/all/0/1&quot;&gt;Shayaan Mehdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pito_R/0/1/0/all/0/1&quot;&gt;Richard Pito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knopke_I/0/1/0/all/0/1&quot;&gt;Ian Knopke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocak_S/0/1/0/all/0/1&quot;&gt;Sedef Akinli Kocak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1&quot;&gt;Leonid Sigal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1&quot;&gt;Yalda Mohsenzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12421">
<title>AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space. (arXiv:2401.12421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12421</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised domain adaptation (SSDA) presents a critical hurdle in
computer vision, especially given the frequent scarcity of labeled data in
real-world settings. This scarcity often causes foundation models, trained on
extensive datasets, to underperform when applied to new domains. AdaEmbed, our
newly proposed methodology for SSDA, offers a promising solution to these
challenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates
the transfer of knowledge from a labeled source domain to an unlabeled target
domain by learning a shared embedding space. By generating accurate and uniform
pseudo-labels based on the established embedding space, the model overcomes the
limitations of conventional SSDA, thus enhancing performance significantly. Our
method&apos;s effectiveness is validated through extensive experiments on benchmark
datasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed
consistently outperforms all the baselines, setting a new state of the art for
SSDA. With its straightforward implementation and high data efficiency,
AdaEmbed stands out as a robust and pragmatic solution for real-world
scenarios, where labeled data is scarce. To foster further research and
application in this area, we are sharing the codebase of our unified framework
for semi-supervised domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottaghi_A/0/1/0/all/0/1&quot;&gt;Ali Mottaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamal_M/0/1/0/all/0/1&quot;&gt;Mohammad Abdullah Jamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Serena Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohareri_O/0/1/0/all/0/1&quot;&gt;Omid Mohareri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12422">
<title>InverseMatrixVT3D: An Efficient Projection Matrix-Based Approach for 3D Occupancy Prediction. (arXiv:2401.12422v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12422</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces InverseMatrixVT3D, an efficient method for transforming
multi-view image features into 3D feature volumes for 3D semantic occupancy
prediction. Existing methods for constructing 3D volumes often rely on depth
estimation, device-specific operators, or transformer queries, which hinders
the widespread adoption of 3D occupancy models. In contrast, our approach
leverages two projection matrices to store the static mapping relationships and
matrix multiplications to efficiently generate global Bird&apos;s Eye View (BEV)
features and local 3D feature volumes. Specifically, we achieve this by
performing matrix multiplications between multi-view image feature maps and two
sparse projection matrices. We introduce a sparse matrix handling technique for
the projection matrices to optimise GPU memory usage. Moreover, a global-local
attention fusion module is proposed to integrate the global BEV features with
the local 3D feature volumes to obtain the final 3D volume. We also employ a
multi-scale supervision mechanism to further enhance performance. Comprehensive
experiments on the nuScenes dataset demonstrate the simplicity and
effectiveness of our method. The code will be made available
at:https://github.com/DanielMing123/InverseMatrixVT3D
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrio_J/0/1/0/all/0/1&quot;&gt;Julie Stephany Berrio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1&quot;&gt;Mao Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worrall_S/0/1/0/all/0/1&quot;&gt;Stewart Worrall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12425">
<title>The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12425</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language models (VLMs) excel in zero-shot recognition but exhibit
drastically imbalanced performance across visual concepts. For example, CLIP,
despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields
$&amp;lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because
these concepts are under-represented in VLMs&apos; imbalanced pretraining data. Yet,
assessing this imbalance is challenging as it is non-trivial to calculate the
frequency of specific concepts within VLMs&apos; large-scale pretraining data. Our
work makes the first attempt to measure the concept frequency by analyzing
pretraining texts. We use off-the-shelf language models to help count relevant
texts that contain synonyms of the given concepts and resolve linguistic
ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit
long-tailed concept distributions, which strongly correlate with per-class
accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and
text-to-image generators, also struggle with the rare concepts identified by
our method. To mitigate VLMs&apos; imbalanced performance in zero-shot recognition,
we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs
using the original class names, REAL uses their most frequent synonyms found in
VLMs&apos; pretraining texts. This already outperforms human-engineered and
LLM-generated prompts over nine benchmark datasets, likely because VLMs have
seen more images associated with the frequently used synonyms. Second, REAL
uses all the concept synonyms to retrieve a small, class-balanced set of
pretraining data to train a robust classifier. REAL surpasses the recent
retrieval-augmented solution REACT, using 400x less storage and 10,000x less
training time!
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1&quot;&gt;Shubham Parashar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiqiu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiangjue Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1&quot;&gt;James Caverlee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shu Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12433">
<title>A Novel Garment Transfer Method Supervised by Distilled Knowledge of Virtual Try-on Model. (arXiv:2401.12433v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12433</link>
<description rdf:parseType="Literal">&lt;p&gt;When a shopper chooses garments online, garment transfer technology wears the
garment from the model image onto the shopper&apos;s image, allowing the shopper to
decide whether the garment is suitable for them. As garment transfer leverages
wild and cheap person image as garment condition, it has attracted tremendous
community attention and holds vast commercial potential. However, since the
ground truth of garment transfer is almost unavailable in reality, previous
studies have treated garment transfer as either pose transfer or garment-pose
disentanglement, and trained garment transfer in self-supervised learning, yet
do not cover garment transfer intentions completely. Therefore, the training
supervising the garment transfer is a rock-hard issue. Notably, virtual try-on
technology has exhibited superior performance using self-supervised learning.
We supervise the garment transfer training via knowledge distillation from
virtual try-on. Specifically, we first train the transfer parsing reasoning
model at multi-phases to provide shape guidance for downstream tasks. The
transfer parsing reasoning model learns the response and feature knowledge from
the try-on parsing reasoning model and absorbs the hard knowledge from the
ground truth. By leveraging the warping knowledge from virtual try-on, we
estimate a progressive flow to precisely warp the garment by learning the shape
and content correspondence. To enhance transfer realism, we propose a
well-designed arm regrowth task to infer exposed skin pixel content.
Experiments demonstrate that our method has state-of-the-art performance in
transferring garments between person compared with other virtual try-on and
garment transfer methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_N/0/1/0/all/0/1&quot;&gt;Naiyu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lemiao Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuyou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kerui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jianrong Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12438">
<title>Secure Federated Learning Approaches to Diagnosing COVID-19. (arXiv:2401.12438v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12438</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent pandemic has underscored the importance of accurately diagnosing
COVID-19 in hospital settings. A major challenge in this regard is
differentiating COVID-19 from other respiratory illnesses based on chest
X-rays, compounded by the restrictions of HIPAA compliance which limit the
comparison of patient X-rays. This paper introduces a HIPAA-compliant model to
aid in the diagnosis of COVID-19, utilizing federated learning. Federated
learning is a distributed machine learning approach that allows for algorithm
training across multiple decentralized devices using local data samples,
without the need for data sharing. Our model advances previous efforts in chest
X-ray diagnostic models. We examined leading models from established
competitions in this domain and developed our own models tailored to be
effective with specific hospital data. Considering the model&apos;s operation in a
federated learning context, we explored the potential impact of biased data
updates on the model&apos;s performance. To enhance hospital understanding of the
model&apos;s decision-making process and to verify that the model is not focusing on
irrelevant features, we employed a visualization technique that highlights key
features in chest X-rays indicative of a positive COVID-19 diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adhikari_R/0/1/0/all/0/1&quot;&gt;Rittika Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Settles_C/0/1/0/all/0/1&quot;&gt;Christopher Settles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12439">
<title>MAST: Video Polyp Segmentation with a Mixture-Attention Siamese Transformer. (arXiv:2401.12439v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12439</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate segmentation of polyps from colonoscopy videos is of great
significance to polyp treatment and early prevention of colorectal cancer.
However, it is challenging due to the difficulties associated with modelling
long-range spatio-temporal relationships within a colonoscopy video. In this
paper, we address this challenging task with a novel Mixture-Attention Siamese
Transformer (MAST), which explicitly models the long-range spatio-temporal
relationships with a mixture-attention mechanism for accurate polyp
segmentation. Specifically, we first construct a Siamese transformer
architecture to jointly encode paired video frames for their feature
representations. We then design a mixture-attention module to exploit the
intra-frame and inter-frame correlations, enhancing the features with rich
spatio-temporal relationships. Finally, the enhanced features are fed to two
parallel decoders for predicting the segmentation maps. To the best of our
knowledge, our MAST is the first transformer model dedicated to video polyp
segmentation. Extensive experiments on the large-scale SUN-SEG benchmark
demonstrate the superior performance of MAST in comparison with the
cutting-edge competitors. Our code is publicly available at
https://github.com/Junqing-Yang/MAST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Geng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1&quot;&gt;Xiaozhou Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Huan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hengfei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12447">
<title>NIV-SSD: Neighbor IoU-Voting Single-Stage Object Detector From Point Cloud. (arXiv:2401.12447v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12447</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous single-stage detectors typically suffer the misalignment between
localization accuracy and classification confidence. To solve the misalignment
problem, we introduce a novel rectification method named neighbor IoU-voting
(NIV) strategy. Typically, classification and regression are treated as
separate branches, making it challenging to establish a connection between
them. Consequently, the classification confidence cannot accurately reflect the
regression quality. NIV strategy can serve as a bridge between classification
and regression branches by calculating two types of statistical data from the
regression output to correct the classification confidence. Furthermore, to
alleviate the imbalance of detection accuracy for complete objects with dense
points (easy objects) and incomplete objects with sparse points (difficult
objects), we propose a new data augmentation scheme named object resampling. It
undersamples easy objects and oversamples difficult objects by randomly
transforming part of easy objects into difficult objects. Finally, combining
the NIV strategy and object resampling augmentation, we design an efficient
single-stage detector termed NIV-SSD. Extensive experiments on several datasets
indicate the effectiveness of the NIV strategy and the competitive performance
of the NIV-SSD detector. The code will be available at
https://github.com/Say2L/NIV-SSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12451">
<title>Methods and strategies for improving the novel view synthesis quality of neural radiation field. (arXiv:2401.12451v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12451</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a
scene from 2D images and synthesize realistic novel view images. This
technology has received widespread attention from the industry and has good
application prospects. In response to the problem that the rendering quality of
NeRF images needs to be improved, many researchers have proposed various
methods to improve the rendering quality in the past three years. The latest
relevant papers are classified and reviewed, the technical principles behind
quality improvement are analyzed, and the future evolution direction of quality
improvement methods is discussed. This study can help researchers quickly
understand the current state and evolutionary context of technology in this
field, which is helpful in inspiring the development of more efficient
algorithms and promoting the application of NeRF technology in related fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Ming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xing Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yanna Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12452">
<title>Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration. (arXiv:2401.12452v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12452</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel self-supervised learning framework for
enhancing 3D perception in autonomous driving scenes. Specifically, our
approach, named NCLR, focuses on 2D-3D neural calibration, a novel pretext task
that estimates the rigid transformation aligning camera and LiDAR coordinate
systems. First, we propose the learnable transformation alignment to bridge the
domain gap between image and point cloud data, converting features into a
unified representation space for effective comparison and matching. Second, we
identify the overlapping area between the image and point cloud with the fused
features. Third, we establish dense 2D-3D correspondences to estimate the rigid
transformation. The framework not only learns fine-grained matching from points
to pixels but also achieves alignment of the image and point cloud at a
holistic level, understanding their relative pose. We demonstrate NCLR&apos;s
efficacy by applying the pre-trained backbone to downstream tasks, such as
LiDAR-based 3D semantic segmentation, object detection, and panoptic
segmentation. Comprehensive experiments on various datasets illustrate the
superiority of NCLR over existing self-supervised methods. The results confirm
that joint learning from different modalities significantly enhances the
network&apos;s understanding abilities and effectiveness of learned representation.
Code will be available at \url{https://github.com/Eaphan/NCLR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Siyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinjian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Guangming Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12456">
<title>Exploration and Improvement of Nerf-based 3D Scene Editing Techniques. (arXiv:2401.12456v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12456</link>
<description rdf:parseType="Literal">&lt;p&gt;NeRF&apos;s high-quality scene synthesis capability was quickly accepted by
scholars in the years after it was proposed, and significant progress has been
made in 3D scene representation and synthesis. However, the high computational
cost limits intuitive and efficient editing of scenes, making NeRF&apos;s
development in the scene editing field facing many challenges. This paper
reviews the preliminary explorations of scholars on NeRF in the scene or object
editing field in recent years, mainly changing the shape and texture of scenes
or objects in new synthesized scenes; through the combination of residual
models such as GaN and Transformer with NeRF, the generalization ability of
NeRF scene editing has been further expanded, including realizing real-time new
perspective editing feedback, multimodal editing of text synthesized 3D scenes,
4D synthesis performance, and in-depth exploration in light and shadow editing,
initially achieving optimization of indirect touch editing and detail
representation in complex scenes. Currently, most NeRF editing methods focus on
the touch points and materials of indirect points, but when dealing with more
complex or larger 3D scenes, it is difficult to balance accuracy, breadth,
efficiency, and quality. Overcoming these challenges may become the direction
of future NeRF 3D scene editing technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Ming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xing Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12471">
<title>Zero Shot Open-ended Video Inference. (arXiv:2401.12471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12471</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot open-ended inference on untrimmed videos poses a significant
challenge, especially when no annotated data is utilized to navigate the
inference direction. In this work, we aim to address this underexplored domain
by introducing an adaptable framework that efficiently combines both the frozen
vision-language (VL) model and off-the-shelf large language model (LLM) for
conducting zero-shot open-ended inference tasks without requiring any
additional training or fine-tuning. Our comprehensive experiments span various
video action datasets for goal inference and action recognition tasks. The
results demonstrate the framework&apos;s superior performance in goal inference
compared to conventional vision-language models in open-ended and close-ended
scenarios. Notably, the proposed framework exhibits the capability to
generalize effectively to action recognition tasks, underscoring its
versatility and potential contributions to advancing the video-based zero-shot
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keat_E/0/1/0/all/0/1&quot;&gt;Ee Yeo Keat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zhang Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matyasko_A/0/1/0/all/0/1&quot;&gt;Alexander Matyasko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1&quot;&gt;Basura Fernando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12479">
<title>TD^2-Net: Toward Denoising and Debiasing for Dynamic Scene Graph Generation. (arXiv:2401.12479v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12479</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic scene graph generation (SGG) focuses on detecting objects in a video
and determining their pairwise relationships. Existing dynamic SGG methods
usually suffer from several issues, including 1) Contextual noise, as some
frames might contain occluded and blurred objects. 2) Label bias, primarily due
to the high imbalance between a few positive relationship samples and numerous
negative ones. Additionally, the distribution of relationships exhibits a
long-tailed pattern. To address the above problems, in this paper, we introduce
a network named TD$^2$-Net that aims at denoising and debiasing for dynamic
SGG. Specifically, we first propose a denoising spatio-temporal transformer
module that enhances object representation with robust contextual information.
This is achieved by designing a differentiable Top-K object selector that
utilizes the gumbel-softmax sampling strategy to select the relevant
neighborhood for each object. Second, we introduce an asymmetrical reweighting
loss to relieve the issue of label bias. This loss function integrates
asymmetry focusing factors and the volume of samples to adjust the weights
assigned to individual samples. Systematic experimental results demonstrate the
superiority of our proposed TD$^2$-Net over existing state-of-the-art
approaches on Action Genome databases. In more detail, TD$^2$-Net outperforms
the second-best competitors by 12.7 \% on mean-Recall@10 for predicate
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zuopeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yaqi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12480">
<title>Explore Synergistic Interaction Across Frames for Interactive Video Object Segmentation. (arXiv:2401.12480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12480</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive Video Object Segmentation (iVOS) is a challenging task that
requires real-time human-computer interaction. To improve the user experience,
it is important to consider the user&apos;s input habits, segmentation quality,
running time and memory consumption.However, existing methods compromise user
experience with single input mode and slow running speed. Specifically, these
methods only allow the user to interact with one single frame, which limits the
expression of the user&apos;s intent.To overcome these limitations and better align
with people&apos;s usage habits, we propose a framework that can accept multiple
frames simultaneously and explore synergistic interaction across frames (SIAF).
Concretely, we designed the Across-Frame Interaction Module that enables users
to annotate different objects freely on multiple frames. The AFI module will
migrate scribble information among multiple interactive frames and generate
multi-frame masks. Additionally, we employ the id-queried mechanism to process
multiple objects in batches. Furthermore, for a more efficient propagation and
lightweight model, we design a truncated re-propagation strategy to replace the
previous multi-round fusion module, which employs an across-round memory that
stores important interaction information. Our SwinB-SIAF achieves new
state-of-the-art performance on DAVIS 2017 (89.6%, J&amp;amp;F@60). Moreover, our
R50-SIAF is more than 3 faster than the state-of-the-art competitor under
challenging multi-object scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kexin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jun Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12488">
<title>An Automated Real-Time Approach for Image Processing and Segmentation of Fluoroscopic Images and Videos Using a Single Deep Learning Network. (arXiv:2401.12488v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12488</link>
<description rdf:parseType="Literal">&lt;p&gt;Image segmentation in total knee arthroplasty is crucial for precise
preoperative planning and accurate implant positioning, leading to improved
surgical outcomes and patient satisfaction. The biggest challenges of image
segmentation in total knee arthroplasty include accurately delineating complex
anatomical structures, dealing with image artifacts and noise, and developing
robust algorithms that can handle anatomical variations and pathologies
commonly encountered in patients. The potential of using machine learning for
image segmentation in total knee arthroplasty lies in its ability to improve
segmentation accuracy, automate the process, and provide real-time assistance
to surgeons, leading to enhanced surgical planning, implant placement, and
patient outcomes. This paper proposes a methodology to use deep learning for
robust and real-time total knee arthroplasty image segmentation. The deep
learning model, trained on a large dataset, demonstrates outstanding
performance in accurately segmenting both the implanted femur and tibia,
achieving an impressive mean-Average-Precision (mAP) of 88.83 when compared to
the ground truth while also achieving a real-time segmented speed of 20 frames
per second (fps). We have introduced a novel methodology for segmenting
implanted knee fluoroscopic or x-ray images that showcases remarkable levels of
accuracy and speed, paving the way for various potential extended applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Viet Dung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+LaCour_M/0/1/0/all/0/1&quot;&gt;Michael T. LaCour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Komistek_R/0/1/0/all/0/1&quot;&gt;Richard D. Komistek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12503">
<title>Small Language Model Meets with Reinforced Vision Vocabulary. (arXiv:2401.12503v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12503</link>
<description rdf:parseType="Literal">&lt;p&gt;Playing Large Vision Language Models (LVLMs) in 2023 is trendy among the AI
community. However, the relatively large number of parameters (more than 7B) of
popular LVLMs makes it difficult to train and deploy on consumer GPUs,
discouraging many researchers with limited resources. Imagine how cool it would
be to experience all the features of current LVLMs on an old GTX1080ti (our
only game card). Accordingly, we present Vary-toy in this report, a small-size
Vary along with Qwen-1.8B as the base ``large&apos;&apos; language model. In Vary-toy, we
introduce an improved vision vocabulary, allowing the model to not only possess
all features of Vary but also gather more generality. Specifically, we replace
negative samples of natural images with positive sample data driven by object
detection in the procedure of generating vision vocabulary, more sufficiently
utilizing the capacity of the vocabulary network and enabling it to efficiently
encode visual information corresponding to natural objects. For experiments,
Vary-toy can achieve 65.6% ANLS on DocVQA, 59.1% accuracy on ChartQA, 88.1%
accuracy on RefCOCO, and 29% on MMVet. The code will be publicly available on
the homepage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Haoran Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingyu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinyue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1&quot;&gt;En Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianjian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chunrui Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12507">
<title>Open-Set Facial Expression Recognition. (arXiv:2401.12507v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12507</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) models are typically trained on datasets
with a fixed number of seven basic classes. However, recent research works
point out that there are far more expressions than the basic ones. Thus, when
these models are deployed in the real world, they may encounter unknown
classes, such as compound expressions that cannot be classified into existing
basic classes. To address this issue, we propose the open-set FER task for the
first time. Though there are many existing open-set recognition methods, we
argue that they do not work well for open-set FER because FER data are all
human faces with very small inter-class distances, which makes the open-set
samples very similar to close-set samples. In this paper, we are the first to
transform the disadvantage of small inter-class distance into an advantage by
proposing a new way for open-set FER. Specifically, we find that small
inter-class distance allows for sparsely distributed pseudo labels of open-set
samples, which can be viewed as symmetric noisy labels. Based on this novel
observation, we convert the open-set FER to a noisy label detection problem. We
further propose a novel method that incorporates attention map consistency and
cycle training to detect the open-set samples. Extensive experiments on various
FER datasets demonstrate that our method clearly outperforms state-of-the-art
open-set recognition methods by large margins. Code is available at
https://github.com/zyh-uaiaaaa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yue Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuannan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lixiong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weihong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12511">
<title>Convolutional Initialization for Data-Efficient Vision Transformers. (arXiv:2401.12511v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12511</link>
<description rdf:parseType="Literal">&lt;p&gt;Training vision transformer networks on small datasets poses challenges. In
contrast, convolutional neural networks (CNNs) can achieve state-of-the-art
performance by leveraging their architectural inductive bias. In this paper, we
investigate whether this inductive bias can be reinterpreted as an
initialization bias within a vision transformer network. Our approach is
motivated by the finding that random impulse filters can achieve almost
comparable performance to learned filters in CNNs. We introduce a novel
initialization strategy for transformer networks that can achieve comparable
performance to CNNs on small datasets while preserving its architectural
flexibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianqiao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1&quot;&gt;Simon Lucey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12513">
<title>Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR. (arXiv:2401.12513v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12513</link>
<description rdf:parseType="Literal">&lt;p&gt;The capacity to isolate and recognize individual characters from facsimile
images of papyrus manuscripts yields rich opportunities for digital analysis.
For this reason the `ICDAR 2023 Competition on Detection and Recognition of
Greek Letters on Papyri&apos; was held as part of the 17th International Conference
on Document Analysis and Recognition. This paper discusses our submission to
the competition. We used an ensemble of YOLOv8 models to detect and classify
individual characters and employed two different approaches for refining the
character predictions, including a transformer based DeiT approach and a
ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a
self-supervised learning method. Our submission won the recognition challenge
with a mAP of 42.2%, and was runner-up in the detection challenge with a mean
average precision (mAP) of 51.4%. At the more relaxed intersection over union
threshold of 0.5, we achieved the highest mean average precision and mean
average recall results for both detection and classification. We ran our
prediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to
illustrate the utility of our approach, and we release the results publicly in
multiple formats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turnbull_R/0/1/0/all/0/1&quot;&gt;Robert Turnbull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannix_E/0/1/0/all/0/1&quot;&gt;Evelyn Mannix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12535">
<title>Self-Supervised Vision Transformers Are Efficient Segmentation Learners for Imperfect Labels. (arXiv:2401.12535v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12535</link>
<description rdf:parseType="Literal">&lt;p&gt;This study demonstrates a cost-effective approach to semantic segmentation
using self-supervised vision transformers (SSVT). By freezing the SSVT backbone
and training a lightweight segmentation head, our approach effectively utilizes
imperfect labels, thereby improving robustness to label imperfections.
Empirical experiments show significant performance improvements over existing
methods for various annotation types, including scribble, point-level, and
image-level labels. The research highlights the effectiveness of
self-supervised vision transformers in dealing with imperfect labels, providing
a practical and efficient solution for semantic segmentation while reducing
annotation costs. Through extensive experiments, we confirm that our method
outperforms baseline models for all types of imperfect labels. Especially under
the zero-shot vision-language-model-based label, our model exhibits 11.5\%p
performance gain compared to the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Seoungyoon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12561">
<title>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene Reconstruction. (arXiv:2401.12561v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12561</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing deformable tissues from endoscopic stereo videos is essential
in many downstream surgical applications. However, existing methods suffer from
slow inference speed, which greatly limits their practical use. In this paper,
we introduce EndoGaussian, a real-time surgical scene reconstruction framework
that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical
scenes as canonical Gaussians and a time-dependent deformation field, which
predicts Gaussian deformations at novel timestamps. Due to the efficient
Gaussian representation and parallel rendering pipeline, our framework
significantly accelerates the rendering speed compared to previous methods. In
addition, we design the deformation field as the combination of a lightweight
encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian
tracking with a minor rendering burden. Furthermore, we design a holistic
Gaussian initialization method to fully leverage the surface distribution
prior, achieved by searching informative points from across the input image
sequence. Experiments on public endoscope datasets demonstrate that our method
can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain)
while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and
the fastest training speed (within 2 min/scene), showing significant promise
for intraoperative surgery applications. Code is available at:
\url{https://yifliu3.github.io/EndoGaussian/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12568">
<title>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis. (arXiv:2401.12568v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12568</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking face synthesis driven by audio is one of the current research
hotspots in the fields of multidimensional signal processing and multimedia.
Neural Radiance Field (NeRF) has recently been brought to this research field
in order to enhance the realism and 3D effect of the generated faces. However,
most existing NeRF-based methods either burden NeRF with complex learning tasks
while lacking methods for supervised multimodal feature fusion, or cannot
precisely map audio to the facial region related to speech movements. These
reasons ultimately result in existing methods generating inaccurate lip shapes.
This paper moves a portion of NeRF learning tasks ahead and proposes a talking
face synthesis method via NeRF with attention-based disentanglement (NeRF-AD).
In particular, an Attention-based Disentanglement module is introduced to
disentangle the face into Audio-face and Identity-face using speech-related
facial action unit (AU) information. To precisely regulate how audio affects
the talking face, we only fuse the Audio-face with audio feature. In addition,
AU information is also utilized to supervise the fusion of these two
modalities. Extensive qualitative and quantitative experiments demonstrate that
our NeRF-AD outperforms state-of-the-art methods in generating realistic
talking face videos, including image quality and lip synchronization. To view
video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_C/0/1/0/all/0/1&quot;&gt;Chongke Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoxing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhilei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12587">
<title>Fast Implicit Neural Representation Image Codec in Resource-limited Devices. (arXiv:2401.12587v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12587</link>
<description rdf:parseType="Literal">&lt;p&gt;Displaying high-quality images on edge devices, such as augmented reality
devices, is essential for enhancing the user experience. However, these devices
often face power consumption and computing resource limitations, making it
challenging to apply many deep learning-based image compression algorithms in
this field. Implicit Neural Representation (INR) for image compression is an
emerging technology that offers two key benefits compared to cutting-edge
autoencoder models: low computational complexity and parameter-free decoding.
It also outperforms many traditional and early neural compression methods in
terms of quality. In this study, we introduce a new Mixed Autoregressive Model
(MARM) to significantly reduce the decoding time for the current INR codec,
along with a new synthesis network to enhance reconstruction quality. MARM
includes our proposed Autoregressive Upsampler (ARU) blocks, which are highly
computationally efficient, and ARM from previous work to balance decoding time
and reconstruction quality. We also propose enhancing ARU&apos;s performance using a
checkerboard two-stage decoding strategy. Moreover, the ratio of different
modules can be adjusted to maintain a balance between quality and speed.
Comprehensive experiments demonstrate that our method significantly improves
computational efficiency while preserving image quality. With different
parameter settings, our method can outperform popular AE-based codecs in
constrained environments in terms of both quality and decoding time, or achieve
state-of-the-art reconstruction quality compared to other INR codecs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zimo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Baoyi An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12592">
<title>RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos. (arXiv:2401.12592v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12592</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a new RGB-D object dataset captured in the wild called
WildRGB-D. Unlike most existing real-world object-centric datasets which only
come with RGB capturing, the direct capture of the depth channel allows better
3D annotations and broader downstream applications. WildRGB-D comprises
large-scale category-level RGB-D object videos, which are taken using an iPhone
to go around the objects in 360 degrees. It contains around 8500 recorded
objects and nearly 20000 RGB-D videos across 46 common object categories. These
videos are taken with diverse cluttered backgrounds with three setups to cover
as many real-world scenarios as possible: (i) a single object in one video;
(ii) multiple objects in one video; and (iii) an object with a static hand in
one video. The dataset is annotated with object masks, real-world scale camera
poses, and reconstructed aggregated point clouds from RGBD videos. We benchmark
four tasks with WildRGB-D including novel view synthesis, camera pose
estimation, object 6d pose estimation, and object surface reconstruction. Our
experiments show that the large-scale capture of RGB-D objects provides a large
potential to advance 3D object learning. Our project page is
https://wildrgbd.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1&quot;&gt;Hongchi Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sifei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12596">
<title>UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators. (arXiv:2401.12596v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12596</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative domain adaptation has achieved remarkable progress, enabling us to
adapt a pre-trained generator to a new target domain. However, existing methods
simply adapt the generator to a single target domain and are limited to a
single modality, either text-driven or image-driven. Moreover, they are prone
to overfitting domain-specific attributes, which inevitably compromises
cross-domain consistency. In this paper, we propose UniHDA, a unified and
versatile framework for generative hybrid domain adaptation with multi-modal
references from multiple domains. We use CLIP encoder to project multi-modal
references into a unified embedding space and then linear interpolate the
direction vectors from multiple target domains to achieve hybrid domain
adaptation. To ensure the cross-domain consistency, we propose a novel
cross-domain spatial structure (CSS) loss that maintains detailed spatial
structure information between source and target generator. Experiments show
that the adapted generator can synthesise realistic images with various
attribute compositions. Additionally, our framework is versatile to multiple
generators, \eg, StyleGAN2 and Diffusion Models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhanwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_w/0/1/0/all/0/1&quot;&gt;weihang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuchun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Boxi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12609">
<title>Fast Semi-supervised Unmixing using Non-convex Optimization. (arXiv:2401.12609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12609</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel linear model tailored for
semisupervised/library-based unmixing. Our model incorporates considerations
for library mismatch while enabling the enforcement of the abundance sum-to-one
constraint (ASC). Unlike conventional sparse unmixing methods, this model
involves nonconvex optimization, presenting significant computational
challenges. We demonstrate the efficacy of Alternating Methods of Multipliers
(ADMM) in cyclically solving these intricate problems. We propose two
semisupervised unmixing approaches, each relying on distinct priors applied to
the new model in addition to the ASC: sparsity prior and convexity constraint.
Our experimental results validate that enforcing the convexity constraint
outperforms the sparsity prior for the endmember library. These results are
corroborated across three simulated datasets (accounting for spectral
variability and varying pixel purity levels) and the Cuprite dataset.
Additionally, our comparison with conventional sparse unmixing methods
showcases considerable advantages of our proposed model, which entails
nonconvex optimization. Notably, our implementations of the proposed
algorithms-fast semisupervised unmixing (FaSUn) and sparse unmixing using
soft-shrinkage (SUnS)-prove considerably more efficient than traditional sparse
unmixing methods. SUnS and FaSUn were implemented using PyTorch and provided in
a dedicated Python package called Fast Semisupervised Unmixing (FUnmix), which
is open-source and available at https://github.com/BehnoodRasti/FUnmix
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1&quot;&gt;Behnood Rasti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zouaoui_A/0/1/0/all/0/1&quot;&gt;Alexandre Zouaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1&quot;&gt;Jocelyn Chanussot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12648">
<title>Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12648</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiview clustering (MVC) segregates data samples into meaningful clusters
by synthesizing information across multiple views. Moreover, deep
learning-based methods have demonstrated their strong feature learning
capabilities in MVC scenarios. However, effectively generalizing feature
representations while maintaining consistency is still an intractable problem.
In addition, most existing deep clustering methods based on contrastive
learning overlook the consistency of the clustering representations during the
clustering process. In this paper, we show how the above problems can be
overcome and propose a consistent enhancement-based deep MVC method via
contrastive learning (CCEC). Specifically, semantic connection blocks are
incorporated into a feature representation to preserve the consistent
information among multiple views. Furthermore, the representation process for
clustering is enhanced through spectral clustering, and the consistency across
multiple views is improved. Experiments conducted on five datasets demonstrate
the effectiveness and superiority of our method in comparison with the
state-of-the-art (SOTA) methods. The code for this method can be accessed at
https://anonymous.4open.science/r/CCEC-E84E/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hua Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1&quot;&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12665">
<title>ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12665</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, foundational models such as CLIP and SAM have shown promising
performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,
either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible
key drawbacks: 1) CLIP primarily focuses on global feature alignment across
different inputs, leading to imprecise segmentation of local anomalous parts;
2) SAM tends to generate numerous redundant masks without proper prompt
constraints, resulting in complex post-processing requirements. In this work,
we innovatively propose a CLIP and SAM collaboration framework called ClipSAM
for ZSAS. The insight behind ClipSAM is to employ CLIP&apos;s semantic understanding
capability for anomaly localization and rough segmentation, which is further
used as the prompt constraints for SAM to refine the anomaly segmentation
results. In details, we introduce a crucial Unified Multi-scale Cross-modal
Interaction (UMCI) module for interacting language with visual features at
multiple scales of CLIP to reason anomaly positions. Then, we design a novel
Multi-level Mask Refinement (MMR) module, which utilizes the positional
information as multi-level prompts for SAM to acquire hierarchical levels of
masks and merges them. Extensive experiments validate the effectiveness of our
approach, achieving the optimal segmentation performance on the MVTec-AD and
VisA datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jianjian Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yuhan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Chongjun Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12689">
<title>Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12689</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE&apos;s validity, together with its
superiority compared with prior approaches. We also prove MDE&apos;s versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1&quot;&gt;Ru Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Heming Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yawen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zenan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12694">
<title>Pragmatic Communication in Multi-Agent Collaborative Perception. (arXiv:2401.12694v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12694</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative perception allows each agent to enhance its perceptual
abilities by exchanging messages with others. It inherently results in a
trade-off between perception ability and communication costs. Previous works
transmit complete full-frame high-dimensional feature maps among agents,
resulting in substantial communication costs. To promote communication
efficiency, we propose only transmitting the information needed for the
collaborator&apos;s downstream task. This pragmatic communication strategy focuses
on three key aspects: i) pragmatic message selection, which selects
task-critical parts from the complete data, resulting in spatially and
temporally sparse feature vectors; ii) pragmatic message representation, which
achieves pragmatic approximation of high-dimensional feature vectors with a
task-adaptive dictionary, enabling communicating with integer indices; iii)
pragmatic collaborator selection, which identifies beneficial collaborators,
pruning unnecessary communication links. Following this strategy, we first
formulate a mathematical optimization framework for the
perception-communication trade-off and then propose PragComm, a multi-agent
collaborative perception system with two key components: i) single-agent
detection and tracking and ii) pragmatic collaboration. The proposed PragComm
promotes pragmatic communication and adapts to a wide range of communication
conditions. We evaluate PragComm for both collaborative 3D object detection and
tracking tasks in both real-world, V2V4Real, and simulation datasets, OPV2V and
V2X-SIM2.0. PragComm consistently outperforms previous methods with more than
32.7K times lower communication volume on OPV2V. Code is available at
github.com/PhyllisH/PragComm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1&quot;&gt;Xianghe Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12725">
<title>Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization. (arXiv:2401.12725v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12725</link>
<description rdf:parseType="Literal">&lt;p&gt;To facilitate a prospective estimation of CT effective dose and risk
minimization process, a prospective spatial dose estimation and the known
anatomical structures are expected. To this end, a CT reconstruction method is
required to reconstruct CT volumes from as few projections as possible, i.e. by
using the topograms, with anatomical structures as correct as possible. In this
work, an optimized CT reconstruction model based on a generative adversarial
network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an
anterior-posterior and a lateral CT projection. To enhance anatomical
structures, a pre-trained organ segmentation network and the 3D perceptual loss
are applied during the training phase, so that the model can then generate both
organ-enhanced CT volume and the organ segmentation mask. The proposed method
can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of
0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of
the anatomical structure, the proposed method effectively enhances the organ
shape and boundary and allows for a straight-forward identification of the
relevant anatomical structures. We note that conventional reconstruction
metrics fail to indicate the enhancement of anatomical structures. In addition
to such metrics, the evaluation is expanded with assessing the organ
segmentation performance. The average organ dice of the proposed method is 0.71
compared with 0.63 in baseline model, indicating the enhancement of anatomical
structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klein_L/0/1/0/all/0/1&quot;&gt;Laura Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yixing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baader_E/0/1/0/all/0/1&quot;&gt;Edith Baader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lell_M/0/1/0/all/0/1&quot;&gt;Michael Lell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kachelriess_M/0/1/0/all/0/1&quot;&gt;Marc Kachelrie&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12729">
<title>Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12729</link>
<description rdf:parseType="Literal">&lt;p&gt;Object Detection (OD) has proven to be a significant computer vision method
in extracting localized class information and has multiple applications in the
industry. Although many of the state-of-the-art (SOTA) OD models perform well
on medium and large sized objects, they seem to under perform on small objects.
In most of the industrial use cases, it is difficult to collect and annotate
data for small objects, as it is time-consuming and prone to human errors.
Additionally, those datasets are likely to be unbalanced and often result in an
inefficient model convergence. To tackle this challenge, this study presents a
novel approach that injects additional data points to improve the performance
of the OD models. Using synthetic data generation, the difficulties in data
collection and annotations for small object data points can be minimized and to
create a dataset with balanced distribution. This paper discusses the effects
of a simple proportional class-balancing technique, to enable better anchor
matching of the OD models. A comparison was carried out on the performances of
the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and
synthetic datasets within an industrial use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1&quot;&gt;Jibinraj Antony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegiste_V/0/1/0/all/0/1&quot;&gt;Vinit Hegiste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazeri_A/0/1/0/all/0/1&quot;&gt;Ali Nazeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavakoli_H/0/1/0/all/0/1&quot;&gt;Hooman Tavakoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walunj_S/0/1/0/all/0/1&quot;&gt;Snehal Walunj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plociennik_C/0/1/0/all/0/1&quot;&gt;Christiane Plociennik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruskowski_M/0/1/0/all/0/1&quot;&gt;Martin Ruskowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12736">
<title>Shift-ConvNets: Small Convolutional Kernel with Large Kernel Effects. (arXiv:2401.12736v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12736</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies reveal that the remarkable performance of Vision transformers
(ViTs) benefits from large receptive fields. For this reason, the large
convolutional kernel design becomes an ideal solution to make Convolutional
Neural Networks (CNNs) great again. However, the typical large convolutional
kernels turn out to be hardware-unfriendly operators, resulting in discount
compatibility of various hardware platforms. Thus, it is unwise to simply
enlarge the convolutional kernel size. In this paper, we reveal that small
convolutional kernels and convolution operations can achieve the closing
effects of large kernel sizes. Then, we propose a shift-wise operator that
ensures the CNNs capture long-range dependencies with the help of the sparse
mechanism, while remaining hardware-friendly. Experimental results show that
our shift-wise operator significantly improves the accuracy of a regular CNN
while markedly reducing computational requirements. On the ImageNet-1k, our
shift-wise enhanced CNN model outperforms the state-of-the-art models. Code &amp;amp;
models at https://github.com/lidc54/shift-wiseConv.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dachong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuangzhuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12743">
<title>Correlation-Embedded Transformer Tracking: A Single-Branch Framework. (arXiv:2401.12743v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12743</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing robust and discriminative appearance models has been a
long-standing research challenge in visual object tracking. In the prevalent
Siamese-based paradigm, the features extracted by the Siamese-like networks are
often insufficient to model the tracked targets and distractor objects, thereby
hindering them from being robust and discriminative simultaneously. While most
Siamese trackers focus on designing robust correlation operations, we propose a
novel single-branch tracking framework inspired by the transformer. Unlike the
Siamese-like feature extraction, our tracker deeply embeds cross-image feature
correlation in multiple layers of the feature network. By extensively matching
the features of the two images through multiple layers, it can suppress
non-target features, resulting in target-aware feature extraction. The output
features can be directly used for predicting target locations without
additional correlation steps. Thus, we reformulate the two-branch Siamese
tracking as a conceptually simple, fully transformer-based Single-Branch
Tracking pipeline, dubbed SBT. After conducting an in-depth analysis of the SBT
baseline, we summarize many effective design principles and propose an improved
tracker dubbed SuperSBT. SuperSBT adopts a hierarchical architecture with a
local modeling layer to enhance shallow-level features. A unified relation
modeling is proposed to remove complex handcrafted layer pattern designs.
SuperSBT is further improved by masked image modeling pre-training, integrating
temporal modeling, and equipping with dedicated prediction heads. Thus,
SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in
LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of
SBT from 37 FPS to 81 FPS. Extensive experiments show that our method achieves
superior results on eight VOT benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1&quot;&gt;Fei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wankou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1&quot;&gt;Lei Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yue Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12751">
<title>PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction. (arXiv:2401.12751v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12751</link>
<description rdf:parseType="Literal">&lt;p&gt;Surface reconstruction has traditionally relied on the Multi-View Stereo
(MVS)-based pipeline, which often suffers from noisy and incomplete geometry.
This is due to that although MVS has been proven to be an effective way to
recover the geometry of the scenes, especially for locally detailed areas with
rich textures, it struggles to deal with areas with low texture and large
variations of illumination where the photometric consistency is unreliable.
Recently, Neural Implicit Surface Reconstruction (NISR) combines surface
rendering and volume rendering techniques and bypasses the MVS as an
intermediate step, which has emerged as a promising alternative to overcome the
limitations of traditional pipelines. While NISR has shown impressive results
on simple scenes, it remains challenging to recover delicate geometry from
uncontrolled real-world scenes which is caused by its underconstrained
optimization. To this end, the framework PSDF is proposed which resorts to
external geometric priors from a pretrained MVS network and internal geometric
priors inherent in the NISR model to facilitate high-quality neural implicit
surface learning. Specifically, the visibility-aware feature consistency loss
and depth prior-assisted sampling based on external geometric priors are
introduced. These proposals provide powerfully geometric consistency
constraints and aid in locating surface intersection points, thereby
significantly improving the accuracy and delicate reconstruction of NISR.
Meanwhile, the internal prior-guided importance rendering is presented to
enhance the fidelity of the reconstructed surface mesh by mitigating the biased
rendering issue in NISR. Extensive experiments on the Tanks and Temples dataset
show that PSDF achieves state-of-the-art performance on complex uncontrolled
scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wanjuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wenbing Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12761">
<title>MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty. (arXiv:2401.12761v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12761</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving level-5 driving automation in autonomous vehicles necessitates a
robust semantic visual perception system capable of parsing data from different
sensors across diverse conditions. However, existing semantic perception
datasets often lack important non-camera modalities typically used in
autonomous vehicles, or they do not exploit such modalities to aid and improve
semantic annotations in challenging conditions. To address this, we introduce
MUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse
conditions under increased uncertainty. MUSES includes synchronized multimodal
recordings with 2D panoptic annotations for 2500 images captured under diverse
weather and illumination. The dataset integrates a frame camera, a lidar, a
radar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic
annotation protocol captures both class-level and instance-level uncertainty in
the ground truth and enables the novel task of uncertainty-aware panoptic
segmentation we introduce, along with standard semantic and panoptic
segmentation. MUSES proves both effective for training and challenging for
evaluating models under diverse visual conditions, and it opens new avenues for
research in multimodal and uncertainty-aware dense semantic perception. Our
dataset and benchmark will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brodermann_T/0/1/0/all/0/1&quot;&gt;Tim Br&amp;#xf6;dermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1&quot;&gt;David Bruggemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1&quot;&gt;Christos Sakaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ta_K/0/1/0/all/0/1&quot;&gt;Kevin Ta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liagouris_O/0/1/0/all/0/1&quot;&gt;Odysseas Liagouris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corkill_J/0/1/0/all/0/1&quot;&gt;Jason Corkill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12820">
<title>DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer. (arXiv:2401.12820v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12820</link>
<description rdf:parseType="Literal">&lt;p&gt;Successive proposals of several self-supervised training schemes continue to
emerge, taking one step closer to developing a universal foundation model. In
this process, the unsupervised downstream tasks are recognized as one of the
evaluation methods to validate the quality of visual features learned with a
self-supervised training scheme. However, unsupervised dense semantic
segmentation has not been explored as a downstream task, which can utilize and
evaluate the quality of semantic information introduced in patch-level feature
representations during self-supervised training of a vision transformer.
Therefore, this paper proposes a novel data-driven approach for unsupervised
semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates
semantically consistent and dense pseudo annotate segmentation masks for the
unlabeled image dataset without using any visual-prior or synchronized data. We
compare these pseudo-annotated segmentation masks with ground truth masks for
evaluating recent self-supervised training schemes to learn shared semantic
properties at the patch level and discriminative semantic properties at the
segment level. Finally, we evaluate existing state-of-the-art self-supervised
training schemes with our proposed downstream task, i.e., DatUS^2. Also, the
best version of DatUS^2 outperforms the existing state-of-the-art method for
the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47%
Pixel accuracy on the SUIM dataset. It also achieves a competitive level of
accuracy for a large-scale and complex dataset, i.e., the COCO dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sonal Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1&quot;&gt;Arijit Sur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baruah_R/0/1/0/all/0/1&quot;&gt;Rashmi Dutta Baruah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12835">
<title>SGTR+: End-to-end Scene Graph Generation with Transformer. (arXiv:2401.12835v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12835</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Graph Generation (SGG) remains a challenging visual understanding task
due to its compositional property. Most previous works adopt a bottom-up,
two-stage or point-based, one-stage approach, which often suffers from high
time complexity or suboptimal designs. In this work, we propose a novel SGG
method to address the aforementioned issues, formulating the task as a
bipartite graph construction problem. To address the issues above, we create a
transformer-based end-to-end framework to generate the entity and entity-aware
predicate proposal set, and infer directed edges to form relation triplets.
Moreover, we design a graph assembling module to infer the connectivity of the
bipartite scene graph based on our entity-aware structure, enabling us to
generate the scene graph in an end-to-end manner. Based on bipartite graph
assembling paradigm, we further propose a new technical design to address the
efficacy of entity-aware modeling and optimization stability of graph
assembling. Equipped with the enhanced entity-aware design, our method achieves
optimal performance and time-complexity. Extensive experimental results show
that our design is able to achieve the state-of-the-art or comparable
performance on three challenging benchmarks, surpassing most of the existing
approaches and enjoying higher efficiency in inference. Code is available:
https://github.com/Scarecrow0/SGTR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rongjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12851">
<title>Classification of grapevine varieties using UAV hyperspectral imaging. (arXiv:2401.12851v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12851</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of different grapevine varieties is a relevant phenotyping
task in Precision Viticulture since it enables estimating the growth of
vineyard rows dedicated to different varieties, among other applications
concerning the wine industry. This task can be performed with destructive
methods that require time-consuming tasks, including data collection and
analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a
more efficient and less prohibitive approach to collecting hyperspectral data,
despite acquiring noisier data. Therefore, the first task is the processing of
these data to correct and downsample large amounts of data. In addition, the
hyperspectral signatures of grape varieties are very similar. In this work, a
Convolutional Neural Network (CNN) is proposed for classifying seventeen
varieties of red and white grape variants. Rather than classifying single
samples, these are processed together with their neighbourhood. Hence, the
extraction of spatial and spectral features is addressed with 1) a spatial
attention layer and 2) Inception blocks. The pipeline goes from processing to
dataset elaboration, finishing with the training phase. The fitted model is
evaluated in terms of response time, accuracy and data separability, and
compared with other state-of-the-art CNNs for classifying hyperspectral data.
Our network was proven to be much more lightweight with a reduced number of
input bands, a lower number of trainable weights and therefore, reduced
training time. Despite this, the evaluated metrics showed much better results
for our network (~99% overall accuracy), in comparison with previous works
barely achieving 81% OA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1&quot;&gt;Alfonso L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogayar_C/0/1/0/all/0/1&quot;&gt;Carlos Javier Ogayar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feito_F/0/1/0/all/0/1&quot;&gt;Francisco Ram&amp;#xf3;n Feito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sousa_J/0/1/0/all/0/1&quot;&gt;Joaquim Jo&amp;#xe3;o Sousa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12862">
<title>FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units. (arXiv:2401.12862v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12862</link>
<description rdf:parseType="Literal">&lt;p&gt;Roadside unit (RSU) can significantly improve the safety and robustness of
autonomous vehicles through Vehicle-to-Everything (V2X) communication.
Currently, the usage of a single RSU mainly focuses on real-time inference and
V2X collaboration, while neglecting the potential value of the high-quality
data collected by RSU sensors. Integrating the vast amounts of data from
numerous RSUs can provide a rich source of data for model training. However,
the absence of ground truth annotations and the difficulty of transmitting
enormous volumes of data are two inevitable barriers to fully exploiting this
hidden value. In this paper, we introduce FedRSU, an innovative federated
learning framework for self-supervised scene flow estimation. In FedRSU, we
present a recurrent self-supervision training paradigm, where for each RSU, the
scene flow prediction of points at every timestamp can be supervised by its
subsequent future multi-modality observation. Another key component of FedRSU
is federated learning, where multiple devices collaboratively train an ML model
while keeping the training data local and private. With the power of the
recurrent self-supervised learning paradigm, FL is able to leverage innumerable
underutilized data from RSU. To verify the FedRSU framework, we construct a
large-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU
clients, covering various scenarios, modalities, and sensor settings. Based on
RSU-SF, we show that FedRSU can greatly improve model performance in ITS and
provide a comprehensive benchmark under diverse FL scenarios. To the best of
our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset
and benchmark for the FL community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shaoheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1&quot;&gt;Rui Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuhong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yafei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12870">
<title>Unlocking the Potential: Multi-task Deep Learning for Spaceborne Quantitative Monitoring of Fugitive Methane Plumes. (arXiv:2401.12870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12870</link>
<description rdf:parseType="Literal">&lt;p&gt;With the intensification of global warming, the monitoring of methane
emission and detection of gas plumes from landfills have increasingly received
attention. We decompose methane emission monitoring into three sub-tasks:
methane concentration inversion, plume segmentation, and emission rate
estimation. Conventional algorithms have limitations: methane concentration
inversion usually uses the matched filter, which is sensitive to global
spectrum distribution and contains a large amount of noises. There is limited
research on plume segmentation, with many studies resorting to manual
segmentation that is likely to be subjective. The estimation of methane
emission rate often utilizes IME algorithm, which relies on obtaining
meteorological measurement data. Using the WENT landfill site in Hong Kong and
PRISMA hyperspectral satellite imagery, we propose a new deep learning-based
framework for quantitative monitoring of methane emissions from remote sensing
images based on physical simulation. We generate simulated methane plumes using
large eddy simulation (LES) and different concentration maps of fugitive
emission using the radiative transfer equation (RTE), while combining
augmentation techniques to create a simulated PRISMA dataset. We train a U-Net
network for methane concentration inversion, a Mask R-CNN network for methane
plume segmentation, and a ResNet-50 network for methane emission rate
estimation. All three deep networks achieve higher validation accuracy compared
to conventional algorithms. We further respectively combine the first two
sub-tasks and the last two sub-tasks to design the multi-task learning models -
MTL-01 and MTL-02, both of which achieve higher accuracy than single-task
models. Our research serves as a demonstration of applying multi-task deep
learning to quantitative methane monitoring and can be extended to a broad
range of methane monitoring tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_G/0/1/0/all/0/1&quot;&gt;Guoxin Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Shiliang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wei Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12888">
<title>Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big Data System, Data Mining, and Closed-Loop Technologies. (arXiv:2401.12888v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12888</link>
<description rdf:parseType="Literal">&lt;p&gt;The aspiration of the next generation&apos;s autonomous driving (AD) technology
relies on the dedicated integration and interaction among intelligent
perception, prediction, planning, and low-level control. There has been a huge
bottleneck regarding the upper bound of autonomous driving algorithm
performance, a consensus from academia and industry believes that the key to
surmount the bottleneck lies in data-centric autonomous driving technology.
Recent advancement in AD simulation, closed-loop model training, and AD big
data engine have gained some valuable experience. However, there is a lack of
systematic knowledge and deep understanding regarding how to build efficient
data-centric AD technology for AD algorithm self-evolution and better AD big
data accumulation. To fill in the identified research gaps, this article will
closely focus on reviewing the state-of-the-art data-driven autonomous driving
technologies, with an emphasis on the comprehensive taxonomy of autonomous
driving datasets characterized by milestone generations, key features, data
acquisition settings, etc. Furthermore, we provide a systematic review of the
existing benchmark closed-loop AD big data pipelines from the industrial
frontier, including the procedure of closed-loop frameworks, key technologies,
and empirical studies. Finally, the future directions, potential applications,
limitations and concerns are discussed to arouse efforts from both academia and
industry for promoting the further development of autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yijun Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12900">
<title>PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Creation with 3D Gaussian Splatting. (arXiv:2401.12900v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2401.12900</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite much progress, creating real-time high-fidelity head avatar is still
difficult and existing methods have to trade-off between speed and quality.
3DMM based methods often fail to model non-facial structures such as eyeglasses
and hairstyles, while neural implicit models suffer from deformation
inflexibility and rendering inefficiency.
&lt;/p&gt;
&lt;p&gt;Although 3D Gaussian has been demonstrated to possess promising capability
for geometry representation and radiance field reconstruction, applying 3D
Gaussian in head avatar creation remains a major challenge since it is
difficult for 3D Gaussian to model the head shape variations caused by changing
poses and expressions. In this paper, we introduce PSAvatar, a novel framework
for animatable head avatar creation that utilizes discrete geometric primitive
to create a parametric morphable shape model and employs 3D Gaussian for fine
detail representation and high fidelity rendering. The parametric morphable
shape model is a Point-based Morphable Shape Model (PMSM) which uses points
instead of meshes for 3D representation to achieve enhanced representation
flexibility. The PMSM first converts the FLAME mesh to points by sampling on
the surfaces as well as off the meshes to enable the reconstruction of not only
surface-like structures but also complex geometries such as eyeglasses and
hairstyles. By aligning these points with the head shape in an
analysis-by-synthesis manner, the PMSM makes it possible to utilize 3D Gaussian
for fine detail representation and appearance modeling, thus enabling the
creation of high-fidelity avatars. We show that PSAvatar can reconstruct
high-fidelity head avatars of a variety of subjects and the avatars can be
animated in real-time ($\ge$ 25 fps at a resolution of 512 x 512 )
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1&quot;&gt;Guoping Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kanglin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12902">
<title>Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?. (arXiv:2401.12902v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12902</link>
<description rdf:parseType="Literal">&lt;p&gt;As the scale of vision models continues to grow, the emergence of Visual
Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has
gained attention due to its superior performance compared to traditional
full-finetuning. However, the conditions favoring VPT (the ``when&quot;) and the
underlying rationale (the ``why&quot;) remain unclear. In this paper, we conduct a
comprehensive analysis across 19 distinct datasets and tasks. To understand the
``when&quot; aspect, we identify the scenarios where VPT proves favorable by two
dimensions: task objectives and data distributions. We find that VPT is
preferrable when there is 1) a substantial disparity between the original and
the downstream task objectives (e.g., transitioning from classification to
counting), or 2) a similarity in data distributions between the two tasks
(e.g., both involve natural images). In exploring the ``why&quot; dimension, our
results indicate VPT&apos;s success cannot be attributed solely to overfitting and
optimization considerations. The unique way VPT preserves original features and
adds parameters appears to be a pivotal factor. Our study provides insights
into VPT&apos;s mechanisms, and offers guidance for its optimal utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lifu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12915">
<title>Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12915</link>
<description rdf:parseType="Literal">&lt;p&gt;VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language
Models) to accept multimodal inputs. Since it has been verified that LLMs can
be induced to generate harmful or inaccurate content through specific test
cases (termed as Red Teaming), how VLMs perform in similar scenarios,
especially with their combination of textual and visual inputs, remains a
question. To explore this problem, we present a novel red teaming dataset
RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal
jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,
privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to
benchmark current VLMs in terms of these 4 different aspects. Detailed analysis
shows that 10 prominent open-sourced VLMs struggle with the red teaming in
different degrees and have up to 31% performance gap with GPT-4V. Additionally,
we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning
(SFT) using RTVLM, and this bolsters the models&apos; performance with 10% in RTVLM
test set, 13% in MM-Hal, and without noticeable decline in MM-Bench,
overpassing other LLaVA-based models with regular alignment data. This reveals
that current open-sourced VLMs still lack red teaming alignment. Our code and
datasets will be open-source.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mukai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yuwei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Masood Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12932">
<title>Segmentation of tibiofemoral joint tissues from knee MRI using MtRA-Unet and incorporating shape information: Data from the Osteoarthritis Initiative. (arXiv:2401.12932v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12932</link>
<description rdf:parseType="Literal">&lt;p&gt;Knee Osteoarthritis (KOA) is the third most prevalent Musculoskeletal
Disorder (MSD) after neck and back pain. To monitor such a severe MSD, a
segmentation map of the femur, tibia and tibiofemoral cartilage is usually
accessed using the automated segmentation algorithm from the Magnetic Resonance
Imaging (MRI) of the knee. But, in recent works, such segmentation is
conceivable only from the multistage framework thus creating data handling
issues and needing continuous manual inference rendering it unable to make a
quick and precise clinical diagnosis. In order to solve these issues, in this
paper the Multi-Resolution Attentive-Unet (MtRA-Unet) is proposed to segment
the femur, tibia and tibiofemoral cartilage automatically. The proposed work
has included a novel Multi-Resolution Feature Fusion (MRFF) and Shape
Reconstruction (SR) loss that focuses on multi-contextual information and
structural anatomical details of the femur, tibia and tibiofemoral cartilage.
Unlike previous approaches, the proposed work is a single-stage and end-to-end
framework producing a Dice Similarity Coefficient (DSC) of 98.5% for the femur,
98.4% for the tibia, 89.1% for Femoral Cartilage (FC) and 86.1% for Tibial
Cartilage (TC) for critical MRI slices that can be helpful to clinicians for
KOA grading. The time to segment MRI volume (160 slices) per subject is 22 sec.
which is one of the fastest among state-of-the-art. Moreover, comprehensive
experimentation on the segmentation of FC and TC which is of utmost importance
for morphology-based studies to check KOA progression reveals that the proposed
method has produced an excellent result with binary segmentation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Daydar_A/0/1/0/all/0/1&quot;&gt;Akshay Daydar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pramanick_A/0/1/0/all/0/1&quot;&gt;Alik Pramanick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1&quot;&gt;Arijit Sur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanagaraj_S/0/1/0/all/0/1&quot;&gt;Subramani Kanagaraj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12938">
<title>Neural deformation fields for template-based reconstruction of cortical surfaces from MRI. (arXiv:2401.12938v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12938</link>
<description rdf:parseType="Literal">&lt;p&gt;The reconstruction of cortical surfaces is a prerequisite for quantitative
analyses of the cerebral cortex in magnetic resonance imaging (MRI). Existing
segmentation-based methods separate the surface registration from the surface
extraction, which is computationally inefficient and prone to distortions. We
introduce Vox2Cortex-Flow (V2C-Flow), a deep mesh-deformation technique that
learns a deformation field from a brain template to the cortical surfaces of an
MRI scan. To this end, we present a geometric neural network that models the
deformation-describing ordinary differential equation in a continuous manner.
The network architecture comprises convolutional and graph-convolutional
layers, which allows it to work with images and meshes at the same time.
V2C-Flow is not only very fast, requiring less than two seconds to infer all
four cortical surfaces, but also establishes vertex-wise correspondences to the
template during reconstruction. In addition, V2C-Flow is the first approach for
cortex reconstruction that models white matter and pial surfaces jointly,
therefore avoiding intersections between them. Our comprehensive experiments on
internal and external test data demonstrate that V2C-Flow results in cortical
surfaces that are state-of-the-art in terms of accuracy. Moreover, we show that
the established correspondences are more consistent than in FreeSurfer and that
they can directly be utilized for cortex parcellation and group analyses of
cortical thickness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bongratz_F/0/1/0/all/0/1&quot;&gt;Fabian Bongratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rickmann_A/0/1/0/all/0/1&quot;&gt;Anne-Marie Rickmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1&quot;&gt;Christian Wachinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12945">
<title>Lumiere: A Space-Time Diffusion Model for Video Generation. (arXiv:2401.12945v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12945</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Lumiere -- a text-to-video diffusion model designed for
synthesizing videos that portray realistic, diverse and coherent motion -- a
pivotal challenge in video synthesis. To this end, we introduce a Space-Time
U-Net architecture that generates the entire temporal duration of the video at
once, through a single pass in the model. This is in contrast to existing video
models which synthesize distant keyframes followed by temporal super-resolution
-- an approach that inherently makes global temporal consistency difficult to
achieve. By deploying both spatial and (importantly) temporal down- and
up-sampling and leveraging a pre-trained text-to-image diffusion model, our
model learns to directly generate a full-frame-rate, low-resolution video by
processing it in multiple space-time scales. We demonstrate state-of-the-art
text-to-video generation results, and show that our design easily facilitates a
wide range of content creation tasks and video editing applications, including
image-to-video, video inpainting, and stylized generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1&quot;&gt;Omer Bar-Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1&quot;&gt;Hila Chefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tov_O/0/1/0/all/0/1&quot;&gt;Omer Tov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1&quot;&gt;Charles Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paiss_R/0/1/0/all/0/1&quot;&gt;Roni Paiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zada_S/0/1/0/all/0/1&quot;&gt;Shiran Zada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ephrat_A/0/1/0/all/0/1&quot;&gt;Ariel Ephrat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hur_J/0/1/0/all/0/1&quot;&gt;Junhwa Hur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1&quot;&gt;Tomer Michaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1&quot;&gt;Oliver Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Deqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1&quot;&gt;Tali Dekel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1&quot;&gt;Inbar Mosseri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12946">
<title>Coverage Axis++: Efficient Inner Point Selection for 3D Shape Skeletonization. (arXiv:2401.12946v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12946</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Coverage Axis++, a novel and efficient approach to 3D shape
skeletonization. The current state-of-the-art approaches for this task often
rely on the watertightness of the input or suffer from substantial
computational costs, thereby limiting their practicality. To address this
challenge, Coverage Axis++ proposes a heuristic algorithm to select skeletal
points, offering a high-accuracy approximation of the Medial Axis Transform
(MAT) while significantly mitigating computational intensity for various shape
representations. We introduce a simple yet effective strategy that considers
both shape coverage and uniformity to derive skeletal points. The selection
procedure enforces consistency with the shape structure while favoring the
dominant medial balls, which thus introduces a compact underlying shape
representation in terms of MAT. As a result, Coverage Axis++ allows for
skeletonization for various shape representations (e.g., water-tight meshes,
triangle soups, point clouds), specification of the number of skeletal points,
few hyperparameters, and highly efficient computation with improved
reconstruction accuracy. Extensive experiments across a wide range of 3D shapes
validate the efficiency and effectiveness of Coverage Axis++. The code will be
publicly available once the paper is published.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zimeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Cheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1&quot;&gt;Shiqing Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaoming Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12963">
<title>AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12963</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models that incorporate language, vision, and more recently
actions have revolutionized the ability to harness internet scale data to
reason about useful tasks. However, one of the key challenges of training
embodied foundation models is the lack of data grounded in the physical world.
In this paper, we propose AutoRT, a system that leverages existing foundation
models to scale up the deployment of operational robots in completely unseen
scenarios with minimal human supervision. AutoRT leverages vision-language
models (VLMs) for scene understanding and grounding, and further uses large
language models (LLMs) for proposing diverse and novel instructions to be
performed by a fleet of robots. Guiding data collection by tapping into the
knowledge of foundation models enables AutoRT to effectively reason about
autonomy tradeoffs and safety while significantly scaling up data collection
for robot learning. We demonstrate AutoRT proposing instructions to over 20
robots across multiple buildings and collecting 77k real robot episodes via
both teleoperation and autonomous robot policies. We experimentally show that
such &quot;in-the-wild&quot; data collected by AutoRT is significantly more diverse, and
that AutoRT&apos;s use of LLMs allows for instruction following data collection
robots that can align to human preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_M/0/1/0/all/0/1&quot;&gt;Michael Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1&quot;&gt;Debidatta Dwibedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1&quot;&gt;Montse Gonzalez Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nikhil Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1&quot;&gt;Ryan Julian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirmani_S/0/1/0/all/0/1&quot;&gt;Sean Kirmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1&quot;&gt;Isabel Leal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Edward Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1&quot;&gt;Isabel Leal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddineni_S/0/1/0/all/0/1&quot;&gt;Sharath Maddineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1&quot;&gt;Pannag Sanketi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1&quot;&gt;Pierre Sermanet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1&quot;&gt;Stefan Welker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Steve Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuo Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.04251">
<title>2.75D: Boosting learning by representing 3D Medical imaging to 2D features for small data. (arXiv:2002.04251v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2002.04251</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical-data driven learning, 3D convolutional neural networks (CNNs) have
started to show superior performance to 2D CNNs in numerous deep learning
tasks, proving the added value of 3D spatial information in feature
representation. However, the difficulty in collecting more training samples to
converge, more computational resources and longer execution time make this
approach less applied. Also, applying transfer learning on 3D CNN is
challenging due to a lack of publicly available pre-trained 3D models. To
tackle these issues, we proposed a novel 2D strategical representation of
volumetric data, namely 2.75D. In this work, the spatial information of 3D
images is captured in a single 2D view by a spiral-spinning technique. As a
result, 2D CNN networks can also be used to learn volumetric information.
Besides, we can fully leverage pre-trained 2D CNNs for downstream vision
problems. We also explore a multi-view 2.75D strategy, 2.75D 3 channels
(2.75Dx3), to boost the advantage of 2.75D. We evaluated the proposed methods
on three public datasets with different modalities or organs (Lung CT, Breast
MRI, and Prostate MRI), against their 2D, 2.5D, and 3D counterparts in
classification tasks. Results show that the proposed methods significantly
outperform other counterparts when all methods were trained from scratch on the
lung dataset. Such performance gain is more pronounced with transfer learning
or in the case of limited training data. Our methods also achieved comparable
performance on other datasets. In addition, our methods achieved a substantial
reduction in time consumption of training and inference compared with the 2.5D
or 3D method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1&quot;&gt;Ruisheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiyi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1&quot;&gt;Ritse Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jungong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.13648">
<title>Weakly-supervised land classification for coastal zone based on deep convolutional neural networks by incorporating dual-polarimetric characteristics into training dataset. (arXiv:2003.13648v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.13648</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we explore the performance of DCNNs on semantic segmentation
using spaceborne polarimetric synthetic aperture radar (PolSAR) datasets. The
semantic segmentation task using PolSAR data can be categorized as weakly
supervised learning when the characteristics of SAR data and data annotating
procedures are factored in. Datasets are initially analyzed for selecting
feasible pre-training images. Then the differences between spaceborne and
airborne datasets are examined in terms of spatial resolution and viewing
geometry. In this study we used two dual-polarimetric images acquired by
TerraSAR-X DLR. A novel method to produce training dataset with more supervised
information is developed. Specifically, a series of typical classified images
as well as intensity images serve as training datasets. A field survey is
conducted for an area of about 20 square kilometers to obtain a ground truth
dataset used for accuracy evaluation. Several transfer learning strategies are
made for aforementioned training datasets which will be combined in a
practicable order. Three DCNN models, including SegNet, U-Net, and LinkNet, are
implemented next.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marino_A/0/1/0/all/0/1&quot;&gt;Armando Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shui_W/0/1/0/all/0/1&quot;&gt;Wenze Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11334">
<title>Generalized Out-of-Distribution Detection: A Survey. (arXiv:2110.11334v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11334</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is critical to ensuring the reliability
and safety of machine learning systems. For instance, in autonomous driving, we
would like the driving system to issue an alert and hand over the control to
humans when it detects unusual scenes or objects that it has never seen during
training time and cannot make a safe decision. The term, OOD detection, first
emerged in 2017 and since then has received increasing attention from the
research community, leading to a plethora of methods developed, ranging from
classification-based to density-based to distance-based ones. Meanwhile,
several other problems, including anomaly detection (AD), novelty detection
(ND), open set recognition (OSR), and outlier detection (OD), are closely
related to OOD detection in terms of motivation and methodology. Despite common
goals, these topics develop in isolation, and their subtle differences in
definition and problem setting often confuse readers and practitioners. In this
survey, we first present a unified framework called generalized OOD detection,
which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD
detection, and OD. Under our framework, these five problems can be seen as
special cases or sub-tasks, and are easier to distinguish. We then review each
of these five areas by summarizing their recent technical developments, with a
special focus on OOD detection methodologies. We conclude this survey with open
challenges and potential research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03087">
<title>Unsupervised Long-Term Person Re-Identification with Clothes Change. (arXiv:2202.03087v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03087</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate unsupervised person re-identification (Re-ID) with clothes
change, a new challenging problem with more practical usability and scalability
to real-world deployment. Most existing re-id methods artificially assume the
clothes of every single person to be stationary across space and time. This
condition is mostly valid for short-term re-id scenarios since an average
person would often change the clothes even within a single day. To alleviate
this assumption, several recent works have introduced the clothes change facet
to re-id, with a focus on supervised learning person identity discriminative
representation with invariance to clothes changes. Taking a step further
towards this long-term re-id direction, we further eliminate the requirement of
person identity labels, as they are significantly more expensive and more
tedious to annotate in comparison to short-term person re-id datasets. Compared
to conventional unsupervised short-term re-id, this new problem is drastically
more challenging as different people may have similar clothes whilst the same
person can wear multiple suites of clothes over different locations and times
with very distinct appearance. To overcome such obstacles, we introduce a novel
Curriculum Person Clustering (CPC) method that can adaptively regulate the
unsupervised clustering criterion according to the clustering confidence.
Experiments on three long-term person re-id datasets show that our CPC
outperforms SOTA unsupervised re-id methods and even closely matches the
supervised re-id models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingkun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shupeng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chun-Guang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jun Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13883">
<title>Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13883</link>
<description rdf:parseType="Literal">&lt;p&gt;As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1&quot;&gt;Sara Abdali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.09930">
<title>Deep Superpixel Generation and Clustering for Weakly Supervised Segmentation of Brain Tumors in MR Images. (arXiv:2209.09930v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.09930</link>
<description rdf:parseType="Literal">&lt;p&gt;Training machine learning models to segment tumors and other anomalies in
medical images is an important step for developing diagnostic tools but
generally requires manually annotated ground truth segmentations, which
necessitates significant time and resources. This work proposes the use of a
superpixel generation model and a superpixel clustering model to enable weakly
supervised brain tumor segmentations. The proposed method utilizes binary
image-level classification labels, which are readily accessible, to
significantly improve the initial region of interest segmentations generated by
standard weakly supervised methods without requiring ground truth annotations.
We used 2D slices of magnetic resonance brain scans from the Multimodal Brain
Tumor Segmentation Challenge 2020 dataset and labels indicating the presence of
tumors to train the pipeline. On the test cohort, our method achieved a mean
Dice coefficient of 0.691 and a mean 95% Hausdorff distance of 18.1,
outperforming existing superpixel-based weakly supervised segmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jay J. Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namdar_K/0/1/0/all/0/1&quot;&gt;Khashayar Namdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalvati_F/0/1/0/all/0/1&quot;&gt;Farzad Khalvati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11915">
<title>Understanding Self-Supervised Pretraining with Part-Aware Representation Learning. (arXiv:2301.11915v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11915</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we are interested in understanding self-supervised pretraining
through studying the capability that self-supervised representation pretraining
methods learn part-aware representations. The study is mainly motivated by that
random views, used in contrastive learning, and random masked (visible)
patches, used in masked image modeling, are often about object parts.
&lt;/p&gt;
&lt;p&gt;We explain that contrastive learning is a part-to-whole task: the projection
layer hallucinates the whole object representation from the object part
representation learned from the encoder, and that masked image modeling is a
part-to-part task: the masked patches of the object are hallucinated from the
visible patches. The explanation suggests that the self-supervised pretrained
encoder is required to understand the object part. We empirically compare the
off-the-shelf encoders pretrained with several representative methods on
object-level recognition and part-level recognition. The results show that the
fully-supervised model outperforms self-supervised models for object-level
recognition, and most self-supervised contrastive learning and masked image
modeling methods outperform the fully-supervised method for part-level
recognition. It is observed that the combination of contrastive learning and
masked image modeling further improves the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jiyang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaokang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Leye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05154">
<title>Industrial and Medical Anomaly Detection Through Cycle-Consistent Adversarial Networks. (arXiv:2302.05154v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05154</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, a new Anomaly Detection (AD) approach for industrial and
medical images is proposed. This method leverages the theoretical strengths of
unsupervised learning and the data availability of both normal and abnormal
classes. Indeed, the AD is often formulated as an unsupervised task, implying
only normal images during training. These normal images are devoted to be
reconstructed, through an autoencoder architecture for instance. However, the
information contained in abnormal data, when available, is also valuable for
this reconstruction. The model would be able to identify its weaknesses by
better learning how to transform an abnormal (respectively normal) image into a
normal (respectively abnormal) one, helping the entire model to learn better
than a single normal to normal reconstruction. To address this challenge, the
proposed method uses Cycle-Generative Adversarial Networks (Cycle-GAN) for
(ab)normal-to-normal translation. After an input image has been reconstructed
by the normal generator, an anomaly score quantifies the differences between
the input and its reconstruction. Based on a threshold set to satisfy a
business quality constraint, the input image is then flagged as normal or not.
The proposed method is evaluated on industrial and medical datasets. The
results demonstrate accurate performance with a zero false negative constraint
compared to state-of-the-art methods. The code is available at
https://github.com/ValDelch/CycleGANS-AnomalyDetection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bougaham_A/0/1/0/all/0/1&quot;&gt;Arnaud Bougaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delchevalerie_V/0/1/0/all/0/1&quot;&gt;Valentin Delchevalerie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adoui_M/0/1/0/all/0/1&quot;&gt;Mohammed El Adoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frenay_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Fr&amp;#xe9;nay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07700">
<title>PATS: Patch Area Transportation with Subdivision for Local Feature Matching. (arXiv:2303.07700v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07700</link>
<description rdf:parseType="Literal">&lt;p&gt;Local feature matching aims at establishing sparse correspondences between a
pair of images. Recently, detector-free methods present generally better
performance but are not satisfactory in image pairs with large scale
differences. In this paper, we propose Patch Area Transportation with
Subdivision (PATS) to tackle this issue. Instead of building an expensive image
pyramid, we start by splitting the original image pair into equal-sized patches
and gradually resizing and subdividing them into smaller patches with the same
scale. However, estimating scale differences between these patches is
non-trivial since the scale differences are determined by both relative camera
poses and scene structures, and thus spatially varying over image pairs.
Moreover, it is hard to obtain the ground truth for real scenes. To this end,
we propose patch area transportation, which enables learning scale differences
in a self-supervised manner. In contrast to bipartite graph matching, which
only handles one-to-one matching, our patch area transportation can deal with
many-to-many relationships. PATS improves both matching accuracy and coverage,
and shows superior performance in downstream tasks, such as relative pose
estimation, visual localization, and optical flow estimation. The source code
is available at \url{https://zju3dv.github.io/pats/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1&quot;&gt;Junjie Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1&quot;&gt;Hujun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guofeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06470">
<title>Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06470</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability of image and video generation models to create photorealistic
images has reached unprecedented heights, making it difficult to distinguish
between real and fake images in many cases. However, despite this progress, a
gap remains between the quality of generated images and those found in the real
world. To address this, we have reviewed a vast body of literature from both
academic publications and social media to identify qualitative shortcomings in
image generation models, which we have classified into five categories. By
understanding these failures, we can identify areas where these models need
improvement, as well as develop strategies for detecting deep fakes. The
prevalence of deep fakes in today&apos;s society is a serious concern, and our
findings can help mitigate their negative impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13014">
<title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art. (arXiv:2304.13014v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13014</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of computer- and robot-assisted minimally invasive surgery,
enormous progress has been made in recent years based on the recognition of
surgical instruments in endoscopic images and videos. In particular, the
determination of the position and type of instruments is of great interest.
Current work involves both spatial and temporal information, with the idea that
predicting the movement of surgical tools over time may improve the quality of
the final segmentations. The provision of publicly available datasets has
recently encouraged the development of new methods, mainly based on deep
learning. In this review, we identify and characterize datasets used for method
development and evaluation and quantify their frequency of use in the
literature. We further present an overview of the current state of research
regarding the segmentation and tracking of minimally invasive surgical
instruments in endoscopic images and videos. The paper focuses on methods that
work purely visually, without markers of any kind attached to the instruments,
considering both single-frame semantic and instance segmentation approaches, as
well as those that incorporate temporal information. The publications analyzed
were identified through the platforms Google Scholar, Web of Science, and
PubMed. The search terms used were &quot;instrument segmentation&quot;, &quot;instrument
tracking&quot;, &quot;surgical tool segmentation&quot;, and &quot;surgical tool tracking&quot;,
resulting in a total of 741 articles published between 01/2015 and 07/2023, of
which 123 were included using systematic selection criteria. A discussion of
the reviewed literature is provided, highlighting existing shortcomings and
emphasizing the available potential for future developments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_T/0/1/0/all/0/1&quot;&gt;Tobias Rueckert&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt; (2 and 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1&quot;&gt;Christoph Palm&lt;/a&gt; (1 and 4) ((1) Regensburg Medical Image Computing (ReMIC), Ostbayerische Technische Hochschule Regensburg (OTH Regensburg), Germany, (2) Artificial Intelligence in Healthcare and Medicine, Klinikum rechts der Isar, Technical University of Munich, Germany, (3) Department of Computing, Imperial College London, UK, (4) Regensburg Center of Health Sciences and Technology (RCHST), OTH Regensburg, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14391">
<title>Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14391</link>
<description rdf:parseType="Literal">&lt;p&gt;Language is compositional; an instruction can express multiple relation
constraints to hold among objects in a scene that a robot is tasked to
rearrange. Our focus in this work is an instructable scene-rearranging
framework that generalizes to longer instructions and to spatial concept
compositions never seen at training time. We propose to represent
language-instructed spatial concepts with energy functions over relative object
arrangements. A language parser maps instructions to corresponding energy
functions and an open-vocabulary visual-language model grounds their arguments
to relevant objects in the scene. We generate goal scene configurations by
gradient descent on the sum of energy functions, one per language predicate in
the instruction. Local vision-based policies then re-locate objects to the
inferred goal locations. We test our model on established instruction-guided
manipulation benchmarks, as well as benchmarks of compositional instructions we
introduce. We show our model can execute highly compositional instructions
zero-shot in simulation and in the real world. It outperforms
language-to-action reactive policies and Large Language Model planners by a
large margin, especially for long instructions that involve compositions of
multiple spatial concepts. Simulation and real-world robot execution videos, as
well as our code and datasets are publicly available on our website:
https://ebmplanner.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1&quot;&gt;Nikolaos Gkanatsios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Ayush Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1&quot;&gt;Zhou Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunchu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkeson_C/0/1/0/all/0/1&quot;&gt;Christopher Atkeson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1&quot;&gt;Katerina Fragkiadaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02317">
<title>Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings. (arXiv:2305.02317v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02317</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models elicit reasoning in a
chain-of-thought that allows models to decompose problems in a human-like
fashion. Though this paradigm improves multi-step reasoning ability in language
models, it is limited by being unimodal and applied mainly to
question-answering tasks. We claim that incorporating visual augmentation into
reasoning is essential, especially for complex, imaginative tasks.
Consequently, we introduce VCoT, a novel method that leverages chain-of-thought
prompting with vision-language grounding to recursively bridge the logical gaps
within sequential data. Our method uses visual guidance to generate synthetic
multimodal infillings that add consistent and novel information to reduce the
logical gaps for downstream tasks that can benefit from temporal reasoning, as
well as provide interpretability into models&apos; multi-step reasoning. We apply
VCoT to the Visual Storytelling and WikiHow summarization datasets and
demonstrate through human evaluation that VCoT offers novel and consistent
synthetic data augmentation beating chain-of-thought baselines, which can be
used to enhance downstream performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1&quot;&gt;Daniel Rose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Himakunthala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1&quot;&gt;Andy Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ryan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1&quot;&gt;Alex Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1&quot;&gt;Michael Saxon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1&quot;&gt;Chinmay Sonar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirza_D/0/1/0/all/0/1&quot;&gt;Diba Mirza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03053">
<title>ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03053</link>
<description rdf:parseType="Literal">&lt;p&gt;Typical deep visual recognition models are capable of performing the one task
they were trained on. In this paper, we tackle the extremely difficult problem
of combining distinct models with different initializations, each solving a
separate task, into one multi-task model without any additional training. Prior
work in model merging permutes one model to the space of the other then
averages them together. While this works for models trained on the same task,
we find that this fails to account for the differences in models trained on
disjoint tasks. Thus, we introduce &quot;ZipIt!&quot;, a general method for merging two
arbitrary models of the same architecture that incorporates two simple
strategies. First, in order to account for features that aren&apos;t shared between
models, we expand the model merging problem to allow for merging features
within each model by defining a general &quot;zip&quot; operation. Second, we add support
for partially zipping the models up until a specified layer, naturally creating
a multi-head model. We find that these two changes combined account for 20-60%
improvement over prior work, making it more feasible to merge models trained on
disjoint tasks without retraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_G/0/1/0/all/0/1&quot;&gt;George Stoica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolya_D/0/1/0/all/0/1&quot;&gt;Daniel Bolya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorner_J/0/1/0/all/0/1&quot;&gt;Jakob Bjorner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_P/0/1/0/all/0/1&quot;&gt;Pratik Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hearn_T/0/1/0/all/0/1&quot;&gt;Taylor Hearn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11321">
<title>JoIN: Joint GANs Inversion for Intrinsic Image Decomposition. (arXiv:2305.11321v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11321</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose to solve ill-posed inverse imaging problems using a
bank of Generative Adversarial Networks (GAN) as a prior and apply our method
to the case of Intrinsic Image Decomposition for faces and materials. Our
method builds on the demonstrated success of GANs to capture complex image
distributions. At the core of our approach is the idea that the latent space of
a GAN is a well-suited optimization domain to solve inverse problems. Given an
input image, we propose to jointly inverse the latent codes of a set of GANs
and combine their outputs to reproduce the input. Contrary to most GAN
inversion methods which are limited to inverting only a single GAN, we
demonstrate that it is possible to maintain distribution priors while inverting
several GANs jointly. We show that our approach is modular, allowing various
forward imaging models, and that it can successfully decompose both synthetic
and real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Viraj Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1&quot;&gt;Svetlana Lazebnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1&quot;&gt;Julien Philip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13208">
<title>Iterative Adversarial Attack on Image-guided Story Ending Generation. (arXiv:2305.13208v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13208</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning involves developing models that can integrate information
from various sources like images and texts. In this field, multimodal text
generation is a crucial aspect that involves processing data from multiple
modalities and outputting text. The image-guided story ending generation
(IgSEG) is a particularly significant task, targeting on an understanding of
complex relationships between text and image data with a complete story text
ending. Unfortunately, deep neural networks, which are the backbone of recent
IgSEG models, are vulnerable to adversarial samples. Current adversarial attack
methods mainly focus on single-modality data and do not analyze adversarial
attacks for multimodal text generation tasks that use cross-modal information.
To this end, we propose an iterative adversarial attack method
(Iterative-attack) that fuses image and text modality attacks, allowing for an
attack search for adversarial text and image in an more effective iterative
way. Experimental results demonstrate that the proposed method outperforms
existing single-modal and non-iterative multimodal attack methods, indicating
the potential for improving the adversarial robustness of multimodal text
generation models, such as multimodal machine translation, multimodal question
answering, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Youze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14800">
<title>Exploring Diverse In-Context Configurations for Image Captioning. (arXiv:2305.14800v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14800</link>
<description rdf:parseType="Literal">&lt;p&gt;After discovering that Language Models (LMs) can be good in-context few-shot
learners, numerous strategies have been proposed to optimize in-context
sequence configurations. Recently, researchers in Vision-Language (VL) domains
also develop their few-shot learners, while they only use the simplest way,
ie., randomly sampling, to configure in-context image-text pairs. In order to
explore the effects of varying configurations on VL in-context learning, we
devised four strategies for image selection and four for caption assignment to
configure in-context image-text pairs for image captioning. Here Image
Captioning is used as the case study since it can be seen as the
visually-conditioned LM. Our comprehensive experiments yield two
counter-intuitive but valuable insights, highlighting the distinct
characteristics of VL in-context learning due to multi-modal synergy, as
compared to the NLP case. Furthermore, in our exploration of optimal
combination strategies, we observed an average performance enhancement of 20.9
of CIDEr scores compared to the baseline. The code is given in
https://github.com/yongliang-wu/ExploreCfg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongliang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingzhuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06075">
<title>DeepSeaNet: Improving Underwater Object Detection using EfficientDet. (arXiv:2306.06075v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06075</link>
<description rdf:parseType="Literal">&lt;p&gt;Marine animals and deep underwater objects are difficult to recognize and
monitor for safety of aquatic life. There is an increasing challenge when the
water is saline with granular particles and impurities. In such natural
adversarial environment, traditional approaches like CNN start to fail and are
expensive to compute. This project involves implementing and evaluating various
object detection models, including EfficientDet, YOLOv5, YOLOv8, and
Detectron2, on an existing annotated underwater dataset, called the
Brackish-Dataset. The dataset comprises annotated image sequences of fish,
crabs, starfish, and other aquatic animals captured in Limfjorden water with
limited visibility. The aim of this research project is to study the efficiency
of newer models on the same dataset and contrast them with the previous results
based on accuracy and inference time. Firstly, I compare the results of YOLOv3
(31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%),
YOLOv8 (98.20%), EfficientDet (98.56% mAP) and Detectron2 (95.20% mAP) on the
same dataset. Secondly, I provide a modified BiSkFPN mechanism (BiFPN neck with
skip connections) to perform complex feature fusion in adversarial noise which
makes modified EfficientDet robust to perturbations. Third, analyzed the effect
on accuracy of EfficientDet (98.63% mAP) and YOLOv5 by adversarial learning
(98.04% mAP). Last, I provide class activation map based explanations (CAM) for
the two models to promote Explainability in black box models. Overall, the
results indicate that modified EfficientDet achieved higher accuracy with
five-fold cross validation than the other models with 88.54% IoU of feature
maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanyam Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14451">
<title>Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection. (arXiv:2306.14451v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14451</link>
<description rdf:parseType="Literal">&lt;p&gt;Video anomaly detection under weak supervision presents significant
challenges, particularly due to the lack of frame-level annotations during
training. While prior research has utilized graph convolution networks and
self-attention mechanisms alongside multiple instance learning (MIL)-based
classification loss to model temporal relations and learn discriminative
features, these methods often employ multi-branch architectures to capture
local and global dependencies separately, resulting in increased parameters and
computational costs. Moreover, the coarse-grained interclass separability
provided by the binary constraint of MIL-based loss neglects the fine-grained
discriminability within anomalous classes. In response, this paper introduces a
weakly supervised anomaly detection framework that focuses on efficient context
modeling and enhanced semantic discriminability. We present a Temporal Context
Aggregation (TCA) module that captures comprehensive contextual information by
reusing the similarity matrix and implementing adaptive fusion. Additionally,
we propose a Prompt-Enhanced Learning (PEL) module that integrates semantic
priors using knowledge-based prompts to boost the discriminative capacity of
context features while ensuring separability between anomaly sub-classes.
Extensive experiments validate the effectiveness of our method&apos;s components,
demonstrating competitive performance with reduced parameters and computational
effort on three challenging benchmarks: UCF-Crime, XD-Violence, and
ShanghaiTech datasets. Notably, our approach significantly improves the
detection accuracy of certain anomaly sub-classes, underscoring its practical
value and efficacy. Our code is available at:
https://github.com/yujiangpu20/PEL4VAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yujiang Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lulu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengjin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03212">
<title>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban region embedding is an important and yet highly challenging issue due
to the complexity and constantly changing nature of urban data. To address the
challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER)
to capture multi-view dependencies and learn expressive representations of
urban regions without the constraints of rigid neighbourhood region conditions.
Our model focus on learn urban region representation from multi-source urban
data. First, we capture the multi-view correlations from mobility flow
patterns, POI semantics and check-in dynamics. Then, we adopt global graph
attention networks to learn similarity of any two vertices in graphs. To
comprehensively consider and share features of multiple views, a two-stage
fusion module is further proposed to learn weights with external attention to
fuse multi-view embeddings. Extensive experiments for two downstream tasks on
real-world datasets demonstrate that our model outperforms state-of-the-art
methods by up to 17\% improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Weiliang Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qianqian Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10610">
<title>Ultrafast and Ultralight Network-Based Intelligent System for Real-time Diagnosis of Ear Diseases in Any Devices. (arXiv:2308.10610v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10610</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional ear disease diagnosis heavily depends on experienced specialists
and specialized equipment, frequently resulting in misdiagnoses, treatment
delays, and financial burdens for some patients. Utilizing deep learning models
for efficient ear disease diagnosis has proven effective and affordable.
However, existing research overlooked model inference speed and parameter size
required for deployment. To tackle these challenges, we constructed a
large-scale dataset comprising eight ear disease categories and normal ear
canal samples from two hospitals. Inspired by ShuffleNetV2, we developed
Best-EarNet, an ultrafast and ultralight network enabling real-time ear disease
diagnosis. Best-EarNet incorporates the novel Local-Global Spatial Feature
Fusion Module which can capture global and local spatial information
simultaneously and guide the network to focus on crucial regions within feature
maps at various levels, mitigating low accuracy issues. Moreover, our network
uses multiple auxiliary classification heads for efficient parameter
optimization. With 0.77M parameters, Best-EarNet achieves an average frames per
second of 80 on CPU. Employing transfer learning and five-fold cross-validation
with 22,581 images from Hospital-1, the model achieves an impressive 95.23%
accuracy. External testing on 1,652 images from Hospital-2 validates its
performance, yielding 92.14% accuracy. Compared to state-of-the-art networks,
Best-EarNet establishes a new state-of-the-art (SOTA) in practical
applications. Most importantly, we developed an intelligent diagnosis system
called Ear Keeper, which can be deployed on common electronic devices. By
manipulating a compact electronic otoscope, users can perform comprehensive
scanning and diagnosis of the ear canal using real-time video. This study
provides a novel paradigm for ear endoscopy and other medical endoscopic image
recognition applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yubiao Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xinyu Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaoqiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meiping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Haihua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanmei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zefeng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenrui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenzhang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14190">
<title>Score-Based Generative Models for PET Image Reconstruction. (arXiv:2308.14190v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14190</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based generative models have demonstrated highly promising results for
medical image reconstruction tasks in magnetic resonance imaging or computed
tomography. However, their application to Positron Emission Tomography (PET) is
still largely unexplored. PET image reconstruction involves a variety of
challenges, including Poisson noise with high variance and a wide dynamic
range. To address these challenges, we propose several PET-specific adaptations
of score-based generative models. The proposed framework is developed for both
2D and 3D PET. In addition, we provide an extension to guided reconstruction
using magnetic resonance images. We validate the approach through extensive 2D
and 3D $\textit{in-silico}$ experiments with a model trained on
patient-realistic data without lesions, and evaluate on data without lesions as
well as out-of-distribution data with lesions. This demonstrates the proposed
method&apos;s robustness and significant potential for improved PET reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Imraj RD Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denker_A/0/1/0/all/0/1&quot;&gt;Alexander Denker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barbano_R/0/1/0/all/0/1&quot;&gt;Riccardo Barbano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kereta_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;eljko Kereta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bangti Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thielemans_K/0/1/0/all/0/1&quot;&gt;Kris Thielemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maass_P/0/1/0/all/0/1&quot;&gt;Peter Maass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arridge_S/0/1/0/all/0/1&quot;&gt;Simon Arridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01141">
<title>VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders. (arXiv:2309.01141v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01141</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale text-to-image diffusion models have shown impressive capabilities
for generative tasks by leveraging strong vision-language alignment from
pre-training. However, most vision-language discriminative tasks require
extensive fine-tuning on carefully-labeled datasets to acquire such alignment,
with great cost in time and computing resources. In this work, we explore
directly applying a pre-trained generative diffusion model to the challenging
discriminative task of visual grounding without any fine-tuning and additional
training dataset. Specifically, we propose VGDiffZero, a simple yet effective
zero-shot visual grounding framework based on text-to-image diffusion models.
We also design a comprehensive region-scoring method considering both global
and local contexts of each isolated proposal. Extensive experiments on RefCOCO,
RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on
zero-shot visual grounding. Our code is available at
https://github.com/xuyang-liu16/VGDiffZero.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siteng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yachen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Honggang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donglin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17105">
<title>Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling. (arXiv:2309.17105v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17105</link>
<description rdf:parseType="Literal">&lt;p&gt;Action Quality Assessment (AQA) is a task that tries to answer how well an
action is carried out. While remarkable progress has been achieved, existing
works on AQA assume that all the training data are visible for training in one
time, but do not enable continual learning on assessing new technical actions.
In this work, we address such a Continual Learning problem in AQA
(Continual-AQA), which urges a unified model to learn AQA tasks sequentially
without forgetting. Our idea for modeling Continual-AQA is to sequentially
learn a task-consistent score-discriminative feature distribution, in which the
latent features express a strong correlation with the score labels regardless
of the task or action types. From this perspective, we aim to mitigate the
forgetting in Continual-AQA from two aspects. Firstly, to fuse the features of
new and previous data into a score-discriminative distribution, a novel
Feature-Score Correlation-Aware Rehearsal is proposed to store and reuse data
from previous tasks with limited memory size. Secondly, an Action
General-Specific Graph is developed to learn and decouple the action-general
and action-specific knowledge so that the task-consistent score-discriminative
features can be better extracted across various tasks. Extensive experiments
are conducted to evaluate the contributions of proposed components. The
comparisons with the existing continual learning methods additionally verify
the effectiveness and versatility of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Ling-An Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1&quot;&gt;Jing-Ke Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17189">
<title>RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation. (arXiv:2309.17189v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17189</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual speech separation methods aim to integrate different modalities
to generate high-quality separated speech, thereby enhancing the performance of
downstream tasks such as speech recognition. Most existing state-of-the-art
(SOTA) models operate in the time domain. However, their overly simplistic
approach to modeling acoustic features often necessitates larger and more
computationally intensive models in order to achieve SOTA performance. In this
paper, we present a novel time-frequency domain audio-visual speech separation
method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies
its algorithms on the complex time-frequency bins yielded by the Short-Time
Fourier Transform. We model and capture the time and frequency dimensions of
the audio independently using a multi-layered RNN along each dimension.
Furthermore, we introduce a unique attention-based fusion technique for the
efficient integration of audio and visual information, and a new mask
separation approach that takes advantage of the intrinsic spectral nature of
the acoustic features for a clearer separation. RTFS-Net outperforms the
previous SOTA method using only 10% of the parameters and 18% of the MACs. This
is the first time-frequency domain audio-visual speech separation method to
outperform all contemporary time-domain counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pegg_S/0/1/0/all/0/1&quot;&gt;Samuel Pegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00367">
<title>AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ. (arXiv:2310.00367v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00367</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating bitmap graphics from text has gained considerable attention, yet
for scientific figures, vector graphics are often preferred. Given that vector
graphics are typically encoded using low-level graphics primitives, generating
them directly is difficult. To address this, we propose the use of TikZ, a
well-known abstract graphics language that can be compiled to vector graphics,
as an intermediate representation of scientific figures. TikZ offers
human-oriented, high-level commands, thereby facilitating conditional language
modeling with any large language model. To this end, we introduce DaTikZ, the
first large-scale TikZ dataset consisting of 120k TikZ drawings aligned with
captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which
augments LLaMA with multimodal CLIP embeddings. In both human and automatic
evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms
of similarity to human-created figures, with CLiMA additionally improving
text-image alignment. Our detailed analysis shows that all models generalize
well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend
to generate more simplistic figures compared to both humans and our models. We
make our framework, AutomaTikZ, along with model weights and datasets, publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belouadi_J/0/1/0/all/0/1&quot;&gt;Jonas Belouadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1&quot;&gt;Anne Lauscher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1&quot;&gt;Steffen Eger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05207">
<title>Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05207</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently how to introduce large amounts of unlabeled facial images in the
wild into supervised Facial Action Unit (AU) detection frameworks has become a
challenging problem. In this paper, we propose a new AU detection framework
where multi-task learning is introduced to jointly learn AU domain separation
and reconstruction and facial landmark detection by sharing the parameters of
homostructural facial extraction modules. In addition, we propose a new feature
alignment scheme based on contrastive learning by simple projectors and an
improved contrastive loss, which adds four additional intermediate supervisors
to promote the feature reconstruction process. Experimental results on two
benchmarks demonstrate our superiority against the state-of-the-art methods for
AU detection in the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1&quot;&gt;Ziqiao Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07189">
<title>SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition. (arXiv:2310.07189v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07189</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras are bio-inspired sensors that respond to local changes in light
intensity and feature low latency, high energy efficiency, and high dynamic
range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant
attention due to their remarkable efficiency and fault tolerance. By
synergistically harnessing the energy efficiency inherent in event cameras and
the spike-based processing capabilities of SNNs, their integration could enable
ultra-low-power application scenarios, such as action recognition tasks.
However, existing approaches often entail converting asynchronous events into
conventional frames, leading to additional data mapping efforts and a loss of
sparsity, contradicting the design concept of SNNs and event cameras. To
address this challenge, we propose SpikePoint, a novel end-to-end point-based
SNN architecture. SpikePoint excels at processing sparse event cloud data,
effectively extracting both global and local features through a singular-stage
structure. Leveraging the surrogate training method, SpikePoint achieves high
accuracy with few parameters and maintains low power consumption, specifically
employing the identity mapping feature extractor on diverse datasets.
SpikePoint achieves state-of-the-art (SOTA) performance on four event-based
action recognition datasets using only 16 timesteps, surpassing other SNN
methods. Moreover, it also achieves SOTA performance across all methods on
three datasets, utilizing approximately 0.3\% of the parameters and 0.5\% of
power consumption employed by artificial neural networks (ANNs). These results
emphasize the significance of Point Cloud and pave the way for many
ultra-low-power event-based data processing applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yulong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haotian Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1&quot;&gt;Bojun Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08276">
<title>Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08276</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Our highlight is to conduct visual and textual representations in latent space,
directing them as close as possible to a redundancy-free regional visual
representation. Concretely, a Regional-Oriented Attention Module (ROAM)
adaptively adjusts the distance between the final visual and textual embeddings
in the latent semantic space, oriented by regional visual features. Meanwhile,
a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the
range of tractable textual representation and enhance global word-level
semantic connections using less attention operations. Ultimately, we exploit a
global visual-semantic constraint to reduce single visual dependency and serve
as an external constraint for the final visual and textual representations. The
effectiveness and superiority of our method are verified by extensive
experiments including parameter evaluation, quantitative comparison, ablation
studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiancheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1&quot;&gt;Cong Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02749">
<title>Fast Point Cloud to Mesh Reconstruction for Deformable Object Tracking. (arXiv:2311.02749v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02749</link>
<description rdf:parseType="Literal">&lt;p&gt;The world around us is full of soft objects we perceive and deform with
dexterous hand movements. For a robotic hand to control soft objects, it has to
acquire online state feedback of the deforming object. While RGB-D cameras can
collect occluded point clouds at a rate of 30Hz, this does not represent a
continuously trackable object surface. Hence, in this work, we developed a
method that takes as input a template mesh which is the mesh of an object in
its non-deformed state and a deformed point cloud of the same object, and then
shapes the template mesh such that it matches the deformed point cloud. The
reconstruction of meshes from point clouds has long been studied in the field
of Computer graphics under 3D reconstruction and 4D reconstruction, however,
both lack the speed and generalizability needed for robotics applications. Our
model is designed using a point cloud auto-encoder and a Real-NVP architecture.
Our trained model can perform mesh reconstruction and tracking at a rate of
58Hz on a template mesh of 3000 vertices and a deformed point cloud of 5000
points and is generalizable to the deformations of six different object
categories which are assumed to be made of soft material in our experiments
(scissors, hammer, foam brick, cleanser bottle, orange, and dice). The object
meshes are taken from the YCB benchmark dataset. An instance of a downstream
application can be the control algorithm for a robotic hand that requires
online feedback from the state of the manipulated object which would allow
online grasp adaptation in a closed-loop manner. Furthermore, the tracking
capacity of our method can help in the system identification of deforming
objects in a marker-free approach. In future work, we will extend our trained
model to generalize beyond six object categories and additionally to real-world
deforming point clouds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_E/0/1/0/all/0/1&quot;&gt;Elham Amin Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hehui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katzschmann_R/0/1/0/all/0/1&quot;&gt;Robert K. Katzschmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15939">
<title>Unleashing the Power of Prompt-driven Nucleus Instance Segmentation. (arXiv:2311.15939v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15939</link>
<description rdf:parseType="Literal">&lt;p&gt;Nucleus instance segmentation in histology images is crucial for a broad
spectrum of clinical applications. Current dominant algorithms rely on
regression of nuclear proxy maps. Distinguishing nucleus instances from the
estimated maps requires carefully curated post-processing, which is error-prone
and parameter-sensitive. Recently, the Segment Anything Model (SAM) has earned
huge attention in medical image segmentation, owing to its impressive
generalization ability and promptable property. Nevertheless, its potential on
nucleus instance segmentation remains largely underexplored. In this paper, we
present a novel prompt-driven framework that consists of a nucleus prompter and
SAM for automatic nucleus instance segmentation. Specifically, the prompter
learns to generate a unique point prompt for each nucleus while the SAM is
fine-tuned to output the corresponding mask for the prompted nucleus.
Furthermore, we propose the inclusion of adjacent nuclei as negative prompts to
enhance the model&apos;s capability to identify overlapping nuclei. Without
complicated post-processing, our proposed method sets a new state-of-the-art
performance on three challenging benchmarks. Code is available at
\url{github.com/windygoo/PromptNucSeg}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shui_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1&quot;&gt;Kai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenglu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sunyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingxiong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Honglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruizhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02218">
<title>WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance Fields. (arXiv:2312.02218v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02218</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to
model moving scenes. However, they are resource intensive and challenging to
compress. To address this issue, this paper presents WavePlanes, a fast and
more compact explicit model. We propose a multi-scale space and space-time
feature plane representation using N-level 2-D wavelet coefficients. The
inverse discrete wavelet transform reconstructs N feature signals at varying
detail, which are linearly decoded to approximate the color and density of
volumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we
compress a Hash Map containing only non-zero coefficients and their locations
on each plane. This results in a compressed model size of ~12 MB. Compared with
state-of-the-art plane-based models, WavePlanes is up to 15x smaller, less
computationally demanding and achieves comparable results in as little as one
hour of training - without requiring custom CUDA code or high performance
computing resources. Additionally, we propose new feature fusion schemes that
work as well as previously proposed schemes while providing greater
interpretability. Our code is available at:
https://github.com/azzarelli/waveplanes/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azzarelli_A/0/1/0/all/0/1&quot;&gt;Adrian Azzarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1&quot;&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1&quot;&gt;David R Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02246">
<title>Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1&quot;&gt;Gabriel della Maggiora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1&quot;&gt;Luis Alberto Croquevielle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_N/0/1/0/all/0/1&quot;&gt;Nikita Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1&quot;&gt;Harry Horsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1&quot;&gt;Thomas Heinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1&quot;&gt;Artur Yakimovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03408">
<title>Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future. (arXiv:2312.03408v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03408</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous maturation and application of autonomous driving
technology, a systematic examination of open-source autonomous driving datasets
becomes instrumental in fostering the robust evolution of the industry
ecosystem. Current autonomous driving datasets can broadly be categorized into
two generations. The first-generation autonomous driving datasets are
characterized by relatively simpler sensor modalities, smaller data scale, and
is limited to perception-level tasks. KITTI, introduced in 2012, serves as a
prominent representative of this initial wave. In contrast, the
second-generation datasets exhibit heightened complexity in sensor modalities,
greater data scale and diversity, and an expansion of tasks from perception to
encompass prediction and control. Leading examples of the second generation
include nuScenes and Waymo, introduced around 2019. This comprehensive review,
conducted in collaboration with esteemed colleagues from both academia and
industry, systematically assesses over seventy open-source autonomous driving
datasets from domestic and international sources. It offers insights into
various aspects, such as the principles underlying the creation of high-quality
datasets, the pivotal role of data engine systems, and the utilization of
generative foundation models to facilitate scalable data generation.
Furthermore, this review undertakes an exhaustive analysis and discourse
regarding the characteristics and data scales that future third-generation
autonomous driving datasets should possess. It also delves into the scientific
and technical challenges that warrant resolution. These endeavors are pivotal
in advancing autonomous innovation and fostering technological enhancement in
critical domains. For further details, please refer to
https://github.com/OpenDriveLab/DriveAGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jia Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huilin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Feng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Lu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Futang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kai Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chunjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1&quot;&gt;Beipeng Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shaoqing Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhihui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06934">
<title>Toward Real Text Manipulation Detection: New Dataset and New Solution. (arXiv:2312.06934v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06934</link>
<description rdf:parseType="Literal">&lt;p&gt;With the surge in realistic text tampering, detecting fraudulent text in
images has gained prominence for maintaining information security. However, the
high costs associated with professional text manipulation and annotation limit
the availability of real-world datasets, with most relying on synthetic
tampering, which inadequately replicates real-world tampering attributes. To
address this issue, we present the Real Text Manipulation (RTM) dataset,
encompassing 14,250 text images, which include 5,986 manually and 5,258
automatically tampered images, created using a variety of techniques, alongside
3,006 unaltered text images for evaluating solution stability. Our evaluations
indicate that existing methods falter in text forgery detection on the RTM
dataset. We propose a robust baseline solution featuring a Consistency-aware
Aggregation Hub and a Gated Cross Neighborhood-attention Fusion module for
efficient multi-modal information fusion, supplemented by a Tampered-Authentic
Contrastive Learning module during training, enriching feature representation
distinction. This framework, extendable to other dual-stream architectures,
demonstrated notable localization performance improvements of 7.33% and 6.38%
on manual and overall manipulations, respectively. Our contributions aim to
propel advancements in real-world text tampering detection. Code and dataset
will be made available at https://github.com/DrLuo/RTM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongliang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jishen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiang Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07063">
<title>Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation. (arXiv:2312.07063v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07063</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing human-object interaction in 3D from a single RGB image is a
challenging task and existing data driven methods do not generalize beyond the
objects present in the carefully curated 3D interaction datasets. Capturing
large-scale real data to learn strong interaction and 3D shape priors is very
expensive due to the combinatorial nature of human-object interactions. In this
paper, we propose ProciGen (Procedural interaction Generation), a method to
procedurally generate datasets with both, plausible interaction and diverse
object variation. We generate 1M+ human-object interaction pairs in 3D and
leverage this large-scale data to train our HDM (Hierarchical Diffusion Model),
a novel method to reconstruct interacting human and unseen objects, without any
templates. Our HDM is an image-conditioned diffusion model that learns both
realistic interaction and highly accurate human and object shapes. Experiments
show that our HDM trained with ProciGen significantly outperforms prior methods
that requires template meshes and that our dataset allows training methods with
strong generalization ability to unseen object instances. Our code and data
will be publicly released at:
https://virtualhumans.mpi-inf.mpg.de/procigen-hdm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xianghui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1&quot;&gt;Bharat Lal Bhatnagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenssen_J/0/1/0/all/0/1&quot;&gt;Jan Eric Lenssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1&quot;&gt;Gerard Pons-Moll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08673">
<title>Segment Beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation. (arXiv:2312.08673v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08673</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented Reality (AR) devices, emerging as prominent mobile interaction
platforms, face challenges in user safety, particularly concerning oncoming
vehicles. While some solutions leverage onboard camera arrays, these cameras
often have limited field-of-view (FoV) with front or downward perspectives.
Addressing this, we propose a new out-of-view semantic segmentation task and
Segment Beyond View (SBV), a novel audio-visual semantic segmentation method.
SBV supplements the visual modality, which miss the information beyond FoV,
with the auditory information using a teacher-student distillation model
(Omni2Ego). The model consists of a vision teacher utilising panoramic
information, an auditory teacher with 8-channel audio, and an audio-visual
student that takes views with limited FoV and binaural audio as input and
produce semantic segmentation for objects outside FoV. SBV outperforms existing
models in comparative evaluations and shows a consistent performance across
varying FoV ranges and in monaural audio settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Renjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1&quot;&gt;Feras Dayoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hsiang-Ting Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12433">
<title>Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12433</link>
<description rdf:parseType="Literal">&lt;p&gt;Amodal perception, the ability to comprehend complete object structures from
partial visibility, is a fundamental skill, even for infants. Its significance
extends to applications like autonomous driving, where a clear understanding of
heavily occluded objects is essential. However, modern detection and tracking
algorithms often overlook this critical capability, perhaps due to the
prevalence of modal annotations in most datasets. To address the scarcity of
amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse
categories in thousands of video sequences. Our dataset includes amodal and
modal bounding boxes for visible and occluded objects, including objects that
are partially out-of-frame. To enhance amodal tracking with object permanence,
we leverage a lightweight plug-in module, the amodal expander, to transform
standard, modal trackers into amodal ones through fine-tuning on a few hundred
video sequences with data augmentation. We achieve a 3.3\% and 1.6\%
improvement on the detection and tracking of occluded objects on TAO-Amodal.
When evaluated on people, our method produces dramatic improvements of 2x
compared to state-of-the-art modal baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_T/0/1/0/all/0/1&quot;&gt;Tarasha Khurana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00334">
<title>Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00334</link>
<description rdf:parseType="Literal">&lt;p&gt;This work focuses on plant leaf disease classification and explores three
crucial aspects: adversarial training, model explainability, and model
compression. The models&apos; robustness against adversarial attacks is enhanced
through adversarial training, ensuring accurate classification even in the
presence of threats. Leveraging explainability techniques, we gain insights
into the model&apos;s decision-making process, improving trust and transparency.
Additionally, we explore model compression techniques to optimize computational
efficiency while maintaining classification performance. Through our
experiments, we determine that on a benchmark dataset, the robustness can be
the price of the classification accuracy with performance reductions of 3%-20%
for regular tests and gains of 50%-70% for adversarial attack tests. We also
demonstrate that a student model can be 15-25 times more computationally
efficient for a slight performance reduction, distilling the knowledge of more
complex models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echim_S/0/1/0/all/0/1&quot;&gt;Sebastian-Vasile Echim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taiatu_I/0/1/0/all/0/1&quot;&gt;Iulian-Marius T&amp;#x103;iatu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1&quot;&gt;Dumitru-Clementin Cercel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1&quot;&gt;Florin Pop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01520">
<title>S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuangyin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01651">
<title>AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks. We have open-sourced the
dataset and evaluation code on the project website:
https://www.benchcouncil.org/AIGCBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wanling Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03179">
<title>Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification. (arXiv:2401.03179v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03179</link>
<description rdf:parseType="Literal">&lt;p&gt;In multimodal land cover classification (MLCC), a common challenge is the
redundancy in data distribution, where irrelevant information from multiple
modalities can hinder the effective integration of their unique features. To
tackle this, we introduce the Multimodal Informative Vit (MIVit), a system with
an innovative information aggregate-distributing mechanism. This approach
redefines redundancy levels and integrates performance-aware elements into the
fused representation, facilitating the learning of semantics in both forward
and backward directions. MIVit stands out by significantly reducing redundancy
in the empirical distribution of each modality&apos;s separate and fused features.
It employs oriented attention fusion (OAF) for extracting shallow local
features across modalities in horizontal and vertical dimensions, and a
Transformer feature extractor for extracting deep global features through
long-range attention. We also propose an information aggregation constraint
(IAC) based on mutual information, designed to remove redundant information and
preserve complementary information within embedded features. Additionally, the
information distribution flow (IDF) in MIVit enhances performance-awareness by
distributing global classification information across different modalities&apos;
feature maps. This architecture also addresses missing modality challenges with
lightweight independent modality classifiers, reducing the computational load
typically associated with Transformers. Our results show that MIVit&apos;s
bidirectional aggregate-distributing mechanism between modalities is highly
effective, achieving an average overall accuracy of 95.56% across three
multimodal datasets. This performance surpasses current state-of-the-art
methods in MLCC. The code for MIVit is accessible at
https://github.com/icey-zhang/MIViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jie Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Geng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daixun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04079">
<title>RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04079</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathology plays a central role in clinical medicine and biomedical
research. While artificial intelligence shows promising results on many
pathological tasks, generalization and dealing with rare diseases, where
training data is scarce, remains a challenge. Distilling knowledge from
unlabeled data into a foundation model before learning from, potentially
limited, labeled data provides a viable path to address these challenges. In
this work, we extend the state of the art of foundation models for digital
pathology whole slide images by semi-automated data curation and incorporating
pathologist domain knowledge. Specifically, we combine computational and
pathologist domain knowledge (1) to curate a diverse dataset of 103k slides
corresponding to 750 million image patches covering data from different
fixation, staining, and scanning protocols as well as data from different
indications and labs across the EU and US, (2) for grouping semantically
similar slides and tissue patches, and (3) to augment the input images during
training. We evaluate the resulting model on a set of public and internal
benchmarks and show that although our foundation model is trained with an order
of magnitude less slides, it performs on par or better than competing models.
We expect that scaling our approach to more data and larger models will further
increase its performance and capacity to deal with increasingly complex real
world tasks in diagnostics and biomedical research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dippel_J/0/1/0/all/0/1&quot;&gt;Jonas Dippel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feulner_B/0/1/0/all/0/1&quot;&gt;Barbara Feulner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winterhoff_T/0/1/0/all/0/1&quot;&gt;Tobias Winterhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schallenberg_S/0/1/0/all/0/1&quot;&gt;Simon Schallenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dernbach_G/0/1/0/all/0/1&quot;&gt;Gabriel Dernbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kunft_A/0/1/0/all/0/1&quot;&gt;Andreas Kunft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tietz_S/0/1/0/all/0/1&quot;&gt;Stephan Tietz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jurmeister_P/0/1/0/all/0/1&quot;&gt;Philipp Jurmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Horst_D/0/1/0/all/0/1&quot;&gt;David Horst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruff_L/0/1/0/all/0/1&quot;&gt;Lukas Ruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klauschen_F/0/1/0/all/0/1&quot;&gt;Frederick Klauschen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alber_M/0/1/0/all/0/1&quot;&gt;Maximilian Alber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06827">
<title>APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06827</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Vision-Language (V-L) models set the benchmark for generalization
to downstream tasks among the noteworthy contenders. Many characteristics of
the V-L model have been explored in existing research including the challenge
of the sensitivity to text input and the tuning process across multi-modal
prompts. With the advanced utilization of the V-L model like CLIP, recent
approaches deploy learnable prompts instead of hand-craft prompts to boost the
generalization performance and address the aforementioned challenges. Inspired
by layer-wise training, which is wildly used in image fusion, we note that
using a sequential training process to adapt different modalities branches of
CLIP efficiently facilitates the improvement of generalization. In the context
of addressing the multi-modal prompting challenge, we propose Token-wise
Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities
prompts, vision and language, as tokens in a sequential manner. APLe addresses
the challenges in V-L models to promote prompt learning across both modalities,
which indicates a competitive generalization performance in line with the
state-of-the-art. Preeminently, APLe shows robustness and favourable
performance in prompt-length experiments with an absolute advantage in adopting
the V-L models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Guiming Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1&quot;&gt;Kaize Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huaiwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Guandong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07709">
<title>Towards Efficient Diffusion-Based Image Editing with Instant Attention Masks. (arXiv:2401.07709v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07709</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which
often applies a semantic mask to control the target area for diffusion-based
editing. However, most existing solutions obtain these masks via manual
operations or off-line processing, greatly reducing their efficiency. In this
paper, we propose a novel and efficient image editing method for Text-to-Image
(T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In
particular, InstDiffEdit aims to employ the cross-modal attention ability of
existing diffusion models to achieve instant mask guidance during the diffusion
steps. To reduce the noise of attention maps and realize the full automatics,
we equip InstDiffEdit with a training-free refinement scheme to adaptively
aggregate the attention distributions for the automatic yet accurate mask
generation. Meanwhile, to supplement the existing evaluations of DIE, we
propose a new benchmark called Editing-Mask to examine the mask accuracy and
local editing ability of existing methods. To validate InstDiffEdit, we also
conduct extensive experiments on ImageNet and Imagen, and compare it with a
bunch of the SOTA methods. The experimental results show that InstDiffEdit not
only outperforms the SOTA methods in both image quality and editing results,
but also has a much faster inference speed, i.e., +5 to +6 times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1&quot;&gt;Siyu Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiji Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09495">
<title>IPR-NeRF: Ownership Verification meets Neural Radiance Field. (arXiv:2401.09495v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09495</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1&quot;&gt;Win Kent Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Kam Woh Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11114">
<title>DengueNet: Dengue Prediction using Spatiotemporal Satellite Imagery for Resource-Limited Countries. (arXiv:2401.11114v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11114</link>
<description rdf:parseType="Literal">&lt;p&gt;Dengue fever presents a substantial challenge in developing countries where
sanitation infrastructure is inadequate. The absence of comprehensive
healthcare systems exacerbates the severity of dengue infections, potentially
leading to life-threatening circumstances. Rapid response to dengue outbreaks
is also challenging due to limited information exchange and integration. While
timely dengue outbreak forecasts have the potential to prevent such outbreaks,
the majority of dengue prediction studies have predominantly relied on data
that impose significant burdens on individual countries for collection. In this
study, our aim is to improve health equity in resource-constrained countries by
exploring the effectiveness of high-resolution satellite imagery as a
nontraditional and readily accessible data source. By leveraging the wealth of
publicly available and easily obtainable satellite imagery, we present a
scalable satellite extraction framework based on Sentinel Hub, a cloud-based
computing platform. Furthermore, we introduce DengueNet, an innovative
architecture that combines Vision Transformer, Radiomics, and Long Short-term
Memory to extract and integrate spatiotemporal features from satellite images.
This enables dengue predictions on an epi-week basis. To evaluate the
effectiveness of our proposed method, we conducted experiments on five
municipalities in Colombia. We utilized a dataset comprising 780
high-resolution Sentinel-2 satellite images for training and evaluation. The
performance of DengueNet was assessed using the mean absolute error (MAE)
metric. Across the five municipalities, DengueNet achieved an average MAE of
43.92. Our findings strongly support the efficacy of satellite imagery as a
valuable resource for dengue prediction, particularly in informing public
health policies within countries where manually collected data is scarce and
dengue virus prevalence is severe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_K/0/1/0/all/0/1&quot;&gt;Kuan-Ting Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1&quot;&gt;Dana Moukheiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_S/0/1/0/all/0/1&quot;&gt;Sebastian Cajas Ordonez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Restrepo_D/0/1/0/all/0/1&quot;&gt;David Restrepo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paddo_A/0/1/0/all/0/1&quot;&gt;Atika Rahman Paddo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tsung-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moukheiber_L/0/1/0/all/0/1&quot;&gt;Lama Moukheiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moukheiber_M/0/1/0/all/0/1&quot;&gt;Mira Moukheiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moukheiber_S/0/1/0/all/0/1&quot;&gt;Sulaiman Moukheiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1&quot;&gt;Saptarshi Purkayastha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1&quot;&gt;Po-Chih Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11115">
<title>MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation. (arXiv:2401.11115v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11115</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable generation of 3D human motions becomes an important topic as the
world embraces digital transformation. Existing works, though making promising
progress with the advent of diffusion models, heavily rely on meticulously
captured and annotated (e.g., text) high-quality motion corpus, a
resource-intensive endeavor in the real world. This motivates our proposed
MotionMix, a simple yet effective weakly-supervised diffusion model that
leverages both noisy and unannotated motion sequences. Specifically, we
separate the denoising objectives of a diffusion model into two stages:
obtaining conditional rough motion approximations in the initial $T-T^*$ steps
by learning the noisy annotated motions, followed by the unconditional
refinement of these preliminary motions during the last $T^*$ steps using
unannotated motions. Notably, though learning from two sources of imperfect
data, our model does not compromise motion generation quality compared to fully
supervised approaches that access gold data. Extensive experiments on several
benchmarks demonstrate that our MotionMix, as a versatile framework,
consistently achieves state-of-the-art performances on text-to-motion,
action-to-motion, and music-to-dance tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1&quot;&gt;Nhat M. Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kehong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1&quot;&gt;Michael Bi Mi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11687">
<title>TIM: An Efficient Temporal Interaction Module for Spiking Transformer. (arXiv:2401.11687v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11687</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs), as the third generation of neural networks,
have gained prominence for their biological plausibility and computational
efficiency, especially in processing diverse datasets. The integration of
attention mechanisms, inspired by advancements in neural network architectures,
has led to the development of Spiking Transformers. These have shown promise in
enhancing SNNs&apos; capabilities, particularly in the realms of both static and
neuromorphic datasets. Despite their progress, a discernible gap exists in
these systems, specifically in the Spiking Self Attention (SSA) mechanism&apos;s
effectiveness in leveraging the temporal processing potential of SNNs. To
address this, we introduce the Temporal Interaction Module (TIM), a novel,
convolution-based enhancement designed to augment the temporal data processing
abilities within SNN architectures. TIM&apos;s integration into existing SNN
frameworks is seamless and efficient, requiring minimal additional parameters
while significantly boosting their temporal information handling capabilities.
Through rigorous experimentation, TIM has demonstrated its effectiveness in
exploiting temporal information, leading to state-of-the-art performance across
various neuromorphic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sicheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongcheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guobin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12001">
<title>Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep. (arXiv:2401.12001v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12001</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel stereo-confidence that can be measured externally to
various stereo-matching networks, offering an alternative input modality choice
of the cost volume for learning-based approaches, especially in safety-critical
systems. Grounded in the foundational concepts of disparity definition and the
disparity plane sweep, the proposed stereo-confidence method is built upon the
idea that any shift in a stereo-image pair should be updated in a corresponding
amount shift in the disparity map. Based on this idea, the proposed
stereo-confidence method can be summarized in three folds. 1) Using the
disparity plane sweep, multiple disparity maps can be obtained and treated as a
3-D volume (predicted disparity volume), like the cost volume is constructed.
2) One of these disparity maps serves as an anchor, allowing us to define a
desirable (or ideal) disparity profile at every spatial point. 3) By comparing
the desirable and predicted disparity profiles, we can quantify the level of
matching ambiguity between left and right images for confidence measurement.
Extensive experimental results using various stereo-matching networks and
datasets demonstrate that the proposed stereo-confidence method not only shows
competitive performance on its own but also consistent performance improvements
when it is used as an input modality for learning-based stereo-confidence
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ka_W/0/1/0/all/0/1&quot;&gt;Woonghyun Ka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaehyun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12019">
<title>Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency. (arXiv:2401.12019v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12019</link>
<description rdf:parseType="Literal">&lt;p&gt;In stereo-matching knowledge distillation methods of the self-supervised
monocular depth estimation, the stereo-matching network&apos;s knowledge is
distilled into a monocular depth network through pseudo-depth maps. In these
methods, the learning-based stereo-confidence network is generally utilized to
identify errors in the pseudo-depth maps to prevent transferring the errors.
However, the learning-based stereo-confidence networks should be trained with
ground truth (GT), which is not feasible in a self-supervised setting. In this
paper, we propose a method to identify and filter errors in the pseudo-depth
map using multiple disparity maps by checking their consistency without the
need for GT and a training process. Experimental results show that the proposed
method outperforms the previous methods and works well on various
configurations by filtering out erroneous areas where the stereo-matching is
vulnerable, especially such as textureless regions, occlusion boundaries, and
reflective surfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ka_W/0/1/0/all/0/1&quot;&gt;Woonghyun Ka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaehyun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12074">
<title>DeepCERES: A Deep learning method for cerebellar lobule segmentation using ultra-high resolution multimodal MRI. (arXiv:2401.12074v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12074</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel multimodal and high-resolution human brain
cerebellum lobule segmentation method. Unlike current tools that operate at
standard resolution ($1 \text{ mm}^{3}$) or using mono-modal data, the proposed
method improves cerebellum lobule segmentation through the use of a multimodal
and ultra-high resolution ($0.125 \text{ mm}^{3}$) training dataset. To develop
the method, first, a database of semi-automatically labelled cerebellum lobules
was created to train the proposed method with ultra-high resolution T1 and T2
MR images. Then, an ensemble of deep networks has been designed and developed,
allowing the proposed method to excel in the complex cerebellum lobule
segmentation task, improving precision while being memory efficient. Notably,
our approach deviates from the traditional U-Net model by exploring alternative
architectures. We have also integrated deep learning with classical machine
learning methods incorporating a priori knowledge from multi-atlas
segmentation, which improved precision and robustness. Finally, a new online
pipeline, named DeepCERES, has been developed to make available the proposed
method to the scientific community requiring as input only a single T1 MR image
at standard resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morell_Ortega_S/0/1/0/all/0/1&quot;&gt;Sergio Morell-Ortega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruiz_Perez_M/0/1/0/all/0/1&quot;&gt;Marina Ruiz-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gadea_M/0/1/0/all/0/1&quot;&gt;Marien Gadea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vivo_Hernando_R/0/1/0/all/0/1&quot;&gt;Roberto Vivo-Hernando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rubio_G/0/1/0/all/0/1&quot;&gt;Gregorio Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aparici_F/0/1/0/all/0/1&quot;&gt;Fernando Aparici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iglesia_Vaya_M/0/1/0/all/0/1&quot;&gt;Maria de la Iglesia-Vaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Catheline_G/0/1/0/all/0/1&quot;&gt;Gwenaelle Catheline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1&quot;&gt;Pierrick Coup&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manjon_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; V. Manj&amp;#xf3;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09994">
<title>Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.09994</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have become increasingly popular for flood prediction
due to their superior accuracy and efficiency compared to traditional methods.
However, current machine learning methods often rely on separate spatial or
temporal feature analysis and have limitations on the types, number, and
dimensions of input data. This study presented a CNN-RNN hybrid feature fusion
modelling approach for urban flood prediction, which integrated the strengths
of CNNs in processing spatial features and RNNs in analyzing different
dimensions of time sequences. This approach allowed for both static and dynamic
flood predictions. Bayesian optimization was applied to identify the seven most
influential flood-driven factors and determine the best combination strategy.
By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM,
BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This
model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE were
0.007, 0.025, 0.973 and 0.755, respectively) under various rainfall input
conditions. Additionally, the processing speed was significantly improved, with
an inference time of 1.158s (approximately 1/125 of the traditional computation
time) compared to the physically-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Situ_Z/0/1/0/all/0/1&quot;&gt;Zuxiang Situ&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1&quot;&gt;Shuai Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wanen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gongfa Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qianqian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1&quot;&gt;Guangtao Fu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>