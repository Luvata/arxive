<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.02551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.01587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.08610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.12239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.16004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01989" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.02084">
<title>ITEm: Unsupervised Image-Text Embedding Learning for eCommerce. (arXiv:2311.02084v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02084</link>
<description rdf:parseType="Literal">&lt;p&gt;Product embedding serves as a cornerstone for a wide range of applications in
eCommerce. The product embedding learned from multiple modalities shows
significant improvement over that from a single modality, since different
modalities provide complementary information. However, some modalities are more
informatively dominant than others. How to teach a model to learn embedding
from different modalities without neglecting information from the less dominant
modality is challenging. We present an image-text embedding model (ITEm), an
unsupervised learning method that is designed to better attend to image and
text modalities. We extend BERT by (1) learning an embedding from text and
image without knowing the regions of interest; (2) training a global
representation to predict masked words and to construct masked image patches
without their individual representations. We evaluate the pre-trained ITEm on
two tasks: the search for extremely similar products and the prediction of
product categories, showing substantial gains compared to strong baseline
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1&quot;&gt;Baohao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozielski_M/0/1/0/all/0/1&quot;&gt;Michael Kozielski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hewavitharana_S/0/1/0/all/0/1&quot;&gt;Sanjika Hewavitharana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiangbo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1&quot;&gt;Shahram Khadivi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1&quot;&gt;Tomer Lancewicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02115">
<title>Towards objective and systematic evaluation of bias in medical imaging AI. (arXiv:2311.02115v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02115</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_E/0/1/0/all/0/1&quot;&gt;Emma A.M. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1&quot;&gt;Raissa Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winder_A/0/1/0/all/0/1&quot;&gt;Anthony Winder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulve_V/0/1/0/all/0/1&quot;&gt;Vedant Gulve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amador_K/0/1/0/all/0/1&quot;&gt;Kimberly Amador&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilms_M/0/1/0/all/0/1&quot;&gt;Matthias Wilms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1&quot;&gt;Nils D. Forkert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02121">
<title>Enhancing Monocular Height Estimation from Aerial Images with Street-view Images. (arXiv:2311.02121v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02121</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate height estimation from monocular aerial imagery presents a
significant challenge due to its inherently ill-posed nature. This limitation
is rooted in the absence of adequate geometric constraints available to the
model when training with monocular imagery. Without additional geometric
information to supplement the monocular image data, the model&apos;s ability to
provide reliable estimations is compromised.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a method that enhances monocular height estimation
by incorporating street-view images. Our insight is that street-view images
provide a distinct viewing perspective and rich structural details of the
scene, serving as geometric constraints to enhance the performance of monocular
height estimation. Specifically, we aim to optimize an implicit 3D scene
representation, density field, with geometry constraints from street-view
images, thereby improving the accuracy and robustness of height estimation. Our
experimental results demonstrate the effectiveness of our proposed method,
outperforming the baseline and offering significant improvements in terms of
accuracy and structural consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xiaomou Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Wanshui Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1&quot;&gt;Naoto Yokoya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02122">
<title>Lost Your Style? Navigating with Semantic-Level Approach for Text-to-Outfit Retrieval. (arXiv:2311.02122v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02122</link>
<description rdf:parseType="Literal">&lt;p&gt;Fashion stylists have historically bridged the gap between consumers&apos; desires
and perfect outfits, which involve intricate combinations of colors, patterns,
and materials. Although recent advancements in fashion recommendation systems
have made strides in outfit compatibility prediction and complementary item
retrieval, these systems rely heavily on pre-selected customer choices.
Therefore, we introduce a groundbreaking approach to fashion recommendations:
text-to-outfit retrieval task that generates a complete outfit set based solely
on textual descriptions given by users. Our model is devised at three semantic
levels-item, style, and outfit-where each level progressively aggregates data
to form a coherent outfit recommendation based on textual input. Here, we
leverage strategies similar to those in the contrastive language-image
pretraining model to address the intricate-style matrix within the outfit sets.
Using the Maryland Polyvore and Polyvore Outfit datasets, our approach
significantly outperformed state-of-the-art models in text-video retrieval
tasks, solidifying its effectiveness in the fashion recommendation domain. This
research not only pioneers a new facet of fashion recommendation systems, but
also introduces a method that captures the essence of individual style
preferences through textual descriptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Junkyu Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_E/0/1/0/all/0/1&quot;&gt;Eugene Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sung-Hyuk Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02126">
<title>PILL: Plug Into LLM with Adapter Expert and Attention Gate. (arXiv:2311.02126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02126</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the remarkable capabilities of powerful Large Language Models (LLMs)
in effectively following instructions, there has been a growing number of
assistants in the community to assist humans. Recently, significant progress
has been made in the development of Vision Language Models (VLMs), expanding
the capabilities of LLMs and enabling them to execute more diverse
instructions. However, it is foreseeable that models will likely need to handle
tasks involving additional modalities such as speech, video, and others. This
poses a particularly prominent challenge of dealing with the complexity of
mixed modalities. To address this, we introduce a novel architecture called
PILL: Plug Into LLM with adapter expert and attention gate to better decouple
these complex modalities and leverage efficient fine-tuning. We introduce two
modules: Firstly, utilizing Mixture-of-Modality-Adapter-Expert to independently
handle different modalities, enabling better adaptation to downstream tasks
while preserving the expressive capability of the original model. Secondly, by
introducing Modality-Attention-Gating, which enables adaptive control of the
contribution of modality tokens to the overall representation. In addition, we
have made improvements to the Adapter to enhance its learning and expressive
capabilities. Experimental results demonstrate that our approach exhibits
competitive performance compared to other mainstream methods for modality
fusion. For researchers interested in our work, we provide free access to the
code and models at https://github.com/DsaltYfish/PILL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fangyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tingting Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yuyu Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02183">
<title>Cross-modal Prominent Fragments Enhancement Aligning Network for Image-text Retrieval. (arXiv:2311.02183v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02183</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text retrieval is a widely studied topic in the field of computer
vision due to the exponential growth of multimedia data, whose core concept is
to measure the similarity between images and text. However, most existing
retrieval methods heavily rely on cross-attention mechanisms for cross-modal
fine-grained alignment, which takes into account excessive irrelevant regions
and treats prominent and non-significant words equally, thereby limiting
retrieval accuracy. This paper aims to investigate an alignment approach that
reduces the involvement of non-significant fragments in images and text while
enhancing the alignment of prominent segments. For this purpose, we introduce
the Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which
achieves improved retrieval accuracy by diminishing the participation of
irrelevant regions during alignment and relatively increasing the alignment
similarity of prominent words. Additionally, we incorporate prior textual
information into image regions to reduce misalignment occurrences. In practice,
we first design a novel intra-modal fragments relationship reasoning method,
and subsequently employ our proposed alignment mechanism to compute the
similarity between images and text. Extensive quantitative comparative
experiments on MS-COCO and Flickr30K datasets demonstrate that our approach
outperforms state-of-the-art methods by about 5% to 10% in the rSum metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02189">
<title>FairSeg: A Large-scale Medical Image Segmentation Dataset for Fairness Learning with Fair Error-Bound Scaling. (arXiv:2311.02189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02189</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in artificial intelligence models has gained significantly more
attention in recent years, especially in the area of medicine, as fairness in
medical models is critical to people&apos;s well-being and lives. High-quality
medical fairness datasets are needed to promote fairness learning research.
Existing medical fairness datasets are all for classification tasks, and no
fairness datasets are available for medical segmentation, while medical
segmentation is an equally important clinical task as classifications, which
can provide detailed spatial information on organ abnormalities ready to be
assessed by clinicians. In this paper, we propose the first fairness dataset
for medical segmentation named FairSeg with 10,000 subject samples. In
addition, we propose a fair error-bound scaling approach to reweight the loss
function with the upper error-bound in each identity group. We anticipate that
the segmentation performance equity can be improved by explicitly tackling the
hard cases with high training errors in each identity group. To facilitate fair
comparisons, we propose new equity-scaled segmentation performance metrics,
such as the equity-scaled Dice coefficient, which is calculated as the overall
Dice coefficient divided by one plus the standard deviation of group Dice
coefficients. Through comprehensive experiments, we demonstrate that our fair
error-bound scaling approach either has superior or comparable fairness
performance to the state-of-the-art fairness learning models. The dataset and
code are publicly accessible via
\url{https://github.com/Harvard-Ophthalmology-AI-Lab/FairSeg}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Min Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouhana_A/0/1/0/all/0/1&quot;&gt;Ava Kouhana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elze_T/0/1/0/all/0/1&quot;&gt;Tobias Elze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02202">
<title>Neural Collage Transfer: Artistic Reconstruction via Material Manipulation. (arXiv:2311.02202v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02202</link>
<description rdf:parseType="Literal">&lt;p&gt;Collage is a creative art form that uses diverse material scraps as a base
unit to compose a single image. Although pixel-wise generation techniques can
reproduce a target image in collage style, it is not a suitable method due to
the solid stroke-by-stroke nature of the collage form. While some previous
works for stroke-based rendering produced decent sketches and paintings,
collages have received much less attention in research despite their popularity
as a style. In this paper, we propose a method for learning to make collages
via reinforcement learning without the need for demonstrations or collage
artwork data. We design the collage Markov Decision Process (MDP), which allows
the agent to handle various materials and propose a model-based soft
actor-critic to mitigate the agent&apos;s training burden derived from the
sophisticated dynamics of collage. Moreover, we devise additional techniques
such as active material selection and complexity-based multi-scale collage to
handle target images at any size and enhance the results&apos; aesthetics by placing
relatively more scraps in areas of high complexity. Experimental results show
that the trained agent appropriately selected and pasted materials to
regenerate the target image into a collage and obtained a higher evaluation
score on content and style than pixel-wise generation methods. Code is
available at https://github.com/northadventure/CollageRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Ganghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minji Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yunsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02236">
<title>Robust Fine-Tuning of Vision-Language Models for Domain Generalization. (arXiv:2311.02236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02236</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning enables the sharing of common knowledge among models for a
variety of downstream tasks, but traditional methods suffer in limited training
data settings and produce narrow models incapable of effectively generalizing
under distribution shifts. Foundation models have recently demonstrated
impressive zero-shot inference capabilities and robustness under distribution
shifts. However, zero-shot evaluation for these models has been predominantly
confined to benchmarks with simple distribution shifts, limiting our
understanding of their effectiveness under the more realistic shifts found in
practice. Moreover, common fine-tuning methods for these models have yet to be
evaluated against vision models in few-shot scenarios where training data is
limited. To address these gaps, we present a new recipe for few-shot
fine-tuning of the popular vision-language foundation model CLIP and evaluate
its performance on challenging benchmark datasets with realistic distribution
shifts from the WILDS collection. Our experimentation demonstrates that, while
zero-shot CLIP fails to match performance of trained vision models on more
complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only
counterparts in terms of in-distribution and out-of-distribution accuracy at
all levels of training data availability. This provides a strong incentive for
adoption of foundation models within few-shot learning applications operating
with real-world data. Code is available at
https://github.com/mit-ll/robust-vision-language-finetuning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_Lowell_K/0/1/0/all/0/1&quot;&gt;Kevin Vogt-Lowell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Noah Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1&quot;&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaillant_M/0/1/0/all/0/1&quot;&gt;Marc Vaillant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02239">
<title>Using DUCK-Net for Polyp Image Segmentation. (arXiv:2311.02239v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02239</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel supervised convolutional neural network
architecture, &quot;DUCK-Net&quot;, capable of effectively learning and generalizing from
small amounts of medical images to perform accurate segmentation tasks. Our
model utilizes an encoder-decoder structure with a residual downsampling
mechanism and a custom convolutional block to capture and process image
information at multiple resolutions in the encoder segment. We employ data
augmentation techniques to enrich the training set, thus increasing our model&apos;s
performance. While our architecture is versatile and applicable to various
segmentation tasks, in this study, we demonstrate its capabilities specifically
for polyp segmentation in colonoscopy images. We evaluate the performance of
our method on several popular benchmark datasets for polyp segmentation,
Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, and ETIS-LARIBPOLYPDB showing that it
achieves state-of-the-art results in terms of mean Dice coefficient, Jaccard
index, Precision, Recall, and Accuracy. Our approach demonstrates strong
generalization capabilities, achieving excellent performance even with limited
training data. The code is publicly available on GitHub:
https://github.com/RazvanDu/DUCK-Net
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumitru_R/0/1/0/all/0/1&quot;&gt;Razvan-Gabriel Dumitru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peteleaza_D/0/1/0/all/0/1&quot;&gt;Darius Peteleaza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Craciun_C/0/1/0/all/0/1&quot;&gt;Catalin Craciun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02240">
<title>Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems. (arXiv:2311.02240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02240</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning is a crucial tool for enabling a classification model to
forget specific data that are used in the training time. Recently, various
studies have presented machine unlearning algorithms and evaluated their
methods on several datasets. However, most of the current machine unlearning
algorithms have been evaluated solely on traditional computer vision datasets
such as CIFAR-10, MNIST, and SVHN. Furthermore, previous studies generally
evaluate the unlearning methods in the class-unlearning setup. Most previous
work first trains the classification models and then evaluates the machine
unlearning performance of machine unlearning algorithms by forgetting selected
image classes (categories) in the experiments. Unfortunately, these
class-unlearning settings might not generalize to real-world scenarios. In this
work, we propose a machine unlearning setting that aims to unlearn specific
instance that contains personal privacy (identity) while maintaining the
original task of a given model. Specifically, we propose two machine unlearning
benchmark datasets, MUFAC and MUCAC, that are greatly useful to evaluate the
performance and robustness of a machine unlearning algorithm. In our benchmark
datasets, the original model performs facial feature recognition tasks: face
age estimation (multi-class classification) and facial attribute classification
(binary class classification), where a class does not depend on any single
target subject (personal identity), which can be a realistic setting. Moreover,
we also report the performance of the state-of-the-art machine unlearning
methods on our proposed benchmark datasets. All the datasets, source codes, and
trained models are publicly available at
https://github.com/ndb796/MachineUnlearning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dasol Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_D/0/1/0/all/0/1&quot;&gt;Dongbin Na&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02249">
<title>Monitoring Inactivity of Single Older Adults at Home. (arXiv:2311.02249v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02249</link>
<description rdf:parseType="Literal">&lt;p&gt;A new application for real-time monitoring of the lack of movement in older
adults&apos; own homes is proposed, aiming to support people&apos;s lives and
independence in their later years. A lightweight camera monitoring system,
based on an RGB-D camera and a compact computer processor, was developed and
piloted in community homes to observe the daily behavior of older adults.
Instances of body inactivity were detected in everyday scenarios anonymously
and unobtrusively. These events can be explained at a higher level, such as a
loss of consciousness or physiological deterioration. The accuracy of the
inactivity monitoring system is assessed, and statistics of inactivity events
related to the daily behavior of the older adults are provided. The results
demonstrate that our method performs accurately in inactivity detection across
various environments, including low room lighting, TV flickering, and different
camera views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Longfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fisher_R/0/1/0/all/0/1&quot;&gt;Robert B. Fisher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02254">
<title>Learning-Based and Quality Preserving Super-Resolution of Noisy Images. (arXiv:2311.02254v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.02254</link>
<description rdf:parseType="Literal">&lt;p&gt;Several applications require the super-resolution of noisy images and the
preservation of geometrical and texture features. State-of-the-art
super-resolution methods do not account for noise and generally enhance the
output image&apos;s artefacts (e.g., aliasing, blurring). We propose a
learning-based method that accounts for the presence of noise and preserves the
properties of the input image, as measured by quantitative metrics (e.g.,
normalised crossed correlation, normalised mean squared error,
peak-signal-to-noise-ration, structural similarity feature-based similarity,
universal image quality). We train our network to up-sample a low-resolution
noisy image while preserving its properties. We perform our tests on the Cineca
Marconi100 cluster, at the 26th position in the top500 list. The experimental
results show that our method outperforms learning-based methods, has comparable
results with standard methods, preserves the properties of the input image as
contours, brightness, and textures, and reduces the artefacts. As average
quantitative metrics, our method has a PSNR value of 23.81 on the
super-resolution of Gaussian noise images with a 2X up-sampling factor. In
contrast, previous work has a PSNR value of 23.09 (standard method) and 21.78
(learning-based method). Our learning-based and quality-preserving
super-resolution improves the high-resolution prediction of noisy images with
respect to state-of-the-art methods with different noise types and up-sampling
factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cammarasana_S/0/1/0/all/0/1&quot;&gt;Simone Cammarasana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Patane_G/0/1/0/all/0/1&quot;&gt;Giuseppe Patan&amp;#xe8;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02256">
<title>Image Recognition of Oil Leakage Area Based on Logical Semantic Discrimination. (arXiv:2311.02256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02256</link>
<description rdf:parseType="Literal">&lt;p&gt;Implementing precise detection of oil leaks in peak load equipment through
image analysis can significantly enhance inspection quality and ensure the
system&apos;s safety and reliability. However, challenges such as varying shapes of
oil-stained regions, background noise, and fluctuating lighting conditions
complicate the detection process. To address this, the integration of logical
rule-based discrimination into image recognition has been proposed. This
approach involves recognizing the spatial relationships among objects to
semantically segment images of oil spills using a Mask RCNN network. The
process begins with histogram equalization to enhance the original image,
followed by the use of Mask RCNN to identify the preliminary positions and
outlines of oil tanks, the ground, and areas of potential oil contamination.
Subsequent to this identification, the spatial relationships between these
objects are analyzed. Logical rules are then applied to ascertain whether the
suspected areas are indeed oil spills. This method&apos;s effectiveness has been
confirmed by testing on images captured from peak power equipment in the field.
The results indicate that this approach can adeptly tackle the challenges in
identifying oil-contaminated areas, showing a substantial improvement in
accuracy compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weiying Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xun Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02266">
<title>Multi-task Learning for Optical Coherence Tomography Angiography (OCTA) Vessel Segmentation. (arXiv:2311.02266v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.02266</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Coherence Tomography Angiography (OCTA) is a non-invasive imaging
technique that provides high-resolution cross-sectional images of the retina,
which are useful for diagnosing and monitoring various retinal diseases.
However, manual segmentation of OCTA images is a time-consuming and
labor-intensive task, which motivates the development of automated segmentation
methods. In this paper, we propose a novel multi-task learning method for OCTA
segmentation, called OCTA-MTL, that leverages an image-to-DT (Distance
Transform) branch and an adaptive loss combination strategy. The image-to-DT
branch predicts the distance from each vessel voxel to the vessel surface,
which can provide useful shape prior and boundary information for the
segmentation task. The adaptive loss combination strategy dynamically adjusts
the loss weights according to the inverse of the average loss values of each
task, to balance the learning process and avoid the dominance of one task over
the other. We evaluate our method on the ROSE-2 dataset its superiority in
terms of segmentation performance against two baseline methods: a single-task
segmentation method and a multi-task segmentation method with a fixed loss
combination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Koz_C/0/1/0/all/0/1&quot;&gt;Can Koz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1&quot;&gt;Onat Dalmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dayanc_M/0/1/0/all/0/1&quot;&gt;Mertay Dayanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02274">
<title>Patch-based Selection and Refinement for Early Object Detection. (arXiv:2311.02274v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02274</link>
<description rdf:parseType="Literal">&lt;p&gt;Early object detection (OD) is a crucial task for the safety of many dynamic
systems. Current OD algorithms have limited success for small objects at a long
distance. To improve the accuracy and efficiency of such a task, we propose a
novel set of algorithms that divide the image into patches, select patches with
objects at various scales, elaborate the details of a small object, and detect
it as early as possible. Our approach is built upon a transformer-based network
and integrates the diffusion model to improve the detection accuracy. As
demonstrated on BDD100K, our algorithms enhance the mAP for small objects from
1.03 to 8.93, and reduce the data volume in computation by more than 77\%. The
source code is available at
\href{https://github.com/destiny301/dpr}{https://github.com/destiny301/dpr}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasichainula_K/0/1/0/all/0/1&quot;&gt;Kishore Kasichainula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_Y/0/1/0/all/0/1&quot;&gt;Yaoxin Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baoxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Jae-Sun Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yu Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02305">
<title>OSM vs HD Maps: Map Representations for Trajectory Prediction. (arXiv:2311.02305v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02305</link>
<description rdf:parseType="Literal">&lt;p&gt;While High Definition (HD) Maps have long been favored for their precise
depictions of static road elements, their accessibility constraints and
susceptibility to rapid environmental changes impede the widespread deployment
of autonomous driving, especially in the motion forecasting task. In this
context, we propose to leverage OpenStreetMap (OSM) as a promising alternative
to HD Maps for long-term motion forecasting. The contributions of this work are
threefold: firstly, we extend the application of OSM to long-horizon
forecasting, doubling the forecasting horizon compared to previous studies.
Secondly, through an expanded receptive field and the integration of
intersection priors, our OSM-based approach exhibits competitive performance,
narrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive
context-aware analysis, providing deeper insights in motion forecasting across
diverse scenarios as well as conducting class-aware comparisons. This research
not only advances long-term motion forecasting with coarse map representations
but additionally offers a potential scalable solution within the domain of
autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jing-Yan Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Parth Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_D/0/1/0/all/0/1&quot;&gt;David Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_H/0/1/0/all/0/1&quot;&gt;Henrik Christensen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02313">
<title>LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes. (arXiv:2311.02313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02313</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale semantic mapping is crucial for outdoor autonomous agents to
fulfill high-level tasks such as planning and navigation. This paper proposes a
novel method for large-scale 3D semantic reconstruction through implicit
representations from LiDAR measurements alone. We firstly leverages an
octree-based and hierarchical structure to store implicit features, then these
implicit features are decoded to semantic information and signed distance value
through shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelf
algorithms to predict the semantic labels and instance IDs of point cloud. Then
we jointly optimize the implicit features and MLPs parameters with
self-supervision paradigm for point cloud geometry and pseudo-supervision
pradigm for semantic and panoptic labels. Subsequently, Marching Cubes
algorithm is exploited to subdivide and visualize the scenes in the inferring
stage. For scenarios with memory constraints, a map stitching strategy is also
developed to merge sub-maps into a complete map. As far as we know, our method
is the first work to reconstruct semantic implicit scenes from LiDAR-only
input. Experiments on three real-world datasets, SemanticKITTI, SemanticPOSS
and nuScenes, demonstrate the effectiveness and efficiency of our framework
compared to current state-of-the-art 3D mapping methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiliu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02314">
<title>Thermal Face Image Classification using Deep Learning Techniques. (arXiv:2311.02314v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02314</link>
<description rdf:parseType="Literal">&lt;p&gt;Thermal images have various applications in security, medical and industrial
domains. This paper proposes a practical deep-learning approach for thermal
image classification. Accurate and efficient classification of thermal images
poses a significant challenge across various fields due to the complex image
content and the scarcity of annotated datasets. This work uses a convolutional
neural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to
extract features from thermal images. This work also applied Kalman filter on
thermal input images for image denoising. The experimental results demonstrate
the effectiveness of the proposed approach in terms of accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1&quot;&gt;Prosenjit Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_A/0/1/0/all/0/1&quot;&gt;ANK Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02315">
<title>Counting Manatee Aggregations using Deep Neural Networks and Anisotropic Gaussian Kernel. (arXiv:2311.02315v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02315</link>
<description rdf:parseType="Literal">&lt;p&gt;Manatees are aquatic mammals with voracious appetites. They rely on sea grass
as the main food source, and often spend up to eight hours a day grazing. They
move slow and frequently stay in group (i.e. aggregations) in shallow water to
search for food, making them vulnerable to environment change and other risks.
Accurate counting manatee aggregations within a region is not only biologically
meaningful in observing their habit, but also crucial for designing safety
rules for human boaters, divers, etc., as well as scheduling nursing,
intervention, and other plans. In this paper, we propose a deep learning based
crowd counting approach to automatically count number of manatees within a
region, by using low quality images as input. Because manatees have unique
shape and they often stay in shallow water in groups, water surface reflection,
occlusion, camouflage etc. making it difficult to accurately count manatee
numbers. To address the challenges, we propose to use Anisotropic Gaussian
Kernel (AGK), with tunable rotation and variances, to ensure that density
functions can maximally capture shapes of individual manatees in different
aggregations. After that, we apply AGK kernel to different types of deep neural
networks primarily designed for crowd counting, including VGG, SANet, Congested
Scene Recognition network (CSRNet), MARUNet etc. to learn manatee densities and
calculate number of manatees in the scene. By using generic low quality images
extracted from surveillance videos, our experiment results and comparison show
that AGK kernel based manatee counting achieves minimum Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). The proposed method works particularly
well for counting manatee aggregations in environments with complex background.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yiran Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulus_C/0/1/0/all/0/1&quot;&gt;Cihan Ulus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xingquan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02329">
<title>Complex Organ Mask Guided Radiology Report Generation. (arXiv:2311.02329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of automatic report generation is to generate a clinically accurate
and coherent phrase from a single given X-ray image, which could alleviate the
workload of traditional radiology reporting.However, in a real-world scenario,
radiologists frequently face the challenge of producing extensive reports
derived from numerous medical images, thereby medical report generation from
multi-image perspective is needed.In this paper, we propose the Complex Organ
Mask Guided (termed as COMG) report generation model, which incorporates masks
from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide
more detailed information and guide the model&apos;s attention to these crucial body
regions. Specifically, we leverage prior knowledge of the disease corresponding
to each organ in the fusion process to enhance the disease identification phase
during the report generation process. Additionally, cosine similarity loss is
introduced as target function to ensure the convergence of cross-modal
consistency and facilitate model optimization.Experimental results on two
public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms
of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiancheng_G/0/1/0/all/0/1&quot;&gt;Gu Tiancheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dongnan_L/0/1/0/all/0/1&quot;&gt;Liu Dongnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhiyuan_L/0/1/0/all/0/1&quot;&gt;Li Zhiyuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weidong_C/0/1/0/all/0/1&quot;&gt;Cai Weidong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.02551">
<title>A Survey of Fish Tracking Techniques Based on Computer Vision. (arXiv:2110.02551v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.02551</link>
<description rdf:parseType="Literal">&lt;p&gt;Fish tracking is a key technology for obtaining movement trajectories and
identifying abnormal behavior. However, it faces considerable challenges,
including occlusion, multi-scale tracking, and fish deformation. Notably,
extant reviews have focused more on behavioral analysis rather than providing a
comprehensive overview of computer vision-based fish tracking approaches. This
paper presents a comprehensive review of the advancements of fish tracking
technologies over the past seven years (2017-2023). It explores diverse fish
tracking techniques with an emphasis on fundamental localization and tracking
methods. Auxiliary plugins commonly integrated into fish tracking systems, such
as underwater image enhancement and re-identification, are also examined.
Additionally, this paper summarizes open-source datasets, evaluation metrics,
challenges, and applications in fish tracking research. Finally, a
comprehensive discussion offers insights and future directions for vision-based
fish tracking techniques. We hope that our work could provide a partial
reference in the development of fish tracking algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Meng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cen_C/0/1/0/all/0/1&quot;&gt;Chaojun Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yanyu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiannan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;You Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.01587">
<title>Improving accuracy and uncertainty quantification of deep learning based quantitative MRI using Monte Carlo dropout. (arXiv:2112.01587v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Dropout is conventionally used during the training phase as regularization
method and for quantifying uncertainty in deep learning. We propose to use
dropout during training as well as inference steps, and average multiple
predictions to improve the accuracy, while reducing and quantifying the
uncertainty. The results are evaluated for fractional anisotropy (FA) and mean
diffusivity (MD) maps which are obtained from only 3 direction scans. With our
method, accuracy can be improved significantly compared to network outputs
without dropout, especially when the training dataset is small. Moreover,
confidence maps are generated which may aid in diagnosis of unseen pathology or
artifacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Avci_M/0/1/0/all/0/1&quot;&gt;Mehmet Yigit Avci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qiuyun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Susie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1&quot;&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qiyuan Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13987">
<title>On the Viability of Monocular Depth Pre-training for Semantic Segmentation. (arXiv:2203.13987v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13987</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore how pre-training a model to infer depth from a single image
compares to pre-training the model for a semantic task, e.g. ImageNet
classification, for the purpose of downstream transfer to semantic
segmentation. The question of whether pre-training on geometric tasks is viable
for downstream transfer to semantic tasks is important for two reasons, one
practical and the other scientific. In practice, if it were viable, one could
reduce pre-training costs and bias due to human annotation at scale. If,
however, it were not, then that would affirm human annotation as an inductive
vehicle so powerful to justify the annotation effort. Yet the bootstrapping
question would still be unanswered: How did the ability to assign labels to
semantically coherent regions emerge? If pre-training on a geometric task was
sufficient to prime a notion of &apos;object&apos;, leveraging the regularities of the
environment (what Gibson called &apos;detached objects&apos;), that would reduce the gap
to semantic inference as a matter of aligning labels, which could be done with
few examples. To test these hypotheses, we have designed multiple controlled
experiments that require minimal fine-tuning, using common benchmarks such as
KITTI, Cityscapes, and NYU-V2: We explore different forms of supervision for
depth estimation, training pipelines, and data resolutions for semantic
fine-tuning. We find that depth pre-training exceeds performance relative to
ImageNet pre-training on average by 5.8% mIoU and 5.2% pixel accuracy.
Surprisingly, we find that optical flow estimation, which is a closely related
task to depth estimation as it optimizes the same photometric reprojection
error, is considerably less effective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1&quot;&gt;Dong Lao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alex Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Samuel Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.08610">
<title>Image Data Augmentation for Deep Learning: A Survey. (arXiv:2204.08610v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.08610</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has achieved remarkable results in many computer vision tasks.
Deep neural networks typically rely on large amounts of training data to avoid
overfitting. However, labeled data for real-world applications may be limited.
By improving the quantity and diversity of training data, data augmentation has
become an inevitable part of deep learning model training with image data.
&lt;/p&gt;
&lt;p&gt;As an effective way to improve the sufficiency and diversity of training
data, data augmentation has become a necessary part of successful application
of deep learning models on image data. In this paper, we systematically review
different image data augmentation methods. We propose a taxonomy of reviewed
methods and present the strengths and limitations of these methods. We also
conduct extensive experiments with various data augmentation methods on three
typical computer vision tasks, including semantic segmentation, image
classification and object detection. Finally, we discuss current challenges
faced by data augmentation and future research directions to put forward some
useful research guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Suorong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Weikang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Suhan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Furao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.12239">
<title>Gacs-Korner Common Information Variational Autoencoder. (arXiv:2205.12239v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.12239</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a notion of common information that allows one to quantify and
separate the information that is shared between two random variables from the
information that is unique to each. Our notion of common information is defined
by an optimization problem over a family of functions and recovers the
G\&apos;acs-K\&quot;orner common information as a special case. Importantly, our notion
can be approximated empirically using samples from the underlying data
distribution. We then provide a method to partition and quantify the common and
unique information using a simple modification of a traditional variational
auto-encoder. Empirically, we demonstrate that our formulation allows us to
learn semantically meaningful common and unique factors of variation even on
high-dimensional data such as images and videos. Moreover, on datasets where
ground-truth latent factors are known, we show that we can accurately quantify
the common information between the random variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinman_M/0/1/0/all/0/1&quot;&gt;Michael Kleinman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1&quot;&gt;Jonathan Kao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.16004">
<title>What Knowledge Gets Distilled in Knowledge Distillation?. (arXiv:2205.16004v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.16004</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation aims to transfer useful information from a teacher
network to a student network, with the primary goal of improving the student&apos;s
performance for the task at hand. Over the years, there has a been a deluge of
novel techniques and use cases of knowledge distillation. Yet, despite the
various improvements, there seems to be a glaring gap in the community&apos;s
fundamental understanding of the process. Specifically, what is the knowledge
that gets distilled in knowledge distillation? In other words, in what ways
does the student become similar to the teacher? Does it start to localize
objects in the same way? Does it get fooled by the same adversarial samples?
Does its data invariance properties become similar? Our work presents a
comprehensive study to try to answer these questions. We show that existing
methods can indeed indirectly distill these properties beyond improving task
performance. We further study why knowledge distillation might work this way,
and show that our findings have practical implications as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_U/0/1/0/all/0/1&quot;&gt;Utkarsh Ojha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Anirudh Sundara Rajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11921">
<title>Multi-scale data reconstruction of turbulent rotating flows with Gappy POD, Extended POD and Generative Adversarial Networks. (arXiv:2210.11921v2 [physics.flu-dyn] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11921</link>
<description rdf:parseType="Literal">&lt;p&gt;Data reconstruction of rotating turbulent snapshots is investigated utilizing
data-driven tools. This problem is crucial for numerous geophysical
applications and fundamental aspects, given the concurrent effects of direct
and inverse energy cascades, which lead to non-Gaussian statistics at both
large and small scales. Data assimilation also serves as a tool to rank
physical features within turbulence, by evaluating the performance of
reconstruction in terms of the quality and quantity of the information used.
Additionally, benchmarking various reconstruction techniques is essential to
assess the trade-off between quantitative supremacy, implementation complexity,
and explicability. In this study, we use linear and non-linear tools based on
the Proper Orthogonal Decomposition (POD) and Generative Adversarial Network
(GAN) for reconstructing rotating turbulence snapshots with spatial damages
(inpainting). We focus on accurately reproducing both statistical properties
and instantaneous velocity fields. Different gap sizes and gap geometries are
investigated in order to assess the importance of coherency and multi-scale
properties of the missing information. Surprisingly enough, concerning
point-wise reconstruction, the non-linear GAN does not outperform one of the
linear POD techniques. On the other hand, supremacy of the GAN approach is
shown when the statistical multi-scale properties are compared. Similarly,
extreme events in the gap region are better predicted when using GAN. The
balance between point-wise error and statistical properties is controlled by
the adversarial ratio, which determines the relative importance of the
generator and the discriminator in the GAN training. Robustness against the
measurement noise is also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1&quot;&gt;Michele Buzzicotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1&quot;&gt;Luca Biferale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1&quot;&gt;Fabio Bonaccorso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wan_M/0/1/0/all/0/1&quot;&gt;Minping Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14851">
<title>Performance evaluation of deep segmentation models for Contrails detection. (arXiv:2211.14851v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14851</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrails, short for condensation trails, are line-shaped ice clouds produced
by aircraft engine exhaust when they fly through cold and humid air. They
generate a greenhouse effect by absorbing or directing back to Earth
approximately 33% of emitted outgoing longwave radiation. They account for over
half of the climate change resulting from aviation activities. Avoiding
contrails and adjusting flight routes could be an inexpensive and effective way
to reduce their impact. An accurate, automated, and reliable detection
algorithm is required to develop and evaluate contrail avoidance strategies.
Advancement in contrail detection has been severely limited due to several
factors, primarily due to a lack of quality-labeled data. Recently, proposed a
large human-labeled Landsat-8 contrails dataset. Each contrail is carefully
labeled with various inputs in various scenes of Landsat-8 satellite imagery.
In this work, we benchmark several popular segmentation models with
combinations of different loss functions and encoder backbones. This work is
the first to apply state-of-the-art segmentation techniques to detect contrails
in low-orbit satellite imagery. Our work can also be used as an open benchmark
for contrail segmentation and is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandari_A/0/1/0/all/0/1&quot;&gt;Akshat Bhandari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1&quot;&gt;Sriya Rallabandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1&quot;&gt;Sanchit Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasliwal_A/0/1/0/all/0/1&quot;&gt;Aditya Kasliwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seth_P/0/1/0/all/0/1&quot;&gt;Pratinav Seth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06776">
<title>Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial Detection. (arXiv:2212.06776v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06776</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNN) define the state-of-the-art solution on
many perceptual tasks. However, current CNN approaches largely remain
vulnerable against adversarial perturbations of the input that have been
crafted specifically to fool the system while being quasi-imperceptible to the
human eye. In recent years, various approaches have been proposed to defend
CNNs against such attacks, for example by model hardening or by adding explicit
defence mechanisms. Thereby, a small &quot;detector&quot; is included in the network and
trained on the binary classification task of distinguishing genuine data from
data containing adversarial perturbations. In this work, we propose a simple
and light-weight detector, which leverages recent findings on the relation
between networks&apos; local intrinsic dimensionality (LID) and adversarial attacks.
Based on a re-interpretation of the LID measure and several simple adaptations,
we surpass the state-of-the-art on adversarial detection by a significant
margin and reach almost perfect results in terms of F1-score for several
networks and datasets. Sources available at:
https://github.com/adverML/multiLID
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07372">
<title>Image Compression with Product Quantized Masked Image Modeling. (arXiv:2212.07372v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07372</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent neural compression methods have been based on the popular hyperprior
framework. It relies on Scalar Quantization and offers a very strong
compression performance. This contrasts from recent advances in image
generation and representation learning, where Vector Quantization is more
commonly employed. In this work, we attempt to bring these lines of research
closer by revisiting vector quantization for image compression. We build upon
the VQ-VAE framework and introduce several modifications. First, we replace the
vanilla vector quantizer by a product quantizer. This intermediate solution
between vector and scalar quantization allows for a much wider set of
rate-distortion points: It implicitly defines high-quality quantizers that
would otherwise require intractably large codebooks. Second, inspired by the
success of Masked Image Modeling (MIM) in the context of self-supervised
learning and generative image models, we propose a novel conditional entropy
model which improves entropy coding by modelling the co-dependencies of the
quantized latent codes. The resulting PQ-MIM model is surprisingly effective:
its compression performance on par with recent hyperprior methods. It also
outperforms HiFiC in terms of FID and KID metrics when optimized with
perceptual losses (e.g. adversarial). Finally, since PQ-MIM is compatible with
image generation frameworks, we show qualitatively that it can operate under a
hybrid mode between compression and generation, with no further training or
finetuning. As a result, we explore the extreme compression regime where an
image is compressed into 200 bytes, i.e., less than a tweet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1&quot;&gt;Alaaeldin El-Nouby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muckley_M/0/1/0/all/0/1&quot;&gt;Matthew J. Muckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1&quot;&gt;Karen Ullrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1&quot;&gt;Ivan Laptev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07907">
<title>Automatic vehicle trajectory data reconstruction at scale. (arXiv:2212.07907v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07907</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose an automatic trajectory data reconciliation to
correct common errors in vision-based vehicle trajectory data. Given &quot;raw&quot;
vehicle detection and tracking information from automatic video processing
algorithms, we propose a pipeline including (a) an online data association
algorithm to match fragments that describe the same object (vehicle), which is
formulated as a min-cost network circulation problem of a graph, and (b) a
one-step trajectory rectification procedure formulated as a quadratic program
to enhance raw detection data. The pipeline leverages vehicle dynamics and
physical constraints to associate tracked objects when they become fragmented,
remove measurement noises and outliers and impute missing data due to
fragmentations. We assess the capability of the proposed two-step pipeline to
reconstruct three benchmarking datasets: (1) a microsimulation dataset that is
artificially downgraded to replicate upstream errors, (2) a 15-min NGSIM data
that is manually perturbed, and (3) tracking data consists of 3 scenes from
collections of video data recorded from 16-17 cameras on a section of the I-24
MOTION system, and compare with the corresponding manually-labeled ground truth
vehicle bounding boxes. All of the experiments show that the reconciled
trajectories improve the accuracy on all the tested input data for a wide range
of measures. Lastly, we show the design of a software architecture that is
currently deployed on the full-scale I-24 MOTION system consisting of 276
cameras that covers 4.2 miles of I-24. We demonstrate the scalability of the
proposed reconciliation pipeline to process high-volume data on a daily basis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloudemans_D/0/1/0/all/0/1&quot;&gt;Derek Gloudemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Junyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_Z/0/1/0/all/0/1&quot;&gt;Zi Nean Teoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lisa Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zachar_G/0/1/0/all/0/1&quot;&gt;Gergely Zach&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbour_W/0/1/0/all/0/1&quot;&gt;William Barbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Work_D/0/1/0/all/0/1&quot;&gt;Daniel Work&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02364">
<title>Object as Query: Lifting any 2D Object Detector to 3D Detection. (arXiv:2301.02364v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02364</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection from multi-view images has drawn much attention over the
past few years. Existing methods mainly establish 3D representations from
multi-view images and adopt a dense detection head for object detection, or
employ object queries distributed in 3D space to localize objects. In this
paper, we design Multi-View 2D Objects guided 3D Object Detector (MV2D), which
can lift any 2D object detector to multi-view 3D object detection. Since 2D
detections can provide valuable priors for object existence, MV2D exploits 2D
detectors to generate object queries conditioned on the rich image semantics.
These dynamically generated queries help MV2D to recall objects in the field of
view and show a strong capability of localizing 3D objects. For the generated
queries, we design a sparse cross attention module to force them to focus on
the features of specific objects, which suppresses interference from noises.
The evaluation results on the nuScenes dataset demonstrate the dynamic object
queries and sparse feature aggregation can promote 3D detection capability.
MV2D also exhibits a state-of-the-art performance among existing methods. We
hope MV2D can serve as a new baseline for future research. Code is available at
\url{https://github.com/tusen-ai/MV2D}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zitian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zehao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jiahui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03505">
<title>Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review. (arXiv:2301.03505v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03505</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable performance of the Transformer architecture in natural
language processing has recently also triggered broad interest in Computer
Vision. Among other merits, Transformers are witnessed as capable of learning
long-range dependencies and spatial correlations, which is a clear advantage
over convolutional neural networks (CNNs), which have been the de facto
standard in Computer Vision problems so far. Thus, Transformers have become an
integral part of modern medical image analysis. In this review, we provide an
encyclopedic review of the applications of Transformers in medical imaging.
Specifically, we present a systematic and thorough review of relevant recent
Transformer literature for different medical image analysis tasks, including
classification, segmentation, detection, registration, synthesis, and clinical
report generation. For each of these applications, we investigate the novelty,
strengths and weaknesses of the different proposed strategies and develop
taxonomies highlighting key properties and contributions. Further, if
applicable, we outline current benchmarks on different datasets. Finally, we
summarize key challenges and discuss different future research directions. In
addition, we have provided cited papers with their corresponding
implementations in https://github.com/mindflow-institue/Awesome-Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1&quot;&gt;Reza Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazerouni_A/0/1/0/all/0/1&quot;&gt;Amirhossein Kazerouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1&quot;&gt;Moein Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghdam_E/0/1/0/all/0/1&quot;&gt;Ehsan Khodapanah Aghdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molaei_A/0/1/0/all/0/1&quot;&gt;Amirali Molaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yiwei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_A/0/1/0/all/0/1&quot;&gt;Abin Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rijo Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07541">
<title>Generative Adversarial Networks to infer velocity components in rotating turbulent flows. (arXiv:2301.07541v2 [physics.flu-dyn] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07541</link>
<description rdf:parseType="Literal">&lt;p&gt;Inference problems for two-dimensional snapshots of rotating turbulent flows
are studied. We perform a systematic quantitative benchmark of point-wise and
statistical reconstruction capabilities of the linear Extended Proper
Orthogonal Decomposition (EPOD) method, a non-linear Convolutional Neural
Network (CNN) and a Generative Adversarial Network (GAN). We attack the
important task of inferring one velocity component out of the measurement of a
second one, and two cases are studied: (I) both components lay in the plane
orthogonal to the rotation axis and (II) one of the two is parallel to the
rotation axis. We show that EPOD method works well only for the former case
where both components are strongly correlated, while CNN and GAN always
outperform EPOD both concerning point-wise and statistical reconstructions. For
case (II), when the input and output data are weakly correlated, all methods
fail to reconstruct faithfully the point-wise information. In this case, only
GAN is able to reconstruct the field in a statistical sense. The analysis is
performed using both standard validation tools based on $L_2$ spatial distance
between the prediction and the ground truth and more sophisticated multi-scale
analysis using wavelet decomposition. Statistical validation is based on
standard Jensen-Shannon divergence between the probability density functions,
spectral properties and multi-scale flatness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Buzzicotti_M/0/1/0/all/0/1&quot;&gt;Michele Buzzicotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1&quot;&gt;Luca Biferale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bonaccorso_F/0/1/0/all/0/1&quot;&gt;Fabio Bonaccorso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10134">
<title>Bipartite Graph Diffusion Model for Human Interaction Generation. (arXiv:2301.10134v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10134</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1&quot;&gt;Baptiste Chopin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1&quot;&gt;Mohamed Daoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03014">
<title>Detection and Localization of Melanoma Skin Cancer in Histopathological Whole Slide Images. (arXiv:2302.03014v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03014</link>
<description rdf:parseType="Literal">&lt;p&gt;Melanoma diagnosed and treated in its early stages can increase the survival
rate. A projected increase in skin cancer incidents and a dearth of
dermatopathologists have emphasized the need for computational pathology
(CPATH) systems. CPATH systems with deep learning (DL) models have the
potential to identify the presence of melanoma by exploiting underlying
morphological and cellular features. This paper proposes a DL method to detect
melanoma and distinguish between normal skin and benign/malignant melanocytic
lesions in Whole Slide Images (WSI). Our method detects lesions with high
accuracy and localizes them on a WSI to identify potential regions of interest
for pathologists. Interestingly, our DL method relies on using a single CNN
network to create localization maps first and use them to perform slide-level
predictions to determine patients who have melanoma. Our best model provides
favorable patch-wise classification results with a 0.992 F1 score and 0.99
sensitivity on unseen data. The source code is
https://github.com/RogerAmundsen/Melanoma-Diagnosis-and-Localization-from-Whole-Slide-Images-using-Convolutional-Neural-Networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanwal_N/0/1/0/all/0/1&quot;&gt;Neel Kanwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Amundsen_R/0/1/0/all/0/1&quot;&gt;Roger Amundsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hardardottir_H/0/1/0/all/0/1&quot;&gt;Helga Hardardottir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomasetti_L/0/1/0/all/0/1&quot;&gt;Luca Tomasetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Undersrud_E/0/1/0/all/0/1&quot;&gt;Erling Sandoy Undersrud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janssen_E/0/1/0/all/0/1&quot;&gt;Emiel A.M. Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1&quot;&gt;Kjersti Engan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10798">
<title>Learning a Consensus Sub-Network with Polarization Regularization and One Pass Training. (arXiv:2302.10798v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10798</link>
<description rdf:parseType="Literal">&lt;p&gt;The subject of green AI has been gaining attention within the deep learning
community given the recent trend of ever larger and more complex neural network
models. Existing solutions for reducing the computational load of training at
inference time usually involve pruning the network parameters. Pruning schemes
often create extra overhead either by iterative training and fine-tuning for
static pruning or repeated computation of a dynamic pruning graph. We propose a
new parameter pruning strategy for learning a lighter-weight sub-network that
minimizes the energy cost while maintaining comparable performance to the fully
parameterised network on given downstream tasks. Our proposed pruning scheme is
green-oriented, as it only requires a one-off training to discover the optimal
static sub-networks by dynamic pruning methods. The pruning scheme consists of
a binary gating module and a novel loss function to uncover sub-networks with
user-defined sparsity. Our method enables pruning and training simultaneously,
which saves energy in both the training and inference phases and avoids extra
computational overhead from gating modules at inference time. Our results on
CIFAR-10 and CIFAR-100 suggest that our scheme can remove 50% of connections in
deep networks with less than 1% reduction in classification accuracy. Compared
to other related pruning methods, our method demonstrates a lower drop in
accuracy for equivalent reductions in computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_X/0/1/0/all/0/1&quot;&gt;Xiaoying Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babbar_V/0/1/0/all/0/1&quot;&gt;Varun Babbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Pheobe Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silavong_F/0/1/0/all/0/1&quot;&gt;Fran Silavong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1&quot;&gt;Ruibo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Sean Moran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11522">
<title>Evaluation of Extra Pixel Interpolation with Mask Processing for Medical Image Segmentation with Deep Learning. (arXiv:2302.11522v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11522</link>
<description rdf:parseType="Literal">&lt;p&gt;Current dataset mask processing operations relies on interpolation algorithms
that do not produce extra pixels, such as nearest neighbor (NN) interpolation,
as opposed to algorithms that do produce extra pixels, like bicubic (BIC) or
bilinear (BIL) interpolation. In our previous study, the author proposed an
alternative approach to NN-based mask processing and evaluated its effects on
deep learning training outcomes. In this study, the author evaluated the
effects of both BIC-based image and mask processing and BIC-and-NN-based image
and mask processing versus NN-based image and mask processing. The evaluation
revealed that the BIC-BIC model/network was an 8.9578 % (with image size 256 x
256) and a 1.0496 % (with image size 384 x 384) increase of the NN-NN network
compared to the NN-BIC network which was an 8.3127 % (with image size 256 x
256) and a 0.2887 % (with image size 384 x 384) increase of the NN-NN network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rukundo_O/0/1/0/all/0/1&quot;&gt;Olivier Rukundo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04772">
<title>Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04772</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based diffusion models (SBDM) have recently emerged as state-of-the-art
approaches for image generation. Existing SBDMs are typically formulated in a
finite-dimensional setting, where images are considered as tensors of finite
size. This paper develops SBDMs in the infinite-dimensional setting, that is,
we model the training data as functions supported on a rectangular domain.
Besides the quest for generating images at ever higher resolution, our primary
motivation is to create a well-posed infinite-dimensional learning problem so
that we can discretize it consistently on multiple resolution levels. We
thereby intend to obtain diffusion models that generalize across different
resolution levels and improve the efficiency of the training process. We
demonstrate how to overcome two shortcomings of current SBDM approaches in the
infinite-dimensional setting. First, we modify the forward process to ensure
that the latent distribution is well-defined in the infinite-dimensional
setting using the notion of trace class operators. We derive the reverse
processes for finite approximations. Second, we illustrate that approximating
the score function with an operator network is beneficial for multilevel
training. After deriving the convergence of the discretization and the
approximation of multilevel training, we implement an infinite-dimensional SBDM
approach and show the first promising results on MNIST and Fashion-MNIST,
underlining our developed theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagemann_P/0/1/0/all/0/1&quot;&gt;Paul Hagemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mildenberger_S/0/1/0/all/0/1&quot;&gt;Sophie Mildenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1&quot;&gt;Lars Ruthotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steidl_G/0/1/0/all/0/1&quot;&gt;Gabriele Steidl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Nicole Tianjiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09270">
<title>SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective. (arXiv:2303.09270v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09270</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to the power of vision-language foundation models, e.g., CLIP, the area
of image synthesis has seen recent important advances. Particularly, for style
transfer, CLIP enables transferring more general and abstract styles without
collecting the style images in advance, as the style can be efficiently
described with natural language, and the result is optimized by minimizing the
CLIP similarity between the text description and the stylized image. However,
directly using CLIP to guide style transfer leads to undesirable artifacts
(mainly written words and unrelated visual entities) spread over the image. In
this paper, we propose SpectralCLIP, which is based on a spectral
representation of the CLIP embedding sequence, where most of the common
artifacts occupy specific frequencies. By masking the band including these
frequencies, we can condition the generation process to adhere to the target
style properties (e.g., color, texture, paint stroke, etc.) while excluding the
generation of larger-scale structures corresponding to the artifacts.
Experimental results show that SpectralCLIP prevents the generation of
artifacts effectively in quantitative and qualitative terms, without impairing
the stylisation quality. We also apply SpectralCLIP to text-conditioned image
generation and show that it prevents written words in the generated images. Our
code is available at https://github.com/zipengxuc/SpectralCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zipeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_S/0/1/0/all/0/1&quot;&gt;Songlong Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1&quot;&gt;Enver Sangineto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12059">
<title>Motion Matters: Neural Motion Transfer for Better Camera Physiological Measurement. (arXiv:2303.12059v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12059</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models for camera-based physiological measurement can have
weak generalization due to a lack of representative training data. Body motion
is one of the most significant sources of noise when attempting to recover the
subtle cardiac pulse from a video. We explore motion transfer as a form of data
augmentation to introduce motion variation while preserving physiological
changes of interest. We adapt a neural video synthesis approach to augment
videos for the task of remote photoplethysmography (rPPG) and study the effects
of motion augmentation with respect to 1) the magnitude and 2) the type of
motion. After training on motion-augmented versions of publicly available
datasets, we demonstrate a 47% improvement over existing inter-dataset results
using various state-of-the-art methods on the PURE dataset. We also present
inter-dataset results on five benchmark datasets to show improvements of up to
79% using TS-CAN, a neural rPPG estimation method. Our findings illustrate the
usefulness of motion transfer as a data augmentation technique for improving
the generalization of models for camera-based physiological sensing. We release
our code for using motion transfer as a data augmentation technique on three
publicly available datasets, UBFC-rPPG, PURE, and SCAMPS, and models
pre-trained on motion-augmented data here: https://motion-matters.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paruchuri_A/0/1/0/all/0/1&quot;&gt;Akshay Paruchuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yulu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12513">
<title>Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12513</link>
<description rdf:parseType="Literal">&lt;p&gt;Most humans use visual imagination to understand and reason about language,
but models such as BERT reason about language using knowledge acquired during
text-only pretraining. In this work, we investigate whether vision-and-language
pretraining can improve performance on text-only tasks that involve implicit
visual reasoning, focusing primarily on zero-shot probing methods. We propose a
suite of visual language understanding (VLU) tasks for probing the visual
reasoning abilities of text encoder models, as well as various non-visual
natural language understanding (NLU) tasks for comparison. We also contribute a
novel zero-shot knowledge probing method, Stroop probing, for applying models
such as CLIP to text-only tasks without needing a prediction head such as the
masked language modelling head of models like BERT. We show that SOTA
multimodally trained text encoders outperform unimodally trained text encoders
on the VLU tasks while being underperformed by them on the NLU tasks, lending
new context to previously mixed results regarding the NLU capabilities of
multimodal models. We conclude that exposure to images during pretraining
affords inherent visual reasoning knowledge that is reflected in language-only
tasks that require implicit visual reasoning. Our findings bear importance in
the broader context of multimodal learning, providing principled guidelines for
the choice of text encoders used in such contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1&quot;&gt;Morris Alper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiman_M/0/1/0/all/0/1&quot;&gt;Michael Fiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1&quot;&gt;Hadar Averbuch-Elor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13129">
<title>Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations. (arXiv:2303.13129v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13129</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital human motion synthesis is a vibrant research field with applications
in movies, AR/VR, and video games. Whereas methods were proposed to generate
natural and realistic human motions, most only focus on modeling humans and
largely ignore object movements. Generating task-oriented human-object
interaction motions in simulation is challenging. For different intents of
using the objects, humans conduct various motions, which requires the human
first to approach the objects and then make them move consistently with the
human instead of staying still. Also, to deploy in downstream applications, the
synthesized motions are desired to be flexible in length, providing options to
personalize the predicted motions for various purposes. To this end, we propose
TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural
Representations, which generates full human-object interaction motions to
conduct specific tasks, given only the task type, the object, and a starting
human status. TOHO generates human-object motions in three steps: 1) it first
estimates the keyframe poses of conducting a task given the task type and
object information; 2) then, it infills the keyframes and generates continuous
motions; 3) finally, it applies a compact closed-form object motion estimation
to generate the object motion. Our method generates continuous motions that are
parameterized only by the temporal coordinate, which allows for upsampling or
downsampling of the sequence to arbitrary frames and adjusting the motion
speeds by designing the temporal coordinate vector. We demonstrate the
effectiveness of our method, both qualitatively and quantitatively. This work
takes a step further toward general human-scene interaction simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13764">
<title>GQE-Net: A Graph-based Quality Enhancement Network for Point Cloud Color Attribute. (arXiv:2303.13764v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13764</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, point clouds have become increasingly popular for
representing three-dimensional (3D) visual objects and scenes. To efficiently
store and transmit point clouds, compression methods have been developed, but
they often result in a degradation of quality. To reduce color distortion in
point clouds, we propose a graph-based quality enhancement network (GQE-Net)
that uses geometry information as an auxiliary input and graph convolution
blocks to extract local features efficiently. Specifically, we use a
parallel-serial graph attention module with a multi-head graph attention
mechanism to focus on important points or features and help them fuse together.
Additionally, we design a feature refinement module that takes into account the
normals and geometry distance between points. To work within the limitations of
GPU memory capacity, the distorted point cloud is divided into overlap-allowed
3D patches, which are sent to GQE-Net for quality enhancement. To account for
differences in data distribution among different color components, three models
are trained for the three color components. Experimental results show that our
method achieves state-of-the-art performance. For example, when implementing
GQE-Net on a recent test model of the geometry-based point cloud compression
(G-PCC) standard, 0.43 dB, 0.25 dB, and 0.36 dB Bjontegaard delta
(BD)-peak-signal-to-noise ratio (PSNR), corresponding to 14.0%, 9.3%, and 14.5%
BD-rate savings can be achieved on dense point clouds for the Y, Cb, and Cr
components, respectively. The source code of our method is available at
https://github.com/xjr998/GQE-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinrui Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamzaoui_R/0/1/0/all/0/1&quot;&gt;Raouf Hamzaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16604">
<title>Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16604</link>
<description rdf:parseType="Literal">&lt;p&gt;Composed image retrieval searches for a target image based on a multi-modal
user query comprised of a reference image and modification text describing the
desired changes. Existing approaches to solving this challenging task learn a
mapping from the (reference image, modification text)-pair to an image
embedding that is then matched against a large image corpus. One area that has
not yet been explored is the reverse direction, which asks the question, what
reference image when modified as described by the text would produce the given
target image? In this work we propose a bi-directional training scheme that
leverages such reversed queries and can be applied to existing composed image
retrieval architectures with minimum changes, which improves the performance of
the model. To encode the bi-directional query we prepend a learnable token to
the modification text that designates the direction of the query and then
finetune the parameters of the text embedding module. We make no other changes
to the network architecture. Experiments on two standard datasets show that our
novel approach achieves improved performance over a baseline BLIP-based model
that itself already achieves competitive performance. Our code is released at
https://github.com/Cuberick-Orion/Bi-Blip4CIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1&quot;&gt;Damien Teney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03164">
<title>Synthesizing Anyone, Anywhere, in Any Pose. (arXiv:2304.03164v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03164</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the task of in-the-wild human figure synthesis, where the primary
goal is to synthesize a full body given any region in any image. In-the-wild
human figure synthesis has long been a challenging and under-explored task,
where current methods struggle to handle extreme poses, occluding objects, and
complex backgrounds.
&lt;/p&gt;
&lt;p&gt;Our main contribution is TriA-GAN, a keypoint-guided GAN that can synthesize
Anyone, Anywhere, in Any given pose. Key to our method is projected GANs
combined with a well-crafted training strategy, where our simple generator
architecture can successfully handle the challenges of in-the-wild full-body
synthesis. We show that TriA-GAN significantly improves over previous
in-the-wild full-body synthesis methods, all while requiring less conditional
information for synthesis (keypoints \vs DensePose). Finally, we show that the
latent space of TriA-GAN is compatible with standard unconditional editing
techniques, enabling text-guided editing of generated human figures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hukkelaas_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe5;kon Hukkel&amp;#xe5;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindseth_F/0/1/0/all/0/1&quot;&gt;Frank Lindseth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06286">
<title>Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report. (arXiv:2304.06286v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06286</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated interpretation of electrocardiograms (ECG) has garnered significant
attention with the advancements in machine learning methodologies. Despite the
growing interest, most current studies focus solely on classification or
regression tasks, which overlook a crucial aspect of clinical cardio-disease
diagnosis: the diagnostic report generated by experienced human clinicians. In
this paper, we introduce a novel approach to ECG interpretation, leveraging
recent breakthroughs in Large Language Models (LLMs) and Vision-Transformer
(ViT) models. Rather than treating ECG diagnosis as a classification or
regression task, we propose an alternative method of automatically identifying
the most similar clinical cases based on the input ECG data. Also, since
interpreting ECG as images is more affordable and accessible, we process ECG as
encoded images and adopt a vision-language learning paradigm to jointly learn
vision-language alignment between encoded ECG images and ECG diagnosis reports.
Encoding ECG into images can result in an efficient ECG retrieval system, which
will be highly practical and useful in clinical applications. More importantly,
our findings could serve as a crucial resource for providing diagnostic
services in underdeveloped regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shiqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;William Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duan_C/0/1/0/all/0/1&quot;&gt;Chaojing Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rosenberg_M/0/1/0/all/0/1&quot;&gt;Michael Rosenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Emerson Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weber_D/0/1/0/all/0/1&quot;&gt;Douglas Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06813">
<title>Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06813</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection aims to identify test examples that do
not belong to the training distribution and are thus unlikely to be predicted
reliably. Despite a plethora of existing works, most of them focused only on
the scenario where OOD examples come from semantic shift (e.g., unseen
categories), ignoring other possible causes (e.g., covariate shift). In this
paper, we present a novel, unifying framework to study OOD detection in a
broader scope. Instead of detecting OOD examples from a particular cause, we
propose to detect examples that a deployed machine learning model (e.g., an
image classifier) is unable to predict correctly. That is, whether a test
example should be detected and rejected or not is ``model-specific&apos;&apos;. We show
that this framework unifies the detection of OOD examples caused by semantic
shift and covariate shift, and closely addresses the concern of applying a
machine learning model to uncontrolled environments. We provide an extensive
analysis that involves a variety of models (e.g., different architectures and
training strategies), sources of OOD examples, and OOD detection approaches,
and reveal several insights into improving and understanding OOD detection in
uncontrolled environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averly_R/0/1/0/all/0/1&quot;&gt;Reza Averly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13219">
<title>ZRG: A Dataset for Multimodal 3D Residential Rooftop Understanding. (arXiv:2304.13219v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13219</link>
<description rdf:parseType="Literal">&lt;p&gt;A crucial part of any home is the roof over our heads to protect us from the
elements. In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset
for residential rooftop understanding. ZRG is a large-scale residential rooftop
dataset of over 20k properties collected through roof inspections from across
the U.S. and contains multiple modalities including high resolution aerial
orthomosaics, digital surface models (DSM), colored point clouds, and 3D roof
wireframe annotations. We provide an in-depth analysis and perform several
experimental baselines including roof outline extraction, monocular height
estimation, and planar roof structure extraction, to illustrate a few of the
numerous potential applications unlocked by this dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corley_I/0/1/0/all/0/1&quot;&gt;Isaac Corley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lwowski_J/0/1/0/all/0/1&quot;&gt;Jonathan Lwowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1&quot;&gt;Peyman Najafirad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02541">
<title>Catch Missing Details: Image Reconstruction with Frequency Augmented Variational Autoencoder. (arXiv:2305.02541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02541</link>
<description rdf:parseType="Literal">&lt;p&gt;The popular VQ-VAE models reconstruct images through learning a discrete
codebook but suffer from a significant issue in the rapid quality degradation
of image reconstruction as the compression rate rises. One major reason is that
a higher compression rate induces more loss of visual signals on the higher
frequency spectrum which reflect the details on pixel space. In this paper, a
Frequency Complement Module (FCM) architecture is proposed to capture the
missing frequency information for enhancing reconstruction quality. The FCM can
be easily incorporated into the VQ-VAE structure, and we refer to the new model
as Frequency Augmented VAE (FA-VAE). In addition, a Dynamic Spectrum Loss (DSL)
is introduced to guide the FCMs to balance between various frequencies
dynamically for optimal reconstruction. FA-VAE is further extended to the
text-to-image synthesis task, and a Cross-attention Autoregressive Transformer
(CAT) is proposed to obtain more precise semantic attributes in texts.
Extensive reconstruction experiments with different compression rates are
conducted on several benchmark datasets, and the results demonstrate that the
proposed FA-VAE is able to restore more faithfully the details compared to SOTA
methods. CAT also shows improved generation quality with better image-text
semantic alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xinmiao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1&quot;&gt;Jenhao Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1&quot;&gt;Chiuman Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Yu Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05803">
<title>Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05803</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised semantic segmentation (WSSS) aims to bypass the need for
laborious pixel-level annotation by using only image-level annotation. Most
existing methods rely on Class Activation Maps (CAM) to derive pixel-level
pseudo-labels and use them to train a fully supervised semantic segmentation
model. Although these pseudo-labels are class-aware, indicating the coarse
regions for particular classes, they are not object-aware and fail to delineate
accurate object boundaries. To address this, we introduce a simple yet
effective method harnessing the Segment Anything Model (SAM), a class-agnostic
foundation model capable of producing fine-grained instance masks of objects,
parts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM
masks, resulting in high-quality pseudo-labels that are both class-aware and
object-aware. Our approach is highly versatile and can be easily integrated
into existing WSSS methods without any modification. Despite its simplicity,
our approach shows consistent gain over the state-of-the-art WSSS methods on
both PASCAL VOC and MS-COCO datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianle Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1&quot;&gt;Zheda Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-lun Chao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07015">
<title>Exploiting Diffusion Prior for Real-World Image Super-Resolution. (arXiv:2305.07015v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07015</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
employ a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches. Code and models are available at
https://github.com/IceClear/StableSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shangchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1&quot;&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07812">
<title>Lightweight Delivery Detection on Doorbell Cameras. (arXiv:2305.07812v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07812</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advances in video-based action recognition and robust
spatio-temporal modeling, most of the proposed approaches rely on the abundance
of computational resources to afford running huge and computation-intensive
convolutional or transformer-based neural networks to obtain satisfactory
results. This limits the deployment of such models on edge devices with limited
power and computing resources. In this work we investigate an important smart
home application, video based delivery detection, and present a simple and
lightweight pipeline for this task that can run on resource-constrained
doorbell cameras. Our method relies on motion cues to generate a set of coarse
activity proposals followed by their classification with a mobile-friendly
3DCNN network. To train we design a novel semi-supervised attention module that
helps the network to learn robust spatio-temporal features and adopt an
evidence-based optimization objective that allows for quantifying the
uncertainty of predictions made by the network. Experimental results on our
curated delivery dataset shows the significant effectiveness of our pipeline
and highlights the benefits of our training phase novelties to achieve free and
considerable inference-time performance gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorramshahi_P/0/1/0/all/0/1&quot;&gt;Pirazh Khorramshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deluccia_L/0/1/0/all/0/1&quot;&gt;Luke Deluccia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongcheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08675">
<title>Improved baselines for vision-language pre-training. (arXiv:2305.08675v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08675</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has emerged as an efficient framework to learn
multimodal representations. CLIP, a seminal work in this area, achieved
impressive results by training on paired image-text data using the contrastive
loss. Recent work claims improvements over CLIP using additional
non-contrastive losses inspired from self-supervised learning. However, it is
sometimes hard to disentangle the contribution of these additional losses from
other implementation details, e.g., data augmentation or regularization
techniques, used to train the model. To shed light on this matter, in this
paper, we first propose, implement and evaluate several baselines obtained by
combining contrastive learning with recent advances in self-supervised
learning. In particular, we use the loss functions that were proven successful
for visual self-supervised learning to align image and text modalities. We find
that these baselines outperform a basic implementation of CLIP. However, when a
stronger training recipe is employed, the advantage disappears. Indeed, we find
that a simple CLIP baseline can also be improved substantially, up to a 25%
relative improvement on downstream zero-shot tasks, by using well-known
training techniques that are popular in other subfields. Moreover, we discover
that it is enough to apply image and text augmentations to make up for most of
the improvement attained by prior works. With our improved training recipe for
CLIP, we obtain state-of-the-art performance on four standard datasets, and
consistently outperform prior work (up to +4% on the largest dataset), while
being substantially simpler. The code is available at
https://github.com/facebookresearch/clip-rocket
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1&quot;&gt;Enrico Fini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astolfi_P/0/1/0/all/0/1&quot;&gt;Pietro Astolfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1&quot;&gt;Adriana Romero-Soriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1&quot;&gt;Michal Drozdzal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08946">
<title>Image Matching by Bare Homography. (arXiv:2305.08946v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Slime, a novel non-deep image matching framework which
models the scene as rough local overlapping planes. This intermediate
representation sits in-between the local affine approximation of the keypoint
patches and the global matching based on both spatial and similarity
constraints, providing a progressive pruning of the correspondences, as planes
are easier to handle with respect to general scenes.
&lt;/p&gt;
&lt;p&gt;Slime decomposes the images into overlapping regions at different scales and
computes loose planar homographies. Planes are mutually extended by compatible
matches and the images are split into fixed tiles, with only the best
homographies retained for each pair of tiles. Stable matches are identified
according to the consensus of the admissible stereo configurations provided by
pairwise homographies. Within tiles, the rough planes are then merged according
to their overlap in terms of matches and further consistent correspondences are
extracted.
&lt;/p&gt;
&lt;p&gt;The whole process only involves homography constraints. As a result, both the
coverage and the stability of correct matches over the scene are amplified,
together with the ability to spot matches in challenging scenes, allowing
traditional hybrid matching pipelines to make up lost ground against recent
end-to-end deep matching methods.
&lt;/p&gt;
&lt;p&gt;In addition, the paper gives a thorough comparative analysis of recent
state-of-the-art in image matching represented by end-to-end deep networks and
hybrid pipelines. The evaluation considers both planar and non-planar scenes,
taking into account critical and challenging scenarios including abrupt
temporal image changes and strong variations in relative image rotations.
According to this analysis, although the impressive progress done in this
field, there is still a wide room for improvements to be investigated in future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1&quot;&gt;Fabio Bellavia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11049">
<title>NODE-ImgNet: a PDE-informed effective and robust model for image denoising. (arXiv:2305.11049v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11049</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the traditional partial differential equation (PDE) approach for
image denoising, we propose a novel neural network architecture, referred as
NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with
convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE
model, where the dynamic system is learned implicitly without the explicit
specification of the PDE. This naturally circumvents the typical issues
associated with introducing artifacts during the learning process. By invoking
such a NODE structure, which can also be viewed as a continuous variant of a
residual network (ResNet) and inherits its advantage in image denoising, our
model achieves enhanced accuracy and parameter efficiency. In particular, our
model exhibits consistent effectiveness in different scenarios, including
denoising gray and color images perturbed by Gaussian noise, as well as
real-noisy images, and demonstrates superiority in learning from small image
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xinheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Hao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Cuiyu He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12529">
<title>DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12529</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DreamWaltz, a novel framework for generating and animating complex
3D avatars given text guidance and parametric human body prior. While recent
methods have shown encouraging results for text-to-3D generation of common
objects, creating high-quality and animatable 3D avatars remains challenging.
To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent
occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural
representations with canonical poses. It provides view-aligned supervision via
3D-aware skeleton conditioning which enables complex avatar generation without
artifacts and multiple faces. For animation, our method learns an animatable 3D
avatar representation from abundant image priors of diffusion model conditioned
on various poses, which could animate complex non-rigged avatars given
arbitrary poses without retraining. Extensive evaluations demonstrate that
DreamWaltz is an effective and robust approach for creating 3D avatars that can
take on complex shapes and appearances as well as novel poses for animation.
The proposed framework further enables the creation of complex scenes with
diverse compositions, including avatar-avatar, avatar-object and avatar-scene
interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and
animation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yukun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;He Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xianbiao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13903">
<title>Let&apos;s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought. (arXiv:2305.13903v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13903</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite exciting recent results showing vision-language systems&apos; capacity to
reason about images using natural language, their capacity for video reasoning
remains under-explored. We motivate framing video reasoning as the sequential
understanding of a small number of keyframes, thereby leveraging the power and
robustness of vision-language while alleviating the computational complexities
of processing videos. To evaluate this novel application, we introduce VIP, an
inference-time challenge dataset designed to explore models&apos; reasoning
capabilities through video chain-of-thought. Inspired by visually descriptive
scene plays, we propose two formats for keyframe description: unstructured
dense captions and structured scene descriptions that identify the focus,
action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video
reasoning, we propose two tasks: Video Infilling and Video Prediction, which
test abilities to generate multiple intermediate keyframes and predict future
keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP,
demonstrate the performance gap in these complex video reasoning tasks, and
encourage future work to prioritize language models for efficient and
generalized video reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himakunthala_V/0/1/0/all/0/1&quot;&gt;Vaishnavi Himakunthala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_A/0/1/0/all/0/1&quot;&gt;Andy Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rose_D/0/1/0/all/0/1&quot;&gt;Daniel Rose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ryan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_A/0/1/0/all/0/1&quot;&gt;Alex Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonar_C/0/1/0/all/0/1&quot;&gt;Chinmay Sonar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1&quot;&gt;Michael Saxon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16914">
<title>PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16914</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images
and camera poses for Novel View Synthesis (NVS). Although NeRF can produce
photorealistic results, it often suffers from overfitting to training views,
leading to poor geometry reconstruction, especially in low-texture areas. This
limitation restricts many important applications which require accurate
geometry, such as extrapolated NVS, HD mapping and scene editing. To address
this limitation, we propose a new method to improve NeRF&apos;s 3D structure using
only RGB images and semantic maps. Our approach introduces a novel plane
regularization based on Singular Value Decomposition (SVD), that does not rely
on any geometric prior. In addition, we leverage the Structural Similarity
Index Measure (SSIM) in our loss design to properly initialize the volumetric
representation of NeRF. Quantitative and qualitative results show that our
method outperforms popular regularization approaches in accurate geometry
reconstruction for large-scale outdoor scenes and achieves SoTA rendering
quality on the KITTI-360 NVS benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fusang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louys_A/0/1/0/all/0/1&quot;&gt;Arnaud Louys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piasco_N/0/1/0/all/0/1&quot;&gt;Nathan Piasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennehar_M/0/1/0/all/0/1&quot;&gt;Moussab Bennehar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1&quot;&gt;Luis Rold&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1&quot;&gt;Dzmitry Tsishkou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17600">
<title>NashFormer: Leveraging Local Nash Equilibria for Semantically Diverse Trajectory Prediction. (arXiv:2305.17600v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17600</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactions between road agents present a significant challenge in
trajectory prediction, especially in cases involving multiple agents. Because
existing diversity-aware predictors do not account for the interactive nature
of multi-agent predictions, they may miss these important interaction outcomes.
In this paper, we propose NashFormer, a framework for trajectory prediction
that leverages game-theoretic inverse reinforcement learning to improve
coverage of multi-modal predictions. We use a training-time game-theoretic
analysis as an auxiliary loss resulting in improved coverage and accuracy
without presuming a taxonomy of actions for the agents. We demonstrate our
approach on the interactive split of the Waymo Open Motion Dataset, including
four subsets involving scenarios with high interaction complexity. Experiment
results show that our predictor produces accurate predictions while covering
$33\%$ more potential interactions versus a baseline model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lidard_J/0/1/0/all/0/1&quot;&gt;Justin Lidard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_O/0/1/0/all/0/1&quot;&gt;Oswin So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanxia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeCastro_J/0/1/0/all/0/1&quot;&gt;Jonathan DeCastro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiongyi Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1&quot;&gt;Yen-Ling Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1&quot;&gt;John Leonard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balachandran_A/0/1/0/all/0/1&quot;&gt;Avinash Balachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonard_N/0/1/0/all/0/1&quot;&gt;Naomi Leonard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1&quot;&gt;Guy Rosman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18273">
<title>Pix2Repair: Implicit Shape Restoration from Images. (arXiv:2305.18273v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18273</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Pix2Repair, an automated shape repair approach that generates
restoration shapes from images to repair fractured objects. Prior repair
approaches require a high-resolution watertight 3D mesh of the fractured object
as input. Input 3D meshes must be obtained using expensive 3D scanners, and
scanned meshes require manual cleanup, limiting accessibility and scalability.
Pix2Repair takes an image of the fractured object as input and automatically
generates a 3D printable restoration shape. We contribute a novel shape
function that deconstructs a latent code representing the fractured object into
a complete shape and a break surface. We show restorations for synthetic
fractures from the Geometric Breaks and Breaking Bad datasets, and cultural
heritage objects from the QP dataset, and for real fractures from the Fantastic
Breaks dataset. We overcome challenges in restoring axially symmetric objects
by predicting view-centered restorations. Our approach outperforms shape
completion approaches adapted for shape repair in terms of chamfer distance,
earth mover&apos;s distance, normal consistency, and percent restorations generated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xinchao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_N/0/1/0/all/0/1&quot;&gt;Nikolas Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Sean Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_N/0/1/0/all/0/1&quot;&gt;Natasha Kholgade Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00975">
<title>Active Vision Reinforcement Learning under Limited Visual Observability. (arXiv:2306.00975v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00975</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate Active Vision Reinforcement Learning
(ActiveVision-RL), where an embodied agent simultaneously learns action policy
for the task while also controlling its visual observations in partially
observable environments. We denote the former as motor policy and the latter as
sensory policy. For example, humans solve real world tasks by hand manipulation
(motor policy) together with eye movements (sensory policy). ActiveVision-RL
poses challenges on coordinating two policies given their mutual influence. We
propose SUGARL, Sensorimotor Understanding Guided Active Reinforcement
Learning, a framework that models motor and sensory policies separately, but
jointly learns them using with an intrinsic sensorimotor reward. This learnable
reward is assigned by sensorimotor reward module, incentivizes the sensory
policy to select observations that are optimal to infer its own motor action,
inspired by the sensorimotor stage of humans. Through a series of experiments,
we show the effectiveness of our method across a range of observability
conditions and its adaptability to existed RL algorithms. The sensory policies
learned through our method are observed to exhibit effective active vision
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jinghuan Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05401">
<title>RDumb: A simple approach that questions our progress in continual test-time adaptation. (arXiv:2306.05401v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05401</link>
<description rdf:parseType="Literal">&lt;p&gt;Test-Time Adaptation (TTA) allows to update pre-trained models to changing
data distributions at deployment time. While early work tested these algorithms
for individual fixed distribution shifts, recent work proposed and applied
methods for continual adaptation over long timescales. To examine the reported
progress in the field, we propose the Continually Changing Corruptions (CCC)
benchmark to measure asymptotic performance of TTA techniques. We find that
eventually all but one state-of-the-art methods collapse and perform worse than
a non-adapting model, including models specifically proposed to be robust to
performance collapse. In addition, we introduce a simple baseline, &quot;RDumb&quot;,
that periodically resets the model to its pretrained state. RDumb performs
better or on par with the previously proposed state-of-the-art in all
considered benchmarks. Our results show that previous TTA approaches are
neither effective at regularizing adaptation to avoid collapse nor able to
outperform a simplistic resetting strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1&quot;&gt;Ori Press&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1&quot;&gt;Steffen Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1&quot;&gt;Matthias K&amp;#xfc;mmerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05720">
<title>Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05720</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent diffusion models (LDMs) exhibit an impressive ability to produce
realistic images, yet the inner workings of these models remain mysterious.
Even when trained purely on images without explicit depth information, they
typically output coherent pictures of 3D scenes. In this work, we investigate a
basic interpretability question: does an LDM create and use an internal
representation of simple scene geometry? Using linear probes, we find evidence
that the internal activations of the LDM encode linear representations of both
3D depth data and a salient-object / background distinction. These
representations appear surprisingly early in the denoising process$-$well
before a human can easily make sense of the noisy images. Intervention
experiments further indicate these representations play a causal role in image
synthesis, and may be used for simple high-level editing of an LDM&apos;s output.
Project page: https://yc015.github.io/scene-representation-diffusion-model/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yida Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1&quot;&gt;Fernanda Vi&amp;#xe9;gas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06203">
<title>FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06203</link>
<description rdf:parseType="Literal">&lt;p&gt;Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO,
VICReg,MOCOv3) target primarily on representations at instance level and do not
generalize well to dense prediction tasks, such as object detection and
segmentation.Towards aligning SSL with dense predictions, this paper
demonstrates for the first time the underlying mean-shift clustering process of
Vision Transformers (ViT), which aligns well with natural image semantics
(e.g., a world of objects and stuffs). By employing transformer for joint
embedding and clustering, we propose a two-level feature clustering SSL method,
coined Feature-Level Self-supervised Learning (FLSL). We present the formal
definition of the FLSL problem and construct the objectives from the mean-shift
and k-means perspectives. We show that FLSL promotes remarkable semantic
cluster representations and learns an embedding scheme amenable to intra-view
and inter-view feature clustering. Experiments show that FLSL yields
significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP
and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in
instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as
backbone, respectively. FLSL consistently outperforms existing SSL methods
across additional benchmarks, including UAV17 object detection on UAVDT, and
video instance segmentation on DAVIS 2017.We conclude by presenting
visualization and various ablation studies to better understand the success of
FLSL. The source code is available at https://github.com/ISL-CV/FLSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netchaev_A/0/1/0/all/0/1&quot;&gt;Anton Netchaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shihao Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06687">
<title>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark. (arXiv:2306.06687v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06687</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have emerged as a promising approach towards achieving
general-purpose AI agents. The thriving open-source LLM community has greatly
accelerated the development of agents that support human-machine dialogue
interaction through natural language processing. However, human interaction
with the world extends beyond only text as a modality, and other modalities
such as vision are also crucial. Recent works on multi-modal large language
models, such as GPT-4V and Bard, have demonstrated their effectiveness in
handling visual modalities. However, the transparency of these works is limited
and insufficient to support academic research. To the best of our knowledge, we
present one of the very first open-source endeavors in the field, LAMM,
encompassing a Language-Assisted Multi-Modal instruction tuning dataset,
framework, and benchmark. Our aim is to establish LAMM as a growing ecosystem
for training and evaluating MLLMs, with a specific focus on facilitating AI
agents capable of bridging the gap between ideas and execution, thereby
enabling seamless human-AI interaction. Our main contribution is three-fold: 1)
We present a comprehensive dataset and benchmark, which cover a wide range of
vision tasks for 2D and 3D vision. Extensive experiments validate the
effectiveness of our dataset and benchmark. 2) We outline the detailed
methodology of constructing multi-modal instruction tuning datasets and
benchmarks for MLLMs, enabling rapid scaling and extension of MLLM research to
diverse domains, tasks, and modalities. 3) We provide a primary but potential
MLLM training framework optimized for modality extension. We also provide
baseline models, comprehensive experimental observations, and analysis to
accelerate future research. Our baseline model is trained within 24 A100 GPU
hours, framework supports training with V100 and RTX3090 is available thanks to
the open-source society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhenfei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jianjian Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhelun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dingning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mukai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoshui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06874">
<title>VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06874</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Models (DMs) are state-of-the-art generative models that learn a
reversible corruption process from iterative noise addition and denoising. They
are the backbone of many generative AI applications, such as text-to-image
conditional generation. However, recent studies have shown that basic
unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a
type of output manipulation attack triggered by a maliciously embedded pattern
at model input. This paper presents a unified backdoor attack framework
(VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our
framework covers mainstream unconditional and conditional DMs (denoising-based
and score-based) and various training-free samplers for holistic evaluations.
Experiments show that our unified framework facilitates the backdoor analysis
of different DM configurations and provides new insights into caption-based
backdoor attacks on DMs. Our code is available on GitHub:
\url{https://github.com/IBM/villandiffusion}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1&quot;&gt;Sheng-Yen Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08687">
<title>Norm-guided latent space exploration for text-to-image generation. (arXiv:2306.08687v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08687</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models show great potential in synthesizing a large
variety of concepts in new compositions and scenarios. However, the latent
space of initial seeds is still not well understood and its structure was shown
to impact the generation of various concepts. Specifically, simple operations
like interpolation and finding the centroid of a set of seeds perform poorly
when using standard Euclidean or spherical metrics in the latent space. This
paper makes the observation that, in current training procedures, diffusion
models observed inputs with a narrow range of norm values. This has strong
implications for methods that rely on seed manipulation for image generation,
with applications to few-shot and long-tail learning tasks. To address this
issue, we propose a novel method for interpolating between two seeds and
demonstrate that it defines a new non-Euclidean metric that takes into account
a norm-based prior on seeds. We describe a simple yet efficient algorithm for
approximating this interpolation procedure and use it to further define
centroids in the latent seed space. We show that our new interpolation and
centroid techniques significantly enhance the generation of rare concept
images. This further leads to state-of-the-art performance on few-shot and
long-tail benchmarks, improving prior approaches in terms of generation speed,
image quality, and semantic content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1&quot;&gt;Dvir Samuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1&quot;&gt;Rami Ben-Ari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1&quot;&gt;Nir Darshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1&quot;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09869">
<title>Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09869</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework for adaptive context
control by modeling the posterior of context vectors. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing. Code:
https://github.com/EnergyAttention/Energy-Based-CrossAttention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Geon Yeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeongsol Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Beomsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang Wan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11368">
<title>RoMe: Towards Large Scale Road Surface Reconstruction via Mesh Representation. (arXiv:2306.11368v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11368</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous driving applications, accurate and efficient road surface
reconstruction is paramount. This paper introduces RoMe, a novel framework
designed for the robust reconstruction of large-scale road surfaces. Leveraging
a unique mesh representation, RoMe ensures that the reconstructed road surfaces
are accurate and seamlessly aligned with semantics. To address challenges in
computational efficiency, we propose a waypoint sampling strategy, enabling
RoMe to reconstruct vast environments by focusing on sub-areas and subsequently
merging them. Furthermore, we incorporate an extrinsic optimization module to
enhance the robustness against inaccuracies in extrinsic calibration. Our
extensive evaluations of both public datasets and wild data underscore RoMe&apos;s
superiority in terms of speed, accuracy, and robustness. For instance, it costs
only 2 GPU hours to recover a road surface of 600*600 square meters from
thousands of images. Notably, RoMe&apos;s capability extends beyond mere
reconstruction, offering significant value for auto-labeling tasks in
autonomous driving applications. All related data and code are available at
https://github.com/DRosemei/RoMe.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_R/0/1/0/all/0/1&quot;&gt;Ruohong Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_W/0/1/0/all/0/1&quot;&gt;Wei Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xue Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11582">
<title>Computing a human-like reaction time metric from stable recurrent vision models. (arXiv:2306.11582v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11582</link>
<description rdf:parseType="Literal">&lt;p&gt;The meteoric rise in the adoption of deep neural networks as computational
models of vision has inspired efforts to &quot;align&quot; these models with humans. One
dimension of interest for alignment includes behavioral choices, but moving
beyond characterizing choice patterns to capturing temporal aspects of visual
decision-making has been challenging. Here, we sketch a general-purpose
methodology to construct computational accounts of reaction times from a
stimulus-computable, task-optimized model. Specifically, we introduce a novel
metric leveraging insights from subjective logic theory summarizing evidence
accumulation in recurrent vision models. We demonstrate that our metric aligns
with patterns of human reaction times for stimulus manipulations across four
disparate visual decision-making tasks spanning perceptual grouping, mental
simulation, and scene categorization. This work paves the way for exploring the
temporal alignment of model and human visual strategies in the context of
various other cognitive tasks toward generating testable hypotheses for
neuroscience. Links to the code and data can be found on the project page:
https://serre-lab.github.io/rnn_rts_site.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goetschalckx_L/0/1/0/all/0/1&quot;&gt;Lore Goetschalckx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindarajan_L/0/1/0/all/0/1&quot;&gt;Lakshmi Narasimhan Govindarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1&quot;&gt;Alekh Karkada Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1&quot;&gt;Aarit Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheinberg_D/0/1/0/all/0/1&quot;&gt;David L. Sheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1&quot;&gt;Thomas Serre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11739">
<title>Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior. (arXiv:2306.11739v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11739</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object reconstruction is important for semantic scene understanding. It is
challenging to reconstruct detailed 3D shapes from monocular images directly
due to a lack of depth information, occlusion and noise. Most current methods
generate deterministic object models without any awareness of the uncertainty
of the reconstruction. We tackle this problem by leveraging a neural object
representation which learns an object shape distribution from large dataset of
3d object models and maps it into a latent space. We propose a method to model
uncertainty as part of the representation and define an uncertainty-aware
encoder which generates latent codes with uncertainty directly from individual
input images. Further, we propose a method to propagate the uncertainty in the
latent code to SDF values and generate a 3d object mesh with local uncertainty
for each mesh component. Finally, we propose an incremental fusion method under
a Bayesian framework to fuse the latent codes from multi-view observations. We
evaluate the system in both synthetic and real datasets to demonstrate the
effectiveness of uncertainty-based fusion to improve 3D object reconstruction
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04596">
<title>Distill-SODA: Distilling Self-Supervised Vision Transformer for Source-Free Open-Set Domain Adaptation in Computational Pathology. (arXiv:2307.04596v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04596</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing computational pathology models is essential for reducing manual
tissue typing from whole slide images, transferring knowledge from the source
domain to an unlabeled, shifted target domain, and identifying unseen
categories. We propose a practical setting by addressing the above-mentioned
challenges in one fell swoop, i.e., source-free open-set domain adaptation. Our
methodology focuses on adapting a pre-trained source model to an unlabeled
target dataset and encompasses both closed-set and open-set classes. Beyond
addressing the semantic shift of unknown classes, our framework also deals with
a covariate shift, which manifests as variations in color appearance between
source and target tissue samples. Our method hinges on distilling knowledge
from a self-supervised vision transformer (ViT), drawing guidance from either
robustly pre-trained transformer models or histopathology datasets, including
those from the target domain. In pursuit of this, we introduce a novel
style-based adversarial data augmentation, serving as hard positives for
self-training a ViT, resulting in highly contextualized embeddings. Following
this, we cluster semantically akin target images, with the source model
offering weak pseudo-labels, albeit with uncertain confidence. To enhance this
process, we present the closed-set affinity score (CSAS), aiming to correct the
confidence levels of these pseudo-labels and to calculate weighted class
prototypes within the contextualized embedding space. Our approach establishes
itself as state-of-the-art across three public histopathological datasets for
colorectal cancer assessment. Notably, our self-training method seamlessly
integrates with open-set detection methods, resulting in enhanced performance
in both closed-set and open-set recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vray_G/0/1/0/all/0/1&quot;&gt;Guillaume Vray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomar_D/0/1/0/all/0/1&quot;&gt;Devavrat Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1&quot;&gt;Behzad Bozorgtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Thiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07643">
<title>AECIF-Net: An Attention-Enhanced Co-Interactive Fusion Network for Automated Structural Condition Assessment in Visual Inspection. (arXiv:2307.07643v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07643</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently monitoring the condition of civil infrastructures necessitates
automating the structural condition assessment in visual inspection. This paper
proposes an Attention-Enhanced Co-Interactive Fusion Network (AECIF-Net) for
automatic structural condition assessment in visual bridge inspection.
AECIF-Net can simultaneously parse structural elements and segment surface
defects on the elements in inspection images. It integrates two task-specific
relearning subnets to extract task-specific features from an overall feature
embedding. A co-interactive feature fusion module further captures the spatial
correlation and facilitates information sharing between tasks. Experimental
results demonstrate that the proposed AECIF-Net outperforms the current
state-of-the-art approaches, achieving promising performance with 92.11% mIoU
for element segmentation and 87.16% mIoU for corrosion segmentation on the test
set of the new benchmark dataset Steel Bridge Condition Inspection Visual
(SBCIV). An ablation study verifies the merits of the designs for AECIF-Net,
and a case study demonstrates its capability to automate structural condition
assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaozheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Ruwen Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08779">
<title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation. (arXiv:2307.08779v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08779</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light conditions not only hamper human visual experience but also degrade
the model&apos;s performance on downstream vision tasks. While existing works make
remarkable progress on day-night domain adaptation, they rely heavily on domain
knowledge derived from the task-specific nighttime dataset. This paper
challenges a more complicated scenario with border applicability, i.e.,
zero-shot day-night domain adaptation, which eliminates reliance on any
nighttime data. Unlike prior zero-shot adaptation approaches emphasizing either
image-level translation or model-level adaptation, we propose a similarity
min-max paradigm that considers them under a unified framework. On the image
level, we darken images towards minimum feature similarity to enlarge the
domain gap. Then on the model level, we maximize the feature similarity between
the darkened images and their normal-light counterparts for better model
adaptation. To the best of our knowledge, this work represents the pioneering
effort in jointly optimizing both aspects, resulting in a significant
improvement of model generalizability. Extensive experiments demonstrate our
method&apos;s effectiveness and broad applicability on various nighttime vision
tasks, including classification, semantic segmentation, visual place
recognition, and video action recognition. Code and pre-trained models are
available at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Rundong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10894">
<title>Human Motion Generation: A Survey. (arXiv:2307.10894v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10894</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion generation aims to generate natural human pose sequences and
shows immense potential for real-world applications. Substantial progress has
been made recently in motion data collection technologies and generation
methods, laying the foundation for increasing interest in human motion
generation. Most research within this field focuses on generating human motions
based on conditional signals, such as text, audio, and scene contexts. While
significant advancements have been made in recent years, the task continues to
pose challenges due to the intricate nature of human motion and its implicit
relationship with conditional signals. In this survey, we present a
comprehensive literature review of human motion generation, which, to the best
of our knowledge, is the first of its kind in this field. We begin by
introducing the background of human motion and generative models, followed by
an examination of representative methods for three mainstream sub-tasks:
text-conditioned, audio-conditioned, and scene-conditioned human motion
generation. Additionally, we provide an overview of common datasets and
evaluation metrics. Lastly, we discuss open problems and outline potential
future research directions. We hope that this survey could provide the
community with a comprehensive glimpse of this rapidly evolving field and
inspire novel ideas that address the outstanding challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_D/0/1/0/all/0/1&quot;&gt;Dongwoo Ro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ci_H/0/1/0/all/0/1&quot;&gt;Hai Ci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinlu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06197">
<title>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06197</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex emotion recognition is a cognitive task that has so far eluded the
same excellent performance of other tasks that are at or above the level of
human cognition. Emotion recognition through facial expressions is particularly
difficult due to the complexity of emotions expressed by the human face. For a
machine to approach the same level of performance in complex facial expression
recognition as a human, it may need to synthesise knowledge and understand new
concepts in real-time, as humans do. Humans are able to learn new concepts
using only few examples by distilling important information from memories.
Inspired by human cognition and learning, we propose a novel continual learning
method for complex facial expression recognition that can accurately recognise
new compound expression classes using few training samples, by building on and
retaining its knowledge of basic expression classes. In this work, we also use
GradCAM visualisations to demonstrate the relationship between basic and
compound facial expressions. Our method leverages this relationship through
knowledge distillation and a novel Predictive Sorting Memory Replay, to achieve
the current state-of-the-art in continual learning for complex facial
expression recognition, with 74.28% Overall Accuracy on new classes. We also
demonstrate that using continual learning for complex facial expression
recognition achieves far better performance than non-continual learning
methods, improving on state-of-the-art non-continual learning methods by
13.95%. Our work is also the first to apply few-shot learning to complex facial
expression recognition, achieving the state-of-the-art with 100% accuracy using
only a single training sample per class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiden_A/0/1/0/all/0/1&quot;&gt;Angus Maiden&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakisa_B/0/1/0/all/0/1&quot;&gt;Bahareh Nakisa&lt;/a&gt; (1) ((1) Deakin University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09544">
<title>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09544</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate exemplar-free class incremental learning (CIL)
with knowledge distillation (KD) as a regularization strategy, aiming to
prevent forgetting. KD-based methods are successfully used in CIL, but they
often struggle to regularize the model without access to exemplars of the
training data from previous tasks. Our analysis reveals that this issue
originates from substantial representation shifts in the teacher network when
dealing with out-of-distribution data. This causes large errors in the KD loss
component, leading to performance degradation in CIL models. Inspired by recent
test-time adaptation methods, we introduce Teacher Adaptation (TA), a method
that concurrently updates the teacher and the main models during incremental
training. Our method seamlessly integrates with KD-based CIL approaches and
allows for consistent enhancement of their performance across multiple
exemplar-free CIL benchmarks. The source code for our method is available at
https://github.com/fszatkowski/cl-teacher-adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szatkowski_F/0/1/0/all/0/1&quot;&gt;Filip Szatkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyla_M/0/1/0/all/0/1&quot;&gt;Mateusz Pyla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1&quot;&gt;Marcin Przewi&amp;#x119;&amp;#x17a;likowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1&quot;&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12522">
<title>Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition. (arXiv:2308.12522v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12522</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large-scale pre-trained vision-language models have presented
benefits for alleviating class imbalance in long-tailed recognition. However,
the long-tailed data distribution can corrupt the representation space, where
the distance between head and tail categories is much larger than the distance
between two tail categories. This uneven feature space distribution causes the
model to exhibit unclear and inseparable decision boundaries on the uniformly
distributed test set, which lowers its performance. To address these
challenges, we propose the uniformly category prototype-guided vision-language
framework to effectively mitigate feature space bias caused by data imbalance.
Especially, we generate a set of category prototypes uniformly distributed on a
hypersphere. Category prototype-guided mechanism for image-text matching makes
the features of different classes converge to these distinct and uniformly
distributed category prototypes, which maintain a uniform distribution in the
feature space, and improve class boundaries. Additionally, our proposed
irrelevant text filtering and attribute enhancement module allows the model to
ignore irrelevant noisy text and focus more on key attribute information,
thereby enhancing the robustness of our framework. In the image recognition
fine-tuning stage, to address the positive bias problem of the learnable
classifier, we design the class feature prototype-guided classifier, which
compensates for the performance of tail classes while maintaining the
performance of head classes. Our method outperforms previous vision-language
methods for long-tailed learning work by a large margin and achieves
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Siming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xinpeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuchen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13670">
<title>Linear Oscillation: A Novel Activation Function for Vision Transformer. (arXiv:2308.13670v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13670</link>
<description rdf:parseType="Literal">&lt;p&gt;Activation functions are the linchpins of deep learning, profoundly
influencing both the representational capacity and training dynamics of neural
networks. They shape not only the nature of representations but also optimize
convergence rates and enhance generalization potential. Appreciating this
critical role, we present the Linear Oscillation (LoC) activation function,
defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional
activation functions which primarily introduce non-linearity, LoC seamlessly
blends linear trajectories with oscillatory deviations. The nomenclature
&quot;Linear Oscillation&quot; is a nod to its unique attribute of infusing linear
activations with harmonious oscillations, capturing the essence of the
&quot;Importance of Confusion&quot;. This concept of &quot;controlled confusion&quot; within
network activations is posited to foster more robust learning, particularly in
contexts that necessitate discerning subtle patterns. Our empirical studies
reveal that, when integrated into diverse neural architectures, the LoC
activation function consistently outperforms established counterparts like ReLU
and Sigmoid. The stellar performance exhibited by the avant-garde Vision
Transformer model using LoC further validates its efficacy. This study
illuminates the remarkable benefits of the LoC over other prominent activation
functions. It champions the notion that intermittently introducing deliberate
complexity or &quot;confusion&quot; during training can spur more profound and nuanced
learning. This accentuates the pivotal role of judiciously selected activation
functions in shaping the future of neural network training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16466">
<title>Self-Sampling Meta SAM: Enhancing Few-shot Medical Image Segmentation with Meta-Learning. (arXiv:2308.16466v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16466</link>
<description rdf:parseType="Literal">&lt;p&gt;While the Segment Anything Model (SAM) excels in semantic segmentation for
general-purpose images, its performance significantly deteriorates when applied
to medical images, primarily attributable to insufficient representation of
medical images in its training dataset. Nonetheless, gathering comprehensive
datasets and training models that are universally applicable is particularly
challenging due to the long-tail problem common in medical images. To address
this gap, here we present a Self-Sampling Meta SAM (SSM-SAM) framework for
few-shot medical image segmentation. Our innovation lies in the design of three
key modules: 1) An online fast gradient descent optimizer, further optimized by
a meta-learner, which ensures swift and robust adaptation to new tasks. 2) A
Self-Sampling module designed to provide well-aligned visual prompts for
improved attention allocation; and 3) A robust attention-based decoder
specifically designed for medical few-shot learning to capture relationship
between different slices. Extensive experiments on a popular abdominal CT
dataset and an MRI dataset demonstrate that the proposed method achieves
significant improvements over state-of-the-art methods in few-shot
segmentation, with an average improvements of 10.21% and 1.80% in terms of DSC,
respectively. In conclusion, we present a novel approach for rapid online
adaptation in interactive image segmentation, adapting to a new organ in just
0.83 minutes. Code is publicly available on GitHub upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_T/0/1/0/all/0/1&quot;&gt;Tianang Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00287">
<title>Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution. (arXiv:2309.00287v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00287</link>
<description rdf:parseType="Literal">&lt;p&gt;Using diffusion models to solve inverse problems is a growing field of
research. Current methods assume the degradation to be known and provide
impressive results in terms of restoration quality and diversity. In this work,
we leverage the efficiency of those models to jointly estimate the restored
image and unknown parameters of the degradation model such as blur kernel. In
particular, we designed an algorithm based on the well-known
Expectation-Minimization (EM) estimation method and diffusion models. Our
method alternates between approximating the expected log-likelihood of the
inverse problem using samples drawn from a diffusion model and a maximization
step to estimate unknown model parameters. For the maximization step, we also
introduce a novel blur kernel regularization based on a Plug \&amp;amp; Play denoiser.
Diffusion models are long to run, thus we provide a fast version of our
algorithm. Extensive experiments on blind image deblurring demonstrate the
effectiveness of our method when compared to other state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_C/0/1/0/all/0/1&quot;&gt;Charles Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Almansa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coupete_E/0/1/0/all/0/1&quot;&gt;Eva Coupete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01539">
<title>TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios. (arXiv:2309.01539v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01539</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-to-Contact (TTC) estimation is a critical task for assessing collision
risk and is widely used in various driver assistance and autonomous driving
systems. The past few decades have witnessed development of related theories
and algorithms. The prevalent learning-based methods call for a large-scale TTC
dataset in real-world scenarios. In this work, we present a large-scale object
oriented TTC dataset in the driving scene for promoting the TTC estimation by a
monocular camera. To collect valuable samples and make data with different TTC
values relatively balanced, we go through thousands of hours of driving data
and select over 200K sequences with a preset data distribution. To augment the
quantity of small TTC cases, we also generate clips using the latest Neural
rendering methods. Additionally, we provide several simple yet effective TTC
estimation baselines and evaluate them extensively on the proposed dataset to
demonstrate their effectiveness. The proposed dataset is publicly available at
https://open-dataset.tusen.ai/TSTTC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zehao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Naiyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaojie Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01950">
<title>RADIO: Reference-Agnostic Dubbing Video Synthesis. (arXiv:2309.01950v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01950</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most challenging problems in audio-driven talking head generation
is achieving high-fidelity detail while ensuring precise synchronization. Given
only a single reference image, extracting meaningful identity attributes
becomes even more challenging, often causing the network to mirror the facial
and lip structures too closely. To address these issues, we introduce RADIO, a
framework engineered to yield high-quality dubbed videos regardless of the pose
or expression in reference images. The key is to modulate the decoder layers
using latent space composed of audio and reference features. Additionally, we
incorporate ViT blocks into the decoder to emphasize high-fidelity details,
especially in the lip region. Our experimental results demonstrate that RADIO
displays high synchronization without the loss of fidelity. Especially in harsh
scenarios where the reference frame deviates significantly from the ground
truth, our method outperforms state-of-the-art methods, highlighting its
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongyeun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chaewon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sangjoon Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Gyeong-Moon Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03329">
<title>MEGANet: Multi-Scale Edge-Guided Attention Network for Weak Boundary Polyp Segmentation. (arXiv:2309.03329v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03329</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient polyp segmentation in healthcare plays a critical role in enabling
early diagnosis of colorectal cancer. However, the segmentation of polyps
presents numerous challenges, including the intricate distribution of
backgrounds, variations in polyp sizes and shapes, and indistinct boundaries.
Defining the boundary between the foreground (i.e. polyp itself) and the
background (surrounding tissue) is difficult. To mitigate these challenges, we
propose Multi-Scale Edge-Guided Attention Network (MEGANet) tailored
specifically for polyp segmentation within colonoscopy images. This network
draws inspiration from the fusion of a classical edge detection technique with
an attention mechanism. By combining these techniques, MEGANet effectively
preserves high-frequency information, notably edges and boundaries, which tend
to erode as neural networks deepen. MEGANet is designed as an end-to-end
framework, encompassing three key modules: an encoder, which is responsible for
capturing and abstracting the features from the input image, a decoder, which
focuses on salient features, and the Edge-Guided Attention module (EGA) that
employs the Laplacian Operator to accentuate polyp boundaries. Extensive
experiments, both qualitative and quantitative, on five benchmark datasets,
demonstrate that our MEGANet outperforms other existing SOTA methods under six
evaluation metrics. Our code is available at
https://github.com/UARK-AICV/MEGANet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_N/0/1/0/all/0/1&quot;&gt;Nhat-Tan Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1&quot;&gt;Dinh-Hieu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang-Thuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03734">
<title>ClusterFusion: Leveraging Radar Spatial Features for Radar-Camera 3D Object Detection in Autonomous Vehicles. (arXiv:2309.03734v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03734</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the complementary nature of millimeter wave radar and camera, deep
learning-based radar-camera 3D object detection methods may reliably produce
accurate detections even in low-visibility conditions. This makes them
preferable to use in autonomous vehicles&apos; perception systems, especially as the
combined cost of both sensors is cheaper than the cost of a lidar. Recent
radar-camera methods commonly perform feature-level fusion which often involves
projecting the radar points onto the same plane as the image features and
fusing the extracted features from both modalities. While performing fusion on
the image plane is generally simpler and faster, projecting radar points onto
the image plane flattens the depth dimension of the point cloud which might
lead to information loss and makes extracting the spatial features of the point
cloud harder. We proposed ClusterFusion, an architecture that leverages the
local spatial features of the radar point cloud by clustering the point cloud
and performing feature extraction directly on the point cloud clusters before
projecting the features onto the image plane. ClusterFusion achieved the
state-of-the-art performance among all radar-monocular camera methods on the
test slice of the nuScenes dataset with 48.7% nuScenes detection score (NDS).
We also investigated the performance of different radar feature extraction
strategies on point cloud clusters: a handcrafted strategy, a learning-based
strategy, and a combination of both, and found that the handcrafted strategy
yielded the best performance. The main goal of this work is to explore the use
of radar&apos;s local spatial and point-wise features by extracting them directly
from radar point cloud clusters for a radar-monocular camera 3D object
detection method that performs cross-modal feature fusion on the image plane.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurniawan_I/0/1/0/all/0/1&quot;&gt;Irfan Tito Kurniawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trilaksono_B/0/1/0/all/0/1&quot;&gt;Bambang Riyanto Trilaksono&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06067">
<title>Implicit Neural Representation for MRI Parallel Imaging Reconstruction. (arXiv:2309.06067v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06067</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) always suffers from long acquisition times.
Parallel imaging (PI) is one solution to reduce scan time by periodically
skipping certain K-space lines and then reconstructing high-quality images from
undersampled measurements. Recently, implicit neural representation (INR) has
emerged as a new deep learning method that represents an object as a continuous
function of spatial coordinates, and this function is normally parameterized by
a multilayer perceptron (MLP). In this paper, we propose a novel MRI PI
reconstruction method based on INR, which represents the reconstructed
fully-sampled images as the function of voxel coordinates and prior feature
vectors of undersampled images to overcome the generalization problem of INR.
Specifically, we introduce a scale-embedded encoder to produce
scale-independent voxel-specific features from MR images with different
undersampling scales and then concatenate with coordinate vectors to recover
fully-sampled MR images, thus achieving multiple scale reconstructions. The
performance of the proposed method was assessed by experimenting with publicly
available MRI datasets and was compared with other reconstruction methods. Our
quantitative evaluation demonstrates the superiority of the proposed method
over alternative reconstruction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yusheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zhihan Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07760">
<title>PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07760</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1&quot;&gt;Anh Pham Thi Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;An Duc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzimiropoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07906">
<title>Generative Image Dynamics. (arXiv:2309.07906v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07906</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approach to modeling an image-space prior on scene motion. Our
prior is learned from a collection of motion trajectories extracted from real
video sequences depicting natural, oscillatory dynamics such as trees, flowers,
candles, and clothes swaying in the wind. We model this dense, long-term motion
prior in the Fourier domain:given a single image, our trained model uses a
frequency-coordinated diffusion sampling process to predict a spectral volume,
which can be converted into a motion texture that spans an entire video. Along
with an image-based rendering module, these trajectories can be used for a
number of downstream applications, such as turning still images into seamlessly
looping videos, or allowing users to realistically interact with objects in
real pictures by interpreting the spectral volumes as image-space modal bases,
which approximate object dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1&quot;&gt;Richard Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1&quot;&gt;Noah Snavely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holynski_A/0/1/0/all/0/1&quot;&gt;Aleksander Holynski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09319">
<title>Active Learning for Semantic Segmentation with Multi-class Label Query. (arXiv:2309.09319v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09319</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new active learning method for semantic segmentation.
The core of our method lies in a new annotation query design. It samples
informative local image regions (e.g., superpixels), and for each of such
regions, asks an oracle for a multi-hot vector indicating all classes existing
in the region. This multi-class labeling strategy is substantially more
efficient than existing ones like segmentation, polygon, and even dominant
class labeling in terms of annotation time per click. However, it introduces
the class ambiguity issue in training as it assigns partial labels (i.e., a set
of candidate classes) to individual pixels. We thus propose a new algorithm for
learning semantic segmentation while disambiguating the partial labels in two
stages. In the first stage, it trains a segmentation model directly with the
partial labels through two new loss functions motivated by partial label
learning and multiple instance learning. In the second stage, it disambiguates
the partial labels by generating pixel-wise pseudo labels, which are used for
supervised learning of the model. Equipped with a new acquisition function
dedicated to the multi-class labeling, our method outperforms previous work on
Cityscapes and PASCAL VOC 2012 while spending less annotation cost. Our code
and results are available at https://github.com/sehyun03/MulActSeg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sehyun Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sohyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hoyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1&quot;&gt;Minhyeon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1&quot;&gt;Jungseul Ok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09812">
<title>R2GenGPT: Radiology Report Generation with Frozen LLMs. (arXiv:2309.09812v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09812</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have consistently showcased remarkable
generalization capabilities when applied to various language tasks.
Nonetheless, harnessing the full potential of LLMs for Radiology Report
Generation (R2Gen) still presents a challenge, stemming from the inherent
disparity in modality between LLMs and the R2Gen task. To bridge this gap
effectively, we propose R2GenGPT, which is a novel solution that aligns visual
features with the word embedding space of LLMs using an efficient visual
alignment module. This innovative approach empowers the previously static LLM
to seamlessly integrate and process image information, marking a step forward
in optimizing R2Gen performance. R2GenGPT offers the following benefits. First,
it attains state-of-the-art (SOTA) performance by training only the lightweight
visual alignment module while freezing all the parameters of LLM. Second, it
exhibits high training efficiency, as it requires the training of an
exceptionally minimal number of parameters while achieving rapid convergence.
By employing delta tuning, our model only trains 5M parameters (which
constitute just 0.07\% of the total parameter count) to achieve performance
close to the SOTA levels. Our code is available at
https://github.com/wang-zhanyu/R2GenGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13190">
<title>Spatial-frequency channels, shape bias, and adversarial robustness. (arXiv:2309.13190v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13190</link>
<description rdf:parseType="Literal">&lt;p&gt;What spatial frequency information do humans and neural networks use to
recognize objects? In neuroscience, critical band masking is an established
tool that can reveal the frequency-selective filters used for object
recognition. Critical band masking measures the sensitivity of recognition
performance to noise added at each spatial frequency. Existing critical band
masking studies show that humans recognize periodic patterns (gratings) and
letters by means of a spatial-frequency filter (or &quot;channel&quot;) that has a
frequency bandwidth of one octave (doubling of frequency). Here, we introduce
critical band masking as a task for network-human comparison and test 14 humans
and 76 neural networks on 16-way ImageNet categorization in the presence of
narrowband noise. We find that humans recognize objects in natural images using
the same one-octave-wide channel that they use for letters and gratings, making
it a canonical feature of human object recognition. Unlike humans, the neural
network channel is very broad, 2-4 times wider than the human channel. Thus,
noise at certain high and low frequencies will impair network performance and
spare human performance. Adversarial and augmented-image training are commonly
used to increase network robustness and shape bias. Does this training align
network and human object recognition channels? Three network channel properties
(bandwidth, center frequency, peak noise sensitivity) correlate strongly with
shape bias (51% variance explained) and robustness of adversarially-trained
networks (66% variance explained). Adversarial training increases robustness
but expands the channel bandwidth even further beyond the human bandwidth.
Thus, critical band masking reveals that the network channel is more than twice
as wide as the human channel, and that adversarial training only makes it
worse. Networks with narrower channels might be more robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1&quot;&gt;Ajay Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1&quot;&gt;Elena Sizikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majaj_N/0/1/0/all/0/1&quot;&gt;Najib J. Majaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelli_D/0/1/0/all/0/1&quot;&gt;Denis G. Pelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14500">
<title>Assessment of a new GeoAI foundation model for flood inundation mapping. (arXiv:2309.14500v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14500</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision foundation models are a new frontier in Geospatial Artificial
Intelligence (GeoAI), an interdisciplinary research area that applies and
extends AI for geospatial problem solving and geographic knowledge discovery,
because of their potential to enable powerful image analysis by learning and
extracting important image features from vast amounts of geospatial data. This
paper evaluates the performance of the first-of-its-kind geospatial foundation
model, IBM-NASA&apos;s Prithvi, to support a crucial geospatial analysis task: flood
inundation mapping. This model is compared with convolutional neural network
and vision transformer-based architectures in terms of mapping accuracy for
flooded areas. A benchmark dataset, Sen1Floods11, is used in the experiments,
and the models&apos; predictability, generalizability, and transferability are
evaluated based on both a test dataset and a dataset that is completely unseen
by the model. Results show the good transferability of the Prithvi model,
highlighting its performance advantages in segmenting flooded areas in
previously unseen regions. The findings also indicate areas for improvement for
the Prithvi model in terms of adopting multi-scale representation learning,
developing more end-to-end pipelines for high-level image analysis tasks, and
offering more flexibility in terms of input data bands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sizhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chia-Yu Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arundel_S/0/1/0/all/0/1&quot;&gt;Samantha T. Arundel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15533">
<title>Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15533</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty quantification is crucial for the deployment of image restoration
models in safety-critical domains, like autonomous driving and biological
imaging. To date, methods for uncertainty visualization have mainly focused on
per-pixel estimates. Yet, a heatmap of per-pixel variances is typically of
little practical use, as it does not capture the strong correlations between
pixels. A more natural measure of uncertainty corresponds to the variances
along the principal components (PCs) of the posterior distribution.
Theoretically, the PCs can be computed by applying PCA on samples generated
from a conditional generative model for the input image. However, this requires
generating a very large number of samples at test time, which is painfully slow
with the current state-of-the-art (diffusion) models. In this work, we present
a method for predicting the PCs of the posterior distribution for any input
image, in a single forward pass of a neural network. Our method can either wrap
around a pre-trained model that was trained to minimize the mean square error
(MSE), or can be trained from scratch to output both a predicted image and the
posterior PCs. We showcase our method on multiple inverse problems in imaging,
including denoising, inpainting, super-resolution, and biological
image-to-image translation. Our method reliably conveys instance-adaptive
uncertainty directions, achieving uncertainty quantification comparable with
posterior samplers while being orders of magnitude faster. Code and examples
are available at https://eliasnehme.github.io/NPPC/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nehme_E/0/1/0/all/0/1&quot;&gt;Elias Nehme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yair_O/0/1/0/all/0/1&quot;&gt;Omer Yair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaeli_T/0/1/0/all/0/1&quot;&gt;Tomer Michaeli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15702">
<title>SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction. (arXiv:2309.15702v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15702</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of 3D scene understanding, 3D scene graphs have emerged as a new
scene representation that combines geometric and semantic information about
objects and their relationships. However, learning semantic 3D scene graphs in
a fully supervised manner is inherently difficult as it requires not only
object-level annotations but also relationship labels. While pre-training
approaches have helped to boost the performance of many methods in various
fields, pre-training for 3D scene graph prediction has received little
attention. Furthermore, we find in this paper that classical contrastive point
cloud-based pre-training approaches are ineffective for 3D scene graph
learning. To this end, we present SGRec3D, a novel self-supervised pre-training
method for 3D scene graph prediction. We propose to reconstruct the 3D input
scene from a graph bottleneck as a pretext task. Pre-training SGRec3D does not
require object relationship labels, making it possible to exploit large-scale
3D scene understanding datasets, which were off-limits for 3D scene graph
learning before. Our experiments demonstrate that in contrast to recent point
cloud-based pre-training approaches, our proposed pre-training improves the 3D
scene graph prediction considerably, which results in SOTA performance,
outperforming other 3D scene graph models by +10% on object prediction and +4%
on relationship prediction. Additionally, we show that only using a small
subset of 10% labeled data during fine-tuning is sufficient to outperform the
same model without pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1&quot;&gt;Sebastian Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1&quot;&gt;Pedro Hermosilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaskevicius_N/0/1/0/all/0/1&quot;&gt;Narunas Vaskevicius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colosi_M/0/1/0/all/0/1&quot;&gt;Mirco Colosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1&quot;&gt;Timo Ropinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15751">
<title>InfraParis: A multi-modal and multi-task autonomous driving dataset. (arXiv:2309.15751v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15751</link>
<description rdf:parseType="Literal">&lt;p&gt;Current deep neural networks (DNNs) for autonomous driving computer vision
are typically trained on specific datasets that only involve a single type of
data and urban scenes. Consequently, these models struggle to handle new
objects, noise, nighttime conditions, and diverse scenarios, which is essential
for safety-critical applications. Despite ongoing efforts to enhance the
resilience of computer vision DNNs, progress has been sluggish, partly due to
the absence of benchmarks featuring multiple modalities. We introduce a novel
and versatile dataset named InfraParis that supports multiple tasks across
three modalities: RGB, depth, and infrared. We assess various state-of-the-art
baseline techniques, encompassing models for the tasks of semantic
segmentation, object detection, and depth estimation. More visualizations and
the download link for InfraParis are available at
\href{https://ensta-u2is.github.io/infraParis/}{https://ensta-u2is.github.io/infraParis/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1&quot;&gt;Gianni Franchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariat_M/0/1/0/all/0/1&quot;&gt;Marwane Hariat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xuanlong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1&quot;&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzanera_A/0/1/0/all/0/1&quot;&gt;Antoine Manzanera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1&quot;&gt;David Filliat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17076">
<title>Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications. (arXiv:2309.17076v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17076</link>
<description rdf:parseType="Literal">&lt;p&gt;3D mesh segmentation is an important task with many biomedical applications.
The human body has bilateral symmetry and some variations in organ positions.
It allows us to expect a positive effect of rotation and inversion invariant
layers in convolutional neural networks that perform biomedical segmentations.
In this study, we show the impact of weight symmetry in neural networks that
perform 3D mesh segmentation. We analyze the problem of 3D mesh segmentation
for pathological vessel structures (aneurysms) and conventional anatomical
structures (endocardium and epicardium of ventricles). Local geometrical
features are encoded as sampling from the signed distance function, and the
neural network performs prediction for each mesh node. We show that weight
symmetry gains from 1 to 3% of additional accuracy and allows decreasing the
number of trainable parameters up to 8 times without suffering the performance
loss if neural networks have at least three convolutional layers. This also
works for very small training sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dordiuk_V/0/1/0/all/0/1&quot;&gt;Vladislav Dordiuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dzhigil_M/0/1/0/all/0/1&quot;&gt;Maksim Dzhigil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ushenin_K/0/1/0/all/0/1&quot;&gt;Konstantin Ushenin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00723">
<title>HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count. (arXiv:2310.00723v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00723</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the HOH (Human-Object-Human) Handover Dataset, a large object
count dataset with 136 objects, to accelerate data-driven research on handover
studies, human-robot handover implementation, and artificial intelligence (AI)
on handover parameter estimation from 2D and 3D data of person interactions.
HOH contains multi-view RGB and depth data, skeletons, fused point clouds,
grasp type and handedness labels, object, giver hand, and receiver hand 2D and
3D segmentations, giver and receiver comfort ratings, and paired object
metadata and aligned 3D models for 2,720 handover interactions spanning 136
objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40
participants. We also show experimental results of neural networks trained
using HOH to perform grasp, orientation, and trajectory prediction. As the only
fully markerless handover capture dataset, HOH represents natural human-human
handover interactions, overcoming challenges with markered datasets that
require specific suiting for body tracking, and lack high-resolution hand
tracking. To date, HOH is the largest handover dataset in number of objects,
participants, pairs with role reversal accounted for, and total interactions
captured.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiederhold_N/0/1/0/all/0/1&quot;&gt;Noah Wiederhold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megyeri_A/0/1/0/all/0/1&quot;&gt;Ava Megyeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paris_D/0/1/0/all/0/1&quot;&gt;DiMaggio Paris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Sean Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_N/0/1/0/all/0/1&quot;&gt;Natasha Kholgade Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01210">
<title>Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01210</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully automatic cardiac segmentation can be a fast and reproducible method to
extract clinical measurements from an echocardiography examination. The U-Net
architecture is the current state-of-the-art deep learning architecture for
medical segmentation and can segment cardiac structures in real-time with
average errors comparable to inter-observer variability. However, this
architecture still generates large outliers that are often anatomically
incorrect. This work uses the concept of graph convolutional neural networks
that predict the contour points of the structures of interest instead of
labeling each pixel. We propose a graph architecture that uses two
convolutional rings based on cardiac anatomy and show that this eliminates
anatomical incorrect multi-structure segmentations on the publicly available
CAMUS dataset. Additionally, this work contributes with an ablation study on
the graph convolutional architecture and an evaluation of clinical measurements
on the clinical HUNT4 dataset. Finally, we propose to use the inter-model
agreement of the U-Net and the graph network as a predictor of both the input
and segmentation quality. We show this predictor can detect out-of-distribution
and unsuitable input images in real-time. Source code is available online:
https://github.com/gillesvntnu/GCN_multistructure
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vyver_G/0/1/0/all/0/1&quot;&gt;Gilles Van De Vyver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thomas_S/0/1/0/all/0/1&quot;&gt;Sarina Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ben_Yosef_G/0/1/0/all/0/1&quot;&gt;Guy Ben-Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Olaisen_S/0/1/0/all/0/1&quot;&gt;Sindre Hellum Olaisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dalen_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe5;vard Dalen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lovstakken_L/0/1/0/all/0/1&quot;&gt;Lasse L&amp;#xf8;vstakken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Smistad_E/0/1/0/all/0/1&quot;&gt;Erik Smistad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01852">
<title>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N&amp;gt;=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval task. Beyond this, our LanguageBind has greatly improved
in the zero-shot video, audio, depth, and infrared understanding tasks. For
instance, LanguageBind surpassing InterVideo by 1.9% on MSR-VTT, 8.8% on MSVD,
6.3% on DiDeMo, and 4.4% on ActivityNet. On the LLVIP and NYU-D datasets,
LanguageBind outperforms ImageBind with 23.8% and 11.1% top-1 accuracy. Code
address: https://github.com/PKU-YuanGroup/LanguageBind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxi Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;HongFa Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yatian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wancai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03525">
<title>V2X Cooperative Perception for Autonomous Driving: Recent Advances and Challenges. (arXiv:2310.03525v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03525</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate perception is essential for advancing autonomous driving and
addressing safety challenges in modern transportation systems. Despite
significant advancements in computer vision for object recognition, current
perception methods still face difficulties in complex real-world traffic
environments. Challenges such as physical occlusion and limited sensor field of
view persist for individual vehicle systems. Cooperative Perception (CP) with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
these obstacles and enhance driving automation systems. While some research has
explored CP&apos;s fundamental architecture and critical components, there remains a
lack of comprehensive summaries of the latest innovations, particularly in the
context of V2X communication technologies. To address this gap, this paper
provides a comprehensive overview of the evolution of CP technologies, spanning
from early explorations to recent developments, including advancements in V2X
communication technologies. Additionally, a contemporary generic framework is
also proposed to illustrate the V2X-based CP workflow, aiding in the structured
understanding of CP system components. Furthermore, this paper categorizes
prevailing V2X-based CP methodologies based on the critical issues they
address. An extensive literature review is conducted within this taxonomy,
evaluating existing datasets and simulators. Finally, open challenges and
future directions in CP for autonomous driving are discussed by considering
both perception and V2X communication advancements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dinh C. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1&quot;&gt;Mostafa Rahimi Azghadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sumei Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05654">
<title>No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling. (arXiv:2310.05654v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05654</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have demonstrated outstanding performance in
computer vision tasks, yet their high computational complexity prevents their
deployment in computing resource-constrained environments. Various token
pruning techniques have been introduced to alleviate the high computational
burden of ViTs by dynamically dropping image tokens. However, some undesirable
pruning at early stages may result in permanent loss of image information in
subsequent layers, consequently hindering model performance. To address this
problem, we propose IdleViT, a dynamic token-idle-based method that achieves an
excellent trade-off between performance and efficiency. Specifically, in each
layer, IdleViT selects a subset of the image tokens to participate in
computations while keeping the rest of the tokens idle and directly passing
them to this layer&apos;s output. By allowing the idle tokens to be re-selected in
the following layers, IdleViT mitigates the negative impact of improper pruning
in the early stages. Furthermore, inspired by the normalized graph cut, we
devise a token cut loss on the attention map as regularization to improve
IdleViT&apos;s token selection ability. Our method is simple yet effective and can
be extended to pyramid ViTs since no token is completely dropped. Extensive
experimental results on various ViT architectures have shown that IdleViT can
diminish the complexity of pretrained ViTs by up to 33\% with no more than
0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.
Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art
EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The
source code is available in the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07749">
<title>OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation. (arXiv:2310.07749v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07749</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates a challenging task named open-domain interleaved
image-text generation, which generates interleaved texts and images following
an input query. We propose a new interleaved generation framework based on
prompting large-language models (LLMs) and pre-trained text-to-image (T2I)
models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions,
coordinates T2I models, creates visual prompts for generating images, and
incorporates global contexts into the T2I models. This global context improves
the entity and style consistencies of images in the interleaved generation. For
model assessment, we first propose to use large multi-modal models (LMMs) to
evaluate the entity and style consistencies of open-domain interleaved
image-text sequences. According to the LMM evaluation on our constructed
evaluation set, the proposed interleaved generation framework can generate
high-quality image-text content for various domains and applications, such as
how-to question answering, storytelling, graphical story rewriting, and
webpage/poster generation tasks. Moreover, we validate the effectiveness of the
proposed LMM evaluation technique with human assessment. We hope our proposed
framework, benchmark, and LMM evaluation could help establish the intriguing
interleaved image-text generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1&quot;&gt;Jie An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08442">
<title>Debias the Training of Diffusion Models. (arXiv:2310.08442v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08442</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated compelling generation quality by
optimizing the variational lower bound through a simple denoising score
matching loss. In this paper, we provide theoretical evidence that the
prevailing practice of using a constant loss weight strategy in diffusion
models leads to biased estimation during the training phase. Simply optimizing
the denoising network to predict Gaussian noise with constant weighting may
hinder precise estimations of original images. To address the issue, we propose
an elegant and effective weighting strategy grounded in the theoretically
unbiased principle. Moreover, we conduct a comprehensive and systematic
exploration to dissect the inherent bias problem deriving from constant
weighting loss from the perspectives of its existence, impact and reasons.
These analyses are expected to advance our understanding and demystify the
inner workings of diffusion models. Through empirical evaluation, we
demonstrate that our proposed debiased estimation method significantly enhances
sample quality without the reliance on complex techniques, and exhibits
improved efficiency compared to the baseline method both in training and
sampling processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Man Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08669">
<title>Multimodal Large Language Model for Visual Navigation. (arXiv:2310.08669v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08669</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent efforts to enable visual navigation using large language models have
mainly focused on developing complex prompt systems. These systems incorporate
instructions, observations, and history into massive text prompts, which are
then combined with pre-trained large language models to facilitate visual
navigation. In contrast, our approach aims to fine-tune large language models
for visual navigation without extensive prompt engineering. Our design involves
a simple text prompt, current observations, and a history collector model that
gathers information from previous observations as input. For output, our design
provides a probability distribution of possible actions that the agent can take
during navigation. We train our model using human demonstrations and collision
signals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results
demonstrate that our method outperforms state-of-the-art behavior cloning
methods and effectively reduces collision rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhar_V/0/1/0/all/0/1&quot;&gt;Vansh Dhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10073">
<title>Expression Domain Translation Network for Cross-domain Head Reenactment. (arXiv:2310.10073v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10073</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable advancements in head reenactment, the existing methods
face challenges in cross-domain head reenactment, which aims to transfer human
motions to domains outside the human, including cartoon characters. It is still
difficult to extract motion from out-of-domain images due to the distinct
appearances, such as large eyes. Recently, previous work introduced a
large-scale anime dataset called AnimeCeleb and a cross-domain head reenactment
model, including an optimization-based mapping function to translate the human
domain&apos;s expressions to the anime domain. However, we found that the mapping
function, which relies on a subset of expressions, imposes limitations on the
mapping of various expressions. To solve this challenge, we introduce a novel
expression domain translation network that transforms human expressions into
anime expressions. Specifically, to maintain the geometric consistency of
expressions between the input and output of the expression domain translation
network, we employ a 3D geometric-aware loss function that reduces the
distances between the vertices in the 3D mesh of the human and anime. By doing
so, it forces high-fidelity and one-to-one mapping with respect to two
cross-expression domains. Our method outperforms existing methods in both
qualitative and quantitative analysis, marking a significant advancement in the
field of cross-domain head reenactment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1&quot;&gt;Taewoong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jeongsik Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeseong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sunghyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11441">
<title>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. (arXiv:2310.11441v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11441</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Set-of-Mark (SoM), a new visual prompting method, to unleash the
visual grounding abilities of large multimodal models (LMMs), such as GPT-4V.
As illustrated in Fig. 1 (right), we employ off-the-shelf interactive
segmentation models, such as SEEM/SAM, to partition an image into regions at
different levels of granularity, and overlay these regions with a set of marks
e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can
answer the questions that require visual grounding. We perform a comprehensive
empirical study to validate the effectiveness of SoM on a wide range of
fine-grained vision and multimodal tasks. For example, our experiments show
that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art
fully-finetuned referring expression comprehension and segmentation model on
RefCOCOg. Code for SoM prompting is made public at:
https://github.com/microsoft/SoM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11864">
<title>VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11864</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hongliang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jing Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14926">
<title>Reference-based Restoration of Digitized Analog Videotapes. (arXiv:2310.14926v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14926</link>
<description rdf:parseType="Literal">&lt;p&gt;Analog magnetic tapes have been the main video data storage device for
several decades. Videos stored on analog videotapes exhibit unique degradation
patterns caused by tape aging and reader device malfunctioning that are
different from those observed in film and digital video restoration tasks. In
this work, we present a reference-based approach for the resToration of
digitized Analog videotaPEs (TAPE). We leverage CLIP for zero-shot artifact
detection to identify the cleanest frames of each video through textual prompts
describing different artifacts. Then, we select the clean frames most similar
to the input ones and employ them as references. We design a transformer-based
Swin-UNet network that exploits both neighboring and reference frames via our
Multi-Reference Spatial Feature Fusion (MRSFF) blocks. MRSFF blocks rely on
cross-attention and attention pooling to take advantage of the most useful
parts of each reference frame. To address the absence of ground truth in
real-world videos, we create a synthetic dataset of videos exhibiting artifacts
that closely resemble those commonly found in analog videotapes. Both
quantitative and qualitative experiments show the effectiveness of our approach
compared to other state-of-the-art methods. The code, the model, and the
synthetic dataset are publicly available at https://github.com/miccunifi/TAPE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnolucci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Agnolucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galteri_L/0/1/0/all/0/1&quot;&gt;Leonardo Galteri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14942">
<title>Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand. (arXiv:2310.14942v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14942</link>
<description rdf:parseType="Literal">&lt;p&gt;The prosperity of deep neural networks (DNNs) is largely benefited from
open-source datasets, based on which users can evaluate and improve their
methods. In this paper, we revisit backdoor-based dataset ownership
verification (DOV), which is currently the only feasible approach to protect
the copyright of open-source datasets. We reveal that these methods are
fundamentally harmful given that they could introduce malicious
misclassification behaviors to watermarked DNNs by the adversaries. In this
paper, we design DOV from another perspective by making watermarked models
(trained on the protected dataset) correctly classify some `hard&apos; samples that
will be misclassified by the benign model. Our method is inspired by the
generalization property of DNNs, where we find a \emph{hardly-generalized
domain} for the original dataset (as its \emph{domain watermark}). It can be
easily learned with the protected dataset containing modified samples.
Specifically, we formulate the domain generation as a bi-level optimization and
propose to optimize a set of visually-indistinguishable clean-label modified
data with similar effects to domain-watermarked samples from the
hardly-generalized domain to ensure watermark stealthiness. We also design a
hypothesis-test-guided ownership verification via our domain watermark and
provide the theoretical analyses of our method. Extensive experiments on three
benchmark datasets are conducted, which verify the effectiveness of our method
and its resistance to potential adaptive methods. The code for reproducing main
experiments is available at
\url{https://github.com/JunfengGo/Domain-Watermark}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Junfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15747">
<title>Large Language Models are Temporal and Causal Reasoners for Video Question Answering. (arXiv:2310.15747v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15747</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown remarkable performances on a wide
range of natural language understanding and generation tasks. We observe that
the LLMs provide effective priors in exploiting $\textit{linguistic shortcuts}$
for temporal and causal reasoning in Video Question Answering (VideoQA).
However, such priors often cause suboptimal results on VideoQA by leading the
model to over-rely on questions, $\textit{i.e.}$, $\textit{linguistic bias}$,
while ignoring visual content. This is also known as `ungrounded guesses&apos; or
`hallucinations&apos;. To address this problem while leveraging LLMs&apos; prior on
VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to
predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping
the source pair and the target label to understand their complex relationships,
$\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs,
respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to
LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five
challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general
framework that is applicable to various LLMs (OPT and GPT-J) and consistently
improves their performances. We empirically demonstrate that Flipped-VQA not
only enhances the exploitation of linguistic shortcuts but also mitigates the
linguistic bias, which causes incorrect answers over-relying on the question.
Code is available at https://github.com/mlvlab/Flipped-VQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1&quot;&gt;Dohwan Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Ji Soo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wooyoung Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roh_B/0/1/0/all/0/1&quot;&gt;Byungseok Roh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17316">
<title>Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics. (arXiv:2310.17316v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17316</link>
<description rdf:parseType="Literal">&lt;p&gt;Defect inspection is paramount within the closed-loop manufacturing system.
However, existing datasets for defect inspection often lack precision and
semantic granularity required for practical applications. In this paper, we
introduce the Defect Spectrum, a comprehensive benchmark that offers precise,
semantic-abundant, and large-scale annotations for a wide range of industrial
defects. Building on four key industrial benchmarks, our dataset refines
existing annotations and introduces rich semantic details, distinguishing
multiple defect types within a single image. Furthermore, we introduce
Defect-Gen, a two-stage diffusion-based generator designed to create
high-quality and diverse defective images, even when working with limited
datasets. The synthetic images generated by Defect-Gen significantly enhance
the efficacy of defect inspection models. Overall, The Defect Spectrum dataset
demonstrates its potential in defect inspection research, offering a solid
platform for testing and refining advanced models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingcong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18651">
<title>Local-Global Self-Supervised Visual Representation Learning. (arXiv:2310.18651v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18651</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised representation learning methods mainly focus on image-level
instance discrimination. This study explores the potential benefits of
incorporating patch-level discrimination into existing methods to enhance the
quality of learned representations by simultaneously looking at local and
global visual features. Towards this idea, we present a straightforward yet
effective patch-matching algorithm that can find the corresponding patches
across the augmented views of an image. The augmented views are subsequently
fed into a self-supervised learning framework employing Vision Transformer
(ViT) as its backbone. The result is the generation of both image-level and
patch-level representations. Leveraging the proposed patch-matching algorithm,
the model minimizes the representation distance between not only the CLS tokens
but also the corresponding patches. As a result, the model gains a more
comprehensive understanding of both the entirety of the image as well as its
finer details. We pretrain the proposed method on small, medium, and
large-scale datasets. It is shown that our approach could outperform
state-of-the-art image-level representation learning methods on both image
classification and downstream tasks. Keywords: Self-Supervised Learning; Visual
Representations; Local-Global Representation Learning; Patch-Wise
Representation Learning; Vision Transformer (ViT)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javidani_A/0/1/0/all/0/1&quot;&gt;Ali Javidani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1&quot;&gt;Mohammad Amin Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araabi_B/0/1/0/all/0/1&quot;&gt;Babak Nadjar Araabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18917">
<title>TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields. (arXiv:2310.18917v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18917</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous attempts to integrate Neural Radiance Fields (NeRF) into
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or treat dynamic objects as outliers. However, most
of real-world scenarios is dynamic. In this paper, we propose a time-varying
representation to track and reconstruct the dynamic scenes. Our system
simultaneously maintains two processes, tracking process and mapping process.
For tracking process, the entire input images are uniformly sampled and
training of the RGB images are self-supervised. For mapping process, we
leverage know masks to differentiate dynamic objects and static backgrounds,
and we apply distinct sampling strategies for two types of areas. The
parameters optimization for both processes are made up by two stages, the first
stage associates time with 3D positions to convert the deformation field to the
canonical field. And the second associates time with 3D positions in canonical
field to obtain colors and Signed Distance Function (SDF). Besides, We propose
a novel keyframe selection strategy based on the overlapping rate. We evaluate
our approach on two publicly available synthetic datasets and validate that our
method is more effective compared to current state-of-the-art dynamic mapping
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1&quot;&gt;Chengyao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiliu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18986">
<title>Controllable Group Choreography using Contrastive Diffusion. (arXiv:2310.18986v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18986</link>
<description rdf:parseType="Literal">&lt;p&gt;Music-driven group choreography poses a considerable challenge but holds
significant potential for a wide range of industrial applications. The ability
to generate synchronized and visually appealing group dance motions that are
aligned with music opens up opportunities in many fields such as entertainment,
advertising, and virtual performances. However, most of the recent works are
not able to generate high-fidelity long-term motions, or fail to enable
controllable experience. In this work, we aim to address the demand for
high-quality and customizable group dance generation by effectively governing
the consistency and diversity of group choreographies. In particular, we
utilize a diffusion-based generative approach to enable the synthesis of
flexible number of dancers and long-term group dances, while ensuring coherence
to the input music. Ultimately, we introduce a Group Contrastive Diffusion
(GCD) strategy to enhance the connection between dancers and their group,
presenting the ability to control the consistency or diversity level of the
synthesized group animation via the classifier-guidance sampling technique.
Through intensive experiments and evaluation, we demonstrate the effectiveness
of our approach in producing visually captivating and consistent group dance
motions. The experimental results show the capability of our method to achieve
the desired levels of consistency and diversity, while maintaining the overall
quality of the generated group choreography. The source code can be found at
https://aioz-ai.github.io/GCD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nhat Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Tuong Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1&quot;&gt;Khoa Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1&quot;&gt;Erman Tjiputra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1&quot;&gt;Quang D. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19080">
<title>Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery. (arXiv:2310.19080v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19080</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning have shown that Reinforcement Learning
from Human Feedback (RLHF) can improve machine learning models and align them
with human preferences. Although very successful for Large Language Models
(LLMs), these advancements have not had a comparable impact in research for
autonomous vehicles -- where alignment with human expectations can be
imperative. In this paper, we propose to adapt similar RL-based methods to
unsupervised object discovery, i.e. learning to detect objects from LiDAR
points without any training labels. Instead of labels, we use simple heuristics
to mimic human feedback. More explicitly, we combine multiple heuristics into a
simple reward function that positively correlates its score with bounding box
accuracy, i.e., boxes containing objects are scored higher than those without.
We start from the detector&apos;s own predictions to explore the space and reinforce
boxes with high rewards through gradient updates. Empirically, we demonstrate
that our approach is not only more accurate, but also orders of magnitudes
faster to train compared to prior works on object discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1&quot;&gt;Katie Z Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yurong You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1&quot;&gt;Sagie Benaim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1&quot;&gt;Cheng Perng Phoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1&quot;&gt;Mark Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1&quot;&gt;Bharath Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1&quot;&gt;Kilian Q. Weinberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19630">
<title>Convolutional Neural Networks for Automatic Detection of Intact Adenovirus from TEM Imaging with Debris, Broken and Artefacts Particles. (arXiv:2310.19630v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19630</link>
<description rdf:parseType="Literal">&lt;p&gt;Regular monitoring of the primary particles and purity profiles of a drug
product during development and manufacturing processes is essential for
manufacturers to avoid product variability and contamination. Transmission
electron microscopy (TEM) imaging helps manufacturers predict how changes
affect particle characteristics and purity for virus-based gene therapy vector
products and intermediates. Since intact particles can characterize efficacious
products, it is beneficial to automate the detection of intact adenovirus
against a non-intact-viral background mixed with debris, broken, and artefact
particles. In the presence of such particles, detecting intact adenoviruses
becomes more challenging. To overcome the challenge, due to such a presence, we
developed a software tool for semi-automatic annotation and segmentation of
adenoviruses and a software tool for automatic segmentation and detection of
intact adenoviruses in TEM imaging systems. The developed semi-automatic tool
exploited conventional image analysis techniques while the automatic tool was
built based on convolutional neural networks and image analysis techniques. Our
quantitative and qualitative evaluations showed outstanding true positive
detection rates compared to false positive and negative rates where
adenoviruses were nicely detected without mistaking them for real debris,
broken adenoviruses, and/or staining artefacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1&quot;&gt;Olivier Rukundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behanova_A/0/1/0/all/0/1&quot;&gt;Andrea Behanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feo_R/0/1/0/all/0/1&quot;&gt;Riccardo De Feo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronkko_S/0/1/0/all/0/1&quot;&gt;Seppo Ronkko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oja_J/0/1/0/all/0/1&quot;&gt;Joni Oja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tohka_J/0/1/0/all/0/1&quot;&gt;Jussi Tohka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19776">
<title>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19776</link>
<description rdf:parseType="Literal">&lt;p&gt;In the quest for unveiling novel categories at test time, we confront the
inherent limitations of traditional supervised recognition models that are
restricted by a predefined category set. While strides have been made in the
realms of self-supervised and open-world learning towards test-time category
discovery, a crucial yet often overlooked question persists: what exactly
delineates a category? In this paper, we conceptualize a category through the
lens of optimization, viewing it as an optimal solution to a well-defined
problem. Harnessing this unique conceptualization, we propose a novel,
efficient and self-supervised method capable of discovering previously unknown
categories at test time. A salient feature of our approach is the assignment of
minimum length category codes to individual data instances, which encapsulates
the implicit category hierarchy prevalent in real-world datasets. This
mechanism affords us enhanced control over category granularity, thereby
equipping our model to handle fine-grained categories adeptly. Experimental
evaluations, bolstered by state-of-the-art benchmark comparisons, testify to
the efficacy of our solution in managing unknown categories at test time.
Furthermore, we fortify our proposition with a theoretical foundation,
providing proof of its optimality. Our code is available at
https://github.com/SarahRastegar/InfoSieve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegar_S/0/1/0/all/0/1&quot;&gt;Sarah Rastegar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20381">
<title>A Comprehensive Study of GPT-4V&apos;s Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20381</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive evaluation of GPT-4V&apos;s capabilities
across diverse medical imaging tasks, including Radiology Report Generation,
Medical Visual Question Answering (VQA), and Visual Grounding. While prior
efforts have explored GPT-4V&apos;s performance in medical image analysis, to the
best of our knowledge, our study represents the first quantitative evaluation
on publicly available benchmarks. Our findings highlight GPT-4V&apos;s potential in
generating descriptive reports for chest X-ray images, particularly when guided
by well-structured prompts. Meanwhile, its performance on the MIMIC-CXR dataset
benchmark reveals areas for improvement in certain evaluation metrics, such as
CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in
distinguishing between question types but falls short of the VQA-RAD benchmark
in terms of accuracy. Furthermore, our analysis finds the limitations of
conventional evaluation metrics like the BLEU scores, advocating for the
development of more semantically robust assessment methods. In the field of
Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding
boxes, but its precision is lacking, especially in identifying specific medical
organs and signs. Our evaluation underscores the significant potential of
GPT-4V in the medical imaging domain, while also emphasizing the need for
targeted refinements to fully unlock its capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xinyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Leyang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20700">
<title>SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction. (arXiv:2310.20700v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20700</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently video generation has achieved substantial progress with realistic
results. Nevertheless, existing AI-generated videos are usually very short
clips (&quot;shot-level&quot;) depicting a single scene. To deliver a coherent long video
(&quot;story-level&quot;), it is desirable to have creative transition and prediction
effects across different clips. This paper presents a short-to-long video
diffusion model, SEINE, that focuses on generative transition and prediction.
The goal is to generate high-quality long videos with smooth and creative
transitions between scenes and varying lengths of shot-level videos.
Specifically, we propose a random-mask video diffusion model to automatically
generate transitions based on textual descriptions. By providing the images of
different scenes as inputs, combined with text-based control, our model
generates transition videos that ensure coherence and visual quality.
Furthermore, the model can be readily extended to various tasks such as
image-to-video animation and autoregressive video prediction. To conduct a
comprehensive evaluation of this new generative task, we propose three
assessing criteria for smooth and creative transition: temporal consistency,
semantic similarity, and video-text semantic alignment. Extensive experiments
validate the effectiveness of our approach over existing methods for generative
transition and prediction, enabling the creation of story-level long videos.
Project page: https://vchitect.github.io/SEINE-project/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lingjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1&quot;&gt;Shaobin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiashuo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yali Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00548">
<title>Continual atlas-based segmentation of prostate MRI. (arXiv:2311.00548v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00548</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) methods designed for natural image classification
often fail to reach basic quality standards for medical image segmentation.
Atlas-based segmentation, a well-established approach in medical imaging,
incorporates domain knowledge on the region of interest, leading to
semantically coherent predictions. This is especially promising for CL, as it
allows us to leverage structural information and strike an optimal balance
between model rigidity and plasticity over time. When combined with
privacy-preserving prototypes, this process offers the advantages of
rehearsal-based CL without compromising patient privacy. We propose Atlas
Replay, an atlas-based segmentation approach that uses prototypes to generate
high-quality segmentation masks through image registration that maintain
consistency even as the training distribution changes. We explore how our
proposed method performs compared to state-of-the-art CL methods in terms of
knowledge transferability across seven publicly available prostate segmentation
datasets. Prostate segmentation plays a vital role in diagnosing prostate
cancer, however, it poses challenges due to substantial anatomical variations,
benign structural differences in older age groups, and fluctuating acquisition
parameters. Our results show that Atlas Replay is both robust and generalizes
well to yet-unseen domains while being able to maintain knowledge, unlike
end-to-end segmentation methods. Our code base is available under
https://github.com/MECLabTUDA/Atlas-Replay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranem_A/0/1/0/all/0/1&quot;&gt;Amin Ranem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1&quot;&gt;Camila Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_D/0/1/0/all/0/1&quot;&gt;Daniel Pinto dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucher_A/0/1/0/all/0/1&quot;&gt;Andreas M. Bucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Othman_A/0/1/0/all/0/1&quot;&gt;Ahmed E. Othman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Anirban Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00734">
<title>On Manipulating Scene Text in the Wild with Diffusion Models. (arXiv:2311.00734v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00734</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have gained attention for image editing yielding impressive
results in text-to-image tasks. On the downside, one might notice that
generated images of stable diffusion models suffer from deteriorated details.
This pitfall impacts image editing tasks that require information preservation
e.g., scene text editing. As a desired result, the model must show the
capability to replace the text on the source image to the target text while
preserving the details e.g., color, font size, and background. To leverage the
potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene
Text manipulation Network so-called DBEST. Specifically, we design two
adaptation strategies, namely one-shot style adaptation and text-recognition
guidance. In experiments, we thoroughly assess and compare our proposed method
against state-of-the-arts on various scene text datasets, then provide
extensive ablation studies for each granularity to analyze our performance
gain. Also, we demonstrate the effectiveness of our proposed method to
synthesize scene text indicated by competitive Optical Character Recognition
(OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and
ICDAR2013 datasets for character-level evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santoso_J/0/1/0/all/0/1&quot;&gt;Joshua Santoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1&quot;&gt;Christian Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pao_W/0/1/0/all/0/1&quot;&gt;Williem Pao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01066">
<title>Dynamic Multimodal Information Bottleneck for Multimodality Classification. (arXiv:2311.01066v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01066</link>
<description rdf:parseType="Literal">&lt;p&gt;Effectively leveraging multimodal data such as various images, laboratory
tests and clinical information is gaining traction in a variety of AI-based
medical diagnosis and prognosis tasks. Most existing multi-modal techniques
only focus on enhancing their performance by leveraging the differences or
shared features from various modalities and fusing feature across different
modalities. These approaches are generally not optimal for clinical settings,
which pose the additional challenges of limited training data, as well as being
rife with redundant data or noisy modality channels, leading to subpar
performance. To address this gap, we study the robustness of existing methods
to data redundancy and noise and propose a generalized dynamic multimodal
information bottleneck framework for attaining a robust fused feature
representation. Specifically, our information bottleneck module serves to
filter out the task-irrelevant information and noises in the fused feature, and
we further introduce a sufficiency loss to prevent dropping of task-relevant
information, thus explicitly preserving the sufficiency of prediction
information in the distilled feature. We validate our model on an in-house and
a public COVID19 dataset for mortality prediction as well as two public
biomedical datasets for diagnostic tasks. Extensive experiments show that our
method surpasses the state-of-the-art and is significantly more robust, being
the only method to remain performance when large-scale noisy channels exist.
Our code is publicly available at https://github.com/BII-wushuang/DMIB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yingying Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xiaodan Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1&quot;&gt;Simon Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01423">
<title>CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar. (arXiv:2311.01423v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01423</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust perception is a vital component for ensuring safe autonomous and
assisted driving. Automotive radar (77 to 81 GHz), which offers
weather-resilient sensing, provides a complementary capability to the vision-
or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radar
tensors contain rich spatiotemporal semantics besides 3D location information.
The majority of previous methods take in 3D (Doppler-range-azimuth) RF radar
tensors, allowing prediction of an object&apos;s location, heading angle, and size
in bird&apos;s-eye-view (BEV). However, they lack the ability to at the same time
infer objects&apos; size, orientation, and identity in the 3D space. To overcome
this limitation, we propose an efficient joint architecture called
CenterRadarNet, designed to facilitate high-resolution representation learning
from 4D (Doppler-range-azimuth-elevation) radar data for 3D object detection
and re-identification (re-ID) tasks. As a single-stage 3D object detector,
CenterRadarNet directly infers the BEV object distribution confidence maps,
corresponding 3D bounding box attributes, and appearance embedding for each
pixel. Moreover, we build an online tracker utilizing the learned appearance
embedding for re-ID. CenterRadarNet achieves the state-of-the-art result on the
K-Radar 3D object detection benchmark. In addition, we present the first 3D
object-tracking result using radar on the K-Radar dataset V2. In diverse
driving scenarios, CenterRadarNet shows consistent, robust performance,
emphasizing its wide applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jen-Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuan_S/0/1/0/all/0/1&quot;&gt;Sheng-Yao Kuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gaowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01723">
<title>Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01723</link>
<description rdf:parseType="Literal">&lt;p&gt;While fine-tuning unlocks the potential of a pre-trained model for a specific
task, it compromises the model&apos;s ability to generalize to out-of-distribution
(OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance
on OOD datasets as well as on an in-distribution (ID) dataset for which the
model is being tuned. However, another criterion for reliable machine learning
(ML), confidence calibration, has been overlooked despite its increasing demand
for real-world high-stakes ML applications (e.g., autonomous driving and
medical diagnosis). For the first time, we raise concerns about the calibration
of fine-tuned vision-language models (VLMs) under distribution shift by showing
that naive fine-tuning and even state-of-the-art robust fine-tuning methods
hurt the calibration of pre-trained VLMs, especially on OOD datasets. To
address this issue, we provide a simple approach, called calibrated robust
fine-tuning (CaRot), that incentivizes calibration and robustness on both ID
and OOD datasets. Empirical results on ImageNet-1K distribution shift
evaluation verify the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mijoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyesu Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1&quot;&gt;Euiseog Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01766">
<title>Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01766</link>
<description rdf:parseType="Literal">&lt;p&gt;Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1&quot;&gt;Weidong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shujun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01989">
<title>Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation. (arXiv:2311.01989v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01989</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large-scale pre-trained models such as Segment-Anything Model (SAM)
and Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable
success and revolutionized the field of computer vision. These foundation
vision models effectively capture knowledge from a large-scale broad data with
their vast model parameters, enabling them to perform zero-shot segmentation on
previously unseen data without additional training. While they showcase
competence in 2D tasks, their potential for enhancing 3D scene understanding
remains relatively unexplored. To this end, we present a novel framework that
adapts various foundational models for the 3D point cloud segmentation task.
Our approach involves making initial predictions of 2D semantic masks using
different large vision models. We then project these mask predictions from
various frames of RGB-D video sequences into 3D space. To generate robust 3D
semantic pseudo labels, we introduce a semantic label fusion strategy that
effectively combines all the results via voting. We examine diverse scenarios,
like zero-shot learning and limited guidance from sparse 2D point labels, to
assess the pros and cons of different vision foundation models. Our approach is
experimented on ScanNet dataset for 3D indoor scenes, and the results
demonstrate the effectiveness of adopting general 2D foundation models on
solving 3D point cloud segmentation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shichao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fayao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>