<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.12622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.09232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.01587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.05246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.05674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.07593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01977" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.02082">
<title>Semantic Modelling of Organizational Knowledge as a Basis for Enterprise Data Governance 4.0 -- Application to a Unified Clinical Data Model. (arXiv:2311.02082v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02082</link>
<description rdf:parseType="Literal">&lt;p&gt;Individuals and organizations cope with an always-growing data amount,
heterogeneous in contents and formats. Prerequisites to get value out this data
and minimise inherent risks related to multiple usages are adequate data
management processes yielding data quality and control over its lifecycle.
Common data governance frameworks relying on people and policies falls short of
the overwhelming data complexity. Yet, harnessing this complexity is necessary
to achieve high quality standards. The later will condition the outcome of any
downstream data usage, including generative artificial intelligence trained on
this data. In this paper, we report our concrete experience establishing a
simple, cost-efficient framework, that enables metadata-driven, agile and
(semi-)automated data governance (i.e. Data Governance 4.0). We explain how we
implement and use this framework to integrate 25 years of clinical study data
at enterprise scale, in a fully productive environment. The framework
encompasses both methodologies and technologies leveraging semantic web
principles. We built an knowledge graph describing data assets avatars in their
business context including governance principles. Multiple ontologies
articulated by an enterprise upper ontology enable key governance actions such
as FAIRification, lifecycle management, definition of roles and
responsibilities, lineage across transformations and provenance from source
systems. This metadata model is a prerequisite to automatize data governance,
make it fit-for-purpose to each use case and dynamically adapting it to
business changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1&quot;&gt;Miguel AP Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manara_S/0/1/0/all/0/1&quot;&gt;Stephane Manara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mole_B/0/1/0/all/0/1&quot;&gt;Bruno Mol&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1&quot;&gt;Thomas Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillouche_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Guillouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hesske_L/0/1/0/all/0/1&quot;&gt;Lysann Hesske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_B/0/1/0/all/0/1&quot;&gt;Bruce Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubert_G/0/1/0/all/0/1&quot;&gt;Gilles Hubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_C/0/1/0/all/0/1&quot;&gt;Chinmay Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jagdev_P/0/1/0/all/0/1&quot;&gt;Pralipta Jagdev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1&quot;&gt;Cedric R. Berger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02083">
<title>MaRU: A Manga Retrieval and Understanding System Connecting Vision and Language. (arXiv:2311.02083v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.02083</link>
<description rdf:parseType="Literal">&lt;p&gt;Manga, a widely celebrated Japanese comic art form, is renowned for its
diverse narratives and distinct artistic styles. However, the inherently visual
and intricate structure of Manga, which comprises images housing multiple
panels, poses significant challenges for content retrieval. To address this, we
present MaRU (Manga Retrieval and Understanding), a multi-staged system that
connects vision and language to facilitate efficient search of both dialogues
and scenes within Manga frames. The architecture of MaRU integrates an object
detection model for identifying text and frame bounding boxes, a Vision
Encoder-Decoder model for text recognition, a text encoder for embedding text,
and a vision-text encoder that merges textual and visual information into a
unified embedding space for scene retrieval. Rigorous evaluations reveal that
MaRU excels in end-to-end dialogue retrieval and exhibits promising results for
scene retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Conghao Tom Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_V/0/1/0/all/0/1&quot;&gt;Violet Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02085">
<title>Preference Elicitation with Soft Attributes in Interactive Recommendation. (arXiv:2311.02085v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.02085</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference elicitation plays a central role in interactive recommender
systems. Most preference elicitation approaches use either item queries that
ask users to select preferred items from a slate, or attribute queries that ask
them to express their preferences for item characteristics. Unfortunately,
users often wish to describe their preferences using soft attributes for which
no ground-truth semantics is given. Leveraging concept activation vectors for
soft attribute semantics, we develop novel preference elicitation methods that
can accommodate soft attributes and bring together both item and
attribute-based preference elicitation. Our techniques query users using both
items and soft attributes to update the recommender system&apos;s belief about their
preferences to improve recommendation quality. We demonstrate the effectiveness
of our methods vis-a-vis competing approaches on both synthetic and real-world
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1&quot;&gt;Erdem Biyik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1&quot;&gt;Fan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1&quot;&gt;Yinlam Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haig_A/0/1/0/all/0/1&quot;&gt;Alex Haig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chih-wei Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1&quot;&gt;Craig Boutilier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02087">
<title>Design Of Rubble Analyzer Probe Using ML For Earthquake. (arXiv:2311.02087v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2311.02087</link>
<description rdf:parseType="Literal">&lt;p&gt;The earthquake rubble analyzer uses machine learning to detect human presence
via ambient sounds, achieving 97.45% accuracy. It also provides real-time
environmental data, aiding in assessing survival prospects for trapped
individuals, crucial for post-earthquake rescue efforts
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1&quot;&gt;Abhishek Sebastian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pragna_R/0/1/0/all/0/1&quot;&gt;R Pragna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vythianathan_K/0/1/0/all/0/1&quot;&gt;K Vishal Vythianathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sai_D/0/1/0/all/0/1&quot;&gt;Dasaraju Sohan Sai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_U/0/1/0/all/0/1&quot;&gt;U Shiva Sri Hari Al&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;R Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1&quot;&gt;Apurv Choudhary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02088">
<title>Combining Deep Learning on Order Books with Reinforcement Learning for Profitable Trading. (arXiv:2311.02088v1 [q-fin.CP])</title>
<link>http://arxiv.org/abs/2311.02088</link>
<description rdf:parseType="Literal">&lt;p&gt;High-frequency trading is prevalent, where automated decisions must be made
quickly to take advantage of price imbalances and patterns in price action that
forecast near-future movements. While many algorithms have been explored and
tested, analytical methods fail to harness the whole nature of the market
environment by focusing on a limited domain. With the evergrowing machine
learning field, many large-scale end-to-end studies on raw data have been
successfully employed to increase the domain scope for profitable trading but
are very difficult to replicate. Combining deep learning on the order books
with reinforcement learning is one way of breaking down large-scale end-to-end
learning into more manageable and lightweight components for reproducibility,
suitable for retail trading.
&lt;/p&gt;
&lt;p&gt;The following work focuses on forecasting returns across multiple horizons
using order flow imbalance and training three temporal-difference learning
models for five financial instruments to provide trading signals. The
instruments used are two foreign exchange pairs (GBPUSD and EURUSD), two
indices (DE40 and FTSE100), and one commodity (XAUUSD). The performances of
these 15 agents are evaluated through backtesting simulation, and successful
models proceed through to forward testing on a retail trading platform. The
results prove potential but require further minimal modifications for
consistently profitable trading to fully handle retail trading costs, slippage,
and spread fluctuation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Jaddu_K/0/1/0/all/0/1&quot;&gt;Koti S. Jaddu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Bilokon_P/0/1/0/all/0/1&quot;&gt;Paul A. Bilokon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02089">
<title>LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking. (arXiv:2311.02089v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.02089</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large language models (LLMs) have exhibited significant progress in
language understanding and generation. By leveraging textual features,
customized LLMs are also applied for recommendation and demonstrate
improvements across diverse recommendation scenarios. Yet the majority of
existing methods perform training-free recommendation that heavily relies on
pretrained knowledge (e.g., movie recommendation). In addition, inference on
LLMs is slow due to autoregressive generation, rendering existing methods less
effective for real-time recommendation. As such, we propose a two-stage
framework using large language models for ranking-based recommendation
(LlamaRec). In particular, we use small-scale sequential recommenders to
retrieve candidates based on the user interaction history. Then, both history
and retrieved items are fed to the LLM in text via a carefully designed prompt
template. Instead of generating next-item titles, we adopt a verbalizer-based
approach that transforms output logits into probability distributions over the
candidate items. Therefore, the proposed LlamaRec can efficiently rank items
without generating long text. To validate the effectiveness of the proposed
framework, we compare against state-of-the-art baseline methods on benchmark
datasets. Our experimental results demonstrate the performance of LlamaRec,
which consistently achieves superior performance in both recommendation
performance and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zhenrui Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1&quot;&gt;Sara Rabhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1&quot;&gt;Gabriel de Souza Pereira Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1&quot;&gt;Even Oldridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02099">
<title>A Preference Learning Approach to Develop Safe and Personalizable Autonomous Vehicles. (arXiv:2311.02099v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02099</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a preference learning method that ensures adherence to
traffic rules for autonomous vehicles. Our approach incorporates priority
ordering of signal temporal logic (STL) formulas, describing traffic rules,
into a learning framework. By leveraging the parametric weighted signal
temporal logic (PWSTL), we formulate the problem of safety-guaranteed
preference learning based on pairwise comparisons, and propose an approach to
solve this learning problem. Our approach finds a feasible valuation for the
weights of the given PWSTL formula such that, with these weights, preferred
signals have weighted quantitative satisfaction measures greater than their
non-preferred counterparts. The feasible valuation of weights given by our
approach leads to a weighted STL formula which can be used in
correct-and-custom-by-construction controller synthesis. We demonstrate the
performance of our method with human subject studies in two different simulated
driving scenarios involving a stop sign and a pedestrian crossing. Our approach
yields competitive results compared to existing preference learning methods in
terms of capturing preferences, and notably outperforms them when safety is
considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karagulle_R/0/1/0/all/0/1&quot;&gt;Ruya Karagulle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arechiga_N/0/1/0/all/0/1&quot;&gt;Nikos Arechiga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Best_A/0/1/0/all/0/1&quot;&gt;Andrew Best&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeCastro_J/0/1/0/all/0/1&quot;&gt;Jonathan DeCastro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozay_N/0/1/0/all/0/1&quot;&gt;Necmiye Ozay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02101">
<title>Solving MaxSAT with Matrix Multiplication. (arXiv:2311.02101v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02101</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an incomplete algorithm for Maximum Satisfiability (MaxSAT)
specifically designed to run on neural network accelerators such as GPUs and
TPUs. Given a MaxSAT problem instance in conjunctive normal form, our procedure
constructs a Restricted Boltzmann Machine (RBM) with an equilibrium
distribution wherein the probability of a Boolean assignment is exponential in
the number of clauses it satisfies. Block Gibbs sampling is used to
stochastically search the space of assignments with parallel Markov chains.
Since matrix multiplication is the main computational primitive for block Gibbs
sampling in an RBM, our approach leads to an elegantly simple algorithm (40
lines of JAX) well-suited for neural network accelerators. Theoretical results
about RBMs guarantee that the required number of visible and hidden units of
the RBM scale only linearly with the number of variables and constant-sized
clauses in the MaxSAT instance, ensuring that the computational cost of a Gibbs
step scales reasonably with the instance size. Search throughput can be
increased by batching parallel chains within a single accelerator as well as by
distributing them across multiple accelerators. As a further enhancement, a
heuristic based on unit propagation running on CPU is periodically applied to
the sampled assignments. Our approach, which we term RbmSAT, is a new design
point in the algorithm-hardware co-design space for MaxSAT. We present timed
results on a subset of problem instances from the annual MaxSAT Evaluation&apos;s
Incomplete Unweighted Track for the years 2018 to 2021. When allotted the same
running time and CPU compute budget (but no TPUs), RbmSAT outperforms other
participating solvers on problems drawn from three out of the four years&apos;
competitions. Given the same running time on a TPU cluster for which RbmSAT is
uniquely designed, it outperforms all solvers on problems drawn from all four
years.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warde_Farley_D/0/1/0/all/0/1&quot;&gt;David Warde-Farley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1&quot;&gt;Vinod Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yujia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobov_I/0/1/0/all/0/1&quot;&gt;Ivan Lobov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1&quot;&gt;Felix Gimeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1&quot;&gt;Simon Osindero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02102">
<title>Notion of Explainable Artificial Intelligence -- An Empirical Investigation from A Users Perspective. (arXiv:2311.02102v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02102</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing attention to artificial intelligence-based applications has led
to research interest in explainability issues. This emerging research attention
on explainable AI (XAI) advocates the need to investigate end user-centric
explainable AI. Thus, this study aims to investigate usercentric explainable AI
and considered recommendation systems as the study context. We conducted focus
group interviews to collect qualitative data on the recommendation system. We
asked participants about the end users&apos; comprehension of a recommended item,
its probable explanation, and their opinion of making a recommendation
explainable. Our findings reveal that end users want a non-technical and
tailor-made explanation with on-demand supplementary information. Moreover, we
also observed users requiring an explanation about personal data usage,
detailed user feedback, and authentic and reliable explanations. Finally, we
propose a synthesized framework that aims at involving the end user in the
development process for requirements collection and validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1&quot;&gt;AKM Bahalul Haque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1&quot;&gt;A.K.M. Najmul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikalef_P/0/1/0/all/0/1&quot;&gt;Patrick Mikalef&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02103">
<title>Relax: Composable Abstractions for End-to-End Dynamic Machine Learning. (arXiv:2311.02103v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02103</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic shape computations have become critical in modern machine learning
workloads, especially in emerging large language models. The success of these
models has driven demand for deploying them to a diverse set of backend
environments. In this paper, we present Relax, a compiler abstraction for
optimizing end-to-end dynamic machine learning workloads. Relax introduces
first-class symbolic shape annotations to track dynamic shape computations
globally across the program. It also introduces a cross-level abstraction that
encapsulates computational graphs, loop-level tensor programs, and library
calls in a single representation to enable cross-level optimizations. We build
an end-to-end compilation framework using the proposed approach to optimize
dynamic shape models. Experimental results on large language models show that
Relax delivers performance competitive with state-of-the-art hand-optimized
systems across platforms and enables deployment of emerging dynamic models to a
broader set of environments, including mobile phones, embedded devices, and web
browsers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1&quot;&gt;Ruihang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Junru Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Siyuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyubomirsky_S/0/1/0/all/0/1&quot;&gt;Steven S. Lyubomirsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bohan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wuwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zihao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hongyi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuchen Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lesheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yaxing Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Ziheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sunghyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1&quot;&gt;Prakalp Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roesch_J/0/1/0/all/0/1&quot;&gt;Jared G. Roesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mowry_T/0/1/0/all/0/1&quot;&gt;Todd C. Mowry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02104">
<title>Efficient Symbolic Policy Learning with Differentiable Symbolic Expression. (arXiv:2311.02104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02104</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (DRL) has led to a wide range of advances in
sequential decision-making tasks. However, the complexity of neural network
policies makes it difficult to understand and deploy with limited computational
resources. Currently, employing compact symbolic expressions as symbolic
policies is a promising strategy to obtain simple and interpretable policies.
Previous symbolic policy methods usually involve complex training processes and
pre-trained neural network policies, which are inefficient and limit the
application of symbolic policies. In this paper, we propose an efficient
gradient-based learning method named Efficient Symbolic Policy Learning (ESPL)
that learns the symbolic policy from scratch in an end-to-end way. We introduce
a symbolic network as the search space and employ a path selector to find the
compact symbolic policy. By doing so we represent the policy with a
differentiable symbolic expression and train it in an off-policy manner which
further improves the efficiency. In addition, in contrast with previous
symbolic policies which only work in single-task RL because of complexity, we
expand ESPL on meta-RL to generate symbolic policies for unseen tasks.
Experimentally, we show that our approach generates symbolic policies with
higher performance and greatly improves data efficiency for single-task RL. In
meta-RL, we demonstrate that compared with neural network policies the proposed
symbolic policy achieves higher performance and efficiency and shows the
potential to be interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiaming Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shaohui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1&quot;&gt;Qi Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruizhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zidong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xishan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Ling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunji Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02105">
<title>Making Harmful Behaviors Unlearnable for Large Language Models. (arXiv:2311.02105v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02105</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown great potential as general-purpose AI
assistants in various domains. To meet the requirements of different
applications, LLMs are often customized by further fine-tuning. However, the
powerful learning ability of LLMs not only enables them to acquire new tasks
but also makes them susceptible to learning undesired behaviors. For example,
even safety-aligned LLMs can be easily fine-tuned into harmful assistants as
the fine-tuning data often contains implicit or explicit harmful content. Can
we train LLMs on harmful data without learning harmful behaviors? This paper
proposes a controllable training framework that makes harmful behaviors
unlearnable during the fine-tuning process. Specifically, we introduce
``security vectors&apos;&apos;, a few new parameters that can be separated from the LLM,
to ensure LLM&apos;s responses are consistent with the harmful behavior. Security
vectors are activated during fine-tuning, the consistent behavior makes LLM
believe that such behavior has already been learned, there is no need to
further optimize for harmful data. During inference, we can deactivate security
vectors to restore the LLM&apos;s normal behavior. The experimental results show
that the security vectors generated by 100 harmful samples are enough to
prevent LLM from learning 1000 harmful samples, while preserving the ability to
learn other useful information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruotian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02106">
<title>Efficient Machine Learning Ensemble Methods for Detecting Gravitational Wave Glitches in LIGO Time Series. (arXiv:2311.02106v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02106</link>
<description rdf:parseType="Literal">&lt;p&gt;The phenomenon of Gravitational Wave (GW) analysis has grown in popularity as
technology has advanced and the process of observing gravitational waves has
become more precise. Although the sensitivity and the frequency of observation
of GW signals are constantly improving, the possibility of noise in the
collected GW data remains. In this paper, we propose two new Machine and Deep
learning ensemble approaches (i.e., ShallowWaves and DeepWaves Ensembles) for
detecting different types of noise and patterns in datasets from GW
observatories. Our research also investigates various Machine and Deep Learning
techniques for multi-class classification and provides a comprehensive
benchmark, emphasizing the best results in terms of three commonly used
performance metrics (i.e., accuracy, precision, and recall). We train and test
our models on a dataset consisting of annotated time series from real-world
data collected by the Advanced Laser Interferometer GW Observatory (LIGO). We
empirically show that the best overall accuracy is obtained by the proposed
DeepWaves Ensemble, followed close by the ShallowWaves Ensemble.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apostol_E/0/1/0/all/0/1&quot;&gt;Elena-Simona Apostol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truica_C/0/1/0/all/0/1&quot;&gt;Ciprian-Octavian Truic&amp;#x103;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02107">
<title>Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist. (arXiv:2311.02107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02107</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread use of ChatGPT and other emerging technology powered by
generative artificial intelligence (AI) has drawn much attention to potential
ethical issues, especially in high-stakes applications such as healthcare.
However, less clear is how to resolve such issues beyond following guidelines
and regulations that are still under discussion and development. On the other
hand, other types of generative AI have been used to synthesize images and
other types of data for research and practical purposes, which have resolved
some ethical issues and exposed other ethical issues, but such technology is
less often the focus of ongoing ethical discussions. Here we highlight gaps in
current ethical discussions of generative AI via a systematic scoping review of
relevant existing research in healthcare, and reduce the gaps by proposing an
ethics checklist for comprehensive assessment and transparent documentation of
ethical discussions in generative AI development. While the checklist can be
readily integrated into the current peer review and publication system to
enhance generative AI research, it may also be used in broader settings to
disclose ethics-related considerations in generative AI-powered products (or
real-life applications of such products) to help users establish reasonable
trust in their capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yilin Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teixayavong_S/0/1/0/all/0/1&quot;&gt;Salinelat Teixayavong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savulescu_J/0/1/0/all/0/1&quot;&gt;Julian Savulescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagaraj_V/0/1/0/all/0/1&quot;&gt;Vaishaanth Nagaraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1&quot;&gt;Di Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mertens_M/0/1/0/all/0/1&quot;&gt;Mayli Mertens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1&quot;&gt;Daniel Shu Wei Ting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_J/0/1/0/all/0/1&quot;&gt;Jasmine Chiat Ling Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiuwen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunn_M/0/1/0/all/0/1&quot;&gt;Michael Dunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaughan_R/0/1/0/all/0/1&quot;&gt;Roger Vaughan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1&quot;&gt;Marcus Eng Hock Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_J/0/1/0/all/0/1&quot;&gt;Joseph Jao-Yiu Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Topol_E/0/1/0/all/0/1&quot;&gt;Eric J Topol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02108">
<title>A Virtual Reality Training System for Automotive Engines Assembly and Disassembly. (arXiv:2311.02108v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.02108</link>
<description rdf:parseType="Literal">&lt;p&gt;Automotive engine assembly and disassembly are common and crucial programs in
the automotive industry. Traditional education trains students to learn
automotive engine assembly and disassembly in lecture courses and then to
operate with physical engines, which are generally low effectiveness and high
cost. In this work, we developed a multi-layer structured Virtual Reality (VR)
system to provide students with training in automotive engine (Buick Verano)
assembly and disassembly. We designed the VR training system with The VR
training system is designed to have several major features, including
replaceable engine parts and reusable tools, friendly user interfaces and
guidance, and bottom-up designed multi-layer architecture, which can be
extended to various engine models. The VR system is evaluated with controlled
experiments of two groups of students. The results demonstrate that our VR
training system provides remarkable usability in terms of effectiveness and
efficiency. Currently, our VR system has been demonstrated and employed in the
courses of Chinese colleges to train students in automotive engine assembly and
disassembly. A free-to-use executable file (Microsoft Windows) and open-source
code are available at https://github.com/LadissonLai/SUSTech_VREngine for
facilitating the development of VR systems in the automotive industry. Finally,
a video describing the operations in our VR training system is available at
https://www.youtube.com/watch?v=yZe4YTwwAC4
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1&quot;&gt;Gongjin Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1&quot;&gt;Qiangqiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bing Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zirui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1&quot;&gt;Qi Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02115">
<title>Towards objective and systematic evaluation of bias in medical imaging AI. (arXiv:2311.02115v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02115</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_E/0/1/0/all/0/1&quot;&gt;Emma A.M. Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1&quot;&gt;Raissa Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winder_A/0/1/0/all/0/1&quot;&gt;Anthony Winder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulve_V/0/1/0/all/0/1&quot;&gt;Vedant Gulve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amador_K/0/1/0/all/0/1&quot;&gt;Kimberly Amador&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilms_M/0/1/0/all/0/1&quot;&gt;Matthias Wilms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1&quot;&gt;Nils D. Forkert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02117">
<title>Cooperative Network Learning for Large-Scale and Decentralized Graphs. (arXiv:2311.02117v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02117</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph research, the systematic study of interconnected data points
represented as graphs, plays a vital role in capturing intricate relationships
within networked systems. However, in the real world, as graphs scale up,
concerns about data security among different data-owning agencies arise,
hindering information sharing and, ultimately, the utilization of graph data.
Therefore, establishing a mutual trust mechanism among graph agencies is
crucial for unlocking the full potential of graphs. Here, we introduce a
Cooperative Network Learning (CNL) framework to ensure secure graph computing
for various graph tasks. Essentially, this CNL framework unifies the local and
global perspectives of GNN computing with distributed data for an agency by
virtually connecting all participating agencies as a global graph without a
fixed central coordinator. Inter-agency computing is protected by various
technologies inherent in our framework, including homomorphic encryption and
secure transmission. Moreover, each agency has a fair right to design or employ
various graph learning models from its local or global perspective. Thus, CNL
can collaboratively train GNN models based on decentralized graphs inferred
from local and global graphs. Experiments on contagion dynamics prediction and
traditional graph tasks (i.e., node classification and link prediction)
demonstrate that our CNL architecture outperforms state-of-the-art GNNs
developed at individual sites, revealing that CNL can provide a reliable, fair,
secure, privacy-preserving, and global perspective to build effective and
personalized models for network applications. We hope this framework will
address privacy concerns in graph-related research and integrate decentralized
graph data structures to benefit the network research community in cooperation
and innovation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yujie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1&quot;&gt;Yujie Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Linyuan L&amp;#xfc;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02119">
<title>Safe Sequential Optimization for Switching Environments. (arXiv:2311.02119v1 [math.OC])</title>
<link>http://arxiv.org/abs/2311.02119</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of designing a sequential decision making agent to
maximize an unknown time-varying function which switches with time. At each
step, the agent receives an observation of the function&apos;s value at a point
decided by the agent. The observation could be corrupted by noise. The agent is
also constrained to take safe decisions with high probability, i.e., the chosen
points should have a function value greater than a threshold. For this
switching environment, we propose a policy called Adaptive-SafeOpt and evaluate
its performance via simulations. The policy incorporates Bayesian optimization
and change point detection for the safe sequential optimization problem. We
observe that a major challenge in adapting to the switching change is to
identify safe decisions when the change point is detected and prevent
attraction to local optima.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kalwar_D/0/1/0/all/0/1&quot;&gt;Durgesh Kalwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+S_V/0/1/0/all/0/1&quot;&gt;Vineeth B. S&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02123">
<title>RigLSTM: Recurrent Independent Grid LSTM for Generalizable Sequence Learning. (arXiv:2311.02123v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02123</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential processes in real-world often carry a combination of simple
subsystems that interact with each other in certain forms. Learning such a
modular structure can often improve the robustness against environmental
changes. In this paper, we propose recurrent independent Grid LSTM (RigLSTM),
composed of a group of independent LSTM cells that cooperate with each other,
for exploiting the underlying modular structure of the target task. Our model
adopts cell selection, input feature selection, hidden state selection, and
soft state updating to achieve a better generalization ability on the basis of
the recent Grid LSTM for the tasks where some factors differ between training
and evaluation. Specifically, at each time step, only a fraction of cells are
activated, and the activated cells select relevant inputs and cells to
communicate with. At the end of one time step, the hidden states of the
activated cells are updated by considering the relevance between the inputs and
the hidden states from the last and current time steps. Extensive experiments
on diversified sequential modeling tasks are conducted to show the superior
generalization ability when there exist changes in the testing environment.
Source code is available at \url{https://github.com/ziyuwwang/rig-lstm}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wei Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02124">
<title>Sliced Denoising: A Physics-Informed Molecular Pre-Training Method. (arXiv:2311.02124v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2311.02124</link>
<description rdf:parseType="Literal">&lt;p&gt;While molecular pre-training has shown great potential in enhancing drug
discovery, the lack of a solid physical interpretation in current methods
raises concerns about whether the learned representation truly captures the
underlying explanatory factors in observed data, ultimately resulting in
limited generalization and robustness. Although denoising methods offer a
physical interpretation, their accuracy is often compromised by ad-hoc noise
design, leading to inaccurate learned force fields. To address this limitation,
this paper proposes a new method for molecular pre-training, called sliced
denoising (SliDe), which is based on the classical mechanical intramolecular
potential theory. SliDe utilizes a novel noise strategy that perturbs bond
lengths, angles, and torsion angles to achieve better sampling over
conformations. Additionally, it introduces a random slicing approach that
circumvents the computationally expensive calculation of the Jacobian matrix,
which is otherwise essential for estimating the force field. By aligning with
physical principles, SliDe shows a 42\% improvement in the accuracy of
estimated force fields compared to current state-of-the-art denoising methods,
and thus outperforms traditional baselines on various molecular property
prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuyan Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shikun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Ying Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhi-Ming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02125">
<title>Using General Value Functions to Learn Domain-Backed Inventory Management Policies. (arXiv:2311.02125v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02125</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the inventory management problem, where the goal is to balance
conflicting objectives such as availability and wastage of a large range of
products in a store. We propose a reinforcement learning (RL) approach that
utilises General Value Functions (GVFs) to derive domain-backed inventory
replenishment policies. The inventory replenishment decisions are modelled as a
sequential decision making problem, which is challenging due to uncertain
demand and the existence of aggregate (cross-product) constraints. In existing
literature, GVFs have primarily been used for auxiliary task learning. We use
this capability to train GVFs on domain-critical characteristics such as
prediction of stock-out probability and wastage quantity. Using this domain
expertise for more effective exploration, we train an RL agent to compute the
inventory replenishment quantities for a large range of products (up to 6000 in
the reported experiments), which share aggregate constraints such as the total
weight/volume per delivery. Additionally, we show that the GVF predictions can
be used to provide additional domain-backed insights into the decisions
proposed by the RL agent. Finally, since the environment dynamics are fully
transferred, the trained GVFs can be used for faster adaptation to vastly
different business objectives (for example, due to the start of a promotional
period or due to deployment in a new customer environment).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalwar_D/0/1/0/all/0/1&quot;&gt;Durgesh Kalwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shelke_O/0/1/0/all/0/1&quot;&gt;Omkar Shelke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1&quot;&gt;Harshad Khadilkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02127">
<title>A Systematic Review of Deep Graph Neural Networks: Challenges, Classification, Architectures, Applications &amp; Potential Utility in Bioinformatics. (arXiv:2311.02127v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02127</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, tasks of machine learning ranging from image processing &amp;amp;
audio/video analysis to natural language understanding have been transformed by
deep learning. The data content in all these scenarios are expressed via
Euclidean space. However, a considerable amount of application data is
structured in non-Euclidean space and is expressed as graphs, e.g. dealing with
complicated interactions &amp;amp; object interdependencies. Modelling physical
systems, learning molecular signatures, identifying protein interactions and
predicting diseases involve utilising a model that can adapt from graph data.
Graph neural networks (GNNs), specified as artificial-neural models, employ
message transmission between graph nodes to represent graph dependencies and
are primarily used in the non-Euclidean domain. Variants of GNN like Graph
Recurrent Networks (GRN), Graph Auto Encoder (GAE), Graph Convolution Networks
(GCN), Graph Adversarial Methods &amp;amp; Graph Reinforcement learning have exhibited
breakthrough productivity on a wide range of tasks, especially in the field of
bioinformatics, in recent years as a result of the rapid collection of
biological network data. Apart from presenting all existing GNN models,
mathematical analysis and comparison of the variants of all types of GNN have
been highlighted in this survey. Graph neural networks are investigated for
their potential real-world applications in various fields, focusing on
Bioinformatics. Furthermore, resources for evaluating graph neural network
models and accessing open-source code &amp;amp; benchmark data sets are included.
Ultimately, we provide some (seven) proposals for future research in this
rapidly evolving domain. GNNs have the potential to be an excellent tool for
solving a wide range of biological challenges in bioinformatics research, as
they are best represented as connected complex graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malla_A/0/1/0/all/0/1&quot;&gt;Adil Mudasir Malla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banka_A/0/1/0/all/0/1&quot;&gt;Asif Ali Banka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02129">
<title>Hierarchical Reinforcement Learning for Power Network Topology Control. (arXiv:2311.02129v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning in high-dimensional action spaces is a key challenge in applying
reinforcement learning (RL) to real-world systems. In this paper, we study the
possibility of controlling power networks using RL methods. Power networks are
critical infrastructures that are complex to control. In particular, the
combinatorial nature of the action space poses a challenge to both conventional
optimizers and learned controllers. Hierarchical reinforcement learning (HRL)
represents one approach to address this challenge. More precisely, a HRL
framework for power network topology control is proposed. The HRL framework
consists of three levels of action abstraction. At the highest level, there is
the overall long-term task of power network operation, namely, keeping the
power grid state within security constraints at all times, which is decomposed
into two temporally extended actions: &apos;do nothing&apos; versus &apos;propose a topology
change&apos;. At the intermediate level, the action space consists of all
controllable substations. Finally, at the lowest level, the action space
consists of all configurations of the chosen substation. By employing this HRL
framework, several hierarchical power network agents are trained for the IEEE
14-bus network. Whereas at the highest level a purely rule-based policy is
still chosen for all agents in this study, at the intermediate level the policy
is trained using different state-of-the-art RL algorithms. At the lowest level,
either an RL algorithm or a greedy algorithm is used. The performance of the
different 3-level agents is compared with standard baseline (RL or greedy)
approaches. A key finding is that the 3-level agent that employs RL both at the
intermediate and the lowest level outperforms all other agents on the most
difficult task. Our code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manczak_B/0/1/0/all/0/1&quot;&gt;Blazej Manczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viebahn_J/0/1/0/all/0/1&quot;&gt;Jan Viebahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoof_H/0/1/0/all/0/1&quot;&gt;Herke van Hoof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02130">
<title>Client Orchestration and Cost-Efficient Joint Optimization for NOMA-Enabled Hierarchical Federated Learning. (arXiv:2311.02130v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02130</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical federated learning (HFL) shows great advantages over
conventional two-layer federated learning (FL) in reducing network overhead and
interaction latency while still retaining the data privacy of distributed FL
clients. However, the communication and energy overhead still pose a bottleneck
for HFL performance, especially as the number of clients raises dramatically.
To tackle this issue, we propose a non-orthogonal multiple access (NOMA)
enabled HFL system under semi-synchronous cloud model aggregation in this
paper, aiming to minimize the total cost of time and energy at each HFL global
round. Specifically, we first propose a novel fuzzy logic based client
orchestration policy considering client heterogenerity in multiple aspects,
including channel quality, data quantity and model staleness. Subsequently,
given the fuzzy based client-edge association, a joint edge server scheduling
and resource allocation problem is formulated. Utilizing problem decomposition,
we firstly derive the closed-form solution for the edge server scheduling
subproblem via the penalty dual decomposition (PDD) method. Next, a deep
deterministic policy gradient (DDPG) based algorithm is proposed to tackle the
resource allocation subproblem considering time-varying environments. Finally,
extensive simulations demonstrate that the proposed scheme outperforms the
considered benchmarks regarding HFL performance improvement and total cost
reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bibo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xianbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Donghong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Shu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02133">
<title>Safe Online Dynamics Learning with Initially Unknown Models and Infeasible Safety Certificates. (arXiv:2311.02133v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.02133</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety-critical control tasks with high levels of uncertainty are becoming
increasingly common. Typically, techniques that guarantee safety during
learning and control utilize constraint-based safety certificates, which can be
leveraged to compute safe control inputs. However, excessive model uncertainty
can render robust safety certification methods or infeasible, meaning no
control input satisfies the constraints imposed by the safety certificate. This
paper considers a learning-based setting with a robust safety certificate based
on a control barrier function (CBF) second-order cone program. If the control
barrier function certificate is feasible, our approach leverages it to
guarantee safety. Otherwise, our method explores the system dynamics to collect
data and recover the feasibility of the control barrier function constraint. To
this end, we employ a method inspired by well-established tools from Bayesian
optimization. We show that if the sampling frequency is high enough, we recover
the feasibility of the robust CBF certificate, guaranteeing safety. Our
approach requires no prior model and corresponds, to the best of our knowledge,
to the first algorithm that guarantees safety in settings with occasionally
infeasible safety certificates without requiring a backup non-learning-based
controller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Capone_A/0/1/0/all/0/1&quot;&gt;Alexandre Capone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cosner_R/0/1/0/all/0/1&quot;&gt;Ryan Cosner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ames_A/0/1/0/all/0/1&quot;&gt;Aaron Ames&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hirche_S/0/1/0/all/0/1&quot;&gt;Sandra Hirche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02142">
<title>Sparse Training of Discrete Diffusion Models for Graph Generation. (arXiv:2311.02142v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02142</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models for graphs often encounter scalability challenges due to
the inherent need to predict interactions for every node pair. Despite the
sparsity often exhibited by real-world graphs, the unpredictable sparsity
patterns of their adjacency matrices, stemming from their unordered nature,
leads to quadratic computational complexity. In this work, we introduce
SparseDiff, a denoising diffusion model for graph generation that is able to
exploit sparsity during its training phase. At the core of SparseDiff is a
message-passing neural network tailored to predict only a subset of edges
during each forward pass. When combined with a sparsity-preserving noise model,
this model can efficiently work with edge lists representations of graphs,
paving the way for scalability to much larger structures. During the sampling
phase, SparseDiff iteratively populates the adjacency matrix from its prior
state, ensuring prediction of the full graph while controlling memory
utilization. Experimental results show that SparseDiff simultaneously matches
state-of-the-art in generation performance on both small and large graphs,
highlighting the versatility of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yiming Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vignac_C/0/1/0/all/0/1&quot;&gt;Clement Vignac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1&quot;&gt;Pascal Frossard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02147">
<title>The Alignment Problem in Context. (arXiv:2311.02147v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02147</link>
<description rdf:parseType="Literal">&lt;p&gt;A core challenge in the development of increasingly capable AI systems is to
make them safe and reliable by ensuring their behaviour is consistent with
human values. This challenge, known as the alignment problem, does not merely
apply to hypothetical future AI systems that may pose catastrophic risks; it
already applies to current systems, such as large language models, whose
potential for harm is rapidly increasing. In this paper, I assess whether we
are on track to solve the alignment problem for large language models, and what
that means for the safety of future AI systems. I argue that existing
strategies for alignment are insufficient, because large language models remain
vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I
offer an explanation of this lingering vulnerability on which it is not simply
a contingent limitation of current language models, but has deep technical ties
to a crucial aspect of what makes these models useful and versatile in the
first place -- namely, their remarkable aptitude to learn &quot;in context&quot; directly
from user instructions. It follows that the alignment problem is not only
unsolved for current AI systems, but may be intrinsically difficult to solve
without severely undermining their capabilities. Furthermore, this assessment
raises concerns about the prospect of ensuring the safety of future and more
capable AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milliere_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Milli&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02171">
<title>Emergence of Abstract State Representations in Embodied Sequence Modeling. (arXiv:2311.02171v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02171</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision making via sequence modeling aims to mimic the success of language
models, where actions taken by an embodied agent are modeled as tokens to
predict. Despite their promising performance, it remains unclear if embodied
sequence modeling leads to the emergence of internal representations that
represent the environmental state information. A model that lacks abstract
state representations would be liable to make decisions based on surface
statistics which fail to generalize. We take the BabyAI environment, a grid
world in which language-conditioned navigation tasks are performed, and build a
sequence modeling Transformer, which takes a language instruction, a sequence
of actions, and environmental observations as its inputs. In order to
investigate the emergence of abstract state representations, we design a
&quot;blindfolded&quot; navigation task, where only the initial environmental layout, the
language instruction, and the action sequence to complete the task are
available for training. Our probing results show that intermediate
environmental layouts can be reasonably reconstructed from the internal
activations of a trained model, and that language instructions play a role in
the reconstruction accuracy. Our results suggest that many key features of
state representations can emerge via embodied sequence modeling, supporting an
optimistic outlook for applications of sequence modeling objectives to more
complex embodied decision-making domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1&quot;&gt;Tian Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zilai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Handa_K/0/1/0/all/0/1&quot;&gt;Kunal Handa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1&quot;&gt;Ashish V Thapliyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bo Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1&quot;&gt;Ellie Pavlick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02181">
<title>Joint Problems in Learning Multiple Dynamical Systems. (arXiv:2311.02181v1 [math.OC])</title>
<link>http://arxiv.org/abs/2311.02181</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering of time series is a well-studied problem, with applications
ranging from quantitative, personalized models of metabolism obtained from
metabolite concentrations to state discrimination in quantum information
theory. We consider a variant, where given a set of trajectories and a number
of parts, we jointly partition the set of trajectories and learn linear
dynamical system (LDS) models for each part, so as to minimize the maximum
error across all the models. We present globally convergent methods and EM
heuristics, accompanied by promising computational results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Niu_M/0/1/0/all/0/1&quot;&gt;Mengjia Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rysavy_P/0/1/0/all/0/1&quot;&gt;Petr Rysavy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Quan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02183">
<title>Cross-modal Prominent Fragments Enhancement Aligning Network for Image-text Retrieval. (arXiv:2311.02183v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02183</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text retrieval is a widely studied topic in the field of computer
vision due to the exponential growth of multimedia data, whose core concept is
to measure the similarity between images and text. However, most existing
retrieval methods heavily rely on cross-attention mechanisms for cross-modal
fine-grained alignment, which takes into account excessive irrelevant regions
and treats prominent and non-significant words equally, thereby limiting
retrieval accuracy. This paper aims to investigate an alignment approach that
reduces the involvement of non-significant fragments in images and text while
enhancing the alignment of prominent segments. For this purpose, we introduce
the Cross-Modal Prominent Fragments Enhancement Aligning Network(CPFEAN), which
achieves improved retrieval accuracy by diminishing the participation of
irrelevant regions during alignment and relatively increasing the alignment
similarity of prominent words. Additionally, we incorporate prior textual
information into image regions to reduce misalignment occurrences. In practice,
we first design a novel intra-modal fragments relationship reasoning method,
and subsequently employ our proposed alignment mechanism to compute the
similarity between images and text. Extensive quantitative comparative
experiments on MS-COCO and Flickr30K datasets demonstrate that our approach
outperforms state-of-the-art methods by about 5% to 10% in the rSum metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02191">
<title>SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data. (arXiv:2311.02191v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2311.02191</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and reliable human motion reconstruction is crucial for creating
natural interactions of full-body avatars in Virtual Reality (VR) and
entertainment applications. As the Metaverse and social applications gain
popularity, users are seeking cost-effective solutions to create full-body
animations that are comparable in quality to those produced by commercial
motion capture systems. In order to provide affordable solutions, though, it is
important to minimize the number of sensors attached to the subject&apos;s body.
Unfortunately, reconstructing the full-body pose from sparse data is a heavily
under-determined problem. Some studies that use IMU sensors face challenges in
reconstructing the pose due to positional drift and ambiguity of the poses. In
recent years, some mainstream VR systems have released 6-degree-of-freedom
(6-DoF) tracking devices providing positional and rotational information.
Nevertheless, most solutions for reconstructing full-body poses rely on
traditional inverse kinematics (IK) solutions, which often produce
non-continuous and unnatural poses. In this article, we introduce SparsePoser,
a novel deep learning-based solution for reconstructing a full-body pose from a
reduced set of six tracking devices. Our system incorporates a
convolutional-based autoencoder that synthesizes high-quality continuous human
poses by learning the human motion manifold from motion capture data. Then, we
employ a learned IK component, made of multiple lightweight feed-forward neural
networks, to adjust the hands and feet toward the corresponding trackers. We
extensively evaluate our method on publicly available motion capture datasets
and with real-time live demos. We show that our method outperforms
state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and
can be used for users with different body dimensions and proportions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponton_J/0/1/0/all/0/1&quot;&gt;Jose Luis Ponton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_H/0/1/0/all/0/1&quot;&gt;Haoran Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aristidou_A/0/1/0/all/0/1&quot;&gt;Andreas Aristidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andujar_C/0/1/0/all/0/1&quot;&gt;Carlos Andujar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelechano_N/0/1/0/all/0/1&quot;&gt;Nuria Pelechano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02194">
<title>AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation. (arXiv:2311.02194v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02194</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main challenges in offline Reinforcement Learning (RL) is the
distribution shift that arises from the learned policy deviating from the data
collection policy. This is often addressed by avoiding out-of-distribution
(OOD) actions during policy improvement as their presence can lead to
substantial performance degradation. This challenge is amplified in the offline
Multi-Agent RL (MARL) setting since the joint action space grows exponentially
with the number of agents. To avoid this curse of dimensionality, existing MARL
methods adopt either value decomposition methods or fully decentralized
training of individual agents. However, even when combined with standard
conservatism principles, these methods can still result in the selection of OOD
joint actions in offline MARL. To this end, we introduce AlberDICE, an offline
MARL algorithm that alternatively performs centralized training of individual
agents based on stationary distribution optimization. AlberDICE circumvents the
exponential complexity of MARL by computing the best response of one agent at a
time while effectively avoiding OOD joint action selection. Theoretically, we
show that the alternating optimization procedure converges to Nash policies. In
the experiments, we demonstrate that AlberDICE significantly outperforms
baseline algorithms on a standard suite of MARL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsunaga_D/0/1/0/all/0/1&quot;&gt;Daiki E. Matsunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jongmin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaeseok Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1&quot;&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kee-Eung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02198">
<title>Imitation Bootstrapped Reinforcement Learning. (arXiv:2311.02198v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02198</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the considerable potential of reinforcement learning (RL), robotics
control tasks predominantly rely on imitation learning (IL) owing to its better
sample efficiency. However, given the high cost of collecting extensive
demonstrations, RL is still appealing if it can utilize limited imitation data
for efficient autonomous self-improvement. Existing RL methods that utilize
demonstrations either initialize the replay buffer with demonstrations and
oversample them during RL training, which does not benefit from the
generalization potential of modern IL methods, or pretrain the RL policy with
IL on the demonstrations, which requires additional mechanisms to prevent
catastrophic forgetting during RL fine-tuning. We propose imitation
bootstrapped reinforcement learning (IBRL), a novel framework that first trains
an IL policy on a limited number of demonstrations and then uses it to propose
alternative actions for both online exploration and target value bootstrapping.
IBRL achieves SoTA performance and sample efficiency on 7 challenging sparse
reward continuous control tasks in simulation while learning directly from
pixels. As a highlight of our method, IBRL achieves $6.4\times$ higher success
rate than RLPD, a strong method that combines the idea of oversampling
demonstrations with modern RL improvements, under the budget of 10 demos and
100K interactions in the challenging PickPlaceCan task in the Robomimic
benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirchandani_S/0/1/0/all/0/1&quot;&gt;Suvir Mirchandani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02202">
<title>Neural Collage Transfer: Artistic Reconstruction via Material Manipulation. (arXiv:2311.02202v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02202</link>
<description rdf:parseType="Literal">&lt;p&gt;Collage is a creative art form that uses diverse material scraps as a base
unit to compose a single image. Although pixel-wise generation techniques can
reproduce a target image in collage style, it is not a suitable method due to
the solid stroke-by-stroke nature of the collage form. While some previous
works for stroke-based rendering produced decent sketches and paintings,
collages have received much less attention in research despite their popularity
as a style. In this paper, we propose a method for learning to make collages
via reinforcement learning without the need for demonstrations or collage
artwork data. We design the collage Markov Decision Process (MDP), which allows
the agent to handle various materials and propose a model-based soft
actor-critic to mitigate the agent&apos;s training burden derived from the
sophisticated dynamics of collage. Moreover, we devise additional techniques
such as active material selection and complexity-based multi-scale collage to
handle target images at any size and enhance the results&apos; aesthetics by placing
relatively more scraps in areas of high complexity. Experimental results show
that the trained agent appropriately selected and pasted materials to
regenerate the target image into a collage and obtained a higher evaluation
score on content and style than pixel-wise generation methods. Code is
available at https://github.com/northadventure/CollageRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Ganghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minji Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yunsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02211">
<title>Rock Climbing Route Generation and Grading as Computational Creativity. (arXiv:2311.02211v1 [cs.OH])</title>
<link>http://arxiv.org/abs/2311.02211</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we bridge work in rock climbing route generation and grading
into the computational creativity community. We provide the necessary
background to situate that literature and demonstrate the domain&apos;s intellectual
merit in the computational creativity community. We provide a guiding set of
desiderata for future work in this area. We propose an approach to
computational route grading. Finally, we identify important gaps in the
literature and consider how they may be filled. This paper thus also serves as
a pilot study, planting a flag for our ongoing research in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1&quot;&gt;Jesse Roberts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02215">
<title>Towards model-free RL algorithms that scale well with unstructured data. (arXiv:2311.02215v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02215</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional reinforcement learning (RL) algorithms exhibit broad generality
in their theoretical formulation and high performance on several challenging
domains when combined with powerful function approximation. However, developing
RL algorithms that perform well across problems with unstructured observations
at scale remains challenging because most function approximation methods rely
on externally provisioned knowledge about the structure of the input for good
performance (e.g. convolutional networks, graph neural networks, tile-coding).
A common practice in RL is to evaluate algorithms on a single problem, or on
problems with limited variation in the observation scale. RL practitioners lack
a systematic way to study how well a single RL algorithm performs when
instantiated across a range of problem scales, and they lack function
approximation techniques that scale well with unstructured observations.
&lt;/p&gt;
&lt;p&gt;We address these limitations by providing environments and algorithms to
study scaling for unstructured observation vectors and flat action spaces. We
introduce a family of combinatorial RL problems with an exponentially large
state space and high-dimensional dynamics but where linear computation is
sufficient to learn a (nonlinear) value function estimate for performant
control. We provide an algorithm that constructs reward-relevant general value
function (GVF) questions to find and exploit predictive structure directly from
the experience stream. In an empirical evaluation of the approach on synthetic
problems, we observe a sample complexity that scales linearly with the
observation size. The proposed algorithm reliably outperforms a conventional
deep RL algorithm on these scaling problems, and they exhibit several desirable
auxiliary properties. These results suggest new algorithmic mechanisms by which
algorithms can learn at scale from unstructured data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1&quot;&gt;Joseph Modayil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_Z/0/1/0/all/0/1&quot;&gt;Zaheer Abbas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02227">
<title>State-wise Safe Reinforcement Learning With Pixel Observations. (arXiv:2311.02227v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02227</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning(RL) in the context of safe exploration has long
grappled with the challenges of the delicate balance between maximizing rewards
and minimizing safety violations, the complexities arising from contact-rich or
non-smooth environments, and high-dimensional pixel observations. Furthermore,
incorporating state-wise safety constraints in the exploration and learning
process, where the agent is prohibited from accessing unsafe regions without
prior knowledge, adds an additional layer of complexity. In this paper, we
propose a novel pixel-observation safe RL algorithm that efficiently encodes
state-wise safety constraints with unknown hazard regions through the
introduction of a latent barrier function learning mechanism. As a joint
learning framework, our approach first involves constructing a latent dynamics
model with low-dimensional latent spaces derived from pixel observations.
Subsequently, we build and learn a latent barrier function on top of the latent
dynamics and conduct policy optimization simultaneously, thereby improving both
safety and the total expected return. Experimental evaluations on the
safety-gym benchmark suite demonstrate that our proposed method significantly
reduces safety violations throughout the training process and demonstrates
faster safety convergence compared to existing methods while achieving
competitive results in reward return.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1&quot;&gt;Simon Sinong Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_R/0/1/0/all/0/1&quot;&gt;Ruochen Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02236">
<title>Robust Fine-Tuning of Vision-Language Models for Domain Generalization. (arXiv:2311.02236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02236</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning enables the sharing of common knowledge among models for a
variety of downstream tasks, but traditional methods suffer in limited training
data settings and produce narrow models incapable of effectively generalizing
under distribution shifts. Foundation models have recently demonstrated
impressive zero-shot inference capabilities and robustness under distribution
shifts. However, zero-shot evaluation for these models has been predominantly
confined to benchmarks with simple distribution shifts, limiting our
understanding of their effectiveness under the more realistic shifts found in
practice. Moreover, common fine-tuning methods for these models have yet to be
evaluated against vision models in few-shot scenarios where training data is
limited. To address these gaps, we present a new recipe for few-shot
fine-tuning of the popular vision-language foundation model CLIP and evaluate
its performance on challenging benchmark datasets with realistic distribution
shifts from the WILDS collection. Our experimentation demonstrates that, while
zero-shot CLIP fails to match performance of trained vision models on more
complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only
counterparts in terms of in-distribution and out-of-distribution accuracy at
all levels of training data availability. This provides a strong incentive for
adoption of foundation models within few-shot learning applications operating
with real-world data. Code is available at
https://github.com/mit-ll/robust-vision-language-finetuning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_Lowell_K/0/1/0/all/0/1&quot;&gt;Kevin Vogt-Lowell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Noah Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1&quot;&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaillant_M/0/1/0/all/0/1&quot;&gt;Marc Vaillant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02248">
<title>COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning. (arXiv:2311.02248v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.02248</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a data and cost efficient way of incorporating the speech modality
into a large language model (LLM). The resulting multi-modal LLM is a
COntextual Speech Model with Instruction-following/in-context-learning
Capabilities - COSMIC. Speech comprehension test question-answer (SQA) pairs
are generated using GPT-3.5 based on the speech transcriptions as a part of the
supervision for the instruction tuning. With fewer than 20M trainable
parameters and as little as 450 hours of English speech data for SQA
generation, COSMIC exhibits emergent instruction-following and in-context
learning capabilities in speech-to-text tasks. The model is able to follow the
given text instructions to generate text response even on the unseen EN$\to$X
speech-to-text translation (S2TT) task with zero-shot setting. We evaluate the
model&apos;s in-context learning via various tasks such as EN$\to$X S2TT and
few-shot domain adaptation. And instruction-following capabilities are
evaluated through a contextual biasing benchmark. Our results demonstrate the
efficacy of the proposed low cost recipe for building a speech LLM and that
with the new instruction-tuning data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jing Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1&quot;&gt;Yashesh Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivasankaran_S/0/1/0/all/0/1&quot;&gt;Sunit Sivasankaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shujie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02251">
<title>The Potential of Wearable Sensors for Assessing Patient Acuity in Intensive Care Unit (ICU). (arXiv:2311.02251v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02251</link>
<description rdf:parseType="Literal">&lt;p&gt;Acuity assessments are vital in critical care settings to provide timely
interventions and fair resource allocation. Traditional acuity scores rely on
manual assessments and documentation of physiological states, which can be
time-consuming, intermittent, and difficult to use for healthcare providers.
Furthermore, such scores do not incorporate granular information such as
patients&apos; mobility level, which can indicate recovery or deterioration in the
ICU. We hypothesized that existing acuity scores could be potentially improved
by employing Artificial Intelligence (AI) techniques in conjunction with
Electronic Health Records (EHR) and wearable sensor data. In this study, we
evaluated the impact of integrating mobility data collected from wrist-worn
accelerometers with clinical data obtained from EHR for developing an AI-driven
acuity assessment score. Accelerometry data were collected from 86 patients
wearing accelerometers on their wrists in an academic hospital setting. The
data was analyzed using five deep neural network models: VGG, ResNet,
MobileNet, SqueezeNet, and a custom Transformer network. These models
outperformed a rule-based clinical score (SOFA= Sequential Organ Failure
Assessment) used as a baseline, particularly regarding the precision,
sensitivity, and F1 score. The results showed that while a model relying solely
on accelerometer data achieved limited performance (AUC 0.50, Precision 0.61,
and F1-score 0.68), including demographic information with the accelerometer
data led to a notable enhancement in performance (AUC 0.69, Precision 0.75, and
F1-score 0.67). This work shows that the combination of mobility and patient
information can successfully differentiate between stable and unstable states
in critically ill patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sena_J/0/1/0/all/0/1&quot;&gt;Jessica Sena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mostafiz_M/0/1/0/all/0/1&quot;&gt;Mohammad Tahsin Mostafiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davidson_A/0/1/0/all/0/1&quot;&gt;Andrea Davidson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Sabyasachi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuanfang_R/0/1/0/all/0/1&quot;&gt;Ren Yuanfang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozrazgat_Baslanti_T/0/1/0/all/0/1&quot;&gt;Tezcan Ozrazgat-Baslanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_T/0/1/0/all/0/1&quot;&gt;Tyler Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_W/0/1/0/all/0/1&quot;&gt;William Robson Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02253">
<title>Comparative Knowledge Distillation. (arXiv:2311.02253v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02253</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of large scale pretrained models, Knowledge Distillation (KD)
serves an important role in transferring the wisdom of computationally heavy
teacher models to lightweight, efficient student models while preserving
performance. Traditional KD paradigms, however, assume readily available access
to teacher models for frequent inference -- a notion increasingly at odds with
the realities of costly, often proprietary, large scale models. Addressing this
gap, our paper considers how to minimize the dependency on teacher model
inferences in KD in a setting we term Few Teacher Inference Knowledge
Distillation (FTI KD). We observe that prevalent KD techniques and state of the
art data augmentation strategies fall short in this constrained setting.
Drawing inspiration from educational principles that emphasize learning through
comparison, we propose Comparative Knowledge Distillation (CKD), which
encourages student models to understand the nuanced differences in a teacher
model&apos;s interpretations of samples. Critically, CKD provides additional
learning signals to the student without making additional teacher calls. We
also extend the principle of CKD to groups of samples, enabling even more
efficient learning from limited teacher calls. Empirical evaluation across
varied experimental settings indicates that CKD consistently outperforms state
of the art data augmentation and KD techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilf_A/0/1/0/all/0/1&quot;&gt;Alex Wilf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1&quot;&gt;Alex Tianyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obolenskiy_A/0/1/0/all/0/1&quot;&gt;Alexander Obolenskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1&quot;&gt;Daniel Fried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02268">
<title>LLMs-augmented Contextual Bandit. (arXiv:2311.02268v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02268</link>
<description rdf:parseType="Literal">&lt;p&gt;Contextual bandits have emerged as a cornerstone in reinforcement learning,
enabling systems to make decisions with partial feedback. However, as contexts
grow in complexity, traditional bandit algorithms can face challenges in
adequately capturing and utilizing such contexts. In this paper, we propose a
novel integration of large language models (LLMs) with the contextual bandit
framework. By leveraging LLMs as an encoder, we enrich the representation of
the context, providing the bandit with a denser and more informative view.
Preliminary results on synthetic datasets demonstrate the potential of this
approach, showing notable improvements in cumulative rewards and reductions in
regret compared to traditional bandit algorithms. This integration not only
showcases the capabilities of LLMs in reinforcement learning but also opens the
door to a new era of contextually-aware decision systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baheri_A/0/1/0/all/0/1&quot;&gt;Ali Baheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alm_C/0/1/0/all/0/1&quot;&gt;Cecilia O. Alm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02271">
<title>FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization. (arXiv:2311.02271v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.02271</link>
<description rdf:parseType="Literal">&lt;p&gt;Summaries of medical text shall be faithful by being consistent and factual
with source inputs, which is an important but understudied topic for safety and
efficiency in healthcare. In this paper, we investigate and improve
faithfulness in summarization on a broad range of medical summarization tasks.
Our investigation reveals that current summarization models often produce
unfaithful outputs for medical input text. We then introduce FAMESUMM, a
framework to improve faithfulness by fine-tuning pre-trained language models
based on medical knowledge. FAMESUMM performs contrastive learning on designed
sets of faithful and unfaithful summaries, and it incorporates medical terms
and their contexts to encourage faithful generation of medical terms. We
conduct comprehensive experiments on three datasets in two languages: health
question and radiology report summarization datasets in English, and a
patient-doctor dialogue dataset in Chinese. Results demonstrate that FAMESUMM
is flexible and effective by delivering consistent improvements over mainstream
language models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art
performances on metrics for faithfulness and general quality. Human evaluation
by doctors also shows that FAMESUMM generates more faithful outputs. Our code
is available at https: //github.com/psunlpgroup/FaMeSumm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Nan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yusen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1&quot;&gt;Prasenjit Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02287">
<title>Predicting Ground Reaction Force from Inertial Sensors. (arXiv:2311.02287v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02287</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of ground reaction forces (GRF) is used to characterize the
mechanical loading experienced by individuals in movements such as running,
which is clinically applicable to identify athletes at risk for stress-related
injuries. Our aim in this paper is to determine if data collected with inertial
measurement units (IMUs), that can be worn by athletes during outdoor runs, can
be used to predict GRF with sufficient accuracy to allow the analysis of its
derived biomechanical variables (e.g., contact time and loading rate).
&lt;/p&gt;
&lt;p&gt;In this paper, we consider lightweight approaches in contrast to
state-of-the-art prediction using LSTM neural networks. Specifically, we
compare use of LSTMs to k-Nearest Neighbors (KNN) regression as well as propose
a novel solution, SVD Embedding Regression (SER), using linear regression
between singular value decomposition embeddings of IMUs data (input) and GRF
data (output). We evaluate the accuracy of these techniques when using training
data collected from different athletes, from the same athlete, or both, and we
explore the use of acceleration and angular velocity data from sensors at
different locations (sacrum and shanks). Our results illustrate that simple
machine learning methods such as SER and KNN can be similarly accurate or more
accurate than LSTM neural networks, with much faster training times and
hyperparameter optimization; in particular, SER and KNN are more accurate when
personal training data are available, and KNN comes with benefit of providing
provenance of prediction. Notably, the use of personal data reduces prediction
errors of all methods for most biomechanical variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bowen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paolieri_M/0/1/0/all/0/1&quot;&gt;Marco Paolieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_H/0/1/0/all/0/1&quot;&gt;Harper E. Stewart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golubchik_L/0/1/0/all/0/1&quot;&gt;Leana Golubchik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McNitt_Gray_J/0/1/0/all/0/1&quot;&gt;Jill L. McNitt-Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1&quot;&gt;Vishal Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Devavrat Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02291">
<title>A Survey of the Various Methodologies Towards making Artificial Intelligence More Explainable. (arXiv:2311.02291v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02291</link>
<description rdf:parseType="Literal">&lt;p&gt;Machines are being increasingly used in decision-making processes, resulting
in the realization that decisions need explanations. Unfortunately, an
increasing number of these deployed models are of a &apos;black-box&apos; nature where
the reasoning behind the decisions is unknown. Hence, there is a need for
clarity behind the reasoning of these decisions. As humans, we would want these
decisions to be presented to us in an explainable manner. However, explanations
alone are insufficient. They do not necessarily tell us how to achieve an
outcome but merely tell us what achieves the given outcome. For this reason, my
research focuses on explainability/interpretability and how it extends to
counterfactual thinking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1&quot;&gt;Sopam Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02300">
<title>Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis. (arXiv:2311.02300v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02300</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta learning is a promising technique for solving few-shot fault prediction
problems, which have attracted the attention of many researchers in recent
years. Existing meta-learning methods for time series prediction, which
predominantly rely on random and similarity matching-based task partitioning,
face three major limitations: (1) feature exploitation inefficiency; (2)
suboptimal task data allocation; and (3) limited robustness with small samples.
To overcome these limitations, we introduce a novel &apos;pseudo meta-task&apos;
partitioning scheme that treats a continuous time period of a time series as a
meta-task, composed of multiple successive short time periods. Employing
continuous time series as pseudo meta-tasks allows our method to extract more
comprehensive features and relationships from the data, resulting in more
accurate predictions. Moreover, we introduce a differential algorithm to
enhance the robustness of our method across different datasets. Through
extensive experiments on several fault and time series prediction datasets, we
demonstrate that our approach substantially enhances prediction performance and
generalization capability under both few-shot and general conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiajun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Songsen Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02303">
<title>MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning. (arXiv:2311.02303v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02303</link>
<description rdf:parseType="Literal">&lt;p&gt;Code LLMs have emerged as a specialized research field, with remarkable
studies dedicated to enhancing model&apos;s coding capabilities through fine-tuning
on pre-trained models. Previous fine-tuning approaches were typically tailored
to specific downstream tasks or scenarios, which meant separate fine-tuning for
each task, requiring extensive training resources and posing challenges in
terms of deployment and maintenance. Furthermore, these approaches failed to
leverage the inherent interconnectedness among different code-related tasks. To
overcome these limitations, we present a multi-task fine-tuning framework,
MFTcoder, that enables simultaneous and parallel fine-tuning on multiple tasks.
By incorporating various loss functions, we effectively address common
challenges in multi-task learning, such as data imbalance, varying difficulty
levels, and inconsistent convergence speeds. Extensive experiments have
conclusively demonstrated that our multi-task fine-tuning approach outperforms
both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble
of tasks. Moreover, MFTcoder offers efficient training capabilities, including
efficient data tokenization modes and PEFT fine-tuning, resulting in
significantly improved speed compared to traditional fine-tuning methods.
MFTcoder seamlessly integrates with several mainstream open-source LLMs, such
as CodeLLama and Qwen. Leveraging the CodeLLama foundation, our MFTcoder
fine-tuned model, \textsc{CodeFuse-CodeLLama-34B}, achieves an impressive
pass@1 score of 74.4\% on the HumaneEval benchmark, surpassing GPT-4
performance (67\%, zero-shot). MFTCoder is open-sourced at
\url{https://github.com/codefuse-ai/MFTCOder}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingchang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Cong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Zi Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1&quot;&gt;Ming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;Min Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hailian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02305">
<title>OSM vs HD Maps: Map Representations for Trajectory Prediction. (arXiv:2311.02305v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02305</link>
<description rdf:parseType="Literal">&lt;p&gt;While High Definition (HD) Maps have long been favored for their precise
depictions of static road elements, their accessibility constraints and
susceptibility to rapid environmental changes impede the widespread deployment
of autonomous driving, especially in the motion forecasting task. In this
context, we propose to leverage OpenStreetMap (OSM) as a promising alternative
to HD Maps for long-term motion forecasting. The contributions of this work are
threefold: firstly, we extend the application of OSM to long-horizon
forecasting, doubling the forecasting horizon compared to previous studies.
Secondly, through an expanded receptive field and the integration of
intersection priors, our OSM-based approach exhibits competitive performance,
narrowing the gap with HD Map-based models. Lastly, we conduct an exhaustive
context-aware analysis, providing deeper insights in motion forecasting across
diverse scenarios as well as conducting class-aware comparisons. This research
not only advances long-term motion forecasting with coarse map representations
but additionally offers a potential scalable solution within the domain of
autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jing-Yan Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Parth Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_D/0/1/0/all/0/1&quot;&gt;David Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_H/0/1/0/all/0/1&quot;&gt;Henrik Christensen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02314">
<title>Thermal Face Image Classification using Deep Learning Techniques. (arXiv:2311.02314v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02314</link>
<description rdf:parseType="Literal">&lt;p&gt;Thermal images have various applications in security, medical and industrial
domains. This paper proposes a practical deep-learning approach for thermal
image classification. Accurate and efficient classification of thermal images
poses a significant challenge across various fields due to the complex image
content and the scarcity of annotated datasets. This work uses a convolutional
neural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to
extract features from thermal images. This work also applied Kalman filter on
thermal input images for image denoising. The experimental results demonstrate
the effectiveness of the proposed approach in terms of accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1&quot;&gt;Prosenjit Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_A/0/1/0/all/0/1&quot;&gt;ANK Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02326">
<title>FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation. (arXiv:2311.02326v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.02326</link>
<description rdf:parseType="Literal">&lt;p&gt;Drug-Target Interaction (DTI) prediction is vital for drug discovery, yet
challenges persist in achieving model interpretability and optimizing
performance. We propose a novel transformer-based model, FragXsiteDTI, that
aims to address these challenges in DTI prediction. Notably, FragXsiteDTI is
the first DTI model to simultaneously leverage drug molecule fragments and
protein pockets. Our information-rich representations for both proteins and
drugs offer a detailed perspective on their interaction. Inspired by the
Perceiver IO framework, our model features a learnable latent array, initially
interacting with protein binding site embeddings using cross-attention and
later refined through self-attention and used as a query to the drug fragments
in the drug&apos;s cross-attention transformer block. This learnable query array
serves as a mediator and enables seamless information translation, preserving
critical nuances in drug-protein interactions. Our computational results on
three benchmarking datasets demonstrate the superior predictive power of our
model over several state-of-the-art models. We also show the interpretability
of our model in terms of the critical components of both target proteins and
drug molecules within drug-target pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yalabadi_A/0/1/0/all/0/1&quot;&gt;Ali Khodabandeh Yalabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdani_Jahromi_M/0/1/0/all/0/1&quot;&gt;Mehdi Yazdani-Jahromi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousefi_N/0/1/0/all/0/1&quot;&gt;Niloofar Yousefi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayebi_A/0/1/0/all/0/1&quot;&gt;Aida Tayebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdidizaji_S/0/1/0/all/0/1&quot;&gt;Sina Abdidizaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garibay_O/0/1/0/all/0/1&quot;&gt;Ozlem Ozmen Garibay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02329">
<title>Complex Organ Mask Guided Radiology Report Generation. (arXiv:2311.02329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02329</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of automatic report generation is to generate a clinically accurate
and coherent phrase from a single given X-ray image, which could alleviate the
workload of traditional radiology reporting.However, in a real-world scenario,
radiologists frequently face the challenge of producing extensive reports
derived from numerous medical images, thereby medical report generation from
multi-image perspective is needed.In this paper, we propose the Complex Organ
Mask Guided (termed as COMG) report generation model, which incorporates masks
from multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide
more detailed information and guide the model&apos;s attention to these crucial body
regions. Specifically, we leverage prior knowledge of the disease corresponding
to each organ in the fusion process to enhance the disease identification phase
during the report generation process. Additionally, cosine similarity loss is
introduced as target function to ensure the convergence of cross-modal
consistency and facilitate model optimization.Experimental results on two
public datasets show that COMG achieves a 11.4% and 9.7% improvement in terms
of BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiancheng_G/0/1/0/all/0/1&quot;&gt;Gu Tiancheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dongnan_L/0/1/0/all/0/1&quot;&gt;Liu Dongnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhiyuan_L/0/1/0/all/0/1&quot;&gt;Li Zhiyuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weidong_C/0/1/0/all/0/1&quot;&gt;Cai Weidong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.12622">
<title>WD3: Taming the Estimation Bias in Deep Reinforcement Learning. (arXiv:2006.12622v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2006.12622</link>
<description rdf:parseType="Literal">&lt;p&gt;The overestimation phenomenon caused by function approximation is a
well-known issue in value-based reinforcement learning algorithms such as deep
Q-networks and DDPG, which could lead to suboptimal policies. To address this
issue, TD3 takes the minimum value between a pair of critics. In this paper, we
show that the TD3 algorithm introduces underestimation bias in mild
assumptions. To obtain a more precise estimation for value function, we unify
these two opposites and propose a novel algorithm \underline{W}eighted
\underline{D}elayed \underline{D}eep \underline{D}eterministic Policy Gradient
(WD3), which can eliminate the estimation bias and further improve the
performance by weighting a pair of critics. To demonstrate the effectiveness of
WD3, we compare the learning process of value function between DDPG, TD3, and
WD3. The results verify that our algorithm does eliminate the estimation error
of value functions. Furthermore, we evaluate our algorithm on the continuous
control tasks. We observe that in each test task, the performance of WD3
consistently outperforms, or at the very least matches, that of the
state-of-the-art algorithms\footnote{Our code is available
at~\href{https://sites.google.com/view/ictai20-wd3/}{https://sites.google.com/view/ictai20-wd3/}.}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xinwen Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.09232">
<title>Using reinforcement learning to autonomously identify sources of error for agents in group missions. (arXiv:2107.09232v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2107.09232</link>
<description rdf:parseType="Literal">&lt;p&gt;When agents swarm to execute a mission, some of them frequently exhibit
sudden failure, as observed from the command base. It is generally difficult to
determine whether a failure is caused by actuators (hypothesis, $h_a$) or
sensors (hypothesis, $h_s$) by solely relying on the communication between the
command base and concerning agent. However, by instigating collusion between
the agents, the cause of failure can be identified; in other words, we expect
to detect corresponding displacements for $h_a$ but not for $h_s$. In this
study, we considered the question as to whether artificial intelligence can
autonomously generate an action plan $\boldsymbol{g}$ to pinpoint the cause as
aforedescribed. Because the expected response to $\boldsymbol{g}$ generally
depends upon the adopted hypothesis [let the difference be denoted by
$D(\boldsymbol{g})$], a formulation that uses $D\left(\boldsymbol{g}\right)$ to
pinpoint the cause can be made. Although a $\boldsymbol{g}^*$ that maximizes
$D(\boldsymbol{g})$ would be a suitable action plan for this task, such an
optimization is difficult to achieve using the conventional gradient method, as
$D(\boldsymbol{g})$ becomes nonzero in rare events such as collisions with
other agents, and most swarm actions $\boldsymbol{g}$ give
$D(\boldsymbol{g})=0$. In other words, throughout almost the entire space of
$\boldsymbol{g}$, $D(\boldsymbol{g})$ has zero gradient, and the gradient
method is not applicable. To overcome this problem, we formulated an action
plan using Q-table reinforcement learning. Surprisingly, the optimal action
plan generated via reinforcement learning presented a human-like solution to
pinpoint the problem by colliding other agents with the failed agent. Using
this simple prototype, we demonstrated the potential of applying Q-table
reinforcement learning methods to plan autonomous actions to pinpoint the
causes of failure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utimula_K/0/1/0/all/0/1&quot;&gt;Keishu Utimula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayaschi_K/0/1/0/all/0/1&quot;&gt;Ken-taro Hayaschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihl_T/0/1/0/all/0/1&quot;&gt;Trevor J. Bihl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hongo_K/0/1/0/all/0/1&quot;&gt;Kenta Hongo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maezono_R/0/1/0/all/0/1&quot;&gt;Ryo Maezono&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.01587">
<title>Improving accuracy and uncertainty quantification of deep learning based quantitative MRI using Monte Carlo dropout. (arXiv:2112.01587v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Dropout is conventionally used during the training phase as regularization
method and for quantifying uncertainty in deep learning. We propose to use
dropout during training as well as inference steps, and average multiple
predictions to improve the accuracy, while reducing and quantifying the
uncertainty. The results are evaluated for fractional anisotropy (FA) and mean
diffusivity (MD) maps which are obtained from only 3 direction scans. With our
method, accuracy can be improved significantly compared to network outputs
without dropout, especially when the training dataset is small. Moreover,
confidence maps are generated which may aid in diagnosis of unseen pathology or
artifacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Avci_M/0/1/0/all/0/1&quot;&gt;Mehmet Yigit Avci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qiuyun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Susie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bilgic_B/0/1/0/all/0/1&quot;&gt;Berkin Bilgic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qiyuan Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12305">
<title>A Post-Quantum Associative Memory. (arXiv:2201.12305v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12305</link>
<description rdf:parseType="Literal">&lt;p&gt;Associative memories are devices storing information that can be fully
retrieved given partial disclosure of it. We examine a toy model of associative
memory and the ultimate limitations it is subjected to within the framework of
general probabilistic theories (GPTs), which represent the most general class
of physical theories satisfying some basic operational axioms. We ask ourselves
how large the dimension of a GPT should be so that it can accommodate $2^m$
states with the property that any $N$ of them are perfectly distinguishable.
Call $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and
Gr\&quot;unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the
GPT is required to be either classical or quantum. This yields an example of a
task where GPTs outperform both classical and quantum theory exponentially.
More generally, we resolve the case of fixed $N$ and asymptotically large $m$,
proving that $d(N,m) \leq m^{1+o_N(1)}$ (as $m\to\infty$) for every $N\geq 2$,
which yields again an exponential improvement over classical and quantum
theories. Finally, we develop a numerical approach to the general problem of
finding the largest $N$-wise mutually distinguishable set for a given GPT,
which can be seen as an instance of the maximum clique problem on $N$-regular
hypergraphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Lami_L/0/1/0/all/0/1&quot;&gt;Ludovico Lami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Goldwater_D/0/1/0/all/0/1&quot;&gt;Daniel Goldwater&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Adesso_G/0/1/0/all/0/1&quot;&gt;Gerardo Adesso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.05246">
<title>Monotone Learning. (arXiv:2202.05246v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.05246</link>
<description rdf:parseType="Literal">&lt;p&gt;The amount of training-data is one of the key factors which determines the
generalization capacity of learning algorithms. Intuitively, one expects the
error rate to decrease as the amount of training-data increases. Perhaps
surprisingly, natural attempts to formalize this intuition give rise to
interesting and challenging mathematical questions. For example, in their
classical book on pattern recognition, Devroye, Gyorfi, and Lugosi (1996) ask
whether there exists a {monotone} Bayes-consistent algorithm. This question
remained open for over 25 years, until recently Pestov (2021) resolved it for
binary classification, using an intricate construction of a monotone
Bayes-consistent algorithm.
&lt;/p&gt;
&lt;p&gt;We derive a general result in multiclass classification, showing that every
learning algorithm A can be transformed to a monotone one with similar
performance. Further, the transformation is efficient and only uses a black-box
oracle access to A. This demonstrates that one can provably avoid non-monotonic
behaviour without compromising performance, thus answering questions asked by
Devroye et al (1996), Viering, Mey, and Loog (2019), Viering and Loog (2021),
and by Mhammedi (2021).
&lt;/p&gt;
&lt;p&gt;Our transformation readily implies monotone learners in a variety of
contexts: for example it extends Pestov&apos;s result to classification tasks with
an arbitrary number of labels. This is in contrast with Pestov&apos;s work which is
tailored to binary classification.
&lt;/p&gt;
&lt;p&gt;In addition, we provide uniform bounds on the error of the monotone
algorithm. This makes our transformation applicable in distribution-free
settings. For example, in PAC learning it implies that every learnable class
admits a monotone PAC learner. This resolves questions by Viering, Mey, and
Loog (2019); Viering and Loog (2021); Mhammedi (2021).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bousquet_O/0/1/0/all/0/1&quot;&gt;Olivier Bousquet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniely_A/0/1/0/all/0/1&quot;&gt;Amit Daniely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1&quot;&gt;Haim Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Shay Moran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1&quot;&gt;Uri Stemmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.05674">
<title>Particle Swarm Optimization based on Novelty Search. (arXiv:2203.05674v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2203.05674</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a Particle Swarm Optimization algorithm combined
with Novelty Search. Novelty Search finds novel place to search in the search
domain and then Particle Swarm Optimization rigorously searches that area for
global optimum solution. This method is never blocked in local optima because
it is controlled by Novelty Search which is objective free. For those functions
where there are many more local optima and second global optimum is far from
true optimum, the present method works successfully. The present algorithm
never stops until it searches entire search area. A series of experimental
trials prove the robustness and effectiveness of the present algorithm on
complex optimization test functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_M/0/1/0/all/0/1&quot;&gt;Mr.Rajesh Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_D/0/1/0/all/0/1&quot;&gt;Dr. Kumar S Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.07593">
<title>Distraction is All You Need for Fairness. (arXiv:2203.07593v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.07593</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias in training datasets must be managed for various groups in
classification tasks to ensure parity or equal treatment. With the recent
growth in artificial intelligence models and their expanding role in automated
decision-making, ensuring that these models are not biased is vital. There is
an abundance of evidence suggesting that these models could contain or even
amplify the bias present in the data on which they are trained, inherent to
their objective function and learning algorithms; Many researchers direct their
attention to this issue in different directions, namely, changing data to be
statistically independent, adversarial training for restricting the
capabilities of a particular competitor who aims to maximize parity, etc. These
methods result in information loss and do not provide a suitable balance
between accuracy and fairness or do not ensure limiting the biases in training.
To this end, we propose a powerful strategy for training deep learning models
called the Distraction module, which can be theoretically proven effective in
controlling bias from affecting the classification results. This method can be
utilized with different data types (e.g., Tabular, images, graphs, etc.). We
demonstrate the potency of the proposed method by testing it on UCI Adult and
Heritage Health datasets (tabular), POKEC-Z, POKEC-N and NBA datasets (graph),
and CelebA dataset (vision). Using state-of-the-art methods proposed in the
fairness literature for each dataset, we exhibit our model is superior to these
proposed methods in minimizing bias and maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdani_Jahromi_M/0/1/0/all/0/1&quot;&gt;Mehdi Yazdani-Jahromi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_A/0/1/0/all/0/1&quot;&gt;AmirArsalan Rajabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yalabadi_A/0/1/0/all/0/1&quot;&gt;Ali Khodabandeh Yalabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayebi_A/0/1/0/all/0/1&quot;&gt;Aida Tayebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garibay_O/0/1/0/all/0/1&quot;&gt;Ozlem Ozmen Garibay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07245">
<title>An Extractive-and-Abstractive Framework for Source Code Summarization. (arXiv:2206.07245v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07245</link>
<description rdf:parseType="Literal">&lt;p&gt;(Source) Code summarization aims to automatically generate summaries/comments
for a given code snippet in the form of natural language. Such summaries play a
key role in helping developers understand and maintain source code. Existing
code summarization techniques can be categorized into extractive methods and
abstractive methods. The extractive methods extract a subset of important
statements and keywords from the code snippet using retrieval techniques, and
generate a summary that preserves factual details in important statements and
keywords. However, such a subset may miss identifier or entity naming, and
consequently, the naturalness of generated summary is usually poor. The
abstractive methods can generate human-written-like summaries leveraging
encoder-decoder models from the neural machine translation domain. The
generated summaries however often miss important factual details.
&lt;/p&gt;
&lt;p&gt;To generate human-written-like summaries with preserved factual details, we
propose a novel extractive-and-abstractive framework. The extractive module in
the framework performs a task of extractive code summarization, which takes in
the code snippet and predicts important statements containing key factual
details. The abstractive module in the framework performs a task of abstractive
code summarization, which takes in the entire code snippet and important
statements in parallel and generates a succinct and human-written-like natural
language summary. We evaluate the effectiveness of our technique, called EACS,
by conducting extensive experiments on three datasets involving six programming
languages. Experimental results show that EACS significantly outperforms
state-of-the-art techniques in terms of all three widely used metrics,
including BLEU, METEOR, and ROUGH-L.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weisong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chunrong Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuchen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1&quot;&gt;Guanhong Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tingxu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yifei Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yudu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00638">
<title>Composable Text Controls in Latent Space with ODEs. (arXiv:2208.00638v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00638</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world text applications often involve composing a wide range of text
control operations, such as editing the text w.r.t. an attribute, manipulating
keywords and structure, and generating new text of desired properties. Prior
work typically learns/finetunes a language model (LM) to perform individual or
specific subsets of operations. Recent research has studied combining
operations in a plug-and-play manner, often with costly search or optimization
in the complex sequence space. This paper proposes a new efficient approach for
composable text operations in the compact latent space of text. The
low-dimensionality and differentiability of the text latent vector allow us to
develop an efficient sampler based on ordinary differential equations (ODEs)
given arbitrary plug-in operators (e.g., attribute classifiers). By connecting
pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we
then decode the sampled vectors into desired text sequences. The flexible
approach permits diverse control operators (sentiment, tense, formality,
keywords, etc.) acquired using any relevant data from different domains.
Experiments show that composing those operators within our approach manages to
generate or edit high-quality text, substantially improving over previous
methods in terms of generation quality and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guangyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zeyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Junwei Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaodong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09495">
<title>Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09495</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Topical, a novel deep neural network for repository level
embeddings. Existing methods, reliant on natural language documentation or
naive aggregation techniques, are outperformed by Topical&apos;s utilization of an
attention mechanism. This mechanism generates repository-level representations
from source code, full dependency graphs, and script level textual data.
Trained on publicly accessible GitHub repositories, Topical surpasses multiple
baselines in tasks such as repository auto-tagging, highlighting the attention
mechanism&apos;s efficacy over traditional aggregation methods. Topical also
demonstrates scalability and efficiency, making it a valuable contribution to
repository-level representation computation. For further research, the
accompanying tools, code, and training dataset are provided at:
https://github.com/jpmorganchase/topical.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lherondelle_A/0/1/0/all/0/1&quot;&gt;Agathe Lherondelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babbar_V/0/1/0/all/0/1&quot;&gt;Varun Babbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satsangi_Y/0/1/0/all/0/1&quot;&gt;Yash Satsangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silavong_F/0/1/0/all/0/1&quot;&gt;Fran Silavong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eloul_S/0/1/0/all/0/1&quot;&gt;Shaltiel Eloul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Sean Moran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02528">
<title>Rethinking Symmetric Matrix Factorization: A More General and Better Clustering Perspective. (arXiv:2209.02528v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02528</link>
<description rdf:parseType="Literal">&lt;p&gt;Nonnegative matrix factorization (NMF) is widely used for clustering with
strong interpretability. Among general NMF problems, symmetric NMF is a special
one that plays an important role in graph clustering where each element
measures the similarity between data points. Most existing symmetric NMF
algorithms require factor matrices to be nonnegative, and only focus on
minimizing the gap between similarity matrix and its approximation for
clustering, without giving a consideration to other potential regularization
terms which can yield better clustering. In this paper, we explore factorizing
a symmetric matrix that does not have to be nonnegative, presenting an
efficient factorization algorithm with a regularization term to boost the
clustering performance. Moreover, a more general framework is proposed to solve
symmetric matrix factorization problems with different constraints on the
factor matrices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07484">
<title>Mutual Information Regularized Offline Reinforcement Learning. (arXiv:2210.07484v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07484</link>
<description rdf:parseType="Literal">&lt;p&gt;The major challenge of offline RL is the distribution shift that appears when
out-of-distribution actions are queried, which makes the policy improvement
direction biased by extrapolation errors. Most existing methods address this
problem by penalizing the policy or value for deviating from the behavior
policy during policy improvement or evaluation. In this work, we propose a
novel MISA framework to approach offline RL from the perspective of Mutual
Information between States and Actions in the dataset by directly constraining
the policy improvement direction. MISA constructs lower bounds of mutual
information parameterized by the policy and Q-values. We show that optimizing
this lower bound is equivalent to maximizing the likelihood of a one-step
improved policy on the offline dataset. Hence, we constrain the policy
improvement direction to lie in the data manifold. The resulting algorithm
simultaneously augments the policy evaluation and improvement by adding mutual
information regularizations. MISA is a general framework that unifies
conservative Q-learning (CQL) and behavior regularization methods (e.g.,
TD3+BC) as special cases. We introduce 3 different variants of MISA, and
empirically demonstrate that tighter mutual information lower bound gives
better offline RL performance. In addition, our extensive experiments show MISA
significantly outperforms a wide range of baselines on various tasks of the
D4RL benchmark,e.g., achieving 742.9 total points on gym-locomotion tasks. Our
code is available at https://github.com/sail-sg/MISA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13111">
<title>Federated Learning and Meta Learning: Approaches, Applications, and Directions. (arXiv:2210.13111v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13111</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, significant advancements have been made in the field
of machine learning (ML) to address resource management, interference
management, autonomy, and decision-making in wireless networks. Traditional ML
approaches rely on centralized methods, where data is collected at a central
server for training. However, this approach poses a challenge in terms of
preserving the data privacy of devices. To address this issue, federated
learning (FL) has emerged as an effective solution that allows edge devices to
collaboratively train ML models without compromising data privacy. In FL, local
datasets are not shared, and the focus is on learning a global model for a
specific task involving all devices. However, FL has limitations when it comes
to adapting the model to devices with different data distributions. In such
cases, meta learning is considered, as it enables the adaptation of learning
models to different data distributions using only a few data samples. In this
tutorial, we present a comprehensive review of FL, meta learning, and federated
meta learning (FedMeta). Unlike other tutorial papers, our objective is to
explore how FL, meta learning, and FedMeta methodologies can be designed,
optimized, and evolved, and their applications over wireless networks. We also
analyze the relationships among these learning algorithms and examine their
advantages and disadvantages in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaonan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yansha Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nallanathan_A/0/1/0/all/0/1&quot;&gt;Arumugam Nallanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1&quot;&gt;Mehdi Bennis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13708">
<title>MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13708</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant challenge facing researchers in the area of multi-agent
reinforcement learning (MARL) pertains to the identification of a library that
can offer fast and compatible development for multi-agent tasks and algorithm
combinations, while obviating the need to consider compatibility issues. In
this paper, we present MARLlib, a library designed to address the
aforementioned challenge by leveraging three key mechanisms: 1) a standardized
multi-agent environment wrapper, 2) an agent-level algorithm implementation,
and 3) a flexible policy mapping strategy. By utilizing these mechanisms,
MARLlib can effectively disentangle the intertwined nature of the multi-agent
task and the learning process of the algorithm, with the ability to
automatically alter the training strategy based on the current task&apos;s
attributes. The MARLlib library&apos;s source code is publicly accessible on GitHub:
\url{https://github.com/Replicable-MARL/MARLlib}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Siyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Minquan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weixun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06627">
<title>Dissociating language and thought in large language models. (arXiv:2301.06627v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06627</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have come closest among all models to date to
mastering human language, yet opinions about their linguistic and cognitive
capabilities remain split. Here, we evaluate LLMs using a distinction between
formal linguistic competence--knowledge of linguistic rules and patterns--and
functional linguistic competence--understanding and using language in the
world. We ground this distinction in human neuroscience, showing that formal
and functional competence rely on different neural mechanisms. Although LLMs
are surprisingly good at formal competence, their performance on functional
competence tasks remains spotty and often requires specialized fine-tuning
and/or coupling with external modules. In short, LLMs are good models of
language but incomplete models of human thought.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1&quot;&gt;Kyle Mahowald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanova_A/0/1/0/all/0/1&quot;&gt;Anna A. Ivanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blank_I/0/1/0/all/0/1&quot;&gt;Idan A. Blank&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanwisher_N/0/1/0/all/0/1&quot;&gt;Nancy Kanwisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08110">
<title>AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08110</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative transformer models have become increasingly complex, with large
numbers of parameters and the ability to process multiple input modalities.
Current methods for explaining their predictions are resource-intensive. Most
crucially, they require prohibitively large amounts of extra memory, since they
rely on backpropagation which allocates almost twice as much GPU memory as the
forward pass. This makes it difficult, if not impossible, to use them in
production. We present AtMan that provides explanations of generative
transformer models at almost no extra cost. Specifically, AtMan is a
modality-agnostic perturbation method that manipulates the attention mechanisms
of transformers to produce relevance maps for the input with respect to the
output prediction. Instead of using backpropagation, AtMan applies a
parallelizable token-based search method based on cosine similarity
neighborhood in the embedding space. Our exhaustive experiments on text and
image-text benchmarks demonstrate that AtMan outperforms current
state-of-the-art gradient-based methods on several metrics while being
computationally efficient. As such, AtMan is suitable for use in large model
inference deployments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Deiseroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1&quot;&gt;Mayukh Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1&quot;&gt;Samuel Weinbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10134">
<title>Bipartite Graph Diffusion Model for Human Interaction Generation. (arXiv:2301.10134v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10134</link>
<description rdf:parseType="Literal">&lt;p&gt;The generation of natural human motion interactions is a hot topic in
computer vision and computer animation. It is a challenging task due to the
diversity of possible human motion interactions. Diffusion models, which have
already shown remarkable generative capabilities in other domains, are a good
candidate for this task. In this paper, we introduce a novel bipartite graph
diffusion method (BiGraphDiff) to generate human motion interactions between
two persons. Specifically, bipartite node sets are constructed to model the
inherent geometric constraints between skeleton nodes during interactions. The
interaction graph diffusion model is transformer-based, combining some
state-of-the-art motion methods. We show that the proposed achieves new
state-of-the-art results on leading benchmarks for the human interaction
generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1&quot;&gt;Baptiste Chopin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1&quot;&gt;Mohamed Daoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03480">
<title>Can an Embodied Agent Find Your &quot;Cat-shaped Mug&quot;? LLM-Guided Exploration for Zero-Shot Object Navigation. (arXiv:2303.03480v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03480</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LGX (Language-guided Exploration), a novel algorithm for
Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied
agent navigates to a uniquely described target object in a previously unseen
environment. Our approach makes use of Large Language Models (LLMs) for this
task by leveraging the LLM&apos;s commonsense reasoning capabilities for making
sequential navigational decisions. Simultaneously, we perform generalized
target object detection using a pre-trained Vision-Language grounding model. We
achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a
success rate (SR) improvement of over 27% over the current baseline of the
OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for
robot navigation and present an analysis of various prompting strategies
affecting the model output. Finally, we showcase the benefits of our approach
via \textit{real-world} experiments that indicate the superior performance of
LGX in detecting and navigating to visually unique objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorbala_V/0/1/0/all/0/1&quot;&gt;Vishnu Sashank Dorbala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullen_J/0/1/0/all/0/1&quot;&gt;James F. Mullen Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07104">
<title>xASTNN: Improved Code Representations for Industrial Practice. (arXiv:2303.07104v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07104</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of deep learning techniques in software engineering becomes
increasingly popular. One key problem is developing high-quality and
easy-to-use source code representations for code-related tasks. The research
community has acquired impressive results in recent years. However, due to the
deployment difficulties and performance bottlenecks, seldom these approaches
are applied to the industry. In this paper, we present xASTNN, an eXtreme
Abstract Syntax Tree (AST)-based Neural Network for source code representation,
aiming to push this technique to industrial practice. The proposed xASTNN has
three advantages. First, xASTNN is completely based on widely-used ASTs and
does not require complicated data pre-processing, making it applicable to
various programming languages and practical scenarios. Second, three
closely-related designs are proposed to guarantee the effectiveness of xASTNN,
including statement subtree sequence for code naturalness, gated recursive unit
for syntactical information, and gated recurrent unit for sequential
information. Third, a dynamic batching algorithm is introduced to significantly
reduce the time complexity of xASTNN. Two code comprehension downstream tasks,
code classification and code clone detection, are adopted for evaluation. The
results demonstrate that our xASTNN can improve the state-of-the-art while
being faster than the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xibin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01246">
<title>Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01246</link>
<description rdf:parseType="Literal">&lt;p&gt;Can safety analysis make use of Large Language Models (LLMs)? A case study
explores Systems Theoretic Process Analysis (STPA) applied to Automatic
Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems
using ChatGPT. We investigate how collaboration schemes, input semantic
complexity, and prompt guidelines influence STPA results. Comparative results
show that using ChatGPT without human intervention may be inadequate due to
reliability related issues, but with careful design, it may outperform human
experts. No statistically significant differences are found when varying the
input semantic complexity or using common prompt guidelines, which suggests the
necessity for developing domain-specific prompt engineering. We also highlight
future challenges, including concerns about LLM trustworthiness and the
necessity for standardisation and regulation in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khastgir_S/0/1/0/all/0/1&quot;&gt;Siddartha Khastgir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06813">
<title>Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06813</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection aims to identify test examples that do
not belong to the training distribution and are thus unlikely to be predicted
reliably. Despite a plethora of existing works, most of them focused only on
the scenario where OOD examples come from semantic shift (e.g., unseen
categories), ignoring other possible causes (e.g., covariate shift). In this
paper, we present a novel, unifying framework to study OOD detection in a
broader scope. Instead of detecting OOD examples from a particular cause, we
propose to detect examples that a deployed machine learning model (e.g., an
image classifier) is unable to predict correctly. That is, whether a test
example should be detected and rejected or not is ``model-specific&apos;&apos;. We show
that this framework unifies the detection of OOD examples caused by semantic
shift and covariate shift, and closely addresses the concern of applying a
machine learning model to uncontrolled environments. We provide an extensive
analysis that involves a variety of models (e.g., different architectures and
training strategies), sources of OOD examples, and OOD detection approaches,
and reveal several insights into improving and understanding OOD detection in
uncontrolled environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averly_R/0/1/0/all/0/1&quot;&gt;Reza Averly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Chao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08897">
<title>An adaptive safety layer with hard constraints for safe reinforcement learning in multi-energy management systems. (arXiv:2304.08897v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08897</link>
<description rdf:parseType="Literal">&lt;p&gt;Safe reinforcement learning (RL) with hard constraint guarantees is a
promising optimal control direction for multi-energy management systems. It
only requires the environment-specific constraint functions itself a priori and
not a complete model. The project-specific upfront and ongoing engineering
efforts are therefore still reduced, better representations of the underlying
system dynamics can still be learnt, and modelling bias is kept to a minimum.
However, even the constraint functions alone are not always trivial to
accurately provide in advance, leading to potentially unsafe behaviour. In this
paper, we present two novel advancements: (I) combining the OptLayer and
SafeFallback method, named OptLayerPolicy, to increase the initial utility
while keeping a high sample efficiency and the possibility to formulate
equality constraints. (II) introducing self-improving hard constraints, to
increase the accuracy of the constraint functions as more and new data becomes
available so that better policies can be learnt. Both advancements keep the
constraint formulation decoupled from the RL formulation, so new (presumably
better) RL algorithms can act as drop-in replacements. We have shown that, in a
simulated multi-energy system case study, the initial utility is increased to
92.4% (OptLayerPolicy) compared to 86.1% (OptLayer) and that the policy after
training is increased to 104.9% (GreyOptLayerPolicy) compared to 103.4%
(OptLayer) - all relative to a vanilla RL benchmark. Although introducing
surrogate functions into the optimisation problem requires special attention,
we conclude that the newly presented GreyOptLayerPolicy method is the most
advantageous.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ceusters_G/0/1/0/all/0/1&quot;&gt;Glenn Ceusters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Putratama_M/0/1/0/all/0/1&quot;&gt;Muhammad Andy Putratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Franke_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfc;diger Franke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nowe_A/0/1/0/all/0/1&quot;&gt;Ann Now&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Messagie_M/0/1/0/all/0/1&quot;&gt;Maarten Messagie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09823">
<title>The Future of ChatGPT-enabled Labor Market: A Preliminary Study in China. (arXiv:2304.09823v4 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09823</link>
<description rdf:parseType="Literal">&lt;p&gt;As a phenomenal large language model, ChatGPT has achieved unparalleled
success in various real-world tasks and increasingly plays an important role in
our daily lives and work. However, extensive concerns are also raised about the
potential ethical issues, especially about whether ChatGPT-like artificial
general intelligence (AGI) will replace human jobs. To this end, in this paper,
we introduce a preliminary data-driven study on the future of ChatGPT-enabled
labor market from the view of Human-AI Symbiosis instead of Human-AI
Confrontation. To be specific, we first conduct an in-depth analysis of
large-scale job posting data in BOSS Zhipin, the largest online recruitment
platform in China. The results indicate that about 28% of occupations in the
current labor market require ChatGPT-related skills. Furthermore, based on a
large-scale occupation-centered knowledge graph, we develop a semantic
information enhanced collaborative filtering algorithm to predict the future
occupation-skill relations in the labor market. As a result, we find that
additional 45% occupations in the future will require ChatGPT-related skills.
In particular, industries related to technology, products, and operations are
expected to have higher proficiency requirements for ChatGPT-related skills,
while the manufacturing, services, education, and health science related
industries will have lower requirements for ChatGPT-related skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Meng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hengshu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11300">
<title>MAWSEO: Adversarial Wiki Search Poisoning for Illicit Online Promotion. (arXiv:2304.11300v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11300</link>
<description rdf:parseType="Literal">&lt;p&gt;As a prominent instance of vandalism edits, Wiki search poisoning for illicit
promotion is a cybercrime in which the adversary aims at editing Wiki articles
to promote illicit businesses through Wiki search results of relevant queries.
In this paper, we report a study that, for the first time, shows that such
stealthy blackhat SEO on Wiki can be automated. Our technique, called MAWSEO,
employs adversarial revisions to achieve real-world cybercriminal objectives,
including rank boosting, vandalism detection evasion, topic relevancy, semantic
consistency, user awareness (but not alarming) of promotional content, etc. Our
evaluation and user study demonstrate that MAWSEO is capable of effectively and
efficiently generating adversarial vandalism edits, which can bypass
state-of-the-art built-in Wiki vandalism detectors, and also get promotional
content through to Wiki users without triggering their alarms. In addition, we
investigated potential defense, including coherence based detection and
adversarial training of vandalism detection, against our attack in the Wiki
ecosystem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zilong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1&quot;&gt;Xiaojing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;XiaoFeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12958">
<title>A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12958</link>
<description rdf:parseType="Literal">&lt;p&gt;Explaining the behaviour of intelligent agents learned by reinforcement
learning (RL) to humans is challenging yet crucial due to their
incomprehensible proprioceptive states, variational intermediate goals, and
resultant unpredictability. Moreover, one-step explanations for RL agents can
be ambiguous as they fail to account for the agent&apos;s future behaviour at each
transition, adding to the complexity of explaining robot actions. By leveraging
abstracted actions that map to task-specific primitives, we avoid explanations
on the movement level. To further improve the transparency and explainability
of robotic systems, we propose an explainable Q-Map learning framework that
combines reward decomposition (RD) with abstracted action spaces, allowing for
non-ambiguous and high-level explanations based on object properties in the
task. We demonstrate the effectiveness of our framework through quantitative
and qualitative analysis of two robotic scenarios, showcasing visual and
textual explanations, from output artefacts of RD explanations, that are easy
for humans to comprehend. Additionally, we demonstrate the versatility of
integrating these artefacts with large language models (LLMs) for reasoning and
interactive querying.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wenhao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xufeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magg_S/0/1/0/all/0/1&quot;&gt;Sven Magg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gromniak_M/0/1/0/all/0/1&quot;&gt;Martin Gromniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengdi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02305">
<title>Calibrated Explanations: with Uncertainty Information and Counterfactuals. (arXiv:2305.02305v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02305</link>
<description rdf:parseType="Literal">&lt;p&gt;While local explanations for AI models can offer insights into individual
predictions, such as feature importance, they are plagued by issues like
instability. The unreliability of feature weights, often skewed due to poorly
calibrated ML models, deepens these challenges. Moreover, the critical aspect
of feature importance uncertainty remains mostly unaddressed in Explainable AI
(XAI). The novel feature importance explanation method presented in this paper,
called Calibrated Explanations (CE), is designed to tackle these issues
head-on. Built on the foundation of Venn-Abers, CE not only calibrates the
underlying model but also delivers reliable feature importance explanations
with an exact definition of the feature weights. CE goes beyond conventional
solutions by addressing output uncertainty. It accomplishes this by providing
uncertainty quantification for both feature weights and the model&apos;s probability
estimates. Additionally, CE is model-agnostic, featuring easily comprehensible
conditional rules and the ability to generate counterfactual explanations with
embedded uncertainty quantification. Results from an evaluation with 25
benchmark datasets underscore the efficacy of CE, making it stand as a fast,
reliable, stable, and robust solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lofstrom_H/0/1/0/all/0/1&quot;&gt;Helena Lofstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lofstrom_T/0/1/0/all/0/1&quot;&gt;Tuwe Lofstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_U/0/1/0/all/0/1&quot;&gt;Ulf Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonstrod_C/0/1/0/all/0/1&quot;&gt;Cecilia Sonstrod&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03017">
<title>Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03017</link>
<description rdf:parseType="Literal">&lt;p&gt;Our research investigates the recommendation of code examples to aid software
developers, a practice that saves developers significant time by providing
ready-to-use code snippets. The focus of our study is Stack Overflow, a
commonly used resource for coding discussions and solutions, particularly in
the context of the Java programming language. We applied BERT, a powerful Large
Language Model (LLM) that enables us to transform code examples into numerical
vectors by extracting their semantic information. Once these numerical
representations are prepared, we identify Approximate Nearest Neighbors (ANN)
using Locality-Sensitive Hashing (LSH). Our research employed two variants of
LSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared
these two approaches across four parameters: HitRate, Mean Reciprocal Rank
(MRR), Average Execution Time, and Relevance. Our study revealed that the
Query-Aware (QA) approach showed superior performance over the Random
Hyperplane-based (RH) method. Specifically, it exhibited a notable improvement
of 20\% to 35\% in HitRate for query pairs compared to the RH approach.
Furthermore, the QA approach proved significantly more time-efficient, with its
speed in creating hashing tables and assigning data samples to buckets being at
least four times faster. It can return code examples within milliseconds,
whereas the RH approach typically requires several seconds to recommend code
examples. Due to the superior performance of the QA approach, we tested it
against PostFinder and FaCoY, the state-of-the-art baselines. Our QA method
showed comparable efficiency proving its potential for effective code
recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1&quot;&gt;Sajjad Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1&quot;&gt;AmirHossein Naghshzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerrouj_L/0/1/0/all/0/1&quot;&gt;Latifa Guerrouj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03942">
<title>HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03942</link>
<description rdf:parseType="Literal">&lt;p&gt;Manipulating objects without grasping them is an essential component of human
dexterity, referred to as non-prehensile manipulation. Non-prehensile
manipulation may enable more complex interactions with the objects, but also
presents challenges in reasoning about gripper-object interactions. In this
work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a
reinforcement learning approach for 6D non-prehensile manipulation of objects
using point cloud observations. HACMan proposes a temporally-abstracted and
spatially-grounded object-centric action representation that consists of
selecting a contact location from the object point cloud and a set of motion
parameters describing how the robot will move after making contact. We modify
an existing off-policy RL algorithm to learn in this hybrid discrete-continuous
action representation. We evaluate HACMan on a 6D object pose alignment task in
both simulation and in the real world. On the hardest version of our task, with
randomized initial poses, randomized 6D goals, and diverse object categories,
our policy demonstrates strong generalization to unseen object categories
without a performance drop, achieving an 89% success rate on unseen objects in
simulation and 50% success rate with zero-shot transfer in the real world.
Compared to alternative action representations, HACMan achieves a success rate
more than three times higher than the best baseline. With zero-shot sim2real
transfer, our policy can successfully manipulate unseen objects in the real
world for challenging non-planar goals, using dynamic and contact-rich
non-prehensile skills. Videos can be found on the project website:
https://hacman-2023.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bowen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05803">
<title>Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05803</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised semantic segmentation (WSSS) aims to bypass the need for
laborious pixel-level annotation by using only image-level annotation. Most
existing methods rely on Class Activation Maps (CAM) to derive pixel-level
pseudo-labels and use them to train a fully supervised semantic segmentation
model. Although these pseudo-labels are class-aware, indicating the coarse
regions for particular classes, they are not object-aware and fail to delineate
accurate object boundaries. To address this, we introduce a simple yet
effective method harnessing the Segment Anything Model (SAM), a class-agnostic
foundation model capable of producing fine-grained instance masks of objects,
parts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM
masks, resulting in high-quality pseudo-labels that are both class-aware and
object-aware. Our approach is highly versatile and can be easily integrated
into existing WSSS methods without any modification. Despite its simplicity,
our approach shows consistent gain over the state-of-the-art WSSS methods on
both PASCAL VOC and MS-COCO datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianle Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1&quot;&gt;Zheda Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wei-lun Chao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06349">
<title>RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06349</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on transformer-based language models show that they can answer
questions by reasoning over knowledge provided as part of the context (i.e.,
in-context reasoning). However, since the available knowledge is often not
filtered for a particular question, in-context reasoning can be sensitive to
distractor facts, additional content that is irrelevant to a question but that
may be relevant for a different question (i.e., not necessarily random noise).
In these situations, the model fails to distinguish the knowledge that is
necessary to answer the question, leading to spurious reasoning and degraded
performance. This reasoning failure contrasts with the model&apos;s apparent ability
to distinguish its contextual knowledge from all the knowledge it has memorized
during pre-training. Following this observation, we propose teaching the model
to reason more robustly by folding the provided contextual knowledge into the
model&apos;s parameters before presenting it with a question. Our method, RECKONING,
is a bi-level learning algorithm that teaches language models to reason by
updating their parametric knowledge through back-propagation, allowing them to
then answer questions using the updated parameters. During training, the inner
loop rapidly adapts a copy of the model weights to encode contextual knowledge
into its parameters. In the outer loop, the model learns to use the updated
weights to reproduce and answer reasoning questions about the memorized
knowledge. Our experiments on two multi-hop reasoning datasets show that
RECKONING&apos;s performance improves over the in-context reasoning baseline (by up
to 4.5%). We also find that compared to in-context reasoning, RECKONING
generalizes better to longer reasoning chains unseen during training, is more
robust to distractors in the context, and is more computationally efficient
when multiple questions are asked about the same knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_G/0/1/0/all/0/1&quot;&gt;Gail Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_E/0/1/0/all/0/1&quot;&gt;Eric Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1&quot;&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1&quot;&gt;Antoine Bosselut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12473">
<title>Continually Improving Extractive QA via Human Feedback. (arXiv:2305.12473v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12473</link>
<description rdf:parseType="Literal">&lt;p&gt;We study continually improving an extractive question answering (QA) system
via human user feedback. We design and deploy an iterative approach, where
information-seeking users ask questions, receive model-predicted answers, and
provide feedback. We conduct experiments involving thousands of user
interactions under diverse setups to broaden the understanding of learning from
feedback over time. Our experiments show effective improvement from user
feedback of extractive QA models over time across different data regimes,
including significant potential for domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Ge Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hung-Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1&quot;&gt;Yoav Artzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Eunsol Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14263">
<title>LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages. (arXiv:2305.14263v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14263</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowing the language of an input text/audio is a necessary first step for
using almost every NLP tool such as taggers, parsers, or translation systems.
Language identification is a well-studied problem, sometimes even considered
solved; in reality, due to lack of data and computational challenges, current
systems cannot accurately identify most of the world&apos;s 7000 languages. To
tackle this bottleneck, we first compile a corpus, MCS-350, of 50K multilingual
and parallel children&apos;s stories in 350+ languages. MCS-350 can serve as a
benchmark for language identification of short texts and for 1400+ new
translation directions in low-resource Indian and African languages. Second, we
propose a novel misprediction-resolution hierarchical model, LIMIt, for
language identification that reduces error by 55% (from 0.71 to 0.32) on our
compiled children&apos;s stories dataset and by 40% (from 0.23 to 0.14) on the
FLORES-200 benchmark. Our method can expand language identification coverage
into low-resource languages by relying solely on systemic misprediction
patterns, bypassing the need to retrain large models from scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1&quot;&gt;Milind Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md Mahfuz Ibn Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1&quot;&gt;Antonios Anastasopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15093">
<title>C-STS: Conditional Semantic Textual Similarity. (arXiv:2305.15093v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15093</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic textual similarity (STS), a cornerstone task in NLP, measures the
degree of similarity between a pair of sentences, and has broad application in
fields such as information retrieval and natural language understanding.
However, sentence similarity can be inherently ambiguous, depending on the
specific aspect of interest. We resolve this ambiguity by proposing a novel
task called Conditional STS (C-STS) which measures sentences&apos; similarity
conditioned on an feature described in natural language (hereon, condition). As
an example, the similarity between the sentences &quot;The NBA player shoots a
three-pointer.&quot; and &quot;A man throws a tennis ball into the air to serve.&quot; is
higher for the condition &quot;The motion of the ball&quot; (both upward) and lower for
&quot;The size of the ball&quot; (one large and one small). C-STS&apos;s advantages are
two-fold: (1) it reduces the subjectivity and ambiguity of STS and (2) enables
fine-grained language model evaluation through diverse natural language
conditions. We put several state-of-the-art models to the test, and even those
performing well on STS (e.g. SimCSE, Flan-T5, and GPT-4) find C-STS
challenging; all with Spearman correlation scores below 50. To encourage a more
comprehensive evaluation of semantic similarity and natural language
understanding, we make nearly 19K C-STS examples and code available for others
to train and test their models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Ameet Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_C/0/1/0/all/0/1&quot;&gt;Carlos E. Jimenez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Howard Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murahari_V/0/1/0/all/0/1&quot;&gt;Vishvak Murahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graf_V/0/1/0/all/0/1&quot;&gt;Victoria Graf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurohit_T/0/1/0/all/0/1&quot;&gt;Tanmay Rajpurohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1&quot;&gt;Ashwin Kalyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Danqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1&quot;&gt;Karthik Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15269">
<title>Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15269</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the intractably large size of the space of proofs, any model that is
capable of general deductive reasoning must generalize to proofs of greater
complexity. Recent studies have shown that large language models (LLMs) possess
some abstract deductive reasoning ability given chain-of-thought prompts.
However, they have primarily been tested on proofs using modus ponens or of a
specific size, and from the same distribution as the in-context examples. To
measure the general deductive reasoning ability of LLMs, we test on a broad set
of deduction rules and measure their ability to generalize to more complex
proofs from simpler demonstrations from multiple angles: depth-, width-, and
compositional generalization. To facilitate systematic exploration, we
construct a new synthetic and programmable reasoning dataset that enables
control over deduction rules and proof complexity. Our experiments on four LLMs
of various sizes and training objectives show that they are able to generalize
to compositional proofs. However, they have difficulty generalizing to longer
proofs, and they require explicit demonstrations to produce hypothetical
subproofs, specifically in proof by cases and proof by contradiction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1&quot;&gt;Abulhair Saparov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1&quot;&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1&quot;&gt;Vishakh Padmakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nitish Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemi_S/0/1/0/all/0/1&quot;&gt;Seyed Mehran Kazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Najoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15771">
<title>On the Planning Abilities of Large Language Models : A Critical Investigation. (arXiv:2305.15771v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15771</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating
plans autonomously in commonsense planning tasks and (2) the potential of LLMs
in LLM-Modulo settings where they act as a source of heuristic guidance for
external planners and verifiers. We conduct a systematic study by generating a
suite of instances on domains similar to the ones employed in the International
Planning Competition and evaluate LLMs in two distinct modes: autonomous and
heuristic. Our findings reveal that LLMs&apos; ability to generate executable plans
autonomously is rather limited, with the best model (GPT-4) having an average
success rate of ~12% across the domains. However, the results in the LLM-Modulo
setting show more promise. In the LLM-Modulo setting, we demonstrate that
LLM-generated plans can improve the search process for underlying sound
planners and additionally show that external verifiers can help provide
feedback on the generated plans and back-prompt the LLM for better plan
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1&quot;&gt;Karthik Valmeekam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1&quot;&gt;Matthew Marquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1&quot;&gt;Sarath Sreedharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16358">
<title>Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16358</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a differentiable clustering method based on stochastic
perturbations of minimum-weight spanning forests. This allows us to include
clustering in end-to-end trainable pipelines, with efficient gradients. We show
that our method performs well even in difficult settings, such as data sets
with high noise and challenging geometries. We also formulate an ad hoc loss to
efficiently learn from partial clustering data using this operation. We
demonstrate its performance on several data sets for supervised and
semi-supervised tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stewart_L/0/1/0/all/0/1&quot;&gt;Lawrence Stewart&lt;/a&gt; (DI-ENS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis S Bach&lt;/a&gt; (DI-ENS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_F/0/1/0/all/0/1&quot;&gt;Felipe Llinares L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthet_Q/0/1/0/all/0/1&quot;&gt;Quentin Berthet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18213">
<title>Gaussian Process Probes (GPP) for Uncertainty-Aware Probing. (arXiv:2305.18213v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18213</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding which concepts models can and cannot represent has been
fundamental to many tasks: from effective and responsible use of models to
detecting out of distribution data. We introduce Gaussian process probes (GPP),
a unified and simple framework for probing and measuring uncertainty about
concepts represented by models. As a Bayesian extension of linear probing
methods, GPP asks what kind of distribution over classifiers (of concepts) is
induced by the model. This distribution can be used to measure both what the
model represents and how confident the probe is about what the model
represents. GPP can be applied to any pre-trained model with vector
representations of inputs (e.g., activations). It does not require access to
training data, gradients, or the architecture. We validate GPP on datasets
containing both synthetic and real images. Our experiments show it can (1)
probe a model&apos;s representations of concepts even with a very small number of
examples, (2) accurately measure both epistemic uncertainty (how confident the
probe is) and aleatory uncertainty (how fuzzy the concepts are to the model),
and (3) detect out of distribution data using those uncertainty measures as
well as classic methods do. By using Gaussian processes to expand what probing
can offer, GPP provides a data-efficient, versatile and uncertainty-aware tool
for understanding and evaluating the capabilities of machine learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1&quot;&gt;Alexander Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1&quot;&gt;Jason Baldridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01001">
<title>DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01001</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrical load forecasting plays a crucial role in decision-making for power
systems, including unit commitment and economic dispatch. The integration of
renewable energy sources and the occurrence of external events, such as the
COVID-19 pandemic, have rapidly increased uncertainties in load forecasting.
The uncertainties in load forecasting can be divided into two types: epistemic
uncertainty and aleatoric uncertainty. Separating these types of uncertainties
can help decision-makers better understand where and to what extent the
uncertainty is, thereby enhancing their confidence in the following
decision-making. This paper proposes a diffusion-based Seq2Seq structure to
estimate epistemic uncertainty and employs the robust additive Cauchy
distribution to estimate aleatoric uncertainty. Our method not only ensures the
accuracy of load forecasting but also demonstrates the ability to separate the
two types of uncertainties and be applicable to different levels of loads. The
relevant code can be found at
\url{https://anonymous.4open.science/r/DiffLoad-4714/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhixian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1&quot;&gt;Qingsong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Liang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02451">
<title>For SALE: State-Action Representation Learning for Deep Reinforcement Learning. (arXiv:2306.02451v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02451</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of reinforcement learning (RL), representation learning is a
proven tool for complex image-based tasks, but is often overlooked for
environments with low-level states, such as physical control problems. This
paper introduces SALE, a novel approach for learning embeddings that model the
nuanced interaction between state and action, enabling effective representation
learning from low-level states. We extensively study the design space of these
embeddings and highlight important design considerations. We integrate SALE and
an adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which
significantly outperforms existing continuous control algorithms. On OpenAI gym
benchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over
TD3 at 300k and 5M time steps, respectively, and works in both the online and
offline settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1&quot;&gt;Scott Fujimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1&quot;&gt;Wei-Di Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1&quot;&gt;Edward J. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shixiang Shane Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1&quot;&gt;David Meger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03929">
<title>Finding Counterfactually Optimal Action Sequences in Continuous State Spaces. (arXiv:2306.03929v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03929</link>
<description rdf:parseType="Literal">&lt;p&gt;Whenever a clinician reflects on the efficacy of a sequence of treatment
decisions for a patient, they may try to identify critical time steps where,
had they made different decisions, the patient&apos;s health would have improved.
While recent methods at the intersection of causal inference and reinforcement
learning promise to aid human experts, as the clinician above, to
retrospectively analyze sequential decision making processes, they have focused
on environments with finitely many discrete states. However, in many practical
applications, the state of the environment is inherently continuous in nature.
In this paper, we aim to fill this gap. We start by formally characterizing a
sequence of discrete actions and continuous states using finite horizon Markov
decision processes and a broad class of bijective structural causal models.
Building upon this characterization, we formalize the problem of finding
counterfactually optimal action sequences and show that, in general, we cannot
expect to solve it in polynomial time. Then, we develop a search method based
on the $A^*$ algorithm that, under a natural form of Lipschitz continuity of
the environment&apos;s dynamics, is guaranteed to return the optimal solution to the
problem. Experiments on real clinical data show that our method is very
efficient in practice, and it has the potential to offer interesting insights
for sequential decision making tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsirtsis_S/0/1/0/all/0/1&quot;&gt;Stratis Tsirtsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1&quot;&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03937">
<title>Guiding The Last Layer in Federated Learning with Pre-Trained Models. (arXiv:2306.03937v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03937</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is an emerging paradigm that allows a model to be
trained across a number of participants without sharing data. Recent works have
begun to consider the effects of using pre-trained models as an initialization
point for existing FL algorithms; however, these approaches ignore the vast
body of efficient transfer learning literature from the centralized learning
setting. Here we revisit the problem of FL from a pre-trained model considered
in prior work and expand it to a set of computer vision transfer learning
problems. We first observe that simply fitting a linear classification head can
be efficient and effective in many cases. We then show that in the FL setting,
fitting a classifier using the Nearest Class Means (NCM) can be done exactly
and orders of magnitude more efficiently than existing proposals, while
obtaining strong performance. Finally, we demonstrate that using a two-phase
approach of obtaining the classifier and then fine-tuning the model can yield
rapid convergence and improved generalization in the federated setting. We
demonstrate the potential our method has to reduce communication and compute
costs while achieving better model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legate_G/0/1/0/all/0/1&quot;&gt;Gwen Legate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernier_N/0/1/0/all/0/1&quot;&gt;Nicolas Bernier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1&quot;&gt;Lucas Caccia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1&quot;&gt;Edouard Oyallon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1&quot;&gt;Eugene Belilovsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05720">
<title>Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model. (arXiv:2306.05720v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05720</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent diffusion models (LDMs) exhibit an impressive ability to produce
realistic images, yet the inner workings of these models remain mysterious.
Even when trained purely on images without explicit depth information, they
typically output coherent pictures of 3D scenes. In this work, we investigate a
basic interpretability question: does an LDM create and use an internal
representation of simple scene geometry? Using linear probes, we find evidence
that the internal activations of the LDM encode linear representations of both
3D depth data and a salient-object / background distinction. These
representations appear surprisingly early in the denoising process$-$well
before a human can easily make sense of the noisy images. Intervention
experiments further indicate these representations play a causal role in image
synthesis, and may be used for simple high-level editing of an LDM&apos;s output.
Project page: https://yc015.github.io/scene-representation-diffusion-model/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yida Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1&quot;&gt;Fernanda Vi&amp;#xe9;gas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06543">
<title>MANER: Multi-Agent Neural Rearrangement Planning of Objects in Cluttered Environments. (arXiv:2306.06543v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06543</link>
<description rdf:parseType="Literal">&lt;p&gt;Object rearrangement is a fundamental problem in robotics with various
practical applications ranging from managing warehouses to cleaning and
organizing home kitchens. While existing research has primarily focused on
single-agent solutions, real-world scenarios often require multiple robots to
work together on rearrangement tasks. This paper proposes a comprehensive
learning-based framework for multi-agent object rearrangement planning,
addressing the challenges of task sequencing and path planning in complex
environments. The proposed method iteratively selects objects, determines their
relocation regions, and pairs them with available robots under kinematic
feasibility and task reachability for execution to achieve the target
arrangement. Our experiments on a diverse range of simulated and real-world
environments demonstrate the effectiveness and robustness of the proposed
framework. Furthermore, results indicate improved performance in terms of
traversal time and success rate compared to baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Vivek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhir_P/0/1/0/all/0/1&quot;&gt;Praphpreet Dhir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dani_J/0/1/0/all/0/1&quot;&gt;Jeegn Dani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_A/0/1/0/all/0/1&quot;&gt;Ahmed H. Qureshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08687">
<title>Norm-guided latent space exploration for text-to-image generation. (arXiv:2306.08687v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08687</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models show great potential in synthesizing a large
variety of concepts in new compositions and scenarios. However, the latent
space of initial seeds is still not well understood and its structure was shown
to impact the generation of various concepts. Specifically, simple operations
like interpolation and finding the centroid of a set of seeds perform poorly
when using standard Euclidean or spherical metrics in the latent space. This
paper makes the observation that, in current training procedures, diffusion
models observed inputs with a narrow range of norm values. This has strong
implications for methods that rely on seed manipulation for image generation,
with applications to few-shot and long-tail learning tasks. To address this
issue, we propose a novel method for interpolating between two seeds and
demonstrate that it defines a new non-Euclidean metric that takes into account
a norm-based prior on seeds. We describe a simple yet efficient algorithm for
approximating this interpolation procedure and use it to further define
centroids in the latent seed space. We show that our new interpolation and
centroid techniques significantly enhance the generation of rare concept
images. This further leads to state-of-the-art performance on few-shot and
long-tail benchmarks, improving prior approaches in terms of generation speed,
image quality, and semantic content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1&quot;&gt;Dvir Samuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1&quot;&gt;Rami Ben-Ari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1&quot;&gt;Nir Darshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1&quot;&gt;Haggai Maron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09869">
<title>Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09869</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable performance of text-to-image diffusion models in image
generation tasks, recent studies have raised the issue that generated images
sometimes cannot capture the intended semantic contents of the text prompts,
which phenomenon is often called semantic misalignment. To address this, here
we present a novel energy-based model (EBM) framework for adaptive context
control by modeling the posterior of context vectors. Specifically, we first
formulate EBMs of latent image representations and text embeddings in each
cross-attention layer of the denoising autoencoder. Then, we obtain the
gradient of the log posterior of context vectors, which can be updated and
transferred to the subsequent cross-attention layer, thereby implicitly
minimizing a nested hierarchy of energy functions. Our latent EBMs further
allow zero-shot compositional generation as a linear combination of
cross-attention outputs from different contexts. Using extensive experiments,
we demonstrate that the proposed method is highly effective in handling various
image generation tasks, including multi-concept generation, text-guided image
inpainting, and real and synthetic image editing. Code:
https://github.com/EnergyAttention/Energy-Based-CrossAttention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Geon Yeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeongsol Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Beomsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang Wan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11582">
<title>Computing a human-like reaction time metric from stable recurrent vision models. (arXiv:2306.11582v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11582</link>
<description rdf:parseType="Literal">&lt;p&gt;The meteoric rise in the adoption of deep neural networks as computational
models of vision has inspired efforts to &quot;align&quot; these models with humans. One
dimension of interest for alignment includes behavioral choices, but moving
beyond characterizing choice patterns to capturing temporal aspects of visual
decision-making has been challenging. Here, we sketch a general-purpose
methodology to construct computational accounts of reaction times from a
stimulus-computable, task-optimized model. Specifically, we introduce a novel
metric leveraging insights from subjective logic theory summarizing evidence
accumulation in recurrent vision models. We demonstrate that our metric aligns
with patterns of human reaction times for stimulus manipulations across four
disparate visual decision-making tasks spanning perceptual grouping, mental
simulation, and scene categorization. This work paves the way for exploring the
temporal alignment of model and human visual strategies in the context of
various other cognitive tasks toward generating testable hypotheses for
neuroscience. Links to the code and data can be found on the project page:
https://serre-lab.github.io/rnn_rts_site.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goetschalckx_L/0/1/0/all/0/1&quot;&gt;Lore Goetschalckx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindarajan_L/0/1/0/all/0/1&quot;&gt;Lakshmi Narasimhan Govindarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1&quot;&gt;Alekh Karkada Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1&quot;&gt;Aarit Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheinberg_D/0/1/0/all/0/1&quot;&gt;David L. Sheinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1&quot;&gt;Thomas Serre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11739">
<title>Multi-view 3D Object Reconstruction and Uncertainty Modelling with Neural Shape Prior. (arXiv:2306.11739v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11739</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object reconstruction is important for semantic scene understanding. It is
challenging to reconstruct detailed 3D shapes from monocular images directly
due to a lack of depth information, occlusion and noise. Most current methods
generate deterministic object models without any awareness of the uncertainty
of the reconstruction. We tackle this problem by leveraging a neural object
representation which learns an object shape distribution from large dataset of
3d object models and maps it into a latent space. We propose a method to model
uncertainty as part of the representation and define an uncertainty-aware
encoder which generates latent codes with uncertainty directly from individual
input images. Further, we propose a method to propagate the uncertainty in the
latent code to SDF values and generate a 3d object mesh with local uncertainty
for each mesh component. Finally, we propose an incremental fusion method under
a Bayesian framework to fuse the latent codes from multi-view observations. We
evaluate the system in both synthetic and real datasets to demonstrate the
effectiveness of uncertainty-based fusion to improve 3D object reconstruction
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14111">
<title>Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14111</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from Human Feedback (RLHF) learns from preference
signals, while standard Reinforcement Learning (RL) directly learns from reward
signals. Preferences arguably contain less information than rewards, which
makes preference-based RL seemingly more difficult. This paper theoretically
proves that, for a wide range of preference models, we can solve
preference-based RL directly using existing algorithms and techniques for
reward-based RL, with small or no extra costs. Specifically, (1) for
preferences that are drawn from reward-based probabilistic models, we reduce
the problem to robust reward-based RL that can tolerate small errors in
rewards; (2) for general arbitrary preferences where the objective is to find
the von Neumann winner, we reduce the problem to multiagent reward-based RL
which finds Nash equilibria for factored Markov games with a restricted set of
policies. The latter case can be further reduced to adversarial MDP when
preferences only depend on the final state. We instantiate all reward-based RL
subroutines by concrete provable algorithms, and apply our theory to a large
class of models including tabular MDPs and MDPs with generic function
approximation. We further provide guarantees when K-wise comparisons are
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15136">
<title>What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15136</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction plays a vital role in the performance of autonomous
driving systems, and prediction accuracy, such as average displacement error
(ADE) or final displacement error (FDE), is widely used as a performance
metric. However, a significant disparity exists between the accuracy of
predictors on fixed datasets and driving performance when the predictors are
used downstream for vehicle control, because of a dynamics gap. In the real
world, the prediction algorithm influences the behavior of the ego vehicle,
which, in turn, influences the behaviors of other vehicles nearby. This
interaction results in predictor-specific dynamics that directly impacts
prediction results. In fixed datasets, since other vehicles&apos; responses are
predetermined, this interaction effect is lost, leading to a significant
dynamics gap. This paper studies the overlooked significance of this dynamics
gap. We also examine several other factors contributing to the disparity
between prediction performance and driving performance. The findings highlight
the trade-off between the predictor&apos;s computational efficiency and prediction
accuracy in determining real-world driving performance. In summary, an
interactive, task-driven evaluation protocol for trajectory prediction is
crucial to capture its effectiveness for autonomous driving. Source code along
with experimental settings is available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1&quot;&gt;Phong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoran Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cunjun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Panpan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sifa Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;David Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15951">
<title>Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15951</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks necessitate good algorithms to reduce
complexity, and sufficient utilization of parallel processors for acceleration.
Within convolutional layers, there are three types of operators: convolution
used in forward propagation, deconvolution and dilated-convolution utilized in
backward propagation. During the execution of these operators, zeros are
typically added to tensors, leading to redundant calculations and unnecessary
strain on hardware. To circumvent these inefficiencies, we propose the C-K-S
algorithm, accompanied by efficient GPU implementations. C-K-S trims filters to
exclude zero-padding. For deconvolution and dilated-convolution, C-K-S
transforms sparse tensors into dense tensors, and standardizes the local
computational rules to simplify the hardware control. The experimental results
demonstrate that C-K-S offers good performance in terms of speed and
convergence, surpassing the capabilities of PyTorch and cuDNN in certain
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuopin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04075">
<title>DEDUCE: Multi-head attention decoupled contrastive learning to discover cancer subtypes based on multi-omics data. (arXiv:2307.04075v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04075</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the high heterogeneity and clinical characteristics of cancer, there
are significant differences in multi-omics data and clinical features among
subtypes of different cancers. Therefore, the identification and discovery of
cancer subtypes are crucial for the diagnosis, treatment, and prognosis of
cancer. In this study, we proposed a generalization framework based on
attention mechanisms for unsupervised contrastive learning to analyze cancer
multi-omics data for the identification and characterization of cancer
subtypes. The framework contains a symmetric unsupervised multi-head attention
encoder, which can deeply extract contextual features and long-range
dependencies of multi-omics data, reducing the impact of noise in multi-omics
data. Importantly, the proposed framework includes a decoupled contrastive
learning model (DEDUCE) based on a multi-head attention mechanism to learn
multi-omics data features and clustering and identify cancer subtypes. This
method clusters subtypes by calculating the similarity between samples in the
feature space and sample space of multi-omics data. The basic idea is to
decouple different attributes of multi-omics data features and learn them as
contrasting terms. Construct a contrastive loss function to measure the
difference between positive examples and negative examples, and minimize this
difference, thereby encouraging the model to learn better feature
representation. The DEDUCE model conducts large-scale experiments on simulated
multi-omics data sets, single-cell multi-omics data sets and cancer multi-omics
data sets, and the results are better than 10 deep learning models. Finally, we
used the DEDUCE model to reveal six cancer subtypes of AML. By analyzing GO
functional enrichment, subtype-specific biological functions and GSEA of AML,
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangrui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dazhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1&quot;&gt;Yutao Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhichao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1&quot;&gt;Pengfei Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shaoliang Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06775">
<title>A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06775</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decade, there has been a vast increase in eating disorder
diagnoses and eating disorder-attributed deaths, reaching their zenith during
the Covid-19 pandemic. This immense growth derived in part from the stressors
of the pandemic but also from increased exposure to social media, which is rife
with content that promotes eating disorders. This study aimed to create a
multimodal deep learning model that can determine if a given social media post
promotes eating disorders based on a combination of visual and textual data. A
labeled dataset of Tweets was collected from Twitter, recently rebranded as X,
upon which twelve deep learning models were trained and evaluated. Based on
model performance, the most effective deep learning model was the multimodal
fusion of the RoBERTa natural language processing model and the MaxViT image
classification model, attaining accuracy and F1 scores of 95.9% and 0.959,
respectively. The RoBERTa and MaxViT fusion model, deployed to classify an
unlabeled dataset of posts from the social media sites Tumblr and Reddit,
generated results akin to those of previous research studies that did not
employ artificial intelligence-based techniques, indicating that deep learning
models can develop insights congruent to those of researchers. Additionally,
the model was used to conduct a time-series analysis of yet unseen Tweets from
eight Twitter hashtags, uncovering that, since 2014, the relative abundance of
content that promotes eating disorders has decreased drastically within those
communities. Despite this reduction, by 2018, content that promotes eating
disorders had either stopped declining or increased in ampleness anew on those
hashtags.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_J/0/1/0/all/0/1&quot;&gt;Jonathan Feldman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06870">
<title>Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06870</link>
<description rdf:parseType="Literal">&lt;p&gt;A robot deployed in a home over long stretches of time faces a true lifelong
learning problem. As it seeks to provide assistance to its users, the robot
should leverage any accumulated experience to improve its own knowledge and
proficiency. We formalize this setting with a novel formulation of lifelong
learning for task and motion planning (TAMP), which endows our learner with the
compositionality of TAMP systems. Exploiting the modularity of TAMP, we develop
a mixture of generative models that produces candidate continuous parameters
for a planner. Whereas most existing lifelong learning approaches determine a
priori how data is shared across various models, our approach learns shared and
non-shared models and determines which to use online during planning based on
auxiliary tasks that serve as a proxy for each model&apos;s understanding of a
state. Our method exhibits substantial improvements (over time and compared to
baselines) in planning success on 2D and BEHAVIOR domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_Mendez_J/0/1/0/all/0/1&quot;&gt;Jorge Mendez-Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11760">
<title>Large Language Models Understand and Can be Enhanced by Emotional Stimuli. (arXiv:2307.11760v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11760</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotional intelligence significantly impacts our daily behaviors and
interactions. Although Large Language Models (LLMs) are increasingly viewed as
a stride toward artificial general intelligence, exhibiting impressive
performance in numerous tasks, it is still uncertain if LLMs can genuinely
grasp psychological emotional stimuli. Understanding and responding to
emotional cues gives humans a distinct advantage in problem-solving. In this
paper, we take the first step towards exploring the ability of LLMs to
understand emotional stimuli. To this end, we first conduct automatic
experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,
Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative
applications that represent comprehensive evaluation scenarios. Our automatic
experiments show that LLMs have a grasp of emotional intelligence, and their
performance can be improved with emotional prompts (which we call
&quot;EmotionPrompt&quot; that combines the original prompt with emotional stimuli),
e.g., 8.00% relative performance improvement in Instruction Induction and 115%
in BIG-Bench. In addition to those deterministic tasks that can be
automatically evaluated using existing metrics, we conducted a human study with
106 participants to assess the quality of generative tasks using both vanilla
and emotional prompts. Our human study results demonstrate that EmotionPrompt
significantly boosts the performance of generative tasks (10.9% average
improvement in terms of performance, truthfulness, and responsibility metrics).
We provide an in-depth discussion regarding why EmotionPrompt works for LLMs
and the factors that may influence its performance. We posit that EmotionPrompt
heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs
interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1&quot;&gt;Wenxin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1&quot;&gt;Jianxun Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15936">
<title>A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15936</link>
<description rdf:parseType="Literal">&lt;p&gt;A major driver of AI products today is the fact that new skills emerge in
language models when their parameter set and training corpora are scaled up.
This phenomenon is poorly understood, and a mechanistic explanation via
mathematical analysis of gradient-based training seems difficult. The current
paper takes a different approach, analysing emergence using the famous (and
empirical) Scaling Laws of LLMs and a simple statistical framework.
Contributions include: (a) A statistical framework that relates cross-entropy
loss of LLMs to competence on the basic skills that underlie language tasks.
(b) Mathematical analysis showing that the Scaling Laws imply a strong form of
inductive bias that allows the pre-trained model to learn very efficiently. We
informally call this {\em slingshot generalization} since naively viewed it
appears to give competence levels at skills that violate usual generalization
theory. (c) A key example of slingshot generalization, that competence at
executing tasks involving $k$-tuples of skills emerges essentially at the same
scaling and same rate as competence on the elementary skills themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Sanjeev Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anirudh Goyal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16104">
<title>AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16104</link>
<description rdf:parseType="Literal">&lt;p&gt;Floods are one of the most common natural disasters, with a disproportionate
impact in developing countries that often lack dense streamflow gauge networks.
Accurate and timely warnings are critical for mitigating flood risks, but
hydrological simulation models typically must be calibrated to long data
records in each watershed. Using AI, we achieve reliability in predicting
extreme riverine events in ungauged watersheds at up to a 5-day lead time that
is similar to or better than the reliability of nowcasts (0-day lead time) from
a current state of the art global modeling system (the Copernicus Emergency
Management Service Global Flood Awareness System). Additionally, we achieve
accuracies over 5-year return period events that are similar to or better than
current accuracies over 1-year return period events. This means that AI can
provide flood warnings earlier and over larger and more impactful events in
ungauged basins. The model developed in this paper was incorporated into an
operational early warning system that produces publicly available (free and
open) forecasts in real time in over 80 countries. This work highlights a need
for increasing the availability of hydrological data to continue to improve
global access to reliable flood warnings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nearing_G/0/1/0/all/0/1&quot;&gt;Grey Nearing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1&quot;&gt;Deborah Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dube_V/0/1/0/all/0/1&quot;&gt;Vusumuzi Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_M/0/1/0/all/0/1&quot;&gt;Martin Gauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilon_O/0/1/0/all/0/1&quot;&gt;Oren Gilon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrigan_S/0/1/0/all/0/1&quot;&gt;Shaun Harrigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1&quot;&gt;Avinatan Hassidim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klotz_D/0/1/0/all/0/1&quot;&gt;Daniel Klotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratzert_F/0/1/0/all/0/1&quot;&gt;Frederik Kratzert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzger_A/0/1/0/all/0/1&quot;&gt;Asher Metzger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevo_S/0/1/0/all/0/1&quot;&gt;Sella Nevo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappenberger_F/0/1/0/all/0/1&quot;&gt;Florian Pappenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prudhomme_C/0/1/0/all/0/1&quot;&gt;Christel Prudhomme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_G/0/1/0/all/0/1&quot;&gt;Guy Shalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenzis_S/0/1/0/all/0/1&quot;&gt;Shlomo Shenzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekalign_T/0/1/0/all/0/1&quot;&gt;Tadele Tekalign&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1&quot;&gt;Dana Weitzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yoss Matias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00158">
<title>Predictive Data Analytics with AI: assessing the need for post-editing of MT output by fine-tuning OpenAI LLMs. (arXiv:2308.00158v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Translation Quality Evaluation (TQE) is an essential step of the modern
translation production process. TQE is critical in assessing both machine
translation (MT) and human translation (HT) quality without reference
translations. The ability to evaluate or even simply estimate the quality of
translation automatically may open significant efficiency gains through process
optimisation. This work examines whether the state-of-the-art large language
models (LLMs) can be used for this purpose. We take OpenAI models as the best
state-of-the-art technology and approach TQE as a binary classification task.
On \textbf{eight language pairs} including English to Italian, German, French,
Japanese, Dutch, Portuguese, Turkish, and Chinese, our experimental results
show that fine-tuned \textbf{\textit{gpt3.5}} can demonstrate good performance
on translation quality prediction tasks, i.e. \textit{whether the translation
needs to be edited}. Another finding is that simply increasing the sizes of
LLMs does not lead to apparent better performances on this task by comparing
the performance of three different versions of OpenAI models:
\textbf{\textit{curie}}, \textbf{\textit{davinci}}, and
\textbf{\textit{gpt3.5}} with 13B, 175B, and 175B parameters, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1&quot;&gt;Serge Gladkoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1&quot;&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00352">
<title>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. (arXiv:2308.00352v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00352</link>
<description rdf:parseType="Literal">&lt;p&gt;Remarkable progress has been made on automated problem solving through
societies of agents based on large language models (LLMs). Existing LLM-based
multi-agent systems can already solve simple dialogue tasks. Solutions to more
complex tasks, however, are complicated through logic inconsistencies due to
cascading hallucinations caused by naively chaining LLMs. Here we introduce
MetaGPT, an innovative meta-programming framework incorporating efficient human
workflows into LLM-based multi-agent collaborations. MetaGPT encodes
Standardized Operating Procedures (SOPs) into prompt sequences for more
streamlined workflows, thus allowing agents with human-like domain expertise to
verify intermediate results and reduce errors. MetaGPT utilizes an assembly
line paradigm to assign diverse roles to various agents, efficiently breaking
down complex tasks into subtasks involving many agents working together. On
collaborative software engineering benchmarks, MetaGPT generates more coherent
solutions than previous chat-based multi-agent systems. Our project can be
found at https://github.com/geekan/MetaGPT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sirui Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1&quot;&gt;Mingchen Zhuge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jonathan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Ceyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yau_S/0/1/0/all/0/1&quot;&gt;Steven Ka Shing Yau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zijuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_C/0/1/0/all/0/1&quot;&gt;Chenyu Ran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Lingfeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenglin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04451">
<title>Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04451</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-based code generators have become pivotal in assisting developers in
writing software starting from natural language (NL). However, they are trained
on large amounts of data, often collected from unsanitized online sources
(e.g., GitHub, HuggingFace). As a consequence, AI models become an easy target
for data poisoning, i.e., an attack that injects malicious samples into the
training data to generate vulnerable code. To address this threat, we
investigate the security of AI code generators by devising a targeted data
poisoning strategy. We poison the training data by injecting increasing amounts
of code containing security vulnerabilities and assess the attack&apos;s success on
different state-of-the-art models for code generation. Our study shows that AI
code generators are vulnerable to even a small amount of poison. Notably, the
attack success strongly depends on the model architecture and poisoning rate,
whereas it is not influenced by the type of vulnerabilities. Moreover, since
the attack does not impact the correctness of code generated by pre-trained
models, it is hard to detect. Lastly, our work offers practical insights into
understanding and potentially mitigating this threat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1&quot;&gt;Domenico Cotroneo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Improta_C/0/1/0/all/0/1&quot;&gt;Cristina Improta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1&quot;&gt;Pietro Liguori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1&quot;&gt;Roberto Natella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06032">
<title>Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06032</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) could enhance access to the legal system.
However, empirical research on their effectiveness in conducting legal tasks is
scant. We study securities cases involving cryptocurrencies as one of numerous
contexts where AI could support the legal process, studying LLMs&apos; legal
reasoning and drafting capabilities. We examine whether a) an LLM can
accurately determine which laws are potentially being violated from a fact
pattern, and b) whether there is a difference in juror decision-making based on
complaints written by a lawyer compared to an LLM. We feed fact patterns from
real-life cases to GPT-3.5 and evaluate its ability to determine correct
potential violations from the scenario and exclude spurious violations. Second,
we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5&apos;s
legal reasoning skills proved weak, though we expect improvement in future
models, particularly given the violations it suggested tended to be correct (it
merely missed additional, correct violations). GPT-3.5 performed better at
legal drafting, and jurors&apos; decisions were not statistically significantly
associated with the author of the document upon which they based their
decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks,
they would be unable to replace lawyers at this stage. However, their drafting
skills (though, perhaps, still inferior to lawyers), could provide access to
justice for more individuals by reducing the cost of legal services. Our
research is the first to systematically study LLMs&apos; legal drafting and
reasoning capabilities in litigation, as well as in securities law and
cryptocurrency-related misconduct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trozze_A/0/1/0/all/0/1&quot;&gt;Arianna Trozze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_T/0/1/0/all/0/1&quot;&gt;Toby Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1&quot;&gt;Bennett Kleinberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06197">
<title>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features. (arXiv:2308.06197v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06197</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex emotion recognition is a cognitive task that has so far eluded the
same excellent performance of other tasks that are at or above the level of
human cognition. Emotion recognition through facial expressions is particularly
difficult due to the complexity of emotions expressed by the human face. For a
machine to approach the same level of performance in complex facial expression
recognition as a human, it may need to synthesise knowledge and understand new
concepts in real-time, as humans do. Humans are able to learn new concepts
using only few examples by distilling important information from memories.
Inspired by human cognition and learning, we propose a novel continual learning
method for complex facial expression recognition that can accurately recognise
new compound expression classes using few training samples, by building on and
retaining its knowledge of basic expression classes. In this work, we also use
GradCAM visualisations to demonstrate the relationship between basic and
compound facial expressions. Our method leverages this relationship through
knowledge distillation and a novel Predictive Sorting Memory Replay, to achieve
the current state-of-the-art in continual learning for complex facial
expression recognition, with 74.28% Overall Accuracy on new classes. We also
demonstrate that using continual learning for complex facial expression
recognition achieves far better performance than non-continual learning
methods, improving on state-of-the-art non-continual learning methods by
13.95%. Our work is also the first to apply few-shot learning to complex facial
expression recognition, achieving the state-of-the-art with 100% accuracy using
only a single training sample per class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maiden_A/0/1/0/all/0/1&quot;&gt;Angus Maiden&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakisa_B/0/1/0/all/0/1&quot;&gt;Bahareh Nakisa&lt;/a&gt; (1) ((1) Deakin University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09544">
<title>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning. (arXiv:2308.09544v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09544</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate exemplar-free class incremental learning (CIL)
with knowledge distillation (KD) as a regularization strategy, aiming to
prevent forgetting. KD-based methods are successfully used in CIL, but they
often struggle to regularize the model without access to exemplars of the
training data from previous tasks. Our analysis reveals that this issue
originates from substantial representation shifts in the teacher network when
dealing with out-of-distribution data. This causes large errors in the KD loss
component, leading to performance degradation in CIL models. Inspired by recent
test-time adaptation methods, we introduce Teacher Adaptation (TA), a method
that concurrently updates the teacher and the main models during incremental
training. Our method seamlessly integrates with KD-based CIL approaches and
allows for consistent enhancement of their performance across multiple
exemplar-free CIL benchmarks. The source code for our method is available at
https://github.com/fszatkowski/cl-teacher-adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szatkowski_F/0/1/0/all/0/1&quot;&gt;Filip Szatkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyla_M/0/1/0/all/0/1&quot;&gt;Mateusz Pyla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Przewiezlikowski_M/0/1/0/all/0/1&quot;&gt;Marcin Przewi&amp;#x119;&amp;#x17a;likowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1&quot;&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12486">
<title>A Brain-Inspired Sequence Learning Model based on a Logic. (arXiv:2308.12486v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12486</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequence learning is an essential aspect of intelligence. In Artificial
Intelligence, sequence prediction task is usually used to test a sequence
learning model. In this paper, a model of sequence learning, which is
interpretable through Non-Axiomatic Logic, is designed and tested. The learning
mechanism is composed of three steps, hypothesizing, revising, and recycling,
which enable the model to work under the Assumption of Insufficient Knowledge
and Resources. Synthetic datasets for sequence prediction task are generated to
test the capacity of the model. The results show that the model works well
within different levels of difficulty. In addition, since the model adopts
concept-centered representation, it theoretically does not suffer from
catastrophic forgetting, and the practical results also support this property.
This paper shows the potential of learning sequences in a logical way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bowen Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14132">
<title>Detecting Language Model Attacks with Perplexity. (arXiv:2308.14132v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14132</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel hack involving Large Language Models (LLMs) has emerged, leveraging
adversarial suffixes to trick models into generating perilous responses. This
method has garnered considerable attention from reputable media outlets such as
the New York Times and Wired, thereby influencing public perception regarding
the security and safety of LLMs. In this study, we advocate the utilization of
perplexity as one of the means to recognize such potential attacks. The
underlying concept behind these hacks revolves around appending an unusually
constructed string of text to a harmful query that would otherwise be blocked.
This maneuver confuses the protective mechanisms and tricks the model into
generating a forbidden response. Such scenarios could result in providing
detailed instructions to a malicious user for constructing explosives or
orchestrating a bank heist. Our investigation demonstrates the feasibility of
employing perplexity, a prevalent natural language processing metric, to detect
these adversarial tactics before generating a forbidden response. By evaluating
the perplexity of queries with and without such adversarial suffixes using an
open-source LLM, we discovered that nearly 90 percent were above a perplexity
of 1000. This contrast underscores the efficacy of perplexity for detecting
this type of exploit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1&quot;&gt;Gabriel Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1&quot;&gt;Michael Kamfonas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01950">
<title>RADIO: Reference-Agnostic Dubbing Video Synthesis. (arXiv:2309.01950v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01950</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most challenging problems in audio-driven talking head generation
is achieving high-fidelity detail while ensuring precise synchronization. Given
only a single reference image, extracting meaningful identity attributes
becomes even more challenging, often causing the network to mirror the facial
and lip structures too closely. To address these issues, we introduce RADIO, a
framework engineered to yield high-quality dubbed videos regardless of the pose
or expression in reference images. The key is to modulate the decoder layers
using latent space composed of audio and reference features. Additionally, we
incorporate ViT blocks into the decoder to emphasize high-fidelity details,
especially in the lip region. Our experimental results demonstrate that RADIO
displays high synchronization without the loss of fidelity. Especially in harsh
scenarios where the reference frame deviates significantly from the ground
truth, our method outperforms state-of-the-art methods, highlighting its
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongyeun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chaewon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sangjoon Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Gyeong-Moon Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05378">
<title>Steps Towards Satisficing Distributed Dynamic Team Trust. (arXiv:2309.05378v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05378</link>
<description rdf:parseType="Literal">&lt;p&gt;Defining and measuring trust in dynamic, multiagent teams is important in a
range of contexts, particularly in defense and security domains. Team members
should be trusted to work towards agreed goals and in accordance with shared
values. In this paper, our concern is with the definition of goals and values
such that it is possible to define &apos;trust&apos; in a way that is interpretable, and
hence usable, by both humans and robots. We argue that the outcome of team
activity can be considered in terms of &apos;goal&apos;, &apos;individual/team values&apos;, and
&apos;legal principles&apos;. We question whether alignment is possible at the level of
&apos;individual/team values&apos;, or only at the &apos;goal&apos; and &apos;legal principles&apos; levels.
We argue for a set of metrics to define trust in human-robot teams that are
interpretable by human or robot team members, and consider an experiment that
could demonstrate the notion of &apos;satisficing trust&apos; over the course of a
simulated mission.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunt_E/0/1/0/all/0/1&quot;&gt;Edmund R. Hunt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baber_C/0/1/0/all/0/1&quot;&gt;Chris Baber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sobhani_M/0/1/0/all/0/1&quot;&gt;Mehdi Sobhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milivojevic_S/0/1/0/all/0/1&quot;&gt;Sanja Milivojevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yusuf_S/0/1/0/all/0/1&quot;&gt;Sagir Yusuf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1&quot;&gt;Mirco Musolesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waterson_P/0/1/0/all/0/1&quot;&gt;Patrick Waterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maynard_S/0/1/0/all/0/1&quot;&gt;Sally Maynard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06038">
<title>Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping. (arXiv:2309.06038v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06038</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of anthropomorphic robotic hands for assisting individuals in
situations where human hands may be unavailable or unsuitable has gained
significant importance. In this paper, we propose a novel task called
human-assisting dexterous grasping that aims to train a policy for controlling
a robotic hand&apos;s fingers to assist users in grasping objects. Unlike
conventional dexterous grasping, this task presents a more complex challenge as
the policy needs to adapt to diverse user intentions, in addition to the
object&apos;s geometry. We address this challenge by proposing an approach
consisting of two sub-modules: a hand-object-conditional grasping primitive
called Grasping Gradient Field~(GraspGF), and a history-conditional residual
policy. GraspGF learns `how&apos; to grasp by estimating the gradient from a success
grasping example set, while the residual policy determines `when&apos; and at what
speed the grasping action should be executed based on the trajectory history.
Experimental results demonstrate the superiority of our proposed method
compared to baselines, highlighting the user-awareness and practicality in
real-world applications. The codes and demonstrations can be viewed at
&quot;https://sites.google.com/view/graspgf&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mingdong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yunchong Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07760">
<title>PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07760</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1&quot;&gt;Anh Pham Thi Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;An Duc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Tzimiropoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08138">
<title>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation. (arXiv:2309.08138v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08138</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Visual Object Navigation (VON) involves an agent&apos;s ability to
locate a particular object within a given scene. In order to successfully
accomplish the VON task, two essential conditions must be fulfilled:1) the user
must know the name of the desired object; and 2) the user-specified object must
actually be present within the scene. To meet these conditions, a simulator can
incorporate pre-defined object names and positions into the metadata of the
scene. However, in real-world scenarios, it is often challenging to ensure that
these conditions are always met. Human in an unfamiliar environment may not
know which objects are present in the scene, or they may mistakenly specify an
object that is not actually present. Nevertheless, despite these challenges,
human may still have a demand for an object, which could potentially be
fulfilled by other objects present within the scene in an equivalent manner.
Hence, we propose Demand-driven Navigation (DDN), which leverages the user&apos;s
demand as the task instruction and prompts the agent to find the object matches
the specified demand. DDN aims to relax the stringent conditions of VON by
focusing on fulfilling the user&apos;s demand rather than relying solely on
predefined object categories or names. We propose a method first acquire
textual attribute features of objects by extracting common knowledge from a
large language model. These textual attribute features are subsequently aligned
with visual attribute features using Contrastive Language-Image Pre-training
(CLIP). By incorporating the visual attribute features as prior knowledge, we
enhance the navigation process. Experiments on AI2Thor with the ProcThor
dataset demonstrate the visual attribute features improve the agent&apos;s
navigation performance and outperform the baseline methods commonly used in
VON.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongcheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Andy Guan Hong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mingdong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08549">
<title>HINT: Healthy Influential-Noise based Training to Defend against Data Poisoning Attacks. (arXiv:2309.08549v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08549</link>
<description rdf:parseType="Literal">&lt;p&gt;While numerous defense methods have been proposed to prohibit potential
poisoning attacks from untrusted data sources, most research works only defend
against specific attacks, which leaves many avenues for an adversary to
exploit. In this work, we propose an efficient and robust training approach to
defend against data poisoning attacks based on influence functions, named
Healthy Influential-Noise based Training. Using influence functions, we craft
healthy noise that helps to harden the classification model against poisoning
attacks without significantly affecting the generalization ability on test
data. In addition, our method can perform effectively when only a subset of the
training data is modified, instead of the current method of adding noise to all
examples that has been used in several previous works. We conduct comprehensive
evaluations over two image datasets with state-of-the-art poisoning attacks
under different realistic attack scenarios. Our empirical results show that
HINT can efficiently protect deep learning models against the effect of both
untargeted and targeted poisoning attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1&quot;&gt;Minh-Hao Van&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carey_A/0/1/0/all/0/1&quot;&gt;Alycia N. Carey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09319">
<title>Active Learning for Semantic Segmentation with Multi-class Label Query. (arXiv:2309.09319v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09319</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a new active learning method for semantic segmentation.
The core of our method lies in a new annotation query design. It samples
informative local image regions (e.g., superpixels), and for each of such
regions, asks an oracle for a multi-hot vector indicating all classes existing
in the region. This multi-class labeling strategy is substantially more
efficient than existing ones like segmentation, polygon, and even dominant
class labeling in terms of annotation time per click. However, it introduces
the class ambiguity issue in training as it assigns partial labels (i.e., a set
of candidate classes) to individual pixels. We thus propose a new algorithm for
learning semantic segmentation while disambiguating the partial labels in two
stages. In the first stage, it trains a segmentation model directly with the
partial labels through two new loss functions motivated by partial label
learning and multiple instance learning. In the second stage, it disambiguates
the partial labels by generating pixel-wise pseudo labels, which are used for
supervised learning of the model. Equipped with a new acquisition function
dedicated to the multi-class labeling, our method outperforms previous work on
Cityscapes and PASCAL VOC 2012 while spending less annotation cost. Our code
and results are available at https://github.com/sehyun03/MulActSeg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sehyun Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sohyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hoyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1&quot;&gt;Minhyeon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ok_J/0/1/0/all/0/1&quot;&gt;Jungseul Ok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10160">
<title>RadOnc-GPT: A Large Language Model for Radiation Oncology. (arXiv:2309.10160v3 [physics.med-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10160</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents RadOnc-GPT, a large language model specialized for
radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on
a large dataset of radiation oncology patient records from the Mayo Clinic in
Arizona. The model employs instruction tuning on three key tasks - generating
radiotherapy treatment regimens, determining optimal radiation modalities, and
providing diagnostic descriptions/ICD codes based on patient diagnostic
details. Evaluations conducted by comparing RadOnc-GPT outputs to general large
language model outputs showed higher ROUGE scores in these three tasks. The
study demonstrated the potential of using large language models fine-tuned
using domain-specific knowledge like RadOnc-GPT to achieve transformational
capabilities in highly specialized healthcare fields such as radiation
oncology. However, our model&apos;s clinical relevance requires confirmation, and it
specializes in only the aforementioned three specific tasks and lacks broader
applicability. Furthermore, its evaluation through ROUGE scores might not
reflect the true semantic and clinical accuracy - challenges we intend to
address in future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Holmes_J/0/1/0/all/0/1&quot;&gt;Jason Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shu_P/0/1/0/all/0/1&quot;&gt;Peng Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dajiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Samir H. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sio_T/0/1/0/all/0/1&quot;&gt;Terence T. Sio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14032">
<title>DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization. (arXiv:2309.14032v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14032</link>
<description rdf:parseType="Literal">&lt;p&gt;Ant Colony Optimization (ACO) is a meta-heuristic algorithm that has been
successfully applied to various Combinatorial Optimization Problems (COPs).
Traditionally, customizing ACO for a specific problem requires the expert
design of knowledge-driven heuristics. In this paper, we propose DeepACO, a
generic framework that leverages deep reinforcement learning to automate
heuristic designs. DeepACO serves to strengthen the heuristic measures of
existing ACO algorithms and dispense with laborious manual design in future ACO
applications. As a neural-enhanced meta-heuristic, DeepACO consistently
outperforms its ACO counterparts on eight COPs using a single neural
architecture and a single set of hyperparameters. As a Neural Combinatorial
Optimization method, DeepACO performs better than or on par with
problem-specific methods on canonical routing problems. Our code is publicly
available at https://github.com/henry-yeh/DeepACO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haoran Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiarui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Helan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16223">
<title>GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16223</link>
<description rdf:parseType="Literal">&lt;p&gt;Diverse explainability methods of graph neural networks (GNN) have recently
been developed to highlight the edges and nodes in the graph that contribute
the most to the model predictions. However, it is not clear yet how to evaluate
the correctness of those explanations, whether it is from a human or a model
perspective. One unaddressed bottleneck in the current evaluation procedure is
the problem of out-of-distribution explanations, whose distribution differs
from those of the training data. This important issue affects existing
evaluation metrics such as the popular faithfulness or fidelity score. In this
paper, we show the limitations of faithfulness metrics. We propose GInX-Eval
(Graph In-distribution eXplanation Evaluation), an evaluation procedure of
graph explanations that overcomes the pitfalls of faithfulness and offers new
insights on explainability methods. Using a fine-tuning strategy, the GInX
score measures how informative removed edges are for the model and the EdgeRank
score evaluates if explanatory edges are correctly ordered by their importance.
GInX-Eval verifies if ground-truth explanations are instructive to the GNN
model. In addition, it shows that many popular methods, including
gradient-based methods, produce explanations that are not better than a random
designation of edges as important subgraphs, challenging the findings of
current works in the area. Results with GInX-Eval are consistent across
multiple datasets and align with human evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1&quot;&gt;Kenza Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Assady_M/0/1/0/all/0/1&quot;&gt;Mennatallah El-Assady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1&quot;&gt;Rex Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17425">
<title>Data Filtering Networks. (arXiv:2309.17425v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17425</link>
<description rdf:parseType="Literal">&lt;p&gt;Large training sets have become a cornerstone of machine learning and are the
foundation for recent advances in language modeling and multimodal learning.
While data curation for pre-training is often still ad-hoc, one common paradigm
is to first collect a massive pool of data from the Web and then filter this
candidate pool down to an actual training set via various heuristics. In this
work, we study the problem of learning a data filtering network (DFN) for this
second step of filtering a large uncurated dataset. Our key finding is that the
quality of a network for filtering is distinct from its performance on
downstream tasks: for instance, a model that performs well on ImageNet can
yield worse training sets than a model with low ImageNet accuracy that is
trained on a small amount of high-quality data. Based on our insights, we
construct new data filtering networks that induce state-of-the-art image-text
datasets. Specifically, our best performing dataset DFN-5B enables us to train
state-of-the-art CLIP models for their compute budgets: among other
improvements on a variety of tasks, a ViT-H trained on our dataset achieves
84.4% zero-shot transfer accuracy on ImageNet, out-performing models trained on
other datasets such as LAION-2B, DataComp-1B, or OpenAI&apos;s WIT. In order to
facilitate further research in dataset design, we also release a new 2 billion
example dataset DFN-2B and show that high performance data filtering networks
can be trained from scratch using only publicly available data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1&quot;&gt;Alex Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jose_A/0/1/0/all/0/1&quot;&gt;Albin Madappally Jose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Amit Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1&quot;&gt;Alexander Toshev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vaishaal Shankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00836">
<title>Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models. (arXiv:2310.00836v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00836</link>
<description rdf:parseType="Literal">&lt;p&gt;Logical reasoning is fundamental for humans yet presents a substantial
challenge in the domain of Artificial Intelligence. Initially, researchers used
Knowledge Representation and Reasoning (KR) systems that did not scale and
required non trivial manual effort. Recently, the emergence of large language
models (LLMs) has demonstrated the ability to overcome various limitations of
formal Knowledge Representation (KR) systems. Consequently, there is a growing
interest in using LLMs for logical reasoning via natural language. This work
strives to understand the proficiency of LLMs in logical reasoning by offering
a brief review of the latest progress in this area; with a focus on the logical
reasoning datasets, tasks, and the methods adopted to utilize LLMs for
reasoning. To offer a thorough analysis, we have compiled a benchmark titled
LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive,
and inductive reasoning. We have standardized these datasets into Seq2Seq tasks
to facilitate straightforward training and evaluation for future research.
Utilizing LogiGLUE as a foundation, we have trained an instruction fine tuned
language model, resulting in LogiT5. We study single task training, multi task
training, and a chain of thought knowledge distillation fine tuning technique
to assess the performance of model across the different logical reasoning
categories. By this comprehensive process, we aim to shed light on the
capabilities and potential pathways for enhancing logical reasoning proficiency
in LLMs, paving the way for more advanced and nuanced developments in this
critical field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Man Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumbhar_S/0/1/0/all/0/1&quot;&gt;Shrinidhi Kumbhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+shen_M/0/1/0/all/0/1&quot;&gt;Ming shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1&quot;&gt;Mihir Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1&quot;&gt;Neeraj Varshney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1&quot;&gt;Pratyay Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1&quot;&gt;Somak Aditya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1&quot;&gt;Chitta Baral&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01352">
<title>RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01352</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieval-augmented language models (RALMs) improve performance by accessing
long-tail and up-to-date knowledge from external data stores, but are
challenging to build. Existing approaches require either expensive
retrieval-specific modifications to LM pre-training or use post-hoc integration
of the data store that leads to suboptimal performance. We introduce
Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning
methodology that provides a third option by retrofitting any LLM with retrieval
capabilities. Our approach operates in two distinct fine-tuning steps: (1) one
updates a pre-trained LM to better use retrieved information, while (2) the
other updates the retriever to return more relevant results, as preferred by
the LM. By fine-tuning over tasks that require both knowledge utilization and
contextual awareness, we demonstrate that each stage yields significant
performance improvements, and using both leads to additional gains. Our best
model, RA-DIT 65B, achieves state-of-the-art performance across a range of
knowledge-intensive zero- and few-shot learning benchmarks, significantly
outperforming existing in-context RALM approaches by up to +8.9% in 0-shot
setting and +1.4% in 5-shot setting on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Victoria Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xilun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1&quot;&gt;Maria Lomeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1&quot;&gt;Rich James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1&quot;&gt;Pedro Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1&quot;&gt;Jacob Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szilvasy_G/0/1/0/all/0/1&quot;&gt;Gergely Szilvasy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1&quot;&gt;Scott Yih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01775">
<title>STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01775</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning for many manipulation tasks, such as using tools or assembling
parts, often requires both symbolic and geometric reasoning. Task and Motion
Planning (TAMP) algorithms typically solve these problems by conducting a tree
search over high-level task sequences while checking for kinematic and dynamic
feasibility. This can be inefficient as the width of the tree can grow
exponentially with the number of possible actions and objects. In this paper,
we propose a novel approach to TAMP that relaxes discrete-and-continuous TAMP
problems into inference problems on a continuous domain. Our method, Stein Task
and Motion Planning (STAMP) subsequently solves this new problem using a
gradient-based variational inference algorithm called Stein Variational
Gradient Descent, by obtaining gradients from a parallelized differentiable
physics simulator. By introducing relaxations to the discrete variables,
leveraging parallelization, and approaching TAMP as an Bayesian inference
problem, our method is able to efficiently find multiple diverse plans in a
single optimization run. We demonstrate our method on two TAMP problems and
benchmark them against existing TAMP baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yewon Lee&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Philip Huang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1&quot;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Andrew Z. Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damken_F/0/1/0/all/0/1&quot;&gt;Fabian Damken&lt;/a&gt; (1 and 4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heiden_E/0/1/0/all/0/1&quot;&gt;Eric Heiden&lt;/a&gt; (5), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin Smith&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt; (6), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt; (5 and 7), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1&quot;&gt;Florian Shkurti&lt;/a&gt; (1) ((1) University of Toronto, (2) Carnegie Mellon University, (3) Massachusetts Institute of Technology, (4) Technische Universitat Darmstadt, (5) NVIDIA, (6) McGill University, (7) University of Sydney)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01852">
<title>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N&amp;gt;=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval task. Beyond this, our LanguageBind has greatly improved
in the zero-shot video, audio, depth, and infrared understanding tasks. For
instance, LanguageBind surpassing InterVideo by 1.9% on MSR-VTT, 8.8% on MSVD,
6.3% on DiDeMo, and 4.4% on ActivityNet. On the LLVIP and NYU-D datasets,
LanguageBind outperforms ImageBind with 23.8% and 11.1% top-1 accuracy. Code
address: https://github.com/PKU-YuanGroup/LanguageBind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxi Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;HongFa Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yatian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wancai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02066">
<title>De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02066</link>
<description rdf:parseType="Literal">&lt;p&gt;De novo drug design requires simultaneously generating novel molecules
outside of training data and predicting their target properties, making it a
hard task for generative models. To address this, we propose Joint Transformer
that combines a Transformer decoder, a Transformer encoder, and a predictor in
a joint generative model with shared weights. We show that training the model
with a penalized log-likelihood objective results in state-of-the-art
performance in molecule generation, while decreasing the prediction error on
newly sampled molecules, as compared to a fine-tuned decoder-only Transformer,
by 42%. Finally, we propose a probabilistic black-box optimization algorithm
that employs Joint Transformer to generate novel molecules with improved target
properties, as compared to the training data, outperforming other SMILES-based
optimization methods in de novo drug design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izdebski_A/0/1/0/all/0/1&quot;&gt;Adam Izdebski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weglarz_Tomczak_E/0/1/0/all/0/1&quot;&gt;Ewelina Weglarz-Tomczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szczurek_E/0/1/0/all/0/1&quot;&gt;Ewa Szczurek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1&quot;&gt;Jakub M. Tomczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03912">
<title>RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03912</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has
proven to be an invaluable technique for efficient, high-dimensional, black-box
optimization, a critical problem inherent to many applications such as
industrial design and scientific computing. Recent contributions have
introduced reinforcement learning (RL) to improve the optimization performance
on both single function optimization and \textit{few-shot} multi-objective
optimization. However, even few-shot techniques fail to exploit similarities
shared between closely related objectives. In this paper, we combine recent
developments in Deep Kernel Learning (DKL) and attention-based Transformer
models to improve the modeling powers of GP surrogates with meta-learning. We
propose a novel method for improving meta-learning BO surrogates by
incorporating attention mechanisms into DKL, empowering the surrogates to adapt
to contextual information gathered during the BO process. We combine this
Transformer Deep Kernel with a learned acquisition function trained with
continuous Soft Actor-Critic Reinforcement Learning to aid in exploration. This
Reinforced Transformer Deep Kernel (RTDK-BO) approach yields state-of-the-art
results in continuous high-dimensional optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmakov_A/0/1/0/all/0/1&quot;&gt;Alexander Shmakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1&quot;&gt;Avisek Naug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1&quot;&gt;Vineet Gundecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbanpour_S/0/1/0/all/0/1&quot;&gt;Sahand Ghorbanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1&quot;&gt;Ricardo Luna Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1&quot;&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillen_A/0/1/0/all/0/1&quot;&gt;Antonio Guillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyendu Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04755">
<title>Pairwise GUI Dataset Construction Between Android Phones and Tablets. (arXiv:2310.04755v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04755</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current landscape of pervasive smartphones and tablets, apps
frequently exist across both platforms. Although apps share most graphic user
interfaces (GUIs) and functionalities across phones and tablets, developers
often rebuild from scratch for tablet versions, escalating costs and
squandering existing design resources. Researchers are attempting to collect
data and employ deep learning in automated GUIs development to enhance
developers&apos; productivity. There are currently several publicly accessible GUI
page datasets for phones, but none for pairwise GUIs between phones and
tablets. This poses a significant barrier to the employment of deep learning in
automated GUI development. In this paper, we introduce the Papt dataset, a
pioneering pairwise GUI dataset tailored for Android phones and tablets,
encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app
pairs. We propose novel pairwise GUI collection approaches for constructing
this dataset and delineate its advantages over currently prevailing datasets in
the field. Through preliminary experiments on this dataset, we analyze the
present challenges of utilizing deep learning in automated GUI development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Haolan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yujin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Di Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04963">
<title>LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation. (arXiv:2310.04963v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04963</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are a new and powerful tool for a wide span of
applications involving natural language and demonstrate impressive code
generation abilities. In this paper, we explore the capabilitity of
state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and
open-source alternatives like Meta AI Codellama, to automatically generate
tests and use these tests to validate and verify compiler implementations of a
directive-based programming paradigm, OpenACC. Our approach entails exploring
various prompt engineering techniques including a code template,
retrieval-augmented generation (RAG) with code template, expressive prompt
using RAG with code template, one-shot example, and RAG with one-shot example.
This paper focuses on (a) exploring the capabilities of the latest LLMs for
code generation, (b) investigating prompt and fine tuning methods, and (c)
analyzing the outcome of LLMs generated tests
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munley_C/0/1/0/all/0/1&quot;&gt;Christian Munley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarmusch_A/0/1/0/all/0/1&quot;&gt;Aaron Jarmusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1&quot;&gt;Sunita Chandrasekaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05654">
<title>No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling. (arXiv:2310.05654v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05654</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have demonstrated outstanding performance in
computer vision tasks, yet their high computational complexity prevents their
deployment in computing resource-constrained environments. Various token
pruning techniques have been introduced to alleviate the high computational
burden of ViTs by dynamically dropping image tokens. However, some undesirable
pruning at early stages may result in permanent loss of image information in
subsequent layers, consequently hindering model performance. To address this
problem, we propose IdleViT, a dynamic token-idle-based method that achieves an
excellent trade-off between performance and efficiency. Specifically, in each
layer, IdleViT selects a subset of the image tokens to participate in
computations while keeping the rest of the tokens idle and directly passing
them to this layer&apos;s output. By allowing the idle tokens to be re-selected in
the following layers, IdleViT mitigates the negative impact of improper pruning
in the early stages. Furthermore, inspired by the normalized graph cut, we
devise a token cut loss on the attention map as regularization to improve
IdleViT&apos;s token selection ability. Our method is simple yet effective and can
be extended to pyramid ViTs since no token is completely dropped. Extensive
experimental results on various ViT architectures have shown that IdleViT can
diminish the complexity of pretrained ViTs by up to 33\% with no more than
0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.
Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art
EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The
source code is available in the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05764">
<title>Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design. (arXiv:2310.05764v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05764</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant amount of protein function requires binding small molecules,
including enzymatic catalysis. As such, designing binding pockets for small
molecules has several impactful applications ranging from drug synthesis to
energy storage. Towards this goal, we first develop HarmonicFlow, an improved
generative process over 3D protein-ligand binding structures based on our
self-conditioned flow matching objective. FlowSite extends this flow model to
jointly generate a protein pocket&apos;s discrete residue types and the molecule&apos;s
binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art
generative processes for docking in simplicity, generality, and average sample
quality in pocket-level docking. Enabled by this structure modeling, FlowSite
designs binding sites substantially better than baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stark_H/0/1/0/all/0/1&quot;&gt;Hannes St&amp;#xe4;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1&quot;&gt;Bowen Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1&quot;&gt;Regina Barzilay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08442">
<title>Debias the Training of Diffusion Models. (arXiv:2310.08442v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08442</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated compelling generation quality by
optimizing the variational lower bound through a simple denoising score
matching loss. In this paper, we provide theoretical evidence that the
prevailing practice of using a constant loss weight strategy in diffusion
models leads to biased estimation during the training phase. Simply optimizing
the denoising network to predict Gaussian noise with constant weighting may
hinder precise estimations of original images. To address the issue, we propose
an elegant and effective weighting strategy grounded in the theoretically
unbiased principle. Moreover, we conduct a comprehensive and systematic
exploration to dissect the inherent bias problem deriving from constant
weighting loss from the perspectives of its existence, impact and reasons.
These analyses are expected to advance our understanding and demystify the
inner workings of diffusion models. Through empirical evaluation, we
demonstrate that our proposed debiased estimation method significantly enhances
sample quality without the reliance on complex techniques, and exhibits
improved efficiency compared to the baseline method both in training and
sampling processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Man Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10544">
<title>Use of probabilistic phrases in a coordination game: human versus GPT-4. (arXiv:2310.10544v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10544</link>
<description rdf:parseType="Literal">&lt;p&gt;English speakers use probabilistic phrases such as likely to communicate
information about the probability or likelihood of events. Communication is
successful to the extent that the listener grasps what the speaker means to
convey and, if communication is successful, individuals can potentially
coordinate their actions based on shared knowledge about uncertainty. We first
assessed human ability to estimate the probability and the ambiguity
(imprecision) of twenty-three probabilistic phrases in a coordination game in
two different contexts, investment advice and medical advice. We then had GPT4
(OpenAI), a Large Language Model, complete the same tasks as the human
participants. We found that the median human participant and GPT4 assigned
probability estimates that were in good agreement (proportions of variance
accounted for close to .90). GPT4&apos;s estimates of probability both in the
investment and Medical contexts were as close or closer to that of the human
participants as the human participants&apos; estimates were to one another.
Estimates of probability for both the human participants and GPT4 were little
affected by context. In contrast, human and GPT4 estimates of ambiguity were
not in such good agreement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maloney_L/0/1/0/all/0/1&quot;&gt;Laurence T Maloney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martello_M/0/1/0/all/0/1&quot;&gt;Maria F Dal Martello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fei_V/0/1/0/all/0/1&quot;&gt;Vivian Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_V/0/1/0/all/0/1&quot;&gt;Valerie Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11441">
<title>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V. (arXiv:2310.11441v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11441</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Set-of-Mark (SoM), a new visual prompting method, to unleash the
visual grounding abilities of large multimodal models (LMMs), such as GPT-4V.
As illustrated in Fig. 1 (right), we employ off-the-shelf interactive
segmentation models, such as SEEM/SAM, to partition an image into regions at
different levels of granularity, and overlay these regions with a set of marks
e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can
answer the questions that require visual grounding. We perform a comprehensive
empirical study to validate the effectiveness of SoM on a wide range of
fine-grained vision and multimodal tasks. For example, our experiments show
that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art
fully-finetuned referring expression comprehension and segmentation model on
RefCOCOg. Code for SoM prompting is made public at:
https://github.com/microsoft/SoM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12508">
<title>SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12508</link>
<description rdf:parseType="Literal">&lt;p&gt;With evolving data regulations, machine unlearning (MU) has become an
important tool for fostering trust and safety in today&apos;s AI models. However,
existing MU methods focusing on data and/or weight perspectives often grapple
with limitations in unlearning accuracy, stability, and cross-domain
applicability. To address these challenges, we introduce the concept of &apos;weight
saliency&apos; in MU, drawing parallels with input saliency in model explanation.
This innovation directs MU&apos;s attention toward specific model weights rather
than the entire model, improving effectiveness and efficiency. The resultant
method that we call saliency unlearning (SalUn) narrows the performance gap
with &apos;exact&apos; unlearning (model retraining from scratch after removing the
forgetting dataset). To the best of our knowledge, SalUn is the first
principled MU approach adaptable enough to effectively erase the influence of
forgetting data, classes, or concepts in both image classification and
generation. For example, SalUn yields a stability advantage in high-variance
random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on
the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from
generating harmful images, SalUn achieves nearly 100% unlearning accuracy,
outperforming current state-of-the-art baselines like Erased Stable Diffusion
and Forget-Me-Not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chongyu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dennis Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13505">
<title>Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13505</link>
<description rdf:parseType="Literal">&lt;p&gt;Models for conversational question answering (ConvQA) over knowledge graphs
(KGs) are usually trained and tested on benchmarks of gold QA pairs. This
implies that training is limited to surface forms seen in the respective
datasets, and evaluation is on a small set of held-out questions. Through our
proposed framework REIGN, we take several steps to remedy this restricted
learning setup. First, we systematically generate reformulations of training
questions to increase robustness of models to surface form variations. This is
a particularly challenging problem, given the incomplete nature of such
questions. Second, we guide ConvQA models towards higher performance by feeding
it only those reformulations that help improve their answering quality, using
deep reinforcement learning. Third, we demonstrate the viability of training
major model components on one benchmark and applying them zero-shot to another.
Finally, for a rigorous evaluation of robustness for trained models, we use and
release large numbers of diverse reformulations generated by prompting GPT for
benchmark test sets (resulting in 20x increase in sizes). Our findings show
that ConvQA models with robust training via reformulations, significantly
outperform those with standard training from gold QA pairs only.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1&quot;&gt;Magdalena Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rishiraj Saha Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1&quot;&gt;Gerhard Weikum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13682">
<title>Optimizing Retrieval-augmented Reader Models via Token Elimination. (arXiv:2310.13682v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13682</link>
<description rdf:parseType="Literal">&lt;p&gt;Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model
applied across a variety of open-domain tasks, such as question answering, fact
checking, etc. In FiD, supporting passages are first retrieved and then
processed using a generative model (Reader), which can cause a significant
bottleneck in decoding time, particularly with long outputs. In this work, we
analyze the contribution and necessity of all the retrieved passages to the
performance of reader models, and propose eliminating some of the retrieved
information, at the token level, that might not contribute essential
information to the answer generation process. We demonstrate that our method
can reduce run-time by up to 62.2%, with only a 2% reduction in performance,
and in some cases, even improve the performance results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berchansky_M/0/1/0/all/0/1&quot;&gt;Moshe Berchansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1&quot;&gt;Peter Izsak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1&quot;&gt;Avi Caciularu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1&quot;&gt;Ido Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasserblat_M/0/1/0/all/0/1&quot;&gt;Moshe Wasserblat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14017">
<title>Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series. (arXiv:2310.14017v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14017</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive representation learning is crucial in medical time series
analysis as it alleviates dependency on labor-intensive, domain-specific, and
scarce expert annotations. However, existing contrastive learning methods
primarily focus on one single data level, which fails to fully exploit the
intricate nature of medical time series. To address this issue, we present
COMET, an innovative hierarchical framework that leverages data consistencies
at all inherent levels in medical time series. Our meticulously designed model
systematically captures data consistency from four potential levels:
observation, sample, trial, and patient levels. By developing contrastive loss
at multiple levels, we can learn effective representations that preserve
comprehensive data consistency, maximizing information utilization in a
self-supervised manner. We conduct experiments in the challenging
patient-independent setting. We compare COMET against six baselines using three
diverse datasets, which include ECG signals for myocardial infarction and EEG
signals for Alzheimer&apos;s and Parkinson&apos;s diseases. The results demonstrate that
COMET consistently outperforms all baselines, particularly in setup with 10%
and 1% labeled data fractions across all datasets. These results underscore the
significant impact of our framework in advancing contrastive representation
learning techniques for medical time series. The source code is available at
https://github.com/DL4mHealth/COMET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yihe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haishuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14421">
<title>On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14421</link>
<description rdf:parseType="Literal">&lt;p&gt;Simply-verifiable mathematical conditions for existence, uniqueness and
explicit analytical computation of minimal adversarial paths (MAP) and minimal
adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for
generalized linear models (GLM), and for entropic AI (EAI) are formulated and
proven. Practical computation of MAP and MAD, their comparison and
interpretations for various classes of AI tools (for neuronal networks, boosted
random forests, GLM and EAI) are demonstrated on the common synthetic
benchmarks: on a double Swiss roll spiral and its extensions, as well as on the
two biomedical data problems (for the health insurance claim predictions, and
for the heart attack lethality classification). On biomedical applications it
is demonstrated how MAP provides unique minimal patient-specific
risk-mitigating interventions in the predefined subsets of accessible control
variables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Horenko_I/0/1/0/all/0/1&quot;&gt;Illia Horenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14455">
<title>An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI. (arXiv:2310.14455v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14455</link>
<description rdf:parseType="Literal">&lt;p&gt;Given rapid progress toward advanced AI and risks from frontier AI systems
(advanced AI systems pushing the boundaries of the AI capabilities frontier),
the creation and implementation of AI governance and regulatory schemes
deserves prioritization and substantial investment. However, the status quo is
untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to
conduct research, development, and deployment activities with minimal
oversight. In response, frontier AI system evaluations have been proposed as a
way of assessing risks from the development and deployment of frontier AI
systems. Yet, the budding AI risk evaluation ecosystem faces significant
coordination challenges, such as a limited diversity of evaluators, suboptimal
allocation of effort, and perverse incentives. This paper proposes a solution
in the form of an international consortium for AI risk evaluations, comprising
both AI developers and third-party AI risk evaluators. Such a consortium could
play a critical role in international efforts to mitigate societal-scale risks
from advanced AI, including in managing responsible scaling policies and
coordinated evaluation-based risk response. In this paper, we discuss the
current evaluation ecosystem and its shortcomings, propose an international
consortium for advanced AI risk evaluations, discuss issues regarding its
implementation, discuss lessons that can be learnt from previous international
institutions and existing proposals for international AI governance
institutions, and, finally, we recommend concrete steps to advance the
establishment of the proposed consortium: (i) solicit feedback from
stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for
stakeholders, (iv) analyze feedback and create final proposal, (v) solicit
funding, and (vi) create a consortium.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gruetzemacher_R/0/1/0/all/0/1&quot;&gt;Ross Gruetzemacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1&quot;&gt;Alan Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frazier_K/0/1/0/all/0/1&quot;&gt;Kevin Frazier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1&quot;&gt;Christy Manning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Los_S/0/1/0/all/0/1&quot;&gt;&amp;#x160;t&amp;#x11b;p&amp;#xe1;n Los&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1&quot;&gt;James Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Hern&amp;#xe1;ndez-Orallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burden_J/0/1/0/all/0/1&quot;&gt;John Burden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franklin_M/0/1/0/all/0/1&quot;&gt;Matija Franklin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghuidhir_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xed;odhna N&amp;#xed; Ghuidhir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_M/0/1/0/all/0/1&quot;&gt;Mark Bailey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eth_D/0/1/0/all/0/1&quot;&gt;Daniel Eth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilditch_T/0/1/0/all/0/1&quot;&gt;Toby Pilditch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilian_K/0/1/0/all/0/1&quot;&gt;Kyle Kilian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14942">
<title>Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand. (arXiv:2310.14942v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14942</link>
<description rdf:parseType="Literal">&lt;p&gt;The prosperity of deep neural networks (DNNs) is largely benefited from
open-source datasets, based on which users can evaluate and improve their
methods. In this paper, we revisit backdoor-based dataset ownership
verification (DOV), which is currently the only feasible approach to protect
the copyright of open-source datasets. We reveal that these methods are
fundamentally harmful given that they could introduce malicious
misclassification behaviors to watermarked DNNs by the adversaries. In this
paper, we design DOV from another perspective by making watermarked models
(trained on the protected dataset) correctly classify some `hard&apos; samples that
will be misclassified by the benign model. Our method is inspired by the
generalization property of DNNs, where we find a \emph{hardly-generalized
domain} for the original dataset (as its \emph{domain watermark}). It can be
easily learned with the protected dataset containing modified samples.
Specifically, we formulate the domain generation as a bi-level optimization and
propose to optimize a set of visually-indistinguishable clean-label modified
data with similar effects to domain-watermarked samples from the
hardly-generalized domain to ensure watermark stealthiness. We also design a
hypothesis-test-guided ownership verification via our domain watermark and
provide the theoretical analyses of our method. Extensive experiments on three
benchmark datasets are conducted, which verify the effectiveness of our method
and its resistance to potential adaptive methods. The code for reproducing main
experiments is available at
\url{https://github.com/JunfengGo/Domain-Watermark}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Junfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lixu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15177">
<title>A Neuro-mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization. (arXiv:2310.15177v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15177</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last few years, large neural generative models, capable of
synthesizing semantically rich passages of text or producing complex images,
have recently emerged as a popular representation of what has come to be known
as ``generative artificial intelligence&apos;&apos; (generative AI). Beyond opening the
door to new opportunities as well as challenges for the domain of statistical
machine learning, the rising popularity of generative AI brings with it
interesting questions for Cognitive Science, which seeks to discover the nature
of the processes that underpin minds and brains as well as to understand how
such functionality might be acquired and instantianted in biological (or
artificial) substrate. With this goal in mind, we argue that a promising
research program lies in the crafting of cognitive architectures, a
long-standing tradition of the field, cast fundamentally in terms of
neuro-mimetic generative building blocks. Concretely, we discuss the COGnitive
Neural GENerative system, such an architecture that casts the Common Model of
Cognition in terms of Hebbian adaptation operating in service of optimizing a
variational free energy functional.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ororbia_A/0/1/0/all/0/1&quot;&gt;Alexander Ororbia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kelly_M/0/1/0/all/0/1&quot;&gt;Mary Alexandria Kelly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16787">
<title>The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing &amp; Attribution in AI. (arXiv:2310.16787v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16787</link>
<description rdf:parseType="Literal">&lt;p&gt;The race to train language models on vast, diverse, and inconsistently
documented datasets has raised pressing concerns about the legal and ethical
risks for practitioners. To remedy these practices threatening data
transparency and understanding, we convene a multi-disciplinary effort between
legal and machine learning experts to systematically audit and trace 1800+ text
datasets. We develop tools and standards to trace the lineage of these
datasets, from their source, creators, series of license conditions,
properties, and subsequent use. Our landscape analysis highlights the sharp
divides in composition and focus of commercially open vs closed datasets, with
closed datasets monopolizing important categories: lower resource languages,
more creative tasks, richer topic variety, newer and more synthetic training
data. This points to a deepening divide in the types of data that are made
available under different license conditions, and heightened implications for
jurisdictional legal interpretations of copyright and fair use. We also observe
frequent miscategorization of licenses on widely used dataset hosting sites,
with license omission of 70%+ and error rates of 50%+. This points to a crisis
in misattribution and informed use of the most popular datasets driving many
recent breakthroughs. As a contribution to ongoing improvements in dataset
transparency and responsible use, we release our entire audit, with an
interactive UI, the Data Provenance Explorer, which allows practitioners to
trace and filter on data provenance for the most popular open source finetuning
data collections: www.dataprovenance.org.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1&quot;&gt;Shayne Longpre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1&quot;&gt;Robert Mahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anthony Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obeng_Marnu_N/0/1/0/all/0/1&quot;&gt;Naana Obeng-Marnu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sileo_D/0/1/0/all/0/1&quot;&gt;Damien Sileo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brannon_W/0/1/0/all/0/1&quot;&gt;William Brannon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khazam_N/0/1/0/all/0/1&quot;&gt;Nathan Khazam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabbara_J/0/1/0/all/0/1&quot;&gt;Jad Kabbara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1&quot;&gt;Kartik Perisetla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shippole_E/0/1/0/all/0/1&quot;&gt;Enrico Shippole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bollacker_K/0/1/0/all/0/1&quot;&gt;Kurt Bollacker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villa_L/0/1/0/all/0/1&quot;&gt;Luis Villa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pentland_S/0/1/0/all/0/1&quot;&gt;Sandy Pentland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1&quot;&gt;Sara Hooker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18919">
<title>Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18919</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies in reinforcement learning (RL) have made significant progress
by leveraging function approximation to alleviate the sample complexity hurdle
for better performance. Despite the success, existing provably efficient
algorithms typically rely on the accessibility of immediate feedback upon
taking actions. The failure to account for the impact of delay in observations
can significantly degrade the performance of real-world systems due to the
regret blow-up. In this work, we tackle the challenge of delayed feedback in RL
with linear function approximation by employing posterior sampling, which has
been shown to empirically outperform the popular UCB algorithms in a wide range
of regimes. We first introduce Delayed-PSVI, an optimistic value-based
algorithm that effectively explores the value function space via noise
perturbation with posterior sampling. We provide the first analysis for
posterior sampling algorithms with delayed feedback in RL and show our
algorithm achieves $\widetilde{O}(\sqrt{d^3H^3 T} + d^2H^2 E[\tau])$ worst-case
regret in the presence of unknown stochastic delays. Here $E[\tau]$ is the
expected delay. To further improve its computational efficiency and to expand
its applicability in high-dimensional RL problems, we incorporate a
gradient-based approximate sampling scheme via Langevin dynamics for
Delayed-LPSVI, which maintains the same order-optimal regret guarantee with
$\widetilde{O}(dHK)$ computational cost. Empirical evaluations are performed to
demonstrate the statistical and computational efficacy of our algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_N/0/1/0/all/0/1&quot;&gt;Nikki Lijing Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Ming Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi-An Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18987">
<title>Path Analysis for Effective Fault Localization in Deep Neural Networks. (arXiv:2310.18987v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18987</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has revolutionized various real-world applications, but the
quality of Deep Neural Networks (DNNs) remains a concern. DNNs are complex and
have millions of parameters, making it difficult to determine their
contributions to fulfilling a task. Moreover, the behavior of a DNN is highly
influenced by the data used during training, making it challenging to collect
enough data to exercise all potential DNN behavior under all possible
scenarios. This paper proposes NP SBFL method to locate faulty neural pathways
(NP) using spectrum-based fault localization (SBFL). Our method identifies
critical neurons using the layer-wise relevance propagation (LRP) technique and
determines which critical neurons are faulty. Moreover, we propose a
multi-stage gradient ascent (MGA), an extension of gradient ascent (GA), to
effectively activate a sequence of neurons one at a time while maintaining the
activation of previous neurons, so we are able to test the reported faulty
pathways. We evaluated the effectiveness of our method, i.e. NP-SBFL-MGA, on
two commonly used datasets, MNIST and CIFAR-10, two baselines DeepFault and
NP-SBFL-GA, and three suspicious neuron measures, Tarantula, Ochiai, and
Barinel. The empirical results showed that NP-SBFL-MGA is statistically more
effective than the baselines at identifying suspicious paths and synthesizing
adversarial inputs. Particularly, Tarantula on NP-SBFL-MGA had the highest
fault detection rate at 96.75%, surpassing DeepFault on Ochiai (89.90%) and
NP-SBFL-GA on Ochiai (60.61%). Our approach also yielded comparable results to
the baselines in synthesizing naturalness inputs, and we found a positive
correlation between the coverage of critical paths and the number of failed
tests in DNN fault localization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemifar_S/0/1/0/all/0/1&quot;&gt;Soroush Hashemifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsa_S/0/1/0/all/0/1&quot;&gt;Saeed Parsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalaee_A/0/1/0/all/0/1&quot;&gt;Akram Kalaee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19776">
<title>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19776</link>
<description rdf:parseType="Literal">&lt;p&gt;In the quest for unveiling novel categories at test time, we confront the
inherent limitations of traditional supervised recognition models that are
restricted by a predefined category set. While strides have been made in the
realms of self-supervised and open-world learning towards test-time category
discovery, a crucial yet often overlooked question persists: what exactly
delineates a category? In this paper, we conceptualize a category through the
lens of optimization, viewing it as an optimal solution to a well-defined
problem. Harnessing this unique conceptualization, we propose a novel,
efficient and self-supervised method capable of discovering previously unknown
categories at test time. A salient feature of our approach is the assignment of
minimum length category codes to individual data instances, which encapsulates
the implicit category hierarchy prevalent in real-world datasets. This
mechanism affords us enhanced control over category granularity, thereby
equipping our model to handle fine-grained categories adeptly. Experimental
evaluations, bolstered by state-of-the-art benchmark comparisons, testify to
the efficacy of our solution in managing unknown categories at test time.
Furthermore, we fortify our proposition with a theoretical foundation,
providing proof of its optimality. Our code is available at
https://github.com/SarahRastegar/InfoSieve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegar_S/0/1/0/all/0/1&quot;&gt;Sarah Rastegar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19975">
<title>BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19975</link>
<description rdf:parseType="Literal">&lt;p&gt;To enhance the performance of large language models (LLMs) in biomedical
natural language processing (BioNLP) by introducing a domain-specific
instruction dataset and examining its impact when combined with multi-task
learning principles. We created the BioInstruct, comprising 25,005 instructions
to instruction-tune LLMs(LLaMA 1 &amp;amp; 2, 7B &amp;amp; 13B version). The instructions were
created by prompting the GPT-4 language model with three-seed samples randomly
drawn from an 80 human curated instructions. We employed Low-Rank
Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these
instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three
major categories: question answering(QA), information extraction(IE), and text
generation(GEN). We also examined whether categories(e.g., QA, IE, and
generation) of instructions impact model performance. Comparing with LLMs
without instruction-tuned, our instruction-tuned LLMs demonstrated marked
performance gains: 17.3% in QA, 5.7% in IE, and 96% in Generation tasks. Our
7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed
other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with
vast domain-specific data or a variety of tasks. Our results also show that the
performance gain is significantly higher when instruction fine-tuning is
conducted with closely related tasks. Our findings align with the observations
of multi-task learning, suggesting the synergies between two tasks. The
BioInstruct dataset serves as a valuable resource and instruction tuned LLMs
lead to the best performing BioNLP applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1&quot;&gt;Hieu Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhichao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zonghai Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20195">
<title>Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20195</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to process idiomatic or literal multiword expressions is a
crucial aspect of understanding and generating any language. The task of
generating contextually relevant continuations for narratives containing
idiomatic (or literal) expressions can allow us to test the ability of
generative language models (LMs) in understanding nuanced language containing
non-compositional figurative text. We conduct a series of experiments using
datasets in two distinct languages (English and Portuguese) under three
different training settings (zero-shot, few-shot, and fine-tuned). Our results
suggest that the models are only slightly better at generating continuations
for literal contexts than idiomatic contexts, with exceedingly small margins.
Furthermore, the models studied in this work perform equally well across both
languages, indicating the robustness of generative models in performing this
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pokharel_R/0/1/0/all/0/1&quot;&gt;Rhitabrat Pokharel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Ameeta Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20327">
<title>Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20327</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain shift is a common problem in the realistic world, where training data
and test data follow different data distributions. To deal with this problem,
fully test-time adaptation (TTA) leverages the unlabeled data encountered
during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA)
methods, which minimize the prediction&apos;s entropy on test samples, have shown
great success. In this paper, we introduce a new perspective on the EBTTA,
which interprets these methods from a view of clustering. It is an iterative
algorithm: 1) in the assignment step, the forward process of the EBTTA models
is the assignment of labels for these test samples, and 2) in the updating
step, the backward process is the update of the model via the assigned samples.
Based on the interpretation, we can gain a deeper understanding of EBTTA, where
we show that the entropy loss would further increase the largest probability.
Accordingly, we offer an alternative explanation for why existing EBTTA methods
are sensitive to initial assignments, outliers, and batch size. This
observation can guide us to put forward the improvement of EBTTA. We propose
robust label assignment, weight adjustment, and gradient accumulation to
alleviate the above problems. Experimental results demonstrate that our method
can achieve consistent improvements on various datasets. Code is provided in
the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guoliang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hanjiang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jian Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20381">
<title>A Comprehensive Study of GPT-4V&apos;s Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20381</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive evaluation of GPT-4V&apos;s capabilities
across diverse medical imaging tasks, including Radiology Report Generation,
Medical Visual Question Answering (VQA), and Visual Grounding. While prior
efforts have explored GPT-4V&apos;s performance in medical image analysis, to the
best of our knowledge, our study represents the first quantitative evaluation
on publicly available benchmarks. Our findings highlight GPT-4V&apos;s potential in
generating descriptive reports for chest X-ray images, particularly when guided
by well-structured prompts. Meanwhile, its performance on the MIMIC-CXR dataset
benchmark reveals areas for improvement in certain evaluation metrics, such as
CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in
distinguishing between question types but falls short of the VQA-RAD benchmark
in terms of accuracy. Furthermore, our analysis finds the limitations of
conventional evaluation metrics like the BLEU scores, advocating for the
development of more semantically robust assessment methods. In the field of
Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding
boxes, but its precision is lacking, especially in identifying specific medical
organs and signs. Our evaluation underscores the significant potential of
GPT-4V in the medical imaging domain, while also emphasizing the need for
targeted refinements to fully unlock its capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xinyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Leyang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00447">
<title>On the Opportunities of Green Computing: A Survey. (arXiv:2311.00447v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00447</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) has achieved significant advancements in
technology and research with the development over several decades, and is
widely used in many areas including computing vision, natural language
processing, time-series analysis, speech synthesis, etc. During the age of deep
learning, especially with the arise of Large Language Models, a large majority
of researchers&apos; attention is paid on pursuing new state-of-the-art (SOTA)
results, resulting in ever increasing of model size and computational
complexity. The needs for high computing power brings higher carbon emission
and undermines research fairness by preventing small or medium-sized research
institutions and companies with limited funding in participating in research.
To tackle the challenges of computing resources and environmental impact of AI,
Green Computing has become a hot research topic. In this survey, we give a
systematic overview of the technologies used in Green Computing. We propose the
framework of Green Computing and devide it into four key components: (1)
Measures of Greenness, (2) Energy-Efficient AI, (3) Energy-Efficient Computing
Systems and (4) AI Use Cases for Sustainability. For each components, we
discuss the research progress made and the commonly used techniques to optimize
the AI efficiency. We conclude that this new research direction has the
potential to address the conflicts between resource constraints and AI
development. We encourage more researchers to put attention on this direction
and make AI more environmental friendly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiujing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Maolin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Gangwei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huakang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yupeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kehang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yongduo Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fengwei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zuoli Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tiannuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weibo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yunong Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_D/0/1/0/all/0/1&quot;&gt;De Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hongrui Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingwen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinchi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+WEI_Y/0/1/0/all/0/1&quot;&gt;Ying WEI&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Hong Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kin_W/0/1/0/all/0/1&quot;&gt;Wai Kin&lt;/a&gt; (Victor) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan/0/1/0/all/0/1&quot;&gt;Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yusen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shiyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jining Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1&quot;&gt;Chao Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shuai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wuxia Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guannan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiaodong Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00634">
<title>A Bi-level Framework for Traffic Accident Duration Prediction: Leveraging Weather and Road Condition Data within a Practical Optimum Pipeline. (arXiv:2311.00634v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00634</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the stochastic nature of events, predicting the duration of a traffic
incident presents a formidable challenge. Accurate duration estimation can
result in substantial advantages for commuters in selecting optimal routes and
for traffic management personnel in addressing non-recurring congestion issues.
In this study, we gathered accident duration, road conditions, and
meteorological data from a database of traffic accidents to check the
feasibility of a traffic accident duration pipeline without accident contextual
information data like accident severity and textual description. Multiple
machine learning models were employed to predict whether an accident&apos;s impact
on road traffic would be of a short-term or long-term nature, and then
utilizing a bimodal approach the precise duration of the incident&apos;s effect was
determined. Our binary classification random forest model distinguished between
short-term and long-term effects with an 83% accuracy rate, while the LightGBM
regression model outperformed other machine learning regression models with
Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and
28.91 for short and long-term accident duration prediction, respectively. Using
the optimal classification and regression model identified in the preceding
section, we then construct an end-to-end pipeline to incorporate the entire
process. The results of both separate and combined approaches were comparable
with previous works, which shows the applicability of only using static
features for predicting traffic accident duration. The SHAP value analysis
identified weather conditions, wind chill and wind speed as the most
influential factors in determining the duration of an accident.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukonna_R/0/1/0/all/0/1&quot;&gt;Rafat Tabassum Sukonna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swapnil_S/0/1/0/all/0/1&quot;&gt;Soham Irtiza Swapnil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01301">
<title>TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01301</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid digitization of real-world data offers an unprecedented opportunity
for optimizing healthcare delivery and accelerating biomedical discovery. In
practice, however, such data is most abundantly available in unstructured
forms, such as clinical notes in electronic medical records (EMRs), and it is
generally plagued by confounders. In this paper, we present TRIALSCOPE, a
unifying framework for distilling real-world evidence from population-level
observational data. TRIALSCOPE leverages biomedical language models to
structure clinical text at scale, employs advanced probabilistic modeling for
denoising and imputation, and incorporates state-of-the-art causal inference
techniques to combat common confounders. Using clinical trial specification as
generic representation, TRIALSCOPE provides a turn-key solution to generate and
reason with clinical hypotheses using observational data. In extensive
experiments and analyses on a large-scale real-world dataset with over one
million cancer patients from a large US healthcare network, we show that
TRIALSCOPE can produce high-quality structuring of real-world data and
generates comparable results to marquee cancer trials. In addition to
facilitating in-silicon clinical trial design and optimization, TRIALSCOPE may
be used to empower synthetic controls, pragmatic trials, post-market
surveillance, as well as support fine-grained patient-like-me reasoning in
precision diagnosis and treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Javier Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Cliff Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gero_Z/0/1/0/all/0/1&quot;&gt;Zelalem Gero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagga_J/0/1/0/all/0/1&quot;&gt;Jass Bagga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ueno_R/0/1/0/all/0/1&quot;&gt;Risa Ueno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_I/0/1/0/all/0/1&quot;&gt;Isabel Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oravkin_E/0/1/0/all/0/1&quot;&gt;Eduard Oravkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1&quot;&gt;Emre Kiciman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1&quot;&gt;Aditya Nori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerasinghe_R/0/1/0/all/0/1&quot;&gt;Roshanthi Weerasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leidner_R/0/1/0/all/0/1&quot;&gt;Rom S. Leidner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piening_B/0/1/0/all/0/1&quot;&gt;Brian Piening&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1&quot;&gt;Tristan Naumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bifulco_C/0/1/0/all/0/1&quot;&gt;Carlo Bifulco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1&quot;&gt;Hoifung Poon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01378">
<title>Vision-Language Foundation Models as Effective Robot Imitators. (arXiv:2311.01378v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01378</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in vision language foundation models has shown their ability
to understand multimodal data and resolve complicated vision language tasks,
including robotics manipulation. We seek a straightforward way of making use of
existing vision-language models (VLMs) with simple fine-tuning on robotics
data. To this end, we derive a simple and novel vision-language manipulation
framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo.
Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step
vision-language comprehension, models sequential history information with an
explicit policy head, and is slightly fine-tuned by imitation learning only on
language-conditioned manipulation datasets. Such a decomposition provides
RoboFlamingo the flexibility for open-loop control and deployment on
low-performance platforms. By exceeding the state-of-the-art performance with a
large margin on the tested benchmark, we show RoboFlamingo can be an effective
and competitive alternative to adapt VLMs to robot control. Our extensive
experimental results also reveal several interesting conclusions regarding the
behavior of different pre-trained VLMs on manipulation tasks. We believe
RoboFlamingo has the potential to be a cost-effective and easy-to-use solution
for robotics manipulation, empowering everyone with the ability to fine-tune
their own robotics policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinghang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cunjun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongtao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1&quot;&gt;Chilam Cheang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1&quot;&gt;Ya Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1&quot;&gt;Tao Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01723">
<title>Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01723</link>
<description rdf:parseType="Literal">&lt;p&gt;While fine-tuning unlocks the potential of a pre-trained model for a specific
task, it compromises the model&apos;s ability to generalize to out-of-distribution
(OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance
on OOD datasets as well as on an in-distribution (ID) dataset for which the
model is being tuned. However, another criterion for reliable machine learning
(ML), confidence calibration, has been overlooked despite its increasing demand
for real-world high-stakes ML applications (e.g., autonomous driving and
medical diagnosis). For the first time, we raise concerns about the calibration
of fine-tuned vision-language models (VLMs) under distribution shift by showing
that naive fine-tuning and even state-of-the-art robust fine-tuning methods
hurt the calibration of pre-trained VLMs, especially on OOD datasets. To
address this issue, we provide a simple approach, called calibrated robust
fine-tuning (CaRot), that incentivizes calibration and robustness on both ID
and OOD datasets. Empirical results on ImageNet-1K distribution shift
evaluation verify the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mijoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyesu Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1&quot;&gt;Euiseog Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01977">
<title>RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches. (arXiv:2311.01977v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01977</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization remains one of the most important desiderata for robust robot
learning systems. While recently proposed approaches show promise in
generalization to novel objects, semantic concepts, or visual distribution
shifts, generalization to new tasks remains challenging. For example, a
language-conditioned policy trained on pick-and-place tasks will not be able to
generalize to a folding task, even if the arm trajectory of folding is similar
to pick-and-place. Our key insight is that this kind of generalization becomes
feasible if we represent the task through rough trajectory sketches. We propose
a policy conditioning method using such rough trajectory sketches, which we
call RT-Trajectory, that is practical, easy to specify, and allows the policy
to effectively perform new tasks that would otherwise be challenging to
perform. We find that trajectory sketches strike a balance between being
detailed enough to express low-level motion-centric guidance while being coarse
enough to allow the learned policy to interpret the trajectory sketch in the
context of situational visual observations. In addition, we show how trajectory
sketches can provide a useful interface to communicate with robotic policies:
they can be specified through simple human inputs like drawings or videos, or
through automated methods such as modern image-generating or
waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of
real-world robotic tasks, and find that RT-Trajectory is able to perform a
wider range of tasks compared to language-conditioned and goal-conditioned
policies, when provided the same training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiayuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirmani_S/0/1/0/all/0/1&quot;&gt;Sean Kirmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlhart_P/0/1/0/all/0/1&quot;&gt;Paul Wohlhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1&quot;&gt;Montserrat Gonzalez Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chuyuan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1&quot;&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>