<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.02779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.05764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08383" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.08579">
<title>Curve-based Neural Style Transfer. (arXiv:2401.08579v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08579</link>
<description rdf:parseType="Literal">&lt;p&gt;This research presents a new parametric style transfer framework specifically
designed for curve-based design sketches. In this research, traditional
challenges faced by neural style transfer methods in handling binary sketch
transformations are effectively addressed through the utilization of parametric
shape-editing rules, efficient curve-to-pixel conversion techniques, and the
fine-tuning of VGG19 on ImageNet-Sketch, enhancing its role as a feature
pyramid network for precise style extraction. By harmonizing intuitive
curve-based imagery with rule-based editing, this study holds the potential to
significantly enhance design articulation and elevate the practice of style
transfer within the realm of product design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-hsuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kara_L/0/1/0/all/0/1&quot;&gt;Levent Burak Kara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagan_J/0/1/0/all/0/1&quot;&gt;Jonathan Cagan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08581">
<title>Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision. (arXiv:2401.08581v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08581</link>
<description rdf:parseType="Literal">&lt;p&gt;There exists a correlation between geospatial activity temporal patterns and
type of land use. A novel self-supervised approach is proposed to stratify
landscape based on mobility activity time series. First, the time series signal
is transformed to the frequency domain and then compressed into task-agnostic
temporal embeddings by a contractive autoencoder, which preserves cyclic
temporal patterns observed in time series. The pixel-wise embeddings are
converted to image-like channels that can be used for task-based, multimodal
modeling of downstream geospatial tasks using deep semantic segmentation.
Experiments show that temporal embeddings are semantically meaningful
representations of time series data and are effective across different tasks
such as classifying residential area and commercial areas. Temporal embeddings
transform sequential, spatiotemporal motion trajectory data into semantically
meaningful image-like tensor representations that can be combined (multimodal
fusion) with other data modalities that are or can be transformed into
image-like tensor representations (for e.g., RBG imagery, graph embeddings of
road networks, passively collected imagery like SAR, etc.) to facilitate
multimodal learning in geospatial computer vision. Multimodal computer vision
is critical for training machine learning models for geospatial feature
detection to keep a geospatial mapping service up-to-date in real-time and can
significantly improve user experience and above all, user safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1&quot;&gt;Swetava Ganguli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1&quot;&gt;Vipul Pandey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08584">
<title>Nahid: AI-based Algorithm for operating fully-automatic surgery. (arXiv:2401.08584v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08584</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, for the first time, a method is presented that can provide a
fully automated surgery based on software and computer vision techniques. Then,
the advantages and challenges of computerization of medical surgery are
examined. Finally, the surgery related to isolated ovarian endometriosis
disease has been examined, and based on the presented method, a more detailed
algorithm is presented that is capable of automatically diagnosing and treating
this disease during surgery as proof of our proposed method where a U-net is
trained to detect the endometriosis during surgery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saadati_S/0/1/0/all/0/1&quot;&gt;Sina Saadati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08585">
<title>From Conceptual Spaces to Quantum Concepts: Formalising and Learning Structured Conceptual Models. (arXiv:2401.08585v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2401.08585</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article we present a new modelling framework for structured concepts
using a category-theoretic generalisation of conceptual spaces, and show how
the conceptual representations can be learned automatically from data, using
two very different instantiations: one classical and one quantum. A
contribution of the work is a thorough category-theoretic formalisation of our
framework. We claim that the use of category theory, and in particular the use
of string diagrams to describe quantum processes, helps elucidate some of the
most important features of our approach. We build upon Gardenfors&apos; classical
framework of conceptual spaces, in which cognition is modelled geometrically
through the use of convex spaces, which in turn factorise in terms of simpler
spaces called domains. We show how concepts from the domains of shape, colour,
size and position can be learned from images of simple shapes, where concepts
are represented as Gaussians in the classical implementation, and quantum
effects in the quantum one. In the classical case we develop a new model which
is inspired by the Beta-VAE model of concepts, but is designed to be more
closely connected with language, so that the names of concepts form part of the
graphical model. In the quantum case, concepts are learned by a hybrid
classical-quantum network trained to perform concept classification, where the
classical image processing is carried out by a convolutional neural network and
the quantum representations are produced by a parameterised quantum circuit.
Finally, we consider the question of whether our quantum models of concepts can
be considered conceptual spaces in the Gardenfors sense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tull_S/0/1/0/all/0/1&quot;&gt;Sean Tull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shaikh_R/0/1/0/all/0/1&quot;&gt;Razin A. Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zemljic_S/0/1/0/all/0/1&quot;&gt;Sara Sabrina Zemljic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Clark_S/0/1/0/all/0/1&quot;&gt;Stephen Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08587">
<title>Automatic extraction and 3D reconstruction of split wire from point cloud data based on improved DPC algorithm. (arXiv:2401.08587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08587</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to solve the problem of point cloud data splitting improved by DPC
algorithm, a research on automatic separation and 3D reconstruction of point
cloud data split lines is proposed. First, the relative coordinates of each
point in the cloud point are calculated. Second, it is planned to develop a
relative ensemble-based DPC swarm algorithm for analyzing the number of
separation lines to determine all parts in the cloud content. Finally, fit each
separator using the least squares method. iron. The cloud point of the
resulting split subconductors has a clear demarcation line, and the distance
between adjacent split subconductors is 0.45 m, divided by the four vertices of
the square.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jia Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08604">
<title>SAM4UDASS: When SAM Meets Unsupervised Domain Adaptive Semantic Segmentation in Intelligent Vehicles. (arXiv:2401.08604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08604</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation plays a critical role in enabling intelligent vehicles
to comprehend their surrounding environments. However, deep learning-based
methods usually perform poorly in domain shift scenarios due to the lack of
labeled data for training. Unsupervised domain adaptation (UDA) techniques have
emerged to bridge the gap across different driving scenes and enhance model
performance on unlabeled target environments. Although self-training UDA
methods have achieved state-of-the-art results, the challenge of generating
precise pseudo-labels persists. These pseudo-labels tend to favor majority
classes, consequently sacrificing the performance of rare classes or small
objects like traffic lights and signs. To address this challenge, we introduce
SAM4UDASS, a novel approach that incorporates the Segment Anything Model (SAM)
into self-training UDA methods for refining pseudo-labels. It involves
Semantic-Guided Mask Labeling, which assigns semantic labels to unlabeled SAM
masks using UDA pseudo-labels. Furthermore, we devise fusion strategies aimed
at mitigating semantic granularity inconsistency between SAM masks and the
target domain. SAM4UDASS innovatively integrate SAM with UDA for semantic
segmentation in driving scenes and seamlessly complements existing
self-training UDA methodologies. Extensive experiments on synthetic-to-real and
normal-to-adverse driving datasets demonstrate its effectiveness. It brings
more than 3% mIoU gains on GTA5-to-Cityscapes, SYNTHIA-to-Cityscapes, and
Cityscapes-to-ACDC when using DAFormer and achieves SOTA when using MIC. The
code will be available at https://github.com/ywher/SAM4UDASS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Weihao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yeqiang Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Hanyang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08619">
<title>MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor. (arXiv:2401.08619v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08619</link>
<description rdf:parseType="Literal">&lt;p&gt;An accurate binding affinity prediction between T-cell receptors and epitopes
contributes decisively to develop successful immunotherapy strategies. Some
state-of-the-art computational methods implement deep learning techniques by
integrating evolutionary features to convert the amino acid residues of cell
receptors and epitope sequences into numerical values, while some other methods
employ pre-trained language models to summarize the embedding vectors at the
amino acid residue level to obtain sequence-wise representations.
&lt;/p&gt;
&lt;p&gt;Here, we propose a highly reliable novel method, MATE-Pred, that performs
multi-modal attention-based prediction of T-cell receptors and epitopes binding
affinity. The MATE-Pred is compared and benchmarked with other deep learning
models that leverage multi-modal representations of T-cell receptors and
epitopes. In the proposed method, the textual representation of proteins is
embedded with a pre-trained bi-directional encoder model and combined with two
additional modalities: a) a comprehensive set of selected physicochemical
properties; b) predicted contact maps that estimate the 3D distances between
amino acid residues in the sequences.
&lt;/p&gt;
&lt;p&gt;The MATE-Pred demonstrates the potential of multi-modal model in achieving
state-of-the-art performance (+8.4\% MCC, +5.5\% AUC compared to baselines) and
efficiently capturing contextual, physicochemical, and structural information
from amino acid residues. The performance of MATE-Pred projects its potential
application in various drug discovery regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goffinet_E/0/1/0/all/0/1&quot;&gt;Etienne Goffinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mall_R/0/1/0/all/0/1&quot;&gt;Raghvendra Mall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaushik_R/0/1/0/all/0/1&quot;&gt;Rahul Kaushik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castiglione_F/0/1/0/all/0/1&quot;&gt;Filippo Castiglione&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08623">
<title>Wake-Sleep Consolidated Learning. (arXiv:2401.08623v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.08623</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy
leveraging Complementary Learning System theory and the wake-sleep phases of
the human brain to improve the performance of deep neural networks for visual
classification tasks in continual learning settings. Our method learns
continually via the synchronization between distinct wake and sleep phases.
During the wake phase, the model is exposed to sensory input and adapts its
representations, ensuring stability through a dynamic parameter freezing
mechanism and storing episodic memories in a short-term temporary memory
(similarly to what happens in the hippocampus). During the sleep phase, the
training process is split into NREM and REM stages. In the NREM stage, the
model&apos;s synaptic weights are consolidated using replayed samples from the
short-term and long-term memory and the synaptic plasticity mechanism is
activated, strengthening important connections and weakening unimportant ones.
In the REM stage, the model is exposed to previously-unseen realistic visual
sensory experience, and the dreaming process is activated, which enables the
model to explore the potential feature space, thus preparing synapses to future
knowledge. We evaluate the effectiveness of our approach on three benchmark
datasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method
outperforms the baselines and prior work, yielding a significant performance
gain on continual visual classification tasks. Furthermore, we demonstrate the
usefulness of all processing stages and the importance of dreaming to enable
positive forward transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorrenti_A/0/1/0/all/0/1&quot;&gt;Amelia Sorrenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellitto_G/0/1/0/all/0/1&quot;&gt;Giovanni Bellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salanitri_F/0/1/0/all/0/1&quot;&gt;Federica Proietto Salanitri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pennisi_M/0/1/0/all/0/1&quot;&gt;Matteo Pennisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1&quot;&gt;Simone Palazzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08632">
<title>Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning. (arXiv:2401.08632v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.08632</link>
<description rdf:parseType="Literal">&lt;p&gt;A fundamental trait of intelligence involves finding novel and creative
solutions to address a given challenge or to adapt to unforeseen situations.
Reflecting this, Quality-Diversity optimization is a family of Evolutionary
Algorithms, that generates collections of both diverse and high-performing
solutions. Among these, MAP-Elites is a prominent example, that has been
successfully applied to a variety of domains, including evolutionary robotics.
However, MAP-Elites performs a divergent search with random mutations
originating from Genetic Algorithms, and thus, is limited to evolving
populations of low-dimensional solutions. PGA-MAP-Elites overcomes this
limitation using a gradient-based variation operator inspired by deep
reinforcement learning which enables the evolution of large neural networks.
Although high-performing in many environments, PGA-MAP-Elites fails on several
tasks where the convergent search of the gradient-based variation operator
hinders diversity. In this work, we present three contributions: (1) we enhance
the Policy Gradient variation operator with a descriptor-conditioned critic
that reconciles diversity search with gradient-based methods, (2) we leverage
the actor-critic training to learn a descriptor-conditioned policy at no
additional cost, distilling the knowledge of the population into one single
versatile policy that can execute a diversity of behaviors, (3) we exploit the
descriptor-conditioned actor by injecting it in the population, despite network
architecture differences. Our method, DCG-MAP-Elites, achieves equal or higher
QD score and coverage compared to all baselines on seven challenging continuous
control locomotion tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faldor_M/0/1/0/all/0/1&quot;&gt;Maxence Faldor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalumeau_F/0/1/0/all/0/1&quot;&gt;F&amp;#xe9;lix Chalumeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flageat_M/0/1/0/all/0/1&quot;&gt;Manon Flageat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cully_A/0/1/0/all/0/1&quot;&gt;Antoine Cully&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08636">
<title>MLCommons Cloud Masking Benchmark with Early Stopping. (arXiv:2401.08636v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.08636</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we report on work performed for the MLCommons Science Working
Group on the cloud masking benchmark. MLCommons is a consortium that develops
and maintains several scientific benchmarks that aim to benefit developments in
AI. The benchmarks are conducted on the High Performance Computing (HPC)
Clusters of New York University and University of Virginia, as well as a
commodity desktop. We provide a description of the cloud masking benchmark, as
well as a summary of our submission to MLCommons on the benchmark experiment we
conducted. It includes a modification to the reference implementation of the
cloud masking benchmark enabling early stopping. This benchmark is executed on
the NYU HPC through a custom batch script that runs the various experiments
through the batch queuing system while allowing for variation on the number of
epochs trained. Our submission includes the modified code, a custom batch
script to modify epochs, documentation, and the benchmark results. We report
the highest accuracy (scientific metric) and the average time taken
(performance metric) for training and inference that was achieved on NYU HPC
Greene. We also provide a comparison of the compute capabilities between
different systems by running the benchmark for one epoch. Our submission can be
found in a Globus repository that is accessible to MLCommons Science Working
Group.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chennamsetti_V/0/1/0/all/0/1&quot;&gt;Varshitha Chennamsetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszewski_G/0/1/0/all/0/1&quot;&gt;Gregor von Laszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1&quot;&gt;Ruochen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehnaz_L/0/1/0/all/0/1&quot;&gt;Laiba Mehnaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papay_J/0/1/0/all/0/1&quot;&gt;Juri Papay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jackson_S/0/1/0/all/0/1&quot;&gt;Samuel Jackson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiyagalingam_J/0/1/0/all/0/1&quot;&gt;Jeyan Thiyagalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samsonau_S/0/1/0/all/0/1&quot;&gt;Sergey V. Samsonau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1&quot;&gt;Geoffrey C. Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08655">
<title>SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation is challenging due to the scarcity of
large-scale visual-audio datasets despite extensive research. Most prior works,
typically focused on learning regression models on a small dataset using the
method of least squares, encounter difficulties generating diverse lip
movements from speech and require substantial effort in refining the generated
outputs. To address these issues, we propose a speech-driven 3D facial
animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net
with a cross-modality alignment bias between audio and visual to enhance lip
synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs
of speech audio and parameters of a blendshape facial model, to address the
scarcity of public resources. Our experimental results demonstrate that the
proposed approach achieves comparable or superior performance in lip
synchronization to baselines, ensures more diverse lip movements, and
streamlines the animation editing process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1&quot;&gt;Inkyu Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08658">
<title>End-To-End Planning of Autonomous Driving in Industry and Academia: 2022-2023. (arXiv:2401.08658v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.08658</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to provide a quick review of the methods including the
technologies in detail that are currently reported in industry and academia.
Specifically, this paper reviews the end-to-end planning, including Tesla FSD
V12, Momenta 2023, Horizon Robotics 2023, Motional RoboTaxi 2022, Woven Planet
(Toyota): Urban Driver, and Nvidia. In addition, we review the state-of-the-art
academic studies that investigate end-to-end planning of autonomous driving.
This paper provides readers with a concise structure and fast learning of
state-of-the-art end-to-end planning for 2022-2023. This article provides a
meaningful overview as introductory material for beginners to follow the
state-of-the-art end-to-end planning of autonomous driving in industry and
academia, as well as supplementary material for advanced researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1&quot;&gt;Gongjin Lan an Qi Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08660">
<title>Gemini Pro Defeated by GPT-4V: Evidence from Education. (arXiv:2401.08660v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08660</link>
<description rdf:parseType="Literal">&lt;p&gt;This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models&apos; abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models&apos; ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V&apos;s superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V&apos;s higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyeong-Geon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_E/0/1/0/all/0/1&quot;&gt;Ehsan Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lehong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaoming Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08663">
<title>An Integrated Imitation and Reinforcement Learning Methodology for Robust Agile Aircraft Control with Limited Pilot Demonstration Data. (arXiv:2401.08663v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08663</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a methodology for constructing data-driven maneuver
generation models for agile aircraft that can generalize across a wide range of
trim conditions and aircraft model parameters. Maneuver generation models play
a crucial role in the testing and evaluation of aircraft prototypes, providing
insights into the maneuverability and agility of the aircraft. However,
constructing the models typically requires extensive amounts of real pilot
data, which can be time-consuming and costly to obtain. Moreover, models built
with limited data often struggle to generalize beyond the specific flight
conditions covered in the original dataset. To address these challenges, we
propose a hybrid architecture that leverages a simulation model, referred to as
the source model. This open-source agile aircraft simulator shares similar
dynamics with the target aircraft and allows us to generate unlimited data for
building a proxy maneuver generation model. We then fine-tune this model to the
target aircraft using a limited amount of real pilot data. Our approach
combines techniques from imitation learning, transfer learning, and
reinforcement learning to achieve this objective. To validate our methodology,
we utilize real agile pilot data provided by Turkish Aerospace Industries
(TAI). By employing the F-16 as the source model, we demonstrate that it is
possible to construct a maneuver generation model that generalizes across
various trim conditions and aircraft parameters without requiring any
additional real pilot data. Our results showcase the effectiveness of our
approach in developing robust and adaptable models for agile aircraft.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sever_G/0/1/0/all/0/1&quot;&gt;Gulay Goktas Sever&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1&quot;&gt;Umut Demir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satir_A/0/1/0/all/0/1&quot;&gt;Abdullah Sadik Satir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_M/0/1/0/all/0/1&quot;&gt;Mustafa Cagatay Sahin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1&quot;&gt;Nazim Kemal Ure&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08664">
<title>Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08664</link>
<description rdf:parseType="Literal">&lt;p&gt;Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to offer
personalized education resources due to the challenge of addressing the diverse
obstacles students encounter throughout their learning journey. Recently, the
emergence of large language models (LLMs), such as ChatGPT, offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM researches related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Based on the current development status, we further outline two
approaches for an LLM-based education system: a unified approach and a
mixture-of-expert (MoE) approach. Finally, we explore the challenges and future
directions, providing new research opportunities and perspectives on adapting
LLMs for education.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Lingyue Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xianyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruiming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08669">
<title>Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08669</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) has been shown to be effective in producing
approximate solutions to some vehicle routing problems (VRPs), especially when
using policies generated by encoder-decoder attention mechanisms. While these
techniques have been quite successful for relatively simple problem instances,
there are still under-researched and highly complex VRP variants for which no
effective RL method has been demonstrated. In this work we focus on one such
VRP variant, which contains multiple trucks and multi-leg routing requirements.
In these problems, demand is required to move along sequences of nodes, instead
of just from a start node to an end node. With the goal of making deep RL a
viable strategy for real-world industrial-scale supply chain logistics, we
develop new extensions to existing encoder-decoder attention models which allow
them to handle multiple trucks and multi-leg routing requirements. Our models
have the advantage that they can be trained for a small number of trucks and
nodes, and then embedded into a large supply chain to yield solutions for
larger numbers of trucks and nodes. We test our approach on a real supply chain
environment arising in the operations of Japanese automotive parts manufacturer
Aisin Corporation, and find that our algorithm outperforms Aisin&apos;s previous
best solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levin_J/0/1/0/all/0/1&quot;&gt;Joshua Levin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correll_R/0/1/0/all/0/1&quot;&gt;Randall Correll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ide_T/0/1/0/all/0/1&quot;&gt;Takanori Ide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1&quot;&gt;Takafumi Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_T/0/1/0/all/0/1&quot;&gt;Takaho Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arai_A/0/1/0/all/0/1&quot;&gt;Alan Arai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08672">
<title>Concept Alignment. (arXiv:2401.08672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08672</link>
<description rdf:parseType="Literal">&lt;p&gt;Discussion of AI alignment (alignment between humans and AI systems) has
focused on value alignment, broadly referring to creating AI systems that share
human values. We argue that before we can even attempt to align values, it is
imperative that AI systems and humans align the concepts they use to understand
the world. We integrate ideas from philosophy, cognitive science, and deep
learning to explain the need for concept alignment, not just value alignment,
between humans and machines. We summarize existing accounts of how humans and
machines currently learn concepts, and we outline opportunities and challenges
in the path towards shared concepts. Finally, we explain how we can leverage
the tools already being developed in cognitive science and AI research to
accelerate progress towards concept alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1&quot;&gt;Sunayana Rane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruna_P/0/1/0/all/0/1&quot;&gt;Polyphony J. Bruna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1&quot;&gt;Ilia Sucholutsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kello_C/0/1/0/all/0/1&quot;&gt;Christopher Kello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08683">
<title>Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models. (arXiv:2401.08683v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.08683</link>
<description rdf:parseType="Literal">&lt;p&gt;The design and optimization of hardware have traditionally been
resource-intensive, demanding considerable expertise and dependence on
established design automation tools. This paper discusses the possibility of
exploiting large language models to streamline the code generation process in
hardware design. In contrast to earlier studies, this paper aims to use large
language models that accepts high-level design specifications through a single
prompt to generate corresponding Register-Transfer Level (RTL) code. The
ability to use large language models on RTL code generation not only expedites
design iteration cycles but also facilitates the exploration of design spaces
that have computational challenges for conventional techniques. Through our
evaluation, we demonstrate the shortcoming of existing attention mechanisms,
and present the abilities of language models to produce functional, optimized,
and industry-standard compliant RTL code when a novel attention mechanism is
used. These findings underscore the expanding role of large language models in
shaping the future landscape of architectural exploration and automation in
hardware design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandal_S/0/1/0/all/0/1&quot;&gt;Selim Sandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akturk_I/0/1/0/all/0/1&quot;&gt;Ismail Akturk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08694">
<title>Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.08694</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models have emerged as prime candidates to tackle
misinformation mitigation. However, existing approaches struggle with
hallucinations and overconfident predictions. We propose an uncertainty
quantification framework that leverages both direct confidence elicitation and
sampled-based consistency methods to provide better calibration for NLP
misinformation mitigation solutions. We first investigate the calibration of
sample-based consistency methods that exploit distinct features of consistency
across sample sizes and stochastic levels. Next, we evaluate the performance
and distributional shift of a robust numeric verbalization prompt across single
vs. two-step confidence elicitation procedure. We also compare the performance
of the same prompt with different versions of GPT and different numerical
scales. Finally, we combine the sample-based consistency and verbalized methods
to propose a hybrid framework that yields a better uncertainty estimation for
GPT models. Overall, our work proposes novel uncertainty quantification methods
that will improve the reliability of Large Language Models in misinformation
mitigation applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivera_M/0/1/0/all/0/1&quot;&gt;Mauricio Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godbout_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Godbout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1&quot;&gt;Reihaneh Rabbany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelrine_K/0/1/0/all/0/1&quot;&gt;Kellin Pelrine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08695">
<title>Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by Integrating Expert Knowledge and Interpretable Data-driven Intelligence. (arXiv:2401.08695v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08695</link>
<description rdf:parseType="Literal">&lt;p&gt;Although data-driven artificial intelligence (AI) in medical image diagnosis
has shown impressive performance in silico, the lack of interpretability makes
it difficult to incorporate the &quot;black box&quot; into clinicians&apos; workflows. To make
the diagnostic patterns learned from data understandable by clinicians, we
develop an interpretable model, knowledge-guided diagnosis model (KGDM), that
provides a visualized reasoning process containing AI-based biomarkers and
retrieved cases that with the same diagnostic patterns. It embraces clinicians&apos;
prompts into the interpreted reasoning through human-AI interaction, leading to
potentially enhanced safety and more accurate predictions. This study
investigates the performance, interpretability, and clinical utility of KGDM in
the diagnosis of infectious keratitis (IK), which is the leading cause of
corneal blindness. The classification performance of KGDM is evaluated on a
prospective validation dataset, an external testing dataset, and an publicly
available testing dataset. The diagnostic odds ratios (DOR) of the interpreted
AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit
consistent diagnostic patterns with clinic experience. Moreover, a human-AI
collaborative diagnosis test is conducted and the participants with
collaboration achieved a performance exceeding that of both humans and AI. By
synergistically integrating interpretability and interaction, this study
facilitates the convergence of clinicians&apos; expertise and data-driven
intelligence. The promotion of inexperienced ophthalmologists with the aid of
AI-based biomarkers, as well as increased AI prediction by intervention from
experienced ones, demonstrate a promising diagnostic paradigm for infectious
keratitis using KGDM, which holds the potential for extension to other diseases
where experienced medical practitioners are limited and the safety of AI is
concerned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuowen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhouhang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yesheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Wenjia Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yu-Feng Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08696">
<title>Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis with GNNs. (arXiv:2401.08696v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.08696</link>
<description rdf:parseType="Literal">&lt;p&gt;High-level synthesis (HLS) notably speeds up the hardware design process by
avoiding RTL programming. However, the turnaround time of HLS increases
significantly when post-route quality of results (QoR) are considered during
optimization. To tackle this issue, we propose a hierarchical post-route QoR
prediction approach for FPGA HLS, which features: (1) a modeling flow that
directly estimates latency and post-route resource usage from C/C++ programs;
(2) a graph construction method that effectively represents the control and
data flow graph of source code and effects of HLS pragmas; and (3) a
hierarchical GNN training and prediction method capable of capturing the impact
of loop hierarchies. Experimental results show that our method presents a
prediction error of less than 10% for different types of QoR metrics, which
gains tremendous improvement compared with the state-of-the-art GNN methods. By
adopting our proposed methodology, the runtime for design space exploration in
HLS is shortened to tens of minutes and the achieved ADRS is reduced to 6.91%
on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Mingzhe Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jieru Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08699">
<title>On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.08699</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathology images of histopathology can be acquired from camera-mounted
microscopes or whole slide scanners. Utilizing similarity calculations to match
patients based on these images holds significant potential in research and
clinical contexts. Recent advancements in search technologies allow for nuanced
quantification of cellular structures across diverse tissue types, facilitating
comparisons and enabling inferences about diagnosis, prognosis, and predictions
for new patients when compared against a curated database of diagnosed and
treated cases. In this paper, we comprehensively review the latest developments
in image search technologies for histopathology, offering a concise overview
tailored for computational pathology researchers seeking effective, fast and
efficient image search methods in their work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tizhoosh_H/0/1/0/all/0/1&quot;&gt;H.R. Tizhoosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pantanowitz_L/0/1/0/all/0/1&quot;&gt;Liron Pantanowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08711">
<title>Assistant, Parrot, or Colonizing Loudspeaker? ChatGPT Metaphors for Developing Critical AI Literacies. (arXiv:2401.08711v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.08711</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores how discussing metaphors for AI can help build awareness
of the frames that shape our understanding of AI systems, particularly large
language models (LLMs) like ChatGPT. Given the pressing need to teach &quot;critical
AI literacy&quot;, discussion of metaphor provides an opportunity for inquiry and
dialogue with space for nuance, playfulness, and critique. Using a
collaborative autoethnographic methodology, we analyzed metaphors from a range
of sources, and reflected on them individually according to seven questions,
then met and discussed our interpretations. We then analyzed how our
reflections contributed to the three kinds of literacies delineated in Selber&apos;s
multiliteracies framework: functional, critical, and rhetorical. These allowed
us to analyze questions of ethics, equity, and accessibility in relation to AI.
We explored each metaphor along the dimension of whether or not it was
promoting anthropomorphizing, and to what extent such metaphors imply that AI
is sentient. Our findings highlight the role of metaphor reflection in
fostering a nuanced understanding of AI, suggesting that our collaborative
autoethnographic approach as well as the heuristic model of plotting AI
metaphors on dimensions of anthropomorphism and multiliteracies, might be
useful for educators and researchers in the pursuit of advancing critical AI
literacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Anuj Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atef_Y/0/1/0/all/0/1&quot;&gt;Yasser Atef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_A/0/1/0/all/0/1&quot;&gt;Anna Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bali_M/0/1/0/all/0/1&quot;&gt;Maha Bali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08714">
<title>Training program on sign language: social inclusion through Virtual Reality in ISENSE project. (arXiv:2401.08714v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.08714</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured hand gestures that incorporate visual motions and signs are used
in sign language. Sign language is a valuable means of daily communication for
individuals who are deaf or have speech impairments, but it is still rare among
hearing people, and fewer are capable of understand it. Within the academic
context, parents and teachers play a crucial role in supporting deaf students
from childhood by facilitating their learning of sign language. In the last
years, among all the teaching tools useful for learning sign language, the use
of Virtual Reality (VR) has increased, as it has been demonstrated to improve
retention, memory and attention during the learning process. The ISENSE project
has been created to assist students with deafness during their academic life by
proposing different technological tools for teaching sign language to the
hearing community in the academic context. As part of the ISENSE project, this
work aims to develop an application for Spanish and Italian sign language
recognition that exploits the VR environment to quickly and easily create a
comprehensive database of signs and an Artificial Intelligence (AI)-based
software to accurately classify and recognize static and dynamic signs: from
letters to sentences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisio_A/0/1/0/all/0/1&quot;&gt;Alessia Bisio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeguas_Bolivar_E/0/1/0/all/0/1&quot;&gt;Enrique Yeguas-Bol&amp;#xed;var&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_Martinez_P/0/1/0/all/0/1&quot;&gt;Pilar Aparicio-Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redel_Macias_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Dolores Redel-Mac&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinzi_S/0/1/0/all/0/1&quot;&gt;Sara Pinzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1&quot;&gt;Stefano Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taborri_J/0/1/0/all/0/1&quot;&gt;Juri Taborri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08715">
<title>Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing. (arXiv:2401.08715v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08715</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering data insufficiency in metal additive manufacturing (AM), transfer
learning (TL) has been adopted to extract knowledge from source domains (e.g.,
completed printings) to improve the modeling performance in target domains
(e.g., new printings). Current applications use all accessible source data
directly in TL with no regard to the similarity between source and target data.
This paper proposes a systematic method to find appropriate subsets of source
data based on similarities between the source and target datasets for a given
set of limited target domain data. Such similarity is characterized by the
spatial and model distance metrics. A Pareto frontier-based source data
selection method is developed, where the source data located on the Pareto
frontier defined by two similarity distance metrics are selected iteratively.
The method is integrated into an instance-based TL method (decision tree
regression model) and a model-based TL method (fine-tuned artificial neural
network). Both models are then tested on several regression tasks in metal AM.
Comparison results demonstrate that 1) the source data selection method is
general and supports integration with various TL methods and distance metrics,
2) compared with using all source data, the proposed method can find a small
subset of source data from the same domain with better TL performance in metal
AM regression tasks involving different processes and machines, and 3) when
multiple source domains exist, the source data selection method could find the
subset from one source domain to obtain comparable or better TL performance
than the model constructed using data from all source domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yifan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehaghani_M/0/1/0/all/0/1&quot;&gt;M. Rahmani Dehaghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajadi_P/0/1/0/all/0/1&quot;&gt;Pouyan Sajadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;G. Gary Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08721">
<title>A Telerehabilitation System for the Selection, Evaluation and Remote Management of Therapies. (arXiv:2401.08721v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.08721</link>
<description rdf:parseType="Literal">&lt;p&gt;Telerehabilitation systems that support physical therapy sessions anywhere
can help save healthcare costs while also improving the quality of life of the
users that need rehabilitation. The main contribution of this paper is to
present, as a whole, all the features supported by the innovative Kinect-based
Telerehabilitation System (KiReS). In addition to the functionalities provided
by current systems, it handles two new ones that could be incorporated into
them, in order to give a step forward towards a new generation of
telerehabilitation systems. The knowledge extraction functionality handles
knowledge about the physical therapy record of patients and treatment protocols
described in an ontology, named TRHONT, to select the adequate exercises for
the rehabilitation of patients. The teleimmersion functionality provides a
convenient, effective and user-friendly experience when performing the
telerehabilitation, through a two-way real-time multimedia communication. The
ontology contains about 2300 classes and 100 properties, and the system allows
a reliable transmission of Kinect video depth, audio and skeleton data, being
able to adapt to various network conditions. Moreover, the system has been
tested with patients who suffered from shoulder disorders or total hip
replacement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anton_D/0/1/0/all/0/1&quot;&gt;David Anton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berges_I/0/1/0/all/0/1&quot;&gt;Idoia Berges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bermudez_J/0/1/0/all/0/1&quot;&gt;Jes&amp;#xfa;s Berm&amp;#xfa;dez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goni_A/0/1/0/all/0/1&quot;&gt;Alfredo Go&amp;#xf1;i&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Illarramendi_A/0/1/0/all/0/1&quot;&gt;Arantza Illarramendi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08727">
<title>MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08727</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of traffic congestion not only causes a large amount of economic
losses, but also seriously endangers the urban environment. Predicting traffic
congestion has important practical significance. So far, most studies have been
based on historical data from sensors placed on different roads to predict
future traffic flow and speed, to analyze the traffic congestion conditions of
a certain road segment. However, due to the fixed position of sensors, it is
difficult to mine new information. On the other hand, vehicle trajectory data
is more flexible and can extract traffic information as needed. Therefore, we
proposed a new traffic congestion prediction model - Multi Adjacency
relationship Attention Graph Convolutional Networks(MA2GCN). This model
transformed vehicle trajectory data into graph structured data in grid form,
and proposed a vehicle entry and exit matrix based on the mobility between
different grids. At the same time, in order to improve the performance of the
model, this paper also built a new adaptive adjacency matrix generation method
and adjacency matrix attention module. This model mainly used gated temporal
convolution and graph convolution to extract temporal and spatial information,
respectively. Compared with multiple baselines, our model achieved the best
performance on Shanghai taxi GPS trajectory dataset. The code is available at
https://github.com/zachysun/Taxi Traffic Benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhengke Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuliang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08728">
<title>AgentMixer: Multi-Agent Correlated Policy Factorization. (arXiv:2401.08728v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.08728</link>
<description rdf:parseType="Literal">&lt;p&gt;Centralized training with decentralized execution (CTDE) is widely employed
to stabilize partially observable multi-agent reinforcement learning (MARL) by
utilizing a centralized value function during training. However, existing
methods typically assume that agents make decisions based on their local
observations independently, which may not lead to a correlated joint policy
with sufficient coordination. Inspired by the concept of correlated
equilibrium, we propose to introduce a \textit{strategy modification} to
provide a mechanism for agents to correlate their policies. Specifically, we
present a novel framework, AgentMixer, which constructs the joint fully
observable policy as a non-linear combination of individual partially
observable policies. To enable decentralized execution, one can derive
individual policies by imitating the joint policy. Unfortunately, such
imitation learning can lead to \textit{asymmetric learning failure} caused by
the mismatch between joint policy and individual policy information. To
mitigate this issue, we jointly train the joint policy and individual policies
and introduce \textit{Individual-Global-Consistency} to guarantee mode
consistency between the centralized and decentralized policies. We then
theoretically prove that AgentMixer converges to an $\epsilon$-approximate
Correlated Equilibrium. The strong experimental performance on three MARL
benchmarks demonstrates the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenshuai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08739">
<title>EgoGen: An Egocentric Synthetic Data Generator. (arXiv:2401.08739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08739</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the world in first-person view is fundamental in Augmented
Reality (AR). This immersive perspective brings dramatic visual changes and
unique challenges compared to third-person views. Synthetic data has empowered
third-person-view vision models, but its application to embodied egocentric
perception tasks remains largely unexplored. A critical challenge lies in
simulating natural human movements and behaviors that effectively steer the
embodied cameras to capture a faithful egocentric representation of the 3D
world. To address this challenge, we introduce EgoGen, a new synthetic data
generator that can produce accurate and rich ground-truth training data for
egocentric perception tasks. At the heart of EgoGen is a novel human motion
synthesis model that directly leverages egocentric visual inputs of a virtual
human to sense the 3D environment. Combined with collision-avoiding motion
primitives and a two-stage reinforcement learning approach, our motion
synthesis model offers a closed-loop solution where the embodied perception and
movement of the virtual human are seamlessly coupled. Compared to previous
works, our model eliminates the need for a pre-defined global path, and is
directly applicable to dynamic environments. Combined with our easy-to-use and
scalable data generation pipeline, we demonstrate EgoGen&apos;s efficacy in three
tasks: mapping and localization for head-mounted cameras, egocentric camera
tracking, and human mesh recovery from egocentric views. EgoGen will be fully
open-sourced, offering a practical solution for creating realistic egocentric
training data and aiming to serve as a useful tool for egocentric computer
vision research. Refer to our project page: https://ego-gen.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kaifeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusmanu_M/0/1/0/all/0/1&quot;&gt;Mihai Dusmanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08741">
<title>Fixed Point Diffusion Models. (arXiv:2401.08741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08741</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to
image generation that integrates the concept of fixed point solving into the
framework of diffusion-based generative modeling. Our approach embeds an
implicit fixed point solving layer into the denoising network of a diffusion
model, transforming the diffusion process into a sequence of closely-related
fixed point problems. Combined with a new stochastic training method, this
approach significantly reduces model size, reduces memory usage, and
accelerates training. Moreover, it enables the development of two new
techniques to improve sampling efficiency: reallocating computation across
timesteps and reusing fixed point solutions between timesteps. We conduct
extensive experiments with state-of-the-art models on ImageNet, FFHQ,
CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in
performance and efficiency. Compared to the state-of-the-art DiT model, FPDM
contains 87% fewer parameters, consumes 60% less memory during training, and
improves image generation quality in situations where sampling computation or
time is limited. Our code and pretrained models are available at
https://lukemelas.github.io/fixed-point-diffusion-models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xingjian Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1&quot;&gt;Luke Melas-Kyriazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08743">
<title>MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08743</link>
<description rdf:parseType="Literal">&lt;p&gt;Theory of Mind (ToM), the ability to understand people&apos;s minds, is an
essential ingredient for developing machines with human-level social
intelligence. Recent machine learning models, particularly large language
models, seem to show some aspects of ToM understanding. However, existing ToM
benchmarks use unimodal datasets - either video or text. Human ToM, on the
other hand, is more than video or text understanding. People can flexibly
reason about another person&apos;s mind based on conceptual representations (e.g.,
goals, beliefs, plans) extracted from any available data, which can include
visual cues, linguistic narratives, or both. To address this, we introduce a
multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA
comprehensively evaluates machine ToM both on multimodal data and on different
kinds of unimodal data about a person&apos;s activity in a household environment. To
engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian
Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified
representations from multimodal data and utilizes language models for scalable
Bayesian inverse planning. We conducted a systematic comparison of human
performance, BIP-ALM, and state-of-the-art models, including GPT-4. The
experiments demonstrate that large language models and large multimodal models
still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising
results, by leveraging the power of both model-based mental inference and
language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chuanyang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yutong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1&quot;&gt;Jiannan Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1&quot;&gt;Yen-Ling Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullman_T/0/1/0/all/0/1&quot;&gt;Tomer Ullman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1&quot;&gt;Tianmin Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08815">
<title>Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. (arXiv:2401.08815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent advances in large-scale diffusion models, little progress
has been made on the layout-to-image (L2I) synthesis task. Current L2I models
either suffer from poor editability via text or weak alignment between the
generated image and the input layout. This limits their usability in practice.
To mitigate this, we propose to integrate adversarial supervision into the
conventional training pipeline of L2I diffusion models (ALDM). Specifically, we
employ a segmentation-based discriminator which provides explicit feedback to
the diffusion generator on the pixel-level alignment between the denoised image
and the input layout. To encourage consistent adherence to the input layout
over the sampling steps, we further introduce the multistep unrolling strategy.
Instead of looking at a single timestep, we unroll a few steps recursively to
imitate the inference process, and ask the discriminator to assess the
alignment of denoised images with the layout over a certain time window. Our
experiments show that ALDM enables layout faithfulness of the generated images,
while allowing broad editability via text prompts. Moreover, we showcase its
usefulness for practical applications: by synthesizing target distribution
samples via text control, we improve domain generalization of semantic
segmentation models by a large margin (~12 mIoU points).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yumeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1&quot;&gt;Anna Khoreva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08819">
<title>Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08819</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) offers a promising direction for learning
policies from pre-collected datasets without requiring further interactions
with the environment. However, existing methods struggle to handle
out-of-distribution (OOD) extrapolation errors, especially in sparse reward or
scarce data settings. In this paper, we propose a novel training algorithm
called Conservative Density Estimation (CDE), which addresses this challenge by
explicitly imposing constraints on the state-action occupancy stationary
distribution. CDE overcomes the limitations of existing approaches, such as the
stationary distribution correction method, by addressing the support mismatch
issue in marginal importance sampling. Our method achieves state-of-the-art
performance on the D4RL benchmark. Notably, CDE consistently outperforms
baselines in challenging tasks with sparse rewards or insufficient data,
demonstrating the advantages of our approach in addressing the extrapolation
error problem in offline RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cen_Z/0/1/0/all/0/1&quot;&gt;Zhepeng Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yihang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1&quot;&gt;Henry Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08850">
<title>REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes. (arXiv:2401.08850v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08850</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrete-action reinforcement learning algorithms often falter in tasks with
high-dimensional discrete action spaces due to the vast number of possible
actions. A recent advancement leverages value-decomposition, a concept from
multi-agent reinforcement learning, to tackle this challenge. This study delves
deep into the effects of this value-decomposition, revealing that whilst it
curtails the over-estimation bias inherent to Q-learning algorithms, it
amplifies target variance. To counteract this, we present an ensemble of
critics to mitigate target variance. Moreover, we introduce a regularisation
loss that helps to mitigate the effects that exploratory actions in one
dimension can have on the value of optimal actions in other dimensions. Our
novel algorithm, REValueD, tested on discretised versions of the DeepMind
Control Suite tasks, showcases superior performance, especially in the
challenging humanoid and dog tasks. We further dissect the factors influencing
REValueD&apos;s performance, evaluating the significance of the regularisation loss
and the scalability of REValueD with increasing sub-actions per dimension.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ireland_D/0/1/0/all/0/1&quot;&gt;David Ireland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montana_G/0/1/0/all/0/1&quot;&gt;Giovanni Montana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08863">
<title>Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems. (arXiv:2401.08863v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08863</link>
<description rdf:parseType="Literal">&lt;p&gt;Using neural networks for localization of key fob within and surrounding a
car as a security feature for keyless entry is fast emerging. In this paper we
study: 1) the performance of pre-computed features of neural networks based UWB
(ultra wide band) localization classification forming the baseline of our
experiments. 2) Investigate the inherent robustness of various neural networks;
therefore, we include the study of robustness of the adversarial examples
without any adversarial training in this work. 3) Propose a multi-head
self-supervised neural network architecture which outperforms the baseline
neural networks without any adversarial training. The model&apos;s performance
improved by 67% at certain ranges of adversarial magnitude for fast gradient
sign method and 37% each for basic iterative method and projected gradient
descent method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolli_A/0/1/0/all/0/1&quot;&gt;Abhiram Kolli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casamassima_F/0/1/0/all/0/1&quot;&gt;Filippo Casamassima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1&quot;&gt;Horst Possegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1&quot;&gt;Horst Bischof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08875">
<title>DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08875</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-touch attribution (MTA) currently plays a pivotal role in achieving a
fair estimation of the contributions of each advertising touchpoint to-wards
conversion behavior, deeply influencing budget allocation and advertising
recommenda-tion. Traditional multi-touch attribution methods initially build a
conversion prediction model, an-ticipating learning the inherent relationship
be-tween touchpoint sequences and user purchasing behavior through historical
data. Based on this, counterfactual touchpoint sequences are con-structed from
the original sequence subset, and conversions are estimated using the
prediction model, thus calculating advertising contributions. A covert
assumption of these methods is the un-biased nature of conversion prediction
models. However, due to confounding variables factors arising from user
preferences and internet recom-mendation mechanisms such as homogenization of
ad recommendations resulting from past shop-ping records, bias can easily occur
in conversion prediction models trained on observational data. This paper
redefines the causal effect of user fea-tures on conversions and proposes a
novel end-to-end approach, Deep Causal Representation for MTA (DCRMTA). Our
model while eliminating confounding variables, extracts features with causal
relations to conversions from users. Fur-thermore, Extensive experiments on
both synthet-ic and real-world Criteo data demonstrate DCRMTA&apos;s superior
performance in converting prediction across varying data distributions, while
also effectively attributing value across dif-ferent advertising channels
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiaming Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08879">
<title>Contribution Functions for Quantitative Bipolar Argumentation Graphs: A Principle-based Analysis. (arXiv:2401.08879v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08879</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a principle-based analysis of contribution functions for
quantitative bipolar argumentation graphs that quantify the contribution of one
argument to another. The introduced principles formalise the intuitions
underlying different contribution functions as well as expectations one would
have regarding the behaviour of contribution functions in general. As none of
the covered contribution functions satisfies all principles, our analysis can
serve as a tool that enables the selection of the most suitable function based
on the requirements of a given use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampik_T/0/1/0/all/0/1&quot;&gt;Timotheus Kampik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potyka_N/0/1/0/all/0/1&quot;&gt;Nico Potyka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cyras_K/0/1/0/all/0/1&quot;&gt;Kristijonas &amp;#x10c;yras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08887">
<title>NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription. (arXiv:2401.08887v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.08887</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the first Natural Office Talkers in Settings of Far-field Audio
Recordings (``NOTSOFAR-1&apos;&apos;) Challenge alongside datasets and baseline system.
The challenge focuses on distant speaker diarization and automatic speech
recognition (DASR) in far-field meeting scenarios, with single-channel and
known-geometry multi-channel tracks, and serves as a launch platform for two
new datasets: First, a benchmarking dataset of 315 meetings, averaging 6
minutes each, capturing a broad spectrum of real-world acoustic conditions and
conversational dynamics. It is recorded across 30 conference rooms, featuring
4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated
training dataset, synthesized with enhanced authenticity for real-world
generalization, incorporating 15,000 real acoustic transfer functions. The
tasks focus on single-device DASR, where multi-channel devices always share the
same known geometry. This is aligned with common setups in actual conference
rooms, and avoids technical complexities associated with multi-device tasks. It
also allows for the development of geometry-specific solutions. The NOTSOFAR-1
Challenge aims to advance research in the field of distant conversational
speech recognition, providing key resources to unlock the potential of
data-driven methods, which we believe are currently constrained by the absence
of comprehensive high-quality training and benchmarking datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinnikov_A/0/1/0/all/0/1&quot;&gt;Alon Vinnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1&quot;&gt;Amir Ivry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurvitz_A/0/1/0/all/0/1&quot;&gt;Aviv Hurvitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abramovski_I/0/1/0/all/0/1&quot;&gt;Igor Abramovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koubi_S/0/1/0/all/0/1&quot;&gt;Sharon Koubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurvich_I/0/1/0/all/0/1&quot;&gt;Ilya Gurvich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pe%60er_S/0/1/0/all/0/1&quot;&gt;Shai Pe`er&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elizalde_B/0/1/0/all/0/1&quot;&gt;Benjamin Martinez Elizalde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1&quot;&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaer_S/0/1/0/all/0/1&quot;&gt;Shalev Shaer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yagev_S/0/1/0/all/0/1&quot;&gt;Stav Yagev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asher_Y/0/1/0/all/0/1&quot;&gt;Yossi Asher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivasankaran_S/0/1/0/all/0/1&quot;&gt;Sunit Sivasankaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yifan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Min Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huaming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krupka_E/0/1/0/all/0/1&quot;&gt;Eyal Krupka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08897">
<title>CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08897</link>
<description rdf:parseType="Literal">&lt;p&gt;Symmetries of input and latent vectors have provided valuable insights for
disentanglement learning in VAEs.However, only a few works were proposed as an
unsupervised method, and even these works require known factor information in
training data. We propose a novel method, Composite Factor-Aligned Symmetry
Learning (CFASL), which is integrated into VAEs for learning symmetry-based
disentanglement in unsupervised learning without any knowledge of the dataset
factor information.CFASL incorporates three novel features for learning
symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable
symmetry codebook 2) Learning a composite symmetry to express unknown factors
change between two random samples by learning factor-aligned symmetries within
the codebook 3) Inducing group equivariant encoder and decoder in training VAEs
with the two conditions. In addition, we propose an extended evaluation metric
for multi-factor changes in comparison to disentanglement evaluation in VAEs.
In quantitative and in-depth qualitative analysis, CFASL demonstrates a
significant improvement of disentanglement in single-factor change, and
multi-factor change conditions compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Hee-Jun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Jaehyoung Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kangil Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08898">
<title>Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08898</link>
<description rdf:parseType="Literal">&lt;p&gt;Representations are at the core of all deep reinforcement learning (RL)
methods for both Markov decision processes (MDPs) and partially observable
Markov decision processes (POMDPs). Many representation learning methods and
theoretical frameworks have been developed to understand what constitutes an
effective representation. However, the relationships between these methods and
the shared properties among them remain unclear. In this paper, we show that
many of these seemingly distinct methods and frameworks for state and history
abstractions are, in fact, based on a common idea of self-predictive
abstraction. Furthermore, we provide theoretical insights into the widely
adopted objectives and optimization, such as the stop-gradient technique, in
learning self-predictive representations. These findings together yield a
minimalist algorithm to learn self-predictive representations for states and
histories. We validate our theories by applying our algorithm to standard MDPs,
MDPs with distractors, and POMDPs with sparse rewards. These findings culminate
in a set of practical guidelines for RL practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_T/0/1/0/all/0/1&quot;&gt;Tianwei Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1&quot;&gt;Benjamin Eysenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyedsalehi_E/0/1/0/all/0/1&quot;&gt;Erfan Seyedsalehi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Michel Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehring_C/0/1/0/all/0/1&quot;&gt;Clement Gehring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1&quot;&gt;Aditya Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacon_P/0/1/0/all/0/1&quot;&gt;Pierre-Luc Bacon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08930">
<title>3D Human Pose Analysis via Diffusion Synthesis. (arXiv:2401.08930v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08930</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable success in generative modeling.
In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel
framework designed to address various challenges in 3D human pose analysis
through a unified pipeline. Central to PADS are two distinctive strategies: i)
learning a task-agnostic pose prior using a diffusion synthesis process to
effectively capture the kinematic constraints in human pose data, and ii)
unifying multiple pose analysis tasks like estimation, completion, denoising,
etc, as instances of inverse problems. The learned pose prior will be treated
as a regularization imposing on task-specific constraints, guiding the
optimization process through a series of conditional denoising steps. PADS
represents the first diffusion-based framework for tackling general 3D human
pose analysis within the inverse problem framework. Its performance has been
validated on different benchmarks, signaling the adaptability and robustness of
this pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Haorui Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08932">
<title>Learning to detect cloud and snow in remote sensing images from noisy labels. (arXiv:2401.08932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08932</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting clouds and snow in remote sensing images is an essential
preprocessing task for remote sensing imagery. Previous works draw inspiration
from semantic segmentation models in computer vision, with most research
focusing on improving model architectures to enhance detection performance.
However, unlike natural images, the complexity of scenes and the diversity of
cloud types in remote sensing images result in many inaccurate labels in cloud
and snow detection datasets, introducing unnecessary noises into the training
and testing processes. By constructing a new dataset and proposing a novel
training strategy with the curriculum learning paradigm, we guide the model in
reducing overfitting to noisy labels. Additionally, we design a more
appropriate model performance evaluation method, that alleviates the
performance assessment bias caused by noisy labels. By conducting experiments
on models with UNet and Segformer, we have validated the effectiveness of our
proposed method. This paper is the first to consider the impact of label noise
on the detection of clouds and snow in remote sensing images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Keyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zipeng Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zhengxia Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08936">
<title>DeLF: Designing Learning Environments with Foundation Models. (arXiv:2401.08936v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08936</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) offers a capable and intuitive structure for the
fundamental sequential decision-making problem. Despite impressive
breakthroughs, it can still be difficult to employ RL in practice in many
simple applications. In this paper, we try to address this issue by introducing
a method for designing the components of the RL environment for a given,
user-intended application. We provide an initial formalization for the problem
of RL component design, that concentrates on designing a good representation
for observation and action space. We propose a method named DeLF: Designing
Learning Environments with Foundation Models, that employs large language
models to design and codify the user&apos;s intended learning scenario. By testing
our method on four different learning environments, we demonstrate that DeLF
can obtain executable environment codes for the corresponding RL problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshar_A/0/1/0/all/0/1&quot;&gt;Aida Afshar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenchao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08940">
<title>CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation. (arXiv:2401.08940v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08940</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning, the ability of a model to learn over time without
forgetting previous knowledge and, therefore, be adaptive to new data, is
paramount in dynamic fields such as disease outbreak prediction. Deep neural
networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This
study introduces a novel CEL model for continual learning by leveraging domain
adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate
the catastrophic forgetting phenomenon in a domain incremental setting. The
Fisher Information Matrix (FIM) is constructed with EWC to develop a
regularization term that penalizes changes to important parameters, namely, the
important previous knowledge. CEL&apos;s performance is evaluated on three distinct
diseases, Influenza, Mpox, and Measles, with different metrics. The high
R-squared values during evaluation and reevaluation outperform the other
state-of-the-art models in several contexts, indicating that CEL adapts to
incremental data well. CEL&apos;s robustness and reliability are underscored by its
minimal 65% forgetting rate and 18% higher memory stability compared to
existing benchmark studies. This study highlights CEL&apos;s versatility in disease
outbreak prediction, addressing evolving data with temporal patterns. It offers
a valuable model for proactive disease control with accurate, timely
predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aslam_S/0/1/0/all/0/1&quot;&gt;Saba Aslam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1&quot;&gt;Abdur Rasool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongyan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08957">
<title>SWBT: Similarity Weighted Behavior Transformer with the Imperfect Demonstration for Robotic Manipulation. (arXiv:2401.08957v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.08957</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning (IL), aiming to learn optimal control policies from expert
demonstrations, has been an effective method for robot manipulation tasks.
However, previous IL methods either only use expensive expert demonstrations
and omit imperfect demonstrations or rely on interacting with the environment
and learning from online experiences. In the context of robotic manipulation,
we aim to conquer the above two challenges and propose a novel framework named
Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from
both expert and imperfect demonstrations without interaction with environments.
We reveal that the easy-to-get imperfect demonstrations, such as forward and
inverse dynamics, significantly enhance the network by learning fruitful
information. To the best of our knowledge, we are the first to attempt to
integrate imperfect demonstrations into the offline imitation learning setting
for robot manipulation tasks. Extensive experiments on the ManiSkill2 benchmark
built on the high-fidelity Sapien simulator and real-world robotic manipulation
tasks demonstrated that the proposed method can extract better features and
improve the success rates for all tasks. Our code will be released upon
acceptance of the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1&quot;&gt;Di Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08959">
<title>Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback. (arXiv:2401.08959v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08959</link>
<description rdf:parseType="Literal">&lt;p&gt;Probabilistic learning to rank (LTR) has been the dominating approach for
optimizing the ranking metric, but cannot maximize long-term rewards.
Reinforcement learning models have been proposed to maximize user long-term
rewards by formulating the recommendation as a sequential decision-making
problem, but could only achieve inferior accuracy compared to LTR counterparts,
primarily due to the lack of online interactions and the characteristics of
ranking. In this paper, we propose a new off-policy value ranking (VR)
algorithm that can simultaneously maximize user long-term rewards and optimize
the ranking metric offline for improved sample efficiency in a unified
Expectation-Maximization (EM) framework. We theoretically and empirically show
that the EM process guides the leaned policy to enjoy the benefit of
integration of the future reward and ranking metric, and learn without any
online interactions. Extensive offline and online experiments demonstrate the
effectiveness of our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Teng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Suhang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08960">
<title>From User Surveys to Telemetry-Driven Agents: Exploring the Potential of Personalized Productivity Solutions. (arXiv:2401.08960v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.08960</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive, user-centric approach to understand preferences
in AI-based productivity agents and develop personalized solutions tailored to
users&apos; needs. Utilizing a two-phase method, we first conducted a survey with
363 participants, exploring various aspects of productivity, communication
style, agent approach, personality traits, personalization, and privacy.
Drawing on the survey insights, we developed a GPT-4 powered personalized
productivity agent that utilizes telemetry data gathered via Viva Insights from
information workers to provide tailored assistance. We compared its performance
with alternative productivity-assistive tools, such as dashboard and narrative,
in a study involving 40 participants. Our findings highlight the importance of
user-centric design, adaptability, and the balance between personalization and
privacy in AI-assisted productivity tools. By building on the insights
distilled from our study, we believe that our work can enable and guide future
research to further enhance productivity solutions, ultimately leading to
optimized efficiency and user experiences for information workers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Subigya Nepal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1&quot;&gt;Javier Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massachi_T/0/1/0/all/0/1&quot;&gt;Talie Massachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowan_K/0/1/0/all/0/1&quot;&gt;Kael Rowan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amores_J/0/1/0/all/0/1&quot;&gt;Judith Amores&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1&quot;&gt;Jina Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_G/0/1/0/all/0/1&quot;&gt;Gonzalo Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houck_B/0/1/0/all/0/1&quot;&gt;Brian Houck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iqbal_S/0/1/0/all/0/1&quot;&gt;Shamsi T. Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czerwinski_M/0/1/0/all/0/1&quot;&gt;Mary Czerwinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08973">
<title>OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality. (arXiv:2401.08973v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.08973</link>
<description rdf:parseType="Literal">&lt;p&gt;One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Aditya Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1&quot;&gt;Luke Yoffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1&quot;&gt;Tobias H&amp;#xf6;llerer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08977">
<title>FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data. (arXiv:2401.08977v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08977</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected
from decentralized local clients manifests a globally prevalent long-tailed
distribution, has garnered considerable attention in recent times. In the
context of Fed-LT, existing works have predominantly centered on addressing the
data imbalance issue to enhance the efficacy of the generic global model while
neglecting the performance at the local level. In contrast, conventional
Personalized Federated Learning (pFL) techniques are primarily devised to
optimize personalized local models under the presumption of a balanced global
data distribution. This paper introduces an approach termed Federated Local and
Generic Model Training in Fed-LT (FedLoGe), which enhances both local and
generic model performance through the integration of representation learning
and classifier alignment within a neural collapse framework. Our investigation
reveals the feasibility of employing a shared backbone as a foundational
framework for capturing overarching global trends, while concurrently employing
individualized classifiers to encapsulate distinct refinements stemming from
each client&apos;s local features. Building upon this discovery, we establish the
Static Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural
collapse principles that naturally prune extraneous noisy features and foster
the acquisition of potent data representations. Furthermore, leveraging
insights from imbalance neural collapse&apos;s classifier norm patterns, we develop
Global and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global
classifier and personalized Euclidean norm transfer to align global features
with client preferences. Extensive experimental results on CIFAR-10/100-LT,
ImageNet, and iNaturalist demonstrate the advantage of our method over
state-of-the-art pFL and Fed-LT approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zikai Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liyinglan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wanlu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Howard Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08984">
<title>A GAN-based data poisoning framework against anomaly detection in vertical federated learning. (arXiv:2401.08984v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;In vertical federated learning (VFL), commercial entities collaboratively
train a model while preserving data privacy. However, a malicious participant&apos;s
poisoning attack may degrade the performance of this collaborative model. The
main challenge in achieving the poisoning attack is the absence of access to
the server-side top model, leaving the malicious participant without a clear
target model. To address this challenge, we introduce an innovative end-to-end
poisoning framework P-GAN. Specifically, the malicious participant initially
employs semi-supervised learning to train a surrogate target model.
Subsequently, this participant employs a GAN-based method to produce
adversarial perturbations to degrade the surrogate target model&apos;s performance.
Finally, the generator is obtained and tailored for VFL poisoning. Besides, we
develop an anomaly detection algorithm based on a deep auto-encoder (DAE),
offering a robust defense mechanism to VFL scenarios. Through extensive
experiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the
factors that influence their performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1&quot;&gt;Daoguang Zan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1&quot;&gt;Bei Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongji Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08996">
<title>MicroNAS: Zero-Shot Neural Architecture Search for MCUs. (arXiv:2401.08996v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.08996</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) effectively discovers new Convolutional
Neural Network (CNN) architectures, particularly for accuracy optimization.
However, prior approaches often require resource-intensive training on super
networks or extensive architecture evaluations, limiting practical
applications. To address these challenges, we propose MicroNAS, a
hardware-aware zero-shot NAS framework designed for microcontroller units
(MCUs) in edge computing. MicroNAS considers target hardware optimality during
the search, utilizing specialized performance indicators to identify optimal
neural architectures without high computational costs. Compared to previous
works, MicroNAS achieves up to 1104x improvement in search efficiency and
discovers models with over 3.23x faster MCU inference while maintaining similar
accuracy
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Ye Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haocheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Sitao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08999">
<title>Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent. (arXiv:2401.08999v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.08999</link>
<description rdf:parseType="Literal">&lt;p&gt;Homeostasis is a biological process by which living beings maintain their
internal balance. Previous research suggests that homeostasis is a learned
behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning
(HRRL) framework attempts to explain this learned homeostatic behavior by
linking Drive Reduction Theory and Reinforcement Learning. This linkage has
been proven in the discrete time-space, but not in the continuous time-space.
In this work, we advance the HRRL framework to a continuous time-space
environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL)
framework. We achieve this by designing a model that mimics the homeostatic
mechanisms in a real-world biological agent. This model uses the
Hamilton-Jacobian Bellman Equation, and function approximation based on neural
networks and Reinforcement Learning. Through a simulation-based experiment we
demonstrate the efficacy of this model and uncover the evidence linked to the
agent&apos;s ability to dynamically choose policies that favor homeostasis in a
continuously changing internal-state milieu. Results of our experiments
demonstrate that agent learns homeostatic behaviour in a CTCS environment,
making CTCS-HRRL a promising framework for modellng animal dynamics and
decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurencon_H/0/1/0/all/0/1&quot;&gt;Hugo Laurencon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargava_Y/0/1/0/all/0/1&quot;&gt;Yesoda Bhargava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zantye_R/0/1/0/all/0/1&quot;&gt;Riddhi Zantye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segerie_C/0/1/0/all/0/1&quot;&gt;Charbel-Rapha&amp;#xeb;l S&amp;#xe9;gerie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lussange_J/0/1/0/all/0/1&quot;&gt;Johann Lussange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1&quot;&gt;Veeky Baths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutkin_B/0/1/0/all/0/1&quot;&gt;Boris Gutkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09003">
<title>Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09003</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent progress in improving the mathematical reasoning ability of
large language models(LLMs), solving competition-level math problems without
the use of external tools remains challenging for open-source LLMs. In this
work, we introduce the MMIQC dataset, a mixture of processed web data and
synthetic question-response pairs, to equip base models with better
mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by
fine-tuning Mistral-7B(&lt;a href=&quot;/abs/2310.06825&quot;&gt;arXiv:2310.06825&lt;/a&gt;) on MMIQC, achieves 36.0\% accuracy on
MATH(&lt;a href=&quot;/abs/2103.03874&quot;&gt;arXiv:2103.03874&lt;/a&gt;), 5.8\% higher than the previous (model size $\sim$7B)
SOTA. Our experiments also show that a large part of the improvement attributes
to our novel augmentation method IQC(Iterative Question Composing), where we
iteratively ask an LLM to compose new questions from the given seed problems
and do rejection sampling from another LLM. MMIQC has now been released on
https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoxiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Andrew Chi-Chih Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09008">
<title>Hybrid of DiffStride and Spectral Pooling in Convolutional Neural Networks. (arXiv:2401.09008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09008</link>
<description rdf:parseType="Literal">&lt;p&gt;Stride determines the distance between adjacent filter positions as the
filter moves across the input. A fixed stride causes important information
contained in the image can not be captured, so that important information is
not classified. Therefore, in previous research, the DiffStride Method was
applied, namely the Strided Convolution Method with which it can learn its own
stride value. Severe Quantization and a constraining lower bound on preserved
information are arises with Max Pooling Downsampling Method. Spectral Pooling
reduce the constraint lower bound on preserved information by cutting off the
representation in the frequency domain. In this research a CNN Model is
proposed with the Downsampling Learnable Stride Technique performed by
Backpropagation combined with the Spectral Pooling Technique. Diffstride and
Spectral Pooling techniques are expected to maintain most of the information
contained in the image. In this study, we compare the Hybrid Method, which is a
combined implementation of Spectral Pooling and DiffStride against the Baseline
Method, which is the DiffStride implementation on ResNet 18. The accuracy
result of the DiffStride combination with Spectral Pooling improves over
DiffStride which is baseline method by 0.0094. This shows that the Hybrid
Method can maintain most of the information by cutting of the representation in
the frequency domain and determine the stride of the learning result through
Backpropagation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafif_S/0/1/0/all/0/1&quot;&gt;Sulthan Rafif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mochamad Arfan Ravy Wahyu Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azhar_M/0/1/0/all/0/1&quot;&gt;Mohammad Faris Azhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibad_A/0/1/0/all/0/1&quot;&gt;Ahmad Mustafidul Ibad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muflikhah_L/0/1/0/all/0/1&quot;&gt;Lailil Muflikhah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1&quot;&gt;Novanto Yudistira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09011">
<title>Inductive Models for Artificial Intelligence Systems are Insufficient without Good Explanations. (arXiv:2401.09011v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09011</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses the limitations of machine learning (ML), particularly
deep artificial neural networks (ANNs), which are effective at approximating
complex functions but often lack transparency and explanatory power. It
highlights the `problem of induction&apos; : the philosophical issue that past
observations may not necessarily predict future events, a challenge that ML
models face when encountering new, unseen data. The paper argues for the
importance of not just making predictions but also providing good explanations,
a feature that current models often fail to deliver. It suggests that for AI to
progress, we must seek models that offer insights and explanations, not just
predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habaraduwa_U/0/1/0/all/0/1&quot;&gt;Udesh Habaraduwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09019">
<title>Change Detection Between Optical Remote Sensing Imagery and Map Data via Segment Anything Model (SAM). (arXiv:2401.09019v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09019</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised multimodal change detection is pivotal for time-sensitive tasks
and comprehensive multi-temporal Earth monitoring. In this study, we explore
unsupervised multimodal change detection between two key remote sensing data
sources: optical high-resolution imagery and OpenStreetMap (OSM) data.
Specifically, we propose to utilize the vision foundation model Segmentation
Anything Model (SAM), for addressing our task. Leveraging SAM&apos;s exceptional
zero-shot transfer capability, high-quality segmentation maps of optical images
can be obtained. Thus, we can directly compare these two heterogeneous data
forms in the so-called segmentation domain. We then introduce two strategies
for guiding SAM&apos;s segmentation process: the &apos;no-prompt&apos; and &apos;box/mask prompt&apos;
methods. The two strategies are designed to detect land-cover changes in
general scenarios and to identify new land-cover objects within existing
backgrounds, respectively. Experimental results on three datasets indicate that
the proposed approach can achieve more competitive results compared to
representative unsupervised multimodal change detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongruixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yokoya_N/0/1/0/all/0/1&quot;&gt;Naoto Yokoya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09029">
<title>Cross-modality Guidance-aided Multi-modal Learning with Dual Attention for MRI Brain Tumor Grading. (arXiv:2401.09029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09029</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain tumor represents one of the most fatal cancers around the world, and is
very common in children and the elderly. Accurate identification of the type
and grade of tumor in the early stages plays an important role in choosing a
precise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of
different sequences provide clinicians with important contradictory information
to identify tumor regions. However, manual assessment is time-consuming and
error-prone due to big amount of data and the diversity of brain tumor types.
Hence, there is an unmet need for MRI automated brain tumor diagnosis. We
observe that the predictive capability of uni-modality models is limited and
their performance varies widely across modalities, and the commonly used
modality fusion methods would introduce potential noise, which results in
significant performance degradation. To overcome these challenges, we propose a
novel cross-modality guidance-aided multi-modal learning with dual attention
for addressing the task of MRI brain tumor grading. To balance the tradeoff
between model efficiency and efficacy, we employ ResNet Mix Convolution as the
backbone network for feature extraction. Besides, dual attention is applied to
capture the semantic interdependencies in spatial and slice dimensions
respectively. To facilitate information interaction among modalities, we design
a cross-modality guidance-aided module where the primary modality guides the
other secondary modalities during the process of training, which can
effectively leverage the complementary information of different MRI modalities
and meanwhile alleviate the impact of the possible noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dunyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jinyue Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09034">
<title>UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User Experiences in Recommender Systems. (arXiv:2401.09034v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.09034</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has gained traction for enhancing user long-term
experiences in recommender systems by effectively exploring users&apos; interests.
However, modern recommender systems exhibit distinct user behavioral patterns
among tens of millions of items, which increases the difficulty of exploration.
For example, user behaviors with different activity levels require varying
intensity of exploration, while previous studies often overlook this aspect and
apply a uniform exploration strategy to all users, which ultimately hurts user
experiences in the long run. To address these challenges, we propose
User-Oriented Exploration Policy (UOEP), a novel approach facilitating
fine-grained exploration among user groups. We first construct a distributional
critic which allows policy optimization under varying quantile levels of
cumulative reward feedbacks from users, representing user groups with varying
activity levels. Guided by this critic, we devise a population of distinct
actors aimed at effective and fine-grained exploration within its respective
user group. To simultaneously enhance diversity and stability during the
exploration process, we further introduce a population-level diversity
regularization term and a supervision module. Experimental results on public
recommendation datasets demonstrate that our approach outperforms all other
baselines in terms of long-term performance, validating its user-oriented
exploration effectiveness. Meanwhile, further analyses reveal our approach&apos;s
benefits of improved performance for low-activity users as well as increased
fairness among users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sirui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1&quot;&gt;Sunhao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weijie Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09042">
<title>LLMs for Relational Reasoning: How Far are We?. (arXiv:2401.09042v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09042</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs possess strong reasoning ability by merely achieving
positive results on these benchmarks. Recent efforts have demonstrated that the
LLMs are poor at solving sequential decision-making problems that require
common-sense planning by evaluating their performance on the reinforcement
learning benchmarks. In this work, we conduct an in-depth assessment of several
state-of-the-art LLMs&apos; reasoning ability based on the inductive logic
programming (ILP) benchmark, which is broadly recognized as a representative
and challenging measurement for evaluating logic program induction/synthesis
systems as it requires inducing strict cause-effect logic to achieve robust
deduction on independent and identically distributed (IID) and
out-of-distribution (OOD) test samples. Our evaluations illustrate that
compared with the neural program induction systems which are much smaller in
model size, the state-of-the-art LLMs are much poorer in terms of reasoning
ability by achieving much lower performance and generalization using either
natural language prompting or truth-value matrix prompting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yushi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiufeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junzhe Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teo_Y/0/1/0/all/0/1&quot;&gt;Yon Shin Teo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shang-wei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09067">
<title>Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding. (arXiv:2401.09067v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09067</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are susceptible to catastrophic forgetting when trained
on sequential tasks. Various continual learning (CL) methods often rely on
exemplar buffers or/and network expansion for balancing model stability and
plasticity, which, however, compromises their practical value due to privacy
and memory concerns. Instead, this paper considers a strict yet realistic
setting, where the training data from previous tasks is unavailable and the
model size remains relatively constant during sequential training. To achieve
such desiderata, we propose a conceptually simple yet effective method that
attributes forgetting to layer-wise parameter overwriting and the resulting
decision boundary distortion. This is achieved by the synergy between two key
components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten
parameter updates mediated by Hilbert-Schmidt independence criterion in an
orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary
adaptation between old and new tasks with predefined basis vectors. Extensive
experiments demonstrate that our method achieves competitive accuracy
performance, even with absolute superiority of zero exemplar buffer and 1.02x
the base model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Depeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qining Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhigang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09068">
<title>DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning. (arXiv:2401.09068v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09068</link>
<description rdf:parseType="Literal">&lt;p&gt;DTMM is a library designed for efficient deployment and execution of machine
learning models on weak IoT devices such as microcontroller units (MCUs). The
motivation for designing DTMM comes from the emerging field of tiny machine
learning (TinyML), which explores extending the reach of machine learning to
many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak
capability of embedded devices, it is necessary to compress models by pruning
enough weights before deploying. Although pruning has been studied extensively
on many computing platforms, two key issues with pruning methods are
exacerbated on MCUs: models need to be deeply compressed without significantly
compromising accuracy, and they should perform efficiently after pruning.
Current solutions only achieve one of these objectives, but not both. In this
paper, we find that pruned models have great potential for efficient deployment
and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,
pre-execution pruning optimizations, runtime acceleration, and post-execution
low-cost storage to fill the gap for efficient deployment and execution of
pruned models. It can be integrated into commercial ML frameworks for practical
deployment, and a prototype system has been developed. Extensive experiments on
various models show promising gains compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lixiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhen Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenjiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09070">
<title>Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for Generalized Knowledge Augmentation and Inference. (arXiv:2401.09070v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09070</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph (KG) based reasoning has been regarded as an effective means
for the analysis of semantic networks and is of great usefulness in areas of
information retrieval, recommendation, decision-making, and man-machine
interaction. It is widely used in recommendation, decision-making,
question-answering, search, and other fields. However, previous studies mainly
used low-level knowledge in the KG for reasoning, which may result in
insufficient generalization and poor robustness of reasoning. To this end, this
paper proposes a new inference approach using a novel knowledge augmentation
strategy to improve the generalization capability of KG. This framework
extracts high-level pyramidal knowledge from low-level knowledge and applies it
to reasoning in a multi-level hierarchical KG, called knowledge pyramid in this
paper. We tested some medical data sets using the proposed approach, and the
experimental results show that the proposed knowledge pyramid has improved the
knowledge inference performance with better generalization. Especially, when
there are fewer training samples, the inference accuracy can be significantly
improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qinghua Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongzhen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09071">
<title>Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09071</link>
<description rdf:parseType="Literal">&lt;p&gt;Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded
in the spectral domain, their practical reliance on polynomial approximation
implies a profound linkage to the spatial domain. As previous studies rarely
examine spectral GNNs from the spatial perspective, their spatial-domain
interpretability remains elusive, e.g., what information is essentially encoded
by spectral GNNs in the spatial domain? In this paper, to answer this question,
we establish a theoretical connection between spectral filtering and spatial
aggregation, unveiling an intrinsic interaction that spectral filtering
implicitly leads the original graph to an adapted new graph, explicitly
computed for spatial aggregation. Both theoretical and empirical investigations
reveal that the adapted new graph not only exhibits non-locality but also
accommodates signed edge weights to reflect label consistency between nodes.
These findings thus highlight the interpretable role of spectral GNNs in the
spatial domain and inspire us to rethink graph spectral filters beyond the
fixed-order polynomials, which neglect global information. Built upon the
theoretical findings, we revisit the state-of-the-art spectral GNNs and propose
a novel Spatially Adaptive Filtering (SAF) framework, which leverages the
adapted new graph by spectral filtering for an auxiliary non-local aggregation.
Notably, our proposed SAF comprehensively models both node similarity and
dissimilarity from a global perspective, therefore alleviating persistent
deficiencies of GNNs related to long-range dependencies and graph heterophily.
Extensive experiments over 13 node classification benchmarks demonstrate the
superiority of our proposed framework to the state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xinping Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1&quot;&gt;Zixian Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09073">
<title>Fixed-Budget Differentially Private Best Arm Identification. (arXiv:2401.09073v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09073</link>
<description rdf:parseType="Literal">&lt;p&gt;We study best arm identification (BAI) in linear bandits in the fixed-budget
regime under differential privacy constraints, when the arm rewards are
supported on the unit interval. Given a finite budget $T$ and a privacy
parameter $\varepsilon&amp;gt;0$, the goal is to minimise the error probability in
finding the arm with the largest mean after $T$ sampling rounds, subject to the
constraint that the policy of the decision maker satisfies a certain {\em
$\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct
a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by
proposing the principle of {\em maximum absolute determinants}, and derive an
upper bound on its error probability. Furthermore, we derive a minimax lower
bound on the error probability, and demonstrate that the lower and the upper
bounds decay exponentially in $T$, with exponents in the two bounds matching
order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and
(c) the problem complexity that is expressible as the sum of two terms, one
characterising the complexity of standard fixed-budget BAI (without privacy
constraints), and the other accounting for the $\varepsilon$-DP constraint.
Additionally, we present some auxiliary results that contribute to the
derivation of the lower bound on the error probability. These results, we
posit, may be of independent interest and could prove instrumental in proving
lower bounds on error probabilities in several other bandit problems. Whereas
prior works provide results for BAI in the fixed-budget regime without privacy
constraints or in the fixed-confidence regime with privacy constraints, our
work fills the gap in the literature by providing the results for BAI in the
fixed-budget regime under the $\varepsilon$-DP constraint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhirui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karthik_P/0/1/0/all/0/1&quot;&gt;P. N. Karthik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chee_Y/0/1/0/all/0/1&quot;&gt;Yeow Meng Chee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09074">
<title>Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09074</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the extent to which Large Language Models (LLMs) can simulate
the execution of computer code and algorithms. We begin by looking straight
line programs, and show that current LLMs demonstrate poor performance even
with such simple programs -- performance rapidly degrades with the length of
code. We then investigate the ability of LLMs to simulate programs that contain
critical paths and redundant instructions. We also go beyond straight line
program simulation with sorting algorithms and nested loops, and we show the
computational complexity of a routine directly affects the ability of an LLM to
simulate its execution. We observe that LLMs execute instructions sequentially
and with a low error margin only for short programs or standard procedures.
LLMs&apos; code simulation is in tension with their pattern recognition and
memorisation capabilities: on tasks where memorisation is detrimental, we
propose a novel prompting method to simulate code execution line by line.
Empirically, our new Chain of Simulation (CoSm) method improves on the standard
Chain of Thought prompting approach by avoiding the pitfalls of memorisation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malfa_E/0/1/0/all/0/1&quot;&gt;Emanuele La Malfa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinhuber_C/0/1/0/all/0/1&quot;&gt;Christoph Weinhuber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_O/0/1/0/all/0/1&quot;&gt;Orazio Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fangru Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1&quot;&gt;Anthony Cohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shadbolt_N/0/1/0/all/0/1&quot;&gt;Nigel Shadbolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1&quot;&gt;Michael Wooldridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09075">
<title>GPT in Sheep&apos;s Clothing: The Risk of Customized GPTs. (arXiv:2401.09075v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.09075</link>
<description rdf:parseType="Literal">&lt;p&gt;In November 2023, OpenAI introduced a new service allowing users to create
custom versions of ChatGPT (GPTs) by using specific instructions and knowledge
to guide the model&apos;s behavior. We aim to raise awareness of the fact that GPTs
can be used maliciously, posing privacy and security risks to their users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antebi_S/0/1/0/all/0/1&quot;&gt;Sagiv Antebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azulay_N/0/1/0/all/0/1&quot;&gt;Noam Azulay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habler_E/0/1/0/all/0/1&quot;&gt;Edan Habler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganon_B/0/1/0/all/0/1&quot;&gt;Ben Ganon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1&quot;&gt;Asaf Shabtai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09082">
<title>What makes for a &apos;good&apos; social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09082</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing popularity of dialogue agents based on large language models
(LLMs), urgent attention has been drawn to finding ways to ensure their
behaviour is ethical and appropriate. These are largely interpreted in terms of
the &apos;HHH&apos; criteria: making outputs more helpful and honest, and avoiding
harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus
is useful from the perspective of viewing LLM agents as mere mediums for
information, it fails to account for pragmatic factors that can make the same
utterance seem more or less offensive or tactless in different social
situations. We propose an approach to ethics that is more centred on relational
and situational factors, exploring what it means for a system, as a social
actor, to treat an individual respectfully in a (series of) interaction(s). Our
work anticipates a set of largely unexplored risks at the level of situated
interaction, and offers practical suggestions to help LLM technologies behave
as &apos;good&apos; social actors and treat people respectfully.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1&quot;&gt;Lize Alberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keeling_G/0/1/0/all/0/1&quot;&gt;Geoff Keeling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCroskery_A/0/1/0/all/0/1&quot;&gt;Amanda McCroskery&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09192">
<title>Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09192</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Transformers in artificial intelligence has come at the
cost of increased resource consumption and greenhouse gas emissions due to
growing model sizes. Prior work suggests using pretrained small models to
improve training efficiency, but this approach may not be suitable for new
model structures. On the other hand, training from scratch can be slow, and
progressively stacking layers often fails to achieve significant acceleration.
To address these challenges, we propose a novel method called Apollo, which
prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by
\textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of
low layers. Our approach involves low-value-prioritized sampling (LVPS) to
train different depths and weight sharing to facilitate efficient expansion. We
also introduce an interpolation method for stable model depth extension.
Experiments demonstrate that Apollo achieves state-of-the-art acceleration
ratios, even rivaling methods using pretrained models, making it a universal
and efficient solution for training deep models while reducing time, financial,
and environmental costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ye Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yichun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09235">
<title>A Characterization Theorem for Equivariant Networks with Point-wise Activations. (arXiv:2401.09235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09235</link>
<description rdf:parseType="Literal">&lt;p&gt;Equivariant neural networks have shown improved performance, expressiveness
and sample complexity on symmetrical domains. But for some specific symmetries,
representations, and choice of coordinates, the most common point-wise
activations, such as ReLU, are not equivariant, hence they cannot be employed
in the design of equivariant neural networks. The theorem we present in this
paper describes all possible combinations of finite-dimensional
representations, choice of coordinates and point-wise activations to obtain an
exactly equivariant layer, generalizing and strengthening existing
characterizations. Notable cases of practical relevance are discussed as
corollaries. Indeed, we prove that rotation-equivariant networks can only be
invariant, as it happens for any network which is equivariant with respect to
connected compact groups. Then, we discuss implications of our findings when
applied to important instances of exactly equivariant networks. First, we
completely characterize permutation equivariant networks such as Invariant
Graph Networks with point-wise nonlinearities and their geometric counterparts,
highlighting a plethora of models whose expressive power and performance are
still unknown. Second, we show that feature spaces of disentangled steerable
convolutional neural networks are trivial representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacini_M/0/1/0/all/0/1&quot;&gt;Marco Pacini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaowen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1&quot;&gt;Bruno Lepri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santin_G/0/1/0/all/0/1&quot;&gt;Gabriele Santin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09239">
<title>DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery. (arXiv:2401.09239v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09239</link>
<description rdf:parseType="Literal">&lt;p&gt;Precisely determining the contact force during safe interaction in Minimally
Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired
by post-operative qualitative analysis from surgical videos, the use of
cross-modality data driven deep neural network models has been one of the
newest approaches to predict sensorless force trends. However, these methods
required for large and variable datasets which are not currently available. In
this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft
environments for the training of deep neural models. In order to reduce the
bias from a single dataset, we present a pipeline to generalize different
vision and state data inputs for mixed dataset training, using a previously
validated dataset with different setup. Finally, we present a variable
encoder-decoder architecture to predict the forces done by the laparoscopic
tool using single input or sequence of inputs. For input sequence, we use a
recurrent decoder, named with the prefix R, and a new temporal sampling to
represent the acceleration of the tool. During our training, we demonstrate
that single dataset training tends to overfit to the training data domain, but
has difficulties on translating the results across new domains. However,
dataset mixing presents a good translation with a mean relative estimated force
error of 5% and 12% for the recurrent and non-recurrent models respectively.
Our method, also marginally increase the effectiveness of transformers for
force estimation up to a maximum of ~15%, as the volume of available data is
increase by 150%. In conclusion, we demonstrate that mixing experimental set
ups for vision-state force estimation in MIRS is a possible approach towards
the general solution of the problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyzabal_M/0/1/0/all/0/1&quot;&gt;Mikel De Iturrate Reyzabal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingcong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongbin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09240">
<title>A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous Information System. (arXiv:2401.09240v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.09240</link>
<description rdf:parseType="Literal">&lt;p&gt;In our digital world, access to personal and public data has become an item
of concern, with challenging security and privacy aspects. Modern information
systems are heterogeneous in nature and have an inherent security
vulnerability, which is susceptible to data interception and data modification
due to unsecured communication data pipelines between connected endpoints. This
re-search article presents a blockchain-based model for securing data pipelines
in a heterogeneous information system using an integrated multi-hazard early
warning system (MHEWS) as a case study. The proposed model utilizes the
inherent security features of blockchain technology to address the security and
privacy concerns that arise in data pipelines. The model is designed to ensure
data integrity, confidentiality, and authenticity in a decentralized manner.
The model is evaluated in a hybrid environment using a prototype implementation
and simulation experiments with outcomes that demonstrate advantages over
traditional approaches for a tamper-proof and immutable data pipeline for data
authenticity and integrity using a confidential ledger.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramahlosi_M/0/1/0/all/0/1&quot;&gt;MN Ramahlosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madani_Y/0/1/0/all/0/1&quot;&gt;Y Madani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akanbi_A/0/1/0/all/0/1&quot;&gt;A Akanbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09243">
<title>DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09243</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot learning tasks are extremely compute-intensive and hardware-specific.
Thus the avenues of tackling these challenges, using a diverse dataset of
offline demonstrations that can be used to train robot manipulation agents, is
very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a
well-curated open-source dataset for offline training comprised mostly of
expert data and also benchmark scores of the common offline-RL and behaviour
cloning agents. In this paper, we introduce DiffClone, an offline algorithm of
enhanced behaviour cloning agent with diffusion-based policy learning, and
measured the efficacy of our method on real online physical robots at test
time. This is also our official submission to the Train-Offline-Test-Online
(TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both
pre-trained visual representation and agent policies. In our experiments, we
find that MOCO finetuned ResNet50 performs the best in comparison to other
finetuned representations. Goal state conditioning and mapping to transitions
resulted in a minute increase in the success rate and mean-reward. As for the
agent policy, we developed DiffClone, a behaviour cloning agent improved using
conditional diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mani_S/0/1/0/all/0/1&quot;&gt;Sabariswaran Mani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1&quot;&gt;Abhranil Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1&quot;&gt;Sreyas Venkataraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvi_A/0/1/0/all/0/1&quot;&gt;Adyan Rizvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirvi_Y/0/1/0/all/0/1&quot;&gt;Yash Sirvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Soumojit Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazra_A/0/1/0/all/0/1&quot;&gt;Aritra Hazra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09252">
<title>3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey. (arXiv:2401.09252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09252</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive survey on pioneer and state-of-the-art 3D
scene geometry estimation methodologies based on single, two, or multiple
images captured under the omnidirectional optics. We first revisit the basic
concepts of the spherical camera model, and review the most common acquisition
technologies and representation formats suitable for omnidirectional (also
called 360$^\circ$, spherical or panoramic) images and videos. We then survey
monocular layout and depth inference approaches, highlighting the recent
advances in learning-based solutions suited for spherical data. The classical
stereo matching is then revised on the spherical domain, where methodologies
for detecting and describing sparse and dense features become crucial. The
stereo matching concepts are then extrapolated for multiple view camera setups,
categorizing them among light fields, multi-view stereo, and structure from
motion (or visual simultaneous localization and mapping). We also compile and
discuss commonly adopted datasets and figures of merit indicated for each
purpose and list recent results for completeness. We conclude this paper by
pointing out current and future trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silveira_T/0/1/0/all/0/1&quot;&gt;Thiago Lopes Trugillo da Silveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_P/0/1/0/all/0/1&quot;&gt;Paulo Gamarra Lessa Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Llerena_J/0/1/0/all/0/1&quot;&gt;Jeffri Erwin Murrugarra Llerena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1&quot;&gt;Claudio Rosito Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.02779">
<title>A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.02779</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and images acquired at different centers
than training images, with labeling errors that violate expert knowledge. Such
errors undermine the trustworthiness of deep learning models for medical image
segmentation. Mechanisms for detecting and correcting such failures are
essential for safely translating this technology into clinics and are likely to
be a requirement of future regulations on artificial intelligence (AI). In this
work, we propose a trustworthy AI theoretical framework and a practical system
that can augment any backbone AI system using a fallback method and a fail-safe
mechanism based on Dempster-Shafer theory. Our approach relies on an actionable
definition of trustworthy AI. Our method automatically discards the voxel-level
labeling predicted by the backbone AI that violate expert knowledge and relies
on a fallback for those voxels. We demonstrate the effectiveness of the
proposed trustworthy AI approach on the largest reported annotated dataset of
fetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13
centers. Our trustworthy AI method improves the robustness of a
state-of-the-art backbone AI for fetal brain MRIs acquired across various
centers and for fetuses with various brain abnormalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1&quot;&gt;Lucas Fidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1&quot;&gt;Michael Aertsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bink_A/0/1/0/all/0/1&quot;&gt;Andrea Bink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1&quot;&gt;Anna L. David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deprest_T/0/1/0/all/0/1&quot;&gt;Thomas Deprest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Emam_D/0/1/0/all/0/1&quot;&gt;Doaa Emam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guffens_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Guffens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jakab_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Jakab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kasprian_G/0/1/0/all/0/1&quot;&gt;Gregor Kasprian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kienast_P/0/1/0/all/0/1&quot;&gt;Patric Kienast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Melbourne_A/0/1/0/all/0/1&quot;&gt;Andrew Melbourne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mufti_N/0/1/0/all/0/1&quot;&gt;Nada Mufti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pogledic_I/0/1/0/all/0/1&quot;&gt;Ivana Pogledic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prayer_D/0/1/0/all/0/1&quot;&gt;Daniela Prayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stuempflen_M/0/1/0/all/0/1&quot;&gt;Marlene Stuempflen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elslander_E/0/1/0/all/0/1&quot;&gt;Esther Van Elslander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1&quot;&gt;Jan Deprest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.05764">
<title>The dynamics of belief: continuously monitoring and visualising complex systems. (arXiv:2208.05764v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.05764</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of AI in human contexts places new demands on automated systems to
be transparent and explainable. We examine some anthropomorphic ideas and
principles relevant to such accountablity in order to develop a theoretical
framework for thinking about digital systems in complex human contexts and the
problem of explaining their behaviour. Structurally, systems are made of
modular and hierachical components, which we abstract in a new system model
using notions of modes and mode transitions. A mode is an independent component
of the system with its own objectives, monitoring data, and algorithms. The
behaviour of a mode, including its transitions to other modes, is determined by
functions that interpret each mode&apos;s monitoring data in the light of its
objectives and algorithms. We show how these belief functions can help explain
system behaviour by visualising their evaluation as trajectories in
higher-dimensional geometric spaces. These ideas are formalised mathematically
by abstract and concrete simplicial complexes. We offer three techniques: a
framework for design heuristics, a general system theory based on modes, and a
geometric visualisation, and apply them in three types of human-centred
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beggs_E/0/1/0/all/0/1&quot;&gt;Edwin J. Beggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_J/0/1/0/all/0/1&quot;&gt;John V. Tucker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02612">
<title>Lyapunov Function Consistent Adaptive Network Signal Control with Back Pressure and Reinforcement Learning. (arXiv:2210.02612v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02612</link>
<description rdf:parseType="Literal">&lt;p&gt;In traffic signal control, flow-based (optimizing the overall flow) and
pressure-based methods (equalizing and alleviating congestion) are commonly
used but often considered separately. This study introduces a unified framework
using Lyapunov control theory, defining specific Lyapunov functions
respectively for these methods. We have found interesting results. For example,
the well-recognized back-pressure method is equal to differential queue lengths
weighted by intersection lane saturation flows. We further improve it by adding
basic traffic flow theory. Rather than ensuring that the control system be
stable, the system should be also capable of adaptive to various performance
metrics. Building on insights from Lyapunov theory, this study designs a reward
function for the Reinforcement Learning (RL)-based network signal control,
whose agent is trained with Double Deep Q-Network (DDQN) for effective control
over complex traffic networks. The proposed algorithm is compared with several
traditional and RL-based methods under pure passenger car flow and heterogenous
traffic flow including freight, respectively. The numerical tests demonstrate
that the proposed method outperforms the alternative control methods across
different traffic scenarios, covering corridor and general network situations
each with varying traffic demands, in terms of the average network vehicle
waiting time per vehicle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chaolun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bruce Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahmoudzadeh_A/0/1/0/all/0/1&quot;&gt;Ahmadreza Mahmoudzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13118">
<title>Decision Diagram-Based Branch-and-Bound with Caching for Dominance and Suboptimality Detection. (arXiv:2211.13118v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13118</link>
<description rdf:parseType="Literal">&lt;p&gt;The branch-and-bound algorithm based on decision diagrams introduced by
Bergman et al. in 2016 is a framework for solving discrete optimization
problems with a dynamic programming formulation. It works by compiling a series
of bounded-width decision diagrams that can provide lower and upper bounds for
any given subproblem. Eventually, every part of the search space will be either
explored or pruned by the algorithm, thus proving optimality. This paper
presents new ingredients to speed up the search by exploiting the structure of
dynamic programming models. The key idea is to prevent the repeated expansion
of nodes corresponding to the same dynamic programming states by querying
expansion thresholds cached throughout the search. These thresholds are based
on dominance relations between partial solutions previously found and on the
pruning inequalities of the filtering techniques introduced by Gillard et al.
in 2021. Computational experiments show that the pruning brought by this
caching mechanism allows significantly reducing the number of nodes expanded by
the algorithm. This results in more benchmark instances of difficult
optimization problems being solved in less time while using narrower decision
diagrams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coppe_V/0/1/0/all/0/1&quot;&gt;Vianney Copp&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillard_X/0/1/0/all/0/1&quot;&gt;Xavier Gillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaus_P/0/1/0/all/0/1&quot;&gt;Pierre Schaus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13247">
<title>Online Loss Function Learning. (arXiv:2301.13247v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13247</link>
<description rdf:parseType="Literal">&lt;p&gt;Loss function learning is a new meta-learning paradigm that aims to automate
the essential task of designing a loss function for a machine learning model.
Existing techniques for loss function learning have shown promising results,
often improving a model&apos;s training dynamics and final inference performance.
However, a significant limitation of these techniques is that the loss
functions are meta-learned in an offline fashion, where the meta-objective only
considers the very first few steps of training, which is a significantly
shorter time horizon than the one typically used for training deep neural
networks. This causes significant bias towards loss functions that perform well
at the very start of training but perform poorly at the end of training. To
address this issue we propose a new loss function learning technique for
adaptively updating the loss function online after each update to the base
model parameters. The experimental results show that our proposed method
consistently outperforms the cross-entropy loss and offline loss function
learning techniques on a diverse range of neural network architectures and
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raymond_C/0/1/0/all/0/1&quot;&gt;Christian Raymond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11563">
<title>Self-supervised network distillation: an effective approach to exploration in sparse reward environments. (arXiv:2302.11563v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11563</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning can solve decision-making problems and train an agent
to behave in an environment according to a predesigned reward function.
However, such an approach becomes very problematic if the reward is too sparse
and so the agent does not come across the reward during the environmental
exploration. The solution to such a problem may be to equip the agent with an
intrinsic motivation that will provide informed exploration during which the
agent is likely to also encounter external reward. Novelty detection is one of
the promising branches of intrinsic motivation research. We present
Self-supervised Network Distillation (SND), a class of intrinsic motivation
algorithms based on the distillation error as a novelty indicator, where the
predictor model and the target model are both trained. We adapted three
existing self-supervised methods for this purpose and experimentally tested
them on a set of ten environments that are considered difficult to explore. The
results show that our approach achieves faster growth and higher external
reward for the same training time compared to the baseline models, which
implies improved exploration in a very sparse reward environment. In addition,
the analytical methods we applied provide valuable explanatory insights into
our proposed models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechac_M/0/1/0/all/0/1&quot;&gt;Matej Pech&amp;#xe1;&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chovanec_M/0/1/0/all/0/1&quot;&gt;Michal Chovanec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1&quot;&gt;Igor Farka&amp;#x161;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02811">
<title>HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02811</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the complex behavior arising from non-uniqueness, symmetry, and
bifurcations in the solution space, solving inverse problems of nonlinear
differential equations (DEs) with multiple solutions is a challenging task. To
address this, we propose homotopy physics-informed neural networks (HomPINNs),
a novel framework that leverages homotopy continuation and neural networks
(NNs) to solve inverse problems. The proposed framework begins with the use of
NNs to simultaneously approximate unlabeled observations across diverse
solutions while adhering to DE constraints. Through homotopy continuation, the
proposed method solves the inverse problem by tracing the observations and
identifying multiple solutions. The experiments involve testing the performance
of the proposed method on one-dimensional DEs and applying it to solve a
two-dimensional Gray-Scott simulation. Our findings demonstrate that the
proposed method is scalable and adaptable, providing an effective solution for
solving DEs with multiple solutions and unknown parameters. Moreover, it has
significant potential for various applications in scientific computing, such as
modeling complex systems and solving inverse problems in physics, chemistry,
biology, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haoyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1&quot;&gt;Wenrui Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10045">
<title>ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10045</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph contrastive learning (GCL) has recently achieved substantial
advancements. Existing GCL approaches compare two different ``views&apos;&apos; of the
same graph in order to learn node/graph representations. The underlying
assumption of these studies is that the graph augmentation strategy is capable
of generating several different graph views such that the graph views are
structurally different but semantically similar to the original graphs, and
thus the ground-truth labels of the original and augmented graph/nodes can be
regarded identical in contrastive learning. However, we observe that this
assumption does not always hold. For instance, the deletion of a super-node
within a social network can exert a substantial influence on the partitioning
of communities for other nodes. Similarly, any perturbation to nodes or edges
in a molecular graph will change the labels of the graph. Therefore, we believe
that augmenting the graph, accompanied by an adaptation of the labels used for
the contrastive loss, will facilitate the encoder to learn a better
representation. Based on this idea, we propose ID-MixGCL, which allows the
simultaneous interpolation of input nodes and corresponding identity labels to
obtain soft-confidence samples, with a controllable degree of change, leading
to the capture of fine-grained representations from self-supervised training on
unlabeled graphs. Experimental results demonstrate that ID-MixGCL improves
performance on graph classification and node classification tasks, as
demonstrated by significant improvements on the Cora, IMDB-B, IMDB-M, and
PROTEINS datasets compared to state-of-the-art techniques, by 3-29% absolute
points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gehang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bowen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiangxia Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinghua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1&quot;&gt;Jiawei Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tingwen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00969">
<title>CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v6 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00969</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the Ubenwa CryCeleb dataset - a labeled collection of
infant cries - and the accompanying CryCeleb 2023 task, which is a public
speaker verification challenge based on cry sounds. We released more than 6
hours of manually segmented cry sounds from 786 newborns for academic use,
aiming to encourage research in infant cry analysis. The inaugural public
competition attracted 59 participants, 11 of whom improved the baseline
performance. The top-performing system achieved a significant improvement
scoring 25.8% equal error rate, which is still far from the performance of
state-of-the-art adult speaker verification systems. Therefore, we believe
there is room for further research on this dataset, potentially extending
beyond the verification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budaghyan_D/0/1/0/all/0/1&quot;&gt;David Budaghyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onu_C/0/1/0/all/0/1&quot;&gt;Charles C. Onu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorin_A/0/1/0/all/0/1&quot;&gt;Arsenii Gorin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subakan_C/0/1/0/all/0/1&quot;&gt;Cem Subakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04180">
<title>Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity. (arXiv:2305.04180v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04180</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Reinforcement Learning (DRL) has exhibited efficacy in resolving the
Local Path Planning (LPP) problem. However, such application in the real world
is immensely limited due to the deficient training efficiency and
generalization capability of DRL. To alleviate these two issues, a solution
named Color is proposed, which consists of an Actor-Sharer-Learner (ASL)
training framework and a mobile robot-oriented simulator Sparrow. Specifically,
the ASL intends to improve the training efficiency of DRL algorithms. It
employs a Vectorized Data Collection (VDC) mode to expedite data acquisition,
decouples the data collection from model optimization by multithreading, and
partially connects the two procedures by harnessing a Time Feedback Mechanism
(TFM) to evade data underuse or overuse. Meanwhile, the Sparrow simulator
utilizes a 2D grid-based world, simplified kinematics, and conversion-free data
flow to achieve a lightweight design. The lightness facilitates vectorized
diversity, allowing diversified simulation setups across extensive copies of
the vectorized environments, resulting in a notable enhancement in the
generalization capability of the DRL algorithm being trained. Comprehensive
experiments, comprising 57 DRL benchmark environments, 32 simulated and 36
real-world LPP scenarios, have been conducted to corroborate the superiority of
our method in terms of efficiency and generalization. The code and the video of
this paper are accessible at https://github.com/XinJingHao/Color.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Jinghao Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Ning Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12162">
<title>A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12162</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated auction design aims to find empirically high-revenue mechanisms
through machine learning. Existing works on multi item auction scenarios can be
roughly divided into RegretNet-like and affine maximizer auctions (AMAs)
approaches. However, the former cannot strictly ensure dominant strategy
incentive compatibility (DSIC), while the latter faces scalability issue due to
the large number of allocation candidates. To address these limitations, we
propose AMenuNet, a scalable neural network that constructs the AMA parameters
(even including the allocation menu) from bidder and item representations.
AMenuNet is always DSIC and individually rational (IR) due to the properties of
AMAs, and it enhances scalability by generating candidate allocations through a
neural network. Additionally, AMenuNet is permutation equivariant, and its
number of parameters is independent of auction scale. We conduct extensive
experiments to demonstrate that AMenuNet outperforms strong baselines in both
contextual and non-contextual multi-item auctions, scales well to larger
auctions, generalizes well to different settings, and identifies useful
deterministic allocations. Overall, our proposed approach offers an effective
solution to automated DSIC auction design, with improved scalability and strong
revenue performance in various settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1&quot;&gt;Zhijian Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haoran Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yurong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiaotie Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14243">
<title>Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14243</link>
<description rdf:parseType="Literal">&lt;p&gt;Training multimodal foundation models is challenging due to the limited
availability of multimodal datasets. While many public datasets pair images
with text, few combine images with audio or text with audio. Even rarer are
datasets that align all three modalities at once. Critical domains such as
healthcare, infrastructure, or transportation are particularly affected by
missing modalities. This makes it difficult to integrate all modalities into a
large pre-trained neural network that can be used out-of-the-box or fine-tuned
for different downstream tasks. We introduce LoReTTa (Linking mOdalities with a
tRansitive and commutativE pre-Training sTrAtegy) to address this understudied
problem. Our self-supervised framework unifies causal modeling and masked
modeling with the rules of commutativity and transitivity. This allows us to
transition within and between modalities. As a result, our pre-trained models
are better at exploring the true underlying joint probability distribution.
Given a dataset containing only the disjoint combinations (A, B) and (B, C),
LoReTTa can model the relation A &amp;lt;-&amp;gt; C with A &amp;lt;-&amp;gt; B &amp;lt;-&amp;gt; C. In particular, we
show that a transformer pre-trained with LoReTTa can handle any mixture of
modalities at inference time, including the never-seen pair (A, C) and the
triplet (A, B, C). We extensively evaluate our approach on a synthetic,
medical, and reinforcement learning dataset. Across different domains, our
universal multimodal transformer consistently outperforms strong baselines such
as GPT, BERT, and CLIP on tasks involving the missing modality tuple.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Manuel Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cid_Y/0/1/0/all/0/1&quot;&gt;Yashin Dicente Cid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiani_A/0/1/0/all/0/1&quot;&gt;Amal Lahiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1&quot;&gt;Fabian J. Theis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klaiman_E/0/1/0/all/0/1&quot;&gt;Eldad Klaiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16494">
<title>Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16494</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are known to be susceptible to adversarial samples: small
variations of natural examples crafted to deliberately mislead the models.
While they can be easily generated using gradient-based techniques in digital
and physical scenarios, they often differ greatly from the actual data
distribution of natural images, resulting in a trade-off between strength and
stealthiness. In this paper, we propose a novel framework dubbed
Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic
adversarial samples. By exploiting a gradient guided by a diffusion model,
Diff-PGD ensures that adversarial samples remain close to the original data
distribution while maintaining their effectiveness. Moreover, our framework can
be easily customized for specific tasks such as digital attacks, physical-world
attacks, and style-based attacks. Compared with existing methods for generating
natural-style adversarial samples, our framework enables the separation of
optimizing adversarial loss from other surrogate losses (e.g.,
content/smoothness/style loss), making it more stable and controllable.
Finally, we demonstrate that the samples generated using Diff-PGD have better
transferability and anti-purification power than traditional gradient-based
methods. Code will be released in https://github.com/xavihart/Diff-PGD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Haotian Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1&quot;&gt;Alexandre Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17225">
<title>Causal Component Analysis. (arXiv:2305.17225v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17225</link>
<description rdf:parseType="Literal">&lt;p&gt;Independent Component Analysis (ICA) aims to recover independent latent
variables from observed mixtures thereof. Causal Representation Learning (CRL)
aims instead to infer causally related (thus often statistically dependent)
latent variables, together with the unknown graph encoding their causal
relationships. We introduce an intermediate problem termed Causal Component
Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the
causal dependence among the latent components, and as a special case of CRL. In
contrast to CRL, it presupposes knowledge of the causal graph, focusing solely
on learning the unmixing function and the causal mechanisms. Any impossibility
results regarding the recovery of the ground truth in CauCA also apply for CRL,
while possibility results may serve as a stepping stone for extensions to CRL.
We characterize CauCA identifiability from multiple datasets generated through
different types of interventions on the latent causal variables. As a
corollary, this interventional perspective also leads to new identifiability
results for nonlinear ICA -- a special case of CauCA with an empty graph --
requiring strictly fewer datasets than previous results. We introduce a
likelihood-based approach using normalizing flows to estimate both the unmixing
function and the causal mechanisms, and demonstrate its effectiveness through
extensive synthetic experiments in the CauCA and ICA setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wendong_L/0/1/0/all/0/1&quot;&gt;Liang Wendong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kekic_A/0/1/0/all/0/1&quot;&gt;Armin Keki&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Buchholz_S/0/1/0/all/0/1&quot;&gt;Simon Buchholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1&quot;&gt;Michel Besserve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1&quot;&gt;Luigi Gresele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06755">
<title>CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution. (arXiv:2306.06755v3 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06755</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an LLM-based code translation method and an
associated tool called CoTran, that translates whole-programs from one
high-level programming language to another. Current LLM-based code translation
methods lack a training approach to ensure that the translated code reliably
compiles or bears substantial functional equivalence to the input code. In our
work, we train an LLM via reinforcement learning, by modifying the fine-tuning
process to incorporate compiler feedback and symbolic execution (symexec)-based
equivalence testing feedback that checks for functional equivalence between the
input and output programs. The idea is to guide an LLM-in-training, via
compiler and symexec-based testing feedback, by letting it know how far it is
from producing perfect translations. We report on extensive experiments
comparing CoTran with 14 other code translation tools that include
human-written transpilers, LLM-based translation tools, and ChatGPT over a
benchmark of more than 57,000 Java-Python equivalent pairs, and we show that
CoTran outperforms them on relevant metrics such as compilation accuracy
(CompAcc) and functional equivalence accuracy (FEqAcc). For example, our tool
achieves 48.68% FEqAcc, 76.98% CompAcc for Python-to-Java translation, whereas
the nearest competing tool (PLBART-base) only gets 38.26% and 75.77% resp.
Also, built upon CodeT5, CoTran achieves +11.23%, +14.89% improvement on FEqAcc
and +4.07%, +8.14% on CompAcc for Java-to-Python and Python-to-Java translation
resp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_P/0/1/0/all/0/1&quot;&gt;Prithwish Jana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1&quot;&gt;Piyush Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_H/0/1/0/all/0/1&quot;&gt;Haoyang Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kishore_G/0/1/0/all/0/1&quot;&gt;Gautham Kishore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1&quot;&gt;Aryan Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_V/0/1/0/all/0/1&quot;&gt;Vijay Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09980">
<title>Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09980</link>
<description rdf:parseType="Literal">&lt;p&gt;What is a useful skill hierarchy for an autonomous agent? We propose an
answer based on a graphical representation of how the interaction between an
agent and its environment may unfold. Our approach uses modularity maximisation
as a central organising principle to expose the structure of the interaction
graph at multiple levels of abstraction. The result is a collection of skills
that operate at varying time scales, organised into a hierarchy, where skills
that operate over longer time scales are composed of skills that operate over
shorter time scales. The entire skill hierarchy is generated automatically,
with no human intervention, including the skills themselves (their behaviour,
when they can be called, and when they terminate) as well as the hierarchical
dependency structure between them. In a wide range of environments, this
approach generates skill hierarchies that are intuitively appealing and that
considerably improve the learning performance of the agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_J/0/1/0/all/0/1&quot;&gt;Joshua B. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simsek_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;zg&amp;#xfc;r &amp;#x15e;im&amp;#x15f;ek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13649">
<title>On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes. (arXiv:2306.13649v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13649</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation (KD) is widely used for compressing a teacher model to
reduce its inference cost and memory footprint, by training a smaller student
model. However, current KD methods for auto-regressive sequence models suffer
from distribution mismatch between output sequences seen during training and
those generated by the student during inference. To address this issue, we
introduce Generalized Knowledge Distillation (GKD). Instead of solely relying
on a fixed set of output sequences, GKD trains the student on its
self-generated output sequences by leveraging feedback from the teacher on such
sequences. Unlike supervised KD approaches, GKD also offers the flexibility to
employ alternative loss functions between the student and teacher, which can be
useful when the student lacks the expressivity to mimic the teacher&apos;s
distribution. Furthermore, GKD facilitates the seamless integration of
distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for
distilling auto-regressive language models on summarization, translation, and
arithmetic reasoning tasks, and task-agnostic distillation for
instruction-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1&quot;&gt;Rishabh Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1&quot;&gt;Nino Vieillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yongchao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanczyk_P/0/1/0/all/0/1&quot;&gt;Piotr Stanczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_S/0/1/0/all/0/1&quot;&gt;Sabela Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1&quot;&gt;Olivier Bachem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02040">
<title>VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02040</link>
<description rdf:parseType="Literal">&lt;p&gt;Vertical Federated Learning (VFL) is a crucial paradigm for training machine
learning models on feature-partitioned, distributed data. However, due to
privacy restrictions, few public real-world VFL datasets exist for algorithm
evaluation, and these represent a limited array of feature distributions.
Existing benchmarks often resort to synthetic datasets, derived from arbitrary
feature splits from a global set, which only capture a subset of feature
distributions, leading to inadequate algorithm performance assessment. This
paper addresses these shortcomings by introducing two key factors affecting VFL
performance - feature importance and feature correlation - and proposing
associated evaluation metrics and dataset splitting methods. Additionally, we
introduce a real VFL dataset to address the deficit in image-image VFL
scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides
valuable insights for future research in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhaomin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junyi Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1&quot;&gt;Bingsheng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08177">
<title>Using an LLM to Help With Code Understanding. (arXiv:2307.08177v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08177</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding code is challenging, especially when working in new and complex
development environments. Code comments and documentation can help, but are
typically scarce or hard to navigate. Large language models (LLMs) are
revolutionizing the process of writing code. Can they do the same for helping
understand it? In this study, we provide a first investigation of an LLM-based
conversational UI built directly in the IDE that is geared towards code
understanding. Our IDE plugin queries OpenAI&apos;s GPT-3.5-turbo model with four
high-level requests without the user having to write explicit prompts: to
explain a highlighted section of code, provide details of API calls used in the
code, explain key domain-specific terms, and provide usage examples for an API.
The plugin also allows for open-ended prompts, which are automatically
contextualized to the LLM with the program being edited. We evaluate this
system in a user study with 32 participants, which confirms that using our
plugin can aid task completion more than web search. We additionally provide a
thorough analysis of the ways developers use, and perceive the usefulness of,
our system, among others finding that the usage and benefits differ between
students and professionals. We conclude that in-IDE prompt-less interaction
with LLMs is a promising future direction for tool builders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1&quot;&gt;Daye Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macvean_A/0/1/0/all/0/1&quot;&gt;Andrew Macvean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1&quot;&gt;Vincent Hellendoorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Vasilescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_B/0/1/0/all/0/1&quot;&gt;Brad Myers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09487">
<title>DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack. (arXiv:2308.09487v4 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09487</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of backdoor attacks, accurate labeling of injected data is
essential for evading rudimentary detection mechanisms. This imperative has
catalyzed the development of clean-label attacks, which are notably more
elusive as they preserve the original labels of the injected data. Current
clean-label attack methodologies primarily depend on extensive knowledge of the
training dataset. However, practically, such comprehensive dataset access is
often unattainable, given that training datasets are typically compiled from
various independent sources. Departing from conventional clean-label attack
methodologies, our research introduces DFB, a data-free, low-budget, and
high-efficacy clean-label backdoor Attack. DFB is unique in its independence
from training data access, requiring solely the knowledge of a specific target
class. Tested on CIFAR10, Tiny-ImageNet, and TSRD, DFB demonstrates remarkable
efficacy with minimal poisoning rates of just 0.1%, 0.025%, and 0.4%,
respectively. These rates are significantly lower than those required by
existing methods such as LC, HTBA, BadNets, and Blend, yet DFB achieves
superior attack success rates. Furthermore, our findings reveal that DFB poses
a formidable challenge to four established backdoor defense algorithms,
indicating its potential as a robust tool in advanced clean-label attack
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1&quot;&gt;Binhao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dejun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1&quot;&gt;Bo Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12143">
<title>A Probabilistic Fluctuation based Membership Inference Attack for Diffusion Models. (arXiv:2308.12143v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12143</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership Inference Attack (MIA) identifies whether a record exists in a
machine learning model&apos;s training set by querying the model. MIAs on the
classic classification models have been well-studied, and recent works have
started to explore how to transplant MIA onto generative models. Our
investigation indicates that existing MIAs designed for generative models
mainly depend on the overfitting in target models. However, overfitting can be
avoided by employing various regularization techniques, whereas existing MIAs
demonstrate poor performance in practice. Unlike overfitting, memorization is
essential for deep learning models to attain optimal performance, making it a
more prevalent phenomenon. Memorization in generative models leads to an
increasing trend in the probability distribution of generating records around
the member record. Therefore, we propose a Probabilistic Fluctuation Assessing
Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by
detecting these trends via analyzing the overall probabilistic fluctuations
around given records. We conduct extensive experiments across multiple
generative models and datasets, which demonstrate PFAMI can improve the attack
success rate (ASR) by about 27.9% when compared with the best baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1&quot;&gt;Wenjie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huandong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13426">
<title>A Chat About Boring Problems: Studying GPT-based text normalization. (arXiv:2309.13426v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13426</link>
<description rdf:parseType="Literal">&lt;p&gt;Text normalization - the conversion of text from written to spoken form - is
traditionally assumed to be an ill-formed task for language models. In this
work, we argue otherwise. We empirically show the capacity of Large-Language
Models (LLM) for text normalization in few-shot scenarios. Combining
self-consistency reasoning with linguistic-informed prompt engineering, we find
LLM based text normalization to achieve error rates around 40\% lower than top
normalization systems. Further, upon error analysis, we note key limitations in
the conventional design of text normalization tasks. We create a new taxonomy
of text normalization errors and apply it to results from GPT-3.5-Turbo and
GPT-4.0. Through this new framework, we can identify strengths and weaknesses
of GPT-based TN, opening opportunities for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartley_T/0/1/0/all/0/1&quot;&gt;Travis M. Bartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graterol_Fuenmayor_M/0/1/0/all/0/1&quot;&gt;Mariana Graterol-Fuenmayor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavrukhin_V/0/1/0/all/0/1&quot;&gt;Vitaly Lavrukhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1&quot;&gt;Evelina Bakhturina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1&quot;&gt;Boris Ginsburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14517">
<title>Watch Your Language: Investigating Content Moderation with Large Language Models. (arXiv:2309.14517v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14517</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have exploded in popularity due to their ability
to perform a wide array of natural language tasks. Text-based content
moderation is one LLM use case that has received recent enthusiasm, however,
there is little research investigating how LLMs perform in content moderation
settings. In this work, we evaluate a suite of commodity LLMs on two common
content moderation tasks: rule-based community moderation and toxic content
detection. For rule-based community moderation, we instantiate 95 subcommunity
specific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We
find that GPT-3.5 is effective at rule-based moderation for many communities,
achieving a median accuracy of 64% and a median precision of 83%. For toxicity
detection, we evaluate a suite of commodity LLMs (GPT-3, GPT-3.5, GPT-4, Gemini
Pro, LLAMA 2) and show that LLMs significantly outperform currently widespread
toxicity classifiers. However, recent increases in model size add only marginal
benefit to toxicity detection, suggesting a potential performance plateau for
LLMs on toxicity detection tasks. We conclude by outlining avenues for future
work in studying LLMs and content moderation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Deepak Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AbuHashem_Y/0/1/0/all/0/1&quot;&gt;Yousef AbuHashem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1&quot;&gt;Zakir Durumeric&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16042">
<title>Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16042</link>
<description rdf:parseType="Literal">&lt;p&gt;Mechanistic interpretability seeks to understand the internal mechanisms of
machine learning models, where localization -- identifying the important model
components -- is a key step. Activation patching, also known as causal tracing
or interchange intervention, is a standard technique for this task (Vig et al.,
2020), but the literature contains many variants with little consensus on the
choice of hyperparameters or methodology. In this work, we systematically
examine the impact of methodological details in activation patching, including
evaluation metrics and corruption methods. In several settings of localization
and circuit discovery in language models, we find that varying these
hyperparameters could lead to disparate interpretability results. Backed by
empirical observations, we give conceptual arguments for why certain metrics or
methods may be preferred. Finally, we provide recommendations for the best
practices of activation patching going forwards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fred Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1&quot;&gt;Neel Nanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00229">
<title>Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00229</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by human conscious planning, we propose Skipper, a model-based
reinforcement learning agent utilizing spatio-temporal abstractions to
generalize learned skills in novel situations. It automatically decomposes the
given task into smaller, more manageable subtasks, and hence enables sparse
decision-making and focused computation on the relevant parts of the
environment. This relies on the extraction of an abstracted proxy problem
represented as a directed graph, in which vertices and edges are learned
end-to-end from hindsight. Our theoretical analyses provide performance
guarantees under appropriate assumptions and establish where our approach is
expected to be helpful. Generalization-focused experiments validate Skipper&apos;s
significant advantage in zero-shot generalization, compared to existing
state-of-the-art hierarchical planning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mingde Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alver_S/0/1/0/all/0/1&quot;&gt;Safa Alver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1&quot;&gt;Harm van Seijen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08056">
<title>Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08056</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from Label Proportions (LLP) is a learning problem where only
aggregate level labels are available for groups of instances, called bags,
during training, and the aim is to get the best performance at the
instance-level on the test data. This setting arises in domains like
advertising and medicine due to privacy considerations. We propose a novel
algorithmic framework for this problem that iteratively performs two main
steps. For the first step (Pseudo Labeling) in every iteration, we define a
Gibbs distribution over binary instance labels that incorporates a) covariate
information through the constraint that instances with similar covariates
should have similar labels and b) the bag level aggregated label. We then use
Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo
labels. In the second step (Embedding Refinement), we use the pseudo labels to
provide supervision for a learner that yields a better embedding. Further, we
iterate on the two steps again by using the second step&apos;s embeddings as new
covariates for the next iteration. In the final iteration, a classifier is
trained using the pseudo labels. Our algorithm displays strong gains against
several SOTA baselines (up to 15%) for the LLP Binary Classification problem on
various dataset types - tabular and Image. We achieve these improvements with
minimal computational overhead above standard supervised learning due to Belief
Propagation, for large bag sizes, even for a million samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreyas Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1&quot;&gt;Navodita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sareen_S/0/1/0/all/0/1&quot;&gt;Shubhi Sareen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1&quot;&gt;Aravindan Raghuveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13121">
<title>Understanding Addition in Transformers. (arXiv:2310.13121v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13121</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the inner workings of machine learning models like Transformers
is vital for their safe and ethical use. This paper presents an in-depth
analysis of a one-layer Transformer model trained for n-digit integer addition.
We reveal that the model divides the task into parallel, digit-specific streams
and employs distinct algorithms for different digit positions. Our study also
finds that the model starts calculations late but executes them rapidly. A rare
use case with high loss is identified and explained. Overall, the model&apos;s
algorithm is explained in detail. These findings are validated through rigorous
testing and mathematical modeling, contributing to the broader works in
Mechanistic Interpretability, AI safety, and alignment. Our approach opens the
door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quirke_P/0/1/0/all/0/1&quot;&gt;Philip Quirke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05937">
<title>Genetic Algorithm enhanced by Deep Reinforcement Learning in parent selection mechanism and mutation : Minimizing makespan in permutation flow shop scheduling problems. (arXiv:2311.05937v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05937</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a reinforcement learning (RL) approach to address the
challenges associated with configuring and optimizing genetic algorithms (GAs)
for solving difficult combinatorial or non-linear problems. The proposed RL+GA
method was specifically tested on the flow shop scheduling problem (FSP). The
hybrid algorithm incorporates neural networks (NN) and uses the off-policy
method Q-learning or the on-policy method Sarsa(0) to control two key genetic
algorithm (GA) operators: parent selection mechanism and mutation. At each
generation, the RL agent&apos;s action is determining the selection method, the
probability of the parent selection and the probability of the offspring
mutation. This allows the RL agent to dynamically adjust the selection and
mutation based on its learned policy. The results of the study highlight the
effectiveness of the RL+GA approach in improving the performance of the
primitive GA. They also demonstrate its ability to learn and adapt from
population diversity and solution improvements over time. This adaptability
leads to improved scheduling solutions compared to static parameter
configurations while maintaining population diversity throughout the
evolutionary process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irmouli_M/0/1/0/all/0/1&quot;&gt;Maissa Irmouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benazzoug_N/0/1/0/all/0/1&quot;&gt;Nourelhouda Benazzoug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adimi_A/0/1/0/all/0/1&quot;&gt;Alaa Dania Adimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezkellah_F/0/1/0/all/0/1&quot;&gt;Fatma Zohra Rezkellah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamzaoui_I/0/1/0/all/0/1&quot;&gt;Imane Hamzaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamitouche_T/0/1/0/all/0/1&quot;&gt;Thanina Hamitouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bessedik_M/0/1/0/all/0/1&quot;&gt;Malika Bessedik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayeb_F/0/1/0/all/0/1&quot;&gt;Fatima Si Tayeb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11866">
<title>Analyzing Emissions and Energy Efficiency at Unsignalized Real-world Intersections Under Mixed Traffic Control. (arXiv:2311.11866v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11866</link>
<description rdf:parseType="Literal">&lt;p&gt;Greenhouse gas emissions have dramatically risen since the early 1900s with
U.S. transportation generating 28% of U.S. emissions. As such, there is
interest in reducing transportation-related emissions. Specifically,
sustainability research has sprouted around signalized intersections as
intersections allow different streams of traffic to cross and change
directions. Recent research has developed mixed traffic control eco-driving
strategies at signalized intersections to decrease emissions. However, the
inherent structure of a signalized intersection generates increased emissions
by creating frequent acceleration/deceleration events, excessive idling from
traffic congestion, and stop-and-go waves. Thus, we believe unsignalized
intersections hold potential for further sustainability improvements. In this
work, we provide an emissions analysis on unsignalized intersections with
complex, real-world topologies and traffic demands where mixed traffic control
strategies are employed by robot vehicles (RVs) to reduce wait times and
congestion. We find with at least 10% RV penetration rate, RVs generate less
fuel consumption, CO2 emissions, and NOx emissions than signalized
intersections by up to 27%, 27% and 28%, respectively. With at least 30% RVs,
CO and HC emissions are reduced by up to 42% and 43%, respectively.
Additionally, RVs can reduce network-wide emissions despite only employing
their strategies at intersections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villarreal_M/0/1/0/all/0/1&quot;&gt;Michael Villarreal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dawei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jia Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weizi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04118">
<title>Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play. (arXiv:2312.04118v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04118</link>
<description rdf:parseType="Literal">&lt;p&gt;Infants&apos; ability to recognize and categorize objects develops gradually. The
second year of life is marked by both the emergence of more semantic visual
representations and a better understanding of word meaning. This suggests that
language input may play an important role in shaping visual representations.
However, even in suitable contexts for word learning like dyadic play sessions,
caregivers utterances are sparse and ambiguous, often referring to objects that
are different from the one to which the child attends. Here, we systematically
investigate to what extent caregivers&apos; utterances can nevertheless enhance
visual representations. For this we propose a computational model of visual
representation learning during dyadic play. We introduce a synthetic dataset of
ego-centric images perceived by a toddler-agent that moves and rotates toy
objects in different parts of its home environment while hearing caregivers&apos;
utterances, modeled as captions. We propose to model toddlers&apos; learning as
simultaneously aligning representations for 1) close-in-time images and 2)
co-occurring images and utterances. We show that utterances with statistics
matching those of real caregivers give rise to representations supporting
improved category recognition. Our analysis reveals that a small
decrease/increase in object-relevant naming frequencies can drastically impact
the learned representations. This affects the attention on object names within
an utterance, which is required for efficient visuo-linguistic alignment.
Overall, our results support the hypothesis that caregivers&apos; naming utterances
can improve toddlers&apos; visual representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaumloffel_T/0/1/0/all/0/1&quot;&gt;Timothy Schauml&amp;#xf6;ffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1&quot;&gt;Arthur Aubret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1&quot;&gt;Gemma Roig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1&quot;&gt;Jochen Triesch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04350">
<title>CLadder: Assessing Causal Reasoning in Language Models. (arXiv:2312.04350v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04350</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to perform causal reasoning is widely considered a core feature
of intelligence. In this work, we investigate whether large language models
(LLMs) can coherently reason about causality. Much of the existing work in
natural language processing (NLP) focuses on evaluating commonsense causal
reasoning in LLMs, thus failing to assess whether a model can perform causal
inference in accordance with a set of well-defined formal rules. To address
this, we propose a new NLP task, causal inference in natural language, inspired
by the &quot;causal inference engine&quot; postulated by Judea Pearl et al. We compose a
large dataset, CLadder, with 10K samples: based on a collection of causal
graphs and queries (associational, interventional, and counterfactual), we
obtain symbolic questions and ground-truth answers, through an oracle causal
inference engine. These are then translated into natural language. We evaluate
multiple LLMs on our dataset, and we introduce and evaluate a bespoke
chain-of-thought prompting strategy, CausalCoT. We show that our task is highly
challenging for LLMs, and we conduct an in-depth analysis to gain deeper
insights into the causal reasoning abilities of LLMs. Our data is open-sourced
at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found
at https://github.com/causalNLP/cladder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhijing Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1&quot;&gt;Felix Leeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gresele_L/0/1/0/all/0/1&quot;&gt;Luigi Gresele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamal_O/0/1/0/all/0/1&quot;&gt;Ojasv Kamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1&quot;&gt;Kevin Blin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adauto_F/0/1/0/all/0/1&quot;&gt;Fernando Gonzalez Adauto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleiman_Weiner_M/0/1/0/all/0/1&quot;&gt;Max Kleiman-Weiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04864">
<title>Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks. (arXiv:2312.04864v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04864</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks, threat models, and malicious actors are advancing quickly. With the
increased deployment of the 5G networks, the security issues of the attached 5G
physical devices have also increased. Therefore, artificial intelligence based
autonomous end-to-end security design is needed that can deal with incoming
threats by detecting network traffic anomalies. To address this requirement, in
this research, we used a recently published 5G traffic dataset, 5G-NIDD, to
detect network traffic anomalies using machine and deep learning approaches.
First, we analyzed the dataset using three visualization techniques:
t-Distributed Stochastic Neighbor Embedding (t-SNE), Uniform Manifold
Approximation and Projection (UMAP), and Principal Component Analysis (PCA).
Second, we reduced the data dimensionality using mutual information and PCA
techniques. Third, we solve the class imbalance issue by inserting synthetic
records of minority classes. Last, we performed classification using six
different classifiers and presented the evaluation metrics. We received the
best results when K-Nearest Neighbors classifier was used: accuracy (97.2%),
detection rate (96.7%), and false positive rate (2.2%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghani_H/0/1/0/all/0/1&quot;&gt;Humera Ghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salekzamankhani_S/0/1/0/all/0/1&quot;&gt;Shahram Salekzamankhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virdee_B/0/1/0/all/0/1&quot;&gt;Bal Virdee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04960">
<title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04960</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) achieve superior performance on various tasks
compared to convolutional neural networks (CNNs), but ViTs are also vulnerable
to adversarial attacks. Adversarial training is one of the most successful
methods to build robust CNN models. Thus, recent works explored new
methodologies for adversarial training of ViTs based on the differences between
ViTs and CNNs, such as better training strategies, preventing attention from
focusing on a single block, or discarding low-attention embeddings. However,
these methods still follow the design of traditional supervised adversarial
training, limiting the potential of adversarial training on ViTs. This paper
proposes a novel defense method, MIMIR, which aims to build a different
adversarial training methodology by utilizing Masked Image Modeling at
pre-training. We create an autoencoder that accepts adversarial examples as
input but takes the clean examples as the modeling target. Then, we create a
mutual information (MI) penalty following the idea of the Information
Bottleneck. Among the two information source inputs and corresponding
adversarial perturbation, the perturbation information is eliminated due to the
constraint of the modeling target. Next, we provide a theoretical analysis of
MIMIR using the bounds of the MI penalty. We also design two adaptive attacks
when the adversary is aware of the MIMIR defense and show that MIMIR still
performs well. The experimental results show that MIMIR improves (natural and
adversarial) accuracy on average by 4.19% on CIFAR-10 and 5.52% on ImageNet-1K,
compared to baselines. On Tiny-ImageNet, we obtained improved natural accuracy
of 2.99\% on average and comparable adversarial accuracy. Our code and trained
models are publicly available https://github.com/xiaoyunxxy/MIMIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingzheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03006">
<title>The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;This survey delves into the application of diffusion models in time-series
forecasting. Diffusion models are demonstrating state-of-the-art results in
various fields of generative AI. The paper includes comprehensive background
information on diffusion models, detailing their conditioning methods and
reviewing their use in time-series forecasting. The analysis covers 11 specific
time-series implementations, the intuition and theory behind them, the
effectiveness on different datasets, and a comparison among each other. Key
contributions of this work are the thorough exploration of diffusion models&apos;
applications in time-series forecasting and a chronologically ordered overview
of these models. Additionally, the paper offers an insightful discussion on the
current state-of-the-art in this domain and outlines potential future research
directions. This serves as a valuable resource for researchers in AI and
time-series analysis, offering a clear view of the latest advancements and
future potential of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meijer_C/0/1/0/all/0/1&quot;&gt;Caspar Meijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lydia Y. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03955">
<title>Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03955</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained models for zero/few-shot learning excel in language and
vision domains but encounter challenges in multivariate time series (TS) due to
the diverse nature and scarcity of publicly available pre-training data.
Consequently, there has been a recent surge in utilizing pre-trained large
language models (LLMs) with token adaptations for TS forecasting. These
approaches employ cross-domain transfer learning and surprisingly yield
impressive results. However, these models are typically very slow and large
(~billion parameters) and do not consider cross-channel correlations. To
address this, we present Tiny Time Mixers (TTM), a significantly small model
based on the lightweight TSMixer architecture. TTM marks the first success in
developing fast and tiny general pre-trained models (&amp;lt;1M parameters),
exclusively trained on public TS datasets, with effective transfer learning
capabilities for forecasting. To tackle the complexity of pre-training on
multiple datasets with varied temporal resolutions, we introduce several novel
enhancements such as adaptive patching, dataset augmentation via downsampling,
and resolution prefix tuning. Moreover, we employ a multi-level modeling
strategy to effectively model channel correlations and infuse exogenous signals
during fine-tuning, a crucial capability lacking in existing benchmarks. TTM
shows significant accuracy gains (12-38\%) over popular benchmarks in
few/zero-shot forecasting. It also drastically reduces the compute needs as
compared to LLM-TS methods, with a 14X cut in learnable parameters, 106X less
total parameters, and substantial reductions in fine-tuning (65X) and inference
time (54X). In fact, TTM&apos;s zero-shot often surpasses the few-shot results in
many popular benchmarks, highlighting the efficacy of our approach. Code and
pre-trained models will be open-sourced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ekambaram_V/0/1/0/all/0/1&quot;&gt;Vijay Ekambaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jati_A/0/1/0/all/0/1&quot;&gt;Arindam Jati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Nam H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayama_P/0/1/0/all/0/1&quot;&gt;Pankaj Dayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandra Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gifford_W/0/1/0/all/0/1&quot;&gt;Wesley M. Gifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalagnanam_J/0/1/0/all/0/1&quot;&gt;Jayant Kalagnanam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04124">
<title>MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. (arXiv:2401.04124v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04124</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents centered around Large Language Models (LLMs) are now capable of
automating mobile device operations for users. After fine-tuning to learn a
user&apos;s mobile operations, these agents can adhere to high-level user
instructions online. They execute tasks such as goal decomposition, sequencing
of sub-goals, and interactive environmental exploration, until the final
objective is achieved. However, privacy concerns related to personalized user
data arise during mobile operations, requiring user confirmation. Moreover,
users&apos; real-world operations are exploratory, with action data being complex
and redundant, posing challenges for agent learning. To address these issues,
in our practical application, we have designed interactive tasks between agents
and humans to identify sensitive information and align with personalized user
needs. Additionally, we integrated Standard Operating Procedure (SOP)
information within the model&apos;s in-context learning to enhance the agent&apos;s
comprehension of complex task execution. Our approach is evaluated on the new
device control benchmark AitW, which encompasses 30K unique instructions across
multi-step tasks, including application operation, web searching, and web
shopping. Experimental results show that the SOP-based agent achieves
state-of-the-art performance in LLMs without incurring additional inference
costs, boasting an overall action success rate of 66.92\%. The code and data
examples are available at https://github.com/alipay/mobile-agent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tinghe Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05268">
<title>AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05268</link>
<description rdf:parseType="Literal">&lt;p&gt;Language agents have achieved considerable performance on various complex
tasks. Despite the incessant exploration in this field, existing language agent
systems still struggle with costly, non-reproducible data reliance and face the
challenge of compelling a single model for multiple functions. To this end, we
introduce AutoAct, an automatic agent learning framework that does not rely on
large-scale annotated data and synthetic trajectories from closed-source models
(e.g., GPT-4). Given limited data with a tool library, AutoAct first
automatically synthesizes planning trajectories without any assistance from
humans or strong closed-source models. Then, AutoAct leverages a
division-of-labor strategy to automatically differentiate based on the target
task information and synthesized trajectories, producing a sub-agent group to
complete the task. We conduct comprehensive experiments with different LLMs,
which demonstrates that AutoAct yields better or parallel performance compared
to various strong baselines. We even notice that AutoAct, when using the
Llama-2-13b model, can achieve performance comparable to that of the zero-shot
GPT-3.5-Turbo agent. Code will be available at
https://github.com/zjunlp/AutoAct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Shuofei Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1&quot;&gt;Runnan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yujie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Eleanor Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chengfei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05302">
<title>Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?. (arXiv:2401.05302v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05302</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models have shown exceptional generative abilities in various
natural language and generation tasks. However, possible anthropomorphization
and leniency towards failure cases have propelled discussions on emergent
abilities of Large Language Models especially on Theory of Mind (ToM) abilities
in Large Language Models. While several false-belief tests exists to verify the
ability to infer and maintain mental models of another entity, we study a
special application of ToM abilities that has higher stakes and possibly
irreversible consequences : Human Robot Interaction. In this work, we explore
the task of Perceived Behavior Recognition, where a robot employs a Large
Language Model (LLM) to assess the robot&apos;s generated behavior in a manner
similar to human observer. We focus on four behavior types, namely -
explicable, legible, predictable, and obfuscatory behavior which have been
extensively used to synthesize interpretable robot behaviors. The LLMs goal is,
therefore to be a human proxy to the agent, and to answer how a certain agent
behavior would be perceived by the human in the loop, for example &quot;Given a
robot&apos;s behavior X, would the human observer find it explicable?&quot;. We conduct a
human subject study to verify that the users are able to correctly answer such
a question in the curated situations (robot setting and plan) across five
domains. A first analysis of the belief test yields extremely positive results
inflating ones expectations of LLMs possessing ToM abilities. We then propose
and perform a suite of perturbation tests which breaks this illusion, i.e.
Inconsistent Belief, Uninformative Context and Conviction Test. We conclude
that, the high score of LLMs on vanilla prompts showcases its potential use in
HRI settings, however to possess ToM demands invariance to trivial or
irrelevant perturbations in the context which LLMs lack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1&quot;&gt;Mudit Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1&quot;&gt;Siddhant Bhambri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05960">
<title>Machine Learning Insides OptVerse AI Solver: Design Principles and Applications. (arXiv:2401.05960v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05960</link>
<description rdf:parseType="Literal">&lt;p&gt;In an era of digital ubiquity, efficient resource management and
decision-making are paramount across numerous industries. To this end, we
present a comprehensive study on the integration of machine learning (ML)
techniques into Huawei Cloud&apos;s OptVerse AI Solver, which aims to mitigate the
scarcity of real-world mathematical programming instances, and to surpass the
capabilities of traditional optimization techniques. We showcase our methods
for generating complex SAT and MILP instances utilizing generative models that
mirror multifaceted structures of real-world problem. Furthermore, we introduce
a training framework leveraging augmentation policies to maintain solvers&apos;
utility in dynamic environments. Besides the data generation and augmentation,
our proposed approaches also include novel ML-driven policies for personalized
solver strategies, with an emphasis on applications like graph convolutional
networks for initial basis selection and reinforcement learning for advanced
presolving and cut selection. Additionally, we detail the incorporation of
state-of-the-art parameter tuning algorithms which markedly elevate solver
performance. Compared with traditional solvers such as Cplex and SCIP, our
ML-augmented OptVerse AI Solver demonstrates superior speed and precision
across both established benchmarks and real-world scenarios, reinforcing the
practical imperative and effectiveness of machine learning techniques in
mathematical programming solvers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fangzhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_H/0/1/0/all/0/1&quot;&gt;Hui-Ling Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weilin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Meng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yimin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhenan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zirui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1&quot;&gt;Yufei Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhihai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1&quot;&gt;Zijie Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1&quot;&gt;Zhiwu An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Muming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Defeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1&quot;&gt;Tao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jia Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingxuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1&quot;&gt;Kun Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06506">
<title>Frequency Masking for Universal Deepfake Detection. (arXiv:2401.06506v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06506</link>
<description rdf:parseType="Literal">&lt;p&gt;We study universal deepfake detection. Our goal is to detect synthetic images
from a range of generative AI approaches, particularly from emerging ones which
are unseen during training of the deepfake detector. Universal deepfake
detection requires outstanding generalization capability. Motivated by recently
proposed masked image modeling which has demonstrated excellent generalization
in self-supervised pre-training, we make the first attempt to explore masked
image modeling for universal deepfake detection. We study spatial and frequency
domain masking in training deepfake detectors. Based on empirical analysis, we
propose a novel deepfake detector via frequency masking. Our focus on frequency
domain is different from the majority, which primarily target spatial domain
detection. Our comparative analyses reveal substantial performance gains over
existing methods. Code and models are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doloriel_C/0/1/0/all/0/1&quot;&gt;Chandler Timm Doloriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06792">
<title>LightHouse: A Survey of AGI Hallucination. (arXiv:2401.06792v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06792</link>
<description rdf:parseType="Literal">&lt;p&gt;With the development of artificial intelligence, large-scale models have
become increasingly intelligent. However, numerous studies indicate that
hallucinations within these large models are a bottleneck hindering the
development of AI research. In the pursuit of achieving strong artificial
intelligence, a significant volume of research effort is being invested in the
AGI (Artificial General Intelligence) hallucination research. Previous
explorations have been conducted in researching hallucinations within LLMs
(Large Language Models). As for multimodal AGI, research on hallucinations is
still in an early stage. To further the progress of research in the domain of
hallucinatory phenomena, we present a bird&apos;s eye view of hallucinations in AGI,
summarizing the current work on AGI hallucinations and proposing some
directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07595">
<title>E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07595</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces E3x, a software package for building neural networks
that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$,
consisting of translations, rotations, and reflections of three-dimensional
space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models
promise benefits whenever input and/or output data are quantities associated
with three-dimensional objects. This is because the numeric values of such
quantities (e.g. positions) typically depend on the chosen coordinate system.
Under transformations of the reference frame, the values change predictably,
but the underlying rules can be difficult to learn for ordinary machine
learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks
are guaranteed to satisfy the relevant transformation rules exactly, resulting
in superior data efficiency and accuracy. The code for E3x is available from
https://github.com/google-research/e3x, detailed documentation and usage
examples can be found on https://e3x.readthedocs.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unke_O/0/1/0/all/0/1&quot;&gt;Oliver T. Unke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maennel_H/0/1/0/all/0/1&quot;&gt;Hartmut Maennel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07993">
<title>Carrying over algorithm in transformers. (arXiv:2401.07993v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07993</link>
<description rdf:parseType="Literal">&lt;p&gt;Addition is perhaps one of the simplest arithmetic tasks one can think of and
is usually performed using the carrying over algorithm. This algorithm consists
of two tasks: adding digits in the same position and carrying over a one
whenever necessary. We study how transformer models implement this algorithm
and how the two aforementioned tasks are allocated to different parts of the
network. We first focus on two-layer encoder-only models and show that the
carrying over algorithm is implemented in a modular fashion. The first layer is
mostly responsible for adding digits in the same position. The second layer
first decides, in the attention, which positions need a carried one or not, and
then performs the carrying of the one in the final MLP. We provide a simple way
of precisely identifying which neurons are responsible for that task. This
implementation of the carrying over algorithm occurs across a range of
hyperparameters for two as well as three-layer models. For small decoder-only
models, we observe the same implementation and provide suggestive evidence for
its existence in three 7B large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruthoff_J/0/1/0/all/0/1&quot;&gt;Jorrit Kruthoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08268">
<title>An Explainable Proxy Model for Multiabel Audio Segmentation. (arXiv:2401.08268v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08268</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio signal segmentation is a key task for automatic audio indexing. It
consists of detecting the boundaries of class-homogeneous segments in the
signal. In many applications, explainable AI is a vital process for
transparency of decision-making with machine learning. In this paper, we
propose an explainable multilabel segmentation model that solves speech
activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD)
simultaneously. This proxy uses the non-negative matrix factorization (NMF) to
map the embedding used for the segmentation to the frequency domain.
Experiments conducted on two datasets show similar performances as the
pre-trained black box model while showing strong explainability features.
Specifically, the frequency bins used for the decision can be easily identified
at both the segment level (local explanations) and global level (class
prototypes).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mariotte_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Mariotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Almudevar_A/0/1/0/all/0/1&quot;&gt;Antonio Almud&amp;#xe9;var&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tahon_M/0/1/0/all/0/1&quot;&gt;Marie Tahon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Alfonso Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08383">
<title>Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference. (arXiv:2401.08383v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08383</link>
<description rdf:parseType="Literal">&lt;p&gt;In large language models like the Generative Pre-trained Transformer, the
Mixture of Experts paradigm has emerged as a powerful technique for enhancing
model expressiveness and accuracy. However, deploying GPT MoE models for
parallel inference on distributed systems presents significant challenges,
primarily due to the extensive Alltoall communication required for expert
routing and aggregation. This communication bottleneck exacerbates the already
complex computational landscape, hindering the efficient utilization of
high-performance computing resources. In this paper, we propose a lightweight
optimization technique called ExFlow, to largely accelerate the inference of
these MoE models. We take a new perspective on alleviating the communication
overhead by exploiting the inter-layer expert affinity. Unlike previous
methods, our solution can be directly applied to pre-trained MoE models without
any fine-tuning or accuracy degradation. By proposing a context-coherent expert
parallelism on distributed systems, our design only uses one Alltoall
communication to deliver the same functionality while previous methods all
require two Alltoalls. By carefully examining the conditional probability in
tokens&apos; routing across multiple layers, we proved that pre-trained GPT MoE
models implicitly exhibit a strong inter-layer expert affinity. We then design
an efficient integer programming model to capture such features and show that
by properly placing the experts on corresponding GPUs, we can reduce up to 67%
cross-GPU routing latency. Our solution beats the cutting-edge MoE
implementations with experts from 8 to 64, with up to 2.2x improvement in
inference throughput. We further provide a detailed study of how the model
implicitly acquires this expert affinity at the very early training stage and
how this affinity evolves and stabilizes during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jinghan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1&quot;&gt;Quentin Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1&quot;&gt;Aamir Shafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1&quot;&gt;Hari Subramoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+K%2E_D/0/1/0/all/0/1&quot;&gt;Dhabaleswar K.&lt;/a&gt; (DK) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda/0/1/0/all/0/1&quot;&gt;Panda&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>