<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-31T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.04053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11073" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.16336">
<title>ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16336</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ToddlerBERTa, a BabyBERTa-like language model, exploring its
capabilities through five different models with varied hyperparameters.
Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the
BabyLM challenge, we find that smaller models can excel in specific tasks,
while larger models perform well with substantial data. Despite training on a
smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling
the state-of-the-art RoBERTa-base. The model showcases robust language
understanding, even with single-sentence pretraining, and competes with
baselines that leverage broader contextual information. Our work provides
insights into hyperparameter choices, and data utilization, contributing to the
advancement of language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagatan_O/0/1/0/all/0/1&quot;&gt;Omer Veysel Cagatan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16349">
<title>Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations. (arXiv:2308.16349v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16349</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Affective Visual Dialog, an emotion explanation and reasoning
task as a testbed for research on understanding the formation of emotions in
visually grounded conversations. The task involves three skills: (1)
Dialog-based Question Answering (2) Dialog-based Emotion Prediction and (3)
Affective emotion explanation generation based on the dialog. Our key
contribution is the collection of a large-scale dataset, dubbed AffectVisDial,
consisting of 50K 10-turn visually grounded dialogs as well as concluding
emotion attributions and dialog-informed textual emotion explanations,
resulting in a total of 27,180 working hours. We explain our design decisions
in collecting the dataset and introduce the questioner and answerer tasks that
are associated with the participants in the conversation. We train and
demonstrate solid Affective Visual Dialog baselines adapted from
state-of-the-art models. Remarkably, the responses generated by our models show
promising emotional reasoning abilities in response to visually grounded
conversations. Our project page is available at
https://affective-visual-dialog.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haydarov_K/0/1/0/all/0/1&quot;&gt;Kilichbek Haydarov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1&quot;&gt;Avinash Madasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salem_M/0/1/0/all/0/1&quot;&gt;Mahmoud Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1&quot;&gt;Gamaleldin Elsayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16415">
<title>Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer. (arXiv:2308.16415v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16415</link>
<description rdf:parseType="Literal">&lt;p&gt;Streaming automatic speech recognition (ASR) models are restricted from
accessing future context, which results in worse performance compared to the
non-streaming models. To improve the performance of streaming ASR, knowledge
distillation (KD) from the non-streaming to streaming model has been studied,
mainly focusing on aligning the output token probabilities. In this paper, we
propose a layer-to-layer KD from the teacher encoder to the student encoder. To
ensure that features are extracted using the same context, we insert auxiliary
non-streaming branches to the student and perform KD from the non-streaming
teacher layer to the non-streaming auxiliary layer. We design a special KD loss
that leverages the autoregressive predictive coding (APC) mechanism to
encourage the streaming model to predict unseen future contexts. Experimental
results show that the proposed method can significantly reduce the word error
rate compared to previous token probability distillation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1&quot;&gt;Kyuhong Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jinkyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Simyung Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_K/0/1/0/all/0/1&quot;&gt;Kyuwoong Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16458">
<title>BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.16458</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained language models like ChatGPT have significantly improved code
generation. As these models scale up, there is an increasing need for the
output to handle more intricate tasks. Moreover, in bioinformatics, generating
functional programs poses additional notable challenges due to the amount of
domain knowledge, the need for complicated data operations, and intricate
functional dependencies between the operations. Here, we present BioCoder, a
benchmark developed to evaluate existing pre-trained models in generating
bioinformatics code. In relation to function-code generation, BioCoder covers
potential package dependencies, class declarations, and global variables. It
incorporates 1026 functions and 1243 methods in Python and Java from GitHub and
253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing
framework for evaluation, and we have applied it to evaluate many models
including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+,
InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes
the importance of domain knowledge, pragmatic code generation, and contextual
understanding. Our dataset, benchmark, Docker images, and scripts required for
testing are all available at https://github.com/gersteinlab/biocoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiangru Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1&quot;&gt;Bill Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rick Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiakang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1&quot;&gt;Mark Gerstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16463">
<title>Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16463</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models exhibit enhanced zero-shot performance on various tasks
when fine-tuned with instruction-following data. Multimodal
instruction-following models extend these capabilities by integrating both text
and images. However, existing models such as MiniGPT-4 face challenges in
maintaining dialogue coherence in scenarios involving multiple images. A
primary reason is the lack of a specialized dataset for this critical
application. To bridge these gaps, we present SparklesChat, a multimodal
instruction-following model for open-ended dialogues across multiple images. To
support the training, we introduce SparklesDialogue, the first
machine-generated dialogue dataset tailored for word-level interleaved
multi-image and text interactions. Furthermore, we construct SparklesEval, a
GPT-assisted benchmark for quantitatively assessing a model&apos;s conversational
competence across multiple images and dialogue turns. Our experiments validate
the effectiveness of SparklesChat in understanding and reasoning across
multiple images and dialogue turns. Specifically, SparklesChat outperformed
MiniGPT-4 on established vision-and-language benchmarks, including the BISON
binary image selection task and the NLVR2 visual reasoning task. Moreover,
SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding
MiniGPT-4&apos;s score of 3.91 and nearing GPT-4&apos;s score of 9.26. Qualitative
evaluations further demonstrate SparklesChat&apos;s generality in handling
real-world applications. All resources will be available at
https://github.com/HYPJUDY/Sparkles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yupan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zaiqiao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fangyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yixuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1&quot;&gt;Nigel Collier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yutong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16469">
<title>Link Prediction for Wikipedia Articles as a Natural Language Inference Task. (arXiv:2308.16469v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16469</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction task is vital to automatically understanding the structure of
large knowledge bases. In this paper, we present our system to solve this task
at the Data Science and Advanced Analytics 2023 Competition &quot;Efficient and
Effective Link Prediction&quot; (DSAA-2023 Competition) with a corpus containing
948,233 training and 238,265 for public testing. This paper introduces an
approach to link prediction in Wikipedia articles by formulating it as a
natural language inference (NLI) task. Drawing inspiration from recent
advancements in natural language processing and understanding, we cast link
prediction as an NLI task, wherein the presence of a link between two articles
is treated as a premise, and the task is to determine whether this premise
holds based on the information presented in the articles. We implemented our
system based on the Sentence Pair Classification for Link Prediction for the
Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000
Macro F1-score for the public and private test sets, respectively. Our team
UIT-NLP ranked 3rd in performance on the private test set, equal to the scores
of the first and second places. Our code is publicly for research purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_C/0/1/0/all/0/1&quot;&gt;Chau-Thang Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc-Nam Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Kiet Van Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16474">
<title>Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16474</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Model (MLLM) refers to a model expanded from a
Large Language Model (LLM) that possesses the capability to handle and infer
multi-modal data. Current MLLMs typically begin by using LLMs to decompose
tasks into multiple subtasks, then employing individual pre-trained models to
complete specific subtasks, and ultimately utilizing LLMs to integrate the
results of each subtasks to obtain the results of the task. In real-world
scenarios, when dealing with large projects, it is common practice to break
down the project into smaller sub-projects, with different teams providing
corresponding solutions or results. The project owner then decides which
solution or result to use, ensuring the best possible outcome for each subtask
and, consequently, for the entire project. Inspired by this, this study
considers selecting multiple pre-trained models to complete the same subtask.
By combining the results from multiple pre-trained models, the optimal subtask
result is obtained, enhancing the performance of the MLLM. Specifically, this
study first selects multiple pre-trained models focused on the same subtask
based on distinct evaluation approaches, and then invokes these models in
parallel to process input data and generate corresponding subtask results.
Finally, the results from multiple pre-trained models for the same subtask are
compared using the LLM, and the best result is chosen as the outcome for that
subtask. Extensive experiments are conducted in this study using GPT-4
annotated datasets and human-annotated datasets. The results of various
evaluation metrics adequately demonstrate the effectiveness of the proposed
approach in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Feng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinhai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Donghong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16475">
<title>Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16475</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose TCSP, a novel method for compressing a transformer model by
focusing on reducing the hidden size of the model. By projecting the whole
transform model into a subspace, we enable matrix operations between the weight
matrices in the model and features in a reduced-dimensional space, leading to
significant reductions in model parameters and computing resources. To
establish this subspace, we decompose the feature matrix, derived from
different layers of sampled data instances, into a projection matrix. For
evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and
SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a
compression ratio of 44\% with at most 1.6\% degradation in accuracy,
surpassing or matching prior compression methods. Furthermore, TCSP exhibits
compatibility with other methods targeting filter and attention head size
compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cuiping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16498">
<title>Generalised Winograd Schema and its Contextuality. (arXiv:2308.16498v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16498</link>
<description rdf:parseType="Literal">&lt;p&gt;Ambiguities in natural language give rise to probability distributions over
interpretations. The distributions are often over multiple ambiguous words at a
time; a multiplicity which makes them a suitable topic for sheaf-theoretic
models of quantum contextuality. Previous research showed that different
quantitative measures of contextuality correlate well with Psycholinguistic
research on lexical ambiguities. In this work, we focus on coreference
ambiguities and investigate the Winograd Schema Challenge (WSC), a test
proposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC
consists of a collection of multiple-choice questions that require
disambiguating pronouns in sentences structured according to the Winograd
schema, in a way that makes it difficult for machines to determine the correct
referents but remains intuitive for human comprehension. In this study, we
propose an approach that analogously models the Winograd schema as an
experiment in quantum physics. However, we argue that the original Winograd
Schema is inherently too simplistic to facilitate contextuality. We introduce a
novel mechanism for generalising the schema, rendering it analogous to a
Bell-CHSH measurement scenario. We report an instance of this generalised
schema, complemented by the human judgements we gathered via a crowdsourcing
platform. The resulting model violates the Bell-CHSH inequality by 0.192, thus
exhibiting contextuality in a coreference resolution setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1&quot;&gt;Kin Ian Lo&lt;/a&gt; (University College London, London, UK), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1&quot;&gt;Mehrnoosh Sadrzadeh&lt;/a&gt; (University College London, London, UK), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansfield_S/0/1/0/all/0/1&quot;&gt;Shane Mansfield&lt;/a&gt; (Quandela, Paris, France)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16537">
<title>The Smart Data Extractor, a Clinician Friendly Solution to Accelerate and Improve the Data Collection During Clinical Trials. (arXiv:2308.16537v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2308.16537</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical research, the traditional way to collect data, i.e. browsing
patient files, has been proven to induce bias, errors, human labor and costs.
We propose a semi-automated system able to extract every type of data,
including notes. The Smart Data Extractor pre-populates clinic research forms
by following rules. We performed a cross-testing experiment to compare
semi-automated to manual data collection. 20 target items had to be collected
for 79 patients. The average time to complete one form was 6&apos;81&apos;&apos; for manual
data collection and 3&apos;22&apos;&apos; with the Smart Data Extractor. There were also more
mistakes during manual data collection (163 for the whole cohort) than with the
Smart Data Extractor (46 for the whole cohort). We present an easy to use,
understandable and agile solution to fill out clinical research forms. It
reduces human effort and provides higher quality data, avoiding data re-entry
and fatigue induced errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Quennelle_S/0/1/0/all/0/1&quot;&gt;Sophie Quennelle&lt;/a&gt; (HeKA, UPCit&amp;#xe9;, CRC), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Douillet_M/0/1/0/all/0/1&quot;&gt;Maxime Douillet&lt;/a&gt; (Imagine), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Friedlander_L/0/1/0/all/0/1&quot;&gt;Lisa Friedlander&lt;/a&gt; (UPCit&amp;#xe9;), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Boyer_O/0/1/0/all/0/1&quot;&gt;Olivia Boyer&lt;/a&gt; (UPCit&amp;#xe9;), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Burgun_A/0/1/0/all/0/1&quot;&gt;Anita Burgun&lt;/a&gt; (HeKA, UPCit&amp;#xe9;, CRC), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Neuraz_A/0/1/0/all/0/1&quot;&gt;Antoine Neuraz&lt;/a&gt; (HeKA, UPCit&amp;#xe9;, CRC), &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Garcelon_N/0/1/0/all/0/1&quot;&gt;Nicolas Garcelon&lt;/a&gt; (HeKA, UPCit&amp;#xe9;, Imagine)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16540">
<title>Time-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking in Speech Signals. (arXiv:2308.16540v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2308.16540</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new method for the accurate estimation and
tracking of formants in speech signals using time-varying quasi-closed-phase
(TVQCP) analysis. Conventional formant tracking methods typically adopt a
two-stage estimate-and-track strategy wherein an initial set of formant
candidates are estimated using short-time analysis (e.g., 10--50 ms), followed
by a tracking stage based on dynamic programming or a linear state-space model.
One of the main disadvantages of these approaches is that the tracking stage,
however good it may be, cannot improve upon the formant estimation accuracy of
the first stage. The proposed TVQCP method provides a single-stage formant
tracking that combines the estimation and tracking stages into one. TVQCP
analysis combines three approaches to improve formant estimation and tracking:
(1) it uses temporally weighted quasi-closed-phase analysis to derive
closed-phase estimates of the vocal tract with reduced interference from the
excitation source, (2) it increases the residual sparsity by using the $L_1$
optimization and (3) it uses time-varying linear prediction analysis over long
time windows (e.g., 100--200 ms) to impose a continuity constraint on the vocal
tract model and hence on the formant trajectories. Formant tracking experiments
with a wide variety of synthetic and natural speech signals show that the
proposed TVQCP method performs better than conventional and popular formant
tracking tools, such as Wavesurfer and Praat (based on dynamic programming),
the KARMA algorithm (based on Kalman filtering), and DeepFormants (based on
deep neural networks trained in a supervised manner). Matlab scripts for the
proposed method can be found at: https://github.com/njaygowda/ftrack
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gowda_D/0/1/0/all/0/1&quot;&gt;Dhananjaya Gowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kadiri_S/0/1/0/all/0/1&quot;&gt;Sudarsana Reddy Kadiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Story_B/0/1/0/all/0/1&quot;&gt;Brad Story&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alku_P/0/1/0/all/0/1&quot;&gt;Paavo Alku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16549">
<title>Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection. (arXiv:2308.16549v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16549</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is a summary of the work in my PhD thesis. In which, I investigate
the impact of bias in NLP models on the task of hate speech detection from
three perspectives: explainability, offensive stereotyping bias, and fairness.
I discuss the main takeaways from my thesis and how they can benefit the
broader NLP community. Finally, I discuss important future research directions.
The findings of my thesis suggest that bias in NLP models impacts the task of
hate speech detection from all three perspectives. And that unless we start
incorporating social sciences in studying bias in NLP models, we will not
effectively overcome the current limitations of measuring and mitigating bias
in NLP models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsafoury_F/0/1/0/all/0/1&quot;&gt;Fatma Elsafoury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16577">
<title>Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information. (arXiv:2308.16577v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.16577</link>
<description rdf:parseType="Literal">&lt;p&gt;For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays
an important role in producing natural and intelligible speech. Although
inter-utterance linguistic information can influence the speech interpretation
of the target utterance, previous works on PSP mainly focus on utilizing
intrautterance linguistic information of the current utterance only. This work
proposes to use inter-utterance linguistic information to improve the
performance of PSP. Multi-level contextual information, which includes both
inter-utterance and intrautterance linguistic information, is extracted by a
hierarchical encoder from character level, utterance level and discourse level
of the input text. Then a multi-task learning (MTL) decoder predicts prosodic
boundaries from multi-level contextual information. Objective evaluation
results on two datasets show that our method achieves better F1 scores in
predicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase
(IPH). It demonstrates the effectiveness of using multi-level contextual
information for PSP. Subjective preference tests also indicate the naturalness
of synthesized speeches are improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Changhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuo_D/0/1/0/all/0/1&quot;&gt;Deyi Tuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xixin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shiyin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1&quot;&gt;Helen Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16584">
<title>Unsupervised Text Style Transfer with Deep Generative Models. (arXiv:2308.16584v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16584</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a general framework for unsupervised text style transfer with deep
generative models. The framework models each sentence-label pair in the
non-parallel corpus as partially observed from a complete quadruplet which
additionally contains two latent codes representing the content and style,
respectively. These codes are learned by exploiting dependencies inside the
observed data. Then a sentence is transferred by manipulating them. Our
framework is able to unify previous embedding and prototype methods as two
special forms. It also provides a principled perspective to explain previously
proposed techniques in the field such as aligned encoder and adversarial
training. We further conduct experiments on three benchmarks. Both automatic
and human evaluation results show that our methods achieve better or
competitive results compared to several strong baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongtao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1&quot;&gt;Yiming Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16588">
<title>Interpreting Sentiment Composition with Latent Semantic Tree. (arXiv:2308.16588v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16588</link>
<description rdf:parseType="Literal">&lt;p&gt;As the key to sentiment analysis, sentiment composition considers the
classification of a constituent via classifications of its contained
sub-constituents and rules operated on them. Such compositionality has been
widely studied previously in the form of hierarchical trees including untagged
and sentiment ones, which are intrinsically suboptimal in our view. To address
this, we propose semantic tree, a new tree form capable of interpreting the
sentiment composition in a principled way. Semantic tree is a derivation of a
context-free grammar (CFG) describing the specific composition rules on
difference semantic roles, which is designed carefully following previous
linguistic conclusions. However, semantic tree is a latent variable since there
is no its annotation in regular datasets. Thus, in our method, it is
marginalized out via inside algorithm and learned to optimize the
classification performance. Quantitative and qualitative results demonstrate
that our method not only achieves better or competitive results compared to
baselines in the setting of regular and domain adaptation classification, and
also generates plausible tree explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongtao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiansong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16593">
<title>Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis. (arXiv:2308.16593v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.16593</link>
<description rdf:parseType="Literal">&lt;p&gt;The spontaneous behavior that often occurs in conversations makes speech more
human-like compared to reading-style. However, synthesizing spontaneous-style
speech is challenging due to the lack of high-quality spontaneous datasets and
the high cost of labeling spontaneous behavior. In this paper, we propose a
semi-supervised pre-training method to increase the amount of spontaneous-style
speech and spontaneous behavioral labels. In the process of semi-supervised
learning, both text and speech information are considered for detecting
spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is
used to model the relationship between each sentence in the conversation.
Experimental results indicate that our proposed method achieves superior
expressive speech synthesis performance with the ability to model spontaneous
behavior in spontaneous-style speech and predict reasonable spontaneous
behavior from text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiqin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shun Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiaochu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yixuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shiyin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1&quot;&gt;Helen Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16622">
<title>Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering. (arXiv:2308.16622v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.16622</link>
<description rdf:parseType="Literal">&lt;p&gt;As the field of Large Language Models (LLMs) evolves at an accelerated pace,
the critical need to assess and monitor their performance emerges. We introduce
a benchmarking framework focused on knowledge graph engineering (KGE)
accompanied by three challenges addressing syntax and error correction, facts
extraction and dataset generation. We show that while being a useful tool, LLMs
are yet unfit to assist in knowledge graph generation with zero-shot prompting.
Consequently, our LLM-KG-Bench framework provides automatic evaluation and
storage of LLM responses as well as statistical data and visualization tools to
support tracking of prompt engineering and model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_L/0/1/0/all/0/1&quot;&gt;Lars-Peter Meyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frey_J/0/1/0/all/0/1&quot;&gt;Johannes Frey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junghanns_K/0/1/0/all/0/1&quot;&gt;Kurt Junghanns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brei_F/0/1/0/all/0/1&quot;&gt;Felix Brei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulert_K/0/1/0/all/0/1&quot;&gt;Kirill Bulert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grunder_Fahrer_S/0/1/0/all/0/1&quot;&gt;Sabine Gr&amp;#xfc;nder-Fahrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1&quot;&gt;Michael Martin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16687">
<title>DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew. (arXiv:2308.16687v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16687</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DictaBERT, a new state-of-the-art pre-trained BERT model for
modern Hebrew, outperforming existing models on most benchmarks. Additionally,
we release two fine-tuned versions of the model, designed to perform two
specific foundational tasks in the analysis of Hebrew texts: prefix
segmentation and morphological tagging. These fine-tuned models allow any
developer to perform prefix segmentation and morphological tagging of a Hebrew
sentence with a single call to a HuggingFace model, without the need to
integrate any additional libraries or code. In this paper we describe the
details of the training as well and the results on the different benchmarks. We
release the models to the community, along with sample code demonstrating their
use. We release these models as part of our goal to help further research and
development in Hebrew NLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmidman_S/0/1/0/all/0/1&quot;&gt;Shaltiel Shmidman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmidman_A/0/1/0/all/0/1&quot;&gt;Avi Shmidman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koppel_M/0/1/0/all/0/1&quot;&gt;Moshe Koppel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16688">
<title>Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology. (arXiv:2308.16688v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16688</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: In this paper, we present an automated method for article
classification, leveraging the power of Large Language Models (LLM). The
primary focus is on the field of ophthalmology, but the model is extendable to
other fields. Methods: We have developed a model based on Natural Language
Processing (NLP) techniques, including advanced LLMs, to process and analyze
the textual content of scientific papers. Specifically, we have employed
zero-shot learning (ZSL) LLM models and compared against Bidirectional and
Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder
Representations from Transformers (BERT), and its variant such as distilBERT,
SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate
the effectiveness of LLMs in categorizing large number of ophthalmology papers
without human intervention. Results: To evalute the LLMs, we compiled a dataset
(RenD) of 1000 ocular disease-related articles, which were expertly annotated
by a panel of six specialists into 15 distinct categories. The model achieved
mean accuracy of 0.86 and mean F1 of 0.85 based on the RenD dataset.
Conclusion: The proposed framework achieves notable improvements in both
accuracy and efficiency. Its application in the domain of ophthalmology
showcases its potential for knowledge organization and retrieval in other
domains too. We performed trend analysis that enables the researchers and
clinicians to easily categorize and retrieve relevant papers, saving time and
effort in literature review and information gathering as well as identification
of emerging scientific trends within different disciplines. Moreover, the
extendibility of the model to other scientific fields broadens its impact in
facilitating research and trend analysis across diverse disciplines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raja_H/0/1/0/all/0/1&quot;&gt;Hina Raja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munawar_A/0/1/0/all/0/1&quot;&gt;Asim Munawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delsoz_M/0/1/0/all/0/1&quot;&gt;Mohammad Delsoz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1&quot;&gt;Mohammad Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madadi_Y/0/1/0/all/0/1&quot;&gt;Yeganeh Madadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1&quot;&gt;Amr Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serhan_H/0/1/0/all/0/1&quot;&gt;Hashem Abu Serhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inam_O/0/1/0/all/0/1&quot;&gt;Onur Inam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermandez_L/0/1/0/all/0/1&quot;&gt;Luis Hermandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1&quot;&gt;Sang Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_W/0/1/0/all/0/1&quot;&gt;Wuqas Munir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abd_Alrazaq_A/0/1/0/all/0/1&quot;&gt;Alaa Abd-Alrazaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+SiamakYousefi/0/1/0/all/0/1&quot;&gt;SiamakYousefi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16692">
<title>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models. (arXiv:2308.16692v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16692</link>
<description rdf:parseType="Literal">&lt;p&gt;Current speech large language models build upon discrete speech
representations, which can be categorized into semantic tokens and acoustic
tokens. However, existing speech tokens are not specifically designed for
speech language modeling. To assess the suitability of speech tokens for
building speech language models, we established the first benchmark,
SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are
ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech
tokenizer for speech large language models. SpeechTokenizer adopts the
Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying
semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of
speech information hierarchically across different RVQ layers. Furthermore, We
construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer.
Experiments show that SpeechTokenizer performs comparably to EnCodec in speech
reconstruction and demonstrates strong performance on the SLMTokBench
benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks.
Code and models are available at
https://github.com/ZhangXInFD/SpeechTokenizer/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shimin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yaqian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16705">
<title>CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16705</link>
<description rdf:parseType="Literal">&lt;p&gt;English datasets predominantly reflect the perspectives of certain
nationalities, which can lead to cultural biases in models and datasets. This
is particularly problematic in tasks heavily influenced by subjectivity, such
as hate speech detection. To delve into how individuals from different
countries perceive hate speech, we introduce CReHate, a cross-cultural
re-annotation of the sampled SBIC dataset. This dataset includes annotations
from five distinct countries: Australia, Singapore, South Africa, the United
Kingdom, and the United States. Our thorough statistical analysis highlights
significant differences based on nationality, with only 59.4% of the samples
achieving consensus among all countries. We also introduce a culturally
sensitive hate speech classifier via transfer learning, adept at capturing
perspectives of different nationalities. These findings underscore the need to
re-evaluate certain aspects of NLP research, especially with regard to the
nuanced nature of hate speech in the English language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Nayeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1&quot;&gt;Chani Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myung_J/0/1/0/all/0/1&quot;&gt;Junho Myung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiho Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1&quot;&gt;Alice Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16763">
<title>Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16763</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of
Large Language Models (LLMs) through the generation of intermediate rationales.
However, these enhancements predominantly benefit large-scale models, leaving
small LMs without significant performance improvements when directly applying
CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily
on their pre-trained internal knowledge. The external knowledge that is
previously unknown to the model remains unexploited. This omission becomes
pronounced in tasks such as stance detection, where the external background
knowledge plays a pivotal role. Additionally, the large-scale architecture of
LLMs inevitably present efficiency challenges during deployment. To address
these challenges, we introduce the Ladder-of-Thought (LoT) for stance
detection. Grounded in a dual-phase Cascaded Optimization framework, LoT
directs the model to incorporate high-quality external knowledge, enhancing the
intermediate rationales it generates. These bolstered rationales subsequently
serve as the foundation for more precise predictions - akin to how a ladder
facilitates reaching elevated goals. LoT achieves a balance between efficiency
and accuracy, making it an adaptable and efficient framework for stance
detection. Our empirical evaluations underscore LoT&apos;s effectiveness, marking a
16% improvement over ChatGPT and a 10% enhancement compared to ChatGPT with
CoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kairui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor W. Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1&quot;&gt;Wen Haw Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yap_Y/0/1/0/all/0/1&quot;&gt;Yong Keong Yap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16770">
<title>Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning and Prompt-tuning with Rules. (arXiv:2308.16770v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16770</link>
<description rdf:parseType="Literal">&lt;p&gt;The increased digitization of the labour market has given researchers,
educators, and companies the means to analyze and better understand the labour
market. However, labour market resources, although available in high volumes,
tend to be unstructured, and as such, research towards methodologies for the
identification, linking, and extraction of entities becomes more and more
important. Against the backdrop of this quest for better labour market
representations, resource constraints and the unavailability of large-scale
annotated data cause a reliance on human domain experts. We demonstrate the
effectiveness of prompt-based tuning of pre-trained language models (PLM) in
labour market specific applications. Our results indicate that cost-efficient
methods such as PTR and instruction tuning without exemplars can significantly
increase the performance of PLMs on downstream labour market applications
without introducing additional model layers, manual annotations, and data
augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vrolijk_J/0/1/0/all/0/1&quot;&gt;Jarno Vrolijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graus_D/0/1/0/all/0/1&quot;&gt;David Graus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16795">
<title>Towards Multilingual Automatic Dialogue Evaluation. (arXiv:2308.16795v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16795</link>
<description rdf:parseType="Literal">&lt;p&gt;The main limiting factor in the development of robust multilingual dialogue
evaluation metrics is the lack of multilingual data and the limited
availability of open sourced multilingual dialogue systems. In this work, we
propose a workaround for this lack of data by leveraging a strong multilingual
pretrained LLM and augmenting existing English dialogue data using Machine
Translation. We empirically show that the naive approach of finetuning a
pretrained multilingual encoder model with translated data is insufficient to
outperform the strong baseline of finetuning a multilingual model with only
source data. Instead, the best approach consists in the careful curation of
translated data using MT Quality Estimation metrics, excluding low quality
translations that hinder its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendonca_J/0/1/0/all/0/1&quot;&gt;John Mendon&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavie_A/0/1/0/all/0/1&quot;&gt;Alon Lavie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trancoso_I/0/1/0/all/0/1&quot;&gt;Isabel Trancoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16797">
<title>Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation. (arXiv:2308.16797v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16797</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant research effort in the development of automatic dialogue
evaluation metrics, little thought is given to evaluating dialogues other than
in English. At the same time, ensuring metrics are invariant to semantically
similar responses is also an overlooked topic. In order to achieve the desired
properties of robustness and multilinguality for dialogue evaluation metrics,
we propose a novel framework that takes advantage of the strengths of current
evaluation models with the newly-established paradigm of prompting Large
Language Models (LLMs). Empirical results show our framework achieves state of
the art results in terms of mean Spearman correlation scores across several
benchmarks and ranks first place on both the Robust and Multilingual tasks of
the DSTC11 Track 4 &quot;Automatic Evaluation Metrics for Open-Domain Dialogue
Systems&quot;, proving the evaluation capabilities of prompted LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendonca_J/0/1/0/all/0/1&quot;&gt;John Mendon&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_P/0/1/0/all/0/1&quot;&gt;Patr&amp;#xed;cia Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavie_A/0/1/0/all/0/1&quot;&gt;Alon Lavie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trancoso_I/0/1/0/all/0/1&quot;&gt;Isabel Trancoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16824">
<title>Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16824</link>
<description rdf:parseType="Literal">&lt;p&gt;When human programmers have mastered a programming language, it would be
easier when they learn a new programming language. In this report, we focus on
exploring whether programming languages can boost each other during the
instruction fine-tuning phase of code large language models. We conduct
extensive experiments of 8 popular programming languages (Python, JavaScript,
TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that
programming languages can significantly improve each other. For example,
CodeM-Python 15B trained on Python is able to increase Java by an absolute
17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B
trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our
training data is released at https://github.com/NL2Code/CodeM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1&quot;&gt;Daoguang Zan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Ailun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bo Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Taihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1&quot;&gt;Bing Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jichuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yafen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongji Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qianxiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16871">
<title>The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages. (arXiv:2308.16871v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16871</link>
<description rdf:parseType="Literal">&lt;p&gt;Gender biases in language generation systems are challenging to mitigate. One
possible source for these biases is gender representation disparities in the
training and evaluation data. Despite recent progress in documenting this
problem and many attempts at mitigating it, we still lack shared methodology
and tooling to report gender representation in large datasets. Such
quantitative reporting will enable further mitigation, e.g., via data
augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware
Polyglot Pipeline), an automatic pipeline to characterize gender representation
in large-scale datasets for 55 languages. The pipeline uses a multilingual
lexicon of gendered person-nouns to quantify the gender representation in text.
We showcase it to report gender representation in WMT training data and
development data for the News task, confirming that current data is skewed
towards masculine representation. Having unbalanced datasets may indirectly
optimize our systems towards outperforming one gender over the others. We
suggest introducing our gender quantification pipeline in current datasets and,
ideally, modifying them toward a balanced representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1&quot;&gt;Benjamin Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1&quot;&gt;Belen Alastruey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansanti_P/0/1/0/all/0/1&quot;&gt;Prangthip Hansanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalbassi_E/0/1/0/all/0/1&quot;&gt;Elahe Kalbassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropers_C/0/1/0/all/0/1&quot;&gt;Christophe Ropers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1&quot;&gt;Eric Michael Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Adina Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrews_P/0/1/0/all/0/1&quot;&gt;Pierre Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1&quot;&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16884">
<title>The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.16884</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Belebele, a multiple-choice machine reading comprehension (MRC)
dataset spanning 122 language variants. Significantly expanding the language
coverage of natural language understanding (NLU) benchmarks, this dataset
enables the evaluation of text models in high-, medium-, and low-resource
languages. Each question is based on a short passage from the Flores-200
dataset and has four multiple-choice answers. The questions were carefully
curated to discriminate between models with different levels of general
language comprehension. The English dataset on its own proves difficult enough
to challenge state-of-the-art language models. Being fully parallel, this
dataset enables direct comparison of model performance across all languages. We
use this dataset to evaluate the capabilities of multilingual masked language
models (MLMs) and large language models (LLMs). We present extensive results
and find that despite significant cross-lingual transfer in English-centric
LLMs, much smaller MLMs pretrained on balanced multilingual data still
understand far more languages. We also observe that larger vocabulary size and
conscious vocabulary construction correlate with better performance on
low-resource languages. Overall, Belebele opens up new avenues for evaluating
and analyzing the multilingual capabilities of NLP systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandarkar_L/0/1/0/all/0/1&quot;&gt;Lucas Bandarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1&quot;&gt;Davis Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1&quot;&gt;Benjamin Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1&quot;&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husa_D/0/1/0/all/0/1&quot;&gt;Donald Husa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1&quot;&gt;Naman Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1&quot;&gt;Abhinandan Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1&quot;&gt;Madian Khabsa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16890">
<title>TouchStone: Evaluating Vision-Language Models by Language Models. (arXiv:2308.16890v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16890</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models (LVLMs) have recently witnessed rapid
advancements, exhibiting a remarkable capacity for perceiving, understanding,
and processing visual information by connecting visual receptor with large
language models (LLMs). However, current assessments mainly focus on
recognizing and reasoning abilities, lacking direct evaluation of
conversational skills and neglecting visual storytelling abilities. In this
paper, we propose an evaluation method that uses strong LLMs as judges to
comprehensively evaluate the various abilities of LVLMs. Firstly, we construct
a comprehensive visual dialogue dataset TouchStone, consisting of open-world
images and questions, covering five major categories of abilities and 27
subtasks. This dataset not only covers fundamental recognition and
comprehension but also extends to literary creation. Secondly, by integrating
detailed image annotations we effectively transform the multimodal input
content into a form understandable by LLMs. This enables us to employ advanced
LLMs for directly evaluating the quality of the multimodal dialogue without
requiring human intervention. Through validation, we demonstrate that powerful
LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging
their textual capabilities alone, aligning with human preferences. We hope our
work can serve as a touchstone for LVLMs&apos; evaluation and pave the way for
building stronger LVLMs. The evaluation code is available at
https://github.com/OFA-Sys/TouchStone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shuai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shusheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinze Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16898">
<title>Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.16898</link>
<description rdf:parseType="Literal">&lt;p&gt;Since its inception in &quot;Attention Is All You Need&quot;, transformer architecture
has led to revolutionary advancements in NLP. The attention layer within the
transformer admits a sequence of input tokens $X$ and makes them interact
through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where
$(K,Q)$ are the trainable key-query parameters. In this work, we establish a
formal equivalence between the optimization geometry of self-attention and a
hard-margin SVM problem that separates optimal input tokens from non-optimal
tokens using linear constraints on the outer-products of token pairs. This
formalism allows us to characterize the implicit bias of 1-layer transformers
optimized with gradient descent: (1) Optimizing the attention layer with
vanishing regularization, parameterized by $(K,Q)$, converges in direction to
an SVM solution minimizing the nuclear norm of the combined parameter
$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm
objective. We characterize this convergence, highlighting that it can occur
toward locally-optimal directions rather than global ones. (2) Complementing
this, we prove the local/global directional convergence of gradient descent
under suitable geometric conditions. Importantly, we show that
over-parameterization catalyzes global convergence by ensuring the feasibility
of the SVM problem and by guaranteeing a benign optimization landscape devoid
of stationary points. (3) While our theory applies primarily to linear
prediction heads, we propose a more general SVM equivalence that predicts the
implicit bias with nonlinear heads. Our findings are applicable to arbitrary
datasets and their validity is verified via experiments. We also introduce
several open problems and research directions. We believe these findings
inspire the interpretation of transformers as a hierarchy of SVMs that
separates and selects optimal tokens.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1&quot;&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16911">
<title>PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.16911</link>
<description rdf:parseType="Literal">&lt;p&gt;The unprecedented advancements in Large Language Models (LLMs) have created a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, thereby enabling LLMs to understand point clouds and offering
a new avenue beyond 2D visual data. PointLLM processes colored object point
clouds with human instructions and generates contextually appropriate
responses, illustrating its grasp of point clouds and common sense.
Specifically, it leverages a point cloud encoder with a powerful LLM to
effectively fuse geometric, appearance, and linguistic information. We collect
a novel dataset comprising 660K simple and 70K complex point-text instruction
pairs to enable a two-stage training strategy: initially aligning latent spaces
and subsequently instruction-tuning the unified model. To rigorously evaluate
our model&apos;s perceptual abilities and its generalization capabilities, we
establish two benchmarks: Generative 3D Object Classification and 3D Object
Captioning, assessed through three different methods, including human
evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment
results show that PointLLM demonstrates superior performance over existing 2D
baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM
outperforms human annotators in over 50% of the samples. Codes, datasets, and
benchmarks are available at https://github.com/OpenRobotLab/PointLLM .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yilun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.04053">
<title>DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models. (arXiv:2202.04053v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.04053</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, DALL-E, a multimodal transformer language model, and its variants,
including diffusion models, have shown high-quality text-to-image generation
capabilities. However, despite the realistic image generation results, there
has not been a detailed analysis of how to evaluate such models. In this work,
we investigate the visual reasoning capabilities and social biases of different
text-to-image models, covering both multimodal transformer language models and
diffusion models. First, we measure three visual reasoning skills: object
recognition, object counting, and spatial relation understanding. For this, we
propose PaintSkills, a compositional diagnostic evaluation dataset that
measures these skills. Despite the high-fidelity image generation capability, a
large gap exists between the performance of recent models and the upper bound
accuracy in object counting and spatial relation understanding skills. Second,
we assess the gender and skin tone biases by measuring the gender/skin tone
distribution of generated images across various professions and attributes. We
demonstrate that recent text-to-image generation models learn specific biases
about gender and skin tone from web image-text pairs. We hope our work will
help guide future progress in improving text-to-image generation models on
visual reasoning skills and learning socially unbiased representations. Code
and data: https://github.com/j-min/DallEval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaemin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1&quot;&gt;Abhay Zala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11483">
<title>Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11483</link>
<description rdf:parseType="Literal">&lt;p&gt;This work is intended as a voice in the discussion over previous claims that
a pretrained large language model (LLM) based on the Transformer model
architecture can be sentient. Such claims have been made concerning the LaMDA
model and also concerning the current wave of LLM-powered chatbots, such as
ChatGPT. This claim, if confirmed, would have serious ramifications in the
Natural Language Processing (NLP) community due to wide-spread use of similar
models. However, here we take the position that such a large language model
cannot be sentient, or conscious, and that LaMDA in particular exhibits no
advances over other similar models that would qualify it. We justify this by
analysing the Transformer architecture through Integrated Information Theory of
consciousness. We see the claims of sentience as part of a wider tendency to
use anthropomorphic language in NLP reporting. Regardless of the veracity of
the claims, we consider this an opportune moment to take stock of progress in
language modelling and consider the ethical implications of the task. In order
to make this work helpful for readers outside the NLP community, we also
present the necessary background in language modelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1&quot;&gt;Matthew Shardlow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1&quot;&gt;Piotr Przyby&amp;#x142;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01879">
<title>SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01879</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LMs) beyond a certain scale, demonstrate the emergent
capability of generating free-text rationales for their predictions via
chain-of-thought (CoT) prompting. While CoT can yield dramatically improved
performance, such gains are only observed for sufficiently large LMs. Even more
concerning, there is little guarantee that the generated rationales are
consistent with LM&apos;s predictions or faithfully justify the decisions. In this
work, we propose a faithful knowledge distillation method to learn a small,
self-consistent CoT model from a teacher model that is orders of magnitude
larger. To form better supervision, we elicit rationales supporting the gold
answers from a large LM (teacher) by contrastive decoding, which encourages the
teacher to generate tokens that become more plausible only when the answer is
considered. To ensure faithful distillation, we use the teacher-generated
rationales to learn a student LM with a counterfactual reasoning objective,
which prevents the student from ignoring the rationales to make inconsistent
predictions. Experiments show that, while yielding comparable end-task
performance, our method can generate CoT rationales that are more faithful than
baselines do. Further analysis suggests that such a model respects the
rationales more when making decisions; thus, we can improve its performance
more by refining its rationales.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06566">
<title>ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models. (arXiv:2305.06566v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06566</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized content-based recommender systems have become indispensable
tools for users to navigate through the vast amount of content available on
platforms like daily news websites and book recommendation services. However,
existing recommenders face significant challenges in understanding the content
of items. Large language models (LLMs), which possess deep semantic
comprehension and extensive knowledge from pretraining, have proven to be
effective in various natural language processing tasks. In this study, we
explore the potential of leveraging both open- and closed-source LLMs to
enhance content-based recommendation. With open-source LLMs, we utilize their
deep layers as content encoders, enriching the representation of content at the
embedding level. For closed-source LLMs, we employ prompting techniques to
enrich the training data at the token level. Through comprehensive experiments,
we demonstrate the high effectiveness of both types of LLMs and show the
synergistic relationship between them. Notably, we observed a significant
relative improvement of up to 19.32% compared to existing state-of-the-art
recommendation models. These findings highlight the immense potential of both
open- and closed-source of LLMs in enhancing content-based recommendation
systems. We will make our code and LLM-generated data available for other
researchers to reproduce our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qijiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Nuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakai_T/0/1/0/all/0/1&quot;&gt;Tetsuya Sakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Ming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06345">
<title>Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC. (arXiv:2306.06345v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06345</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-autoregressive approaches aim to improve the inference speed of
translation models, particularly those that generate output in a one-pass
forward manner. However, these approaches often suffer from a significant drop
in translation quality compared to autoregressive models. This paper introduces
a series of innovative techniques to enhance the translation quality of
Non-Autoregressive Translation (NAT) models while maintaining a substantial
acceleration in inference speed. We propose fine-tuning Pretrained Multilingual
Language Models (PMLMs) with the CTC loss to train NAT models effectively.
Furthermore, we adopt the MASK insertion scheme for up-sampling instead of
token duplication, and we present an embedding distillation method to further
enhance performance. In our experiments, our model outperforms the baseline
autoregressive model (Transformer \textit{base}) on multiple datasets,
including WMT&apos;14 DE$\leftrightarrow$EN, WMT&apos;16 RO$\leftrightarrow$EN, and
IWSLT&apos;14 DE$\leftrightarrow$EN. Notably, our model achieves better performance
than the baseline autoregressive model on the IWSLT&apos;14 En$\leftrightarrow$De
and WMT&apos;16 En$\leftrightarrow$Ro datasets, even without using distillation data
during training. It is worth highlighting that on the IWSLT&apos;14
DE$\rightarrow$EN dataset, our model achieves an impressive BLEU score of
39.59, setting a new state-of-the-art performance. Additionally, our model
exhibits a remarkable speed improvement of 16.35 times compared to the
autoregressive model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syu_S/0/1/0/all/0/1&quot;&gt;Shen-sian Syu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Juncheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15245">
<title>C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation. (arXiv:2306.15245v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15245</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing reference-free turn-level evaluation metrics for chatbots
inadequately capture the interaction between the user and the system.
Consequently, they often correlate poorly with human evaluations. To address
this issue, we propose a novel model-agnostic approach that leverages
Conditional Pointwise Mutual Information (C-PMI) to measure the turn-level
interaction between the system and the user based on a given evaluation
dimension. Experimental results on the widely used FED dialogue evaluation
dataset demonstrate that our approach significantly improves the correlation
with human judgment compared with existing evaluation systems. By replacing the
negative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve
a relative 60.5% higher Spearman correlation on average for the FED evaluation
metric. Our code is publicly available at https://github.com/renll/C-PMI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidhu_M/0/1/0/all/0/1&quot;&gt;Mankeerat Sidhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Qi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1&quot;&gt;Revanth Gangi Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1&quot;&gt;ChengXiang Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00925">
<title>Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00925</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic similarity measures are widely used in natural language processing
to catalyze various computer-related tasks. However, no single semantic
similarity measure is the most appropriate for all tasks, and researchers often
use ensemble strategies to ensure performance. This research work proposes a
method for automatically designing semantic similarity ensembles. In fact, our
proposed method uses grammatical evolution, for the first time, to
automatically select and aggregate measures from a pool of candidates to create
an ensemble that maximizes correlation to human judgment. The method is
evaluated on several benchmark datasets and compared to state-of-the-art
ensembles, showing that it can significantly improve similarity assessment
accuracy and outperform existing methods in some cases. As a result, our
research demonstrates the potential of using grammatical evolution to
automatically compare text and prove the benefits of using ensembles for
semantic similarity tasks. The source code that illustrates our approach can be
downloaded from https://github.com/jorge-martinez-gil/sesige.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1&quot;&gt;Jorge Martinez-Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01458">
<title>CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01458</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advances in natural language processing (NLP), have led to a new
trend of applying large language models (LLMs) to real-world scenarios. While
the latest LLMs are astonishingly fluent when interacting with humans, they
suffer from the misinformation problem by unintentionally generating factually
false statements. This can lead to harmful consequences, especially when
produced within sensitive contexts, such as healthcare. Yet few previous works
have focused on evaluating misinformation in the long-form (LF) generation of
LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have
been shown to perform well in different languages, misinformation evaluation
has been mostly conducted in English. To this end, we present a benchmark,
CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic,
specifically the maternity and infant care domain; and 2) a language other than
English, namely Chinese. Most importantly, we provide an innovative paradigm
for building LF generation evaluation benchmarks that can be transferred to
other knowledge-intensive domains and low-resourced languages. Our proposed
benchmark fills the gap between the extensive usage of LLMs and the lack of
datasets for assessing the misinformation generated by these models. It
contains 1,612 expert-checked questions, accompanied with human-selected
references. Using our benchmark, we conduct extensive experiments and found
that current Chinese LLMs are far from perfect in the topic of maternity and
infant care. In an effort to minimize the reliance on human resources for
performance evaluation, we offer off-the-shelf judgment models for
automatically assessing the LF output of LLMs given benchmark questions.
Moreover, we compare potential solutions for LF generation evaluation and
provide insights for building better automated metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wangyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1&quot;&gt;Mingbai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1&quot;&gt;Noa Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09312">
<title>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09312</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal
graph-based transformer model for detecting hate speech in online social
networks, such as Reddit discussions. In contrast to traditional comment-only
methods, our approach to labelling a comment as hate speech involves a holistic
analysis of text and images grounded in the discussion context. This is done by
leveraging graph transformers to capture the contextual relationships in the
entire discussion surrounding a comment and grounding the interwoven fusion
layers that combine individual comments&apos; text and image embeddings instead of
processing modalities separately. We compare the performance of our model to
baselines that only process individual comments and conduct extensive ablation
studies. To evaluate our work, we present a new dataset, HatefulDiscussions,
comprising complete multi-modal discussions from multiple online communities on
Reddit. We conclude with future work for multimodal solutions to deliver social
value in online contexts, arguing that capturing a holistic view of a
conversation significantly advances the effort to detect anti-social behaviour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1&quot;&gt;Liam Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1&quot;&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivas_N/0/1/0/all/0/1&quot;&gt;Nanda Kishore Sreenivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1&quot;&gt;Lukasz Golab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Robin Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10811">
<title>&quot;It Felt Like Having a Second Mind&quot;: Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10811</link>
<description rdf:parseType="Literal">&lt;p&gt;Prewriting is the process of discovering and developing ideas before a first
draft, which requires divergent thinking and often implies unstructured
strategies such as diagramming, outlining, free-writing, etc. Although large
language models (LLMs) have been demonstrated to be useful for a variety of
tasks including creative writing, little is known about how users would
collaborate with LLMs to support prewriting. The preferred collaborative role
and initiative of LLMs during such a creativity process is also unclear. To
investigate human-LLM collaboration patterns and dynamics during prewriting, we
conducted a three-session qualitative study with 15 participants in two
creative tasks: story writing and slogan writing. The findings indicated that
during collaborative prewriting, there appears to be a three-stage iterative
Human-AI Co-creativity process that includes Ideation, Illumination, and
Implementation stages. This collaborative process champions the human in a
dominant role, in addition to mixed and shifting levels of initiative that
exist between humans and LLMs. This research also reports on collaboration
breakdowns that occur during this process, user perceptions of using existing
LLMs during Human-AI Co-creativity, and discusses design implications to
support this co-creativity process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1&quot;&gt;Qian Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Siying Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Piaohong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bo Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhicong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11764">
<title>Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT. (arXiv:2307.11764v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11764</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained language models have recently gained significant traction
due to their improved performance on various down-stream tasks like text
classification and question answering, requiring only few epochs of
fine-tuning. However, their large model sizes often prohibit their applications
on resource-constrained edge devices. Existing solutions of yielding
parameter-efficient BERT models largely rely on compute-exhaustive training and
fine-tuning. Moreover, they often rely on additional compute heavy models to
mitigate the performance gap. In this paper, we present Sensi-BERT, a
sensitivity driven efficient fine-tuning of BERT models that can take an
off-the-shelf pre-trained BERT model and yield highly parameter-efficient
models for downstream tasks. In particular, we perform sensitivity analysis to
rank each individual parameter tensor, that then is used to trim them
accordingly during fine-tuning for a given parameter or FLOPs budget. Our
experiments show the efficacy of Sensi-BERT across different downstream tasks
including MNLI, QQP, QNLI, SST-2 and SQuAD, showing better performance at
similar or smaller parameter budget compared to various alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Souvik Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1&quot;&gt;Sharath Nittur Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szankin_M/0/1/0/all/0/1&quot;&gt;Maciej Szankin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundaresan_S/0/1/0/all/0/1&quot;&gt;Sairam Sundaresan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07462">
<title>Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07462</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of Artificial Intelligence (AI) generative language models
such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has
triggered a revolution that can transform how text is generated. This has many
implications, for example, as AI-generated text becomes a significant fraction
of the text, would this have an effect on the language capabilities of readers
and also on the training of newer AI tools? Would it affect the evolution of
languages? Focusing on one specific aspect of the language: words; will the use
of tools such as ChatGPT increase or reduce the vocabulary used or the lexical
richness? This has implications for words, as those not included in
AI-generated content will tend to be less and less popular and may eventually
be lost. In this work, we perform an initial comparison of the vocabulary and
lexical richness of ChatGPT and humans when performing the same tasks. In more
detail, two datasets containing the answers to different types of questions
answered by ChatGPT and humans, and a third dataset in which ChatGPT
paraphrases sentences and questions are used. The analysis shows that ChatGPT
tends to use fewer distinct words and lower lexical richness than humans. These
results are very preliminary and additional datasets and ChatGPT configurations
have to be evaluated to extract more general conclusions. Therefore, further
research is needed to understand how the use of ChatGPT and more broadly
generative AI tools will affect the vocabulary and lexical richness in
different types of text and languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reviriego_P/0/1/0/all/0/1&quot;&gt;Pedro Reviriego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conde_J/0/1/0/all/0/1&quot;&gt;Javier Conde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merino_Gomez_E/0/1/0/all/0/1&quot;&gt;Elena Merino-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_G/0/1/0/all/0/1&quot;&gt;Gonzalo Mart&amp;#xed;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Alberto Hern&amp;#xe1;ndez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10959">
<title>DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10959</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose Docprompt for document question answering tasks
with powerful zero-shot and few-shot performance. We proposed a novel weakly
supervised data generation method, a novel multl-stage training method and a
novel understanding model \&amp;amp; generation model ensemble method. We achieved
state-of-the-art performance on 4 document question answering tasks. This
method greatly improves the delivery efficiency and model performance of
document question answering customer projects, reducing annotation costs and
labor costs. Our demo can be found at
https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sijin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Teng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shikun Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13916">
<title>Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13916</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs play a vital role in numerous artificial intelligence tasks,
yet they frequently face the issue of incompleteness. In this study, we explore
utilizing Large Language Models (LLM) for knowledge graph completion. We
consider triples in knowledge graphs as text sequences and introduce an
innovative framework called Knowledge Graph LLM (KG-LLM) to model these
triples. Our technique employs entity and relation descriptions of a triple as
prompts and utilizes the response for predictions. Experiments on various
benchmark knowledge graphs demonstrate that our method attains state-of-the-art
performance in tasks such as triple classification and relation prediction. We
also find that fine-tuning relatively smaller models (e.g., LLaMA-7B,
ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Liang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jiazhen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chengsheng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11073">
<title>OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v3 [eess.AS] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.11073</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Dialogue State Tracking (DST) is a core component of spoken dialogue
systems, recent work on this task mostly deals with chat corpora, disregarding
the discrepancies between spoken and written language.In this paper, we propose
OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR)
model and a DST model. We introduce several adaptations in the ASR and DST
modules to improve integration and robustness to spoken conversations.With
these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to
evaluate spoken DST. We conduct an in-depth analysis of the results and find
that normalizing the ASR outputs and adapting the DST inputs through data
augmentation, along with increasing the pre-trained models size all play an
important role in reducing the performance discrepancy between written and
spoken conversations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacqmin_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;o Jacqmin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Druart_L/0/1/0/all/0/1&quot;&gt;Lucas Druart&lt;/a&gt; (LIA), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Esteve_Y/0/1/0/all/0/1&quot;&gt;Yannick Est&amp;#xe8;ve&lt;/a&gt; (LIA), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Favre_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Favre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rojas_Barahona_L/0/1/0/all/0/1&quot;&gt;Lina Maria Rojas-Barahona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1&quot;&gt;Valentin Vielzeuf&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>