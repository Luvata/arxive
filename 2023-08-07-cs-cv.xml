<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.09168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.00596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01867" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.01916">
<title>Semi Supervised Meta Learning for Spatiotemporal Learning. (arXiv:2308.01916v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01916</link>
<description rdf:parseType="Literal">&lt;p&gt;We approached the goal of applying meta-learning to self-supervised masked
autoencoders for spatiotemporal learning in three steps. Broadly, we seek to
understand the impact of applying meta-learning to existing state-of-the-art
representation learning architectures. Thus, we test spatiotemporal learning
through: a meta-learning architecture only, a representation learning
architecture only, and an architecture applying representation learning
alongside a meta learning architecture. We utilize the Memory Augmented Neural
Network (MANN) architecture to apply meta-learning to our framework.
Specifically, we first experiment with applying a pre-trained MAE and
fine-tuning on our small-scale spatiotemporal dataset for video reconstruction
tasks. Next, we experiment with training an MAE encoder and applying a
classification head for action classification tasks. Finally, we experiment
with applying a pre-trained MAE and fine-tune with MANN backbone for action
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waseem_F/0/1/0/all/0/1&quot;&gt;Faraz Waseem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthukumar_P/0/1/0/all/0/1&quot;&gt;Pratyush Muthukumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01919">
<title>Emotion recognition based on multi-modal electrophysiology multi-head attention Contrastive Learning. (arXiv:2308.01919v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2308.01919</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotion recognition is an important research direction in artificial
intelligence, helping machines understand and adapt to human emotional states.
Multimodal electrophysiological(ME) signals, such as EEG, GSR,
respiration(Resp), and temperature(Temp), are effective biomarkers for
reflecting changes in human emotions. However, using electrophysiological
signals for emotion recognition faces challenges such as data scarcity,
inconsistent labeling, and difficulty in cross-individual generalization. To
address these issues, we propose ME-MHACL, a self-supervised contrastive
learning-based multimodal emotion recognition method that can learn meaningful
feature representations from unlabeled electrophysiological signals and use
multi-head attention mechanisms for feature fusion to improve recognition
performance. Our method includes two stages: first, we use the Meiosis method
to group sample and augment unlabeled electrophysiological signals and design a
self-supervised contrastive learning task; second, we apply the trained feature
extractor to labeled electrophysiological signals and use multi-head attention
mechanisms for feature fusion. We conducted experiments on two public datasets,
DEAP and MAHNOB-HCI, and our method outperformed existing benchmark methods in
emotion recognition tasks and had good cross-individual generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01925">
<title>Accessibility and Inclusiveness of New Information and Communication Technologies for Disabled Users and Content Creators in the Metaverse. (arXiv:2308.01925v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2308.01925</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the proliferation of Blockchain Metaverse projects, the inclusion of
physically disabled individuals in the Metaverse remains distant, with limited
standards and regulations in place. However, the article proposes a concept of
the Metaverse that leverages emerging technologies, such as Virtual and
Augmented Reality, and the Internet of Things, to enable greater engagement of
disabled creatives. This approach aims to enhance inclusiveness in the
Metaverse landscape. Based on the findings, the paper concludes that the active
involvement of physically disabled individuals in the design and development of
Metaverse platforms is crucial for promoting inclusivity. The proposed
framework for accessibility and inclusiveness in Virtual, Augmented, and Mixed
realities of decentralised Metaverses provides a basis for the meaningful
participation of disabled creatives. The article emphasises the importance of
addressing the mechanisms for art production by individuals with disabilities
in the emerging Metaverse landscape. Additionally, it highlights the need for
further research and collaboration to establish standards and regulations that
facilitate the inclusion of physically disabled individuals in Metaverse
projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radanliev_D/0/1/0/all/0/1&quot;&gt;Dr Petar Radanliev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roure_P/0/1/0/all/0/1&quot;&gt;Professor David De Roure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novitzky_D/0/1/0/all/0/1&quot;&gt;Dr Peter Novitzky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sluganovic_D/0/1/0/all/0/1&quot;&gt;Dr Ivo Sluganovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01937">
<title>Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Compartmentalized Diffusion Models (CDM), a method to train
different diffusion models (or prompts) on distinct data sources and
arbitrarily compose them at inference time. The individual models can be
trained in isolation, at different times, and on different distributions and
domains and can be later composed to achieve performance comparable to a
paragon model trained on all data simultaneously. Furthermore, each model only
contains information about the subset of the data it was exposed to during
training, enabling several forms of training data protection. In particular,
CDMs are the first method to enable both selective forgetting and continual
learning for large-scale diffusion models, as well as allowing serving
customized models based on the user&apos;s access rights. CDMs also allow
determining the importance of a subset of the data in generating particular
samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1&quot;&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1&quot;&gt;Ashwin Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01940">
<title>TSMD: A Database for Static Color Mesh Quality Assessment Study. (arXiv:2308.01940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01940</link>
<description rdf:parseType="Literal">&lt;p&gt;Static meshes with texture map are widely used in modern industrial and
manufacturing sectors, attracting considerable attention in the mesh
compression community due to its huge amount of data. To facilitate the study
of static mesh compression algorithm and objective quality metric, we create
the Tencent - Static Mesh Dataset (TSMD) containing 42 reference meshes with
rich visual characteristics. 210 distorted samples are generated by the lossy
compression scheme developed for the Call for Proposals on polygonal static
mesh coding, released on June 23 by the Alliance for Open Media Volumetric
Visual Media group. Using processed video sequences, a large-scale,
crowdsourcing-based, subjective experiment was conducted to collect subjective
scores from 74 viewers. The dataset undergoes analysis to validate its sample
diversity and Mean Opinion Scores (MOS) accuracy, establishing its
heterogeneous nature and reliability. State-of-the-art objective metrics are
evaluated on the new dataset. Pearson and Spearman correlations around 0.75 are
reported, deviating from results typically observed on less heterogeneous
datasets, demonstrating the need for further development of more robust
metrics. The TSMD, including meshes, PVSs, bitstreams, and MOS, is made
publicly available at the following location:
https://multimedia.tencent.com/resources/tsmd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Joel Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01944">
<title>Dynamic Token-Pass Transformers for Semantic Segmentation. (arXiv:2308.01944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01944</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViT) usually extract features via forwarding all the
tokens in the self-attention layers from top to toe. In this paper, we
introduce dynamic token-pass vision transformers (DoViT) for semantic
segmentation, which can adaptively reduce the inference cost for images with
different complexity. DoViT gradually stops partial easy tokens from
self-attention calculation and keeps the hard tokens forwarding until meeting
the stopping criteria. We employ lightweight auxiliary heads to make the
token-pass decision and divide the tokens into keeping/stopping parts. With a
token separate calculation, the self-attention layers are speeded up with
sparse tokens and still work friendly with hardware. A token reconstruction
module is built to collect and reset the grouped tokens to their original
position in the sequence, which is necessary to predict correct semantic masks.
We conduct extensive experiments on two common semantic segmentation tasks, and
demonstrate that our method greatly reduces about 40% $\sim$ 60% FLOPs and the
drop of mIoU is within 0.8% for various segmentation transformers. The
throughput and inference speed of ViT-L/B are increased to more than 2$\times$
on Cityscapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01948">
<title>A Multidimensional Analysis of Social Biases in Vision Transformers. (arXiv:2308.01948v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01948</link>
<description rdf:parseType="Literal">&lt;p&gt;The embedding spaces of image models have been shown to encode a range of
social biases such as racism and sexism. Here, we investigate specific factors
that contribute to the emergence of these biases in Vision Transformers (ViT).
Therefore, we measure the impact of training data, model architecture, and
training objectives on social biases in the learned representations of ViTs.
Our findings indicate that counterfactual augmentation training using
diffusion-based image editing can mitigate biases, but does not eliminate them.
Moreover, we find that larger models are less biased than smaller models, and
that models trained using discriminative objectives are less biased than those
trained using generative objectives. In addition, we observe inconsistencies in
the learned social biases. To our surprise, ViTs can exhibit opposite biases
when trained on the same data set using different self-supervised objectives.
Our findings give insights into the factors that contribute to the emergence of
social biases and suggests that we could achieve substantial fairness
improvements based on model design choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinkmann_J/0/1/0/all/0/1&quot;&gt;Jannik Brinkmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1&quot;&gt;Paul Swoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartelt_C/0/1/0/all/0/1&quot;&gt;Christian Bartelt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01971">
<title>SpaDen : Sparse and Dense Keypoint Estimation for Real-World Chart Understanding. (arXiv:2308.01971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01971</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel bottom-up approach for the extraction of chart data. Our
model utilizes images of charts as inputs and learns to detect keypoints (KP),
which are used to reconstruct the components within the plot area. Our novelty
lies in detecting a fusion of continuous and discrete KP as predicted heatmaps.
A combination of sparse and dense per-pixel objectives coupled with a uni-modal
self-attention-based feature-fusion layer is applied to learn KP embeddings.
Further leveraging deep metric learning for unsupervised clustering, allows us
to segment the chart plot area into various objects. By further matching the
chart components to the legend, we are able to obtain the data series names. A
post-processing threshold is applied to the KP embeddings to refine the object
reconstructions and improve accuracy. Our extensive experiments include an
evaluation of different modules for KP estimation and the combination of deep
layer aggregation and corner pooling approaches. The results of our experiments
provide extensive evaluation for the task of real-world chart data extraction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Saleem Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pengyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David Doermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01979">
<title>RealCQA: Scientific Chart Question Answering as a Test-bed for First-Order Logic. (arXiv:2308.01979v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01979</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a comprehensive study of chart visual question-answering(QA) task,
to address the challenges faced in comprehending and extracting data from chart
visualizations within documents. Despite efforts to tackle this problem using
synthetic charts, solutions are limited by the shortage of annotated real-world
data. To fill this gap, we introduce a benchmark and dataset for chart visual
QA on real-world charts, offering a systematic analysis of the task and a novel
taxonomy for template-based chart question creation. Our contribution includes
the introduction of a new answer type, &apos;list&apos;, with both ranked and unranked
variations. Our study is conducted on a real-world chart dataset from
scientific literature, showcasing higher visual complexity compared to other
works. Our focus is on template-based QA and how it can serve as a standard for
evaluating the first-order logic capabilities of models. The results of our
experiments, conducted on a real-world out-of-distribution dataset, provide a
robust evaluation of large-scale pre-trained models and advance the field of
chart visual QA and formal logic verification for neural networks in general.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Saleem Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1&quot;&gt;Shubham Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01981">
<title>CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01981</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce CartiMorph, a framework for automated knee articular cartilage
morphometrics. It takes an image as input and generates quantitative metrics
for cartilage subregions, including the percentage of full-thickness cartilage
loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the
power of deep learning models for hierarchical image feature representation.
Deep learning models were trained and validated for tissue segmentation,
template construction, and template-to-image registration. We established
methods for surface-normal-based cartilage thickness mapping, FCL estimation,
and rule-based cartilage parcellation. Our cartilage thickness map showed less
error in thin and peripheral regions. We evaluated the effectiveness of the
adopted segmentation model by comparing the quantitative metrics obtained from
model segmentation and those from manual segmentation. The root-mean-squared
deviation of the FCL measurements was less than 8%, and strong correlations
were observed for the mean thickness (Pearson&apos;s correlation coefficient $\rho
\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in
[0.89,0.98]$) measurements. We compared our FCL measurements with those from a
previous study and found that our measurements deviated less from the ground
truths. We observed superior performance of the proposed rule-based cartilage
parcellation method compared with the atlas-based approach. CartiMorph has the
potential to promote imaging biomarkers discovery for knee osteoarthritis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Junru Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Sheheryar Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01982">
<title>Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images. (arXiv:2308.01982v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01982</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the advances in machine learning and digital pathology, it is not yet
clear if machine learning methods can accurately predict molecular information
merely from histomorphology. In a quest to answer this question, we built a
large-scale dataset (185538 images) with reliable measurements for Ki67, ER,
PR, and HER2 statuses. The dataset is composed of mirrored images of H\&amp;amp;E and
corresponding images of immunohistochemistry (IHC) assays (Ki67, ER, PR, and
HER2. These images are mirrored through registration. To increase reliability,
individual pairs were inspected and discarded if artifacts were present (tissue
folding, bubbles, etc). Measurements for Ki67, ER and PR were determined by
calculating H-Score from image analysis. HER2 measurement is based on binary
classification: 0 and 1+ (IHC scores representing a negative subset) vs 3+ (IHC
score positive subset). Cases with IHC equivocal score (2+) were excluded. We
show that a standard ViT-based pipeline can achieve prediction performances
around 90% in terms of Area Under the Curve (AUC) when trained with a proper
labeling protocol. Finally, we shed light on the ability of the trained
classifiers to localize relevant regions, which encourages future work to
improve the localizations. Our proposed dataset is publicly available:
https://ihc4bc.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akbarnejad_A/0/1/0/all/0/1&quot;&gt;Amir Akbarnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barnes_P/0/1/0/all/0/1&quot;&gt;Penny J. Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bigras_G/0/1/0/all/0/1&quot;&gt;Gilbert Bigras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01994">
<title>Explainable unsupervised multi-modal image registration using deep networks. (arXiv:2308.01994v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01994</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical decision making from magnetic resonance imaging (MRI) combines
complementary information from multiple MRI sequences (defined as
&apos;modalities&apos;). MRI image registration aims to geometrically &apos;pair&apos; diagnoses
from different modalities, time points and slices. Both intra- and
inter-modality MRI registration are essential components in clinical MRI
settings. Further, an MRI image processing pipeline that can address both afine
and non-rigid registration is critical, as both types of deformations may be
occuring in real MRI data scenarios. Unlike image classification,
explainability is not commonly addressed in image registration deep learning
(DL) methods, as it is challenging to interpet model-data behaviours against
transformation fields. To properly address this, we incorporate Grad-CAM-based
explainability frameworks in each major component of our unsupervised
multi-modal and multi-organ image registration DL methodology. We previously
demonstrated that we were able to reach superior performance (against the
current standard Syn method). In this work, we show that our DL model becomes
fully explainable, setting the framework to generalise our approach on further
medical imaging data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Papanastasiou_G/0/1/0/all/0/1&quot;&gt;Giorgos Papanastasiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02000">
<title>On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.02000</link>
<description rdf:parseType="Literal">&lt;p&gt;Bridging the huge disparity between neural and symbolic representation can
potentially enable the incorporation of symbolic thinking into neural networks
from essence. Motivated by how human gradually builds complex symbolic
representation from the prototype symbols that are learned through perception
and environmental interactions. We propose a Neural-Symbolic Transitional
Dictionary Learning (TDL) framework that employs an EM algorithm to learn a
transitional representation of data that compresses high-dimension information
of visual parts of an input into a set of tensors as neural variables and
discover the implicit predicate structure in a self-supervised way. We
implement the framework with a diffusion model by regarding the decomposition
of input as a cooperative game, then learn predicates by prototype clustering.
We additionally use RL enabled by the Markovian of diffusion models to further
tune the learned prototypes by incorporating subjective factors. Extensive
experiments on 3 abstract compositional visual objects datasets that require
the model to segment parts without any visual features like texture, color, or
shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the
learned representation enables interpretable decomposition of visual input and
smooth adaption to downstream tasks which are not available by existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junyan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1&quot;&gt;Peter Chin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02027">
<title>ETran: Energy-Based Transferability Estimation. (arXiv:2308.02027v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02027</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of ranking pre-trained models for object
detection and image classification. Selecting the best pre-trained model by
fine-tuning is an expensive and time-consuming task. Previous works have
proposed transferability estimation based on features extracted by the
pre-trained models. We argue that quantifying whether the target dataset is
in-distribution (IND) or out-of-distribution (OOD) for the pre-trained model is
an important factor in the transferability estimation. To this end, we propose
ETran, an energy-based transferability assessment metric, which includes three
scores: 1) energy score, 2) classification score, and 3) regression score. We
use energy-based models to determine whether the target dataset is OOD or IND
for the pre-trained model. In contrast to the prior works, ETran is applicable
to a wide range of tasks including classification, regression, and object
detection (classification+regression). This is the first work that proposes
transferability estimation for object detection task. Our extensive experiments
on four benchmarks and two tasks show that ETran outperforms previous works on
object detection and classification benchmarks by an average of 21% and 12%,
respectively, and achieves SOTA in transferability assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_M/0/1/0/all/0/1&quot;&gt;Mohsen Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1&quot;&gt;Mohammad Akbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamranian_B/0/1/0/all/0/1&quot;&gt;Behnam Kamranian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02046">
<title>UGainS: Uncertainty Guided Anomaly Instance Segmentation. (arXiv:2308.02046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02046</link>
<description rdf:parseType="Literal">&lt;p&gt;A single unexpected object on the road can cause an accident or may lead to
injuries. To prevent this, we need a reliable mechanism for finding anomalous
objects on the road. This task, called anomaly segmentation, can be a stepping
stone to safe and reliable autonomous driving. Current approaches tackle
anomaly segmentation by assigning an anomaly score to each pixel and by
grouping anomalous regions using simple heuristics. However, pixel grouping is
a limiting factor when it comes to evaluating the segmentation performance of
individual anomalous objects. To address the issue of grouping multiple anomaly
instances into one, we propose an approach that produces accurate anomaly
instance masks. Our approach centers on an out-of-distribution segmentation
model for identifying uncertain regions and a strong generalist segmentation
model for anomaly instances segmentation. We investigate ways to use uncertain
regions to guide such a segmentation model to perform segmentation of anomalous
instances. By incorporating strong object priors from a generalist model we
additionally improve the per-pixel anomaly segmentation performance. Our
approach outperforms current pixel-level anomaly segmentation methods,
achieving an AP of 80.08% and 88.98% on the Fishyscapes Lost and Found and the
RoadAnomaly validation sets respectively. Project page:
https://vision.rwth-aachen.de/ugains
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nekrasov_A/0/1/0/all/0/1&quot;&gt;Alexey Nekrasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hermans_A/0/1/0/all/0/1&quot;&gt;Alexander Hermans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhnert_L/0/1/0/all/0/1&quot;&gt;Lars Kuhnert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1&quot;&gt;Bastian Leibe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02062">
<title>Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images. (arXiv:2308.02062v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.02062</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation masks of pathological areas are useful in many medical
applications, such as brain tumour and stroke management. Moreover, healthy
counterfactuals of diseased images can be used to enhance radiologists&apos;
training files and to improve the interpretability of segmentation models. In
this work, we present a weakly supervised method to generate a healthy version
of a diseased image and then use it to obtain a pixel-wise anomaly map. To do
so, we start by considering a saliency map that approximately covers the
pathological areas, obtained with ACAT. Then, we propose a technique that
allows to perform targeted modifications to these regions, while preserving the
rest of the image. In particular, we employ a diffusion model trained on
healthy samples and combine Denoising Diffusion Probabilistic Model (DDPM) and
Denoising Diffusion Implicit Model (DDIM) at each step of the sampling process.
DDPM is used to modify the areas affected by a lesion within the saliency map,
while DDIM guarantees reconstruction of the normal anatomy outside of it. The
two parts are also fused at each timestep, to guarantee the generation of a
sample with a coherent appearance and a seamless transition between edited and
unedited parts. We verify that when our method is applied to healthy samples,
the input images are reconstructed without significant modifications. We
compare our approach with alternative weakly supervised methods on IST-3 for
stroke lesion segmentation and on BraTS2021 for brain tumour segmentation,
where we improve the DICE score of the best competing method from $0.6534$ to
$0.7056$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fontanella_A/0/1/0/all/0/1&quot;&gt;Alessandro Fontanella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mair_G/0/1/0/all/0/1&quot;&gt;Grant Mair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wardlaw_J/0/1/0/all/0/1&quot;&gt;Joanna Wardlaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trucco_E/0/1/0/all/0/1&quot;&gt;Emanuele Trucco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02065">
<title>On the Biometric Capacity of Generative Face Models. (arXiv:2308.02065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02065</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been tremendous progress in generating realistic faces with high
fidelity over the past few years. Despite this progress, a crucial question
remains unanswered: &quot;Given a generative face model, how many unique identities
can it generate?&quot; In other words, what is the biometric capacity of the
generative face model? A scientific basis for answering this question will
benefit evaluating and comparing different generative face models and establish
an upper bound on their scalability. This paper proposes a statistical approach
to estimate the biometric capacity of generated face images in a hyperspherical
feature space. We employ our approach on multiple generative models, including
unconditional generators like StyleGAN, Latent Diffusion Model, and &quot;Generated
Photos,&quot; as well as DCFace, a class-conditional generator. We also estimate
capacity w.r.t. demographic attributes such as gender and age. Our capacity
estimates indicate that (a) under ArcFace representation at a false acceptance
rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of
$1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces
drastically as we lower the desired FAR with an estimate of $1.796\times10^4$
and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no
discernible disparity in the capacity w.r.t gender; and (d) for some generative
models, there is an appreciable disparity in the capacity w.r.t age. Code is
available at https://github.com/human-analysis/capacity-generative-face-models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreekumar_G/0/1/0/all/0/1&quot;&gt;Gautam Sreekumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02066">
<title>Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. (arXiv:2308.02066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02066</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) seeks to learn a single model to accomplish
multiple tasks by leveraging shared information among the tasks. Existing MTL
models, however, have been known to suffer from negative interference among
tasks. Efforts to mitigate task interference have focused on either
loss/gradient balancing or implicit parameter partitioning with partial
overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task
interference through a synergistic combination of non-learnable primitives
(NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable
primitives to extract a diverse set of task-agnostic features and recombine
them into a shared branch common to all tasks and explicit task-specific
branches reserved for each task. The non-learnable primitives and the explicit
decoupling of learnable parameters into shared and task-specific ones afford
the flexibility needed for minimizing task interference. We evaluate the
efficacy of ETR-NLP networks for both image-level classification and
pixel-level dense prediction MTL problems. Experimental results indicate that
ETR-NLP significantly outperforms state-of-the-art baselines with fewer
learnable parameters and similar FLOPs across all datasets. Code is available
at this \href{https://github.com/zhichao-lu/etr-nlp-mtl}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Chuntao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ran Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02084">
<title>Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02084</link>
<description rdf:parseType="Literal">&lt;p&gt;Most machine learning (ML) systems assume stationary and matching data
distributions during training and deployment. This is often a false assumption.
When ML models are deployed on real devices, data distributions often shift
over time due to changes in environmental factors, sensor characteristics, and
task-of-interest. While it is possible to have a human-in-the-loop to monitor
for distribution shifts and engineer new architectures in response to these
shifts, such a setup is not cost-effective. Instead, non-stationary automated
ML (AutoML) models are needed. This paper presents the
Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning
under domain shifts. The EAR framework uses a fixed deep neural network (DNN)
feature encoder and trains shallow networks on top of the encoder to handle
novel data. The EAR framework is capable of 1) detecting when new data is
out-of-distribution (OOD) by combining DNNs with hyperdimensional computing
(HDC), 2) identifying low-parameter neural adaptors to adapt the model to the
OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing
catastrophic forgetting on previous tasks by progressively growing the neural
architecture as needed and dynamically routing data through the appropriate
adaptors and reconfigurators for handling domain-incremental and
class-incremental continual learning. We systematically evaluate our approach
on several benchmark datasets for domain adaptation and demonstrate strong
performance compared to state-of-the-art algorithms for OOD detection and
few-/zero-shot NAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniels_Z/0/1/0/all/0/1&quot;&gt;Zachary A. Daniels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomnitz_M/0/1/0/all/0/1&quot;&gt;Michael Lomnitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_P/0/1/0/all/0/1&quot;&gt;Phil Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1&quot;&gt;Aswin Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Joe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piacentino_M/0/1/0/all/0/1&quot;&gt;Michael Piacentino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02097">
<title>Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation. (arXiv:2308.02097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02097</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modality image fusion and segmentation play a vital role in autonomous
driving and robotic operation. Early efforts focus on boosting the performance
for only one task, \emph{e.g.,} fusion or segmentation, making it hard to
reach~`Best of Both Worlds&apos;. To overcome this issue, in this paper, we propose
a \textbf{M}ulti-\textbf{i}nteractive \textbf{F}eature learning architecture
for image fusion and \textbf{Seg}mentation, namely SegMiF, and exploit
dual-task correlation to promote the performance of both tasks. The SegMiF is
of a cascade structure, containing a fusion sub-network and a commonly used
segmentation sub-network. By slickly bridging intermediate features between two
components, the knowledge learned from the segmentation task can effectively
assist the fusion task. Also, the benefited fusion network supports the
segmentation one to perform more pretentiously. Besides, a hierarchical
interactive attention block is established to ensure fine-grained mapping of
all the vital information between two tasks, so that the modality/semantic
features can be fully mutual-interactive. In addition, a dynamic weight factor
is introduced to automatically adjust the corresponding weights of each task,
which can balance the interactive feature correspondence and break through the
limitation of laborious tuning. Furthermore, we construct a smart multi-wave
binocular imaging system and collect a full-time multi-modality benchmark with
15 annotated pixel-level categories for image fusion and segmentation.
Extensive experiments on several public datasets and our benchmark demonstrate
that the proposed method outputs visually appealing fused images and perform
averagely $7.66\%$ higher segmentation mIoU in the real-world scene than the
state-of-the-art approaches. The source code and benchmark are available at
\url{https://github.com/JinyuanLiu-CV/SegMiF}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guanyao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Wei Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongxuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02100">
<title>CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy. (arXiv:2308.02100v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.02100</link>
<description rdf:parseType="Literal">&lt;p&gt;CT scans are the standard-of-care for many clinical ailments, and are needed
for treatments like external beam radiotherapy. Unfortunately, CT scanners are
rare in low and mid-resource settings due to their costs. Planar X-ray
radiography units, in comparison, are far more prevalent, but can only provide
limited 2D observations of the 3D anatomy. In this work, we propose a method to
generate CT volumes from few (&amp;lt;5) planar X-ray observations using a prior data
distribution, and perform the first evaluation of such a reconstruction
algorithm for a clinical application: radiotherapy planning. We propose a deep
generative model, building on advances in neural implicit representations to
synthesize volumetric CT scans from few input planar X-ray images at different
angles. To focus the generation task on clinically-relevant features, our model
can also leverage anatomical guidance during training (via segmentation masks).
We generated 2-field opposed, palliative radiotherapy plans on thoracic CTs
reconstructed by our method, and found that isocenter radiation dose on
reconstructed scans have &amp;lt;1% error with respect to the dose calculated on
clinically acquired CTs using &amp;lt;=4 X-ray views. In addition, our method is
better than recent sparse CT reconstruction baselines in terms of standard
pixel and structure-level metrics (PSNR, SSIM, Dice score) on the public LIDC
lung CT dataset. Code is available at: https://github.com/wanderinrain/Xray2CT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiran Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Netherton_T/0/1/0/all/0/1&quot;&gt;Tucker Netherton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Court_L/0/1/0/all/0/1&quot;&gt;Laurence Court&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1&quot;&gt;Ashok Veeraraghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balakrishnan_G/0/1/0/all/0/1&quot;&gt;Guha Balakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02101">
<title>Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network. (arXiv:2308.02101v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.02101</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing global contextual information plays a critical role in breast
ultrasound (BUS) image classification. Although convolutional neural networks
(CNNs) have demonstrated reliable performance in tumor classification, they
have inherent limitations for modeling global and long-range dependencies due
to the localized nature of convolution operations. Vision Transformers have an
improved capability of capturing global contextual information but may distort
the local image patterns due to the tokenization operations. In this study, we
proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN,
designed to perform BUS tumor classification and segmentation using a hybrid
architecture composed of CNNs and Swin Transformer components. The proposed
approach was compared to nine BUS classification methods and evaluated using
seven quantitative metrics on a dataset of 3,320 BUS images. The results
indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and
F1 score of 82.7%, 86.4%, and 86.0%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shareef_B/0/1/0/all/0/1&quot;&gt;Bryar Shareef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1&quot;&gt;Min Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haotian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02116">
<title>AdvFAS: A robust face anti-spoofing framework against adversarial examples. (arXiv:2308.02116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02116</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the reliability of face recognition systems against presentation
attacks necessitates the deployment of face anti-spoofing techniques. Despite
considerable advancements in this domain, the ability of even the most
state-of-the-art methods to defend against adversarial examples remains
elusive. While several adversarial defense strategies have been proposed, they
typically suffer from constrained practicability due to inevitable trade-offs
between universality, effectiveness, and efficiency. To overcome these
challenges, we thoroughly delve into the coupled relationship between
adversarial detection and face anti-spoofing. Based on this, we propose a
robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled
scores to accurately distinguish between correctly detected and wrongly
detected face images. Extensive experiments demonstrate the effectiveness of
our framework in a variety of settings, including different attacks, datasets,
and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we
successfully apply the proposed method to detect real-world adversarial
examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Heng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mingzhi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bihui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jianteng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yandong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhaoxia Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02117">
<title>VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02117</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) conduct message passing which aggregates local
neighbors to update node representations. Such message passing leads to
scalability issues in practical latency-constrained applications. To address
this issue, recent methods adopt knowledge distillation (KD) to learn
computationally-efficient multi-layer perceptron (MLP) by mimicking the output
of GNN. However, the existing GNN representation space may not be expressive
enough for representing diverse local structures of the underlying graph, which
limits the knowledge transfer from GNN to MLP. Here we present a novel
framework VQGraph to learn a powerful graph representation space for bridging
GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized
variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which
explicitly represents the nodes of diverse local structures as numerous
discrete tokens and constitutes a meaningful codebook. Equipped with the
learned codebook, we propose a new token-based distillation objective based on
soft token assignments to sufficiently transfer the structural knowledge from
GNN to MLP. Extensive experiments and analyses demonstrate the strong
performance of VQGraph, where we achieve new state-of-the-art performance on
GNN-MLP distillation in both transductive and inductive settings across seven
graph datasets. We show that VQGraph with better performance infers faster than
GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone
MLPs by 3.90% and 28.05% on average, respectively. Code:
https://github.com/YangLing0818/VQGraph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Ling Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shenda Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_W/0/1/0/all/0/1&quot;&gt;Wei Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02118">
<title>Rethinking Class Activation Maps for Segmentation: Revealing Semantic Information in Shallow Layers by Reducing Noise. (arXiv:2308.02118v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02118</link>
<description rdf:parseType="Literal">&lt;p&gt;Class activation maps are widely used for explaining deep neural networks.
Due to its ability to highlight regions of interest, it has evolved in recent
years as a key step in weakly supervised learning. A major limitation to the
performance of the class activation maps is the small spatial resolution of the
feature maps in the last layer of the convolutional neural network. Therefore,
we expect to generate high-resolution feature maps that result in high-quality
semantic information. In this paper, we rethink the properties of semantic
information in shallow feature maps. We find that the shallow feature maps
still have fine-grained non-discriminative features while mixing considerable
non-target noise. Furthermore, we propose a simple gradient-based denoising
method to filter the noise by truncating the positive gradient. Our proposed
scheme can be easily deployed in other CAM-related methods, facilitating these
methods to obtain higher-quality class activation maps. We evaluate the
proposed approach through a weakly-supervised semantic segmentation task, and a
large number of experiments demonstrate the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang-Cheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yingyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jingxiao Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingguo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Dong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guodong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02119">
<title>Attention-Driven Lightweight Model for Pigmented Skin Lesion Detection. (arXiv:2308.02119v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02119</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents a lightweight pipeline for skin lesion detection,
addressing the challenges posed by imbalanced class distribution and subtle or
atypical appearances of some lesions. The pipeline is built around a
lightweight model that leverages ghosted features and the DFC attention
mechanism to reduce computational complexity while maintaining high
performance. The model was trained on the HAM10000 dataset, which includes
various types of skin lesions. To address the class imbalance in the dataset,
the synthetic minority over-sampling technique and various image augmentation
techniques were used. The model also incorporates a knowledge-based loss
weighting technique, which assigns different weights to the loss function at
the class level and the instance level, helping the model focus on minority
classes and challenging samples. This technique involves assigning different
weights to the loss function on two levels - the class level and the instance
level. By applying appropriate loss weights, the model pays more attention to
the minority classes and challenging samples, thus improving its ability to
correctly detect and classify different skin lesions. The model achieved an
accuracy of 92.4%, a precision of 84.2%, a recall of 86.9%, a f1-score of 85.4%
with particularly strong performance in identifying Benign Keratosis-like
lesions (BKL) and Nevus (NV). Despite its superior performance, the model&apos;s
computational cost is considerably lower than some models with less accuracy,
making it an optimal solution for real-world applications where both accuracy
and efficiency are essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mingzhe Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02153">
<title>Robust Self-Supervised Extrinsic Self-Calibration. (arXiv:2308.02153v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02153</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles and robots need to operate over a wide variety of
scenarios in order to complete tasks efficiently and safely. Multi-camera
self-supervised monocular depth estimation from videos is a promising way to
reason about the environment, as it generates metrically scaled geometric
predictions from visual data without requiring additional sensors. However,
most works assume well-calibrated extrinsics to fully leverage this
multi-camera setup, even though accurate and efficient calibration is still a
challenging problem. In this work, we introduce a novel method for extrinsic
calibration that builds upon the principles of self-supervised monocular depth
and ego-motion learning. Our proposed curriculum learning strategy uses
monocular depth and pose estimators with velocity supervision to estimate
extrinsics, and then jointly learns extrinsic calibration along with depth and
pose for a set of overlapping cameras rigidly attached to a moving vehicle.
Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our
method enables self-calibration in various scenes robustly and efficiently
compared to a traditional vision-based pose estimation pipeline. Furthermore,
we demonstrate the benefits of extrinsics self-calibration as a way to improve
depth prediction via joint optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_T/0/1/0/all/0/1&quot;&gt;Takayuki Kanai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasiljevic_I/0/1/0/all/0/1&quot;&gt;Igor Vasiljevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1&quot;&gt;Vitor Guizilini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1&quot;&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1&quot;&gt;Rares Ambrus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02154">
<title>SDDM: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation. (arXiv:2308.02154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02154</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent score-based diffusion models (SBDMs) show promising results in
unpaired image-to-image translation (I2I). However, existing methods, either
energy-based or statistically-based, provide no explicit form of the interfered
intermediate generative distributions. This work presents a new
score-decomposed diffusion model (SDDM) on manifolds to explicitly optimize the
tangled distributions during image generation. SDDM derives manifolds to make
the distributions of adjacent time steps separable and decompose the score
function or energy guidance into an image ``denoising&quot; part and a content
``refinement&quot; part. To refine the image in the same noise level, we equalize
the refinement parts of the score function and energy guidance, which permits
multi-objective optimization on the manifold. We also leverage the block
adaptive instance normalization module to construct manifolds with lower
dimensions but still concentrated with the perturbed reference image. SDDM
outperforms existing SBDM-based methods with much fewer diffusion steps on
several I2I benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shikun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Longhui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jia Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02158">
<title>CTP-Net: Character Texture Perception Network for Document Image Forgery Localization. (arXiv:2308.02158v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02158</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the progression of information technology in recent years, document
images have been widely disseminated in social networks. With the help of
powerful image editing tools, document images are easily forged without leaving
visible manipulation traces, which leads to severe issues if significant
information is falsified for malicious use. Therefore, the research of document
image forensics is worth further exploring. In a document image, the character
with specific semantic information is most vulnerable to tampering, for which
capturing the forgery traces of the character is the key to localizing the
forged region in document images. Considering both character and image
textures, in this paper, we propose a Character Texture Perception Network
(CTP-Net) to localize the forgery of document images. Based on optical
character recognition, a Character Texture Stream (CTS) is designed to capture
features of text areas that are essential components of a document image.
Meanwhile, texture features of the whole document image are exploited by an
Image Texture Stream (ITS). Combining the features extracted from the CTS and
the ITS, the CTP-Net can reveal more subtle forgery traces from document
images. To overcome the challenge caused by the lack of fake document images,
we design a data generation strategy that is utilized to construct a Fake
Chinese Trademark dataset (FCTM). Through a series of experiments, we show that
the proposed CTP-Net is able to capture tampering traces in document images,
especially in text regions. Experimental results demonstrate that CTP-Net can
localize multi-scale forged areas in document images and outperform the
state-of-the-art forgery localization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1&quot;&gt;Xin Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiehua Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02161">
<title>M2Former: Multi-Scale Patch Selection for Fine-Grained Visual Recognition. (arXiv:2308.02161v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02161</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, vision Transformers (ViTs) have been actively applied to
fine-grained visual recognition (FGVR). ViT can effectively model the
interdependencies between patch-divided object regions through an inherent
self-attention mechanism. In addition, patch selection is used with ViT to
remove redundant patch information and highlight the most discriminative object
patches. However, existing ViT-based FGVR models are limited to single-scale
processing, and their fixed receptive fields hinder representational richness
and exacerbate vulnerability to scale variability. Therefore, we propose
multi-scale patch selection (MSPS) to improve the multi-scale capabilities of
existing ViT-based models. Specifically, MSPS selects salient patches of
different scales at different stages of a multi-scale vision Transformer
(MS-ViT). In addition, we introduce class token transfer (CTT) and multi-scale
cross-attention (MSCA) to model cross-scale interactions between selected
multi-scale patches and fully reflect them in model decisions. Compared to
previous single-scale patch selection (SSPS), our proposed MSPS encourages
richer object representations based on feature hierarchy and consistently
improves performance from small-sized to large-sized objects. As a result, we
propose M2Former, which outperforms CNN-/ViT-based models on several widely
used FGVR benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jiyong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junseok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yunju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seongsik Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02162">
<title>Learning Referring Video Object Segmentation from Weak Annotation. (arXiv:2308.02162v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02162</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring video object segmentation (RVOS) is a task that aims to segment the
target object in all video frames based on a sentence describing the object.
Previous RVOS methods have achieved significant performance with
densely-annotated datasets, whose construction is expensive and time-consuming.
To relieve the burden of data annotation while maintaining sufficient
supervision for segmentation, we propose a new annotation scheme, in which we
label the frame where the object first appears with a mask and use bounding
boxes for the subsequent frames. Based on this scheme, we propose a method to
learn from this weak annotation. Specifically, we design a cross frame
segmentation method, which uses the language-guided dynamic filters to
thoroughly leverage the valuable mask annotation and bounding boxes. We further
develop a bi-level contrastive learning method to encourage the model to learn
discriminative representation at the pixel level. Extensive experiments and
ablative analyses show that our method is able to achieve competitive
performance without the demand of dense mask annotation. The code will be
available at https://github.com/wangbo-zhao/WRVOS/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wangbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_K/0/1/0/all/0/1&quot;&gt;Kepan Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02173">
<title>Efficient Labelling of Affective Video Datasets via Few-Shot &amp; Multi-Task Contrastive Learning. (arXiv:2308.02173v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02173</link>
<description rdf:parseType="Literal">&lt;p&gt;Whilst deep learning techniques have achieved excellent emotion prediction,
they still require large amounts of labelled training data, which are (a)
onerous and tedious to compile, and (b) prone to errors and biases. We propose
Multi-Task Contrastive Learning for Affect Representation (\textbf{MT-CLAR})
for few-shot affect inference. MT-CLAR combines multi-task learning with a
Siamese network trained via contrastive learning to infer from a pair of
expressive facial images (a) the (dis)similarity between the facial
expressions, and (b) the difference in valence and arousal levels of the two
faces. We further extend the image-based MT-CLAR framework for automated video
labelling where, given one or a few labelled video frames (termed
\textit{support-set}), MT-CLAR labels the remainder of the video for valence
and arousal. Experiments are performed on the AFEW-VA dataset with multiple
support-set configurations; moreover, supervised learning on representations
learnt via MT-CLAR are used for valence, arousal and categorical emotion
prediction on the AffectNet and AFEW-VA datasets. The results show that valence
and arousal predictions via MT-CLAR are very comparable to the state-of-the-art
(SOTA), and we significantly outperform SOTA with a support-set $\approx$6\%
the size of the video dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parameshwara_R/0/1/0/all/0/1&quot;&gt;Ravikiran Parameshwara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radwan_I/0/1/0/all/0/1&quot;&gt;Ibrahim Radwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asthana_A/0/1/0/all/0/1&quot;&gt;Akshay Asthana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasnejad_I/0/1/0/all/0/1&quot;&gt;Iman Abbasnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_R/0/1/0/all/0/1&quot;&gt;Ramanathan Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goecke_R/0/1/0/all/0/1&quot;&gt;Roland Goecke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02177">
<title>Scene-aware Human Pose Generation using Transformer. (arXiv:2308.02177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02177</link>
<description rdf:parseType="Literal">&lt;p&gt;Affordance learning considers the interaction opportunities for an actor in
the scene and thus has wide application in scene understanding and intelligent
robotics. In this paper, we focus on contextual affordance learning, i.e.,
using affordance as context to generate a reasonable human pose in a scene.
Existing scene-aware human pose generation methods could be divided into two
categories depending on whether using pose templates. Our proposed method
belongs to the template-based category, which benefits from the representative
pose templates. Moreover, inspired by recent transformer-based methods, we
associate each query embedding with a pose template, and use the interaction
between query embeddings and scene feature map to effectively predict the scale
and offsets for each pose template. In addition, we employ knowledge
distillation to facilitate the offset learning given the predicted scale.
Comprehensive experiments on Sitcom dataset demonstrate the effectiveness of
our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jieteng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Li Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_B/0/1/0/all/0/1&quot;&gt;Bin Sheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02184">
<title>Synthetic outlier generation for anomaly detection in autonomous driving. (arXiv:2308.02184v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02184</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection, or outlier detection, is a crucial task in various domains
to identify instances that significantly deviate from established patterns or
the majority of data. In the context of autonomous driving, the identification
of anomalies is particularly important to prevent safety-critical incidents, as
deep learning models often exhibit overconfidence in anomalous or outlier
samples. In this study, we explore different strategies for training an image
semantic segmentation model with an anomaly detection module. By introducing
modifications to the training stage of the state-of-the-art DenseHybrid model,
we achieve significant performance improvements in anomaly detection. Moreover,
we propose a simplified detector that achieves comparable results to our
modified DenseHybrid approach, while also surpassing the performance of the
original DenseHybrid model. These findings demonstrate the efficacy of our
proposed strategies for enhancing anomaly detection in the context of
autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bikandi_M/0/1/0/all/0/1&quot;&gt;Martin Bikandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velez_G/0/1/0/all/0/1&quot;&gt;Gorka Velez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aginako_N/0/1/0/all/0/1&quot;&gt;Naiara Aginako&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irigoien_I/0/1/0/all/0/1&quot;&gt;Itziar Irigoien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02191">
<title>ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View Stereo. (arXiv:2308.02191v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02191</link>
<description rdf:parseType="Literal">&lt;p&gt;Compared to the multi-stage self-supervised multi-view stereo (MVS) method,
the end-to-end (E2E) approach has received more attention due to its concise
and efficient training pipeline. Recent E2E self-supervised MVS approaches have
integrated third-party models (such as optical flow models, semantic
segmentation models, NeRF models, etc.) to provide additional consistency
constraints, which grows GPU memory consumption and complicates the model&apos;s
structure and training pipeline. In this work, we propose an efficient
framework for end-to-end self-supervised MVS, dubbed ES-MVSNet. To alleviate
the high memory consumption of current E2E self-supervised MVS frameworks, we
present a memory-efficient architecture that reduces memory usage by 43%
without compromising model performance. Furthermore, with the novel design of
asymmetric view selection policy and region-aware depth consistency, we achieve
state-of-the-art performance among E2E self-supervised MVS methods, without
relying on third-party models for additional consistency signals. Extensive
experiments on DTU and Tanks&amp;amp;Temples benchmarks demonstrate that the proposed
ES-MVSNet approach achieves state-of-the-art performance among E2E
self-supervised MVS methods and competitive performance to many supervised and
multi-stage self-supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chaohui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02194">
<title>Paired Competing Neurons Improving STDP Supervised Local Learning In Spiking Neural Networks. (arXiv:2308.02194v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02194</link>
<description rdf:parseType="Literal">&lt;p&gt;Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware
has the potential to significantly reduce the high energy consumption of
Artificial Neural Networks (ANNs) training on modern computers. The biological
plausibility of SNNs allows them to benefit from bio-inspired plasticity rules,
such as Spike Timing-Dependent Plasticity (STDP). STDP offers gradient-free and
unsupervised local learning, which can be easily implemented on neuromorphic
hardware. However, relying solely on unsupervised STDP to perform
classification tasks is not enough. In this paper, we propose Stabilized
Supervised STDP (S2-STDP), a supervised STDP learning rule to train the
classification layer of an SNN equipped with unsupervised STDP. S2-STDP
integrates error-modulated weight updates that align neuron spikes with desired
timestamps derived from the average firing time within the layer. Then, we
introduce a training architecture called Paired Competing Neurons (PCN) to
further enhance the learning capabilities of our classification layer trained
with S2-STDP. PCN associates each class with paired neurons and encourages
neuron specialization through intra-class competition. We evaluated our
proposed methods on image recognition datasets, including MNIST, Fashion-MNIST,
and CIFAR-10. Results showed that our methods outperform current supervised
STDP-based state of the art, for comparable architectures and numbers of
neurons. Also, the use of PCN enhances the performance of S2-STDP, regardless
of the configuration, and without introducing any hyperparameters.Further
analysis demonstrated that our methods exhibited improved hyperparameter
robustness, which reduces the need for tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goupy_G/0/1/0/all/0/1&quot;&gt;Gaspard Goupy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirilly_P/0/1/0/all/0/1&quot;&gt;Pierre Tirilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilasco_I/0/1/0/all/0/1&quot;&gt;Ioan Marius Bilasco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02213">
<title>Balanced Classification: A Unified Framework for Long-Tailed Object Detection. (arXiv:2308.02213v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02213</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional detectors suffer from performance degradation when dealing with
long-tailed data due to a classification bias towards the majority head
categories. In this paper, we contend that the learning bias originates from
two factors: 1) the unequal competition arising from the imbalanced
distribution of foreground categories, and 2) the lack of sample diversity in
tail categories. To tackle these issues, we introduce a unified framework
called BAlanced CLassification (BACL), which enables adaptive rectification of
inequalities caused by disparities in category distribution and dynamic
intensification of sample diversities in a synchronized manner. Specifically, a
novel foreground classification balance loss (FCBL) is developed to ameliorate
the domination of head categories and shift attention to
difficult-to-differentiate categories by introducing pairwise class-aware
margins and auto-adjusted weight terms, respectively. This loss prevents the
over-suppression of tail categories in the context of unequal competition.
Moreover, we propose a dynamic feature hallucination module (FHM), which
enhances the representation of tail categories in the feature space by
synthesizing hallucinated samples to introduce additional data variances. In
this divide-and-conquer approach, BACL sets a new state-of-the-art on the
challenging LVIS benchmark with a decoupled training pipeline, surpassing
vanilla Faster R-CNN with ResNet-50-FPN by 5.8% AP and 16.1% AP for overall and
tail categories. Extensive experiments demonstrate that BACL consistently
achieves performance improvements across various datasets with different
backbones and architectures. Code and models are available at
https://github.com/Tianhao-Qi/BACL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1&quot;&gt;Tianhao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hongtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pandeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jiannan Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02225">
<title>Deep Semantic Model Fusion for Ancient Agricultural Terrace Detection. (arXiv:2308.02225v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02225</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering ancient agricultural terraces in desert regions is important for
the monitoring of long-term climate changes on the Earth&apos;s surface. However,
traditional ground surveys are both costly and limited in scale. With the
increasing accessibility of aerial and satellite data, machine learning
techniques bear large potential for the automatic detection and recognition of
archaeological landscapes. In this paper, we propose a deep semantic model
fusion method for ancient agricultural terrace detection. The input data
includes aerial images and LiDAR generated terrain features in the Negev
desert. Two deep semantic segmentation models, namely DeepLabv3+ and UNet, with
EfficientNet backbone, are trained and fused to provide segmentation maps of
ancient terraces and walls. The proposed method won the first prize in the
International AI Archaeology Challenge. Codes are available at
https://github.com/wangyi111/international-archaeology-ai-challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1&quot;&gt;Arti Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silver_M/0/1/0/all/0/1&quot;&gt;Micha Silver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnieli_A/0/1/0/all/0/1&quot;&gt;Arnon Karnieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1&quot;&gt;Conrad M Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02228">
<title>Painterly Image Harmonization using Diffusion Model. (arXiv:2308.02228v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02228</link>
<description rdf:parseType="Literal">&lt;p&gt;Painterly image harmonization aims to insert photographic objects into
paintings and obtain artistically coherent composite images. Previous methods
for this task mainly rely on inference optimization or generative adversarial
network, but they are either very time-consuming or struggling at fine control
of the foreground objects (e.g., texture and content details). To address these
issues, we propose a novel Painterly Harmonization stable Diffusion model
(PHDiffusion), which includes a lightweight adaptive encoder and a Dual Encoder
Fusion (DEF) module. Specifically, the adaptive encoder and the DEF module
first stylize foreground features within each encoder. Then, the stylized
foreground features from both encoders are combined to guide the harmonization
process. During training, besides the noise loss in diffusion model, we
additionally employ content loss and two style losses, i.e., AdaIN style loss
and contrastive style loss, aiming to balance the trade-off between style
migration and content preservation. Compared with the state-of-the-art models
from related fields, our PHDiffusion can stylize the foreground more
sufficiently and simultaneously retain finer content. Our code and model are
available at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lingxiao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangtong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Junyan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Li Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02236">
<title>FB-BEV: BEV Representation from Forward-Backward View Transformations. (arXiv:2308.02236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02236</link>
<description rdf:parseType="Literal">&lt;p&gt;View Transformation Module (VTM), where transformations happen between
multi-view image features and Bird-Eye-View (BEV) representation, is a crucial
step in camera-based BEV perception systems. Currently, the two most prominent
VTM paradigms are forward projection and backward projection. Forward
projection, represented by Lift-Splat-Shoot, leads to sparsely projected BEV
features without post-processing. Backward projection, with BEVFormer being an
example, tends to generate false-positive BEV features from incorrect
projections due to the lack of utilization on depth. To address the above
limitations, we propose a novel forward-backward view transformation module.
Our approach compensates for the deficiencies in both existing methods,
allowing them to enhance each other to obtain higher quality BEV
representations mutually. We instantiate the proposed module with FB-BEV, which
achieves a new state-of-the-art result of 62.4\% NDS on the nuScenes test set.
The code will be released at \url{https://github.com/NVlabs/FB-BEV}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02237">
<title>MSECNet: Accurate and Robust Normal Estimation for 3D Point Clouds by Multi-Scale Edge Conditioning. (arXiv:2308.02237v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02237</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating surface normals from 3D point clouds is critical for various
applications, including surface reconstruction and rendering. While existing
methods for normal estimation perform well in regions where normals change
slowly, they tend to fail where normals vary rapidly. To address this issue, we
propose a novel approach called MSECNet, which improves estimation in normal
varying regions by treating normal variation modeling as an edge detection
problem. MSECNet consists of a backbone network and a multi-scale edge
conditioning (MSEC) stream. The MSEC stream achieves robust edge detection
through multi-scale feature fusion and adaptive edge detection. The detected
edges are then combined with the output of the backbone network using the edge
conditioning module to produce edge-aware representations. Extensive
experiments show that MSECNet outperforms existing methods on both synthetic
(PCPNet) and real-world (SceneNN) datasets while running significantly faster.
We also conduct various analyses to investigate the contribution of each
component in the MSEC stream. Finally, we demonstrate the effectiveness of our
approach in surface reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiu_H/0/1/0/all/0/1&quot;&gt;Haoyi Xiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weimin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyoung-Sook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuoka_M/0/1/0/all/0/1&quot;&gt;Masashi Matsuoka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02239">
<title>DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via Deformable Template Field. (arXiv:2308.02239v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02239</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating 6D poses and reconstructing 3D shapes of objects in open-world
scenes from RGB-depth image pairs is challenging. Many existing methods rely on
learning geometric features that correspond to specific templates while
disregarding shape variations and pose differences among objects in the same
category. As a result, these methods underperform when handling unseen object
instances in complex environments. In contrast, other approaches aim to achieve
category-level estimation and reconstruction by leveraging normalized geometric
structure priors, but the static prior-based reconstruction struggles with
substantial intra-class variations. To solve these problems, we propose the
DTF-Net, a novel framework for pose estimation and shape reconstruction based
on implicit neural fields of object categories. In DTF-Net, we design a
deformable template field to represent the general category-wise shape latent
features and intra-category geometric deformation features. The field
establishes continuous shape correspondences, deforming the category template
into arbitrary observed instances to accomplish shape reconstruction. We
introduce a pose regression module that shares the deformation features and
template codes from the fields to estimate the accurate 6D pose of each object
in the scene. We integrate a multi-modal representation extraction module to
extract object features and semantic masks, enabling end-to-end inference.
Moreover, during training, we implement a shape-invariant training strategy and
a viewpoint sampling method to further enhance the model&apos;s capability to
extract object pose features. Extensive experiments on the REAL275 and CAMERA25
datasets demonstrate the superiority of DTF-Net in both synthetic and real
scenes. Furthermore, we show that DTF-Net effectively supports grasping tasks
with a real robot arm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Feifei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yakun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1&quot;&gt;Xiuquan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02248">
<title>On the Calibration of Uncertainty Estimation in LiDAR-based Semantic Segmentation. (arXiv:2308.02248v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02248</link>
<description rdf:parseType="Literal">&lt;p&gt;The confidence calibration of deep learning-based perception models plays a
crucial role in their reliability. Especially in the context of autonomous
driving, downstream tasks like prediction and planning depend on accurate
confidence estimates. In point-wise multiclass classification tasks like
sematic segmentation the model has to deal with heavy class imbalances. Due to
their underrepresentation, the confidence calibration of classes with smaller
instances is challenging but essential, not only for safety reasons. We propose
a metric to measure the confidence calibration quality of a semantic
segmentation model with respect to individual classes. It is calculated by
computing sparsification curves for each class based on the uncertainty
estimates. We use the classification calibration metric to evaluate uncertainty
estimation methods with respect to their confidence calibration of
underrepresented classes. We furthermore suggest a double use for the method to
automatically find label problems to improve the quality of hand- or
auto-annotated datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1&quot;&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piewak_F/0/1/0/all/0/1&quot;&gt;Florian Piewak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1&quot;&gt;Joschka Boedecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02266">
<title>SURE-Val: Safe Urban Relevance Extension and Validation. (arXiv:2308.02266v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02266</link>
<description rdf:parseType="Literal">&lt;p&gt;To evaluate perception components of an automated driving system, it is
necessary to define the relevant objects. While the urban domain is popular
among perception datasets, relevance is insufficiently specified for this
domain. Therefore, this work adopts an existing method to define relevance in
the highway domain and expands it to the urban domain. While different
conceptualizations and definitions of relevance are present in literature,
there is a lack of methods to validate these definitions. Therefore, this work
presents a novel relevance validation method leveraging a motion prediction
component. The validation leverages the idea that removing irrelevant objects
should not influence a prediction component which reflects human driving
behavior. The influence on the prediction is quantified by considering the
statistical distribution of prediction performance across a large-scale
dataset. The validation procedure is verified using criteria specifically
designed to exclude relevant objects. The validation method is successfully
applied to the relevance criteria from this work, thus supporting their
validity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storms_K/0/1/0/all/0/1&quot;&gt;Kai Storms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1&quot;&gt;Ken Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_S/0/1/0/all/0/1&quot;&gt;Steven Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02283">
<title>Diffusion-Augmented Depth Prediction with Sparse Annotations. (arXiv:2308.02283v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02283</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth estimation aims to predict dense depth maps. In autonomous driving
scenes, sparsity of annotations makes the task challenging. Supervised models
produce concave objects due to insufficient structural information. They
overfit to valid pixels and fail to restore spatial structures. Self-supervised
methods are proposed for the problem. Their robustness is limited by pose
estimation, leading to erroneous results in natural scenes. In this paper, we
propose a supervised framework termed Diffusion-Augmented Depth Prediction
(DADP). We leverage the structural characteristics of diffusion model to
enforce depth structures of depth models in a plug-and-play manner. An
object-guided integrality loss is also proposed to further enhance regional
structure integrality by fetching objective information. We evaluate DADP on
three driving benchmarks and achieve significant improvements in depth
structures and robustness. Our work provides a new perspective on depth
estimation with sparse annotations in autonomous driving scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zihao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinghong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_K/0/1/0/all/0/1&quot;&gt;Ke Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02299">
<title>RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension. (arXiv:2308.02299v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02299</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate extending the comprehension of Multi-modal Large
Language Models (MLLMs) to regional objects. To this end, we propose to extract
features corresponding to regional objects as soft prompts for LLM, which
provides a straightforward and scalable approach and eliminates the need for
LLM fine-tuning. To effectively extract regional features from regular image
features and irregular point cloud features, we present a novel and unified
position-assisted feature extraction module. Furthermore, training an MLLM from
scratch is highly time-consuming. Thus, we propose incrementally extending
existing pre-trained MLLMs to comprehend more modalities and the regional
objects of those modalities. Specifically, we freeze the Q-Former from BLIP-2,
an impressive MLLM, and optimize the modality-specific Lora parameters in
Q-Former and LLM for each newly introduced modality. The freezing of the
Q-Former eliminates the need for extensive pre-training on massive image-text
data. The freezed Q-Former pre-trained from massive image-text data is also
beneficial for the pre-training on image-region-text data. We name our
framework RegionBLIP. We pre-train RegionBLIP on image-region-text,
point-cloud-text, and point-cloud-region-text data. Experimental results verify
that \Ours{} can preserve the image comprehension capability of BILP-2 and
further gain a comprehension of the newly introduced point cloud modality and
regional objects. The Data, Code, and Pre-trained models will be available at
https://github.com/mightyzau/RegionBLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chaohui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaofeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sitong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02335">
<title>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02335</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph classification is a crucial task in many real-world multimedia
applications, where graphs can represent various multimedia data types such as
images, videos, and social networks. Previous efforts have applied graph neural
networks (GNNs) in balanced situations where the class distribution is
balanced. However, real-world data typically exhibit long-tailed class
distributions, resulting in a bias towards the head classes when using GNNs and
limited generalization ability over the tail classes. Recent approaches mainly
focus on re-balancing different classes during model training, which fails to
explicitly introduce new knowledge and sacrifices the performance of the head
classes. To address these drawbacks, we propose a novel framework called
Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature
extractor and an unbiased classifier in a decoupled manner. In the feature
extractor training stage, we develop a graph retrieval module to search for
relevant graphs that directly enrich the intra-class diversity for the tail
classes. Moreover, we innovatively optimize a category-centered supervised
contrastive loss to obtain discriminative representations, which is more
suitable for long-tailed scenarios. In the classifier fine-tuning stage, we
balance the classifier weights with two weight regularization techniques, i.e.,
Max-norm and weight decay. Experiments on various popular benchmarks verify the
superiority of the proposed method against state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1&quot;&gt;Wei Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yifang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02339">
<title>Improving Scene Graph Generation with Superpixel-Based Interaction Learning. (arXiv:2308.02339v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02339</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Scene Graph Generation (SGG) typically model the
relationships among entities utilizing box-level features from pre-defined
detectors. We argue that an overlooked problem in SGG is the coarse-grained
interactions between boxes, which inadequately capture contextual semantics for
relationship modeling, practically limiting the development of the field. In
this paper, we take the initiative to explore and propose a generic paradigm
termed Superpixel-based Interaction Learning (SIL) to remedy coarse-grained
interactions at the box level. It allows us to model fine-grained interactions
at the superpixel level in SGG. Specifically, (i) we treat a scene as a set of
points and cluster them into superpixels representing sub-regions of the scene.
(ii) We explore intra-entity and cross-entity interactions among the
superpixels to enrich fine-grained interactions between entities at an earlier
stage. Extensive experiments on two challenging benchmarks (Visual Genome and
Open Image V6) prove that our SIL enables fine-grained interaction at the
superpixel level above previous box-level methods, and significantly
outperforms previous state-of-the-art methods across all metrics. More
encouragingly, the proposed method can be applied to boost the performance of
existing box-level approaches in a plug-and-play fashion. In particular, SIL
brings an average improvement of 2.0% mR (even up to 3.4%) of baselines for the
PredCls task on Visual Genome, which facilitates its integration into any
existing box-level method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Can Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jinfa Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1&quot;&gt;Botao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhidong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02340">
<title>Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images. (arXiv:2308.02340v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.02340</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: In this work, we present a workflow to construct generic and robust
generative image priors from magnitude-only images. The priors can then be used
for regularization in reconstruction to improve image quality. Methods: The
workflow begins with the preparation of training datasets from magnitude-only
MR images. This dataset is then augmented with phase information and used to
train generative priors of complex images. Finally, trained priors are
evaluated using both linear and nonlinear reconstruction for compressed sensing
parallel imaging with various undersampling schemes. Results: The results of
our experiments demonstrate that priors trained on complex images outperform
priors trained only on magnitude images. Additionally, a prior trained on a
larger dataset exhibits higher robustness. Finally, we show that the generative
priors are superior to L1 -wavelet regularization for compressed sensing
parallel imaging with high undersampling. Conclusion: These findings stress the
importance of incorporating phase information and leveraging large datasets to
raise the performance and reliability of the generative priors for MRI
reconstruction. Phase augmentation makes it possible to use existing image
databases for training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Guanxiong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Blumenthal_M/0/1/0/all/0/1&quot;&gt;Mortiz Blumenthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schilling_M/0/1/0/all/0/1&quot;&gt;Martin Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauf_E/0/1/0/all/0/1&quot;&gt;Erik Hans Ulrich Rauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kotikalapudi_R/0/1/0/all/0/1&quot;&gt;Raviteja Kotikalapudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Focke_N/0/1/0/all/0/1&quot;&gt;Niels Focke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uecker_M/0/1/0/all/0/1&quot;&gt;Martin Uecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02346">
<title>Class Incremental Learning with Self-Supervised Pre-Training and Prototype Learning. (arXiv:2308.02346v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02346</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Network (DNN) has achieved great success on datasets of closed
class set. However, new classes, like new categories of social media topics,
are continuously added to the real world, making it necessary to incrementally
learn. This is hard for DNN because it tends to focus on fitting to new classes
while ignoring old classes, a phenomenon known as catastrophic forgetting.
State-of-the-art methods rely on knowledge distillation and data replay
techniques but still have limitations. In this work, we analyze the causes of
catastrophic forgetting in class incremental learning, which owes to three
factors: representation drift, representation confusion, and classifier
distortion. Based on this view, we propose a two-stage learning framework with
a fixed encoder and an incrementally updated prototype classifier. The encoder
is trained with self-supervised learning to generate a feature space with high
intrinsic dimensionality, thus improving its transferability and generality.
The classifier incrementally learns new prototypes while retaining the
prototypes of previously learned data, which is crucial in preserving the
decision boundary.Our method does not rely on preserved samples of old classes,
is thus a non-exemplar based CIL method. Experiments on public datasets show
that our method can significantly outperform state-of-the-art exemplar-based
methods when they reserved 5 examplers per class, under the incremental setting
of 10 phases, by 18.24% on CIFAR-100 and 9.37% on ImageNet100.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenzhuo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinjian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mingming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng-Lin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02350">
<title>RobustMQ: Benchmarking Robustness of Quantized Models. (arXiv:2308.02350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02350</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization has emerged as an essential technique for deploying deep neural
networks (DNNs) on devices with limited resources. However, quantized models
exhibit vulnerabilities when exposed to various noises in real-world
applications. Despite the importance of evaluating the impact of quantization
on robustness, existing research on this topic is limited and often disregards
established principles of robustness evaluation, resulting in incomplete and
inconclusive findings. To address this gap, we thoroughly evaluated the
robustness of quantized models against various noises (adversarial attacks,
natural corruptions, and systematic noises) on ImageNet. The comprehensive
evaluation results empirically provide valuable insights into the robustness of
quantized models in various scenarios, for example: (1) quantized models
exhibit higher adversarial robustness than their floating-point counterparts,
but are more vulnerable to natural corruptions and systematic noises; (2) in
general, increasing the quantization bit-width results in a decrease in
adversarial robustness, an increase in natural robustness, and an increase in
systematic robustness; (3) among corruption methods, \textit{impulse noise} and
\textit{glass blur} are the most harmful to quantized models, while
\textit{brightness} has the least impact; (4) among systematic noises, the
\textit{nearest neighbor interpolation} has the highest impact, while bilinear
interpolation, cubic interpolation, and area interpolation are the three least
harmful. Our research contributes to advancing the robust quantization of
models and their deployment in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yisong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Haotong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02351">
<title>A Parameter-efficient Multi-subject Model for Predicting fMRI Activity. (arXiv:2308.02351v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02351</link>
<description rdf:parseType="Literal">&lt;p&gt;This is the Algonauts 2023 submission report for team &quot;BlobGPT&quot;. Our model
consists of a multi-subject linear encoding head attached to a pretrained trunk
model. The multi-subject head consists of three components: (1) a shared
multi-layer feature projection, (2) shared plus subject-specific low-dimension
linear transformations, and (3) a shared PCA fMRI embedding. In this report, we
explain these components in more detail and present some experimental results.
Our code is available at https://github.com/cmi-dair/algonauts23.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lane_C/0/1/0/all/0/1&quot;&gt;Connor Lane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1&quot;&gt;Gregory Kiar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02356">
<title>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images. (arXiv:2308.02356v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02356</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing image change detection aims to identify the differences
between images acquired at different times in the same area. It is widely used
in land management, environmental monitoring, disaster assessment and other
fields. Currently, most change detection methods are based on Siamese network
structure or early fusion structure. Siamese structure focuses on extracting
object features at different times but lacks attention to change information,
which leads to false alarms and missed detections. Early fusion (EF) structure
focuses on extracting features after the fusion of images of different phases
but ignores the significance of object features at different times for
detecting change details, making it difficult to accurately discern the edges
of changed objects. To address these issues and obtain more accurate results,
we propose a novel network, Triplet UNet(T-UNet), based on a three-branch
encoder, which is capable to simultaneously extract the object features and the
change features between the pre- and post-time-phase images through triplet
encoder. To effectively interact and fuse the features extracted from the three
branches of triplet encoder, we propose a multi-branch spatial-spectral
cross-attention module (MBSSCA). In the decoder stage, we introduce the channel
attention mechanism (CAM) and spatial attention mechanism (SAM) to fully mine
and integrate detailed textures information at the shallow layer and semantic
localization information at the deep layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Huan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02363">
<title>Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation. (arXiv:2308.02363v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.02363</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models usually require sufficient training data to achieve high
accuracy, but obtaining labeled data can be time-consuming and labor-intensive.
Here we introduce a template-based training method to train a 3D U-Net model
from scratch using only one population-averaged brain MRI template and its
associated segmentation label. The process incorporated visual perception
augmentation to enhance the model&apos;s robustness in handling diverse image inputs
and mitigating overfitting. Leveraging this approach, we trained 3D U-Net
models for mouse, rat, marmoset, rhesus, and human brain MRI to achieve
segmentation tasks such as skull-stripping, brain segmentation, and tissue
probability mapping. This tool effectively addresses the limited availability
of training data and holds significant potential for expanding deep learning
applications in image analysis, providing researchers with a unified solution
to train deep neural networks with only one image sample.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yeh_F/0/1/0/all/0/1&quot;&gt;Fang-Cheng Yeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02369">
<title>Universal Defensive Underpainting Patch: Making Your Text Invisible to Optical Character Recognition. (arXiv:2308.02369v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02369</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Character Recognition (OCR) enables automatic text extraction from
scanned or digitized text images, but it also makes it easy to pirate valuable
or sensitive text from these images. Previous methods to prevent OCR piracy by
distorting characters in text images are impractical in real-world scenarios,
as pirates can capture arbitrary portions of the text images, rendering the
defenses ineffective. In this work, we propose a novel and effective defense
mechanism termed the Universal Defensive Underpainting Patch (UDUP) that
modifies the underpainting of text images instead of the characters. UDUP is
created through an iterative optimization process to craft a small, fixed-size
defensive patch that can generate non-overlapping underpainting for text images
of any size. Experimental results show that UDUP effectively defends against
unauthorized OCR under the setting of any screenshot range or complex image
background. It is agnostic to the content, size, colors, and languages of
characters, and is robust to typical image operations such as scaling and
compressing. In addition, the transferability of UDUP is demonstrated by
evading several off-the-shelf OCRs. The code is available at
https://github.com/QRICKDD/UDUP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;JiaCheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Li Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Diqun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rangding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Dengpan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lingchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinyu Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02393">
<title>A Bi-variant Variational Model for Diffeomorphic Image Registration with Relaxed Jacobian Determinant Constraints. (arXiv:2308.02393v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02393</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffeomorphic registration has become a powerful approach for seeking a
smooth and invertible spatial transformation between two coordinate systems
which have been measured via the template and reference images. While the
pointwise volume-preserving constraint is effective for some problems, it is
too stringent for many other problems especially when the local deformations
are relatively large, because it may lead to a poor large-deformation for
enforcing local matching.In this paper, we propose a novel bi-variant
diffeomorphic image registration model with the soft constraint of Jacobian
equation, which allows local deformations to shrink and grow in a flexible
range.The Jacobian determinant of the transformation is explicitly controlled
by optimizing the relaxation function. To prevent deformation folding and
enhance the smoothness of deformation, we not only impose a positivity
constraint in optimizing the relaxation function, but also employ a regularizer
to ensure the smoothness of the relaxation function.Furthermore, the positivity
constraint ensures that is as close to one as possible, which helps to obtain a
volume-preserving transformation on average.We further analyze the existence of
the minimizer for the variational model and propose a penalty splitting method
with a multilevel strategy to solve this model. Numerical experiments show that
the proposed algorithm is convergent, and the positivity constraint can control
the range of relative volume and not compromise registration accuracy.
Moreover, the proposed model produces diffeomorphic maps for large deformation,
and achieves better performance compared to the several existing registration
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02396">
<title>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar. (arXiv:2308.02396v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02396</link>
<description rdf:parseType="Literal">&lt;p&gt;Human presence detection in indoor environments using millimeter-wave
frequency-modulated continuous-wave (FMCW) radar is challenging due to the
presence of moving and stationary clutters in indoor places. This work proposes
&quot;HOOD&quot; as a real-time robust human presence and out-of-distribution (OOD)
detection method by exploiting 60 GHz short-range FMCW radar. We approach the
presence detection application as an OOD detection problem and solve the two
problems simultaneously using a single pipeline. Our solution relies on a
reconstruction-based architecture and works with radar macro and micro
range-Doppler images (RDIs). HOOD aims to accurately detect the &quot;presence&quot; of
humans in the presence or absence of moving and stationary disturbers. Since it
is also an OOD detector, it aims to detect moving or stationary clutters as OOD
in humans&apos; absence and predicts the current scene&apos;s output as &quot;no presence.&quot;
HOOD is an activity-free approach that performs well in different human
scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we
achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and
experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD
detection methods in terms of common OOD detection metrics. Our real-time
experiments are available at: https://muskahya.github.io/HOOD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kahya_S/0/1/0/all/0/1&quot;&gt;Sabri Mustafa Kahya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Muhammet Sami Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Steinbach_E/0/1/0/all/0/1&quot;&gt;Eckehard Steinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02420">
<title>P\=uioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System. (arXiv:2308.02420v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02420</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated exercise repetition counting has applications across the physical
fitness realm, from personal health to rehabilitation. Motivated by the
ubiquity of mobile phones and the benefits of tracking physical activity, this
study explored the feasibility of counting exercise repetitions in real-time,
using only on-device inference, on smartphones. In this work, after providing
an extensive overview of the state-of-the-art automatic exercise repetition
counting methods, we introduce a deep learning based exercise repetition
counting system for smartphones consisting of five components: (1) Pose
estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5)
Counter. The system is then implemented via a cross-platform mobile application
named P\=uioio that uses only the smartphone camera to track repetitions in
real time for three standard exercises: Squats, Push-ups, and Pull-ups. The
proposed system was evaluated via a dataset of pre-recorded videos of
individuals exercising as well as testing by subjects exercising in real time.
Evaluation results indicated the system was 98.89% accurate in real-world tests
and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an
effective, low-cost, and convenient alternative to existing solutions since the
proposed system has minimal hardware requirements without requiring any
wearable or specific sensors or network connectivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinclair_A/0/1/0/all/0/1&quot;&gt;Adam Sinclair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kautai_K/0/1/0/all/0/1&quot;&gt;Kayla Kautai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shahamiri_S/0/1/0/all/0/1&quot;&gt;Seyed Reza Shahamiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02463">
<title>Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02463</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM.We consider the construction of foundational models from
the perspectives of data, model design, and evaluation thoroughly. Our
contribution can be concluded as follows: (i), we construct a large-scale
Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.
To the best of our knowledge, this is the first multi-modal dataset containing
3D medical scans. (ii), We propose an architecture that enables visually
conditioned generative pre-training, allowing for the integration of text input
interleaved with 2D or 3D medical scans to generate response for diverse
radiologic tasks. The model was initially pre-trained on MedMD and subsequently
domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,
containing 3M radiologic visual-language pairs. (iii), we propose a new
evaluation benchmark that comprises five tasks, aiming to comprehensively
assess the capability of foundation models in handling practical clinical
problems. Our experimental results confirm that RadFM significantly outperforms
existing multi-modal foundation models. The codes, data, and model checkpoint
will all be made publicly available to promote further research and development
in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02487">
<title>Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP. (arXiv:2308.02487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02487</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary segmentation is a challenging task requiring segmenting and
recognizing objects from an open set of categories. One way to address this
challenge is to leverage multi-modal models, such as CLIP, to provide image and
text features in a shared embedding space, which bridges the gap between
closed-vocabulary and open-vocabulary recognition. Hence, existing methods
often adopt a two-stage framework to tackle the problem, where the inputs first
go through a mask generator and then through the CLIP model along with the
predicted masks. This process involves extracting features from images multiple
times, which can be ineffective and inefficient. By contrast, we propose to
build everything into a single-stage framework using a shared Frozen
Convolutional CLIP backbone, which not only significantly simplifies the
current two-stage pipeline, but also remarkably yields a better accuracy-cost
trade-off. The proposed FC-CLIP, benefits from the following observations: the
frozen CLIP backbone maintains the ability of open-vocabulary classification
and can also serve as a strong mask generator, and the convolutional CLIP
generalizes well to a larger input resolution than the one used during
contrastive image-text pretraining. When training on COCO panoptic data only
and testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1
mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2
mIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoU
on ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,
respectively. Additionally, the training and testing time of FC-CLIP is 7.5x
and 6.6x significantly faster than the same prior art, while using 5.9x fewer
parameters. FC-CLIP also sets a new state-of-the-art performance across various
open-vocabulary semantic segmentation datasets. Code at
https://github.com/bytedance/fc-clip
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qihang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Ju He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xueqing Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang-Chieh Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02490">
<title>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.02490</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models. Code and data are
available at https://github.com/yuweihao/MM-Vet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weihao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.09168">
<title>Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention. (arXiv:2003.09168v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.09168</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a scheme for supervised image classification that uses privileged
information, in the form of keypoint annotations for the training data, to
learn strong models from small and/or biased training sets. Our main motivation
is the recognition of animal species for ecological applications such as
biodiversity modelling, which is challenging because of long-tailed species
distributions due to rare species, and strong dataset biases such as repetitive
scene background in camera traps. To counteract these challenges, we propose a
visual attention mechanism that is supervised via keypoint annotations that
highlight important object parts. This privileged information, implemented as a
novel privileged pooling operation, is only required during training and helps
the model to focus on regions that are discriminative. In experiments with
three different animal species datasets, we show that deep networks with
privileged pooling can use small training sets more efficiently and generalize
better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres C. Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1&quot;&gt;Stefano D&amp;#x27;Aronco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1&quot;&gt;Jan Dirk Wegner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.00596">
<title>A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2007.00596</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids &quot;deflation&quot; and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1&quot;&gt;Karl Rohe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09195">
<title>Mitigating the Bias of Centered Objects in Common Datasets. (arXiv:2112.09195v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09195</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional networks are considered shift invariant, but it was
demonstrated that their response may vary according to the exact location of
the objects. In this paper we will demonstrate that most commonly investigated
datasets have a bias, where objects are over-represented at the center of the
image during training. This bias and the boundary condition of these networks
can have a significant effect on the performance of these architectures and
their accuracy drops significantly as an object approaches the boundary. We
will also demonstrate how this effect can be mitigated with data augmentation
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1&quot;&gt;Gergely Szabo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1&quot;&gt;Andras Horvath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11459">
<title>Explore Spatio-temporal Aggregation for Insubstantial Object Detection: Benchmark Dataset and Baseline. (arXiv:2206.11459v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11459</link>
<description rdf:parseType="Literal">&lt;p&gt;We endeavor on a rarely explored task named Insubstantial Object Detection
(IOD), which aims to localize the object with following characteristics: (1)
amorphous shape with indistinct boundary; (2) similarity to surroundings; (3)
absence in color. Accordingly, it is far more challenging to distinguish
insubstantial objects in a single static frame and the collaborative
representation of spatial and temporal information is crucial. Thus, we
construct an IOD-Video dataset comprised of 600 videos (141,017 frames)
covering various distances, sizes, visibility, and scenes captured by different
spectral ranges. In addition, we develop a spatio-temporal aggregation
framework for IOD, in which different backbones are deployed and a
spatio-temporal aggregation loss (STAloss) is elaborately designed to leverage
the consistency along the time axis. Experiments conducted on IOD-Video dataset
demonstrate that spatio-temporal aggregation can significantly improve the
performance of IOD. We hope our work will attract further researches into this
valuable yet challenging task. The code will be available at:
\url{https://github.com/CalayZhou/IOD-Video}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kailai Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1&quot;&gt;Tao Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Linsen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1&quot;&gt;Qiu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12850">
<title>SSIVD-Net: A Novel Salient Super Image Classification &amp; Detection Technique for Weaponized Violence. (arXiv:2207.12850v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12850</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of violence and weaponized violence in closed-circuit television
(CCTV) footage requires a comprehensive approach. In this work, we introduce
the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically
designed to facilitate the learning of weapon distribution in surveillance
videos. To tackle the complexities of analyzing 3D surveillance video for
violence recognition tasks, we propose a novel technique called,
\emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for
\textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data
complexity, dimensionality, and information loss while improving inference,
performance, and explainability through the use of Salient-Super-Image
representations. Considering the scalability and sustainability requirements of
futuristic smart cities, the authors introduce the \emph{Salient-Classifier}, a
novel architecture combining a kernelized approach with a residual learning
strategy. We evaluate variations of SSIVD-Net and Salient Classifier on our
SCVD dataset and benchmark against state-of-the-art (SOTA) models commonly
employed in violence detection. Our approach exhibits significant improvements
in detecting both weaponized and non-weaponized violence instances. By
advancing the SOTA in violence detection, our work offers a practical and
scalable solution suitable for real-world applications. The proposed
methodology not only addresses the challenges of violence detection in CCTV
footage but also contributes to the understanding of weapon distribution in
smart surveillance. Ultimately, our research findings should enable smarter and
more secure cities, as well as enhance public safety measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aremu_T/0/1/0/all/0/1&quot;&gt;Toluwani Aremu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhiyuan_L/0/1/0/all/0/1&quot;&gt;Li Zhiyuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameeri_R/0/1/0/all/0/1&quot;&gt;Reem Alameeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mustaqeem Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00919">
<title>Benchmarking Visual-Inertial Deep Multimodal Fusion for Relative Pose Regression and Odometry-aided Absolute Pose Regression. (arXiv:2208.00919v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00919</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual-inertial localization is a key problem in computer vision and robotics
applications such as virtual reality, self-driving cars, and aerial vehicles.
The goal is to estimate an accurate pose of an object when either the
environment or the dynamics are known. Absolute pose regression (APR)
techniques directly regress the absolute pose from an image input in a known
scene using convolutional and spatio-temporal networks. Odometry methods
perform relative pose regression (RPR) that predicts the relative pose from a
known object dynamic (visual or inertial inputs). The localization task can be
improved by retrieving information from both data sources for a cross-modal
setup, which is a challenging problem due to contradictory tasks. In this work,
we conduct a benchmark to evaluate deep multimodal fusion based on pose graph
optimization and attention networks. Auxiliary and Bayesian learning are
utilized for the APR task. We show accuracy improvements for the APR-RPR task
and for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct
experiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a
novel industry dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1&quot;&gt;Felix Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raichur_N/0/1/0/all/0/1&quot;&gt;Nisha Lakshmana Raichur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feigl_T/0/1/0/all/0/1&quot;&gt;Tobias Feigl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_H/0/1/0/all/0/1&quot;&gt;Heiko Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1&quot;&gt;Christopher Mutschler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09923">
<title>Bridging Language and Geometric Primitives for Zero-shot Point Cloud Segmentation. (arXiv:2210.09923v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09923</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate transductive zero-shot point cloud semantic segmentation,
where the network is trained on seen objects and able to segment unseen
objects. The 3D geometric elements are essential cues to imply a novel 3D
object type. However, previous methods neglect the fine-grained relationship
between the language and the 3D geometric elements. To this end, we propose a
novel framework to learn the geometric primitives shared in seen and unseen
categories&apos; objects and employ a fine-grained alignment between language and
the learned geometric primitives. Therefore, guided by language, the network
recognizes the novel objects represented with geometric primitives.
Specifically, we formulate a novel point visual representation, the similarity
vector of the point&apos;s feature to the learnable prototypes, where the prototypes
automatically encode geometric primitives via back-propagation. Besides, we
propose a novel Unknown-aware InfoNCE Loss to fine-grained align the visual
representation with language. Extensive experiments show that our method
significantly outperforms other state-of-the-art methods in the harmonic
mean-intersection-over-union (hIoU), with the improvement of 17.8\%, 30.4\%,
9.2\% and 7.9\% on S3DIS, ScanNet, SemanticKITTI and nuScenes datasets,
respectively. Codes are available
(https://github.com/runnanchen/Zero-Shot-Point-Cloud-Segmentation)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Runnan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xinge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Nenglun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuexin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruigang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05308">
<title>Enhancing Clinical Support for Breast Cancer with Deep Learning Models using Synthetic Correlated Diffusion Imaging. (arXiv:2211.05308v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05308</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer is the second most common type of cancer in women in Canada and
the United States, representing over 25\% of all new female cancer cases. As
such, there has been immense research and progress on improving screening and
clinical support for breast cancer. In this paper, we investigate enhancing
clinical support for breast cancer with deep learning models using a newly
introduced magnetic resonance imaging (MRI) modality called synthetic
correlated diffusion imaging (CDI$^s$). More specifically, we leverage a
volumetric convolutional neural network to learn volumetric deep radiomic
features from a pre-treatment cohort and construct a predictor based on the
learnt features for grade and post-treatment response prediction. As the first
study to learn CDI$^s$-centric radiomic sequences within a deep learning
perspective for clinical decision support, we evaluated the proposed approach
using the ACRIN-6698 study against those learnt using gold-standard imaging
modalities. We find that the proposed approach can achieve better performance
for both grade and post-treatment response prediction and thus may be a useful
tool to aid oncologists in improving recommendation of treatment of patients.
Subsequently, the approach to leverage volumetric deep radiomic features for
breast cancer can be further extended to other applications of CDI$^s$ in the
cancer domain to further improve clinical support.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1&quot;&gt;Chi-en Amy Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunraj_H/0/1/0/all/0/1&quot;&gt;Hayden Gunraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1&quot;&gt;Nedim Hodzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flanagan_N/0/1/0/all/0/1&quot;&gt;Nic Flanagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabri_A/0/1/0/all/0/1&quot;&gt;Ali Sabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11248">
<title>Video Background Music Generation: Dataset, Method and Evaluation. (arXiv:2211.11248v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11248</link>
<description rdf:parseType="Literal">&lt;p&gt;Music is essential when editing videos, but selecting music manually is
difficult and time-consuming. Thus, we seek to automatically generate
background music tracks given video input. This is a challenging task since it
requires music-video datasets, efficient architectures for video-to-music
generation, and reasonable metrics, none of which currently exist. To close
this gap, we introduce a complete recipe including dataset, benchmark model,
and evaluation metric for video background music generation. We present SymMV,
a video and symbolic music dataset with various musical annotations. To the
best of our knowledge, it is the first video-music dataset with rich musical
annotations. We also propose a benchmark video background music generation
framework named V-MusProd, which utilizes music priors of chords, melody, and
accompaniment along with video-music relations of semantic, color, and motion
features. To address the lack of objective metrics for video-music
correspondence, we design a retrieval-based metric VMCP built upon a powerful
video-music representation learning model. Experiments show that with our
dataset, V-MusProd outperforms the state-of-the-art method in both music
quality and correspondence with videos. We believe our dataset, benchmark
model, and evaluation metric will boost the development of video background
music generation. Our dataset and code are available at
https://github.com/zhuole1025/SymMV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1&quot;&gt;Le Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaokai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yue Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1&quot;&gt;Chenxi Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Stanley Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Songhao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aixi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01261">
<title>Generative Reasoning Integrated Label Noise Robust Deep Image Representation Learning. (arXiv:2212.01261v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01261</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of deep learning based image representation learning (IRL)
methods has attracted great attention for various image understanding problems.
Most of these methods require the availability of a high quantity and quality
of annotated training images, which can be time-consuming and costly to gather.
To reduce labeling costs, crowdsourced data, automatic labeling procedures or
citizen science projects can be considered. However, such approaches increase
the risk of including label noise in training data. It may result in
overfitting on noisy labels when discriminative reasoning is employed. This
leads to sub-optimal learning procedures, and thus inaccurate characterization
of images. To address this, we introduce a generative reasoning integrated
label noise robust deep representation learning (GRID) approach. Our approach
aims to model the complementary characteristics of discriminative and
generative reasoning for IRL under noisy labels. To this end, we first
integrate generative reasoning into discriminative reasoning through a
supervised variational autoencoder. This allows GRID to automatically detect
training samples with noisy labels. Then, through our label noise robust hybrid
representation learning strategy, GRID adjusts the whole learning procedure for
IRL of these samples through generative reasoning and that of other samples
through discriminative reasoning. Our approach learns discriminative image
representations while preventing interference of noisy labels independently
from the IRL method being selected. Thus, unlike the existing methods, GRID
does not depend on the type of annotation, neural network architecture, loss
function or learning task, and thus can be directly utilized for various
problems. Experimental results show its effectiveness compared to
state-of-the-art methods. The code of GRID is publicly available at
https://github.com/gencersumbul/GRID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1&quot;&gt;Gencer Sumbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1&quot;&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13726">
<title>A Clustering-guided Contrastive Fusion for Multi-view Representation Learning. (arXiv:2212.13726v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13726</link>
<description rdf:parseType="Literal">&lt;p&gt;The past two decades have seen increasingly rapid advances in the field of
multi-view representation learning due to it extracting useful information from
diverse domains to facilitate the development of multi-view applications.
However, the community faces two challenges: i) how to learn robust
representations from a large amount of unlabeled data to against noise or
incomplete views setting, and ii) how to balance view consistency and
complementary for various downstream tasks. To this end, we utilize a deep
fusion network to fuse view-specific representations into the view-common
representation, extracting high-level semantics for obtaining robust
representation. In addition, we employ a clustering task to guide the fusion
network to prevent it from leading to trivial solutions. For balancing
consistency and complementary, then, we design an asymmetrical contrastive
strategy that aligns the view-common representation and each view-specific
representation. These modules are incorporated into a unified method known as
CLustering-guided cOntrastiVE fusioN (CLOVEN). We quantitatively and
qualitatively evaluate the proposed method on five datasets, demonstrating that
CLOVEN outperforms 11 competitive multi-view learning methods in clustering and
classification. In the incomplete view scenario, our proposed method resists
noise interference better than those of our competitors. Furthermore, the
visualization analysis shows that CLOVEN can preserve the intrinsic structure
of view-specific representation while also improving the compactness of
view-commom representation. Our source code will be available soon at
https://github.com/guanzhou-ke/cloven.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1&quot;&gt;Guanzhou Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08757">
<title>CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patients With Suspected Ischemic Stroke. (arXiv:2303.08757v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08757</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and fast prediction methods for ischemic areas comprised of dead
tissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)
patients are of significant clinical interest. They play an essential role in
improving diagnosis and treatment planning. Computed Tomography (CT) scan is
one of the primary modalities for early assessment in patients with suspected
AIS. CT Perfusion (CTP) is often used as a primary assessment to determine
stroke location, severity, and volume of ischemic lesions. Current automatic
segmentation methods for CTP mostly use already processed 3D parametric maps
conventionally used for clinical interpretation by radiologists as input.
Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time
input, where the spatial information over the volume is ignored. In addition,
these methods are only interested in segmenting core regions, while predicting
penumbra can be essential for treatment planning. This paper investigates
different methods to utilize the entire 4D CTP as input to fully exploit the
spatio-temporal information, leading us to propose a novel 4D convolution
layer. Our comprehensive experiments on a local dataset of 152 patients divided
into three groups show that our proposed models generate more precise results
than other methods explored. Adopting the proposed 4D mJ-Net, a Dice
Coefficient of 0.53 and 0.23 is achieved for segmenting penumbra and core
areas, respectively. The code is available on
https://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomasetti_L/0/1/0/all/0/1&quot;&gt;Luca Tomasetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1&quot;&gt;Kjersti Engan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollesli_L/0/1/0/all/0/1&quot;&gt;Liv Jorunn H&amp;#xf8;llesli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurz_K/0/1/0/all/0/1&quot;&gt;Kathinka D&amp;#xe6;hli Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanmohammadi_M/0/1/0/all/0/1&quot;&gt;Mahdieh Khanmohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11098">
<title>A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11098</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we revisit the efficacy of knowledge distillation as a function
matching and metric learning problem. In doing so we verify three important
design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector
implicitly encodes information on past examples, enabling relational gradients
for the student. We then show that the normalisation of representations is
tightly coupled with the training dynamics of this projector, which can have a
large impact on the students performance. Finally, we show that a simple soft
maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these
insights can lead to superior or comparable performance to state-of-the-art
knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification
(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult
distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Roy Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11591">
<title>SVCNet: Scribble-based Video Colorization Network with Temporal Aggregation. (arXiv:2303.11591v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11591</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a scribble-based video colorization network with
temporal aggregation called SVCNet. It can colorize monochrome videos based on
different user-given color scribbles. It addresses three common issues in the
scribble-based video colorization area: colorization vividness, temporal
consistency, and color bleeding. To improve the colorization quality and
strengthen the temporal consistency, we adopt two sequential sub-networks in
SVCNet for precise colorization and temporal smoothing, respectively. The first
stage includes a pyramid feature encoder to incorporate color scribbles with a
grayscale frame, and a semantic feature encoder to extract semantics. The
second stage finetunes the output from the first stage by aggregating the
information of neighboring colorized frames (as short-range connections) and
the first colorized frame (as a long-range connection). To alleviate the color
bleeding artifacts, we learn video colorization and segmentation
simultaneously. Furthermore, we set the majority of operations on a fixed small
image resolution and use a Super-resolution Module at the tail of SVCNet to
recover original sizes. It allows the SVCNet to fit different image resolutions
at the inference. Finally, we evaluate the proposed SVCNet on DAVIS and Videvo
benchmarks. The experimental results demonstrate that SVCNet produces both
higher-quality and more temporally consistent videos than other well-known
video colorization approaches. The codes and models can be found at
https://github.com/zhaoyuzhi/SVCNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1&quot;&gt;Lai-Man Po&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuehui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wing-Yin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_P/0/1/0/all/0/1&quot;&gt;Pengfei Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yujia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12745">
<title>Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning. (arXiv:2303.12745v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12745</link>
<description rdf:parseType="Literal">&lt;p&gt;Deception detection in conversations is a challenging yet important task,
having pivotal applications in many fields such as credibility assessment in
business, multimedia anti-frauds, and custom security. Despite this, deception
detection research is hindered by the lack of high-quality deception datasets,
as well as the difficulties of learning multimodal features effectively. To
address this issue, we introduce DOLOS\footnote {The name ``DOLOS&quot; comes from
Greek mythology.}, the largest gameshow deception detection dataset with rich
deceptive conversations. DOLOS includes 1,675 video clips featuring 213
subjects, and it has been labeled with audio-visual feature annotations. We
provide train-test, duration, and gender protocols to investigate the impact of
different factors. We benchmark our dataset on previously proposed deception
detection approaches. To further improve the performance by fine-tuning fewer
parameters, we propose Parameter-Efficient Crossmodal Learning (PECL), where a
Uniform Temporal Adapter (UT-Adapter) explores temporal attention in
transformer-based architectures, and a crossmodal fusion module, Plug-in
Audio-Visual Fusion (PAVF), combines crossmodal information from audio-visual
features. Based on the rich fine-grained audio-visual annotations on DOLOS, we
also exploit multi-task learning to enhance performance by concurrently
predicting deception and audio-visual features. Experimental results
demonstrate the desired quality of the DOLOS dataset and the effectiveness of
the PECL. The DOLOS dataset and the source codes are available at
https://github.com/NMS05/Audio-Visual-Deception-Detection-DOLOS-Dataset-and-Parameter-Efficient-Crossmodal-Learning/tree/main.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiaobao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selvaraj_N/0/1/0/all/0/1&quot;&gt;Nithish Muthuchamy Selvaraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zitong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_A/0/1/0/all/0/1&quot;&gt;Adams Wai-Kin Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bingquan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex Kot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15764">
<title>X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance. (arXiv:2303.15764v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15764</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-driven 3D stylization is a complex and crucial task in the fields of
computer vision (CV) and computer graphics (CG), aimed at transforming a bare
mesh to fit a target text. Prior methods adopt text-independent multilayer
perceptrons (MLPs) to predict the attributes of the target mesh with the
supervision of CLIP loss. However, such text-independent architecture lacks
textual guidance during predicting attributes, thus leading to unsatisfactory
stylization and slow convergence. To address these limitations, we present
X-Mesh, an innovative text-driven 3D stylization framework that incorporates a
novel Text-guided Dynamic Attention Module (TDAM). The TDAM dynamically
integrates the guidance of the target text by utilizing text-relevant spatial
and channel-wise attentions during vertex feature extraction, resulting in more
accurate attribute prediction and faster convergence speed. Furthermore,
existing works lack standard benchmarks and automated metrics for evaluation,
often relying on subjective and non-reproducible user studies to assess the
quality of stylized 3D assets. To overcome this limitation, we introduce a new
standard text-mesh benchmark, namely MIT-30, and two automated metrics, which
will enable future research to achieve fair and objective comparisons. Our
extensive qualitative and quantitative experiments demonstrate that X-Mesh
outperforms previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaioqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiayi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guannan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weilin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02978">
<title>Simplifying Low-Light Image Enhancement Networks with Relative Loss Functions. (arXiv:2304.02978v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02978</link>
<description rdf:parseType="Literal">&lt;p&gt;Image enhancement is a common technique used to mitigate issues such as
severe noise, low brightness, low contrast, and color deviation in low-light
images. However, providing an optimal high-light image as a reference for
low-light image enhancement tasks is impossible, which makes the learning
process more difficult than other image processing tasks. As a result, although
several low-light image enhancement methods have been proposed, most of them
are either too complex or insufficient in addressing all the issues in
low-light images. In this paper, to make the learning easier in low-light image
enhancement, we introduce FLW-Net (Fast and LightWeight Network) and two
relative loss functions. Specifically, we first recognize the challenges of the
need for a large receptive field to obtain global contrast and the lack of an
absolute reference, which limits the simplification of network structures in
this task. Then, we propose an efficient global feature information extraction
component and two loss functions based on relative information to overcome
these challenges. Finally, we conducted comparative experiments to demonstrate
the effectiveness of the proposed method, and the results confirm that the
proposed method can significantly reduce the complexity of supervised low-light
image enhancement networks while improving processing effect. The code is
available at \url{https://github.com/hitzhangyu/FLW-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junde Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Rao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guohui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunhui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04653">
<title>Do We Train on Test Data? The Impact of Near-Duplicates on License Plate Recognition. (arXiv:2304.04653v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04653</link>
<description rdf:parseType="Literal">&lt;p&gt;This work draws attention to the large fraction of near-duplicates in the
training and test sets of datasets widely adopted in License Plate Recognition
(LPR) research. These duplicates refer to images that, although different, show
the same license plate. Our experiments, conducted on the two most popular
datasets in the field, show a substantial decrease in recognition rate when six
well-known models are trained and tested under fair splits, that is, in the
absence of duplicates in the training and test sets. Moreover, in one of the
datasets, the ranking of models changed considerably when they were trained and
tested under duplicate-free splits. These findings suggest that such duplicates
have significantly biased the evaluation and development of deep learning-based
models for LPR. The list of near-duplicates we have found and proposals for
fair splits are publicly available for further research at
https://raysonlaroca.github.io/supp/lpr-train-on-test/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1&quot;&gt;Rayson Laroca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estevam_V/0/1/0/all/0/1&quot;&gt;Valter Estevam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1&quot;&gt;Alceu S. Britto Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1&quot;&gt;Rodrigo Minetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1&quot;&gt;David Menotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09670">
<title>CMID: A Unified Self-Supervised Learning Framework for Remote Sensing Image Understanding. (arXiv:2304.09670v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09670</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has gained widespread attention in the remote
sensing (RS) and earth observation (EO) communities owing to its ability to
learn task-agnostic representations without human-annotated labels.
Nevertheless, most existing RS SSL methods are limited to learning either
global semantic separable or local spatial perceptible representations. We
argue that this learning strategy is suboptimal in the realm of RS, since the
required representations for different RS downstream tasks are often varied and
complex. In this study, we proposed a unified SSL framework that is better
suited for RS images representation learning. The proposed SSL framework,
Contrastive Mask Image Distillation (CMID), is capable of learning
representations with both global semantic separability and local spatial
perceptibility by combining contrastive learning (CL) with masked image
modeling (MIM) in a self-distillation way. Furthermore, our CMID learning
framework is architecture-agnostic, which is compatible with both convolutional
neural networks (CNN) and vision transformers (ViT), allowing CMID to be easily
adapted to a variety of deep learning (DL) applications for RS understanding.
Comprehensive experiments have been carried out on four downstream tasks (i.e.
scene classification, semantic segmentation, object-detection, and change
detection) and the results show that models pre-trained using CMID achieve
better performance than other state-of-the-art SSL methods on multiple
downstream tasks. The code and pre-trained models will be made available at
https://github.com/NJU-LHRS/official-CMID to facilitate SSL research and speed
up the development of RS images DL applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhtar_D/0/1/0/all/0/1&quot;&gt;Dilxat Muhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Pengfeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenshi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1&quot;&gt;Feng Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10224">
<title>Multi-view Vision-Prompt Fusion Network: Can 2D Pre-trained Model Boost 3D Point Cloud Data-scarce Learning?. (arXiv:2304.10224v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10224</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud based 3D deep model has wide applications in many applications
such as autonomous driving, house robot, and so on. Inspired by the recent
prompt learning in natural language processing, this work proposes a novel
Multi-view Vision-Prompt Fusion Network (MvNet) for few-shot 3D point cloud
classification. MvNet investigates the possibility of leveraging the
off-the-shelf 2D pre-trained models to achieve the few-shot classification,
which can alleviate the over-dependence issue of the existing baseline models
towards the large-scale annotated 3D point cloud data. Specifically, MvNet
first encodes a 3D point cloud into multi-view image features for a number of
different views. Then, a novel multi-view prompt fusion module is developed to
effectively fuse information from different views to bridge the gap between 3D
point cloud data and 2D pre-trained models. A set of 2D image prompts can then
be derived to better describe the suitable prior knowledge for a large-scale
pre-trained image model for few-shot 3D point cloud classification. Extensive
experiments on ModelNet, ScanObjectNN, and ShapeNet datasets demonstrate that
MvNet achieves new state-of-the-art performance for 3D few-shot point cloud
image classification. The source code of this work will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Haoyang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baopu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01255">
<title>RT-K-Net: Revisiting K-Net for Real-Time Panoptic Segmentation. (arXiv:2305.01255v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01255</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic segmentation is one of the most challenging scene parsing tasks,
combining the tasks of semantic segmentation and instance segmentation. While
much progress has been made, few works focus on the real-time application of
panoptic segmentation methods. In this paper, we revisit the recently
introduced K-Net architecture. We propose vital changes to the architecture,
training, and inference procedure, which massively decrease latency and improve
performance. Our resulting RT-K-Net sets a new state-of-the-art performance for
real-time panoptic segmentation methods on the Cityscapes dataset and shows
promising results on the challenging Mapillary Vistas dataset. On Cityscapes,
RT-K-Net reaches 60.2 % PQ with an average inference time of 32 ms for full
resolution 1024x2048 pixel images on a single Titan RTX GPU. On Mapillary
Vistas, RT-K-Net reaches 33.2 % PQ with an average inference time of 69 ms.
Source code is available at https://github.com/markusschoen/RT-K-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_M/0/1/0/all/0/1&quot;&gt;Markus Sch&amp;#xf6;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchholz_M/0/1/0/all/0/1&quot;&gt;Michael Buchholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1&quot;&gt;Klaus Dietmayer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06710">
<title>Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06710</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifier-free guidance is an effective sampling technique in diffusion
models that has been widely adopted. The main idea is to extrapolate the model
in the direction of text guidance and away from null-text guidance. In this
paper, we demonstrate that null-text guidance in diffusion models is secretly a
cartoon-style creator, i.e., the generated images can be efficiently
transformed into cartoons by simply perturbing the null-text guidance.
Specifically, we proposed two disturbance methods, i.e., Rollback disturbance
(Back-D) and Image disturbance (Image-D), to construct misalignment between the
noisy images used for predicting null-text guidance and text guidance
(subsequently referred to as \textbf{null-text noisy image} and \textbf{text
noisy image} respectively) in the sampling process. Back-D achieves
cartoonization by altering the noise level of null-text noisy image via
replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces
high-fidelity, diverse cartoons by defining $x_t$ as a clean input image, which
further improves the incorporation of finer image details. Through
comprehensive experiments, we delved into the principle of noise disturbing for
null-text and uncovered that the efficacy of disturbance depends on the
correlation between the null-text noisy image and the source image. Moreover,
our proposed techniques, which can generate cartoon images and cartoonize
specific ones, are training-free and easily integrated as a plug-and-play
component in any classifier-free guided diffusion model. Project page is
available at \url{https://nulltextforcartoon.github.io/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Heliang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Long Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wanrong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenjing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12964">
<title>Text-based Person Search without Parallel Image-Text Data. (arXiv:2305.12964v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12964</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based person search (TBPS) aims to retrieve the images of the target
person from a large image gallery based on a given natural language
description. Existing methods are dominated by training models with parallel
image-text pairs, which are very costly to collect. In this paper, we make the
first attempt to explore TBPS without parallel image-text data ($\mu$-TBPS), in
which only non-parallel images and texts, or even image-only data, can be
adopted. Towards this end, we propose a two-stage framework,
generation-then-retrieval (GTR), to first generate the corresponding pseudo
text for each image and then perform the retrieval in a supervised manner. In
the generation stage, we propose a fine-grained image captioning strategy to
obtain an enriched description of the person image, which firstly utilizes a
set of instruction prompts to activate the off-the-shelf pretrained
vision-language model to capture and generate fine-grained person attributes,
and then converts the extracted attributes into a textual description via the
finetuned large language model or the hand-crafted template. In the retrieval
stage, considering the noise interference of the generated texts for training
model, we develop a confidence score-based training scheme by enabling more
reliable texts to contribute more during the training. Experimental results on
multiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show that
the proposed GTR can achieve a promising performance without relying on
parallel image-text data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Min Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15030">
<title>Jointly Optimizing Image Compression with Low-light Image Enhancement. (arXiv:2305.15030v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15030</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based image compression methods have made great progress. Most of
them are designed for generic natural images. In fact, low-light images
frequently occur due to unavoidable environmental influences or technical
limitations, such as insufficient lighting or limited exposure time. %When
general-purpose image compression algorithms compress low-light images, useful
detail information is lost, resulting in a dramatic decrease in image
enhancement. Once low-light images are compressed by existing general image
compression approaches, useful information(e.g., texture details) would be lost
resulting in a dramatic performance decrease in low-light image enhancement. To
simultaneously achieve a higher compression rate and better enhancement
performance for low-light images, we propose a novel image compression
framework with joint optimization of low-light image enhancement. We design an
end-to-end trainable two-branch architecture with lower computational cost,
which includes the main enhancement branch and the signal-to-noise ratio~(SNR)
aware branch. Experimental results show that our proposed joint optimization
framework achieves a significant improvement over existing ``Compress before
Enhance&quot; or ``Enhance before Compress&quot; sequential solutions for low-light
images. Source codes are included in the supplementary material.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shilv Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xu Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liqun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Luxin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Sheng Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00393">
<title>Teacher Agent: A Knowledge Distillation-Free Framework for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Rehearsal-based video incremental learning often employs knowledge
distillation to mitigate catastrophic forgetting of previously learned data.
However, this method faces two major challenges for video task: substantial
computing resources from loading teacher model and limited replay capability
from performance-limited teacher model. To address these problems, we first
propose a knowledge distillation-free framework for rehearsal-based video
incremental learning called \textit{Teacher Agent}. Instead of loading
parameter-heavy teacher networks, we introduce an agent generator that is
either parameter-free or uses only a few parameters to obtain accurate and
reliable soft labels. This method not only greatly reduces the computing
requirement but also circumvents the problem of knowledge misleading caused by
inaccurate predictions of the teacher model. Moreover, we put forward a
self-correction loss which provides an effective regularization signal for the
review of old knowledge, which in turn alleviates the problem of catastrophic
forgetting. Further, to ensure that the samples in the memory buffer are
memory-efficient and representative, we introduce a unified sampler for
rehearsal-based video incremental learning to mine fixed-length key video
frames. Interestingly, based on the proposed strategies, the network exhibits a
high level of robustness against spatial resolution reduction when compared to
the baseline. Extensive experiments demonstrate the advantages of our method,
yielding significant performance improvements while utilizing only half the
spatial resolution of video clips as network inputs in the incremental phases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shengqin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haokui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuankai Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04166">
<title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives. (arXiv:2306.04166v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04166</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural representation has emerged as a powerful method for
reconstructing 3D scenes from 2D images. Given a set of camera poses and
associated images, the models can be trained to synthesize novel, unseen views.
In order to expand the use cases for implicit neural representations, we need
to incorporate camera pose estimation capabilities as part of the
representation learning, as this is necessary for reconstructing scenes from
real-world video sequences where cameras are generally not being tracked.
Existing approaches like COLMAP and, most recently, bundle-adjusting neural
radiance field methods often suffer from lengthy processing times. These delays
ranging from hours to days, arise from laborious feature matching, hardware
limitations, dense point sampling, and long training times required by a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP). Our approach leverages accelerated
sampling and hash encoding to expedite both pose refinement/estimation and 3D
scene reconstruction. Experimental results demonstrate that our method achieves
a more than 10 to 20 $\times$ speed improvement in novel view synthesis
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sainan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jingpei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Shreya Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Supikov_A/0/1/0/all/0/1&quot;&gt;Alexey Supikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1&quot;&gt;Michael Yip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12676">
<title>Damage Vision Mining Opportunity for Imbalanced Anomaly Detection. (arXiv:2307.12676v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12676</link>
<description rdf:parseType="Literal">&lt;p&gt;In past decade, previous balanced datasets have been used to advance
algorithms for classification, object detection, semantic segmentation, and
anomaly detection in industrial applications. Specifically, for condition-based
maintenance, automating visual inspection is crucial to ensure high quality.
Deterioration prognostic attempts to optimize the fine decision process for
predictive maintenance and proactive repair. In civil infrastructure and living
environment, damage data mining cannot avoid the imbalanced data issue because
of rare unseen events and high quality status by improved operations. For
visual inspection, deteriorated class acquired from the surface of concrete and
steel components are occasionally imbalanced. From numerous related surveys, we
summarize that imbalanced data problems can be categorized into four types; 1)
missing range of target and label valuables, 2) majority-minority class
imbalance, 3) foreground-background of spatial imbalance, 4) long-tailed class
of pixel-wise imbalance. Since 2015, there has been many imbalanced studies
using deep learning approaches that includes regression, image classification,
object detection, semantic segmentation. However, anomaly detection for
imbalanced data is not yet well known. In the study, we highlight one-class
anomaly detection application whether anomalous class or not, and demonstrate
clear examples on imbalanced vision datasets: blood smear, lung infection,
wooden, concrete deterioration, and disaster damage. We provide key results on
damage vision mining advantage, hypothesizing that the more effective range of
positive ratio, the higher accuracy gain of anomaly detection application.
Finally, the applicability of the damage learning methods, limitations, and
future works are mentioned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasuno_T/0/1/0/all/0/1&quot;&gt;Takato Yasuno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15539">
<title>Beating Backdoor Attack at Its Own Game. (arXiv:2307.15539v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15539</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not
affect the network&apos;s performance on clean data but would manipulate the network
behavior once a trigger pattern is added. Existing defense methods have greatly
reduced attack success rate, but their prediction accuracy on clean data still
lags behind a clean model by a large margin. Inspired by the stealthiness and
effectiveness of backdoor attack, we propose a simple but highly effective
defense framework which injects non-adversarial backdoors targeting poisoned
samples. Following the general steps in backdoor attack, we detect a small set
of suspected samples and then apply a poisoning strategy to them. The
non-adversarial backdoor, once triggered, suppresses the attacker&apos;s backdoor on
poisoned data, but has limited influence on clean data. The defense can be
carried out during data preprocessing, without any modification to the standard
end-to-end training pipeline. We conduct extensive experiments on multiple
benchmarks with different architectures and representative attacks. Results
demonstrate that our method achieves state-of-the-art defense effectiveness
with by far the lowest performance drop on clean data. Considering the
surprising defense ability displayed by our framework, we call for more
attention to utilizing backdoor for backdoor defense. Code is available at
https://github.com/damianliumin/non-adversarial_backdoor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangiovanni_Vincentelli_A/0/1/0/all/0/1&quot;&gt;Alberto Sangiovanni-Vincentelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01119">
<title>Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01119</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image classification models are frequently trained using training
datasets derived from multiple data sources. While leveraging multiple data
sources is crucial for achieving model generalization, it is important to
acknowledge that the diverse nature of these sources inherently introduces
unintended confounders and other challenges that can impact both model accuracy
and transparency. A notable confounding factor in medical image classification,
particularly in musculoskeletal image classification, is skeletal
maturation-induced bone growth observed during adolescence. We train a deep
learning model using a Covid-19 chest X-ray dataset and we showcase how this
dataset can lead to spurious correlations due to unintended confounding
regions. eXplanation Based Learning (XBL) is a deep learning approach that goes
beyond interpretability by utilizing model explanations to interactively
unlearn spurious correlations. This is achieved by integrating interactive user
feedback, specifically feature annotations. In our study, we employed two
non-demanding manual feedback mechanisms to implement an XBL-based approach for
effectively eliminating these spurious correlations. Our results underscore the
promising potential of XBL in constructing robust models even in the presence
of confounding factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01236">
<title>Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01236</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Grounded Image Text Matching with Mismatched Relation
(GITM-MR), a novel visual-linguistic joint task that evaluates the relation
understanding capabilities of transformer-based pre-trained models. GITM-MR
requires a model to first determine if an expression describes an image, then
localize referred objects or ground the mismatched parts of the text. We
provide a benchmark for evaluating pre-trained models on this task, with a
focus on the challenging settings of limited data and out-of-distribution
sentence lengths. Our evaluation demonstrates that pre-trained models lack data
efficiency and length generalization ability. To address this, we propose the
Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates
relation-aware reasoning via bi-directional message propagation guided by
language structure. RCRN can be interpreted as a modular program and delivers
strong performance in both length generalization and data efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yana Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01568">
<title>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior. (arXiv:2308.01568v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01568</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, many deep learning-based methods have been proposed to
tackle the problem of optical flow estimation and achieved promising results.
However, they hardly consider that most videos are compressed and thus ignore
the pre-computed information in compressed video streams. Motion vectors, one
of the compression information, record the motion of the video frames. They can
be directly extracted from the compression code stream without computational
cost and serve as a solid prior for optical flow estimation. Therefore, we
propose an optical flow model, MVFlow, which uses motion vectors to improve the
speed and accuracy of optical flow estimation for compressed videos. In detail,
MVFlow includes a key Motion-Vector Converting Module, which ensures that the
motion vectors can be transformed into the same domain of optical flow and then
be utilized fully by the flow estimation module. Meanwhile, we construct four
optical flow datasets for compressed videos containing frames and motion
vectors in pairs. The experimental results demonstrate the superiority of our
proposed MVFlow, which can reduce the AEPE by 1.09 compared to existing models
or save 52% time to achieve similar accuracy to existing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01634">
<title>Disentangling Multi-view Representations Beyond Inductive Bias. (arXiv:2308.01634v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01634</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view (or -modality) representation learning aims to understand the
relationships between different view representations. Existing methods
disentangle multi-view representations into consistent and view-specific
representations by introducing strong inductive biases, which can limit their
generalization ability. In this paper, we propose a novel multi-view
representation disentangling method that aims to go beyond inductive biases,
ensuring both interpretability and generalizability of the resulting
representations. Our method is based on the observation that discovering
multi-view consistency in advance can determine the disentangling information
boundary, leading to a decoupled learning objective. We also found that the
consistency can be easily extracted by maximizing the transformation invariance
and clustering consistency between views. These observations drive us to
propose a two-stage framework. In the first stage, we obtain multi-view
consistency by training a consistent encoder to produce semantically-consistent
representations across views as well as their corresponding pseudo-labels. In
the second stage, we disentangle specificity from comprehensive representations
by minimizing the upper bound of mutual information between consistent and
comprehensive representations. Finally, we reconstruct the original data by
concatenating pseudo-labels and view-specific representations. Our experiments
on four multi-view datasets demonstrate that our proposed method outperforms 12
comparison methods in terms of clustering and classification performance. The
visualization results also show that the extracted consistency and specificity
are compact and interpretable. Our code can be found at
\url{https://github.com/Guanzhou-Ke/DMRIB}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1&quot;&gt;Guanzhou Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shengfeng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01661">
<title>BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout. (arXiv:2308.01661v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01661</link>
<description rdf:parseType="Literal">&lt;p&gt;Using synthesized images to boost the performance of perception models is a
long-standing research challenge in computer vision. It becomes more eminent in
visual-centric autonomous driving systems with multi-view cameras as some
long-tail scenarios can never be collected. Guided by the BEV segmentation
layouts, the existing generative networks seem to synthesize photo-realistic
street-view images when evaluated solely on scene-level metrics. However, once
zoom-in, they usually fail to produce accurate foreground and background
details such as heading. To this end, we propose a two-stage generative method,
dubbed BEVControl, that can generate accurate foreground and background
contents. In contrast to segmentation-like input, it also supports sketch style
input, which is more flexible for humans to edit. In addition, we propose a
comprehensive multi-level evaluation protocol to fairly compare the quality of
the generated scene, foreground object, and background geometry. Our extensive
experiments show that our BEVControl surpasses the state-of-the-art method,
BEVGen, by a significant margin, from 5.89 to 26.80 on foreground segmentation
mIoU. In addition, we show that using images generated by BEVControl to train
the downstream perception model, it achieves on average 1.29 improvement in NDS
score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kairui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_E/0/1/0/all/0/1&quot;&gt;Enhui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jibin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Di Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01854">
<title>Reconstructing Three-Dimensional Models of Interacting Humans. (arXiv:2308.01854v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01854</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding 3d human interactions is fundamental for fine-grained scene
analysis and behavioural modeling. However, most of the existing models predict
incorrect, lifeless 3d estimates, that miss the subtle human contact
aspects--the essence of the event--and are of little use for detailed
behavioral understanding. This paper addresses such issues with several
contributions: (1) we introduce models for interaction signature estimation
(ISP) encompassing contact detection, segmentation, and 3d contact signature
prediction; (2) we show how such components can be leveraged to ensure contact
consistency during 3d reconstruction; (3) we construct several large datasets
for learning and evaluating 3d contact prediction and reconstruction methods;
specifically, we introduce CHI3D, a lab-based accurate 3d motion capture
dataset with 631 sequences containing $2,525$ contact events, $728,664$ ground
truth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with
$14,081$ processed pairs of people, and $81,233$ facet-level surface
correspondences. Finally, (4) we propose methodology for recovering the
ground-truth pose and shape of interacting people in a controlled setup and (5)
annotate all 3d interaction motions in CHI3D with textual descriptions. Motion
data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is
made available for research purposes at \url{https://ci3d.imar.ro}, together
with an evaluation server and a public benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fieraru_M/0/1/0/all/0/1&quot;&gt;Mihai Fieraru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1&quot;&gt;Mihai Zanfir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oneata_E/0/1/0/all/0/1&quot;&gt;Elisabeta Oneata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popa_A/0/1/0/all/0/1&quot;&gt;Alin-Ionut Popa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olaru_V/0/1/0/all/0/1&quot;&gt;Vlad Olaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1&quot;&gt;Cristian Sminchisescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01867">
<title>MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01867</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,
DPU), deploying deep learning models on edge devices with fixed-point hardware
is still challenging due to complex model quantization and conversion. Existing
model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and
Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,
only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep
learning models cannot be easily quantized for diverse fixed-point hardwares,
mainly due to slightly different quantization requirements. In this paper, we
envision a new type of model quantization approach called MRQ (model
re-quantization), which takes existing quantized models and quickly transforms
the models to meet different quantization requirements (e.g., asymmetric -&amp;gt;
symmetric, non-power-of-2 scale -&amp;gt; power-of-2 scale). Re-quantization is much
simpler than quantizing from scratch because it avoids costly re-training and
provides support for multiple quantization schemes simultaneously. To minimize
re-quantization error, we developed a new set of re-quantization algorithms
including weight correction and rounding error folding. We have demonstrated
that MobileNetV2 QAT model [7] can be quickly re-quantized into two different
quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less
than 0.64 units of accuracy loss. We believe our work is the first to leverage
this concept of re-quantization for model quantization and models obtained from
the re-quantization process have been successfully deployed on NNA in the Echo
Show devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manohara_M/0/1/0/all/0/1&quot;&gt;Manasa Manohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayal_S/0/1/0/all/0/1&quot;&gt;Sankalp Dayal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afzal_T/0/1/0/all/0/1&quot;&gt;Tariq Afzal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1&quot;&gt;Rahul Bakshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kahkuen Fu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>