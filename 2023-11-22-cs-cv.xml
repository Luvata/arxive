<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.04514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.05582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.00851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.08717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.11851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.04658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.13677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.12699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10543" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.10754">
<title>A Recent Survey of the Advancements in Deep Learning Techniques for Monkeypox Disease Detection. (arXiv:2311.10754v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10754</link>
<description rdf:parseType="Literal">&lt;p&gt;Monkeypox is a zoonotic infectious disease induced by the Monkeypox virus,
part of the poxviridae orthopoxvirus group initially discovered in Africa and
gained global attention in mid-2022 with cases reported outside endemic areas.
Symptoms include headaches, chills, fever, smallpox, measles, and
chickenpox-like skin manifestations and the WHO officially announced monkeypox
as a global public health pandemic, in July-2022. Timely diagnosis is
imperative for assessing disease severity, conducting clinical evaluations, and
determining suitable treatment plans. Traditionally, PCR testing of skin
lesions is considered a benchmark for the primary diagnosis by WHO, with
symptom management as the primary treatment and antiviral drugs like
tecovirimat for severe cases. However, manual analysis within hospitals poses a
substantial challenge during public health emergencies, particularly in the
case of epidemics and pandemics. Therefore, this survey paper provides an
extensive and efficient analysis of deep learning (DL) methods for the
automatic detection of MP in skin lesion images. These DL techniques are
broadly grouped into categories, including deep CNN, Deep CNNs ensemble, deep
hybrid learning, the newly developed, and Vision transformer for diagnosing MP.
Additionally, the paper addresses benchmark datasets and their collection from
various authentic sources, pre-processing techniques, and evaluation metrics.
The survey also briefly delves into emerging concepts, identifies research
gaps, limitations, and applications, and outlines challenges in the diagnosis
process. This survey furnishes valuable insights into the prospective areas of
DL study and is anticipated to serve as a path for researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Saddam Hussain Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iqbal_R/0/1/0/all/0/1&quot;&gt;Rashid Iqbal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naz_S/0/1/0/all/0/1&quot;&gt;Saeeda Naz&lt;/a&gt; (Artifical Intelligence Lab, Department of Computer Systems Engineering, University of Engineering and Applied Science (UEAS), Swat, Pakistan)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10787">
<title>Assurance for Deployed Continual Learning Systems. (arXiv:2311.10787v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10787</link>
<description rdf:parseType="Literal">&lt;p&gt;The future success of the Navy will depend, in part, on artificial
intelligence. In practice, many artificially intelligent algorithms, and in
particular deep learning models, rely on continual learning to maintain
performance in dynamic environments. The software requires adaptation to
maintain its initial level of performance in unseen situations. However, if not
monitored properly, continual learning may lead to several issues including
catastrophic forgetting in which a trained model forgets previously learned
tasks when being retrained on new data. The authors created a new framework for
safely performing continual learning with the goal of pairing this safety
framework with a deep learning computer vision algorithm to allow for safe and
high-performing automatic deck tracking on carriers and amphibious assault
ships. The safety framework includes several features, such as an ensemble of
convolutional neural networks to perform image classification, a manager to
record confidences and determine the best answer from the ensemble, a model of
the environment to predict when the system may fail to meet minimum performance
metrics, a performance monitor to log system and domain performance and check
against requirements, and a retraining component to update the ensemble and
manager to maintain performance. The authors validated the proposed method
using extensive simulation studies based on dynamic image classification. The
authors showed the safety framework could probabilistically detect out of
distribution data. The results also show the framework can detect when the
system is no longer performing safely and can significantly extend the working
envelope of an image classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_A/0/1/0/all/0/1&quot;&gt;Ari Goodman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OShea_R/0/1/0/all/0/1&quot;&gt;Ryan O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirschorn_N/0/1/0/all/0/1&quot;&gt;Noam Hirschorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrostowski_H/0/1/0/all/0/1&quot;&gt;Hubert Chrostowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10788">
<title>Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors. (arXiv:2311.10788v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10788</link>
<description rdf:parseType="Literal">&lt;p&gt;Video DeepFakes are fake media created with Deep Learning (DL) that
manipulate a person&apos;s expression or identity. Most current DeepFake detection
methods analyze each frame independently, ignoring inconsistencies and
unnatural movements between frames. Some newer methods employ optical flow
models to capture this temporal aspect, but they are computationally expensive.
In contrast, we propose using the related but often ignored Motion Vectors
(MVs) and Information Masks (IMs) from the H.264 video codec, to detect
temporal inconsistencies in DeepFakes. Our experiments show that this approach
is effective and has minimal computational costs, compared with per-frame
RGB-only methods. This could lead to new, real-time temporally-aware DeepFake
detection methods for video calls and streaming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gronquist_P/0/1/0/all/0/1&quot;&gt;Peter Gr&amp;#xf6;nquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yufan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qingyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verardo_A/0/1/0/all/0/1&quot;&gt;Alessio Verardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1&quot;&gt;Sabine S&amp;#xfc;sstrunk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10793">
<title>Traffic Sign Interpretation in Real Road Scene. (arXiv:2311.10793v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10793</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing traffic sign-related works are dedicated to detecting and
recognizing part of traffic signs individually, which fails to analyze the
global semantic logic among signs and may convey inaccurate traffic
instruction. Following the above issues, we propose a traffic sign
interpretation (TSI) task, which aims to interpret global semantic interrelated
traffic signs (e.g.,~driving instruction-related texts, symbols, and guide
panels) into a natural language for providing accurate instruction support to
autonomous or assistant driving. Meanwhile, we design a multi-task learning
architecture for TSI, which is responsible for detecting and recognizing
various traffic signs and interpreting them into a natural language like a
human. Furthermore, the absence of a public TSI available dataset prompts us to
build a traffic sign interpretation dataset, namely TSI-CN. The dataset
consists of real road scene images, which are captured from the highway and the
urban way in China from a driver&apos;s perspective. It contains rich location
labels of texts, symbols, and guide panels, and the corresponding natural
language description labels. Experiments on TSI-CN demonstrate that the TSI
task is achievable and the TSI architecture can interpret traffic signs from
scenes successfully even if there is a complex semantic logic among signs. The
TSI-CN dataset and the source code of the TSI architecture will be publicly
available after the revision process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chuang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_K/0/1/0/all/0/1&quot;&gt;Kai Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mulin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haozhao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Changxing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Han Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingxuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10794">
<title>Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression. (arXiv:2311.10794v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10794</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Style Tailoring, a recipe to finetune Latent Diffusion Models
(LDMs) in a distinct domain with high visual quality, prompt alignment and
scene diversity. We choose sticker image generation as the target domain, as
the images significantly differ from photorealistic samples typically generated
by large-scale LDMs. We start with a competent text-to-image model, like Emu,
and show that relying on prompt engineering with a photorealistic model to
generate stickers leads to poor prompt alignment and scene diversity. To
overcome these drawbacks, we first finetune Emu on millions of sticker-like
images collected using weak supervision to elicit diversity. Next, we curate
human-in-the-loop (HITL) Alignment and Style datasets from model generations,
and finetune to improve prompt alignment and style alignment respectively.
Sequential finetuning on these datasets poses a tradeoff between better style
alignment and prompt alignment gains. To address this tradeoff, we propose a
novel fine-tuning method called Style Tailoring, which jointly fits the content
and style distribution and achieves best tradeoff. Evaluation results show our
method improves visual quality by 14%, prompt alignment by 16.2% and scene
diversity by 15.3%, compared to prompt engineering the base Emu model for
stickers generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Animesh Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalia_A/0/1/0/all/0/1&quot;&gt;Anmol Kalia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casanova_A/0/1/0/all/0/1&quot;&gt;Arantxa Casanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanchard_E/0/1/0/all/0/1&quot;&gt;Elliot Blanchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;David Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Winnie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nelli_T/0/1/0/all/0/1&quot;&gt;Tony Nelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1&quot;&gt;Hardik Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Licheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mitesh Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramchandani_A/0/1/0/all/0/1&quot;&gt;Ankit Ramchandani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1&quot;&gt;Maziar Sanjabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sonal Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bearman_A/0/1/0/all/0/1&quot;&gt;Amy Bearman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1&quot;&gt;Dhruv Mahajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10796">
<title>Emotion-Aware Music Recommendation System: Enhancing User Experience Through Real-Time Emotional Context. (arXiv:2311.10796v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.10796</link>
<description rdf:parseType="Literal">&lt;p&gt;This study addresses the deficiency in conventional music recommendation
systems by focusing on the vital role of emotions in shaping users music
choices. These systems often disregard the emotional context, relying
predominantly on past listening behavior and failing to consider the dynamic
and evolving nature of users emotional preferences. This gap leads to several
limitations. Users may receive recommendations that do not match their current
mood, which diminishes the quality of their music experience. Furthermore,
without accounting for emotions, the systems might overlook undiscovered or
lesser-known songs that have a profound emotional impact on users. To combat
these limitations, this research introduces an AI model that incorporates
emotional context into the song recommendation process. By accurately detecting
users real-time emotions, the model can generate personalized song
recommendations that align with the users emotional state. This approach aims
to enhance the user experience by offering music that resonates with their
current mood, elicits the desired emotions, and creates a more immersive and
meaningful listening experience. By considering emotional context in the song
recommendation process, the proposed model offers an opportunity for a more
personalized and emotionally resonant musical journey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_T/0/1/0/all/0/1&quot;&gt;Tina Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1&quot;&gt;Rekha R Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+A_G/0/1/0/all/0/1&quot;&gt;Geetha A&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10798">
<title>INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis. (arXiv:2311.10798v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10798</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing information from multiple data sources plays a crucial role in
the practice of modern medicine. Current applications of artificial
intelligence in medicine often focus on single-modality data due to a lack of
publicly available, multimodal medical datasets. To address this limitation, we
introduce INSPECT, which contains de-identified longitudinal records from a
large cohort of patients at risk for pulmonary embolism (PE), along with ground
truth labels for multiple outcomes. INSPECT contains data from 19,402 patients,
including CT images, radiology report impression sections, and structured
electronic health record (EHR) data (i.e. demographics, diagnoses, procedures,
vitals, and medications). Using INSPECT, we develop and release a benchmark for
evaluating several baseline modeling approaches on a variety of important PE
related tasks. We evaluate image-only, EHR-only, and multimodal fusion models.
Trained models and the de-identified dataset are made available for
non-commercial use under a data use agreement. To the best of our knowledge,
INSPECT is the largest multimodal dataset integrating 3D medical imaging and
EHR for reproducible methods evaluation and research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shih-Cheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zepeng Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1&quot;&gt;Ethan Steinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1&quot;&gt;Chia-Chun Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Serena Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1&quot;&gt;Jason A. Fries&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10807">
<title>SENetV2: Aggregated dense layer for channelwise and global representations. (arXiv:2311.10807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10807</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have revolutionized image classification
by extracting spatial features and enabling state-of-the-art accuracy in
vision-based tasks. The squeeze and excitation network proposed module gathers
channelwise representations of the input. Multilayer perceptrons (MLP) learn
global representation from the data and in most image classification models
used to learn extracted features of the image. In this paper, we introduce a
novel aggregated multilayer perceptron, a multi-branch dense layer, within the
Squeeze excitation residual module designed to surpass the performance of
existing architectures. Our approach leverages a combination of squeeze
excitation network module with dense layers. This fusion enhances the network&apos;s
ability to capture channel-wise patterns and have global knowledge, leading to
a better feature representation. This proposed model has a negligible increase
in parameters when compared to SENet. We conduct extensive experiments on
benchmark datasets to validate the model and compare them with established
architectures. Experimental results demonstrate a remarkable increase in the
classification accuracy of the proposed model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_M/0/1/0/all/0/1&quot;&gt;Mahendran Narayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10812">
<title>SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos. (arXiv:2311.10812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10812</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose SplatArmor, a novel approach for recovering detailed and
animatable human models by `armoring&apos; a parameterized body model with 3D
Gaussians. Our approach represents the human as a set of 3D Gaussians within a
canonical space, whose articulation is defined by extending the skinning of the
underlying SMPL geometry to arbitrary locations in the canonical space. To
account for pose-dependent effects, we introduce a SE(3) field, which allows us
to capture both the location and anisotropy of the Gaussians. Furthermore, we
propose the use of a neural color field to provide color regularization and 3D
supervision for the precise positioning of these Gaussians. We show that
Gaussian splatting provides an interesting alternative to neural rendering
based methods by leverging a rasterization primitive without facing any of the
non-differentiability and optimization challenges typically faced in such
approaches. The rasterization paradigms allows us to leverage forward skinning,
and does not suffer from the ambiguities associated with inverse skinning and
warping. We show compelling results on the ZJU MoCap and People Snapshot
datasets, which underscore the effectiveness of our method for controllable
human synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jena_R/0/1/0/all/0/1&quot;&gt;Rohit Jena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_G/0/1/0/all/0/1&quot;&gt;Ganesh Subramanian Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1&quot;&gt;Siddharth Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1&quot;&gt;Brandon Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James Gee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10813">
<title>A Language Agent for Autonomous Driving. (arXiv:2311.10813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Project page:
\href{https://github.com/USC-GVL/Agent-Driver/blob/main/index.html}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiageng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuxi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10845">
<title>Domain Generalization of 3D Object Detection by Density-Resampling. (arXiv:2311.10845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10845</link>
<description rdf:parseType="Literal">&lt;p&gt;Point-cloud-based 3D object detection suffers from performance degradation
when encountering data with novel domain gaps. To tackle it, the single-domain
generalization (SDG) aims to generalize the detection model trained in a
limited single source domain to perform robustly on unexplored domains. In this
paper, we propose an SDG method to improve the generalizability of 3D object
detection to unseen target domains. Unlike prior SDG works for 3D object
detection solely focusing on data augmentation, our work introduces a novel
data augmentation method and contributes a new multi-task learning strategy in
the methodology. Specifically, from the perspective of data augmentation, we
design a universal physical-aware density-based data augmentation (PDDA) method
to mitigate the performance loss stemming from diverse point densities. From
the learning methodology viewpoint, we develop a multi-task learning for 3D
object detection: during source training, besides the main standard detection
task, we leverage an auxiliary self-supervised 3D scene restoration task to
enhance the comprehension of the encoder on background and foreground details
for better recognition and detection of objects. Furthermore, based on the
auxiliary self-supervised task, we propose the first test-time adaptation
method for domain generalization of 3D object detection, which efficiently
adjusts the encoder&apos;s parameters to adapt to unseen target domains during
testing time, to further bridge domain gaps. Extensive cross-dataset
experiments covering &quot;Car&quot;, &quot;Pedestrian&quot;, and &quot;Cyclist&quot; detections, demonstrate
our method outperforms state-of-the-art SDG methods and even overpass
unsupervised domain adaptation methods under some circumstances. The code will
be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xingyu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10857">
<title>WATUNet: A Deep Neural Network for Segmentation of Volumetric Sweep Imaging Ultrasound. (arXiv:2311.10857v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10857</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective. Limited access to breast cancer diagnosis globally leads to
delayed treatment. Ultrasound, an effective yet underutilized method, requires
specialized training for sonographers, which hinders its widespread use.
Approach. Volume sweep imaging (VSI) is an innovative approach that enables
untrained operators to capture high-quality ultrasound images. Combined with
deep learning, like convolutional neural networks (CNNs), it can potentially
transform breast cancer diagnosis, enhancing accuracy, saving time and costs,
and improving patient outcomes. The widely used UNet architecture, known for
medical image segmentation, has limitations, such as vanishing gradients and a
lack of multi-scale feature extraction and selective region attention. In this
study, we present a novel segmentation model known as Wavelet_Attention_UNet
(WATUNet). In this model, we incorporate wavelet gates (WGs) and attention
gates (AGs) between the encoder and decoder instead of a simple connection to
overcome the limitations mentioned, thereby improving model performance. Main
results. Two datasets are utilized for the analysis. The public &quot;Breast
Ultrasound Images&quot; (BUSI) dataset of 780 images and a VSI dataset of 3818
images. Both datasets contained segmented lesions categorized into three types:
no mass, benign mass, and malignant mass. Our segmentation results show
superior performance compared to other deep networks. The proposed algorithm
attained a Dice coefficient of 0.94 and an F1 score of 0.94 on the VSI dataset
and scored 0.93 and 0.94 on the public dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1&quot;&gt;Donya Khaledyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marini_T/0/1/0/all/0/1&quot;&gt;Thomas J. Marini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+OConnell_A/0/1/0/all/0/1&quot;&gt;Avice OConnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_S/0/1/0/all/0/1&quot;&gt;Steven Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kan_J/0/1/0/all/0/1&quot;&gt;Jonah Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brennan_G/0/1/0/all/0/1&quot;&gt;Galen Brennan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baran_T/0/1/0/all/0/1&quot;&gt;Timothy M.Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parker_K/0/1/0/all/0/1&quot;&gt;Kevin J. Parker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10865">
<title>Zero-Shot Digital Rock Image Segmentation with a Fine-Tuned Segment Anything Model. (arXiv:2311.10865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10865</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate image segmentation is crucial in reservoir modelling and material
characterization, enhancing oil and gas extraction efficiency through detailed
reservoir models. This precision offers insights into rock properties,
advancing digital rock physics understanding. However, creating pixel-level
annotations for complex CT and SEM rock images is challenging due to their size
and low contrast, lengthening analysis time. This has spurred interest in
advanced semi-supervised and unsupervised segmentation techniques in digital
rock image analysis, promising more efficient, accurate, and less
labour-intensive methods. Meta AI&apos;s Segment Anything Model (SAM) revolutionized
image segmentation in 2023, offering interactive and automated segmentation
with zero-shot capabilities, essential for digital rock physics with limited
training data and complex image features. Despite its advanced features, SAM
struggles with rock CT/SEM images due to their absence in its training set and
the low-contrast nature of grayscale images. Our research fine-tunes SAM for
rock CT/SEM image segmentation, optimizing parameters and handling large-scale
images to improve accuracy. Experiments on rock CT and SEM images show that
fine-tuning significantly enhances SAM&apos;s performance, enabling high-quality
mask generation in digital rock image analysis. Our results demonstrate the
feasibility and effectiveness of the fine-tuned SAM model (RockSAM) for rock
images, offering segmentation without extensive training or complex labelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xupeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1&quot;&gt;Hyung Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10873">
<title>Multi-entity Video Transformers for Fine-Grained Video Representation Learning. (arXiv:2311.10873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10873</link>
<description rdf:parseType="Literal">&lt;p&gt;The area of temporally fine-grained video representation learning aims to
generate frame-by-frame representations for temporally dense tasks. In this
work, we advance the state-of-the-art for this area by re-examining the design
of transformer architectures for video representation learning. A salient
aspect of our self-supervised method is the improved integration of spatial
information in the temporal pipeline by representing multiple entities per
frame. Prior works use late fusion architectures that reduce frames to a single
dimensional vector before any cross-frame information is shared, while our
method represents each frame as a group of entities or tokens. Our Multi-entity
Video Transformer (MV-Former) architecture achieves state-of-the-art results on
multiple fine-grained video benchmarks. MV-Former leverages image features from
self-supervised ViTs, and employs several strategies to maximize the utility of
the extracted features while also avoiding the need to fine-tune the complex
ViT backbone. This includes a Learnable Spatial Token Pooling strategy, which
is used to identify and extract features for multiple salient regions per
frame. Our experiments show that MV-Former not only outperforms previous
self-supervised methods, but also surpasses some prior works that use
additional supervision or training data. When combined with additional
pre-training data from Kinetics-400, MV-Former achieves a further performance
boost. The code for MV-Former is available at
https://github.com/facebookresearch/video_rep_learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walmer_M/0/1/0/all/0/1&quot;&gt;Matthew Walmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanjirathinkal_R/0/1/0/all/0/1&quot;&gt;Rose Kanjirathinkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_K/0/1/0/all/0/1&quot;&gt;Kai Sheng Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muzumdar_K/0/1/0/all/0/1&quot;&gt;Keyur Muzumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_T/0/1/0/all/0/1&quot;&gt;Taipeng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Abhinav Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10879">
<title>Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation. (arXiv:2311.10879v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10879</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite its benefits for tumour detection and treatment, the administration
of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated
with a range of issues, including their invasiveness, bioaccumulation, and a
risk of nephrogenic systemic fibrosis. This study explores the feasibility of
producing synthetic contrast enhancements by translating pre-contrast
T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI
sequence leveraging the capabilities of a generative adversarial network (GAN).
Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for
quantitatively evaluating the quality of synthetic data in a principled manner
and serving as a basis for selecting the optimal generative model. We assess
the generated DCE-MRI data using quantitative image quality metrics and apply
them to the downstream task of 3D breast tumour segmentation. Our results
highlight the potential of post-contrast DCE-MRI synthesis in enhancing the
robustness of breast tumour segmentation models via data augmentation. Our code
is available at https://github.com/RichardObi/pre_post_synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1&quot;&gt;Richard Osuala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Smriti Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsirikoglou_A/0/1/0/all/0/1&quot;&gt;Apostolia Tsirikoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1&quot;&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pinaya_W/0/1/0/all/0/1&quot;&gt;Walter H. L. Pinaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1&quot;&gt;Oliver Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1&quot;&gt;Karim Lekadir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10883">
<title>Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models. (arXiv:2311.10883v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10883</link>
<description rdf:parseType="Literal">&lt;p&gt;The image annotation stage is a critical and often the most time-consuming
part required for training and evaluating object detection and semantic
segmentation models. Deployment of the existing models in novel environments
often requires detecting novel semantic classes not present in the training
data. Furthermore, indoor scenes contain significant viewpoint variations,
which need to be handled properly by trained perception models. We propose to
leverage the recent advancements in state-of-the-art models for bottom-up
segmentation (SAM), object detection (Detic), and semantic segmentation
(MaskFormer), all trained on large-scale datasets. We aim to develop a
cost-effective labeling approach to obtain pseudo-labels for semantic
segmentation and object instance detection in indoor environments, with the
ultimate goal of facilitating the training of lightweight models for various
downstream tasks. We also propose a multi-view labeling fusion stage, which
considers the setting where multiple views of the scenes are available and can
be used to identify and rectify single-view inconsistencies. We demonstrate the
effectiveness of the proposed approach on the Active Vision dataset and the
ADE20K dataset. We evaluate the quality of our labeling process by comparing it
with human annotations. Also, we demonstrate the effectiveness of the obtained
labels in downstream tasks such as object goal navigation and part discovery.
In the context of object goal navigation, we depict enhanced performance using
this fusion approach compared to a zero-shot baseline that utilizes large
monolithic vision-language pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yimeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_N/0/1/0/all/0/1&quot;&gt;Navid Rajabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1&quot;&gt;Sulabh Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1&quot;&gt;Md Alimoor Reza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1&quot;&gt;Jana Kosecka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10885">
<title>A Video-Based Activity Classification of Human Pickers in Agriculture. (arXiv:2311.10885v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10885</link>
<description rdf:parseType="Literal">&lt;p&gt;In farming systems, harvesting operations are tedious, time- and
resource-consuming tasks. Based on this, deploying a fleet of autonomous robots
to work alongside farmworkers may provide vast productivity and logistics
benefits. Then, an intelligent robotic system should monitor human behavior,
identify the ongoing activities and anticipate the worker&apos;s needs. In this
work, the main contribution consists of creating a benchmark model for
video-based human pickers detection, classifying their activities to serve in
harvesting operations for different agricultural scenarios. Our solution uses
the combination of a Mask Region-based Convolutional Neural Network (Mask
R-CNN) for object detection and optical flow for motion estimation with newly
added statistical attributes of flow motion descriptors, named as Correlation
Sensitivity (CS). A classification criterion is defined based on the Kernel
Density Estimation (KDE) analysis and K-means clustering algorithm, which are
implemented upon in-house collected dataset from different crop fields like
strawberry polytunnels and apple tree orchards. The proposed framework is
quantitatively analyzed using sensitivity, specificity, and accuracy measures
and shows satisfactory results amidst various dataset challenges such as
lighting variation, blur, and occlusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1&quot;&gt;Abhishesh Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leite_A/0/1/0/all/0/1&quot;&gt;Antonio C. Leite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjevestad_J/0/1/0/all/0/1&quot;&gt;Jon G. O. Gjevestad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+From_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe5;l J. From&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10887">
<title>Point Cloud Self-supervised Learning via 3D to Multi-view Masked Autoencoder. (arXiv:2311.10887v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10887</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of 3D self-supervised learning has witnessed
significant progress, resulting in the emergence of Multi-Modality Masked
AutoEncoders (MAE) methods that leverage both 2D images and 3D point clouds for
pre-training. However, a notable limitation of these approaches is that they do
not fully utilize the multi-view attributes inherent in 3D point clouds, which
is crucial for a deeper understanding of 3D structures. Building upon this
insight, we introduce a novel approach employing a 3D to multi-view masked
autoencoder to fully harness the multi-modal attributes of 3D point clouds. To
be specific, our method uses the encoded tokens from 3D masked point clouds to
generate original point clouds and multi-view depth images across various
poses. This approach not only enriches the model&apos;s comprehension of geometric
structures but also leverages the inherent multi-modal properties of point
clouds. Our experiments illustrate the effectiveness of the proposed method for
different tasks and under different settings. Remarkably, our method
outperforms state-of-the-art counterparts by a large margin in a variety of
downstream tasks, including 3D object classification, few-shot learning, part
segmentation, and 3D object detection. Code will be available at:
https://github.com/Zhimin-C/Multiview-MAE
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhimin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Longlong Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10899">
<title>Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning. (arXiv:2311.10899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10899</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline&apos;s effectiveness in the end using standard
metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Shaunak Joshi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaggar_R/0/1/0/all/0/1&quot;&gt;Raghav Gaggar&lt;/a&gt; (1) ((1) University of Southern California)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10902">
<title>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy. (arXiv:2311.10902v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10902</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography (OCT) and confocal microscopy are pivotal in
retinal imaging, each presenting unique benefits and limitations. In vivo OCT
offers rapid, non-invasive imaging but can be hampered by clarity issues and
motion artifacts. Ex vivo confocal microscopy provides high-resolution,
cellular detailed color images but is invasive and poses ethical concerns and
potential tissue damage. To bridge these modalities, we developed a 3D CycleGAN
framework for unsupervised translation of in vivo OCT to ex vivo confocal
microscopy images. Applied to our OCT2Confocal dataset, this framework
effectively translates between 3D medical data domains, capturing vascular,
textural, and cellular details with precision. This marks the first attempt to
exploit the inherent 3D information of OCT and translate it into the rich,
detailed color domain of confocal microscopy. Assessed through quantitative and
qualitative metrics, the 3D CycleGAN framework demonstrates commendable image
fidelity and quality, outperforming existing methods despite the constraints of
limited data. This non-invasive generation of retinal confocal images has the
potential to further enhance diagnostic and monitoring capabilities in
ophthalmology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1&quot;&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nicholson_L/0/1/0/all/0/1&quot;&gt;Lindsay Nicholson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Achim_A/0/1/0/all/0/1&quot;&gt;Alin Achim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10904">
<title>Closely-Spaced Object Classification Using MuyGPyS. (arXiv:2311.10904v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10904</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately detecting rendezvous and proximity operations (RPO) is crucial for
understanding how objects are behaving in the space domain. However, detecting
closely-spaced objects (CSO) is challenging for ground-based optical space
domain awareness (SDA) algorithms as two objects close together along the
line-of-sight can appear blended as a single object within the point-spread
function (PSF) of the optical system. Traditional machine learning methods can
be useful for differentiating between singular objects and closely-spaced
objects, but many methods require large training sample sizes or high
signal-to-noise conditions. The quality and quantity of realistic data make
probabilistic classification methods a superior approach, as they are better
suited to handle these data inadequacies. We present CSO classification results
using the Gaussian process python package, MuyGPyS, and examine classification
accuracy as a function of angular separation and magnitude difference between
the simulated satellites. This orbit-independent analysis is done on highly
accurate simulated SDA images that emulate realistic ground-based
commercial-of-the-shelf (COTS) optical sensor observations of CSOs. We find
that MuyGPyS outperforms traditional machine learning methods, especially under
more challenging circumstances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pruett_K/0/1/0/all/0/1&quot;&gt;Kerianne Pruett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McNaughton_N/0/1/0/all/0/1&quot;&gt;Nathan McNaughton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_M/0/1/0/all/0/1&quot;&gt;Michael Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10918">
<title>Jenga Stacking Based on 6D Pose Estimation for Architectural Form Finding Process. (arXiv:2311.10918v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10918</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper includes a review of current state of the art 6d pose estimation
methods, as well as a discussion of which pose estimation method should be used
in two types of architectural design scenarios. Taking the latest pose
estimation research Gen6d as an example, we make a qualitative assessment of
the current openset methods in terms of application level, prediction speed,
resistance to occlusion, accuracy, resistance to environmental interference,
etc. In addition, we try to combine 6D pose estimation and building wind
environment assessment to create tangible architectural design approach, we
discuss the limitations of the method and point out the direction in which 6d
pose estimation is eager to progress in this scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10931">
<title>FLORIDA: Fake-looking Real Images Dataset. (arXiv:2311.10931v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10931</link>
<description rdf:parseType="Literal">&lt;p&gt;Although extensive research has been carried out to evaluate the
effectiveness of AI tools and models in detecting deep fakes, the question
remains unanswered regarding whether these models can accurately identify
genuine images that appear artificial. In this study, as an initial step
towards addressing this issue, we have curated a dataset of 510 genuine images
that exhibit a fake appearance and conducted an assessment using two AI models.
We show that two models exhibited subpar performance when applied to our
dataset. Additionally, our dataset can serve as a valuable tool for assessing
the ability of deep learning models to comprehend complex visual stimuli. We
anticipate that this research will stimulate further discussions and
investigations in this area. Our dataset is accessible at
https://github.com/aliborji/FLORIDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10933">
<title>Representing visual classification as a linear combination of words. (arXiv:2311.10933v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10933</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability is a longstanding challenge in deep learning, especially in
high-stakes domains like healthcare. Common explainability methods highlight
image regions that drive an AI model&apos;s decision. Humans, however, heavily rely
on language to convey explanations of not only &quot;where&quot; but &quot;what&quot;.
Additionally, most explainability approaches focus on explaining individual AI
predictions, rather than describing the features used by an AI model in
general. The latter would be especially useful for model and dataset auditing,
and potentially even knowledge generation as AI is increasingly being used in
novel tasks. Here, we present an explainability strategy that uses a
vision-language model to identify language-based descriptors of a visual
classification task. By leveraging a pre-trained joint embedding space between
images and text, our approach estimates a new classification task as a linear
combination of words, resulting in a weight for each word that indicates its
alignment with the vision-based classifier. We assess our approach using two
medical imaging classification tasks, where we find that the resulting
descriptors largely align with clinical knowledge despite a lack of
domain-specific language training. However, our approach also identifies the
potential for &apos;shortcut connections&apos; in the public datasets used. Towards a
functional measure of explainability, we perform a pilot reader study where we
find that the AI-identified words can enable non-expert humans to perform a
specialized medical task at a non-trivial level. Altogether, our results
emphasize the potential of using multimodal foundational models to deliver
intuitive, language-based explanations of visual tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Shobhit Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenov_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy R. Semenov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotter_W/0/1/0/all/0/1&quot;&gt;William Lotter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10950">
<title>Single-shot Phase Retrieval from a Fractional Fourier Transform Perspective. (arXiv:2311.10950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10950</link>
<description rdf:parseType="Literal">&lt;p&gt;The realm of classical phase retrieval concerns itself with the arduous task
of recovering a signal from its Fourier magnitude measurements, which are
fraught with inherent ambiguities. A single-exposure intensity measurement is
commonly deemed insufficient for the reconstruction of the primal signal, given
that the absent phase component is imperative for the inverse transformation.
In this work, we present a novel single-shot phase retrieval paradigm from a
fractional Fourier transform (FrFT) perspective, which involves integrating the
FrFT-based physical measurement model within a self-supervised reconstruction
scheme. Specifically, the proposed FrFT-based measurement model addresses the
aliasing artifacts problem in the numerical calculation of Fresnel diffraction,
featuring adaptability to both short-distance and long-distance propagation
scenarios. Moreover, the intensity measurement in the FrFT domain proves highly
effective in alleviating the ambiguities of phase retrieval and relaxing the
previous conditions on oversampled or multiple measurements in the Fourier
domain. Furthermore, the proposed self-supervised reconstruction approach
harnesses the fast discrete algorithm of FrFT alongside untrained neural
network priors, thereby attaining preeminent results. Through numerical
simulations, we demonstrate that both amplitude and phase objects can be
effectively retrieved from a single-shot intensity measurement using the
proposed approach and provide a promising technique for support-free coherent
diffraction imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yixiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Ran Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kaixuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jun Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10952">
<title>NAS-ASDet: An Adaptive Design Method for Surface Defect Detection Network using Neural Architecture Search. (arXiv:2311.10952v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10952</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep convolutional neural networks (CNNs) have been widely used in surface
defect detection. However, no CNN architecture is suitable for all detection
tasks and designing effective task-specific requires considerable effort. The
neural architecture search (NAS) technology makes it possible to automatically
generate adaptive data-driven networks. Here, we propose a new method called
NAS-ASDet to adaptively design network for surface defect detection. First, a
refined and industry-appropriate search space that can adaptively adjust the
feature distribution is designed, which consists of repeatedly stacked basic
novel cells with searchable attention operations. Then, a progressive search
strategy with a deep supervision mechanism is used to explore the search space
faster and better. This method can design high-performance and lightweight
defect detection networks with data scarcity in industrial scenarios. The
experimental results on four datasets demonstrate that the proposed method
achieves superior performance and a relatively lighter model size compared to
other competitive methods, including both manual and NAS-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weifeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1&quot;&gt;Shuanlong Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1&quot;&gt;Wang Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1&quot;&gt;Tongzhi Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10959">
<title>Structure-Aware Sparse-View X-ray 3D Reconstruction. (arXiv:2311.10959v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10959</link>
<description rdf:parseType="Literal">&lt;p&gt;X-ray, known for its ability to reveal internal structures of objects, is
expected to provide richer information for 3D reconstruction than visible
light. Yet, existing neural radiance fields (NeRF) algorithms overlook this
important nature of X-ray, leading to their limitations in capturing structural
contents of imaged objects. In this paper, we propose a framework,
Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view
X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer
(Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal
structures of objects in 3D space by modeling the dependencies within each line
segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray
sampling strategy to extract contextual and geometric information in 2D
projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray
applications. Experiments on X3D show that SAX-NeRF surpasses previous
NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT
reconstruction. Code, models, and data will be released at
https://github.com/caiyuanhao1998/SAX-NeRF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zongwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Angtian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10982">
<title>Make Pixels Dance: High-Dynamic Video Generation. (arXiv:2311.10982v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10982</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating high-dynamic videos such as motion-rich actions and sophisticated
visual effects poses a significant challenge in the field of artificial
intelligence. Unfortunately, current state-of-the-art video generation methods,
primarily focusing on text-to-video generation, tend to produce video clips
with minimal motions despite maintaining high fidelity. We argue that relying
solely on text instructions is insufficient and suboptimal for video
generation. In this paper, we introduce PixelDance, a novel approach based on
diffusion models that incorporates image instructions for both the first and
last frames in conjunction with text instructions for video generation.
Comprehensive experimental results demonstrate that PixelDance trained with
public data exhibits significantly better proficiency in synthesizing videos
with complex scenes and intricate motions, setting a new standard for video
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1&quot;&gt;Guoqiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiani Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Jiaxin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10983">
<title>Multiple View Geometry Transformers for 3D Human Pose Estimation. (arXiv:2311.10983v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10983</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to improve the 3D reasoning ability of Transformers in
multi-view 3D human pose estimation. Recent works have focused on end-to-end
learning-based transformer designs, which struggle to resolve geometric
information accurately, particularly during occlusion. Instead, we propose a
novel hybrid model, MVGFormer, which has a series of geometric and appearance
modules organized in an iterative manner. The geometry modules are
learning-free and handle all viewpoint-dependent 3D tasks geometrically which
notably improves the model&apos;s generalization ability. The appearance modules are
learnable and are dedicated to estimating 2D poses from image signals
end-to-end which enables them to achieve accurate estimates even when occlusion
occurs, leading to a model that is both accurate and generalizable to new
cameras and geometries. We evaluate our approach for both in-domain and
out-of-domain settings, where our model consistently outperforms
state-of-the-art methods, and especially does so by a significant margin in the
out-of-domain setting. We will release the code and models:
https://github.com/XunshanMan/MVGFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jialiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10988">
<title>Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention. (arXiv:2311.10988v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10988</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Graph Generation (SGG) offers a structured representation critical in
many computer vision applications. Traditional SGG approaches, however, are
limited by a closed-set assumption, restricting their ability to recognize only
predefined object and relation categories. To overcome this, we categorize SGG
scenarios into four distinct settings based on the node and edge: Closed-set
SGG, Open Vocabulary (object) Detection-based SGG (OvD-SGG), Open Vocabulary
Relation-based SGG (OvR-SGG), and Open Vocabulary Detection + Relation-based
SGG (OvD+R-SGG). While object-centric open vocabulary SGG has been studied
recently, the more challenging problem of relation-involved open-vocabulary SGG
remains relatively unexplored. To fill this gap, we propose a unified framework
named OvSGTR towards fully open vocabulary SGG from a holistic view. The
proposed framework is an end-toend transformer architecture, which learns a
visual-concept alignment for both nodes and edges, enabling the model to
recognize unseen categories. For the more challenging settings of
relation-involved open vocabulary SGG, the proposed approach integrates
relation-aware pre-training utilizing image-caption data and retains
visual-concept alignment through knowledge distillation. Comprehensive
experimental results on the Visual Genome benchmark demonstrate the
effectiveness and superiority of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zuyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinlin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changwen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10992">
<title>Towards Robust and Accurate Visual Prompting. (arXiv:2311.10992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10992</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual prompting, an efficient method for transfer learning, has shown its
potential in vision tasks. However, previous works focus exclusively on VP from
standard source models, it is still unknown how it performs under the scenario
of a robust source model: Whether a visual prompt derived from a robust model
can inherit the robustness while suffering from the generalization performance
decline, albeit for a downstream dataset that is different from the source
dataset? In this work, we get an affirmative answer of the above question and
give an explanation on the visual representation level. Moreover, we introduce
a novel technique named Prompt Boundary Loose (PBL) to effectively mitigates
the suboptimal results of visual prompt on standard accuracy without losing (or
even significantly improving) its adversarial robustness when using a robust
model as source model. Extensive experiments across various datasets show that
our findings are universal and demonstrate the significant benefits of our
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhouqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10995">
<title>Behavior Optimized Image Generation. (arXiv:2311.10995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10995</link>
<description rdf:parseType="Literal">&lt;p&gt;The last few years have witnessed great success on image generation, which
has crossed the acceptance thresholds of aesthetics, making it directly
applicable to personal and commercial applications. However, images, especially
in marketing and advertising applications, are often created as a means to an
end as opposed to just aesthetic concerns. The goal can be increasing sales,
getting more clicks, likes, or image sales (in the case of stock businesses).
Therefore, the generated images need to perform well on these key performance
indicators (KPIs), in addition to being aesthetically good. In this paper, we
make the first endeavor to answer the question of &quot;How can one infuse the
knowledge of the end-goal within the image generation process itself to create
not just better-looking images but also &quot;better-performing&apos;&apos; images?&apos;&apos;. We
propose BoigLLM, an LLM that understands both image content and user behavior.
BoigLLM knows how an image should look to get a certain required KPI. We show
that BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this
task, demonstrating that while these state-of-the-art models can understand
images, they lack information on how these images perform in the real world. To
generate actual pixels of behavior-conditioned images, we train a
diffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward.
We show the performance of the overall pipeline on two datasets covering two
different behaviors: a stock dataset with the number of forward actions as the
KPI and a dataset containing tweets with the total likes as the KPI, denoted as
BoigBench. To advance research in the direction of utility-driven image
generation and understanding, we release BoigBench, a benchmark dataset
containing 168 million enterprise tweets with their media, brand account names,
time of post, and total likes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_V/0/1/0/all/0/1&quot;&gt;Varun Khurana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1&quot;&gt;Yaman K Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_J/0/1/0/all/0/1&quot;&gt;Jayakumar Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajiv Ratn Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10998">
<title>Learning Scene Context Without Images. (arXiv:2311.10998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10998</link>
<description rdf:parseType="Literal">&lt;p&gt;Teaching machines of scene contextual knowledge would enable them to interact
more effectively with the environment and to anticipate or predict objects that
may not be immediately apparent in their perceptual field. In this paper, we
introduce a novel transformer-based approach called $LMOD$ ( Label-based
Missing Object Detection) to teach scene contextual knowledge to machines using
an attention mechanism. A distinctive aspect of the proposed approach is its
reliance solely on labels from image datasets to teach scene context, entirely
eliminating the need for the actual image itself. We show how scene-wide
relationships among different objects can be learned using a self-attention
mechanism. We further show that the contextual knowledge gained from label
based learning can enhance performance of other visual based object detection
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rouhi_A/0/1/0/all/0/1&quot;&gt;Amirreza Rouhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;David Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11013">
<title>Implicit Event-RGBD Neural SLAM. (arXiv:2311.11013v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11013</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural SLAM has achieved remarkable progress recently. Nevertheless,
existing methods face significant challenges in non-ideal scenarios, such as
motion blur or lighting variation, which often leads to issues like convergence
failures, localization drifts, and distorted mapping. To address these
challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural
SLAM framework, which effectively leverages the high rate and high dynamic
range advantages of event data for tracking and mapping. Specifically, EN-SLAM
proposes a differentiable CRF (Camera Response Function) rendering technique to
generate distinct RGB and event camera data via a shared radiance field, which
is optimized by learning a unified implicit representation with the captured
event and RGBD supervision. Moreover, based on the temporal difference property
of events, we propose a temporal aggregating optimization strategy for the
event joint tracking and global bundle adjustment, capitalizing on the
consecutive difference constraints of events, significantly enhancing tracking
accuracy and robustness. Finally, we construct the simulated dataset
$\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$
containing 6 scenes, 17 sequences with practical motion blur and lighting
changes for evaluations. Experimental results show that our method outperforms
the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS
in various challenging environments. The code and dataset will be released upon
the paper publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1&quot;&gt;Delin Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11014">
<title>Lesion Search with Self-supervised Learning. (arXiv:2311.11014v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11014</link>
<description rdf:parseType="Literal">&lt;p&gt;Content-based image retrieval (CBIR) with self-supervised learning (SSL)
accelerates clinicians&apos; interpretation of similar images without manual
annotations. We develop a CBIR from the contrastive learning SimCLR and
incorporate a generalized-mean (GeM) pooling followed by L2 normalization to
classify lesion types and retrieve similar images before clinicians&apos; analysis.
Results have shown improved performance. We additionally build an open-source
application for image analysis and retrieval. The application is easy to
integrate, relieving manual efforts and suggesting the potential to support
clinicians&apos; everyday activities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_K/0/1/0/all/0/1&quot;&gt;Kristin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiali Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haehn_D/0/1/0/all/0/1&quot;&gt;Daniel Haehn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11017">
<title>Improving Adversarial Transferability by Stable Diffusion. (arXiv:2311.11017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11017</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are susceptible to adversarial examples, which
introduce imperceptible perturbations to benign samples, deceiving DNN
predictions. While some attack methods excel in the white-box setting, they
often struggle in the black-box scenario, particularly against models fortified
with defense mechanisms. Various techniques have emerged to enhance the
transferability of adversarial attacks for the black-box scenario. Among these,
input transformation-based attacks have demonstrated their effectiveness. In
this paper, we explore the potential of leveraging data generated by Stable
Diffusion to boost adversarial transferability. This approach draws inspiration
from recent research that harnessed synthetic data generated by Stable
Diffusion to enhance model generalization. In particular, previous work has
highlighted the correlation between the presence of both real and synthetic
data and improved model generalization. Building upon this insight, we
introduce a novel attack method called Stable Diffusion Attack Method (SDAM),
which incorporates samples generated by Stable Diffusion to augment input
images. Furthermore, we propose a fast variant of SDAM to reduce computational
overhead while preserving high adversarial transferability. Our extensive
experimental results demonstrate that our method outperforms state-of-the-art
baselines by a substantial margin. Moreover, our approach is compatible with
existing transfer-based attacks to further enhance adversarial transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiayang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Siyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Han Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Ee-Chien Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11019">
<title>Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels. (arXiv:2311.11019v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11019</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning fine-grained embeddings from coarse labels is a challenging task due
to limited label granularity supervision, i.e., lacking the detailed
distinctions required for fine-grained tasks. The task becomes even more
demanding when attempting few-shot fine-grained recognition, which holds
practical significance in various applications. To address these challenges, we
propose a novel method that embeds visual embeddings into a hyperbolic space
and enhances their discriminative ability with a hierarchical cosine margins
manner. Specifically, the hyperbolic space offers distinct advantages,
including the ability to capture hierarchical relationships and increased
expressive power, which favors modeling fine-grained objects. Based on the
hyperbolic space, we further enforce relatively large/small similarity margins
between coarse/fine classes, respectively, yielding the so-called hierarchical
cosine margins manner. While enforcing similarity margins in the regular
Euclidean space has become popular for deep embedding learning, applying it to
the hyperbolic space is non-trivial and validating the benefit for
coarse-to-fine generalization is valuable. Extensive experiments conducted on
five benchmark datasets showcase the effectiveness of our proposed method,
yielding state-of-the-art results surpassing competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shu-Lin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Faen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1&quot;&gt;Anqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11029">
<title>Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen Classification from Microscopic Images. (arXiv:2311.11029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11029</link>
<description rdf:parseType="Literal">&lt;p&gt;Distribution shifts are characterized by differences between the training and
test data distributions. They can significantly reduce the accuracy of machine
learning models deployed in real-world scenarios. This paper explores the
distribution shift problem when classifying pollen grains from microscopic
images collected in the wild with a low-cost camera sensor. We leverage the
domain knowledge that geometric features are highly important for accurate
pollen identification and introduce two novel geometric image augmentation
techniques to significantly narrow the accuracy gap between the model
performance on the train and test datasets. In particular, we show that
Tenengrad and ImageToSketch filters are highly effective to balance the shape
and texture information while leaving out unimportant details that may confuse
the model. Extensive evaluations on various model architectures demonstrate a
consistent improvement of the model generalization to field data of up to 14%
achieved by the geometric augmentation techniques when compared to a wide range
of standard image augmentations. The approach is validated through an ablation
study using pollen hydration tests to recover the shape of dry pollen grains.
The proposed geometric augmentations also receive the highest scores according
to the affinity and diversity measures from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1&quot;&gt;Nam Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1&quot;&gt;Olga Saukh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11039">
<title>Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment. (arXiv:2311.11039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11039</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data is being used lately for training deep neural networks in
computer vision applications such as object detection, object segmentation and
6D object pose estimation. Domain randomization hereby plays an important role
in reducing the simulation to reality gap. However, this generalization might
not be effective in specialized domains like a production environment involving
complex assemblies. Either the individual parts, trained with synthetic images,
are integrated in much larger assemblies making them indistinguishable from
their counterparts and result in false positives or are partially occluded just
enough to give rise to false negatives. Domain knowledge is vital in these
cases and if conceived effectively while generating synthetic data, can show a
considerable improvement in bridging the simulation to reality gap. This paper
focuses on synthetic data generation procedures for parts and assemblies used
in a production environment. The basic procedures for synthetic data generation
and their various combinations are evaluated and compared on images captured in
a production environment, where results show up to 15% improvement using
combinations of basic procedures. Reducing the simulation to reality gap in
this way can aid to utilize the true potential of robot assisted production
using artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_P/0/1/0/all/0/1&quot;&gt;Parth Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sompura_M/0/1/0/all/0/1&quot;&gt;Mrunal Sompura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintze_W/0/1/0/all/0/1&quot;&gt;Wolfgang Hintze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.00789">
<title>Role Taxonomy of Units in Deep Neural Networks. (arXiv:2011.00789v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2011.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying the role of network units in deep neural networks (DNNs) is
critical in many aspects including giving understandings on the mechanisms of
DNNs and building basic connections between deep learning and neuroscience.
However, there remains unclear on which roles the units in DNNs with different
generalization ability could present. To this end, we give role taxonomy of
units in DNNs via introducing the retrieval-of-function test, where units are
categorized into four types in terms of their functional preference on
separately the training set and testing set. We show that ratios of the four
categories are highly associated with the generalization ability of DNNs from
two distinct perspectives, based on which we give signs of DNNs with well
generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.04514">
<title>Human Motion Tracking by Registering an Articulated Surface to 3-D Points and Normals. (arXiv:2012.04514v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.04514</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of human motion tracking by registering a surface to
3-D data. We propose a method that iteratively computes two things: Maximum
likelihood estimates for both the kinematic and free-motion parameters of a
kinematic human-body representation, as well as probabilities that the data are
assigned either to a body part, or to an outlier cluster. We introduce a new
metric between observed points and normals on one side, and a parameterized
surface on the other side, the latter being defined as a blending over a set of
ellipsoids. We claim that this metric is well suited when one deals with either
visual-hull or visual-shape observations. We illustrate the method by tracking
human motions using sparse visual-shape data (3-D surface points and normals)
gathered from imperfect silhouettes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niskanen_M/0/1/0/all/0/1&quot;&gt;Matti Niskanen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dewaele_G/0/1/0/all/0/1&quot;&gt;Guillaume Dewaele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1&quot;&gt;Edmond Boyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.05582">
<title>Image Matching with Scale Adjustment. (arXiv:2012.05582v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.05582</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we address the problem of matching two images with two
different resolutions: a high-resolution image and a low-resolution one. The
difference in resolution between the two images is not known and without loss
of generality one of the images is assumed to be the high-resolution one. On
the premise that changes in resolution act as a smoothing equivalent to changes
in scale, a scale-space representation of the high-resolution image is
produced. Hence the one-to-one classical image matching paradigm becomes
one-to-many because the low-resolution image is compared with all the
scale-space representations of the high-resolution one. Key to the success of
such a process is the proper representation of the features to be matched in
scale-space. We show how to represent and extract interest points at variable
scales and we devise a method allowing the comparison of two images at two
different resolutions. The method comprises the use of photometric- and
rotation-invariant descriptors, a geometric model mapping the high-resolution
image onto a low-resolution image region, and an image matching strategy based
on local constraints and on the robust estimation of this geometric model.
Extensive experiments show that our matching method can be used for scale
changes up to a factor of 6.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dufournaud_Y/0/1/0/all/0/1&quot;&gt;Yves Dufournaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.00851">
<title>Estimating the Generalization in Deep Neural Networks via Sparsity. (arXiv:2104.00851v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.00851</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization is the key capability for deep neural networks (DNNs).
However, it is challenging to give a reliable measure of the generalization
ability of a DNN via only its nature. In this paper, we propose a novel method
for estimating the generalization gap based on network sparsity. In our method,
two key quantities are proposed first. They have close relationship with the
generalization ability and can be calculated directly from the training results
alone. Then a simple linear model involving two key quantities are constructed
to give accurate estimation of the generalization gap. By training DNNs with a
wide range of generalization gap on popular datasets, we show that our key
quantities and linear model could be efficient tools for estimating the
generalization gap of DNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.08717">
<title>Do We Really Need Dice? The Hidden Region-Size Biases of Segmentation Losses. (arXiv:2104.08717v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.08717</link>
<description rdf:parseType="Literal">&lt;p&gt;Most segmentation losses are arguably variants of the Cross-Entropy (CE) or
Dice losses. On the surface, these two categories of losses seem unrelated, and
there is no clear consensus as to which category is a better choice, with
varying performances for each across different benchmarks and applications.
Furthermore, it is widely argued within the medical-imaging community that Dice
and CE are complementary, which has motivated the use of compound CE-Dice
losses. In this work, we provide a theoretical analysis, which shows that CE
and Dice share a much deeper connection than previously thought. First, we show
that, from a constrained-optimization perspective, they both decompose into two
components, i.e., a similar ground-truth matching term, which pushes the
predicted foreground regions towards the ground-truth, and a region-size
penalty term imposing different biases on the size of the predicted regions.
Then, we provide bound relationships and an information-theoretic analysis,
which uncover hidden region-size biases: Dice has an intrinsic bias towards
specific extremely imbalanced solutions, whereas CE implicitly encourages the
ground-truth region proportions. Our theoretical results explain the wide
experimental evidence in the medical-imaging literature, whereby Dice losses
bring improvements for imbalanced segmentation. Based on our theoretical
analysis, we propose a principled and simple solution, which enables to control
explicitly the region-size bias. The proposed method integrates CE with
explicit terms based on L1 or the KL divergence, which encourage segmenting
region proportions to match target class proportions, thereby mitigating class
imbalance but without losing generality. Comprehensive experiments and ablation
studies over different losses and applications validate our theoretical
analysis, as well as the effectiveness of explicit and simple region-size
terms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1&quot;&gt;Adrian Galdran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobbi_R/0/1/0/all/0/1&quot;&gt;Riadh Kobbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.11851">
<title>Transcript to Video: Efficient Clip Sequencing from Texts. (arXiv:2107.11851v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.11851</link>
<description rdf:parseType="Literal">&lt;p&gt;Among numerous videos shared on the web, well-edited ones always attract more
attention. However, it is difficult for inexperienced users to make well-edited
videos because it requires professional expertise and immense manual labor. To
meet the demands for non-experts, we present Transcript-to-Video -- a
weakly-supervised framework that uses texts as input to automatically create
video sequences from an extensive collection of shots. Specifically, we propose
a Content Retrieval Module and a Temporal Coherent Module to learn
visual-language representations and model shot sequencing styles, respectively.
For fast inference, we introduce an efficient search strategy for real-time
video clip sequencing. Quantitative results and user studies demonstrate
empirically that the proposed learning framework can retrieve content-relevant
shots while creating plausible video sequences in terms of style. Besides, the
run-time performance analysis shows that our framework can support real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1&quot;&gt;Fabian Caba Heilbron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.04658">
<title>Differential Motion Evolution for Fine-Grained Motion Deformation in Unsupervised Image Animation. (arXiv:2110.04658v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.04658</link>
<description rdf:parseType="Literal">&lt;p&gt;Image animation is the task of transferring the motion of a driving video to
a given object in a source image. While great progress has recently been made
in unsupervised motion transfer, requiring no labeled data or domain priors,
many current unsupervised approaches still struggle to capture the motion
deformations when large motion/view discrepancies occur between the source and
driving domains. Under such conditions, there is simply not enough information
to capture the motion field properly. We introduce DiME (Differential Motion
Evolution), an end-to-end unsupervised motion transfer framework integrating
differential refinement for motion estimation. Key findings are twofold: (1) by
capturing the motion transfer with an ordinary differential equation (ODE), it
helps to regularize the motion field, and (2) by utilizing the source image
itself, we are able to inpaint occluded/missing regions arising from large
motion changes. Additionally, we also propose a natural extension to the ODE
idea, which is that DiME can easily leverage multiple different views of the
source object whenever they are available by modeling an ODE per view.
Extensive experiments across 9 benchmarks show DiME outperforms the
state-of-the-arts by a significant margin and generalizes much better to unseen
objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peirong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xuefei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yipin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ashish Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.13677">
<title>SWAT: Spatial Structure Within and Among Tokens. (arXiv:2111.13677v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.13677</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling visual data as tokens (i.e., image patches) using attention
mechanisms, feed-forward networks or convolutions has been highly effective in
recent years. Such methods usually have a common pipeline: a tokenization
method, followed by a set of layers/blocks for information mixing, both within
and among tokens. When image patches are converted into tokens, they are often
flattened, discarding the spatial structure within each patch. As a result, any
processing that follows (eg: multi-head self-attention) may fail to recover
and/or benefit from such information. In this paper, we argue that models can
have significant gains when spatial structure is preserved during tokenization,
and is explicitly used during the mixing stage. We propose two key
contributions: (1) Structure-aware Tokenization and, (2) Structure-aware
Mixing, both of which can be combined with existing models with minimal effort.
We introduce a family of models (SWAT), showing improvements over the likes of
DeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including
ImageNet classification and ADE20K segmentation. Our code is available at
https://github.com/kkahatapitiya/SWAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahatapitiya_K/0/1/0/all/0/1&quot;&gt;Kumara Kahatapitiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.12699">
<title>Accurate and Efficient Stereo Matching via Attention Concatenation Volume. (arXiv:2209.12699v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.12699</link>
<description rdf:parseType="Literal">&lt;p&gt;Stereo matching is a fundamental building block for many vision and robotics
applications. An informative and concise cost volume representation is vital
for stereo matching of high accuracy and efficiency. In this paper, we present
a novel cost volume construction method, named attention concatenation volume
(ACV), which generates attention weights from correlation clues to suppress
redundant information and enhance matching-related information in the
concatenation volume. The ACV can be seamlessly embedded into most stereo
matching networks, the resulting networks can use a more lightweight
aggregation network and meanwhile achieve higher accuracy. We further design a
fast version of ACV to enable real-time performance, named Fast-ACV, which
generates high likelihood disparity hypotheses and the corresponding attention
weights from low-resolution correlation clues to significantly reduce
computational and memory cost and meanwhile maintain a satisfactory accuracy.
The core idea of our Fast-ACV is volume attention propagation (VAP) which can
automatically select accurate correlation values from an upsampled correlation
volume and propagate these accurate values to the surroundings pixels with
ambiguous correlation clues. Furthermore, we design a highly accurate network
ACVNet and a real-time network Fast-ACVNet based on our ACV and Fast-ACV
respectively, which achieve the state-of-the-art performance on several
benchmarks (i.e., our ACVNet ranks the 2nd on KITTI 2015 and Scene Flow, and
the 3rd on KITTI 2012 and ETH3D among all the published methods; our
Fast-ACVNet outperforms almost all state-of-the-art real-time methods on Scene
Flow, KITTI 2012 and 2015 and meanwhile has better generalization ability)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1&quot;&gt;Gangwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junda Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04574">
<title>ARUBA: An Architecture-Agnostic Balanced Loss for Aerial Object Detection. (arXiv:2210.04574v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04574</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks tend to reciprocate the bias of their training dataset.
In object detection, the bias exists in the form of various imbalances such as
class, background-foreground, and object size. In this paper, we denote size of
an object as the number of pixels it covers in an image and size imbalance as
the over-representation of certain sizes of objects in a dataset. We aim to
address the problem of size imbalance in drone-based aerial image datasets.
Existing methods for solving size imbalance are based on architectural changes
that utilize multiple scales of images or feature maps for detecting objects of
different sizes. We, on the other hand, propose a novel ARchitectUre-agnostic
BAlanced Loss (ARUBA) that can be applied as a plugin on top of any object
detection model. It follows a neighborhood-driven approach inspired by the
ordinality of object size. We evaluate the effectiveness of our approach
through comprehensive experiments on aerial datasets such as HRSC2016,
DOTAv1.0, DOTAv1.5 and VisDrone and obtain consistent improvement in
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sairam_R/0/1/0/all/0/1&quot;&gt;Rebbapragada V C Sairam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keswani_M/0/1/0/all/0/1&quot;&gt;Monish Keswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1&quot;&gt;Uttaran Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nishit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07273">
<title>MLIC: Multi-Reference Entropy Model for Learned Image Compression. (arXiv:2211.07273v8 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07273</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learned image compression has achieved remarkable performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in boosting rate-distortion performance. However, most
entropy models only capture correlations in one dimension, while the latent
representation contain channel-wise, local spatial, and global spatial
correlations. To tackle this issue, we propose the Multi-Reference Entropy
Model (MEM) and the advanced version, MEM$^+$. These models capture the
different types of correlations present in latent representation. Specifically,
We first divide the latent representation into slices. When decoding the
current slice, we use previously decoded slices as context and employ the
attention map of the previously decoded slice to predict global correlations in
the current slice. To capture local contexts, we introduce two enhanced
checkerboard context capturing techniques that avoids performance degradation.
Based on MEM and MEM$^+$, we propose image compression models MLIC and
MLIC$^+$. Extensive experimental evaluations demonstrate that our MLIC and
MLIC$^+$ models achieve state-of-the-art performance, reducing BD-rate by
$8.05\%$ and $11.39\%$ on the Kodak dataset compared to VTM-17.0 when measured
in PSNR. Our code is available at https://github.com/JiangWeibeta/MLIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ning_P/0/1/0/all/0/1&quot;&gt;Peirong Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11077">
<title>Unifying Tracking and Image-Video Object Detection. (arXiv:2211.11077v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11077</link>
<description rdf:parseType="Literal">&lt;p&gt;Objection detection (OD) has been one of the most fundamental tasks in
computer vision. Recent developments in deep learning have pushed the
performance of image OD to new heights by learning-based, data-driven
approaches. On the other hand, video OD remains less explored, mostly due to
much more expensive data annotation needs. At the same time, multi-object
tracking (MOT) which requires reasoning about track identities and
spatio-temporal trajectories, shares similar spirits with video OD. However,
most MOT datasets are class-specific (e.g., person-annotated only), which
constrains a model&apos;s flexibility to perform tracking on other objects. We
propose TrIVD (Tracking and Image-Video Detection), the first framework that
unifies image OD, video OD, and MOT within one end-to-end model. To handle the
discrepancies and semantic overlaps of category labels across datasets, TrIVD
formulates detection/tracking as grounding and reasons about object categories
via visual-text alignments. The unified formulation enables cross-dataset,
multi-task training, and thus equips TrIVD with the ability to leverage
frame-level features, video-level spatio-temporal relations, as well as track
identity associations. With such joint training, we can now extend the
knowledge from OD data, that comes with much richer object category
annotations, to MOT and achieve zero-shot tracking capability. Experiments
demonstrate that multi-task co-trained TrIVD outperforms single-task baselines
across all image/video OD and MOT tasks. We further set the first baseline on
the new task of zero-shot tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peirong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1&quot;&gt;Omid Poursaeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yipin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xuefei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sreya Dutta Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ashish Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16098">
<title>Three-stage binarization of color document images based on discrete wavelet transform and generative adversarial networks. (arXiv:2211.16098v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16098</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficient segmentation of foreground text information from the background
in degraded color document images is a critical challenge in the preservation
of ancient manuscripts. The imperfect preservation of ancient manuscripts has
led to various types of degradation over time, such as staining, yellowing, and
ink seepage, significantly affecting image binarization results. This work
proposes a three-stage method using generative adversarial networks (GANs) for
the degraded color document images binarization. Stage-1 involves applying
discrete wavelet transform (DWT) and retaining the low-low (LL) subband images
for image enhancement. In Stage-2, the original input image is split into red,
green, and blue (RGB) three single-channel images and one grayscale image, and
each image is trained with independent adversarial networks to extract color
foreground information. In Stage-3, the output image from Stage-2 and the
resized input image are used to train independent adversarial networks for
document binarization, enabling the integration of global and local features.
The experimental results demonstrate that our proposed method outperforms other
traditional and state-of-the-art (SOTA) methods on the Document Image
Binarization Contest (DIBCO) datasets. We have released our implementation code
at https://github.com/abcpp12383/ThreeStageBinarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1&quot;&gt;Rui-Yang Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Shian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yanlin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chih-Chia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_C/0/1/0/all/0/1&quot;&gt;Chun-Tse Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1&quot;&gt;Jen-Shiun Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05087">
<title>Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models. (arXiv:2302.05087v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05087</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Anomaly Detection (VAD) serves as a pivotal technology in the
intelligent surveillance systems, enabling the temporal or spatial
identification of anomalous events within videos. While existing reviews
predominantly concentrate on conventional unsupervised methods, they often
overlook the emergence of weakly-supervised and fully-unsupervised approaches.
To address this gap, this survey extends the conventional scope of VAD beyond
unsupervised methods, encompassing a broader spectrum termed Generalized Video
Anomaly Event Detection (GVAED). By skillfully incorporating recent
advancements rooted in diverse assumptions and learning frameworks, this survey
introduces an intuitive taxonomy that seamlessly navigates through
unsupervised, weakly-supervised, supervised and fully-unsupervised VAD
methodologies, elucidating the distinctions and interconnections within these
research trajectories. In addition, this survey facilitates prospective
researchers by assembling a compilation of research resources, including public
datasets, available codebases, programming tools, and pertinent literature.
Furthermore, this survey quantitatively assesses model performance, delves into
research challenges and directions, and outlines potential avenues for future
exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukerche_A/0/1/0/all/0/1&quot;&gt;Azzedine Boukerche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Liang Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06335">
<title>Online Arbitrary Shaped Clustering through Correlated Gaussian Functions. (arXiv:2302.06335v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06335</link>
<description rdf:parseType="Literal">&lt;p&gt;There is no convincing evidence that backpropagation is a biologically
plausible mechanism, and further studies of alternative learning methods are
needed. A novel online clustering algorithm is presented that can produce
arbitrary shaped clusters from inputs in an unsupervised manner, and requires
no prior knowledge of the number of clusters in the input data. This is
achieved by finding correlated outputs from functions that capture commonly
occurring input patterns. The algorithm can be deemed more biologically
plausible than model optimization through backpropagation, although practical
applicability may require additional research. However, the method yields
satisfactory results on several toy datasets on a noteworthy range of
hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eidheim_O/0/1/0/all/0/1&quot;&gt;Ole Christian Eidheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06494">
<title>Explicit3D: Graph Network with Spatial Inference for Single Image 3D Object Detection. (arXiv:2302.06494v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06494</link>
<description rdf:parseType="Literal">&lt;p&gt;Indoor 3D object detection is an essential task in single image scene
understanding, impacting spatial cognition fundamentally in visual reasoning.
Existing works on 3D object detection from a single image either pursue this
goal through independent predictions of each object or implicitly reason over
all possible objects, failing to harness relational geometric information
between objects. To address this problem, we propose a dynamic sparse graph
pipeline named Explicit3D based on object geometry and semantics features.
Taking the efficiency into consideration, we further define a relatedness score
and design a novel dynamic pruning algorithm followed by a cluster sampling
method for sparse scene graph generation and updating. Furthermore, our
Explicit3D introduces homogeneous matrices and defines new relative loss and
corner loss to model the spatial difference between target pairs explicitly.
Instead of using ground-truth labels as direct supervision, our relative and
corner loss are derived from the homogeneous transformation, which renders the
model to learn the geometric consistency between objects. The experimental
results on the SUN RGB-D dataset demonstrate that our Explicit3D achieves
better performance balance than the-state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11552">
<title>Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11552</link>
<description rdf:parseType="Literal">&lt;p&gt;Since their introduction, diffusion models have quickly become the prevailing
approach to generative modeling in many domains. They can be interpreted as
learning the gradients of a time-varying sequence of log-probability density
functions. This interpretation has motivated classifier-based and
classifier-free guidance as methods for post-hoc control of diffusion models.
In this work, we build upon these ideas using the score-based interpretation of
diffusion models, and explore alternative ways to condition, modify, and reuse
diffusion models for tasks involving compositional generation and guidance. In
particular, we investigate why certain types of composition fail using current
techniques and present a number of solutions. We conclude that the sampler (not
the model) is responsible for this failure and propose new samplers, inspired
by MCMC, which enable successful compositional generation. Further, we propose
an energy-based parameterization of diffusion models which enables the use of
new compositional operators and more sophisticated, Metropolis-corrected
samplers. Intriguingly we find these samplers lead to notable improvements in
compositional generation across a wide set of problems such as
classifier-guided ImageNet modeling and compositional text-to-image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durkan_C/0/1/0/all/0/1&quot;&gt;Conor Durkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1&quot;&gt;Robin Strudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1&quot;&gt;Sander Dieleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1&quot;&gt;Will Grathwohl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14670">
<title>Balanced Training for Sparse GANs. (arXiv:2302.14670v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14670</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, there has been growing interest in developing larger
and deeper neural networks, including deep generative models like generative
adversarial networks (GANs). However, GANs typically come with high
computational complexity, leading researchers to explore methods for reducing
the training and inference costs. One such approach gaining popularity in
supervised learning is dynamic sparse training (DST), which maintains good
performance while enjoying excellent training efficiency. Despite its potential
benefits, applying DST to GANs presents challenges due to the adversarial
nature of the training process. In this paper, we propose a novel metric called
the balance ratio (BR) to study the balance between the sparse generator and
discriminator. We also introduce a new method called balanced dynamic sparse
training (ADAPT), which seeks to control the BR during GAN training to achieve
a good trade-off between performance and computational cost. Our proposed
method shows promising results on multiple datasets, demonstrating its
effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yite Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1&quot;&gt;Naira Hovakimyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruoyu Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15445">
<title>IRFL: Image Recognition of Figurative Language. (arXiv:2303.15445v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15445</link>
<description rdf:parseType="Literal">&lt;p&gt;Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge.
&lt;/p&gt;
&lt;p&gt;In this work, we develop the Image Recognition of Figurative Language (IRFL)
dataset. We leverage human annotation and an automatic pipeline we created to
generate a multimodal dataset, and introduce two novel tasks as a benchmark for
multimodal figurative language understanding. We experimented with
state-of-the-art vision and language models and found that the best (22%)
performed substantially worse than humans (97%). We release our dataset,
benchmark, and code, in hopes of driving the development of models that can
better understand figurative language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1&quot;&gt;Ron Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16343">
<title>Facial recognition technology and human raters can predict political orientation from images of expressionless faces even when controlling for demographics and self-presentation. (arXiv:2303.16343v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16343</link>
<description rdf:parseType="Literal">&lt;p&gt;Carefully standardized facial images of 591 participants were taken in the
laboratory, while controlling for self-presentation, facial expression, head
orientation, and image properties. They were presented to human raters and a
facial recognition algorithm: both humans (r=.21) and the algorithm (r=.22)
could predict participants&apos; scores on a political orientation scale (Cronbach&apos;s
alpha=.94) decorrelated with age, gender, and ethnicity. These effects are on
par with how well job interviews predict job success, or alcohol drives
aggressiveness. Algorithm&apos;s predictive accuracy was even higher (r=.31) when it
leveraged information on participants&apos; age, gender, and ethnicity. Moreover,
the associations between facial appearance and political orientation seem to
generalize beyond our sample: The predictive model derived from standardized
images (while controlling for age, gender, and ethnicity) could predict
political orientation (r=.13) from naturalistic images of 3,401 politicians
from the U.S., UK, and Canada. The analysis of facial features associated with
political orientation revealed that conservatives tended to have larger lower
faces. The predictability of political orientation from standardized images has
critical implications for privacy, the regulation of facial recognition
technology, and understanding the origins and consequences of political
orientation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1&quot;&gt;Michal Kosinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khambatta_P/0/1/0/all/0/1&quot;&gt;Poruz Khambatta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yilun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02722">
<title>Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with Uncertainty. (arXiv:2305.02722v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02722</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation is an effective paradigm for boosting the performance
of pocket-size model, especially when multiple teacher models are available,
the student would break the upper limit again. However, it is not economical to
train diverse teacher models for the disposable distillation. In this paper, we
introduce a new concept dubbed Avatars for distillation, which are the
inference ensemble models derived from the teacher. Concretely, (1) For each
iteration of distillation training, various Avatars are generated by a
perturbation transformation. We validate that Avatars own higher upper limit of
working capacity and teaching ability, aiding the student model in learning
diverse and receptive knowledge perspectives from the teacher model. (2) During
the distillation, we propose an uncertainty-aware factor from the variance of
statistical differences between the vanilla teacher and Avatars, to adjust
Avatars&apos; contribution on knowledge transfer adaptively. Avatar Knowledge
Distillation AKD is fundamentally different from existing methods and refines
with the innovative view of unequal training. Comprehensive experiments
demonstrate the effectiveness of our Avatars mechanism, which polishes up the
state-of-the-art distillation methods for dense prediction without more extra
computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object
Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,
respectively. Code is available at https://github.com/Gumpest/AvatarKD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weihua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yichen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jian Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10808">
<title>Manifold-Aware Self-Training for Unsupervised Domain Adaptation on Regressing 6D Object Pose. (arXiv:2305.10808v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10808</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain gap between synthetic and real data in visual regression (e.g. 6D pose
estimation) is bridged in this paper via global feature alignment and local
refinement on the coarse classification of discretized anchor classes in target
space, which imposes a piece-wise target manifold regularization into
domain-invariant representation learning. Specifically, our method incorporates
an explicit self-supervised manifold regularization, revealing consistent
cumulative target dependency across domains, to a self-training scheme (e.g.
the popular Self-Paced Self-Training) to encourage more discriminative
transferable representations of regression tasks. Moreover, learning unified
implicit neural functions to estimate relative direction and distance of
targets to their nearest class bins aims to refine target classification
predictions, which can gain robust performance against inconsistent feature
scaling sensitive to UDA regressors. Experiment results on three public
benchmarks of the challenging 6D pose estimation task can verify the
effectiveness of our method, consistently achieving superior performance to the
state-of-the-art for UDA on 6D pose estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiehong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zelin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1&quot;&gt;Kui Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10925">
<title>Unsupervised Hyperspectral Pansharpening via Low-rank Diffusion Model. (arXiv:2305.10925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10925</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral pansharpening is a process of merging a high-resolution
panchromatic (PAN) image and a low-resolution hyperspectral (LRHS) image to
create a single high-resolution hyperspectral (HRHS) image. Existing
Bayesian-based HS pansharpening methods require designing handcraft image prior
to characterize the image features, and deep learning-based HS pansharpening
methods usually require a large number of paired training data and suffer from
poor generalization ability. To address these issues, in this work, we propose
a low-rank diffusion model for hyperspectral pansharpening by simultaneously
leveraging the power of the pre-trained deep diffusion model and better
generalization ability of Bayesian methods. Specifically, we assume that the
HRHS image can be recovered from the product of two low-rank tensors, i.e., the
base tensor and the coefficient matrix. The base tensor lies on the image field
and has a low spectral dimension. Thus, we can conveniently utilize a
pre-trained remote sensing diffusion model to capture its image structures.
Additionally, we derive a simple yet quite effective way to pre-estimate the
coefficient matrix from the observed LRHS image, which preserves the spectral
information of the HRHS. Experimental results demonstrate that the proposed
method performs better than some popular traditional approaches and gains
better generalization ability than some DL-based methods. The code is released
in https://github.com/xyrui/PLRDiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_X/0/1/0/all/0/1&quot;&gt;Xiangyu Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiangyong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1&quot;&gt;Li Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11203">
<title>PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11203</link>
<description rdf:parseType="Literal">&lt;p&gt;DNN pruning is a popular way to reduce the size of a model, improve the
inference latency, and minimize the power consumption on DNN accelerators.
However, existing approaches might be too complex, expensive or ineffective to
apply to a variety of vision/language tasks, DNN architectures and to honor
structured pruning constraints. In this paper, we propose an efficient yet
effective train-time pruning scheme, Parameter-free Differentiable Pruning
(PDP), which offers state-of-the-art qualities in model size, accuracy, and
training cost. PDP uses a dynamic function of weights during training to
generate soft pruning masks for the weights in a parameter-free manner for a
given pruning target. While differentiable, the simplicity and efficiency of
PDP make it universal enough to deliver state-of-the-art
random/structured/channel pruning results on various vision and natural
language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1
ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those
from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on
Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the
next best from the existing techniques shows 81.5% accuracy. In addition, PDP
can be applied to structured pruning, such as N:M pruning and channel pruning.
For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k
accuracy by over 3.6% over the state-of-the-art. For channel pruning of
ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsik Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1&quot;&gt;Saurabh Adya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1&quot;&gt;Devang Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12457">
<title>Unsupervised Multi-view Pedestrian Detection. (arXiv:2305.12457v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12457</link>
<description rdf:parseType="Literal">&lt;p&gt;With the prosperity of the video surveillance, multiple cameras have been
applied to accurately locate pedestrians in a specific area. However, previous
methods rely on the human-labeled annotations in every video frame and camera
view, leading to heavier burden than necessary camera calibration and
synchronization. Therefore, we propose in this paper an Unsupervised Multi-view
Pedestrian Detection approach (UMPD) to eliminate the need of annotations to
learn a multi-view pedestrian detector via 2D-3D mapping. 1) Firstly,
Semantic-aware Iterative Segmentation (SIS) is proposed to extract unsupervised
representations of multi-view images, which are converted into 2D pedestrian
masks as pseudo labels, via our proposed iterative PCA and zero-shot semantic
classes from vision-language models. 2) Secondly, we propose Geometry-aware
Volume-based Detector (GVD) to end-to-end encode multi-view 2D images into a 3D
volume to predict voxel-wise density and color via 2D-to-3D geometric
projection, trained by 3D-to-2D rendering losses with SIS pseudo labels. 3)
Thirdly, for better detection results, i.e., the 3D density projected on
Birds-Eye-View from GVD, we propose Vertical-aware BEV Regularization (VBR) to
constraint them to be vertical like the natural pedestrian poses. Extensive
experiments on popular multi-view pedestrian detection benchmarks Wildtrack,
Terrace, and MultiviewX, show that our proposed UMPD approach, as the first
fully-unsupervised method to our best knowledge, performs competitively to the
previous state-of-the-art supervised techniques. Code will be available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shiqi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xu-Cheng Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18007">
<title>Conditional Score Guidance for Text-Driven Image-to-Image Translation. (arXiv:2305.18007v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18007</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel algorithm for text-driven image-to-image translation based
on a pretrained text-to-image diffusion model. Our method aims to generate a
target image by selectively editing the regions of interest in a source image,
defined by a modifying text, while preserving the remaining parts. In contrast
to existing techniques that solely rely on a target prompt, we introduce a new
score function that additionally considers both the source image and the source
text prompt, tailored to address specific translation tasks. To this end, we
derive the conditional score function in a principled manner, decomposing it
into the standard score and a guiding term for target image generation. For the
gradient computation of the guiding term, we assume a Gaussian distribution of
the posterior distribution and estimate its mean and variance to adjust the
gradient without additional training. In addition, to improve the quality of
the conditional score guidance, we incorporate a simple yet effective mixup
technique, which combines two cross-attention maps derived from the source and
target latents. This strategy is effective for promoting a desirable fusion of
the invariant parts in the source image and the edited regions aligned with the
target prompt, leading to high-fidelity target image generation. Through
comprehensive experiments, we demonstrate that our approach achieves
outstanding image-to-image translation performance on various tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minsoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02080">
<title>Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02080</link>
<description rdf:parseType="Literal">&lt;p&gt;Various adaptation methods, such as LoRA, prompts, and adapters, have been
proposed to enhance the performance of pre-trained vision-language models in
specific domains. The robustness of these adaptation methods against
distribution shifts have not been studied. In this study, we assess the
robustness of 11 widely-used adaptation methods across 4 vision-language
datasets under multimodal corruptions. Concretely, we introduce 7 benchmark
datasets, including 96 visual and 87 textual corruptions, to investigate the
robustness of different adaptation methods, the impact of available adaptation
examples, and the influence of trainable parameter size during adaptation. Our
analysis reveals that: 1) Adaptation methods are more sensitive to text
corruptions than visual corruptions. 2) Full fine-tuning does not consistently
provide the highest robustness; instead, adapters can achieve better robustness
with comparable clean performance. 3) Contrary to expectations, our findings
indicate that increasing the number of adaptation data and parameters does not
guarantee enhanced robustness; instead it results in even lower robustness. We
hope this study could benefit future research in the development of robust
multimodal adaptation methods. The benchmark, code, and dataset used in this
study can be accessed at https://adarobustness.github.io .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02850">
<title>TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments. (arXiv:2306.02850v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02850</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the estimation of 3D human pose and shape (HPS) is rapidly
progressing, current methods still cannot reliably estimate moving humans in
global coordinates, which is critical for many applications. This is
particularly challenging when the camera is also moving, entangling human and
camera motion. To address these issues, we adopt a novel 5D representation
(space, time, and identity) that enables end-to-end reasoning about people in
scenes. Our method, called TRACE, introduces several novel architectural
components. Most importantly, it uses two new &quot;maps&quot; to reason about the 3D
trajectory of people over time in camera, and world, coordinates. An additional
memory unit enables persistent tracking of people even during long occlusions.
TRACE is the first one-stage method to jointly recover and track 3D humans in
global coordinates from dynamic cameras. By training it end-to-end, and using
full image information, TRACE achieves state-of-the-art performance on tracking
and HPS benchmarks. The code and dataset are released for research purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1&quot;&gt;Qian Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1&quot;&gt;Michael J. Black&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04216">
<title>MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos. (arXiv:2306.04216v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04216</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal summarization with multimodal output (MSMO) has emerged as a
promising research direction. Nonetheless, numerous limitations exist within
existing public MSMO datasets, including insufficient maintenance, data
inaccessibility, limited size, and the absence of proper categorization, which
pose significant challenges. To address these challenges and provide a
comprehensive dataset for this new direction, we have meticulously curated the
\textbf{MMSum} dataset. Our new dataset features (1) Human-validated summaries
for both video and textual content, providing superior human instruction and
labels for multimodal learning. (2) Comprehensively and meticulously arranged
categorization, spanning 17 principal categories and 170 subcategories to
encapsulate a diverse array of real-world scenarios. (3) Benchmark tests
performed on the proposed dataset to assess various tasks and methods,
including \textit{video summarization}, \textit{text summarization}, and
\textit{multimodal summarization}. To champion accessibility and collaboration,
we will release the \textbf{MMSum} dataset and the data collection tool as
fully open-source resources, fostering transparency and accelerating future
developments. Our project website can be found
at~\url{https://mmsum-dataset.github.io/}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;William Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aditesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_K/0/1/0/all/0/1&quot;&gt;Karthik Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Claire Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05238">
<title>SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth. (arXiv:2306.05238v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05238</link>
<description rdf:parseType="Literal">&lt;p&gt;Exploring robust and efficient association methods has always been an
important issue in multiple-object tracking (MOT). Although existing tracking
methods have achieved impressive performance, congestion and frequent
occlusions still pose challenging problems in multi-object tracking. We reveal
that performing sparse decomposition on dense scenes is a crucial step to
enhance the performance of associating occluded targets. To this end, we
propose a pseudo-depth estimation method for obtaining the relative depth of
targets from 2D images. Secondly, we design a depth cascading matching (DCM)
algorithm, which can use the obtained depth information to convert a dense
target set into multiple sparse target subsets and perform data association on
these sparse target subsets in order from near to far. By integrating the
pseudo-depth method and the DCM strategy into the data association process, we
propose a new tracker, called SparseTrack. SparseTrack provides a new
perspective for solving the challenging crowded scene MOT problem. Only using
IoU matching, SparseTrack achieves comparable performance with the
state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and
models are publicly available at \url{https://github.com/hustvl/SparseTrack}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zelin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiang Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07613">
<title>Revisiting and Advancing Adversarial Training Through A Simple Baseline. (arXiv:2306.07613v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07613</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we delve into the essential components of adversarial training
which is a pioneering defense technique against adversarial attacks. We
indicate that some factors such as the loss function, learning rate scheduler,
and data augmentation, which are independent of the model architecture, will
influence adversarial robustness and generalization. When these factors are
controlled for, we introduce a simple baseline approach, termed SimpleAT, that
performs competitively with recent methods and mitigates robust overfitting. We
conduct extensive experiments on CIFAR-10/100 and Tiny-ImageNet, which validate
the robustness of SimpleAT against state-of-the-art adversarial attackers such
as AutoAttack. Our results also demonstrate that SimpleAT exhibits good
performance in the presence of various image corruptions, such as those found
in the CIFAR-10-C. In addition, we empirically show that SimpleAT is capable of
reducing the variance in model predictions, which is considered the primary
contributor to robust overfitting. Our results also reveal the connections
between SimpleAT and many advanced state-of-the-art adversarial defense
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11925">
<title>LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching. (arXiv:2306.11925v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11925</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining large pre-trained models that can be fine-tuned to new tasks with
limited annotated samples has remained an open challenge for medical imaging
data. While pre-trained deep networks on ImageNet and vision-language
foundation models trained on web-scale data are prevailing approaches, their
effectiveness on medical tasks is limited due to the significant domain shift
between natural and medical images. To bridge this gap, we introduce LVM-Med,
the first family of deep networks trained on large-scale medical datasets. We
have collected approximately 1.3 million medical images from 55 publicly
available datasets, covering a large number of organs and modalities such as
CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art
self-supervised algorithms on this dataset and propose a novel self-supervised
contrastive learning algorithm using a graph-matching formulation. The proposed
approach makes three contributions: (i) it integrates prior pair-wise image
similarity metrics based on local and global information; (ii) it captures the
structural constraints of feature embeddings through a loss function
constructed via a combinatorial graph-matching objective; and (iii) it can be
trained efficiently end-to-end using modern gradient-estimation techniques for
black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream
medical tasks ranging from segmentation and classification to object detection,
and both for the in and out-of-distribution settings. LVM-Med empirically
outperforms a number of state-of-the-art supervised, self-supervised, and
foundation models. For challenging tasks such as Brain Tumor Classification or
Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models
trained on 1 billion masks by 6-7% while using only a ResNet-50.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duy M. H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hoang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diep_N/0/1/0/all/0/1&quot;&gt;Nghiem T. Diep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tan N. Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tri Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1&quot;&gt;Paul Swoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1&quot;&gt;Nhat Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1&quot;&gt;Shadi Albarqouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1&quot;&gt;Daniel Sonntag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12045">
<title>Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v5 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12045</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing paradigm.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a {\it natural paradigm}. We show
that TeCoS-LVM models can produce more realistic spike activities and
accurately fit spike statistics than powerful alternatives. Additionally,
learned TeCoS-LVM models can generalize well to longer time scales. Overall,
while remaining computationally tractable, our model effectively captures key
features of neural coding systems. It thus provides a useful tool for building
accurate predictive computational accounts for various sensory perception
circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gehua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Runhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huajin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12685">
<title>Rethinking the Backward Propagation for Adversarial Transferability. (arXiv:2306.12685v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12685</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer-based attacks generate adversarial examples on the surrogate model,
which can mislead other black-box models without access, making it promising to
attack real-world applications. Recently, several works have been proposed to
boost adversarial transferability, in which the surrogate model is usually
overlooked. In this work, we identify that non-linear layers (e.g., ReLU,
max-pooling, etc.) truncate the gradient during backward propagation, making
the gradient w.r.t. input image imprecise to the loss function. We hypothesize
and empirically validate that such truncation undermines the transferability of
adversarial examples. Based on these findings, we propose a novel method called
Backward Propagation Attack (BPA) to increase the relevance between the
gradient w.r.t. input image and loss function so as to generate adversarial
examples with higher transferability. Specifically, BPA adopts a non-monotonic
function as the derivative of ReLU and incorporates softmax with temperature to
smooth the derivative of max-pooling, thereby mitigating the information loss
during the backward propagation of gradients. Empirical results on the ImageNet
dataset demonstrate that not only does our method substantially boost the
adversarial transferability, but it is also general to existing transfer-based
attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_K/0/1/0/all/0/1&quot;&gt;Kangheng Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00583">
<title>A region and category confidence-based multi-task network for carotid ultrasound image segmentation and classification. (arXiv:2307.00583v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00583</link>
<description rdf:parseType="Literal">&lt;p&gt;The segmentation and classification of carotid plaques in ultrasound images
play important roles in the treatment of atherosclerosis and assessment for the
risk of stroke. Although deep learning methods have been used for carotid
plaque segmentation and classification, two-stage methods will increase the
complexity of the overall analysis and the existing multi-task methods ignored
the relationship between the segmentation and classification. These will lead
to suboptimal performance as valuable information might not be fully leveraged
across all tasks. Therefore, we propose a multi-task learning framework
(RCCM-Net) for ultrasound carotid plaque segmentation and classification, which
utilizes a region confidence module (RCM) and a sample category confidence
module (CCM) to exploit the correlation between these two tasks. The RCM
provides knowledge from the probability of plaque regions to the classification
task, while the CCM is designed to learn the categorical sample weight for the
segmentation task. A total of 1270 2D ultrasound images of carotid plaques were
collected from Zhongnan Hospital (Wuhan, China) for our experiments. The
results showed that the proposed method can improve both segmentation and
classification performance compared to existing single-task networks (i.e.,
SegNet, Deeplabv3+, UNet++, EfficientNet, Res2Net, RepVGG, DPN) and multi-task
algorithms (i.e., HRNet, MTANet), with an accuracy of 85.82% for classification
and a Dice-similarity-coefficient of 84.92% for segmentation. In the ablation
study, the results demonstrated that both the designed RCM and CCM were
beneficial in improving the network&apos;s performance. Therefore, we believe that
the proposed method could be useful for carotid plaque analysis in clinical
trials and practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gan_H/0/1/0/all/0/1&quot;&gt;Haitao Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Ran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yanghan Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Furong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xinyao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fenster_A/0/1/0/all/0/1&quot;&gt;Aaron Fenster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02421">
<title>DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models. (arXiv:2307.02421v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02421</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the ability of existing large-scale text-to-image (T2I) models to
generate high-quality images from detailed textual descriptions, they often
lack the ability to precisely edit the generated or real images. In this paper,
we propose a novel image editing method, DragonDiffusion, enabling Drag-style
manipulation on Diffusion models. Specifically, we construct classifier
guidance based on the strong correspondence of intermediate features in the
diffusion model. It can transform the editing signals into gradients via
feature correspondence loss to modify the intermediate representation of the
diffusion model. Based on this guidance strategy, we also build a multi-scale
guidance to consider both semantic and geometric alignment. Moreover, a
cross-branch self-attention is added to maintain the consistency between the
original image and the editing result. Our method, through an efficient design,
achieves various editing modes for the generated or real images, such as object
moving, object resizing, object appearance replacement, and content dragging.
It is worth noting that all editing and content preservation signals come from
the image itself, and the model does not require fine-tuning or additional
modules. Our source code will be available at
https://github.com/MC-E/DragonDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1&quot;&gt;Chong Mou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiechong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02500">
<title>Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection. (arXiv:2307.02500v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02500</link>
<description rdf:parseType="Literal">&lt;p&gt;With the perpetual increase of complexity of the state-of-the-art deep neural
networks, it becomes a more and more challenging task to maintain their
interpretability. Our work aims to evaluate the effects of adversarial training
utilized to produce robust models - less vulnerable to adversarial attacks. It
has been shown to make computer vision models more interpretable.
Interpretability is as essential as robustness when we deploy the models to the
real world. To prove the correlation between these two problems, we extensively
examine the models using local feature-importance methods (SHAP, Integrated
Gradients) and feature visualization techniques (Representation Inversion,
Class Specific Image Generation). Standard models, compared to robust are more
susceptible to adversarial attacks, and their learned representations are less
meaningful to humans. Conversely, these models focus on distinctive regions of
the images which support their predictions. Moreover, the features learned by
the robust model are closer to the real ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boychev_D/0/1/0/all/0/1&quot;&gt;Delyan Boychev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02783">
<title>UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering. (arXiv:2307.02783v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02783</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, artificial intelligence has played an important role in
medicine and disease diagnosis, with many applications to be mentioned, one of
which is Medical Visual Question Answering (MedVQA). By combining computer
vision and natural language processing, MedVQA systems can assist experts in
extracting relevant information from medical image based on a given question
and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023
challenge carried out visual question answering task in the gastrointestinal
domain, which includes gastroscopy and colonoscopy images. Our team approached
Task 1 of the challenge by proposing a multimodal learning method with image
enhancement to improve the VQA performance on gastrointestinal images. The
multimodal architecture is set up with BERT encoder and different pre-trained
vision models based on convolutional neural network (CNN) and Transformer
architecture for features extraction from question and endoscopy image. The
result of this study highlights the dominance of Transformer-based vision
models over the CNNs and demonstrates the effectiveness of the image
enhancement process, with six out of the eight vision models achieving better
F1-Score. Our best method, which takes advantages of BERT+BEiT fusion and image
enhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on the
development test set, while also producing good result on the private test set
with accuracy of 82.01%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thai_T/0/1/0/all/0/1&quot;&gt;Triet M. Thai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_A/0/1/0/all/0/1&quot;&gt;Anh T. Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tieu_H/0/1/0/all/0/1&quot;&gt;Hao K. Tieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_L/0/1/0/all/0/1&quot;&gt;Linh N.P. Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thien T.B. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05832">
<title>Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has experienced an increasing surge of interest due to recent
advancements in image processing and deep learning-based techniques. View
planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish view planning for active
reconstruction; the former approach poses a challenge for complex or newly
identified targets while the latter is computationally expensive. In this work,
we present Bag-of-Views (BoV), a fully appearance-based model used to assign
utility to the captured views for both offline dataset refinement and online
next-best-view (NBV) planning applications targeting the task of 3D
reconstruction. With this contribution, we also developed the View Planning
Toolbox (VPT), a lightweight package for training and testing machine
learning-based view planning frameworks, custom view dataset generation of
arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a
BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of
our model in reducing the number of required views for high-quality
reconstructions in dataset refinement and NBV planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazani_S/0/1/0/all/0/1&quot;&gt;Sara Hatami Gazani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucsok_M/0/1/0/all/0/1&quot;&gt;Matthew Tucsok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mantegh_I/0/1/0/all/0/1&quot;&gt;Iraj Mantegh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07813">
<title>TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation. (arXiv:2307.07813v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07813</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent edge vision tasks encounter the critical challenge of ensuring
power and latency efficiency due to the typically heavy computational load they
impose on edge platforms.This work leverages one of the first &quot;AI in sensor&quot;
vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power
end-to-end edge vision applications. We evaluate the IMX500 and compare it to
other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by
exploring gaze estimation as a case study. We propose TinyTracker, a highly
efficient, fully quantized model for 2D gaze estimation designed to maximize
the performance of the edge vision systems considered in this study.
TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]
without significant loss in gaze estimation accuracy (maximum of 0.16 cm when
fully quantized). TinyTracker&apos;s deployment on the Sony IMX500 vision sensor
results in end-to-end latency of around 19ms. The camera takes around 17.9ms to
read, process and transmit the pixels to the accelerator. The inference time of
the network is 0.86ms with an additional 0.24 ms for retrieving the results
from the sensor. The overall energy consumption of the end-to-end system is 4.9
mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is
1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ
VS 34.2mJ)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1&quot;&gt;Thomas Ruegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10373">
<title>TokenFlow: Consistent Diffusion Features for Consistent Video Editing. (arXiv:2307.10373v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10373</link>
<description rdf:parseType="Literal">&lt;p&gt;The generative AI revolution has recently expanded to videos. Nevertheless,
current state-of-the-art video models are still lagging behind image models in
terms of visual quality and user control over the generated content. In this
work, we present a framework that harnesses the power of a text-to-image
diffusion model for the task of text-driven video editing. Specifically, given
a source video and a target text-prompt, our method generates a high-quality
video that adheres to the target text, while preserving the spatial layout and
motion of the input video. Our method is based on a key observation that
consistency in the edited video can be obtained by enforcing consistency in the
diffusion feature space. We achieve this by explicitly propagating diffusion
features based on inter-frame correspondences, readily available in the model.
Thus, our framework does not require any training or fine-tuning, and can work
in conjunction with any off-the-shelf text-to-image editing method. We
demonstrate state-of-the-art editing results on a variety of real-world videos.
Webpage: https://diffusion-tokenflow.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1&quot;&gt;Michal Geyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1&quot;&gt;Omer Bar-Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1&quot;&gt;Shai Bagon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1&quot;&gt;Tali Dekel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06377">
<title>CATS v2: Hybrid encoders for robust medical segmentation. (arXiv:2308.06377v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06377</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have exhibited strong performance in
medical image segmentation tasks by capturing high-level (local) information,
such as edges and textures. However, due to the limited field of view of
convolution kernel, it is hard for CNNs to fully represent global information.
Recently, transformers have shown good performance for medical image
segmentation due to their ability to better model long-range dependencies.
Nevertheless, transformers struggle to capture high-level spatial features as
effectively as CNNs. A good segmentation model should learn a better
representation from local and global features to be both precise and
semantically accurate. In our previous work, we proposed CATS, which is a
U-shaped segmentation network augmented with transformer encoder. In this work,
we further extend this model and propose CATS v2 with hybrid encoders.
Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to
a transformer path with a shifted window, which better leverage both local and
global information to produce robust 3D medical image segmentation. We fuse the
information from the convolutional encoder and the transformer at the skip
connections of different resolutions to form the final segmentation. The
proposed method is evaluated on two public challenge datasets: Cross-Modality
Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon
(MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively.
Compared with the state-of-the-art methods, our approach demonstrates superior
performance in terms of higher Dice scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11408">
<title>MatFuse: Controllable Material Generation with Diffusion Models. (arXiv:2308.11408v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11408</link>
<description rdf:parseType="Literal">&lt;p&gt;Creating high-quality materials in computer graphics is a challenging and
time-consuming task, which requires great expertise. To simply this process, we
introduce MatFuse, a unified approach that harnesses the generative power of
diffusion models to simplify the creation of SVBRDF maps. Our pipeline
integrates multiple sources of conditioning, including color palettes,
sketches, text, and pictures, for a fine-grained control and flexibility in
material synthesis. This design enables the combination of diverse information
sources (e.g., sketch + text), enhancing creative possibilities in line with
the principle of compositionality. Additionally, we propose a multi-encoder
compression model with a two-fold purpose: it improves reconstruction
performance by learning a separate latent representation for each map and
enables a map-level material editing capabilities. We demonstrate the
effectiveness of MatFuse under multiple conditioning settings and explore the
potential of material editing. We also quantitatively assess the quality of the
generated materials in terms of CLIP-IQA and FID scores. \\ Source code for
training MatFuse will be made publically available at
https://gvecchio.com/matfuse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vecchio_G/0/1/0/all/0/1&quot;&gt;Giuseppe Vecchio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sortino_R/0/1/0/all/0/1&quot;&gt;Renato Sortino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1&quot;&gt;Simone Palazzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11887">
<title>A Unified Framework for 3D Point Cloud Visual Grounding. (arXiv:2308.11887v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11887</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to its precise spatial referencing, 3D point cloud visual grounding is
essential for deep understanding and dynamic interaction in 3D environments,
encompassing 3D Referring Expression Comprehension (3DREC) and Segmentation
(3DRES). We argue that 3DREC and 3DRES should be unified in one framework,
which is also a natural progression in the community. To explain, 3DREC help
3DRES locate the referent, while 3DRES also facilitate 3DREC via more
fine-grained language-visual alignment. To achieve this, this paper takes the
initiative step to integrate 3DREC and 3DRES into a unified framework, termed
3D Referring Transformer (3DRefTR). Its key idea is to build upon a mature
3DREC model and leverage ready query embeddings and visual tokens from the
3DREC model to construct a dedicated mask branch. Specially, we propose
Superpoint Mask Branch, which serves a dual purpose: i) By harnessing on the
inherent association between the superpoints and point cloud, it eliminates the
heavy computational overhead on the high-resolution visual features for
upsampling; ii) By leveraging the heterogeneous CPU-GPU parallelism, while the
GPU is occupied generating visual and language tokens, the CPU concurrently
produces superpoints, equivalently accomplishing the upsampling computation.
This elaborate design enables 3DRefTR to achieve both well-performing 3DRES and
3DREC capacities with only a 6% additional latency compared to the original
3DREC model. Empirical evaluations affirm the superiority of 3DRefTR.
Specifically, on the ScanRefer dataset, 3DRefTR surpasses the state-of-the-art
3DRES method by 12.43% in mIoU and improves upon the SOTA 3DREC method by 0.6%
Acc@0.25IoU. The codes and models will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haojia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yongdong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lijiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1&quot;&gt;Fei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1&quot;&gt;Taisong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Donghao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Liujuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12313">
<title>Gaze Estimation on Spresense. (arXiv:2308.12313v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12313</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaze estimation is a valuable technology with numerous applications in fields
such as human-computer interaction, virtual reality, and medicine. This report
presents the implementation of a gaze estimation system using the Sony
Spresense microcontroller board and explores its performance in latency,
MAC/cycle, and power consumption. The report also provides insights into the
system&apos;s architecture, including the gaze estimation model used. Additionally,
a demonstration of the system is presented, showcasing its functionality and
performance. Our lightweight model TinyTrackerS is a mere 169Kb in size, using
85.8k parameters and runs on the Spresense platform at 3 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1&quot;&gt;Thomas Ruegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronco_A/0/1/0/all/0/1&quot;&gt;Andrea Ronco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12634">
<title>Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cersovsky_J/0/1/0/all/0/1&quot;&gt;Josef Cersovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1&quot;&gt;Dagmar Kainmueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoehne_J/0/1/0/all/0/1&quot;&gt;Johannes Hoehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12914">
<title>3D Pose Nowcasting: Forecast the Future to Improve the Present. (arXiv:2308.12914v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12914</link>
<description rdf:parseType="Literal">&lt;p&gt;Technologies to enable safe and effective collaboration and coexistence
between humans and robots have gained significant importance in the last few
years. A critical component useful for realizing this collaborative paradigm is
the understanding of human and robot 3D poses using non-invasive systems.
Therefore, in this paper, we propose a novel vision-based system leveraging
depth data to accurately establish the 3D locations of skeleton joints.
Specifically, we introduce the concept of Pose Nowcasting, denoting the
capability of the proposed system to enhance its current pose estimation
accuracy by jointly learning to forecast future poses. The experimental
evaluation is conducted on two different datasets, providing accurate and
real-time performance and confirming the validity of the proposed method on
both the robotic and human scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoni_A/0/1/0/all/0/1&quot;&gt;Alessandro Simoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchetti_F/0/1/0/all/0/1&quot;&gt;Francesco Marchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1&quot;&gt;Guido Borghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1&quot;&gt;Federico Becattini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1&quot;&gt;Lorenzo Seidenari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vezzani_R/0/1/0/all/0/1&quot;&gt;Roberto Vezzani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13779">
<title>Zero-Shot Edge Detection with SCESAME: Spectral Clustering-based Ensemble for Segment Anything Model Estimation. (arXiv:2308.13779v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13779</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel zero-shot edge detection with SCESAME, which
stands for Spectral Clustering-based Ensemble for Segment Anything Model
Estimation, based on the recently proposed Segment Anything Model (SAM). SAM is
a foundation model for segmentation tasks, and one of the interesting
applications of SAM is Automatic Mask Generation (AMG), which generates
zero-shot segmentation masks of an entire image. AMG can be applied to edge
detection, but suffers from the problem of overdetecting edges. Edge detection
with SCESAME overcomes this problem by three steps: (1) eliminating small
generated masks, (2) combining masks by spectral clustering, taking into
account mask positions and overlaps, and (3) removing artifacts after edge
detection. We performed edge detection experiments on two datasets, BSDS500 and
NYUDv2. Although our zero-shot approach is simple, the experimental results on
BSDS500 showed almost identical performance to human performance and CNN-based
methods from seven years ago. In the NYUDv2 experiments, it performed almost as
well as recent CNN-based methods. These results indicate that our method
effectively enhances the utility of SAM and can be a new direction in zero-shot
edge detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamagiwa_H/0/1/0/all/0/1&quot;&gt;Hiroaki Yamagiwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takase_Y/0/1/0/all/0/1&quot;&gt;Yusuke Takase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambe_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Kambe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakamoto_R/0/1/0/all/0/1&quot;&gt;Ryosuke Nakamoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15216">
<title>On-the-Fly Guidance Training for Medical Image Registration. (arXiv:2308.15216v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15216</link>
<description rdf:parseType="Literal">&lt;p&gt;This research explores a novel approach in the realm of learning-based image
registration, addressing the limitations inherent in weakly-supervised and
unsupervised methods. Weakly-supervised techniques depend heavily on scarce
labeled data, while unsupervised strategies rely on indirect measures of
accuracy through image similarity. Notably, traditional supervised learning is
not utilized due to the lack of precise deformation ground-truth in medical
imaging. Our study introduces a unique training framework with
\textbf{On-the-Fly Guidance} (OFG) to enhance existing models. This framework,
during training, generates pseudo-ground truth a few steps ahead by refining
the current deformation prediction with our custom optimizer. This
pseudo-ground truth then serves to directly supervise the model in a supervised
learning context. The process involves optimizing the predicted deformation
with a limited number of steps, ensuring training efficiency and setting
achievable goals for each training phase. OFG notably boosts the precision of
existing image registration techniques while maintaining the speed of
learning-based methods. We assessed our approach using various pseudo-ground
truth generation strategies, including predictions and optimized outputs from
established registration models. Our experiments spanned three benchmark
datasets and three cutting-edge models, with OFG demonstrating significant and
consistent enhancements, surpassing previous state-of-the-arts in the field.
OFG offers an easily integrable plug-and-play solution to enhance the training
effectiveness of learning-based image registration models. Code at
https://github.com/miraclefactory/on-the-fly-guidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yicheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shengxiang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yuelin Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07510">
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent&apos;s morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
Project page at https://chengkaiacademycity.github.io/EnvAwareAfford/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Guanqi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08387">
<title>Efficient Graphics Representation with Differentiable Indirection. (arXiv:2309.08387v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08387</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce differentiable indirection -- a novel learned primitive that
employs differentiable multi-scale lookup tables as an effective substitute for
traditional compute and data operations across the graphics pipeline. We
demonstrate its flexibility on a number of graphics tasks, i.e., geometric and
image representation, texture mapping, shading, and radiance field
representation. In all cases, differentiable indirection seamlessly integrates
into existing architectures, trains rapidly, and yields both versatile and
efficient results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1&quot;&gt;Sayantan Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1&quot;&gt;Carl Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengqin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08402">
<title>3D SA-UNet: 3D Spatial Attention UNet with 3D ASPP for White Matter Hyperintensities Segmentation. (arXiv:2309.08402v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08402</link>
<description rdf:parseType="Literal">&lt;p&gt;White Matter Hyperintensity (WMH) is an imaging feature related to various
diseases such as dementia and stroke. Accurately segmenting WMH using computer
technology is crucial for early disease diagnosis. However, this task remains
challenging due to the small lesions with low contrast and high discontinuity
in the images, which contain limited contextual and spatial information. To
address this challenge, we propose a deep learning model called 3D Spatial
Attention U-Net (3D SA-UNet) for automatic WMH segmentation using only Fluid
Attenuation Inversion Recovery (FLAIR) scans. The 3D SA-UNet introduces a 3D
Spatial Attention Module that highlights important lesion features, such as
WMH, while suppressing unimportant regions. Additionally, to capture features
at different scales, we extend the Atrous Spatial Pyramid Pooling (ASPP) module
to a 3D version, enhancing the segmentation performance of the network. We
evaluate our method on publicly available dataset and demonstrate the
effectiveness of 3D spatial attention module and 3D ASPP in WMH segmentation.
Through experimental results, it has been demonstrated that our proposed 3D
SA-UNet model achieves higher accuracy compared to other state-of-the-art 3D
convolutional neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Changlu Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16661">
<title>SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16661</link>
<description rdf:parseType="Literal">&lt;p&gt;Microscopic image segmentation is a challenging task, wherein the objective
is to assign semantic labels to each pixel in a given microscopic image. While
convolutional neural networks (CNNs) form the foundation of many existing
frameworks, they often struggle to explicitly capture long-range dependencies.
Although transformers were initially devised to address this issue using
self-attention, it has been proven that both local and global features are
crucial for addressing diverse challenges in microscopic images, including
variations in shape, size, appearance, and target region density. In this
paper, we introduce SA2-Net, an attention-guided method that leverages
multi-scale feature learning to effectively handle diverse structures within
microscopic images. Specifically, we propose scale-aware attention (SA2) module
designed to capture inherent variations in scales and shapes of microscopic
regions, such as cells, for accurate segmentation. This module incorporates
local attention at each level of multi-stage features, as well as global
attention across multiple resolutions. Furthermore, we address the issue of
blurred region boundaries (e.g., cell boundaries) by introducing a novel
upsampling strategy called the Adaptive Up-Attention (AuA) module. This module
enhances the discriminative ability for improved localization of microscopic
regions using an explicit attention mechanism. Extensive experiments on five
challenging datasets demonstrate the benefits of our SA2-Net model. Our source
code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiaz_M/0/1/0/all/0/1&quot;&gt;Mustansar Fiaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1&quot;&gt;Moein Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16738">
<title>ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens. (arXiv:2309.16738v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16738</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a versatile language-image model is computationally prohibitive
under a limited computing budget. This paper delves into the \emph{efficient
language-image pre-training}, an area that has received relatively little
attention despite its importance in reducing computational cost and footprint.
To that end, we propose a vision token pruning and merging method ELIP, to
remove less influential tokens based on the supervision of language outputs.
Our method is designed with several strengths, such as being
computation-efficient, memory-efficient, and trainable-parameter-free, and is
distinguished from previous vision-only token pruning approaches by its
alignment with task objectives. We implement this method in a progressively
pruning manner using several sequential blocks. To evaluate its generalization
performance, we apply ELIP to three commonly used language-image pre-training
models and utilize public image-caption pairs with 4M images for pre-training.
Our experiments demonstrate that with the removal of ~30$\%$ vision tokens
across 12 ViT layers, ELIP maintains significantly comparable performance with
baselines ($\sim$0.32 accuracy drop on average) over various downstream tasks
including cross-modal retrieval, VQA, image captioning, \emph{etc}. In
addition, the spared GPU resources by our ELIP allow us to scale up with larger
batch sizes, thereby accelerating model pre-training and even sometimes
enhancing downstream model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yangyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1&quot;&gt;Yongkang Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00641">
<title>RegBN: Batch Normalization of Multimodal Data with Regularization. (arXiv:2310.00641v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00641</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a surge of interest in integrating
high-dimensional data captured by multisource sensors, driven by the impressive
success of neural networks in the integration of multimodal data. However, the
integration of heterogeneous multimodal data poses a significant challenge, as
confounding effects and dependencies among such heterogeneous data sources
introduce unwanted variability and bias, leading to suboptimal performance of
multimodal models. Therefore, it becomes crucial to normalize the low- or
high-level features extracted from data modalities before their fusion takes
place. This paper introduces a novel approach for the normalization of
multimodal data, called RegBN, that incorporates regularization. RegBN uses the
Frobenius norm as a regularizer term to address the side effects of confounders
and underlying dependencies among different data sources. The proposed method
generalizes well across multiple modalities and eliminates the need for
learnable parameters, simplifying training and inference. We validate the
effectiveness of RegBN on eight databases from five research areas,
encompassing diverse modalities such as language, audio, image, video, depth,
tabular, and 3D MRI. The proposed method demonstrates broad applicability
across different architectures such as multilayer perceptrons, convolutional
neural networks, and vision transformers, enabling effective normalization of
both low- and high-level features in multimodal neural networks. RegBN is
available at \url{https://github.com/mogvision/regbn}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghahremani_M/0/1/0/all/0/1&quot;&gt;Morteza Ghahremani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1&quot;&gt;Christian Wachinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02230">
<title>Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02230</link>
<description rdf:parseType="Literal">&lt;p&gt;Spurious correlations in the data, where multiple cues are predictive of the
target labels, often lead to shortcut learning phenomena, where a model may
rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this
work, we propose an ensemble diversification framework exploiting the
generation of synthetic counterfactuals using Diffusion Probabilistic Models
(DPMs). We discover that DPMs have the inherent capability to represent
multiple visual cues independently, even when they are largely correlated in
the training data. We leverage this characteristic to encourage model diversity
and empirically show the efficacy of the approach with respect to several
diversification objectives. We show that diffusion-guided diversification can
lead models to avert attention from shortcut cues, achieving ensemble diversity
performance comparable to previous methods requiring additional data
collection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scimeca_L/0/1/0/all/0/1&quot;&gt;Luca Scimeca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1&quot;&gt;Alexander Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Armand Mihai Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1&quot;&gt;Damien Teney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03358">
<title>Enhancing Robust Representation in Adversarial Training: Alignment and Exclusion Criteria. (arXiv:2310.03358v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03358</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Nuoyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04741">
<title>Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04741</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthes_D/0/1/0/all/0/1&quot;&gt;Daniel Anthes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorat_S/0/1/0/all/0/1&quot;&gt;Sushrut Thorat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konig_P/0/1/0/all/0/1&quot;&gt;Peter K&amp;#xf6;nig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kietzmann_T/0/1/0/all/0/1&quot;&gt;Tim C. Kietzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10123">
<title>AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion. (arXiv:2310.10123v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we aim to solve complex real-world image restoration
situations, in which, one image may have a variety of unknown degradations. To
this end, we propose an all-in-one image restoration framework with latent
diffusion (AutoDIR), which can automatically detect and address multiple
unknown degradations. Our framework first utilizes a Blind Image Quality
Assessment Module (BIQA) to automatically detect and identify the unknown
dominant image degradation type of the image. Then, an All-in-One Image
Refinement (AIR) Module handles multiple kinds of degradation image restoration
with the guidance of BIQA. Finally, a Structure Correction Module (SCM) is
proposed to recover the image details distorted by AIR. Our comprehensive
evaluation demonstrates that AutoDIR outperforms state-of-the-art approaches by
achieving superior restoration results while supporting a wider range of tasks.
Notably, AutoDIR is also the first method to automatically handle real-scenario
images with multiple unknown degradations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yitong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tianfan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinwei Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10700">
<title>PELA: Learning Parameter-Efficient Models with Low-Rank Approximation. (arXiv:2310.10700v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10700</link>
<description rdf:parseType="Literal">&lt;p&gt;Applying a pre-trained large model to downstream tasks is prohibitive under
resource-constrained conditions. Recent dominant approaches for addressing
efficiency issues involve adding a few learnable parameters to the fixed
backbone model. This strategy, however, leads to more challenges in loading
large models for downstream fine-tuning with limited resources. In this paper,
we propose a novel method for increasing the parameter efficiency of
pre-trained models by introducing an intermediate pre-training stage. To this
end, we first employ low-rank approximation to compress the original large
model and then devise a feature distillation module and a weight perturbation
regularization module. These modules are specifically designed to enhance the
low-rank model. In particular, we update only the low-rank model while freezing
the backbone parameters during pre-training. This allows for direct and
efficient utilization of the low-rank model for downstream fine-tuning tasks.
The proposed method achieves both efficiencies in terms of required parameters
and computation time while maintaining comparable results with minimal
modifications to the backbone architecture. Specifically, when applied to three
vision-only and one vision-language Transformer models, our approach often
demonstrates a merely $\sim$0.6 point decrease in performance while reducing
the original parameter size by 1/3 to 2/3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yangyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10861">
<title>SoybeanNet: Transformer-Based Convolutional Neural Network for Soybean Pod Counting from Unmanned Aerial Vehicle (UAV) Images. (arXiv:2310.10861v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10861</link>
<description rdf:parseType="Literal">&lt;p&gt;Soybeans are a critical source of food, protein and oil, and thus have
received extensive research aimed at enhancing their yield, refining
cultivation practices, and advancing soybean breeding techniques. Within this
context, soybean pod counting plays an essential role in understanding and
optimizing production. Despite recent advancements, the development of a robust
pod-counting algorithm capable of performing effectively in real-field
conditions remains a significant challenge This paper presents a pioneering
work of accurate soybean pod counting utilizing unmanned aerial vehicle (UAV)
images captured from actual soybean fields in Michigan, USA. Specifically, this
paper presents SoybeanNet, a novel point-based counting network that harnesses
powerful transformer backbones for simultaneous soybean pod counting and
localization with high accuracy. In addition, a new dataset of UAV-acquired
images for soybean pod counting was created and open-sourced, consisting of 113
drone images with more than 260k manually annotated soybean pods captured under
natural lighting conditions. Through comprehensive evaluations, SoybeanNet
demonstrated superior performance over five state-of-the-art approaches when
tested on the collected images. Remarkably, SoybeanNet achieved a counting
accuracy of $84.51\%$ when tested on the testing dataset, attesting to its
efficacy in real-world scenarios. The publication also provides both the source
code (\url{https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images})
and the labeled soybean dataset
(\url{https://www.kaggle.com/datasets/jiajiali/uav-based-soybean-pod-images}),
offering a valuable resource for future research endeavors in soybean pod
counting and related fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magar_R/0/1/0/all/0/1&quot;&gt;Raju Thada Magar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Feng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dechun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weichao Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaojian Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13849">
<title>A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains. (arXiv:2310.13849v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13849</link>
<description rdf:parseType="Literal">&lt;p&gt;The human visual system uses two parallel pathways for spatial processing and
object recognition. In contrast, computer vision systems tend to use a single
feedforward pathway, rendering them less robust, adaptive, or efficient than
human vision. To bridge this gap, we developed a dual-stream vision model
inspired by the human eyes and brain. At the input level, the model samples two
complementary visual patterns to mimic how the human eyes use magnocellular and
parvocellular retinal ganglion cells to separate retinal inputs to the brain.
At the backend, the model processes the separate input patterns through two
branches of convolutional neural networks (CNN) to mimic how the human brain
uses the dorsal and ventral cortical pathways for parallel visual processing.
The first branch (WhereCNN) samples a global view to learn spatial attention
and control eye movements. The second branch (WhatCNN) samples a local view to
represent the object around the fixation. Over time, the two branches interact
recurrently to build a scene representation from moving fixations. We compared
this model with the human brains processing the same movie and evaluated their
functional alignment by linear transformation. The WhereCNN and WhatCNN
branches were found to differentially match the dorsal and ventral pathways of
the visual cortex, respectively, primarily due to their different learning
objectives. These model-based results lead us to speculate that the distinct
responses and representations of the ventral and dorsal streams are more
influenced by their distinct goals in visual attention and object recognition
than by their specific bias or selectivity in retinal inputs. This dual-stream
model takes a further step in brain-inspired computer vision, enabling parallel
neural networks to actively explore and understand the visual surroundings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minkyu Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kuan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaokai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yizhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15308">
<title>SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15308</link>
<description rdf:parseType="Literal">&lt;p&gt;The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that absorbs their
expertise. Our method integrates techniques of multi-task learning, continual
learning, and distillation. Further, it demands significantly less
computational cost compared to traditional multi-task training from scratch,
and it only needs a small fraction of the pre-training datasets that were
initially used to train individual models. By applying our method to SAM and
CLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM
and CLIP into a single vision transformer. Compared with deploying SAM and CLIP
independently, our merged model, SAM-CLIP, reduces storage and compute costs
for inference, making it well-suited for edge device applications. We show that
SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also
introduces synergistic functionalities, notably in zero-shot semantic
segmentation, where SAM-CLIP establishes new state-of-the-art results on 5
benchmarks. It outperforms previous models that are specifically designed for
this task by a large margin, including +6.8% and +5.9% mean IoU improvement on
Pascal-VOC and COCO-Stuff datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1&quot;&gt;Pavan Kumar Anasosalu Vasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1&quot;&gt;Fartash Faghri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Sachin Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1&quot;&gt;Mohammad Rastegari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1&quot;&gt;Oncel Tuzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouransari_H/0/1/0/all/0/1&quot;&gt;Hadi Pouransari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15778">
<title>Preserving Patient Privacy in MRI Scans: A Comprehensive Approach with 3D Masked Autoencoders. (arXiv:2310.15778v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15778</link>
<description rdf:parseType="Literal">&lt;p&gt;MRI scans provide valuable medical information, however they also contain
sensitive and personally identifiable information (PII) that needs to be
protected. Whereas MRI metadata is easily sanitized, MRI image data is a
privacy risk because it contains information to render highly-realistic 3D
visualizations of a patient&apos;s head, enabling malicious actors to possibly
identify the subject by cross-referencing a database. Data anonymization and
de-identification is concerned with ensuring the privacy and confidentiality of
individuals&apos; personal information. Traditional MRI de-identification methods
remove privacy-sensitive parts (e.g. eyes, nose etc.) from a given scan. This
comes at the expense of introducing a domain shift that can throw off
downstream analyses. Recently, a GAN-based approach was proposed to de-identify
a patient&apos;s scan by remodeling it (\eg changing the face) rather than by
removing parts. In this work, we propose CP-MAE, a model that de-identifies the
face using masked autoencoders and that outperforms all previous approaches in
terms of downstream task performance as well as de-identification. With our
method we are able to synthesize scans of resolution up to $256^3$ (previously
$128^3$) which constitutes an eight-fold increase in the number of voxels.
Using our construction we were able to design a system that exhibits a highly
robust training stage, making it easy to fit the network on novel data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goten_L/0/1/0/all/0/1&quot;&gt;Lennart Alexander Van der Goten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18936">
<title>Adversarial Examples Are Not Real Features. (arXiv:2310.18936v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18936</link>
<description rdf:parseType="Literal">&lt;p&gt;The existence of adversarial examples has been a mystery for years and
attracted much interest. A well-known theory by \citet{ilyas2019adversarial}
explains adversarial vulnerability from a data perspective by showing that one
can extract non-robust features from adversarial examples and these features
alone are useful for classification. However, the explanation remains quite
counter-intuitive since non-robust features are mostly noise features to
humans. In this paper, we re-examine the theory from a larger context by
incorporating multiple learning paradigms. Notably, we find that contrary to
their good usefulness under supervised learning, non-robust features attain
poor usefulness when transferred to other self-supervised learning paradigms,
such as contrastive learning, masked image modeling, and diffusion models. It
reveals that non-robust features are not really as useful as robust or natural
features that enjoy good transferability between these paradigms. Meanwhile,
for robustness, we also show that naturally trained encoders from robust
features are largely non-robust under AutoAttack. Our cross-paradigm
examination suggests that the non-robust features are not really useful but
more like paradigm-wise shortcuts, and robust features alone might be
insufficient to attain reliable model robustness. Code is available at
\url{https://github.com/PKU-ML/AdvNotRealFeatures}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19130">
<title>Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender. (arXiv:2310.19130v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19130</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the impact of objects on gender bias in image
captioning systems. Our results show that only gender-specific objects have a
strong gender bias (e.g., women-lipstick). In addition, we propose a visual
semantic-based gender score that measures the degree of bias and can be used as
a plug-in for any image captioning system. Our experiments demonstrate the
utility of the gender score, since we observe that our score can measure the
bias relation between a caption and its related gender; therefore, our score
can be used as an additional metric to the existing Object Gender Co-Occ
approach. Code and data are publicly available at
\url{https://github.com/ahmedssabir/GenderScore}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_A/0/1/0/all/0/1&quot;&gt;Ahmed Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padro_L/0/1/0/all/0/1&quot;&gt;Llu&amp;#xed;s Padr&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19909">
<title>Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks. (arXiv:2310.19909v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19909</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network based computer vision systems are typically built on a
backbone, a pretrained or randomly initialized feature extractor. Several years
ago, the default option was an ImageNet-trained convolutional neural network.
However, the recent past has seen the emergence of countless backbones
pretrained using various algorithms and datasets. While this abundance of
choice has led to performance increases for a range of systems, it is difficult
for practitioners to make informed decisions about which backbone to choose.
Battle of the Backbones (BoB) makes this choice easier by benchmarking a
diverse suite of pretrained models, including vision-language models, those
trained via self-supervised learning, and the Stable Diffusion backbone, across
a diverse set of computer vision tasks ranging from classification to object
detection to OOD generalization and more. Furthermore, BoB sheds light on
promising directions for the research community to advance computer vision by
illuminating strengths and weakness of existing approaches through a
comprehensive analysis conducted on more than 1500 training runs. While vision
transformers (ViTs) and self-supervised learning (SSL) are increasingly
popular, we find that convolutional neural networks pretrained in a supervised
fashion on large training sets still perform best on most tasks among the
models we consider. Moreover, in apples-to-apples comparisons on the same
architectures and similarly sized pretraining datasets, we find that SSL
backbones are highly competitive, indicating that future works should perform
SSL pretraining with advanced architectures and larger pretraining datasets. We
release the raw results of our experiments along with code that allows
researchers to put their own backbones through the gauntlet here:
https://github.com/hsouri/Battle-of-the-Backbones
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1&quot;&gt;Hossein Souri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Renkun Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1&quot;&gt;Manli Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1&quot;&gt;Viraj Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1&quot;&gt;Gowthami Somepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1&quot;&gt;Prithvijit Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mark Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bardes_A/0/1/0/all/0/1&quot;&gt;Adrien Bardes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01310">
<title>Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01310</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1&quot;&gt;Badri N. Patro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1&quot;&gt;Vijay Srinivas Agneeswaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01520">
<title>4D-Former: Multimodal 4D Panoptic Segmentation. (arXiv:2311.01520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;4D panoptic segmentation is a challenging but practically useful task that
requires every point in a LiDAR point-cloud sequence to be assigned a semantic
class label, and individual objects to be segmented and tracked over time.
Existing approaches utilize only LiDAR inputs which convey limited information
in regions with point sparsity. This problem can, however, be mitigated by
utilizing RGB camera images which offer appearance-based information that can
reinforce the geometry-based LiDAR features. Motivated by this, we propose
4D-Former: a novel method for 4D panoptic segmentation which leverages both
LiDAR and image modalities, and predicts semantic masks as well as temporally
consistent object masks for the input point-cloud sequence. We encode semantic
classes and objects using a set of concise queries which absorb feature
information from both data modalities. Additionally, we propose a learned
mechanism to associate object tracks over time which reasons over both
appearance and spatial location. We apply 4D-Former to the nuScenes and
SemanticKITTI datasets where it achieves state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athar_A/0/1/0/all/0/1&quot;&gt;Ali Athar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1&quot;&gt;Enxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02358">
<title>Domain Transfer in Latent Space (DTLS) Wins on Image Super-Resolution -- a Non-Denoising Model. (arXiv:2311.02358v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02358</link>
<description rdf:parseType="Literal">&lt;p&gt;Large scale image super-resolution is a challenging computer vision task,
since vast information is missing in a highly degraded image, say for example
forscale x16 super-resolution. Diffusion models are used successfully in recent
years in extreme super-resolution applications, in which Gaussian noise is used
as a means to form a latent photo-realistic space, and acts as a link between
the space of latent vectors and the latent photo-realistic space. There are
quite a few sophisticated mathematical derivations on mapping the statistics of
Gaussian noises making Diffusion Models successful. In this paper we propose a
simple approach which gets away from using Gaussian noise but adopts some basic
structures of diffusion models for efficient image super-resolution.
Essentially, we propose a DNN to perform domain transfer between neighbor
domains, which can learn the differences in statistical properties to
facilitate gradual interpolation with results of reasonable quality. Further
quality improvement is achieved by conditioning the domain transfer with
reference to the input LR image. Experimental results show that our method
outperforms not only state-of-the-art large scale super resolution models, but
also the current diffusion models for image super-resolution. The approach can
readily be extended to other image-to-image tasks, such as image enlightening,
inpainting, denoising, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hui_C/0/1/0/all/0/1&quot;&gt;Chun-Chuen Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siu_W/0/1/0/all/0/1&quot;&gt;Wan-Chi Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Law_N/0/1/0/all/0/1&quot;&gt;Ngai-Fong Law&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07750">
<title>SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification. (arXiv:2311.07750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07750</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the &quot;ChestX-ray14&quot; dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashraf_S/0/1/0/all/0/1&quot;&gt;S.M. Nabil Ashraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1&quot;&gt;Md. Adyelullahil Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1&quot;&gt;Hasnat Md. Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08059">
<title>FS-Net: Full Scale Network and Adaptive Threshold for Improving Extraction of Micro-Retinal Vessel Structures. (arXiv:2311.08059v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08059</link>
<description rdf:parseType="Literal">&lt;p&gt;Retinal vascular segmentation, is a widely researched subject in biomedical
image processing, aims to relieve ophthalmologists&apos; workload when treating and
detecting retinal disorders. However, segmenting retinal vessels has its own
set of challenges, with prior techniques failing to generate adequate results
when segmenting branches and microvascular structures. The neural network
approaches used recently are characterized by the inability to keep local and
global properties together and the failure to capture tiny end vessels make it
challenging to attain the desired result. To reduce this retinal vessel
segmentation problem, we propose a full-scale micro-vessel extraction mechanism
based on an encoder-decoder neural network architecture, sigmoid smoothing, and
an adaptive threshold method. The network consists of of residual, encoder
booster, bottleneck enhancement, squeeze, and excitation building blocks. All
of these blocks together help to improve the feature extraction and prediction
of the segmentation map. The proposed solution has been evaluated using the
DRIVE, CHASE-DB1, and STARE datasets, and competitive results are obtained when
compared with previous studies. The AUC and accuracy on the DRIVE dataset are
0.9884 and 0.9702, respectively. On the CHASE-DB1 dataset, the scores are
0.9903 and 0.9755, respectively. On the STARE dataset, the scores are 0.9916
and 0.9750, respectively. The performance achieved is one step ahead of what
has been done in previous studies, and this results in a higher chance of
having this solution in real-life diagnostic centers that seek ophthalmologists
attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Getahun_M/0/1/0/all/0/1&quot;&gt;Melaku N. Getahun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rogov_O/0/1/0/all/0/1&quot;&gt;Oleg Y. Rogov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1&quot;&gt;Dmitry V. Dylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Somov_A/0/1/0/all/0/1&quot;&gt;Andrey Somov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bouridane_A/0/1/0/all/0/1&quot;&gt;Ahmed Bouridane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamoudi_R/0/1/0/all/0/1&quot;&gt;Rifat Hamoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08393">
<title>MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation. (arXiv:2311.08393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08393</link>
<description rdf:parseType="Literal">&lt;p&gt;The learn-from-observation (LfO) paradigm is a human-inspired mode for a
robot to learn to perform a task simply by watching it being performed. LfO can
facilitate robot integration on factory floors by minimizing disruption and
reducing tedious programming. A key component of the LfO pipeline is a
transformation of the depth camera frames to the corresponding task state and
action pairs, which are then relayed to learning techniques such as imitation
or inverse reinforcement learning for understanding the task parameters. While
several existing computer vision models analyze videos for activity
recognition, SA-Net specifically targets robotic LfO from RGB-D data. However,
SA-Net and many other models analyze frame data captured from a single
viewpoint. Their analysis is therefore highly sensitive to occlusions of the
observed task, which are frequent in deployments. An obvious way of reducing
occlusions is to simultaneously observe the task from multiple viewpoints and
synchronously fuse the multiple streams in the model. Toward this, we present
multi-view SA-Net, which generalizes the SA-Net model to allow the perception
of multiple viewpoints of the task activity, integrate them, and better
recognize the state and action in each frame. Performance evaluations on two
distinct domains establish that MVSA-Net recognizes the state-action pairs
under occlusion more accurately compared to single-view MVSA-Net and other
baselines. Our ablation studies further evaluate its performance under
different ambient conditions and establish the contribution of the architecture
components. As such, MVSA-Net offers a significantly more robust and deployable
state-action trajectory generation compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asali_E/0/1/0/all/0/1&quot;&gt;Ehsan Asali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Prashant Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jin Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08774">
<title>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation. (arXiv:2311.08774v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08774</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-assisted nuclei segmentation in histopathological images is a crucial task
in the diagnosis and treatment of cancer diseases. It decreases the time
required to manually screen microscopic tissue images and can resolve the
conflict between pathologists during diagnosis. Deep Learning has proven useful
in such a task. However, lack of labeled data is a significant barrier for deep
learning-based approaches. In this study, we propose a novel approach to nuclei
segmentation that leverages the available labelled and unlabelled data. The
proposed method combines the strengths of both transductive and inductive
learning, which have been previously attempted separately, into a single
framework. Inductive learning aims at approximating the general function and
generalizing to unseen test data, while transductive learning has the potential
of leveraging the unlabelled test data to improve the classification. To the
best of our knowledge, this is the first study to propose such a hybrid
approach for medical image segmentation. Moreover, we propose a novel two-stage
transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to
demonstrate the efficacy and potential of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1&quot;&gt;Hesham Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tondji_I/0/1/0/all/0/1&quot;&gt;Idriss Tondji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08835">
<title>Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding. (arXiv:2311.08835v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08835</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent endeavors in video temporal grounding enforce strong cross-modal
interactions through attention mechanisms to overcome the modality gap between
video and text query. However, previous works treat all video clips equally
regardless of their semantic relevance with the text query in attention
modules. In this paper, our goal is to provide clues for query-associated video
clips within the crossmodal encoding process. With our Correlation-Guided
Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of
cross-modal interactions and how to exploit such degrees for prediction. First,
we design an adaptive cross-attention layer with dummy tokens. Dummy tokens
conditioned by text query take a portion of the attention weights, preventing
irrelevant video clips from being represented by the text query. Yet, not all
word tokens equally inherit the text query&apos;s correlation to video clips. Thus,
we further guide the cross-attention map by inferring the fine-grained
correlation between video clips and words. We enable this by learning a joint
embedding space for high-level concepts, i.e., moment and sentence level, and
inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency
detector to exploit each video clip&apos;s degrees of text engagement. We validate
the superiority of CG-DETR with the state-of-the-art results on various
benchmarks for both moment retrieval and highlight detection. Codes are
available at https://github.com/wjun0830/CGDETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_W/0/1/0/all/0/1&quot;&gt;WonJun Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_S/0/1/0/all/0/1&quot;&gt;Sangeek Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;SuBeen Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Jae-Pil Heo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09257">
<title>UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09257</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09500">
<title>Pseudo-keypoint RKHS Learning for Self-supervised 6DoF Pose Estimation. (arXiv:2311.09500v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09500</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the simulation-to-real domain gap in 6DoF PE, and
proposes a novel self-supervised keypoint radial voting-based 6DoF PE
framework, effectively narrowing this gap using a learnable kernel in RKHS. We
formulate this domain gap as a distance in high-dimensional feature space,
distinct from previous iterative matching methods. We propose an adapter
network, which evolves the network parameters from the source domain, which has
been massively trained on synthetic data with synthetic poses, to the target
domain, which is trained on real data. Importantly, the real data training only
uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real
groundtruth data annotations. RKHSPose achieves state-of-the-art performance on
three commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion
LINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully
supervised methods on all six applicable BOP core datasets, achieving within
-10.8% to -0.3% of the top fully supervised results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangzheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1&quot;&gt;Michael Greenspan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09574">
<title>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML&apos;s interpretable models, developed on a
limited volume of H&amp;amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vivek Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Vrishab Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Brent Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1&quot;&gt;Oscar Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1&quot;&gt;Rebecca Rojansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1&quot;&gt;Fabiola Valvert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1&quot;&gt;Edward Briercheck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1&quot;&gt;David Weinstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1&quot;&gt;Yasodha Natkunam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1&quot;&gt;Sebastian Fernandez-Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09642">
<title>Weakly Supervised Anomaly Detection for Chest X-Ray Image. (arXiv:2311.09642v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09642</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-Ray (CXR) examination is a common method for assessing thoracic
diseases in clinical applications. While recent advances in deep learning have
enhanced the significance of visual analysis for CXR anomaly detection, current
methods often miss key cues in anomaly images crucial for identifying disease
regions, as they predominantly rely on unsupervised training with normal
images. This letter focuses on a more practical setup in which few-shot anomaly
images with only image-level labels are available during training. For this
purpose, we propose WSCXR, a weakly supervised anomaly detection framework for
CXR. WSCXR firstly constructs sets of normal and anomaly image features
respectively. It then refines the anomaly image features by eliminating normal
region features through anomaly feature mining, thus fully leveraging the
scarce yet crucial features of diseased areas. Additionally, WSCXR employs a
linear mixing strategy to augment the anomaly features, facilitating the
training of anomaly detector with few-shot anomaly images. Experiments on two
CXR datasets demonstrate the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Haoqi Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Ximiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lang_N/0/1/0/all/0/1&quot;&gt;Ning Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiuzhuang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09680">
<title>Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
by reliable methods. Despite the abundance of literature on trustworthy LMs in
NLP, a systematic survey specifically delving into the trustworthiness of LMs
in CV remains absent. In order to mitigate this gap, we summarize four relevant
concerns that obstruct the trustworthy usage in vision of LMs in this survey,
including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers&apos;
understanding of this field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09806">
<title>EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices. (arXiv:2311.09806v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09806</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use differentiable volume
rendering or surface-based representation to generate high-fidelity scenes.
However, these methods require excessive runtime for rendering, making them
impractical for daily applications. To address these challenges, we present
$\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware
implicit textured $\textbf{Surf}$ace reconstruction method on mobile devices.
In our method, we first employ an efficient surface-based model with a
multi-view supervision module to ensure accurate mesh reconstruction. To enable
high-fidelity rendering, we learn an implicit texture embedded with a set of
Gaussian lobes to capture view-dependent information. Furthermore, with the
explicit geometry and the implicit texture, we can employ a lightweight neural
shader to reduce the expense of computation and further support real-time
rendering on common mobile devices. Extensive experiments demonstrate that our
method can reconstruct high-quality appearance and accurate mesh on both
synthetic and real-world datasets. Moreover, our method can be trained in just
1-2 hours using a single GPU and run on mobile devices at over 40 FPS (Frames
Per Second), with a final package required for rendering taking up only 40-50
MB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jingnan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yichao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1&quot;&gt;Bowen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiangjing Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10251">
<title>UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets. (arXiv:2311.10251v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10251</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Can Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Sheng Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Junyi Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pang_S/0/1/0/all/0/1&quot;&gt;Shuchao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Orgun_M/0/1/0/all/0/1&quot;&gt;Mehmet A. Orgun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10543">
<title>Joint covariance property under geometric image transformations for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields. (arXiv:2311.10543v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10543</link>
<description rdf:parseType="Literal">&lt;p&gt;The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations and for formulating invariant visual
operations at higher levels. This paper defines and proves a joint covariance
property under compositions of spatial scaling transformations, spatial affine
transformations, Galilean transformations and temporal scaling transformations,
which makes it possible to characterize how different types of image
transformations interact with each other. Specifically, the derived relations
show how the receptive field parameters need to be transformed, in order to
match the output from spatio-temporal receptive fields with the underlying
spatio-temporal image transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1&quot;&gt;Tony Lindeberg&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>