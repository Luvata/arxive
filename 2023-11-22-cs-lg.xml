<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1904.10552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.07606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.02785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.03920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.00527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.13836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.01140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.09369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.04712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.06750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.15439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.13272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.13405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19845" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.10718">
<title>Harnessing Deep Q-Learning for Enhanced Statistical Arbitrage in High-Frequency Trading: A Comprehensive Exploration. (arXiv:2311.10718v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/2311.10718</link>
<description rdf:parseType="Literal">&lt;p&gt;The realm of High-Frequency Trading (HFT) is characterized by rapid
decision-making processes that capitalize on fleeting market inefficiencies. As
the financial markets become increasingly competitive, there is a pressing need
for innovative strategies that can adapt and evolve with changing market
dynamics. Enter Reinforcement Learning (RL), a branch of machine learning where
agents learn by interacting with their environment, making it an intriguing
candidate for HFT applications. This paper dives deep into the integration of
RL in statistical arbitrage strategies tailored for HFT scenarios. By
leveraging the adaptive learning capabilities of RL, we explore its potential
to unearth patterns and devise trading strategies that traditional methods
might overlook. We delve into the intricate exploration-exploitation trade-offs
inherent in RL and how they manifest in the volatile world of HFT. Furthermore,
we confront the challenges of applying RL in non-stationary environments,
typical of financial markets, and investigate methodologies to mitigate
associated risks. Through extensive simulations and backtests, our research
reveals that RL not only enhances the adaptability of trading strategies but
also shows promise in improving profitability metrics and risk-adjusted
returns. This paper, therefore, positions RL as a pivotal tool for the next
generation of HFT-based statistical arbitrage, offering insights for both
researchers and practitioners in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10719">
<title>Analysis of frequent trading effects of various machine learning models. (arXiv:2311.10719v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/2311.10719</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, high-frequency trading has emerged as a crucial strategy in
stock trading. This study aims to develop an advanced high-frequency trading
algorithm and compare the performance of three different mathematical models:
the combination of the cross-entropy loss function and the quasi-Newton
algorithm, the FCNN model, and the vector machine. The proposed algorithm
employs neural network predictions to generate trading signals and execute buy
and sell operations based on specific conditions. By harnessing the power of
neural networks, the algorithm enhances the accuracy and reliability of the
trading strategy. To assess the effectiveness of the algorithm, the study
evaluates the performance of the three mathematical models. The combination of
the cross-entropy loss function and the quasi-Newton algorithm is a widely
utilized logistic regression approach. The FCNN model, on the other hand, is a
deep learning algorithm that can extract and classify features from stock data.
Meanwhile, the vector machine is a supervised learning algorithm recognized for
achieving improved classification results by mapping data into high-dimensional
spaces. By comparing the performance of these three models, the study aims to
determine the most effective approach for high-frequency trading. This research
makes a valuable contribution by introducing a novel methodology for
high-frequency trading, thereby providing investors with a more accurate and
reliable stock trading strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaofei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10721">
<title>Deep Neuromorphic Networks with Superconducting Single Flux Quanta. (arXiv:2311.10721v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2311.10721</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional semiconductor-based integrated circuits are gradually
approaching fundamental scaling limits. Many prospective solutions have
recently emerged to supplement or replace both the technology on which basic
devices are built and the architecture of data processing. Neuromorphic
circuits are a promising approach to computing where techniques used by the
brain to achieve high efficiency are exploited. Many existing neuromorphic
circuits rely on unconventional and useful properties of novel technologies to
better mimic the operation of the brain. One such technology is single flux
quantum (SFQ) logic -- a cryogenic superconductive technology in which the data
are represented by quanta of magnetic flux (fluxons) produced and processed by
Josephson junctions embedded within inductive loops. The movement of a fluxon
within a circuit produces a quantized voltage pulse (SFQ pulse), resembling a
neuronal spiking event. These circuits routinely operate at clock frequencies
of tens to hundreds of gigahertz, making SFQ a natural technology for
processing high frequency pulse trains.
&lt;/p&gt;
&lt;p&gt;Prior proposals for SFQ neural networks often require energy-expensive fluxon
conversions, involve heterogeneous technologies, or exclusively focus on device
level behavior. In this paper, a design methodology for deep single flux
quantum neuromorphic networks is presented. Synaptic and neuronal circuits
based on SFQ technology are presented and characterized. Based on these
primitives, a deep neuromorphic XOR network is evaluated as a case study, both
at the architectural and circuit levels, achieving wide classification margins.
The proposed methodology does not employ unconventional superconductive devices
or semiconductor transistors. The resulting networks are tunable by an external
current, making this proposed system an effective approach for scalable
cryogenic neuromorphic computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krylov_G/0/1/0/all/0/1&quot;&gt;Gleb Krylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1&quot;&gt;Alexander J. Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1&quot;&gt;Joseph S. Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1&quot;&gt;Eby G. Friedman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10731">
<title>Gender-Based Comparative Study of Type 2 Diabetes Risk Factors in Kolkata, India: A Machine Learning Approach. (arXiv:2311.10731v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10731</link>
<description rdf:parseType="Literal">&lt;p&gt;Type 2 diabetes mellitus represents a prevalent and widespread global health
concern, necessitating a comprehensive assessment of its risk factors. This
study aimed towards learning whether there is any differential impact of age,
Lifestyle, BMI and Waist to height ratio on the risk of Type 2 diabetes
mellitus in males and females in Kolkata, West Bengal, India based on a sample
observed from the out-patient consultation department of Belle Vue Clinic in
Kolkata. Various machine learning models like Logistic Regression, Random
Forest, and Support Vector Classifier, were used to predict the risk of
diabetes, and performance was compared based on different predictors. Our
findings indicate a significant age-related increase in risk of diabetes for
both males and females. Although exercising and BMI was found to have
significant impact on the risk of Type 2 diabetes in males, in females both
turned out to be statistically insignificant. For both males and females,
predictive models based on WhtR demonstrated superior performance in risk
assessment compared to those based on BMI. This study sheds light on the
gender-specific differences in the risk factors for Type 2 diabetes, offering
valuable insights that can be used towards more targeted healthcare
interventions and public health strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rahul Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Anoushka Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daga_G/0/1/0/all/0/1&quot;&gt;Gourav Daga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Durba Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Madhura Das Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Sourav Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1&quot;&gt;Suparna Roychowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10746">
<title>EIT: Earnest Insight Toolkit for Evaluating Students&apos; Earnestness in Interactive Lecture Participation Exercises. (arXiv:2311.10746v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.10746</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s rapidly evolving educational landscape, traditional modes of
passive information delivery are giving way to transformative pedagogical
approaches that prioritize active student engagement. Within the context of
large-scale hybrid classrooms, the challenge lies in fostering meaningful and
active interaction between students and course content. This study delves into
the significance of measuring students&apos; earnestness during interactive lecture
participation exercises. By analyzing students&apos; responses to interactive
lecture poll questions, establishing a clear rubric for evaluating earnestness,
and conducting a comprehensive assessment, we introduce EIT (Earnest Insight
Toolkit), a tool designed to assess students&apos; engagement within interactive
lecture participation exercises - particularly in the context of large-scale
hybrid classrooms. Through the utilization of EIT, our objective is to equip
educators with valuable means of identifying at-risk students for enhancing
intervention and support strategies, as well as measuring students&apos; levels of
engagement with course content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miroyan_M/0/1/0/all/0/1&quot;&gt;Mihran Miroyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_S/0/1/0/all/0/1&quot;&gt;Shiny Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rahul Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Lisa Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norouzi_N/0/1/0/all/0/1&quot;&gt;Narges Norouzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10747">
<title>Safety-aware Causal Representation for Trustworthy Reinforcement Learning in Autonomous Driving. (arXiv:2311.10747v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10747</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of autonomous driving, the Learning from Demonstration (LfD)
paradigm has exhibited notable efficacy in addressing sequential
decision-making problems. However, consistently achieving safety in varying
traffic contexts, especially in safety-critical scenarios, poses a significant
challenge due to the long-tailed and unforeseen scenarios absent from offline
datasets. In this paper, we introduce the saFety-aware strUctured Scenario
representatION (FUSION), a pioneering methodology conceived to facilitate the
learning of an adaptive end-to-end driving policy by leveraging structured
scenario information. FUSION capitalizes on the causal relationships between
decomposed reward, cost, state, and action space, constructing a framework for
structured sequential reasoning under dynamic traffic environments. We conduct
rigorous evaluations in two typical real-world settings of distribution shift
in autonomous vehicles, demonstrating the good balance between safety cost and
utility reward of FUSION compared to contemporary state-of-the-art safety-aware
LfD baselines. Empirical evidence under diverse driving scenarios attests that
FUSION significantly enhances the safety and generalizability of autonomous
driving agents, even in the face of challenging and unseen environments.
Furthermore, our ablation studies reveal noticeable improvements in the
integration of causal representation into the safe offline RL problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haohong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenhao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yaru Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yuming Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10749">
<title>Measuring Five Accountable Talk Moves to Improve Instruction at Scale. (arXiv:2311.10749v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.10749</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing consistent, individualized feedback to teachers on their
instruction can improve student learning outcomes. Such feedback can especially
benefit novice instructors who teach on online platforms and have limited
access to instructional training. To build scalable measures of instruction, we
fine-tune RoBERTa and GPT models to identify five instructional talk moves
inspired by accountable talk theory: adding on, connecting, eliciting, probing
and revoicing students&apos; ideas. We fine-tune these models on a newly annotated
dataset of 2500 instructor utterances derived from transcripts of small group
instruction in an online computer science course, Code in Place. Although we
find that GPT-3 consistently outperforms RoBERTa in terms of precision, its
recall varies significantly. We correlate the instructors&apos; use of each talk
move with indicators of student engagement and satisfaction, including
students&apos; section attendance, section ratings, and assignment completion rates.
We find that using talk moves generally correlates positively with student
outcomes, and connecting student ideas has the largest positive impact. These
results corroborate previous research on the effectiveness of accountable talk
moves and provide exciting avenues for using these models to provide
instructors with useful, scalable feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kupor_A/0/1/0/all/0/1&quot;&gt;Ashlee Kupor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morgan_C/0/1/0/all/0/1&quot;&gt;Candice Morgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demszky_D/0/1/0/all/0/1&quot;&gt;Dorottya Demszky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10752">
<title>Differentiating patients with obstructive sleep apnea from healthy controls based on heart rate - blood pressure coupling quantified by entropy-based indices. (arXiv:2311.10752v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2311.10752</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an entropy-based classification method for pairs of sequences
(ECPS) for quantifying mutual dependencies in heart rate and beat-to-beat blood
pressure recordings. The purpose of the method is to build a classifier for
data in which each item consists of the two intertwined data series taken for
each subject. The method is based on ordinal patterns, and uses entropy-like
indices. Machine learning is used to select a subset of indices most suitable
for our classification problem in order to build an optimal yet simple model
for distinguishing between patients suffering from obstructive sleep apnea and
a control group.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pilarczyk_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Pilarczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Graff_G/0/1/0/all/0/1&quot;&gt;Grzegorz Graff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Amigo_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; M. Amig&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Tessmer_K/0/1/0/all/0/1&quot;&gt;Katarzyna Tessmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Narkiewicz_K/0/1/0/all/0/1&quot;&gt;Krzysztof Narkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Graff_B/0/1/0/all/0/1&quot;&gt;Beata Graff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10756">
<title>Earnings Prediction Using Recurrent Neural Networks. (arXiv:2311.10756v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2311.10756</link>
<description rdf:parseType="Literal">&lt;p&gt;Firm disclosures about future prospects are crucial for corporate valuation
and compliance with global regulations, such as the EU&apos;s MAR and the US&apos;s SEC
Rule 10b-5 and RegFD. To comply with disclosure obligations, issuers must
identify nonpublic information with potential material impact on security
prices as only new, relevant and unexpected information materially affects
prices in efficient markets. Financial analysts, assumed to represent public
knowledge on firms&apos; earnings prospects, face limitations in offering
comprehensive coverage and unbiased estimates. This study develops a neural
network to forecast future firm earnings, using four decades of financial data,
addressing analysts&apos; coverage gaps and potentially revealing hidden insights.
The model avoids selectivity and survivorship biases as it allows for missing
data. Furthermore, the model is able to produce both fiscal-year-end and
quarterly earnings predictions. Its performance surpasses benchmark models from
the academic literature by a wide margin and outperforms analysts&apos; forecasts
for fiscal-year-end earnings predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Scherrmann_M/0/1/0/all/0/1&quot;&gt;Moritz Scherrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Elsas_R/0/1/0/all/0/1&quot;&gt;Ralf Elsas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10763">
<title>Comparing Generalization in Learning with Limited Numbers of Exemplars: Transformer vs. RNN in Attractor Dynamics. (arXiv:2311.10763v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT, a widely-recognized large language model (LLM), has recently gained
substantial attention for its performance scaling, attributed to the billions
of web-sourced natural language sentences used for training. Its underlying
architecture, Transformer, has found applications across diverse fields,
including video, audio signals, and robotic movement. %The crucial question
this raises concerns the Transformer&apos;s generalization-in-learning (GIL)
capacity. However, this raises a crucial question about Transformer&apos;s
generalization in learning (GIL) capacity. Is ChatGPT&apos;s success chiefly due to
the vast dataset used for training, or is there more to the story? To
investigate this, we compared Transformer&apos;s GIL capabilities with those of a
traditional Recurrent Neural Network (RNN) in tasks involving attractor
dynamics learning. For performance evaluation, the Dynamic Time Warping (DTW)
method has been employed. Our simulation results suggest that under conditions
of limited data availability, Transformer&apos;s GIL abilities are markedly inferior
to those of RNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukushima_R/0/1/0/all/0/1&quot;&gt;Rui Fukushima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1&quot;&gt;Jun Tani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10770">
<title>Exponentially Faster Language Modelling. (arXiv:2311.10770v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10770</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present FastBERT, a BERT
variant that uses 0.3\% of its neurons during inference while performing on par
with similar BERT models. FastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belcak_P/0/1/0/all/0/1&quot;&gt;Peter Belcak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10775">
<title>ToolTalk: Evaluating Tool-Usage in a Conversational Setting. (arXiv:2311.10775v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10775</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have displayed massive improvements in reasoning
and decision-making skills and can hold natural conversations with users. Many
recent works seek to augment LLM-based assistants with external tools so they
can access private or up-to-date information and carry out actions on behalf of
users. To better measure the performance of these assistants, this paper
introduces ToolTalk, a benchmark consisting of complex user intents requiring
multi-step tool usage specified through dialogue. ToolTalk contains 28 tools
grouped into 7 plugins, and includes a complete simulated implementation of
each tool, allowing for fully automated evaluation of assistants that rely on
execution feedback. ToolTalk also emphasizes tools that externally affect the
world rather than only tools for referencing or searching information. We
evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and
50% respectively. Our analysis of the errors reveals three major categories and
suggests some future directions for improvement. We release ToolTalk at
https://github.com/microsoft/ToolTalk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farn_N/0/1/0/all/0/1&quot;&gt;Nicholas Farn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1&quot;&gt;Richard Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10777">
<title>A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends. (arXiv:2311.10777v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10777</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment
analysis (SA) that identifies aspects and the associated opinions from a given
text. In the digital era, ABSA gained increasing popularity and applications in
mining opinionated text data to obtain insights and support decisions. ABSA
research employs linguistic, statistical, and machine-learning approaches and
utilises resources such as labelled datasets, aspect and sentiment lexicons and
ontology. By its nature, ABSA is domain-dependent and can be sensitive to the
impact of misalignment between the resource and application domains. However,
to our knowledge, this topic has not been explored by the existing ABSA
literature reviews. In this paper, we present a Systematic Literature Review
(SLR) of ABSA studies with a focus on the research application domain, dataset
domain, and the research methods to examine their relationships and identify
trends over time. Our results suggest a number of potential systemic issues in
the ABSA research literature, including the predominance of the
``product/service review&apos;&apos; dataset domain among the majority of studies that
did not have a specific research application domain, coupled with the
prevalence of dataset-reliant methods such as supervised machine learning. This
review makes a number of unique contributions to the ABSA research field: 1) To
our knowledge, it is the first SLR that links the research domain, dataset
domain, and research method through a systematic perspective; 2) it is one of
the largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191
search results without time constraint; and 3) our review methodology adopted
an innovative automatic filtering process based on PDF-mining, which enhanced
screening quality and reliability. Suggestions and our review limitations are
also discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yan Cathy Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taskova_K/0/1/0/all/0/1&quot;&gt;Katerina Taskova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wicker_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;erg Wicker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10778">
<title>uHD: Unary Processing for Lightweight and Dynamic Hyperdimensional Computing. (arXiv:2311.10778v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.10778</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperdimensional computing (HDC) is a novel computational paradigm that
operates on long-dimensional vectors known as hypervectors. The hypervectors
are constructed as long bit-streams and form the basic building blocks of HDC
systems. In HDC, hypervectors are generated from scalar values without taking
their bit significance into consideration. HDC has been shown to be efficient
and robust in various data processing applications, including computer vision
tasks. To construct HDC models for vision applications, the current
state-of-the-art practice utilizes two parameters for data encoding: pixel
intensity and pixel position. However, the intensity and position information
embedded in high-dimensional vectors are generally not generated dynamically in
the HDC models. Consequently, the optimal design of hypervectors with high
model accuracy requires powerful computing platforms for training. A more
efficient approach to generating hypervectors is to create them dynamically
during the training phase, which results in accurate, low-cost, and highly
performable vectors. To this aim, we use low-discrepancy sequences to generate
intensity hypervectors only, while avoiding position hypervectors. By doing so,
the multiplication step in vector encoding is eliminated, resulting in a
power-efficient HDC system. For the first time in the literature, our proposed
approach employs lightweight vector generators utilizing unary bit-streams for
efficient encoding of data instead of using conventional comparator-based
generators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aygun_S/0/1/0/all/0/1&quot;&gt;Sercan Aygun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_M/0/1/0/all/0/1&quot;&gt;Mehran Shoushtari Moghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafi_M/0/1/0/all/0/1&quot;&gt;M. Hassan Najafi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10780">
<title>Extending Neural Network Verification to a Larger Family of Piece-wise Linear Activation Functions. (arXiv:2311.10780v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10780</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we extend an available neural network verification technique
to support a wider class of piece-wise linear activation functions.
Furthermore, we extend the algorithms, which provide in their original form
exact respectively over-approximative results for bounded input sets
represented as start sets, to allow also unbounded input set. We implemented
our algorithms and demonstrated their effectiveness in some case studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antal_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe1;szl&amp;#xf3; Antal&lt;/a&gt; (RWTH Aachen University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masara_H/0/1/0/all/0/1&quot;&gt;Hana Masara&lt;/a&gt; (RWTH Aachen University), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abraham_E/0/1/0/all/0/1&quot;&gt;Erika &amp;#xc1;brah&amp;#xe1;m&lt;/a&gt; (RWTH Aachen University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10782">
<title>A BERT based Ensemble Approach for Sentiment Classification of Customer Reviews and its Application to Nudge Marketing in e-Commerce. (arXiv:2311.10782v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10782</link>
<description rdf:parseType="Literal">&lt;p&gt;According to the literature, Product reviews are an important source of
information for customers to support their buying decision. Product reviews
improve customer trust and loyalty. Reviews help customers in understanding
what other customers think about a particular product and helps in driving
purchase decisions. Therefore, for an e-commerce platform it is important to
understand the sentiments in customer reviews to understand their products and
services, and it also allows them to potentially create positive consumer
interaction as well as long lasting relationships. Reviews also provide
innovative ways to market the products for an ecommerce company. One such
approach is Nudge Marketing. Nudge marketing is a subtle way for an ecommerce
company to help their customers make better decisions without hesitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1&quot;&gt;Sayan Putatunda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1&quot;&gt;Anwesha Bhowmik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiruvenkadam_G/0/1/0/all/0/1&quot;&gt;Girish Thiruvenkadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_R/0/1/0/all/0/1&quot;&gt;Rahul Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10784">
<title>ExFake: Towards an Explainable Fake News Detection Based on Content and Social Context Information. (arXiv:2311.10784v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10784</link>
<description rdf:parseType="Literal">&lt;p&gt;ExFake is an explainable fake news detection system based on content and
context-level information. It is concerned with the veracity analysis of online
posts based on their content, social context (i.e., online users&apos; credibility
and historical behaviour), and data coming from trusted entities such as
fact-checking websites and named entities. Unlike state-of-the-art systems, an
Explainable AI (XAI) assistant is also adopted to help online social networks
(OSN) users develop good reflexes when faced with any doubted information that
spreads on social networks. The trustworthiness of OSN users is also addressed
by assigning a credibility score to OSN users, as OSN users are one of the main
culprits for spreading fake news. Experimental analysis on a real-world dataset
demonstrates that ExFake significantly outperforms other baseline methods for
fake news detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amri_S/0/1/0/all/0/1&quot;&gt;Sabrine Amri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boleilanga_H/0/1/0/all/0/1&quot;&gt;Henri-Cedric Mputu Boleilanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aimeur_E/0/1/0/all/0/1&quot;&gt;Esma A&amp;#xef;meur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10785">
<title>Text Sanitization Beyond Specific Domains: Zero-Shot Redaction &amp; Substitution with Large Language Models. (arXiv:2311.10785v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10785</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of information systems, text sanitization techniques are used
to identify and remove sensitive data to comply with security and regulatory
requirements. Even though many methods for privacy preservation have been
proposed, most of them are focused on the detection of entities from specific
domains (e.g., credit card numbers, social security numbers), lacking
generality and requiring customization for each desirable domain. Moreover,
removing words is, in general, a drastic measure, as it can degrade text
coherence and contextual information. Less severe measures include substituting
a word for a safe alternative, yet it can be challenging to automatically find
meaningful substitutions. We present a zero-shot text sanitization technique
that detects and substitutes potentially sensitive information using Large
Language Models. Our evaluation shows that our method excels at protecting
privacy while maintaining text coherence and contextual information, preserving
data utility for downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanese_F/0/1/0/all/0/1&quot;&gt;Federico Albanese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciolek_D/0/1/0/all/0/1&quot;&gt;Daniel Ciolek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DIppolito_N/0/1/0/all/0/1&quot;&gt;Nicolas D&amp;#x27;Ippolito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10787">
<title>Assurance for Deployed Continual Learning Systems. (arXiv:2311.10787v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10787</link>
<description rdf:parseType="Literal">&lt;p&gt;The future success of the Navy will depend, in part, on artificial
intelligence. In practice, many artificially intelligent algorithms, and in
particular deep learning models, rely on continual learning to maintain
performance in dynamic environments. The software requires adaptation to
maintain its initial level of performance in unseen situations. However, if not
monitored properly, continual learning may lead to several issues including
catastrophic forgetting in which a trained model forgets previously learned
tasks when being retrained on new data. The authors created a new framework for
safely performing continual learning with the goal of pairing this safety
framework with a deep learning computer vision algorithm to allow for safe and
high-performing automatic deck tracking on carriers and amphibious assault
ships. The safety framework includes several features, such as an ensemble of
convolutional neural networks to perform image classification, a manager to
record confidences and determine the best answer from the ensemble, a model of
the environment to predict when the system may fail to meet minimum performance
metrics, a performance monitor to log system and domain performance and check
against requirements, and a retraining component to update the ensemble and
manager to maintain performance. The authors validated the proposed method
using extensive simulation studies based on dynamic image classification. The
authors showed the safety framework could probabilistically detect out of
distribution data. The results also show the framework can detect when the
system is no longer performing safely and can significantly extend the working
envelope of an image classifier.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_A/0/1/0/all/0/1&quot;&gt;Ari Goodman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OShea_R/0/1/0/all/0/1&quot;&gt;Ryan O&amp;#x27;Shea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirschorn_N/0/1/0/all/0/1&quot;&gt;Noam Hirschorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrostowski_H/0/1/0/all/0/1&quot;&gt;Hubert Chrostowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10789">
<title>Stratified-NMF for Heterogeneous Data. (arXiv:2311.10789v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10789</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-negative matrix factorization (NMF) is an important technique for
obtaining low dimensional representations of datasets. However, classical NMF
does not take into account data that is collected at different times or in
different locations, which may exhibit heterogeneity. We resolve this problem
by solving a modified NMF objective, Stratified-NMF, that simultaneously learns
strata-dependent statistics and a shared topics matrix. We develop
multiplicative update rules for this novel objective and prove convergence of
the objective. Then, we experiment on synthetic data to demonstrate the
efficiency and accuracy of the method. Lastly, we apply our method to three
real world datasets and empirically investigate their learned features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapman_J/0/1/0/all/0/1&quot;&gt;James Chapman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaniv_Y/0/1/0/all/0/1&quot;&gt;Yotam Yaniv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1&quot;&gt;Deanna Needell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10790">
<title>Degeneration of kernel regression with Matern kernels into low-order polynomial regression in high dimension. (arXiv:2311.10790v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2311.10790</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel methods such as kernel ridge regression and Gaussian process
regressions with Matern type kernels have been increasingly used, in
particular, to fit potential energy surfaces (PES) and density functionals, and
for materials informatics. When the dimensionality of the feature space is
high, these methods are used with necessarily sparse data. In this regime, the
optimal length parameter of a Matern-type kernel tends to become so large that
the method effectively degenerates into a low-order polynomial regression and
therefore loses any advantage over such regression. This is demonstrated
theoretically as well as numerically on the examples of six- and
fifteen-dimensional molecular PES using squared exponential and simple
exponential kernels. The results shed additional light on the success of
polynomial approximations such as PIP for medium size molecules and on the
importance of orders-of-coupling based models for preserving the advantages of
kernel methods with Matern type kernels or on the use of physically-motivated
(reproducing) kernels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Manzhos_S/0/1/0/all/0/1&quot;&gt;Sergei Manzhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ihara_M/0/1/0/all/0/1&quot;&gt;Manabu Ihara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10792">
<title>Attention Mechanism for Lithium-Ion Battery Lifespan Prediction: Temporal and Cyclic Attention. (arXiv:2311.10792v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10792</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting the lifespan of lithium-ion batteries (LIBs) is pivotal
for optimizing usage and preventing accidents. Previous studies in constructing
prediction models often relied on inputs challenging to measure in real-time
operations and failed to capture intra-cycle and inter-cycle data patterns,
essential features for accurate predictions, comprehensively. In this study, we
employ attention mechanisms (AM) to develop data-driven models for predicting
LIB lifespan using easily measurable inputs such as voltage, current,
temperature, and capacity data. The developed model integrates recurrent neural
network (RNN) and convolutional neural network (CNN) components, featuring two
types of attention mechanisms: temporal attention (TA) and cyclic attention
(CA). The inclusion of TA aims to identify important time steps within each
cycle by scoring the hidden states of the RNN, whereas CA strives to capture
key features of inter-cycle correlations through self-attention (SA). This
enhances model accuracy and elucidates critical features in the input data. To
validate our method, we apply it to publicly available cycling data consisting
of three batches of cycling modes. The calculated TA scores highlight the rest
phase as a key characteristic distinguishing LIB data among different batches.
Additionally, CA scores reveal variations in the importance of cycles across
batches. By leveraging CA scores, we explore the potential to reduce the number
of cycles in the input data. The single-head and multi-head attentions enable
us to decrease the input dimension from 100 to 50 and 30 cycles, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1&quot;&gt;Seongmin Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jay H. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10795">
<title>How False Data Affects Machine Learning Models in Electrochemistry?. (arXiv:2311.10795v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10795</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the selection of machine learning model based on only the data
distribution without concerning the noise of the data. This study aims to
distinguish, which models perform well under noisy data, and establish whether
stacking machine learning models actually provide robustness to otherwise
weak-to-noise models. The electrochemical data were tested with 12 standalone
models and stacking model. This includes XGB, LGBM, RF, GB, ADA, NN, ELAS,
LASS, RIDGE, SVM, KNN, DT, and the stacking model. It is found that linear
models handle noise well with the average error of (slope) to 1.75 F g-1 up to
error per 100% percent noise added; but it suffers from prediction accuracy due
to having an average of 60.19 F g-1 estimated at minimal error at 0% noise
added. Tree-based models fail in terms of noise handling (average slope is
55.24 F g-1 at 100% percent noise), but it can provide higher prediction
accuracy (lowest error of 23.9 F g-1) than that of linear. To address the
controversial between prediction accuracy and error handling, the stacking
model was constructed, which is not only show high accuracy (intercept of 25.03
F g-1), but it also exhibits good noise handling (slope of 43.58 F g-1), making
stacking models a relatively low risk and viable choice for beginner and
experienced machine learning research in electrochemistry. Even though neural
networks (NN) are gaining popularity in the electrochemistry field. However,
this study presents that NN is not suitable for electrochemical data, and
improper tuning resulting in a model that is susceptible to noise. Thus, STACK
models should provide better benefits in that even with untuned base models,
they can achieve an accurate and noise-tolerant model. Overall, this work
provides insight into machine learning model selection for electrochemical
data, which should aid the understanding of data science in chemistry context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshsorna_K/0/1/0/all/0/1&quot;&gt;Krittapong Deshsorna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawtrakul_L/0/1/0/all/0/1&quot;&gt;Luckhana Lawtrakul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iamprasertkun_P/0/1/0/all/0/1&quot;&gt;Pawin Iamprasertkun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10798">
<title>INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis. (arXiv:2311.10798v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10798</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing information from multiple data sources plays a crucial role in
the practice of modern medicine. Current applications of artificial
intelligence in medicine often focus on single-modality data due to a lack of
publicly available, multimodal medical datasets. To address this limitation, we
introduce INSPECT, which contains de-identified longitudinal records from a
large cohort of patients at risk for pulmonary embolism (PE), along with ground
truth labels for multiple outcomes. INSPECT contains data from 19,402 patients,
including CT images, radiology report impression sections, and structured
electronic health record (EHR) data (i.e. demographics, diagnoses, procedures,
vitals, and medications). Using INSPECT, we develop and release a benchmark for
evaluating several baseline modeling approaches on a variety of important PE
related tasks. We evaluate image-only, EHR-only, and multimodal fusion models.
Trained models and the de-identified dataset are made available for
non-commercial use under a data use agreement. To the best of our knowledge,
INSPECT is the largest multimodal dataset integrating 3D medical imaging and
EHR for reproducible methods evaluation and research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shih-Cheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1&quot;&gt;Zepeng Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1&quot;&gt;Ethan Steinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1&quot;&gt;Chia-Chun Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Serena Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1&quot;&gt;Jason A. Fries&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10799">
<title>Adaptive Modelling Approach for Row-Type Dependent Predictive Analysis (RTDPA): A Framework for Designing Machine Learning Models for Credit Risk Analysis in Banking Sector. (arXiv:2311.10799v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10799</link>
<description rdf:parseType="Literal">&lt;p&gt;In many real-world datasets, rows may have distinct characteristics and
require different modeling approaches for accurate predictions. In this paper,
we propose an adaptive modeling approach for row-type dependent predictive
analysis(RTDPA). Our framework enables the development of models that can
effectively handle diverse row types within a single dataset. Our dataset from
XXX bank contains two different risk categories, personal loan and agriculture
loan. each of them are categorised into four classes standard, sub-standard,
doubtful and loss. We performed tailored data pre processing and feature
engineering to different row types. We selected traditional machine learning
predictive models and advanced ensemble techniques. Our findings indicate that
all predictive approaches consistently achieve a precision rate of no less than
90%. For RTDPA, the algorithms are applied separately for each row type,
allowing the models to capture the specific patterns and characteristics of
each row type. This approach enables targeted predictions based on the row
type, providing a more accurate and tailored classification for the given
dataset.Additionally, the suggested model consistently offers decision makers
valuable and enduring insights that are strategic in nature in banking sector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rath_M/0/1/0/all/0/1&quot;&gt;Minati Rath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Date_H/0/1/0/all/0/1&quot;&gt;Hema Date&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10800">
<title>The Next 700 ML-Enabled Compiler Optimizations. (arXiv:2311.10800v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2311.10800</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing interest in enhancing compiler optimizations with ML
models, yet interactions between compilers and ML frameworks remain
challenging. Some optimizations require tightly coupled models and compiler
internals,raising issues with modularity, performance and framework
independence. Practical deployment and transparency for the end-user are also
important concerns. We propose ML-Compiler-Bridge to enable ML model
development within a traditional Python framework while making end-to-end
integration with an optimizing compiler possible and efficient. We evaluate it
on both research and production use cases, for training and inference, over
several optimization problems, multiple compilers and its versions, and gym
infrastructures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+VenkataKeerthy_S/0/1/0/all/0/1&quot;&gt;S. VenkataKeerthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Siddharth Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalvakuntla_U/0/1/0/all/0/1&quot;&gt;Umesh Kalvakuntla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorantla_P/0/1/0/all/0/1&quot;&gt;Pranav Sai Gorantla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitale_R/0/1/0/all/0/1&quot;&gt;Rajiv Shailesh Chitale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1&quot;&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1&quot;&gt;Albert Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trofin_M/0/1/0/all/0/1&quot;&gt;Mircea Trofin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadrasta_R/0/1/0/all/0/1&quot;&gt;Ramakrishna Upadrasta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10801">
<title>Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools. (arXiv:2311.10801v1 [q-fin.PM])</title>
<link>http://arxiv.org/abs/2311.10801</link>
<description rdf:parseType="Literal">&lt;p&gt;Portfolio management (PM) is a fundamental financial trading task, which
explores the optimal periodical reallocation of capitals into different stocks
to pursue long-term profits. Reinforcement learning (RL) has recently shown its
potential to train profitable agents for PM through interacting with financial
markets. However, existing work mostly focuses on fixed stock pools, which is
inconsistent with investors&apos; practical demand. Specifically, the target stock
pool of different investors varies dramatically due to their discrepancy on
market states and individual investors may temporally adjust stocks they desire
to trade (e.g., adding one popular stocks), which lead to customizable stock
pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny
change of the stock pool, which leads to high computational cost and unstable
performance. To tackle this challenge, we propose EarnMore, a rEinforcement
leARNing framework with Maskable stOck REpresentation to handle PM with CSPs
through one-shot training in a global stock pool (GSP). Specifically, we first
introduce a mechanism to mask out the representation of the stocks outside the
target pool. Second, we learn meaningful stock representations through a
self-supervised masking and reconstruction process. Third, a re-weighting
mechanism is designed to make the portfolio concentrate on favorable stocks and
neglect the stocks outside the target pool. Through extensive experiments on 8
subset stock pools of the US stock market, we demonstrate that EarnMore
significantly outperforms 14 state-of-the-art baselines in terms of 6 popular
financial metrics with over 40% improvement on profit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10803">
<title>Robustness Enhancement in Neural Networks with Alpha-Stable Training Noise. (arXiv:2311.10803v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10803</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing use of deep learning on data collected by non-perfect
sensors and in non-perfect environments, the robustness of deep learning
systems has become an important issue. A common approach for obtaining
robustness to noise has been to train deep learning systems with data augmented
with Gaussian noise. In this work, we challenge the common choice of Gaussian
noise and explore the possibility of stronger robustness for non-Gaussian
impulsive noise, specifically alpha-stable noise. Justified by the Generalized
Central Limit Theorem and evidenced by observations in various application
areas, alpha-stable noise is widely present in nature. By comparing the testing
accuracy of models trained with Gaussian noise and alpha-stable noise on data
corrupted by different noise, we find that training with alpha-stable noise is
more effective than Gaussian noise, especially when the dataset is corrupted by
impulsive noise, thus improving the robustness of the model. The generality of
this conclusion is validated through experiments conducted on various deep
learning models with image and time series datasets, and other benchmark
corrupted datasets. Consequently, we propose a novel data augmentation method
that replaces Gaussian noise, which is typically added to the training data,
with alpha-stable noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xueqiong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jipeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuruoglu_E/0/1/0/all/0/1&quot;&gt;Ercan Engin Kuruo&amp;#x11f;lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10805">
<title>Towards a Standardized Reinforcement Learning Framework for AAM Contingency Management. (arXiv:2311.10805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10805</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced Air Mobility (AAM) is the next generation of air transportation that
includes new entrants such as electric vertical takeoff and landing (eVTOL)
aircraft, increasingly autonomous flight operations, and small UAS package
delivery. With these new vehicles and operational concepts comes a desire to
increase densities far beyond what occurs today in and around urban areas, to
utilize new battery technology, and to move toward more autonomously-piloted
aircraft. To achieve these goals, it becomes essential to introduce new safety
management system capabilities that can rapidly assess risk as it evolves
across a span of complex hazards and, if necessary, mitigate risk by executing
appropriate contingencies via supervised or automated decision-making during
flights. Recently, reinforcement learning has shown promise for real-time
decision making across a wide variety of applications including contingency
management. In this work, we formulate the contingency management problem as a
Markov Decision Process (MDP) and integrate the contingency management MDP into
the AAM-Gym simulation framework. This enables rapid prototyping of
reinforcement learning algorithms and evaluation of existing systems, thus
providing a community benchmark for future algorithm development. We report
baseline statistical information for the environment and provide example
performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_L/0/1/0/all/0/1&quot;&gt;Luis E. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brittain_M/0/1/0/all/0/1&quot;&gt;Marc W. Brittain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breeden_K/0/1/0/all/0/1&quot;&gt;Kara Breeden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10806">
<title>SEA++: Multi-Graph-based High-Order Sensor Alignment for Multivariate Time-Series Unsupervised Domain Adaptation. (arXiv:2311.10806v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10806</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) methods have been successful in reducing
label dependency by minimizing the domain discrepancy between a labeled source
domain and an unlabeled target domain. However, these methods face challenges
when dealing with Multivariate Time-Series (MTS) data. MTS data typically
consist of multiple sensors, each with its own unique distribution. This
characteristic makes it hard to adapt existing UDA methods, which mainly focus
on aligning global features while overlooking the distribution discrepancies at
the sensor level, to reduce domain discrepancies for MTS data. To address this
issue, a practical domain adaptation scenario is formulated as Multivariate
Time-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose
SEnsor Alignment (SEA) for MTS-UDA, aiming to reduce domain discrepancy at both
the local and global sensor levels. At the local sensor level, we design
endo-feature alignment, which aligns sensor features and their correlations
across domains. To reduce domain discrepancy at the global sensor level, we
design exo-feature alignment that enforces restrictions on global sensor
features. We further extend SEA to SEA++ by enhancing the endo-feature
alignment. Particularly, we incorporate multi-graph-based high-order alignment
for both sensor features and their correlations. Extensive empirical results
have demonstrated the state-of-the-art performance of our SEA and SEA++ on
public MTS datasets for MTS-UDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuecong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lihua Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10808">
<title>Multiparameter Persistent Homology for Molecular Property Prediction. (arXiv:2311.10808v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2311.10808</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a novel molecular fingerprint generation method
based on multiparameter persistent homology. This approach reveals the latent
structures and relationships within molecular geometry, and detects topological
features that exhibit persistence across multiple scales along multiple
parameters, such as atomic mass, partial charge, and bond type, and can be
further enhanced by incorporating additional parameters like ionization energy,
electron affinity, chirality and orbital hybridization. The proposed
fingerprinting method provides fresh perspectives on molecular structure that
are not easily discernible from single-parameter or single-scale analysis.
Besides, in comparison with traditional graph neural networks, multiparameter
persistent homology has the advantage of providing a more comprehensive and
interpretable characterization of the topology of the molecular data. We have
established theoretical stability guarantees for multiparameter persistent
homology, and have conducted extensive experiments on the Lipophilicity,
FreeSolv, and ESOL datasets to demonstrate its effectiveness in predicting
molecular properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Demir_A/0/1/0/all/0/1&quot;&gt;Andac Demir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kiziltan_B/0/1/0/all/0/1&quot;&gt;Bulent Kiziltan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10811">
<title>A novel post-hoc explanation comparison metric and applications. (arXiv:2311.10811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10811</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanatory systems make the behavior of machine learning models more
transparent, but are often inconsistent. To quantify the differences between
explanatory systems, this paper presents the Shreyan Distance, a novel metric
based on the weighted difference between ranked feature importance lists
produced by such systems. This paper uses the Shreyan Distance to compare two
explanatory systems, SHAP and LIME, for both regression and classification
learning tasks. Because we find that the average Shreyan Distance varies
significantly between these two tasks, we conclude that consistency between
explainers not only depends on inherent properties of the explainers
themselves, but also the type of learning task. This paper further contributes
the XAISuite library, which integrates the Shreyan distance algorithm into
machine learning pipelines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Shreyan Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilpin_L/0/1/0/all/0/1&quot;&gt;Leilani Gilpin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10812">
<title>SplatArmor: Articulated Gaussian splatting for animatable humans from monocular RGB videos. (arXiv:2311.10812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10812</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose SplatArmor, a novel approach for recovering detailed and
animatable human models by `armoring&apos; a parameterized body model with 3D
Gaussians. Our approach represents the human as a set of 3D Gaussians within a
canonical space, whose articulation is defined by extending the skinning of the
underlying SMPL geometry to arbitrary locations in the canonical space. To
account for pose-dependent effects, we introduce a SE(3) field, which allows us
to capture both the location and anisotropy of the Gaussians. Furthermore, we
propose the use of a neural color field to provide color regularization and 3D
supervision for the precise positioning of these Gaussians. We show that
Gaussian splatting provides an interesting alternative to neural rendering
based methods by leverging a rasterization primitive without facing any of the
non-differentiability and optimization challenges typically faced in such
approaches. The rasterization paradigms allows us to leverage forward skinning,
and does not suffer from the ambiguities associated with inverse skinning and
warping. We show compelling results on the ZJU MoCap and People Snapshot
datasets, which underscore the effectiveness of our method for controllable
human synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jena_R/0/1/0/all/0/1&quot;&gt;Rohit Jena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_G/0/1/0/all/0/1&quot;&gt;Ganesh Subramanian Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1&quot;&gt;Siddharth Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1&quot;&gt;Brandon Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James Gee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10832">
<title>Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations. (arXiv:2311.10832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10832</link>
<description rdf:parseType="Literal">&lt;p&gt;In the growing world of artificial intelligence, federated learning is a
distributed learning framework enhanced to preserve the privacy of individuals&apos;
data. Federated learning lays the groundwork for collaborative research in
areas where the data is sensitive. Federated learning has several implications
for real-world problems. In times of crisis, when real-time decision-making is
critical, federated learning allows multiple entities to work collectively
without sharing sensitive data. This distributed approach enables us to
leverage information from multiple sources and gain more diverse insights. This
paper is a systematic review of the literature on privacy-preserving machine
learning in the last few years based on the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Specifically, we have
presented an extensive review of supervised/unsupervised machine learning
algorithms, ensemble methods, meta-heuristic approaches, blockchain technology,
and reinforcement learning used in the framework of federated learning, in
addition to an overview of federated learning applications. This paper reviews
the literature on the components of federated learning and its applications in
the last few years. The main purpose of this work is to provide researchers and
practitioners with a comprehensive overview of federated learning from the
machine learning point of view. A discussion of some open problems and future
research directions in federated learning is also provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jafarigol_E/0/1/0/all/0/1&quot;&gt;Elaheh Jafarigol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1&quot;&gt;Theodore Trafalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razzaghi_T/0/1/0/all/0/1&quot;&gt;Talayeh Razzaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamankhani_M/0/1/0/all/0/1&quot;&gt;Mona Zamankhani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10835">
<title>Accelerating L-shaped Two-stage Stochastic SCUC with Learning Integrated Benders Decomposition. (arXiv:2311.10835v1 [math.OC])</title>
<link>http://arxiv.org/abs/2311.10835</link>
<description rdf:parseType="Literal">&lt;p&gt;Benders decomposition is widely used to solve large mixed-integer problems.
This paper takes advantage of machine learning and proposes enhanced variants
of Benders decomposition for solving two-stage stochastic security-constrained
unit commitment (SCUC). The problem is decomposed into a master problem and
subproblems corresponding to a load scenario. The goal is to reduce the
computational costs and memory usage of Benders decomposition by creating
tighter cuts and reducing the size of the master problem. Three approaches are
proposed, namely regression Benders, classification Benders, and
regression-classification Benders. A regressor reads load profile scenarios and
predicts subproblem objective function proxy variables to form tighter cuts for
the master problem. A criterion is defined to measure the level of usefulness
of cuts with respect to their contribution to lower bound improvement. Useful
cuts that contain the necessary information to form the feasible region are
identified with and without a classification learner. Useful cuts are
iteratively added to the master problem, and non-useful cuts are discarded to
reduce the computational burden of each Benders iteration. Simulation studies
on multiple test systems show the effectiveness of the proposed learning-aided
Benders decomposition for solving two-stage SCUC as compared to conventional
multi-cut Benders decomposition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hasan_F/0/1/0/all/0/1&quot;&gt;Fouad Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kargarian_A/0/1/0/all/0/1&quot;&gt;Amin Kargarian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10847">
<title>Token-level Adaptation of LoRA Adapters for Downstream Task Generalization. (arXiv:2311.10847v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.10847</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a method for adapting LoRA adapters in smaller-sized
language models to arbitrary downstream tasks. Unlike standard
mixture-of-expert architectures, our method employs a gradient-free routing
function to choose a weighted combination of experts without increasing the
compute requirements for training or inference. The results show that
token-level adaptation of LoRA adapters outperforms the base Llama-2-7b model
across mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension
(SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that
the average performance of token-level adaptation outperforms individual models
fine-tuned for each of the tasks with the best performance observed in
adaptation of every-other token during inference. The code for this study is
made available through a public repository.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belofsky_J/0/1/0/all/0/1&quot;&gt;Joshua Belofsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10857">
<title>WATUNet: A Deep Neural Network for Segmentation of Volumetric Sweep Imaging Ultrasound. (arXiv:2311.10857v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10857</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective. Limited access to breast cancer diagnosis globally leads to
delayed treatment. Ultrasound, an effective yet underutilized method, requires
specialized training for sonographers, which hinders its widespread use.
Approach. Volume sweep imaging (VSI) is an innovative approach that enables
untrained operators to capture high-quality ultrasound images. Combined with
deep learning, like convolutional neural networks (CNNs), it can potentially
transform breast cancer diagnosis, enhancing accuracy, saving time and costs,
and improving patient outcomes. The widely used UNet architecture, known for
medical image segmentation, has limitations, such as vanishing gradients and a
lack of multi-scale feature extraction and selective region attention. In this
study, we present a novel segmentation model known as Wavelet_Attention_UNet
(WATUNet). In this model, we incorporate wavelet gates (WGs) and attention
gates (AGs) between the encoder and decoder instead of a simple connection to
overcome the limitations mentioned, thereby improving model performance. Main
results. Two datasets are utilized for the analysis. The public &quot;Breast
Ultrasound Images&quot; (BUSI) dataset of 780 images and a VSI dataset of 3818
images. Both datasets contained segmented lesions categorized into three types:
no mass, benign mass, and malignant mass. Our segmentation results show
superior performance compared to other deep networks. The proposed algorithm
attained a Dice coefficient of 0.94 and an F1 score of 0.94 on the VSI dataset
and scored 0.93 and 0.94 on the public dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khaledyan_D/0/1/0/all/0/1&quot;&gt;Donya Khaledyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marini_T/0/1/0/all/0/1&quot;&gt;Thomas J. Marini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+OConnell_A/0/1/0/all/0/1&quot;&gt;Avice OConnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_S/0/1/0/all/0/1&quot;&gt;Steven Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kan_J/0/1/0/all/0/1&quot;&gt;Jonah Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brennan_G/0/1/0/all/0/1&quot;&gt;Galen Brennan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baran_T/0/1/0/all/0/1&quot;&gt;Timothy M.Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parker_K/0/1/0/all/0/1&quot;&gt;Kevin J. Parker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10859">
<title>A Quadratic Speedup in Finding Nash Equilibria of Quantum Zero-Sum Games. (arXiv:2311.10859v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.10859</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in domains such as non-local games, quantum interactive
proofs, and quantum generative adversarial networks have renewed interest in
quantum game theory and, specifically, quantum zero-sum games. Central to
classical game theory is the efficient algorithmic computation of Nash
equilibria, which represent optimal strategies for both players. In 2008, Jain
and Watrous proposed the first classical algorithm for computing equilibria in
quantum zero-sum games using the Matrix Multiplicative Weight Updates (MMWU)
method to achieve a convergence rate of $\mathcal{O}(d/\epsilon^2)$ iterations
to $\epsilon$-Nash equilibria in the $4^d$-dimensional spectraplex. In this
work, we propose a hierarchy of quantum optimization algorithms that generalize
MMWU via an extra-gradient mechanism. Notably, within this proposed hierarchy,
we introduce the Optimistic Matrix Multiplicative Weights Update (OMMWU)
algorithm and establish its average-iterate convergence complexity as
$\mathcal{O}(d/\epsilon)$ iterations to $\epsilon$-Nash equilibria. This
quadratic speed-up relative to Jain and Watrous&apos; original algorithm sets a new
benchmark for computing $\epsilon$-Nash equilibria in quantum zero-sum games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Vasconcelos_F/0/1/0/all/0/1&quot;&gt;Francisca Vasconcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Vlatakis_Gkaragkounis_E/0/1/0/all/0/1&quot;&gt;Emmanouil-Vasileios Vlatakis-Gkaragkounis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Mertikopoulos_P/0/1/0/all/0/1&quot;&gt;Panayotis Mertikopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Piliouras_G/0/1/0/all/0/1&quot;&gt;Georgios Piliouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael I. Jordan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10863">
<title>Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks. (arXiv:2311.10863v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10863</link>
<description rdf:parseType="Literal">&lt;p&gt;Several methods have been proposed recently to learn neural network (NN)
controllers for autonomous agents, with unknown and stochastic dynamics, tasked
with complex missions captured by Linear Temporal Logic (LTL). Due to the
sample-inefficiency of the majority of these works, compositional learning
methods have been proposed decomposing the LTL specification into smaller
sub-tasks. Then, separate controllers are learned and composed to satisfy the
original task. A key challenge within these approaches is that they often lack
safety guarantees or the provided guarantees are impractical. This paper aims
to address this challenge. Particularly, we consider autonomous systems with
unknown and stochastic dynamics and LTL-encoded tasks. We assume that the
system is equipped with a finite set of base skills modeled by trained NN
feedback controllers. Our goal is to check if there exists a temporal
composition of the trained NN controllers - and if so, to compute it - that
will yield a composite system behavior that satisfies the assigned LTL task
with probability one. We propose a new approach that relies on a novel
integration of automata theory and data-driven reachability analysis tools for
NN-controlled stochastic systems. The resulting neuro-symbolic controller
allows the agent to generate safe behaviors for unseen complex temporal logic
tasks in a zero-shot fashion by leveraging its base skills. We show correctness
of the proposed method and we provide conditions under which it is complete. To
the best of our knowledge, this is the first work that designs verified
temporal compositions of NN controllers for unknown and stochastic systems.
Finally, we provide extensive numerical simulations and hardware experiments on
robot navigation tasks to demonstrate the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zihe Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1&quot;&gt;Yiannis Kantaros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10879">
<title>Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation. (arXiv:2311.10879v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10879</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite its benefits for tumour detection and treatment, the administration
of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated
with a range of issues, including their invasiveness, bioaccumulation, and a
risk of nephrogenic systemic fibrosis. This study explores the feasibility of
producing synthetic contrast enhancements by translating pre-contrast
T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI
sequence leveraging the capabilities of a generative adversarial network (GAN).
Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for
quantitatively evaluating the quality of synthetic data in a principled manner
and serving as a basis for selecting the optimal generative model. We assess
the generated DCE-MRI data using quantitative image quality metrics and apply
them to the downstream task of 3D breast tumour segmentation. Our results
highlight the potential of post-contrast DCE-MRI synthesis in enhancing the
robustness of breast tumour segmentation models via data augmentation. Our code
is available at https://github.com/RichardObi/pre_post_synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1&quot;&gt;Richard Osuala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Smriti Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsirikoglou_A/0/1/0/all/0/1&quot;&gt;Apostolia Tsirikoglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1&quot;&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pinaya_W/0/1/0/all/0/1&quot;&gt;Walter H. L. Pinaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1&quot;&gt;Oliver Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1&quot;&gt;Karim Lekadir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10886">
<title>A Whole New Ball Game: A Primal Accelerated Method for Matrix Games and Minimizing the Maximum of Smooth Functions. (arXiv:2311.10886v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2311.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;We design algorithms for minimizing $\max_{i\in[n]} f_i(x)$ over a
$d$-dimensional Euclidean or simplex domain. When each $f_i$ is $1$-Lipschitz
and $1$-smooth, our method computes an $\epsilon$-approximate solution using
$\widetilde{O}(n \epsilon^{-1/3} + \epsilon^{-2})$ gradient and function
evaluations, and $\widetilde{O}(n \epsilon^{-4/3})$ additional runtime. For
large $n$, our evaluation complexity is optimal up to polylogarithmic factors.
In the special case where each $f_i$ is linear -- which corresponds to finding
a near-optimal primal strategy in a matrix game -- our method finds an
$\epsilon$-approximate solution in runtime $\widetilde{O}(n (d/\epsilon)^{2/3}
+ nd + d\epsilon^{-2})$. For $n&amp;gt;d$ and $\epsilon=1/\sqrt{n}$ this improves over
all existing first-order methods. When additionally $d = \omega(n^{8/11})$ our
runtime also improves over all known interior point methods.
&lt;/p&gt;
&lt;p&gt;Our algorithm combines three novel primitives: (1) A dynamic data structure
which enables efficient stochastic gradient estimation in small $\ell_2$ or
$\ell_1$ balls. (2) A mirror descent algorithm tailored to our data structure
implementing an oracle which minimizes the objective over these balls. (3) A
simple ball oracle acceleration framework suitable for non-Euclidean geometry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1&quot;&gt;Yair Carmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jambulapati_A/0/1/0/all/0/1&quot;&gt;Arun Jambulapati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yujia Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sidford_A/0/1/0/all/0/1&quot;&gt;Aaron Sidford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10892">
<title>The Hidden Linear Structure in Score-Based Models and its Application. (arXiv:2311.10892v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10892</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based models have achieved remarkable results in the generative
modeling of many domains. By learning the gradient of smoothed data
distribution, they can iteratively generate samples from complex distribution
e.g. natural images.
&lt;/p&gt;
&lt;p&gt;However, is there any universal structure in the gradient field that will
eventually be learned by any neural network? Here, we aim to find such
structures through a normative analysis of the score function.
&lt;/p&gt;
&lt;p&gt;First, we derived the closed-form solution to the scored-based model with a
Gaussian score. We claimed that for well-trained diffusion models, the learned
score at a high noise scale is well approximated by the linear score of
Gaussian. We demonstrated this through empirical validation of pre-trained
images diffusion model and theoretical analysis of the score function. This
finding enabled us to precisely predict the initial diffusion trajectory using
the analytical solution and to accelerate image sampling by 15-30\% by skipping
the initial phase without sacrificing image quality. Our finding of the linear
structure in the score-based model has implications for better model design and
data pre-processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vastola_J/0/1/0/all/0/1&quot;&gt;John J. Vastola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10899">
<title>Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning. (arXiv:2311.10899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10899</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline&apos;s effectiveness in the end using standard
metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Shaunak Joshi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaggar_R/0/1/0/all/0/1&quot;&gt;Raghav Gaggar&lt;/a&gt; (1) ((1) University of Southern California)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10908">
<title>Equivariant Neural Operator Learning with Graphon Convolution. (arXiv:2311.10908v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10908</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a general architecture that combines the coefficient learning
scheme with a residual operator layer for learning mappings between continuous
functions in the 3D Euclidean space. Our proposed model is guaranteed to
achieve SE(3)-equivariance by design. From the graph spectrum view, our method
can be interpreted as convolution on graphons (dense graphs with infinitely
many nodes), which we term InfGCN. By leveraging both the continuous graphon
structure and the discrete graph structure of the input data, our model can
effectively capture the geometric information while preserving equivariance.
Through extensive experiments on large-scale electron density datasets, we
observed that our model significantly outperformed the current state-of-the-art
architectures. Multiple ablation studies were also carried out to demonstrate
the effectiveness of the proposed architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chaoran Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jian Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10919">
<title>PACOL: Poisoning Attacks Against Continual Learners. (arXiv:2311.10919v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10919</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning algorithms are typically exposed to untrusted sources that
contain training data inserted by adversaries and bad actors. An adversary can
insert a small number of poisoned samples, such as mislabeled samples from
previously learned tasks, or intentional adversarial perturbed samples, into
the training datasets, which can drastically reduce the model&apos;s performance. In
this work, we demonstrate that continual learning systems can be manipulated by
malicious misinformation and present a new category of data poisoning attacks
specific for continual learners, which we refer to as {\em Poisoning Attacks
Against Continual Learners} (PACOL). The effectiveness of labeling flipping
attacks inspires PACOL; however, PACOL produces attack samples that do not
change the sample&apos;s label and produce an attack that causes catastrophic
forgetting. A comprehensive set of experiments shows the vulnerability of
commonly used generative replay and regularization-based continual learning
approaches against attack methods. We evaluate the ability of label-flipping
and a new adversarial poison attack, namely PACOL proposed in this work, to
force the continual learning system to forget the knowledge of a learned
task(s). More specifically, we compared the performance degradation of
continual learning systems trained on benchmark data streams with and without
poisoning attacks. Moreover, we discuss the stealthiness of the attacks in
which we test the success rate of data sanitization defense and other outlier
detection-based defenses for filtering out adversarial samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ditzler_G/0/1/0/all/0/1&quot;&gt;Gregory Ditzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10921">
<title>Compact and Intuitive Airfoil Parameterization Method through Physics-aware Variational Autoencoder. (arXiv:2311.10921v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10921</link>
<description rdf:parseType="Literal">&lt;p&gt;Airfoil shape optimization plays a critical role in the design of
high-performance aircraft. However, the high-dimensional nature of airfoil
representation causes the challenging problem known as the &quot;curse of
dimensionality&quot;. To overcome this problem, numerous airfoil parameterization
methods have been developed, which can be broadly classified as
polynomial-based and data-driven approaches. Each of these methods has
desirable characteristics such as flexibility, parsimony, feasibility, and
intuitiveness, but a single approach that encompasses all of these attributes
has yet to be found. For example, polynomial-based methods struggle to balance
parsimony and flexibility, while data-driven methods lack in feasibility and
intuitiveness. In recent years, generative models, such as generative
adversarial networks and variational autoencoders, have shown promising
potential in airfoil parameterization. However, these models still face
challenges related to intuitiveness due to their black-box nature. To address
this issue, we developed a novel airfoil parameterization method using
physics-aware variational autoencoder. The proposed method not only explicitly
separates the generation of thickness and camber distributions to produce
smooth and non-intersecting airfoils, thereby improving feasibility, but it
also directly aligns its latent dimensions with geometric features of the
airfoil, significantly enhancing intuitiveness. Finally, extensive comparative
studies were performed to demonstrate the effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yu-Eop Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dawoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1&quot;&gt;Kwanjung Yee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10927">
<title>Near-Optimal Fair Resource Allocation for Strategic Agents without Money: A Data-Driven Approach. (arXiv:2311.10927v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2311.10927</link>
<description rdf:parseType="Literal">&lt;p&gt;We study learning-based design of fair allocation mechanisms for divisible
resources, using proportional fairness (PF) as a benchmark. The learning
setting is a significant departure from the classic mechanism design
literature, in that, we need to learn fair mechanisms solely from data. In
particular, we consider the challenging problem of learning one-shot allocation
mechanisms -- without the use of money -- that incentivize strategic agents to
be truthful when reporting their valuations. It is well-known that the
mechanism that directly seeks to optimize PF is not incentive compatible,
meaning that the agents can potentially misreport their preferences to gain
increased allocations. We introduce the notion of &quot;exploitability&quot; of a
mechanism to measure the relative gain in utility from misreport, and make the
following important contributions in the paper: (i) Using sophisticated
techniques inspired by differentiable convex programming literature, we design
a numerically efficient approach for computing the exploitability of the PF
mechanism. This novel contribution enables us to quantify the gap that needs to
be bridged to approximate PF via incentive compatible mechanisms. (ii) Next, we
modify the PF mechanism to introduce a trade-off between fairness and
exploitability. By properly controlling this trade-off using data, we show that
our proposed mechanism, ExPF-Net, provides a strong approximation to the PF
mechanism while maintaining low exploitability. This mechanism, however, comes
with a high computational cost. (iii) To address the computational challenges,
we propose another mechanism ExS-Net, which is end-to-end parameterized by a
neural network. ExS-Net enjoys similar (slightly inferior) performance and
significantly accelerated training and inference time performance. (iv)
Extensive numerical simulations demonstrate the robustness and efficacy of the
proposed mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Sihan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1&quot;&gt;Sujay Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreacic_E/0/1/0/all/0/1&quot;&gt;Eleonora Kreacic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanzadeh_P/0/1/0/all/0/1&quot;&gt;Parisa Hassanzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koppel_A/0/1/0/all/0/1&quot;&gt;Alec Koppel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1&quot;&gt;Sumitra Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10935">
<title>Short-term Volatility Estimation for High Frequency Trades using Gaussian processes (GPs). (arXiv:2311.10935v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2311.10935</link>
<description rdf:parseType="Literal">&lt;p&gt;The fundamental theorem behind financial markets is that stock prices are
intrinsically complex and stochastic. One of the complexities is the volatility
associated with stock prices. Volatility is a tendency for prices to change
unexpectedly [1]. Price volatility is often detrimental to the return
economics, and thus, investors should factor it in whenever making investment
decisions, choices, and temporal or permanent moves. It is, therefore, crucial
to make necessary and regular short and long-term stock price volatility
forecasts for the safety and economics of investors returns. These forecasts
should be accurate and not misleading. Different models and methods, such as
ARCH GARCH models, have been intuitively implemented to make such forecasts.
However, such traditional means fail to capture the short-term volatility
forecasts effectively. This paper, therefore, investigates and implements a
combination of numeric and probabilistic models for short-term volatility and
return forecasting for high-frequency trades. The essence is that one-day-ahead
volatility forecasts were made with Gaussian Processes (GPs) applied to the
outputs of a Numerical market prediction (NMP) model. Firstly, the stock price
data from NMP was corrected by a GP. Since it is not easy to set price limits
in a market due to its free nature and randomness, a Censored GP was used to
model the relationship between the corrected stock prices and returns.
Forecasting errors were evaluated using the implied and estimated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Mushunje_L/0/1/0/all/0/1&quot;&gt;Leonard Mushunje&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Mashasha_M/0/1/0/all/0/1&quot;&gt;Maxwell Mashasha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Chandiwana_E/0/1/0/all/0/1&quot;&gt;Edina Chandiwana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10937">
<title>Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical Scenario Generation in Automated Vehicle Validation. (arXiv:2311.10937v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10937</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated driving vehicles~(ADV) promise to enhance driving efficiency and
safety, yet they face intricate challenges in safety-critical scenarios. As a
result, validating ADV within generated safety-critical scenarios is essential
for both development and performance evaluations. This paper investigates the
complexities of employing two major scenario-generation solutions: data-driven
and knowledge-driven methods. Data-driven methods derive scenarios from
recorded datasets, efficiently generating scenarios by altering the existing
behavior or trajectories of traffic participants but often falling short in
considering ADV perception; knowledge-driven methods provide effective coverage
through expert-designed rules, but they may lead to inefficiency in generating
safety-critical scenarios within that coverage. To overcome these challenges,
we introduce BridgeGen, a safety-critical scenario generation framework,
designed to bridge the benefits of both methodologies. Specifically, by
utilizing ontology-based techniques, BridgeGen models the five scenario layers
in the operational design domain (ODD) from knowledge-driven methods, ensuring
broad coverage, and incorporating data-driven strategies to efficiently
generate safety-critical scenarios. An optimized scenario generation toolkit is
developed within BridgeGen. This expedites the crafting of safety-critical
scenarios through a combination of traditional optimization and reinforcement
learning schemes. Extensive experiments conducted using Carla simulator
demonstrate the effectiveness of BridgeGen in generating diverse
safety-critical scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1&quot;&gt;Kunkun Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1&quot;&gt;Wen Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Songyang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yuxi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zijiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10953">
<title>HungerGist: An Interpretable Predictive Model for Food Insecurity. (arXiv:2311.10953v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.10953</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating food insecurity in Africa, caused by factors such as war,
climate change, and poverty, demonstrates the critical need for advanced early
warning systems. Traditional methodologies, relying on expert-curated data
encompassing climate, geography, and social disturbances, often fall short due
to data limitations, hindering comprehensive analysis and potential discovery
of new predictive factors. To address this, this paper introduces &quot;HungerGist&quot;,
a multi-task deep learning model utilizing news texts and NLP techniques. Using
a corpus of over 53,000 news articles from nine African countries over four
years, we demonstrate that our model, trained solely on news data, outperforms
the baseline method trained on both traditional risk factors and human-curated
keywords. In addition, our method has the ability to detect critical texts that
contain interpretable signals known as &quot;gists.&quot; Moreover, our examination of
these gists indicates that this approach has the potential to reveal latent
factors that would otherwise remain concealed in unstructured texts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1&quot;&gt;Yongsu Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Muheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Ru Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10954">
<title>Taxonomic analysis of asteroids with artificial neural networks. (arXiv:2311.10954v1 [astro-ph.EP])</title>
<link>http://arxiv.org/abs/2311.10954</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the surface composition of asteroids with visible and/or infrared
spectroscopy. For example, asteroid taxonomy is based on the spectral features
or multiple color indices in visible and near-infrared wavelengths. The
composition of asteroids gives key information to understand their origin and
evolution. However, we lack compositional information for faint asteroids due
to limits of ground-based observational instruments. In the near future, the
Chinese Space Survey telescope (CSST) will provide multiple colors and
spectroscopic data for asteroids of apparent magnitude brighter than 25 mag and
23 mag, respectively. For the aim of analysis of the CSST spectroscopic data,
we applied an algorithm using artificial neural networks (ANNs) to establish a
preliminary classification model for asteroid taxonomy according to the design
of the survey module of CSST. Using the SMASS II spectra and the Bus-Binzel
taxonomy system, our ANN classification tool composed of 5 individual ANNs is
constructed, and the accuracy of this classification system is higher than 92
%. As the first application of our ANN tool, 64 spectra of 42 asteroids
obtained in 2006 and 2007 by us with the 2.16-m telescope in the Xinglong
station (Observatory Code 327) of National Astronomical Observatory of China
are analyzed. The predicted labels of these spectra using our ANN tool are
found to be reasonable when compared to their known taxonomic labels.
Considering the accuracy and stability, our ANN tool can be applied to analyse
the CSST asteroid spectra in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Luo_N/0/1/0/all/0/1&quot;&gt;Nanping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaobin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Shenghong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Penttila_A/0/1/0/all/0/1&quot;&gt;Antti Penttil&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Muinonen_K/0/1/0/all/0/1&quot;&gt;Karri Muinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yisi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10962">
<title>Classification Methods Based on Machine Learning for the Analysis of Fetal Health Data. (arXiv:2311.10962v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10962</link>
<description rdf:parseType="Literal">&lt;p&gt;The persistent battle to decrease childhood mortality serves as a commonly
employed benchmark for gauging advancements in the field of medicine. Globally,
the under-5 mortality rate stands at approximately 5 million, with a
significant portion of these deaths being avoidable. Given the significance of
this problem, Machine learning-based techniques have emerged as a prominent
tool for assessing fetal health. In this work, we have analyzed the
classification performance of various machine learning models for fetal health
analysis. Classification performance of various machine learning models, such
as support vector machine (SVM), random forest(RF), and attentive interpretable
tabular learning (TabNet) have been assessed on fetal health. Moreover,
dimensionality reduction techniques, such as Principal component analysis (PCA)
and Linear discriminant analysis (LDA) have been implemented to obtain better
classification performance with less number of features. A TabNet model on a
fetal health dataset provides a classification accuracy of 94.36%. In general,
this technology empowers doctors and healthcare experts to achieve precise
fetal health classification and identify the most influential features in the
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regmi_B/0/1/0/all/0/1&quot;&gt;Binod Regmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chiranjibi Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10963">
<title>Learning Deterministic Finite Automata from Confidence Oracles. (arXiv:2311.10963v1 [cs.FL])</title>
<link>http://arxiv.org/abs/2311.10963</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss the problem of learning a deterministic finite automaton (DFA)
from a confidence oracle. That is, we are given access to an oracle $Q$ with
incomplete knowledge of some target language $L$ over an alphabet $\Sigma$; the
oracle maps a string $x\in\Sigma^*$ to a score in the interval $[-1,1]$
indicating its confidence that the string is in the language. The
interpretation is that the sign of the score signifies whether $x\in L$, while
the magnitude $|Q(x)|$ represents the oracle&apos;s confidence. Our goal is to learn
a DFA representation of the oracle that preserves the information that it is
confident in. The learned DFA should closely match the oracle wherever it is
highly confident, but it need not do this when the oracle is less sure of
itself.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wilson Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10972">
<title>Polynomial-Time Solutions for ReLU Network Training: A Complexity Classification via Max-Cut and Zonotopes. (arXiv:2311.10972v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10972</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the complexity of training a two-layer ReLU neural network
with weight decay regularization. Previous research has shown that the optimal
solution of this problem can be found by solving a standard cone-constrained
convex program. Using this convex formulation, we prove that the hardness of
approximation of ReLU networks not only mirrors the complexity of the Max-Cut
problem but also, in certain special cases, exactly corresponds to it. In
particular, when $\epsilon\leq\sqrt{84/83}-1\approx 0.006$, we show that it is
NP-hard to find an approximate global optimizer of the ReLU network objective
with relative error $\epsilon$ with respect to the objective value. Moreover,
we develop a randomized algorithm which mirrors the Goemans-Williamson rounding
of semidefinite Max-Cut relaxations. To provide polynomial-time approximations,
we classify training datasets into three categories: (i) For orthogonal
separable datasets, a precise solution can be obtained in polynomial-time. (ii)
When there is a negative correlation between samples of different classes, we
give a polynomial-time approximation with relative error $\sqrt{\pi/2}-1\approx
0.253$. (iii) For general datasets, the degree to which the problem can be
approximated in polynomial-time is governed by a geometric factor that controls
the diameter of two zonotopes intrinsic to the dataset. To our knowledge, these
results present the first polynomial-time approximation guarantees along with
first hardness of approximation results for regularized ReLU networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1&quot;&gt;Mert Pilanci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10983">
<title>Multiple View Geometry Transformers for 3D Human Pose Estimation. (arXiv:2311.10983v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10983</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we aim to improve the 3D reasoning ability of Transformers in
multi-view 3D human pose estimation. Recent works have focused on end-to-end
learning-based transformer designs, which struggle to resolve geometric
information accurately, particularly during occlusion. Instead, we propose a
novel hybrid model, MVGFormer, which has a series of geometric and appearance
modules organized in an iterative manner. The geometry modules are
learning-free and handle all viewpoint-dependent 3D tasks geometrically which
notably improves the model&apos;s generalization ability. The appearance modules are
learnable and are dedicated to estimating 2D poses from image signals
end-to-end which enables them to achieve accurate estimates even when occlusion
occurs, leading to a model that is both accurate and generalizable to new
cameras and geometries. We evaluate our approach for both in-domain and
out-of-domain settings, where our model consistently outperforms
state-of-the-art methods, and especially does so by a significant margin in the
out-of-domain setting. We will release the code and models:
https://github.com/XunshanMan/MVGFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jialiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven L. Waslander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10986">
<title>EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge. (arXiv:2311.10986v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10986</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) models have been widely deployed on IoT devices with the
help of advancements in DL algorithms and chips. However, the limited resources
of edge devices make these on-device DL models hard to be generalizable to
diverse environments and tasks. Although the recently emerged foundation models
(FMs) show impressive generalization power, how to effectively leverage the
rich knowledge of FMs on resource-limited edge devices is still not explored.
In this paper, we propose EdgeFM, a novel edge-cloud cooperative system with
open-set recognition capability. EdgeFM selectively uploads unlabeled data to
query the FM on the cloud and customizes the specific knowledge and
architectures for edge models. Meanwhile, EdgeFM conducts dynamic model
switching at run-time taking into account both data uncertainty and dynamic
network variations, which ensures the accuracy always close to the original FM.
We implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on
three public datasets and two self-collected datasets. Results show that EdgeFM
can reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy
increase compared with the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bufang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lixing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_N/0/1/0/all/0/1&quot;&gt;Neiwen Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_G/0/1/0/all/0/1&quot;&gt;Guoliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shuai_X/0/1/0/all/0/1&quot;&gt;Xian Shuai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10996">
<title>BrainZ-BP: A Non-invasive Cuff-less Blood Pressure Estimation Approach Leveraging Brain Bio-impedance and Electrocardiogram. (arXiv:2311.10996v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10996</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and continuous blood pressure (BP) monitoring is essential to the
early prevention of cardiovascular diseases. Non-invasive and cuff-less BP
estimation algorithm has gained much attention in recent years. Previous
studies have demonstrated that brain bio-impedance (BIOZ) is a promising
technique for non-invasive intracranial pressure (ICP) monitoring. Clinically,
treatment for patients with traumatic brain injuries (TBI) requires monitoring
the ICP and BP of patients simultaneously. Estimating BP by brain BIOZ directly
can reduce the number of sensors attached to the patients, thus improving their
comfort. To address the issues, in this study, we explore the feasibility of
leveraging brain BIOZ for BP estimation and propose a novel cuff-less BP
estimation approach called BrainZ-BP. Two electrodes are placed on the forehead
and occipital bone of the head in the anterior-posterior direction for brain
BIOZ measurement. Various features including pulse transit time and
morphological features of brain BIOZ are extracted and fed into four regression
models for BP estimation. Results show that the mean absolute error, root mean
square error, and correlation coefficient of random forest regression model are
2.17 mmHg, 3.91 mmHg, and 0.90 for systolic pressure estimation, and are 1.71
mmHg, 3.02 mmHg, and 0.89 for diastolic pressure estimation. The presented
BrainZ-BP can be applied in the brain BIOZ-based ICP monitoring scenario to
monitor BP simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bufang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Le Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mengliang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongxing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xinbao Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11003">
<title>Wasserstein Convergence Guarantees for a General Class of Score-Based Generative Models. (arXiv:2311.11003v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.11003</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based generative models (SGMs) is a recent class of deep generative
models with state-of-the-art performance in many applications. In this paper,
we establish convergence guarantees for a general class of SGMs in
2-Wasserstein distance, assuming accurate score estimates and smooth
log-concave data distribution. We specialize our result to several concrete
SGMs with specific choices of forward processes modelled by stochastic
differential equations, and obtain an upper bound on the iteration complexity
for each model, which demonstrates the impacts of different choices of the
forward processes. We also provide a lower bound when the data distribution is
Gaussian. Numerically, we experiment SGMs with different forward processes,
some of which are newly proposed in this paper, for unconditional image
generation on CIFAR-10. We find that the experimental results are in good
agreement with our theoretical predictions on the iteration complexity, and the
models with our newly proposed forward processes can outperform existing
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hoang M. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lingjiong Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11018">
<title>SORTAD: Self-Supervised Optimized Random Transformations for Anomaly Detection in Tabular Data. (arXiv:2311.11018v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.11018</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a self-supervised approach to anomaly detection in tabular data.
Random transformations are applied to the data, and then each transformation is
identified based on its output. These predicted transformations are used to
identify anomalies. In tabular data this approach faces many challenges that
are related to the uncorrelated nature of the data. These challenges affect the
transformations that should be used, as well as the use of their predictions.
To this end, we propose SORTAD, a novel algorithm that is tailor-made to solve
these challenges. SORTAD optimally chooses random transformations that help the
classification process, and have a scoring function that is more sensitive to
the changes in the transformations classification prediction encountered in
tabular data. SORTAD achieved state-of-the-art results on multiple commonly
used anomaly detection data sets, as well as in the overall results across all
data sets tested.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_G/0/1/0/all/0/1&quot;&gt;Guy Hay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liberman_P/0/1/0/all/0/1&quot;&gt;Pablo Liberman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11019">
<title>Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels. (arXiv:2311.11019v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.11019</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning fine-grained embeddings from coarse labels is a challenging task due
to limited label granularity supervision, i.e., lacking the detailed
distinctions required for fine-grained tasks. The task becomes even more
demanding when attempting few-shot fine-grained recognition, which holds
practical significance in various applications. To address these challenges, we
propose a novel method that embeds visual embeddings into a hyperbolic space
and enhances their discriminative ability with a hierarchical cosine margins
manner. Specifically, the hyperbolic space offers distinct advantages,
including the ability to capture hierarchical relationships and increased
expressive power, which favors modeling fine-grained objects. Based on the
hyperbolic space, we further enforce relatively large/small similarity margins
between coarse/fine classes, respectively, yielding the so-called hierarchical
cosine margins manner. While enforcing similarity margins in the regular
Euclidean space has become popular for deep embedding learning, applying it to
the hyperbolic space is non-trivial and validating the benefit for
coarse-to-fine generalization is valuable. Extensive experiments conducted on
five benchmark datasets showcase the effectiveness of our proposed method,
yielding state-of-the-art results surpassing competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shu-Lin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Faen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1&quot;&gt;Anqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11046">
<title>DenseNet and Support Vector Machine classifications of major depressive disorder using vertex-wise cortical features. (arXiv:2311.11046v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2311.11046</link>
<description rdf:parseType="Literal">&lt;p&gt;Major depressive disorder (MDD) is a complex psychiatric disorder that
affects the lives of hundreds of millions of individuals around the globe. Even
today, researchers debate if morphological alterations in the brain are linked
to MDD, likely due to the heterogeneity of this disorder. The application of
deep learning tools to neuroimaging data, capable of capturing complex
non-linear patterns, has the potential to provide diagnostic and predictive
biomarkers for MDD. However, previous attempts to demarcate MDD patients and
healthy controls (HC) based on segmented cortical features via linear machine
learning approaches have reported low accuracies. In this study, we used
globally representative data from the ENIGMA-MDD working group containing an
extensive sample of people with MDD (N=2,772) and HC (N=4,240), which allows a
comprehensive analysis with generalizable results. Based on the hypothesis that
integration of vertex-wise cortical features can improve classification
performance, we evaluated the classification of a DenseNet and a Support Vector
Machine (SVM), with the expectation that the former would outperform the
latter. As we analyzed a multi-site sample, we additionally applied the ComBat
harmonization tool to remove potential nuisance effects of site. We found that
both classifiers exhibited close to chance performance (balanced accuracy
DenseNet: 51%; SVM: 53%), when estimated on unseen sites. Slightly higher
classification performance (balanced accuracy DenseNet: 58%; SVM: 55%) was
found when the cross-validation folds contained subjects from all sites,
indicating site effect. In conclusion, the integration of vertex-wise
morphometric features and the use of the non-linear classifier did not lead to
the differentiability between MDD and HC. Our results support the notion that
MDD classification on this combination of features and classifiers is
unfeasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Belov_V/0/1/0/all/0/1&quot;&gt;Vladimir Belov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Erwin_Grabner_T/0/1/0/all/0/1&quot;&gt;Tracy Erwin-Grabner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Ling-Li Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ching_C/0/1/0/all/0/1&quot;&gt;Christopher R. K. Ching&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aleman_A/0/1/0/all/0/1&quot;&gt;Andre Aleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Amod_A/0/1/0/all/0/1&quot;&gt;Alyssa R. Amod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Basgoze_Z/0/1/0/all/0/1&quot;&gt;Zeynep Basgoze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Benedetti_F/0/1/0/all/0/1&quot;&gt;Francesco Benedetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Besteher_B/0/1/0/all/0/1&quot;&gt;Bianca Besteher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Brosch_K/0/1/0/all/0/1&quot;&gt;Katharina Brosch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bulow_R/0/1/0/all/0/1&quot;&gt;Robin B&amp;#xfc;low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Colle_R/0/1/0/all/0/1&quot;&gt;Romain Colle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Connolly_C/0/1/0/all/0/1&quot;&gt;Colm G. Connolly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Corruble_E/0/1/0/all/0/1&quot;&gt;Emmanuelle Corruble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Couvy_Duchesne_B/0/1/0/all/0/1&quot;&gt;Baptiste Couvy-Duchesne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cullen_K/0/1/0/all/0/1&quot;&gt;Kathryn Cullen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dannlowski_U/0/1/0/all/0/1&quot;&gt;Udo Dannlowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Davey_C/0/1/0/all/0/1&quot;&gt;Christopher G. Davey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dols_A/0/1/0/all/0/1&quot;&gt;Annemiek Dols&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ernsting_J/0/1/0/all/0/1&quot;&gt;Jan Ernsting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Evans_J/0/1/0/all/0/1&quot;&gt;Jennifer W. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fisch_L/0/1/0/all/0/1&quot;&gt;Lukas Fisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fuentes_Claramonte_P/0/1/0/all/0/1&quot;&gt;Paola Fuentes-Claramonte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gonul_A/0/1/0/all/0/1&quot;&gt;Ali Saffet Gonul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gotlib_I/0/1/0/all/0/1&quot;&gt;Ian H. Gotlib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Grabe_H/0/1/0/all/0/1&quot;&gt;Hans J. Grabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Groenewold_N/0/1/0/all/0/1&quot;&gt;Nynke A. Groenewold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Grotegerd_D/0/1/0/all/0/1&quot;&gt;Dominik Grotegerd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hahn_T/0/1/0/all/0/1&quot;&gt;Tim Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hamilton_J/0/1/0/all/0/1&quot;&gt;J. Paul Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Laura K.M. Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Harrison_B/0/1/0/all/0/1&quot;&gt;Ben J Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tiffany C. Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jahanshad_N/0/1/0/all/0/1&quot;&gt;Neda Jahanshad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jamieson_A/0/1/0/all/0/1&quot;&gt;Alec J. Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Karuk_A/0/1/0/all/0/1&quot;&gt;Andriana Karuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kircher_T/0/1/0/all/0/1&quot;&gt;Tilo Kircher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Klimes_Dougan_B/0/1/0/all/0/1&quot;&gt;Bonnie Klimes-Dougan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Koopowitz_S/0/1/0/all/0/1&quot;&gt;Sheri-Michelle Koopowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lancaster_T/0/1/0/all/0/1&quot;&gt;Thomas Lancaster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Leenings_R/0/1/0/all/0/1&quot;&gt;Ramona Leenings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Meng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Linden_D/0/1/0/all/0/1&quot;&gt;David E. J. Linden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+MacMaster_F/0/1/0/all/0/1&quot;&gt;Frank P. MacMaster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mehler_D/0/1/0/all/0/1&quot;&gt;David M. A. Mehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Meinert_S/0/1/0/all/0/1&quot;&gt;Susanne Meinert&lt;/a&gt;, et al. (42 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11055">
<title>Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework. (arXiv:2311.11055v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.11055</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-based medical technologies, including wearables, telemedicine, LLMs, and
digital care twins, significantly impact healthcare. Ensuring AI results are
accurate and interpretable is crucial, especially for clinicians. This paper
reviews processes and challenges of interpretable ML (IML) and explainable AI
(XAI) in healthcare. Objectives include reviewing XAI processes, methods,
applications, and challenges, with a focus on quality control. The IML process
is classified into data pre-processing interpretability, interpretable
modeling, and post-processing interpretability. The paper aims to establish the
importance of robust interpretability in healthcare through experimental
results, providing insights for creating communicable clinician-AI tools.
Research questions, eligibility criteria, and goals were identified following
PRISMA and PICO methods. PubMed, Scopus, and Web of Science were systematically
searched using specific strings. The survey introduces a step-by-step roadmap
for implementing XAI in clinical applications, addressing existing gaps and
acknowledging XAI model limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasarian_E/0/1/0/all/0/1&quot;&gt;Elham Nasarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1&quot;&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharyac_U/0/1/0/all/0/1&quot;&gt;U. Rajendra Acharyac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsui_d/0/1/0/all/0/1&quot;&gt;d Kwok-Leung Tsui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1904.10552">
<title>ML-KFHE: Multi-label ensemble classification algorithm exploiting sensor fusion properties of the Kalman filter. (arXiv:1904.10552v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1904.10552</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of ensemble classification methods in multi-class
classification problems, ensemble methods based on approaches other than
bagging have not been widely explored for multi-label classification problems.
The Kalman Filter-based Heuristic Ensemble (KFHE) is an ensemble method that
exploits the sensor fusion properties of the Kalman filter to combine several
classifier models, and that has been shown to be very effective. This work
proposes a multi-label version of KFHE, ML-KFHE, demonstrating the
effectiveness of the KFHE method on multi-label datasets. Two variants are
introduced based on the underlying component classifier algorithm,
ML-KFHE-HOMER, and ML-KFHE-CC which uses HOMER and Classifier Chain (CC) as the
underlying multi-label algorithms respectively. ML-KFHE-HOMER and ML-KFHE-CC
sequentially train multiple HOMER and CC multi-label classifiers and aggregate
their outputs using the sensor fusion properties of the Kalman filter.
Extensive experiments and detailed analysis were performed on thirteen
multi-label datasets and eight other algorithms, which included
state-of-the-art ensemble methods. The results show, for both versions, the
ML-KFHE framework improves the predictive performance significantly with
respect to bagged combinations of HOMER (named E-HOMER), also introduced in
this paper, and bagged combination of CC, Ensemble Classifier Chains (ECC),
thus demonstrating the effectiveness of ML-KFHE. Also, the ML-KFHE-HOMER
variant was found to perform consistently and significantly better than the
compared multi-label methods including existing approaches based on ensembles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakrashi_A/0/1/0/all/0/1&quot;&gt;Arjun Pakrashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.07606">
<title>timeXplain -- A Framework for Explaining the Predictions of Time Series Classifiers. (arXiv:2007.07606v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2007.07606</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern time series classifiers display impressive predictive capabilities,
yet their decision-making processes mostly remain black boxes to the user. At
the same time, model-agnostic explainers, such as the recently proposed SHAP,
promise to make the predictions of machine learning models interpretable,
provided there are well-designed domain mappings. We bring both worlds together
in our timeXplain framework, extending the reach of explainable artificial
intelligence to time series classification and value prediction. We present
novel domain mappings for the time domain, frequency domain, and time series
statistics and analyze their explicative power as well as their limits. We
employ a novel evaluation metric to experimentally compare timeXplain to
several model-specific explanation approaches for state-of-the-art time series
classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mujkanovic_F/0/1/0/all/0/1&quot;&gt;Felix Mujkanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doskoc_V/0/1/0/all/0/1&quot;&gt;Vanja Dosko&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirneck_M/0/1/0/all/0/1&quot;&gt;Martin Schirneck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_P/0/1/0/all/0/1&quot;&gt;Patrick Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_T/0/1/0/all/0/1&quot;&gt;Tobias Friedrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.00789">
<title>Role Taxonomy of Units in Deep Neural Networks. (arXiv:2011.00789v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2011.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying the role of network units in deep neural networks (DNNs) is
critical in many aspects including giving understandings on the mechanisms of
DNNs and building basic connections between deep learning and neuroscience.
However, there remains unclear on which roles the units in DNNs with different
generalization ability could present. To this end, we give role taxonomy of
units in DNNs via introducing the retrieval-of-function test, where units are
categorized into four types in terms of their functional preference on
separately the training set and testing set. We show that ratios of the four
categories are highly associated with the generalization ability of DNNs from
two distinct perspectives, based on which we give signs of DNNs with well
generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.02785">
<title>Unsupervised embedding of trajectories captures the latent structure of scientific migration. (arXiv:2012.02785v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2012.02785</link>
<description rdf:parseType="Literal">&lt;p&gt;Human migration and mobility drives major societal phenomena including
epidemics, economies, innovation, and the diffusion of ideas. Although human
mobility and migration have been heavily constrained by geographic distance
throughout the history, advances and globalization are making other factors
such as language and culture increasingly more important. Advances in neural
embedding models, originally designed for natural language, provide an
opportunity to tame this complexity and open new avenues for the study of
migration. Here, we demonstrate the ability of the model word2vec to encode
nuanced relationships between discrete locations from migration trajectories,
producing an accurate, dense, continuous, and meaningful vector-space
representation. The resulting representation provides a functional distance
between locations, as well as a digital double that can be distributed,
re-used, and itself interrogated to understand the many dimensions of
migration. We show that the unique power of word2vec to encode migration
patterns stems from its mathematical equivalence with the gravity model of
mobility. Focusing on the case of scientific migration, we apply word2vec to a
database of three million migration trajectories of scientists derived from the
affiliations listed on their publication records. Using techniques that
leverage its semantic structure, we demonstrate that embeddings can learn the
rich structure that underpins scientific migration, such as cultural,
linguistic, and prestige relationships at multiple levels of granularity. Our
results provide a theoretical foundation and methodological framework for using
neural embeddings to represent and understand migration both within and beyond
science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1&quot;&gt;Dakota Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jisung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kojaku_S/0/1/0/all/0/1&quot;&gt;Sadamori Kojaku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costas_R/0/1/0/all/0/1&quot;&gt;Rodrigo Costas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_W/0/1/0/all/0/1&quot;&gt;Woo-Sung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milojevic_S/0/1/0/all/0/1&quot;&gt;Sta&amp;#x161;a Milojevi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1&quot;&gt;Yong-Yeol Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.03920">
<title>Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning for Reliable Simulator-Based Inference. (arXiv:2107.03920v8 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2107.03920</link>
<description rdf:parseType="Literal">&lt;p&gt;Many areas of science make extensive use of computer simulators that
implicitly encode intractable likelihood functions of complex systems.
Classical statistical methods are poorly suited for these so-called
likelihood-free inference (LFI) settings, especially outside asymptotic and
low-dimensional regimes. At the same time, traditional LFI methods - such as
Approximate Bayesian Computation or more recent machine learning techniques -
do not guarantee confidence sets with nominal coverage in general settings
(i.e., with high-dimensional data, finite sample sizes, and for any parameter
value). In addition, there are no diagnostic tools to check the empirical
coverage of confidence sets provided by such methods across the entire
parameter space. In this work, we propose a unified and modular inference
framework that bridges classical statistics and modern machine learning
providing (i) a practical approach to the Neyman construction of confidence
sets with frequentist finite-sample coverage for any value of the unknown
parameters; and (ii) interpretable diagnostics that estimate the empirical
coverage across the entire parameter space. We refer to the general framework
as likelihood-free frequentist inference (LF2I). Any method that defines a test
statistic can leverage LF2I to create valid confidence sets and diagnostics
without costly Monte Carlo samples at fixed parameter settings. We study the
power of two likelihood-based test statistics (ACORE and BFF) and demonstrate
their empirical performance on high-dimensional, complex data. Code is
available at https://github.com/lee-group-cmu/lf2i.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dalmasso_N/0/1/0/all/0/1&quot;&gt;Niccol&amp;#xf2; Dalmasso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Masserano_L/0/1/0/all/0/1&quot;&gt;Luca Masserano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;David Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1&quot;&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.00527">
<title>Gates Are Not What You Need in RNNs. (arXiv:2108.00527v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2108.00527</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks have flourished in many areas. Consequently, we can
see new RNN cells being developed continuously, usually by creating or using
gates in a new, original way. But what if we told you that gates in RNNs are
redundant? In this paper, we propose a new recurrent cell called Residual
Recurrent Unit (RRU) which beats traditional cells and does not employ a single
gate. It is based on the residual shortcut connection, linear transformations,
ReLU, and normalization. To evaluate our cell&apos;s effectiveness, we compare its
performance against the widely-used GRU and LSTM cells and the recently
proposed Mogrifier LSTM on several tasks including, polyphonic music modeling,
language modeling, and sentiment analysis. Our experiments show that RRU
outperforms the traditional gated units on most of these tasks. Also, it has
better robustness to parameter selection, allowing immediate application in new
tasks without much tuning. We have implemented the RRU in TensorFlow, and the
code is made available at https://github.com/LUMII-Syslab/RRU .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakovskis_R/0/1/0/all/0/1&quot;&gt;Ronalds Zakovskis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Draguns_A/0/1/0/all/0/1&quot;&gt;Andis Draguns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaile_E/0/1/0/all/0/1&quot;&gt;Eliza Gaile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozolins_E/0/1/0/all/0/1&quot;&gt;Emils Ozolins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1&quot;&gt;Karlis Freivalds&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.13836">
<title>Explainable AI for engineering design: A unified approach of systems engineering and component-based deep learning. (arXiv:2108.13836v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2108.13836</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven models created by machine learning gain in importance in all
fields of design and engineering. They have high potential to assist
decision-makers in creating novel artefacts with better performance and
sustainability. However, limited generalization and the black-box nature of
these models lead to limited explainability and reusability. To overcome this
situation, we propose a component-based approach to create partial component
models by machine learning (ML). This component-based approach aligns deep
learning with systems engineering (SE). For the domain of energy efficient
building design, we first demonstrate better generalization of the
component-based method by analyzing prediction accuracy outside the training
data. We observe a much higher accuracy (R2 = 0.94) compared to conventional
monolithic methods (R2 = 0.71). Second, we illustrate explainability by
exemplary demonstrating how sensitivity information from SE and rules from
low-depth decision trees serve engineering. Third, we evaluate explainability
by qualitative and quantitative methods demonstrating the matching of
preliminary knowledge and data-driven derived strategies and show correctness
of activations at component interfaces compared to white-box simulation results
(envelope components: R2 = 0.92..0.99; zones: R2 = 0.78..0.93). The key for
component-based explainability is that activations at interfaces between the
components are interpretable engineering quantities. The large range of
possible configurations in composing components allows the examination of novel
unseen design cases with understandable data-driven models. The matching of
parameter ranges of components by similar probability distribution produces
reusable, well-generalizing, and trustworthy models. The approach adapts the
model structure to engineering methods of systems engineering and to domain
knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_P/0/1/0/all/0/1&quot;&gt;Philipp Geyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Manav Mahan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xia Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.01140">
<title>Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.01140</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid mutation of the influenza virus threatens public health.
Reassortment among viruses with different hosts can lead to a fatal pandemic.
However, it is difficult to detect the original host of the virus during or
after an outbreak as influenza viruses can circulate between different species.
Therefore, early and rapid detection of the viral host would help reduce the
further spread of the virus. We use various machine learning models with
features derived from the position-specific scoring matrix (PSSM) and features
learned from word embedding and word encoding to infer the origin host of
viruses. The results show that the performance of the PSSM-based model reaches
the MCC around 95%, and the F1 around 96%. The MCC obtained using the model
with word embedding is around 96%, and the F1 is around 97%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanhua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wojtczak_D/0/1/0/all/0/1&quot;&gt;Dominik Wojtczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.09369">
<title>A Variational Autoencoder for Heterogeneous Temporal and Longitudinal Data. (arXiv:2204.09369v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.09369</link>
<description rdf:parseType="Literal">&lt;p&gt;The variational autoencoder (VAE) is a popular deep latent variable model
used to analyse high-dimensional datasets by learning a low-dimensional latent
representation of the data. It simultaneously learns a generative model and an
inference network to perform approximate posterior inference. Recently proposed
extensions to VAEs that can handle temporal and longitudinal data have
applications in healthcare, behavioural modelling, and predictive maintenance.
However, these extensions do not account for heterogeneous data (i.e., data
comprising of continuous and discrete attributes), which is common in many
real-life applications. In this work, we propose the heterogeneous longitudinal
VAE (HL-VAE) that extends the existing temporal and longitudinal VAEs to
heterogeneous data. HL-VAE provides efficient inference for high-dimensional
datasets and includes likelihood models for continuous, count, categorical, and
ordinal data while accounting for missing observations. We demonstrate our
model&apos;s efficacy through simulated as well as clinical datasets, and show that
our proposed model achieves competitive performance in missing value imputation
and predictive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogretir_M/0/1/0/all/0/1&quot;&gt;Mine &amp;#xd6;&amp;#x11f;retir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramchandran_S/0/1/0/all/0/1&quot;&gt;Siddharth Ramchandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papatheodorou_D/0/1/0/all/0/1&quot;&gt;Dimitrios Papatheodorou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahdesmaki_H/0/1/0/all/0/1&quot;&gt;Harri L&amp;#xe4;hdesm&amp;#xe4;ki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.04712">
<title>Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey. (arXiv:2205.04712v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.04712</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of representative datasets is an essential prerequisite for
many successful artificial intelligence and machine learning models. However,
in real life applications these models often encounter scenarios that are
inadequately represented in the data used for training. There are various
reasons for the absence of sufficient data, ranging from time and cost
constraints to ethical considerations. As a consequence, the reliable usage of
these models, especially in safety-critical applications, is still a tremendous
challenge. Leveraging additional, already existing sources of knowledge is key
to overcome the limitations of purely data-driven approaches. Knowledge
augmented machine learning approaches offer the possibility of compensating for
deficiencies, errors, or ambiguities in the data, thus increasing the
generalization capability of the applied models. Even more, predictions that
conform with knowledge are crucial for making trustworthy and safe decisions
even in underrepresented scenarios. This work provides an overview of existing
techniques and methods in the literature that combine data-driven models with
existing knowledge. The identified approaches are structured according to the
categories knowledge integration, extraction and conformity. In particular, we
address the application of the presented methods in the field of autonomous
driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wormann_J/0/1/0/all/0/1&quot;&gt;Julian W&amp;#xf6;rmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1&quot;&gt;Daniel Bogdoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunner_C/0/1/0/all/0/1&quot;&gt;Christian Brunner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buhrle_E/0/1/0/all/0/1&quot;&gt;Etienne B&amp;#xfc;hrle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Han Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuo_E/0/1/0/all/0/1&quot;&gt;Evaristus Fuh Chuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1&quot;&gt;Kostadin Cvejoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elst_L/0/1/0/all/0/1&quot;&gt;Ludger van Elst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschall_P/0/1/0/all/0/1&quot;&gt;Philip Gottschall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griesche_S/0/1/0/all/0/1&quot;&gt;Stefan Griesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellert_C/0/1/0/all/0/1&quot;&gt;Christian Hellert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hesels_C/0/1/0/all/0/1&quot;&gt;Christian Hesels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houben_S/0/1/0/all/0/1&quot;&gt;Sebastian Houben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_T/0/1/0/all/0/1&quot;&gt;Tim Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keil_N/0/1/0/all/0/1&quot;&gt;Niklas Keil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelsch_J/0/1/0/all/0/1&quot;&gt;Johann Kelsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keser_M/0/1/0/all/0/1&quot;&gt;Mert Keser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konigshof_H/0/1/0/all/0/1&quot;&gt;Hendrik K&amp;#xf6;nigshof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraft_E/0/1/0/all/0/1&quot;&gt;Erwin Kraft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreuser_L/0/1/0/all/0/1&quot;&gt;Leonie Kreuser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krone_K/0/1/0/all/0/1&quot;&gt;Kevin Krone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latka_T/0/1/0/all/0/1&quot;&gt;Tobias Latka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattern_D/0/1/0/all/0/1&quot;&gt;Denny Mattern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_S/0/1/0/all/0/1&quot;&gt;Stefan Matthes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motzkus_F/0/1/0/all/0/1&quot;&gt;Franz Motzkus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1&quot;&gt;Mohsin Munir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nekolla_M/0/1/0/all/0/1&quot;&gt;Moritz Nekolla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1&quot;&gt;Adrian Paschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilchau_S/0/1/0/all/0/1&quot;&gt;Stefan Pilar von Pilchau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pintz_M/0/1/0/all/0/1&quot;&gt;Maximilian Alexander Pintz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tianming Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureishi_F/0/1/0/all/0/1&quot;&gt;Faraz Qureishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvi_S/0/1/0/all/0/1&quot;&gt;Syed Tahseen Raza Rizvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichardt_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Reichardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueden_L/0/1/0/all/0/1&quot;&gt;Laura von Rueden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagel_A/0/1/0/all/0/1&quot;&gt;Alexander Sagel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasdelli_D/0/1/0/all/0/1&quot;&gt;Diogo Sasdelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholl_T/0/1/0/all/0/1&quot;&gt;Tobias Scholl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schunk_G/0/1/0/all/0/1&quot;&gt;Gerhard Schunk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwalbe_G/0/1/0/all/0/1&quot;&gt;Gesina Schwalbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeb_Y/0/1/0/all/0/1&quot;&gt;Youssef Shoeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stapelbroek_H/0/1/0/all/0/1&quot;&gt;Hendrik Stapelbroek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stehr_V/0/1/0/all/0/1&quot;&gt;Vera Stehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_G/0/1/0/all/0/1&quot;&gt;Gurucharan Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;Anh Tuan Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vivekanandan_A/0/1/0/all/0/1&quot;&gt;Abhishek Vivekanandan&lt;/a&gt;, et al. (5 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.06750">
<title>Provably Safe Reinforcement Learning: Conceptual Analysis, Survey, and Benchmarking. (arXiv:2205.06750v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.06750</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the safety of reinforcement learning (RL) algorithms is crucial to
unlock their potential for many real-world tasks. However, vanilla RL and most
safe RL approaches do not guarantee safety. In recent years, several methods
have been proposed to provide hard safety guarantees for RL, which is essential
for applications where unsafe actions could have disastrous consequences.
Nevertheless, there is no comprehensive comparison of these provably safe RL
methods. Therefore, we introduce a categorization of existing provably safe RL
methods, present the conceptual foundations for both continuous and discrete
action spaces, and empirically benchmark existing methods. We categorize the
methods based on how they adapt the action: action replacement, action
projection, and action masking. Our experiments on an inverted pendulum and a
quadrotor stabilization task indicate that action replacement is the
best-performing approach for these applications despite its comparatively
simple realization. Furthermore, adding a reward penalty, every time the safety
verification is engaged, improved training performance in our experiments.
Finally, we provide practical guidance on selecting provably safe RL approaches
depending on the safety specification, RL algorithm, and type of action space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krasowski_H/0/1/0/all/0/1&quot;&gt;Hanna Krasowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thumm_J/0/1/0/all/0/1&quot;&gt;Jakob Thumm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1&quot;&gt;Marlon M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1&quot;&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Althoff_M/0/1/0/all/0/1&quot;&gt;Matthias Althoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11365">
<title>Graph-Based Methods for Discrete Choice. (arXiv:2205.11365v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11365</link>
<description rdf:parseType="Literal">&lt;p&gt;Choices made by individuals have widespread impacts--for instance, people
choose between political candidates to vote for, between social media posts to
share, and between brands to purchase--moreover, data on these choices are
increasingly abundant. Discrete choice models are a key tool for learning
individual preferences from such data. Additionally, social factors like
conformity and contagion influence individual choice. Traditional methods for
incorporating these factors into choice models do not account for the entire
social network and require hand-crafted features. To overcome these
limitations, we use graph learning to study choice in networked contexts. We
identify three ways in which graph learning techniques can be used for discrete
choice: learning chooser representations, regularizing choice model parameters,
and directly constructing predictions from a network. We design methods in each
category and test them on real-world choice datasets, including county-level
2016 US election results and Android app installation and usage data. We show
that incorporating social network structure can improve the predictions of the
standard econometric choice model, the multinomial logit. We provide evidence
that app installations are influenced by social context, but we find no such
effect on app usage among the same participants, which instead is habit-driven.
In the election data, we highlight the additional insights a discrete choice
framework provides over classification or regression, the typical approaches.
On synthetic data, we demonstrate the sample complexity benefit of using social
information in choice models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomlinson_K/0/1/0/all/0/1&quot;&gt;Kiran Tomlinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1&quot;&gt;Austin R. Benson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.15439">
<title>StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis. (arXiv:2205.15439v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2205.15439</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-Speech (TTS) has recently seen great progress in synthesizing
high-quality speech owing to the rapid development of parallel TTS systems, but
producing speech with naturalistic prosodic variations, speaking styles and
emotional tones remains challenging. Moreover, since duration and speech are
generated separately, parallel TTS models still have problems finding the best
monotonic alignments that are crucial for naturalistic speech synthesis. Here,
we propose StyleTTS, a style-based generative model for parallel TTS that can
synthesize diverse speech with natural prosody from a reference speech
utterance. With novel Transferable Monotonic Aligner (TMA) and
duration-invariant data augmentation schemes, our method significantly
outperforms state-of-the-art models on both single and multi-speaker datasets
in subjective tests of speech naturalness and speaker similarity. Through
self-supervised learning of the speaking styles, our model can synthesize
speech with the same prosodic and emotional tone as any given reference speech
without the need for explicitly labeling these categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghao Aaron Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1&quot;&gt;Nima Mesgarani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02789">
<title>Efficient and Accurate Physics-aware Multiplex Graph Neural Networks for 3D Small Molecules and Macromolecule Complexes. (arXiv:2206.02789v3 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02789</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in applying Graph Neural Networks (GNNs) to molecular science
have showcased the power of learning three-dimensional (3D) structure
representations with GNNs. However, most existing GNNs suffer from the
limitations of insufficient modeling of diverse interactions, computational
expensive operations, and ignorance of vectorial values. Here, we tackle these
limitations by proposing a novel GNN model, Physics-aware Multiplex Graph
Neural Network (PaxNet), to efficiently and accurately learn the
representations of 3D molecules for both small organic compounds and
macromolecule complexes. PaxNet separates the modeling of local and non-local
interactions inspired by molecular mechanics, and reduces the expensive
angle-related computations. Besides scalar properties, PaxNet can also predict
vectorial properties by learning an associated vector for each atom. To
evaluate the performance of PaxNet, we compare it with state-of-the-art
baselines in two tasks. On small molecule dataset for predicting quantum
chemical properties, PaxNet reduces the prediction error by 15% and uses 73%
less memory than the best baseline. On macromolecule dataset for predicting
protein-ligand binding affinities, PaxNet outperforms the best baseline while
reducing the memory consumption by 33% and the inference time by 85%. Thus,
PaxNet provides a universal, robust and accurate method for large-scale machine
learning of molecules. Our code is available at
https://github.com/zetayue/Physics-aware-Multiplex-GNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lei Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11303">
<title>Community Recovery in the Geometric Block Model. (arXiv:2206.11303v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11303</link>
<description rdf:parseType="Literal">&lt;p&gt;To capture the inherent geometric features of many community detection
problems, we propose to use a new random graph model of communities that we
call a Geometric Block Model. The geometric block model builds on the random
geometric graphs (Gilbert, 1961), one of the basic models of random graphs for
spatial networks, in the same way that the well-studied stochastic block model
builds on the Erd\H{o}s-R\&apos;{en}yi random graphs. It is also a natural extension
of random community models inspired by the recent theoretical and practical
advancements in community detection. To analyze the geometric block model, we
first provide new connectivity results for random annulus graphs which are
generalizations of random geometric graphs. The connectivity properties of
geometric graphs have been studied since their introduction, and analyzing them
has been more difficult than their Erd\H{o}s-R\&apos;{en}yi counterparts due to
correlated edge formation.
&lt;/p&gt;
&lt;p&gt;We then use the connectivity results of random annulus graphs to provide
necessary and sufficient conditions for efficient recovery of communities for
the geometric block model. We show that a simple triangle-counting algorithm to
detect communities in the geometric block model is near-optimal. For this we
consider the following two regimes of graph density.
&lt;/p&gt;
&lt;p&gt;In the regime where the average degree of the graph grows logarithmically
with the number of vertices, we show that our algorithm performs extremely
well, both theoretically and practically. In contrast, the triangle-counting
algorithm is far from being optimum for the stochastic block model in the
logarithmic degree regime. We simulate our results on both real and synthetic
datasets to show superior performance of both the new model as well as our
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1&quot;&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1&quot;&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1&quot;&gt;Soumyabrata Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1&quot;&gt;Barna Saha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.13272">
<title>Wideband Audio Waveform Evaluation Networks: Efficient, Accurate Estimation of Speech Qualities. (arXiv:2206.13272v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2206.13272</link>
<description rdf:parseType="Literal">&lt;p&gt;Wideband Audio Waveform Evaluation Networks (WAWEnets) are convolutional
neural networks that operate directly on wideband audio waveforms in order to
produce evaluations of those waveforms. In the present work these evaluations
give qualities of telecommunications speech (e.g., noisiness, intelligibility,
overall speech quality). WAWEnets are no-reference networks because they do not
require ``reference&apos;&apos; (original or undistorted) versions of the waveforms they
evaluate. Our initial WAWEnet publication introduced four WAWEnets and each
emulated the output of an established full-reference speech quality or
intelligibility estimation algorithm. We have updated the WAWEnet architecture
to be more efficient and effective. Here we present a single WAWEnet that
closely tracks seven different quality and intelligibility values. We create a
second network that additionally tracks four subjective speech quality
dimensions. We offer a third network that focuses on just subjective quality
scores and achieves very high levels of agreement. This work has leveraged 334
hours of speech in 13 languages, over two million full-reference target values
and over 93,000 subjective mean opinion scores. We also interpret the operation
of WAWEnets and identify the key to their operation using the language of
signal processing: ReLUs strategically move spectral information from non-DC
components into the DC component. The DC values of 96 output signals define a
vector in a 96-D latent space and this vector is then mapped to a quality or
intelligibility value for the input waveform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Catellier_A/0/1/0/all/0/1&quot;&gt;Andrew Catellier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Voran_S/0/1/0/all/0/1&quot;&gt;Stephen Voran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02249">
<title>Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning. (arXiv:2207.02249v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02249</link>
<description rdf:parseType="Literal">&lt;p&gt;Successful deployment of multi-agent reinforcement learning often requires
agents to adapt their behaviour. In this work, we discuss the problem of
teamwork adaptation in which a team of agents needs to adapt their policies to
solve novel tasks with limited fine-tuning. Motivated by the intuition that
agents need to be able to identify and distinguish tasks in order to adapt
their behaviour to the current task, we propose to learn multi-agent task
embeddings (MATE). These task embeddings are trained using an encoder-decoder
architecture optimised for reconstruction of the transition and reward
functions which uniquely identify tasks. We show that a team of agents is able
to adapt to novel tasks when provided with task embeddings. We propose three
MATE training paradigms: independent MATE, centralised MATE, and mixed MATE
which vary in the information used for the task encoding. We show that the
embeddings learned by MATE identify tasks and provide useful information which
agents leverage during adaptation to novel tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_L/0/1/0/all/0/1&quot;&gt;Lukas Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1&quot;&gt;Filippos Christianos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.13405">
<title>Interpreting Black-box Machine Learning Models for High Dimensional Datasets. (arXiv:2208.13405v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.13405</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been shown to outperform traditional machine
learning algorithms in a broad variety of application domains due to their
effectiveness in modeling complex problems and handling high-dimensional
datasets. Many real-life datasets, however, are of increasingly high
dimensionality, where a large number of features may be irrelevant for both
supervised and unsupervised learning tasks. The inclusion of such features
would not only introduce unwanted noise but also increase computational
complexity. Furthermore, due to high non-linearity and dependency among a large
number of features, DNN models tend to be unavoidably opaque and perceived as
black-box methods because of their not well-understood internal functioning.
Their algorithmic complexity is often simply beyond the capacities of humans to
understand the interplay among myriads of hyperparameters. A well-interpretable
model can identify statistically significant features and explain the way they
affect the model&apos;s outcome. In this paper, we propose an efficient method to
improve the interpretability of black-box models for classification tasks in
the case of high-dimensional datasets. First, we train a black-box model on a
high-dimensional dataset to learn the embeddings on which the classification is
performed. To decompose the inner working principles of the black-box model and
to identify top-k important features, we employ different probing and
perturbing techniques. We then approximate the behavior of the black-box model
by means of an interpretable surrogate model on the top-k feature space.
Finally, we derive decision rules and local explanations from the surrogate
model to explain individual decisions. Our approach outperforms
state-of-the-art methods like TabNet and XGboost when tested on different
datasets with varying dimensionality between 50 and 20,000 w.r.t metrics and
explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1&quot;&gt;Md. Rezaul Karim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shajalal_M/0/1/0/all/0/1&quot;&gt;Md. Shajalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grass_A/0/1/0/all/0/1&quot;&gt;Alex Gra&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dohmen_T/0/1/0/all/0/1&quot;&gt;Till D&amp;#xf6;hmen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chala_S/0/1/0/all/0/1&quot;&gt;Sisay Adugna Chala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beecks_C/0/1/0/all/0/1&quot;&gt;Christian Beecks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1&quot;&gt;Stefan Decker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14989">
<title>Learning Multiscale Non-stationary Causal Structures. (arXiv:2208.14989v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14989</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses a gap in the current state of the art by providing a
solution for modeling causal relationships that evolve over time and occur at
different time scales. Specifically, we introduce the multiscale non-stationary
directed acyclic graph (MN-DAG), a framework for modeling multivariate time
series data. Our contribution is twofold. Firstly, we expose a probabilistic
generative model by leveraging results from spectral and causality theories.
Our model allows sampling an MN-DAG according to user-specified priors on the
time-dependence and multiscale properties of the causal graph. Secondly, we
devise a Bayesian method named Multiscale Non-stationary Causal Structure
Learner (MN-CASTLE) that uses stochastic variational inference to estimate
MN-DAGs. The method also exploits information from the local partial
correlation between time series over different time resolutions. The data
generated from an MN-DAG reproduces well-known features of time series in
different domains, such as volatility clustering and serial correlation.
Additionally, we show the superior performance of MN-CASTLE on synthetic data
with different multiscale and non-stationary properties compared to baseline
models. Finally, we apply MN-CASTLE to identify the drivers of the natural gas
prices in the US market. Causal relationships have strengthened during the
COVID-19 outbreak and the Russian invasion of Ukraine, a fact that baseline
methods fail to capture. MN-CASTLE identifies the causal impact of critical
economic drivers on natural gas prices, such as seasonal factors, economic
uncertainty, oil prices, and gas storage deviations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAcunto_G/0/1/0/all/0/1&quot;&gt;Gabriele D&amp;#x27;Acunto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_G/0/1/0/all/0/1&quot;&gt;Gianmarco De Francisci Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajardi_P/0/1/0/all/0/1&quot;&gt;Paolo Bajardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07067">
<title>Efficient learning of nonlinear prediction models with time-series privileged information. (arXiv:2209.07067v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07067</link>
<description rdf:parseType="Literal">&lt;p&gt;In domains where sample sizes are limited, efficient learning algorithms are
critical. Learning using privileged information (LuPI) offers increased sample
efficiency by allowing prediction models access to auxiliary information at
training time which is unavailable when the models are used. In recent work, it
was shown that for prediction in linear-Gaussian dynamical systems, a LuPI
learner with access to intermediate time series data is never worse and often
better in expectation than any unbiased classical learner. We provide new
insights into this analysis and generalize it to nonlinear prediction tasks in
latent dynamical systems, extending theoretical guarantees to the case where
the map connecting latent variables and observations is known up to a linear
transform. In addition, we propose algorithms based on random features and
representation learning for the case when this map is unknown. A suite of
empirical results confirm theoretical findings and show the potential of using
privileged time-series information in nonlinear prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_B/0/1/0/all/0/1&quot;&gt;Bastian Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_F/0/1/0/all/0/1&quot;&gt;Fredrik D Johansson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15345">
<title>PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15345</link>
<description rdf:parseType="Literal">&lt;p&gt;In sparse linear bandits, a learning agent sequentially selects an action and
receive reward feedback, and the reward function depends linearly on a few
coordinates of the covariates of the actions. This has applications in many
real-world sequential decision making problems. In this paper, we propose a
simple and computationally efficient sparse linear estimation method called
PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso
(Tibshirani, 1996) in many problems. Our bound naturally motivates an
experimental design criterion that is convex and thus computationally efficient
to solve. Based on our novel estimator and design criterion, we derive sparse
linear bandit algorithms that enjoy improved regret upper bounds upon the state
of the art (Hao et al., 2020), especially w.r.t. the geometry of the given
action set. Finally, we prove a matching lower bound for sparse linear bandits
in the data-poor regime, which closes the gap between upper and lower bounds in
prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jang_K/0/1/0/all/0/1&quot;&gt;Kyoungseok Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jun_K/0/1/0/all/0/1&quot;&gt;Kwang-Sung Jun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04132">
<title>Stochastic Coded Federated Learning: Theoretical Analysis and Incentive Mechanism Design. (arXiv:2211.04132v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04132</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has achieved great success as a privacy-preserving
distributed training paradigm, where many edge devices collaboratively train a
machine learning model by sharing the model updates instead of the raw data
with a server. However, the heterogeneous computational and communication
resources of edge devices give rise to stragglers that significantly decelerate
the training process. To mitigate this issue, we propose a novel FL framework
named stochastic coded federated learning (SCFL) that leverages coded computing
techniques. In SCFL, before the training process starts, each edge device
uploads a privacy-preserving coded dataset to the server, which is generated by
adding Gaussian noise to the projected local dataset. During training, the
server computes gradients on the global coded dataset to compensate for the
missing model updates of the straggling devices. We design a gradient
aggregation scheme to ensure that the aggregated model update is an unbiased
estimate of the desired global update. Moreover, this aggregation scheme
enables periodical model averaging to improve the training efficiency. We
characterize the tradeoff between the convergence performance and privacy
guarantee of SCFL. In particular, a more noisy coded dataset provides stronger
privacy protection for edge devices but results in learning performance
degradation. We further develop a contract-based incentive mechanism to
coordinate such a conflict. The simulation results show that SCFL learns a
better model within the given time and achieves a better privacy-performance
tradeoff than the baseline methods. In addition, the proposed incentive
mechanism grants better training performance than the conventional Stackelberg
game approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuchang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jiawei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuyi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Songze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10890">
<title>Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph. (arXiv:2211.10890v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10890</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing graph contrastive learning (GCL) techniques typically require two
forward passes for a single instance to construct the contrastive loss, which
is effective for capturing the low-frequency signals of node features. Such a
dual-pass design has shown empirical success on homophilic graphs, but its
effectiveness on heterophilic graphs, where directly connected nodes typically
have different labels, is unknown. In addition, existing GCL approaches fail to
provide strong performance guarantees. Coupled with the unpredictability of GCL
approaches on heterophilic graphs, their applicability in real-world contexts
is limited. Then, a natural question arises: Can we design a GCL method that
works for both homophilic and heterophilic graphs with a performance guarantee?
To answer this question, we theoretically study the concentration property of
features obtained by neighborhood aggregation on homophilic and heterophilic
graphs, introduce the single-pass augmentation-free graph contrastive learning
loss based on the property, and provide performance guarantees for the
minimizer of the loss on downstream tasks. As a direct consequence of our
analysis, we implement the Single-Pass Graph Contrastive Learning method
(SP-GCL). Empirically, on 14 benchmark datasets with varying degrees of
homophily, the features learned by the SP-GCL can match or outperform existing
strong baselines with significantly less computational overhead, which
demonstrates the usefulness of our findings in real-world cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiaokui Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01333">
<title>oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep Learning Compilation. (arXiv:2301.01333v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01333</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of deep learning models and hardware support for
dense computing, the deep learning workload characteristics changed
significantly from a few hot spots on compute-intensive operations to a broad
range of operations scattered across the models. Accelerating a few
compute-intensive operations using the expert-tuned implementation of
primitives does not fully exploit the performance potential of AI hardware.
Various efforts have been made to compile a full deep neural network (DNN)
graph. One of the biggest challenges is to achieve high-performance tensor
compilation by generating expert level performance code for the dense
compute-intensive operations and applying compilation optimization at the scope
of DNN computation graph across multiple compute-intensive operations.
&lt;/p&gt;
&lt;p&gt;We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid
approach of using techniques from both compiler optimization and expert-tuned
kernels for high performance code generation of the deep neural network graph.
oneDNN Graph Compiler addresses unique optimization challenges in the deep
learning domain, such as low-precision computation, aggressive fusion of graph
operations, optimization for static tensor shapes and memory layout, constant
weight optimization, and memory buffer reuse. Experimental results demonstrate
significant performance gains over existing tensor compiler and primitives
library for performance-critical DNN computation graphs and end-to-end models
on Intel Xeon Scalable Processors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhennan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1&quot;&gt;Yijie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jingze Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yunfei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Ciyong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Longsheng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xianhang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Baihui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safonov_I/0/1/0/all/0/1&quot;&gt;Igor Safonov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jason Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1&quot;&gt;Eric Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavery_D/0/1/0/all/0/1&quot;&gt;Dan Lavery&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11290">
<title>Graph Encoder Ensemble for Simultaneous Vertex Embedding and Community Detection. (arXiv:2301.11290v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11290</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel and computationally efficient method for
vertex embedding, community detection, and community size determination. Our
approach leverages a normalized one-hot graph encoder and a rank-based cluster
size measure. Through extensive simulations, we demonstrate the excellent
numerical performance of our proposed graph encoder ensemble algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cencheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Youngser Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03169">
<title>Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03169</link>
<description rdf:parseType="Literal">&lt;p&gt;Selecting a suitable pretraining dataset is crucial for both general-domain
(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We
formalize this problem as selecting a subset of a large raw unlabeled dataset
to match a desired target distribution given unlabeled target samples. Due to
the scale and dimensionality of the raw text data, existing methods use simple
heuristics or require human experts to manually curate data. Instead, we extend
the classic importance resampling approach used in low-dimensions for LM data
selection. We propose Data Selection with Importance Resampling (DSIR), an
efficient and scalable framework that estimates importance weights in a reduced
feature space for tractability and selects data with importance resampling
according to these weights. We instantiate the DSIR framework with hashed
n-gram features for efficiency, enabling the selection of 100M documents from
the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features
preserve the aspects of the data that are relevant to the target, we define KL
reduction, a data metric that measures the proximity between the selected
pretraining data and the target on some feature space. Across 8 data selection
methods (including expert selection), KL reduction on hashed n-gram features
highly correlates with average downstream accuracy (r=0.82). When selecting
data for continued pretraining on a specific domain, DSIR performs comparably
to expert curation across 8 target distributions. When pretraining
general-domain models (target is Wikipedia and books), DSIR improves over
random selection and heuristic filtering baselines by 2-2.5% on the GLUE
benchmark. Code is available at https://github.com/p-lambda/dsir.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Sang Michael Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santurkar_S/0/1/0/all/0/1&quot;&gt;Shibani Santurkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05507">
<title>Language Decision Transformers with Exponential Tilt for Interactive Text Environments. (arXiv:2302.05507v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05507</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based game environments are challenging because agents must deal with
long sequences of text, execute compositional actions using text and learn from
sparse rewards. We address these challenges by proposing Language Decision
Transformers (LDTs), a framework that is based on transformer language models
and decision transformers (DTs). Our LDTs extend DTs with 3 components: (1)
exponential tilt to guide the agent towards high obtainable goals, (2) novel
goal conditioning methods yielding better results than the traditional
return-to-go (sum of all future rewards), and (3) a model of future
observations that improves agent performance. LDTs are the first to address
offline RL with DTs on these challenging games. Our experiments show that LDTs
achieve the highest scores among many different types of agents on some of the
most challenging Jericho games, such as Enchanter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gontier_N/0/1/0/all/0/1&quot;&gt;Nicolas Gontier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1&quot;&gt;Pau Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1&quot;&gt;Issam Laradji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1&quot;&gt;David Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06335">
<title>Online Arbitrary Shaped Clustering through Correlated Gaussian Functions. (arXiv:2302.06335v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06335</link>
<description rdf:parseType="Literal">&lt;p&gt;There is no convincing evidence that backpropagation is a biologically
plausible mechanism, and further studies of alternative learning methods are
needed. A novel online clustering algorithm is presented that can produce
arbitrary shaped clusters from inputs in an unsupervised manner, and requires
no prior knowledge of the number of clusters in the input data. This is
achieved by finding correlated outputs from functions that capture commonly
occurring input patterns. The algorithm can be deemed more biologically
plausible than model optimization through backpropagation, although practical
applicability may require additional research. However, the method yields
satisfactory results on several toy datasets on a noteworthy range of
hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eidheim_O/0/1/0/all/0/1&quot;&gt;Ole Christian Eidheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11552">
<title>Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11552</link>
<description rdf:parseType="Literal">&lt;p&gt;Since their introduction, diffusion models have quickly become the prevailing
approach to generative modeling in many domains. They can be interpreted as
learning the gradients of a time-varying sequence of log-probability density
functions. This interpretation has motivated classifier-based and
classifier-free guidance as methods for post-hoc control of diffusion models.
In this work, we build upon these ideas using the score-based interpretation of
diffusion models, and explore alternative ways to condition, modify, and reuse
diffusion models for tasks involving compositional generation and guidance. In
particular, we investigate why certain types of composition fail using current
techniques and present a number of solutions. We conclude that the sampler (not
the model) is responsible for this failure and propose new samplers, inspired
by MCMC, which enable successful compositional generation. Further, we propose
an energy-based parameterization of diffusion models which enables the use of
new compositional operators and more sophisticated, Metropolis-corrected
samplers. Intriguingly we find these samplers lead to notable improvements in
compositional generation across a wide set of problems such as
classifier-guided ImageNet modeling and compositional text-to-image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durkan_C/0/1/0/all/0/1&quot;&gt;Conor Durkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1&quot;&gt;Robin Strudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dieleman_S/0/1/0/all/0/1&quot;&gt;Sander Dieleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1&quot;&gt;Will Grathwohl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12682">
<title>A DeepONet multi-fidelity approach for residual learning in reduced order modeling. (arXiv:2302.12682v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12682</link>
<description rdf:parseType="Literal">&lt;p&gt;In the present work, we introduce a novel approach to enhance the precision
of reduced order models by exploiting a multi-fidelity perspective and
DeepONets. Reduced models provide a real-time numerical approximation by
simplifying the original model. The error introduced by the such operation is
usually neglected and sacrificed in order to reach a fast computation. We
propose to couple the model reduction to a machine learning residual learning,
such that the above-mentioned error can be learned by a neural network and
inferred for new predictions. We emphasize that the framework maximizes the
exploitation of high-fidelity information, using it for building the reduced
order model and for learning the residual. In this work, we explore the
integration of proper orthogonal decomposition (POD), and gappy POD for sensors
data, with the recent DeepONet architecture. Numerical investigations for a
parametric benchmark function and a nonlinear parametric Navier-Stokes problem
are presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Demo_N/0/1/0/all/0/1&quot;&gt;Nicola Demo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tezzele_M/0/1/0/all/0/1&quot;&gt;Marco Tezzele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rozza_G/0/1/0/all/0/1&quot;&gt;Gianluigi Rozza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13335">
<title>Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13335</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning addresses the challenge of learning by observing an
expert&apos;s demonstrations without access to reward signals from environments.
Most existing imitation learning methods that do not require interacting with
environments either model the expert distribution as the conditional
probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s,
a). Despite its simplicity, modeling the conditional probability with BC
usually struggles with generalization. While modeling the joint probability can
lead to improved generalization performance, the inference procedure is often
time-consuming and the model can suffer from manifold overfitting. This work
proposes an imitation learning framework that benefits from modeling both the
conditional and joint probability of the expert distribution. Our proposed
diffusion model-augmented behavioral cloning (DBC) employs a diffusion model
trained to model expert behaviors and learns a policy to optimize both the BC
loss (conditional) and our proposed diffusion model loss (joint). DBC
outperforms baselines in various continuous control tasks in navigation, robot
arm manipulation, dexterous manipulation, and locomotion. We design additional
experiments to verify the limitations of modeling either the conditional
probability or the joint probability of the expert distribution as well as
compare different generative models. Ablation studies justify the effectiveness
of our design choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shang-Fu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_M/0/1/0/all/0/1&quot;&gt;Ming-Hao Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chun-Mao Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shao-Hua Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14670">
<title>Balanced Training for Sparse GANs. (arXiv:2302.14670v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14670</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, there has been growing interest in developing larger
and deeper neural networks, including deep generative models like generative
adversarial networks (GANs). However, GANs typically come with high
computational complexity, leading researchers to explore methods for reducing
the training and inference costs. One such approach gaining popularity in
supervised learning is dynamic sparse training (DST), which maintains good
performance while enjoying excellent training efficiency. Despite its potential
benefits, applying DST to GANs presents challenges due to the adversarial
nature of the training process. In this paper, we propose a novel metric called
the balance ratio (BR) to study the balance between the sparse generator and
discriminator. We also introduce a new method called balanced dynamic sparse
training (ADAPT), which seeks to control the BR during GAN training to achieve
a good trade-off between performance and computational cost. Our proposed
method shows promising results on multiple datasets, demonstrating its
effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yite Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1&quot;&gt;Naira Hovakimyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruoyu Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03944">
<title>On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level. (arXiv:2303.03944v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03944</link>
<description rdf:parseType="Literal">&lt;p&gt;Bilevel optimization is a popular two-level hierarchical optimization, which
has been widely applied to many machine learning tasks such as hyperparameter
learning, meta learning and continual learning. Although many bilevel
optimization methods recently have been developed, the bilevel methods are not
well studied when the lower-level problem is nonconvex. To fill this gap, in
the paper, we study a class of nonconvex bilevel optimization problems, where
both upper-level and lower-level problems are nonconvex, and the lower-level
problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient
momentum-based gradient bilevel method (MGBiO) to solve these deterministic
problems. Meanwhile, we propose a class of efficient momentum-based stochastic
gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic
problems. Moreover, we provide a useful convergence analysis framework for our
methods. Specifically, under some mild conditions, we prove that our MGBiO
method has a sample (or gradient) complexity of $O(\epsilon^{-2})$ for finding
an $\epsilon$-stationary solution of the deterministic bilevel problems (i.e.,
$\|\nabla F(x)\|\leq \epsilon$), which improves the existing best results by a
factor of $O(\epsilon^{-1})$. Meanwhile, we prove that our MSGBiO and VR-MSGBiO
methods have sample complexities of $\tilde{O}(\epsilon^{-4})$ and
$\tilde{O}(\epsilon^{-3})$, respectively, in finding an $\epsilon$-stationary
solution of the stochastic bilevel problems (i.e., $\mathbb{E}\|\nabla
F(x)\|\leq \epsilon$), which improves the existing best results by a factor of
$\tilde{O}(\epsilon^{-3})$. Extensive experimental results on bilevel PL game
and hyper-representation learning demonstrate the efficiency of our algorithms.
This paper commemorates the mathematician Boris Polyak (1935 -2023).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Feihu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04292">
<title>ERUDITE: Human-in-the-Loop IoT for an Adaptive Personalized Learning System. (arXiv:2303.04292v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04292</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the rapid growth in wearable technologies and recent advancement in
machine learning and signal processing, monitoring complex human contexts
becomes feasible, paving the way to develop human-in-the-loop IoT systems that
naturally evolve to adapt to the human and environment state autonomously.
Nevertheless, a central challenge in designing many of these IoT systems arises
from the requirement to infer the human mental state, such as intention,
stress, cognition load, or learning ability. While different human contexts can
be inferred from the fusion of different sensor modalities that can correlate
to a particular mental state, the human brain provides a richer sensor modality
that gives us more insights into the required human context. This paper
proposes ERUDITE, a human-in-the-loop IoT system for the learning environment
that exploits recent wearable neurotechnology to decode brain signals. Through
insights from concept learning theory, ERUDITE can infer the human state of
learning and understand when human learning increases or declines. By
quantifying human learning as an input sensory signal, ERUDITE can provide
adequate personalized feedback to humans in a learning environment to enhance
their learning experience. ERUDITE is evaluated across $15$ participants and
showed that by using the brain signals as a sensor modality to infer the human
learning state and providing personalized adaptation to the learning
environment, the participants&apos; learning performance increased on average by
$26\%$. Furthermore, we showed that ERUDITE can be deployed on an edge-based
prototype to evaluate its practicality and scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taherisadr_M/0/1/0/all/0/1&quot;&gt;Mojtaba Taherisadr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1&quot;&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmalaki_S/0/1/0/all/0/1&quot;&gt;Salma Elmalaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07393">
<title>Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v3 [q-fin.TR] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07393</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the dynamics and the interactions of multiple reinforcement
learning optimal execution trading agents interacting with a reactive
Agent-Based Model (ABM) of a financial market in event time. The model
represents a market ecology with 3-trophic levels represented by: optimal
execution learning agents, minimally intelligent liquidity takers, and fast
electronic liquidity providers. The optimal execution agent classes include
buying and selling agents that can either use a combination of limit orders and
market orders, or only trade using market orders. The reward function
explicitly balances trade execution slippage against the penalty of not
executing the order timeously. This work demonstrates how multiple competing
learning agents impact a minimally intelligent market simulation as functions
of the number of agents, the size of agents&apos; initial orders, and the state
spaces used for learning. We use phase space plots to examine the dynamics of
the ABM, when various specifications of learning agents are included. Further,
we examine whether the inclusion of optimal execution agents that can learn is
able to produce dynamics with the same complexity as empirical data. We find
that the inclusion of optimal execution agents changes the stylised facts
produced by ABM to conform more with empirical data, and are a necessary
inclusion for ABMs investigating market micro-structure. However, including
execution agents to chartist-fundamentalist-noise ABMs is insufficient to
recover the complexity observed in empirical data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Dicks_M/0/1/0/all/0/1&quot;&gt;Matthew Dicks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Paskaramoorthy_A/0/1/0/all/0/1&quot;&gt;Andrew Paskaramoorthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Gebbie_T/0/1/0/all/0/1&quot;&gt;Tim Gebbie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16343">
<title>Facial recognition technology and human raters can predict political orientation from images of expressionless faces even when controlling for demographics and self-presentation. (arXiv:2303.16343v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16343</link>
<description rdf:parseType="Literal">&lt;p&gt;Carefully standardized facial images of 591 participants were taken in the
laboratory, while controlling for self-presentation, facial expression, head
orientation, and image properties. They were presented to human raters and a
facial recognition algorithm: both humans (r=.21) and the algorithm (r=.22)
could predict participants&apos; scores on a political orientation scale (Cronbach&apos;s
alpha=.94) decorrelated with age, gender, and ethnicity. These effects are on
par with how well job interviews predict job success, or alcohol drives
aggressiveness. Algorithm&apos;s predictive accuracy was even higher (r=.31) when it
leveraged information on participants&apos; age, gender, and ethnicity. Moreover,
the associations between facial appearance and political orientation seem to
generalize beyond our sample: The predictive model derived from standardized
images (while controlling for age, gender, and ethnicity) could predict
political orientation (r=.13) from naturalistic images of 3,401 politicians
from the U.S., UK, and Canada. The analysis of facial features associated with
political orientation revealed that conservatives tended to have larger lower
faces. The predictability of political orientation from standardized images has
critical implications for privacy, the regulation of facial recognition
technology, and understanding the origins and consequences of political
orientation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1&quot;&gt;Michal Kosinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khambatta_P/0/1/0/all/0/1&quot;&gt;Poruz Khambatta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yilun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17713">
<title>Mitigating Source Bias for Fairer Weak Supervision. (arXiv:2303.17713v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17713</link>
<description rdf:parseType="Literal">&lt;p&gt;Weak supervision enables efficient development of training sets by reducing
the need for ground truth labels. However, the techniques that make weak
supervision attractive -- such as integrating any source of signal to estimate
unknown labels -- also entail the danger that the produced pseudolabels are
highly biased. Surprisingly, given everyday use and the potential for increased
bias, weak supervision has not been studied from the point of view of fairness.
We begin such a study, starting with the observation that even when a fair
model can be built from a dataset with access to ground-truth labels, the
corresponding dataset labeled via weak supervision can be arbitrarily unfair.
To address this, we propose and empirically validate a model for source
unfairness in weak supervision, then introduce a simple counterfactual
fairness-based technique that can mitigate these biases. Theoretically, we show
that it is possible for our approach to simultaneously improve both accuracy
and fairness -- in contrast to standard fairness approaches that suffer from
tradeoffs. Empirically, we show that our technique improves accuracy on weak
supervision baselines by as much as 32\% while reducing demographic parity gap
by 82.5\%. A simple extension of our method aimed at maximizing performance
produces state-of-the-art performance in five out of ten datasets in the WRENCH
benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Changho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1&quot;&gt;Sonia Cromp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1&quot;&gt;Dyah Adila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04934">
<title>Model Sparsity Can Simplify Machine Unlearning. (arXiv:2304.04934v10 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to recent data regulation requirements, machine unlearning (MU)
has emerged as a critical process to remove the influence of specific examples
from a given model. Although exact unlearning can be achieved through complete
model retraining using the remaining dataset, the associated computational
costs have driven the development of efficient, approximate unlearning
techniques. Moving beyond data-centric MU approaches, our study introduces a
novel model-based perspective: model sparsification via weight pruning, which
is capable of reducing the gap between exact unlearning and approximate
unlearning. We show in both theory and practice that model sparsity can boost
the multi-criteria unlearning performance of an approximate unlearner, closing
the approximation gap, while continuing to be efficient. This leads to a new MU
paradigm, termed prune first, then unlearn, which infuses a sparse model prior
into the unlearning process. Building on this insight, we also develop a
sparsity-aware unlearning method that utilizes sparsity regularization to
enhance the training process of approximate unlearning. Extensive experiments
show that our proposals consistently benefit MU in various unlearning
scenarios. A notable highlight is the 77% unlearning efficacy gain of
fine-tuning (one of the simplest unlearning methods) when using sparsity-aware
unlearning. Furthermore, we demonstrate the practical impact of our proposed MU
methods in addressing other machine learning challenges, such as defending
against backdoor attacks and enhancing transfer learning. Codes are available
at https://github.com/OPTML-Group/Unlearn-Sparse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinghan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1&quot;&gt;Parikshit Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuguang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gaowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pranay Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02441">
<title>Reward Teaching for Federated Multi-armed Bandits. (arXiv:2305.02441v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02441</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the existing federated multi-armed bandits (FMAB) designs are based
on the presumption that clients will implement the specified design to
collaborate with the server. In reality, however, it may not be possible to
modify the clients&apos; existing protocols. To address this challenge, this work
focuses on clients who always maximize their individual cumulative rewards, and
introduces a novel idea of ``reward teaching&apos;&apos;, where the server guides the
clients towards global optimality through implicit local reward adjustments.
Under this framework, the server faces two tightly coupled tasks of bandit
learning and target teaching, whose combination is non-trivial and challenging.
A phased approach, called Teaching-After-Learning (TAL), is first designed to
encourage and discourage clients&apos; explorations separately. General performance
analyses of TAL are established when the clients&apos; strategies satisfy certain
mild requirements. With novel technical approaches developed to analyze the
warm-start behaviors of bandit algorithms, particularized guarantees of TAL
with clients running UCB or epsilon-greedy strategies are then obtained. These
results demonstrate that TAL achieves logarithmic regrets while only incurring
logarithmic adjustment costs, which is order-optimal w.r.t. a natural lower
bound. As a further extension, the Teaching-While-Learning (TWL) algorithm is
developed with the idea of successive arm elimination to break the non-adaptive
phase separation in TAL. Rigorous analyses demonstrate that when facing clients
with UCB1, TWL outperforms TAL in terms of the dependencies on sub-optimality
gaps thanks to its adaptive design. Experimental results demonstrate the
effectiveness and generality of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chengshuai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06657">
<title>On Practical Robust Reinforcement Learning: Practical Uncertainty Set and Double-Agent Algorithm. (arXiv:2305.06657v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06657</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust reinforcement learning (RRL) aims at seeking a robust policy to
optimize the worst case performance over an uncertainty set of Markov decision
processes (MDPs). This set contains some perturbed MDPs from a nominal MDP
(N-MDP) that generate samples for training, which reflects some potential
mismatches between training (i.e., N-MDP) and true environments. In this paper
we present an elaborated uncertainty set by excluding some implausible MDPs
from the existing sets. Under this uncertainty set, we develop a sample-based
RRL algorithm (named ARQ-Learning) for tabular setting and characterize its
finite-time error bound. Also, it is proved that ARQ-Learning converges as fast
as the standard Q-Learning and robust Q-Learning while ensuring better
robustness. We introduce an additional pessimistic agent which can tackle the
major bottleneck for the extension of ARQ-Learning into the cases with larger
or continuous state spaces. Incorporating this idea into RL algorithms, we
propose double-agent algorithms for model-free RRL. Via experiments, we
demonstrate the effectiveness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1&quot;&gt;Ukjo Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Songnam Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11203">
<title>PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11203</link>
<description rdf:parseType="Literal">&lt;p&gt;DNN pruning is a popular way to reduce the size of a model, improve the
inference latency, and minimize the power consumption on DNN accelerators.
However, existing approaches might be too complex, expensive or ineffective to
apply to a variety of vision/language tasks, DNN architectures and to honor
structured pruning constraints. In this paper, we propose an efficient yet
effective train-time pruning scheme, Parameter-free Differentiable Pruning
(PDP), which offers state-of-the-art qualities in model size, accuracy, and
training cost. PDP uses a dynamic function of weights during training to
generate soft pruning masks for the weights in a parameter-free manner for a
given pruning target. While differentiable, the simplicity and efficiency of
PDP make it universal enough to deliver state-of-the-art
random/structured/channel pruning results on various vision and natural
language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1
ImageNet1k accuracy at 86.6% sparsity, which is 1.7% higher accuracy than those
from the state-of-the-art algorithms. Also, PDP yields over 83.1% accuracy on
Multi-Genre Natural Language Inference with 90% sparsity for BERT, while the
next best from the existing techniques shows 81.5% accuracy. In addition, PDP
can be applied to structured pruning, such as N:M pruning and channel pruning.
For 1:4 structured pruning of ResNet18, PDP improved the top-1 ImageNet1k
accuracy by over 3.6% over the state-of-the-art. For channel pruning of
ResNet50, PDP reduced the top-1 ImageNet1k accuracy by 0.6% from the
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsik Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1&quot;&gt;Saurabh Adya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1&quot;&gt;Devang Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13153">
<title>Effective Bilevel Optimization via Minimax Reformulation. (arXiv:2305.13153v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13153</link>
<description rdf:parseType="Literal">&lt;p&gt;Bilevel optimization has found successful applications in various machine
learning problems, including hyper-parameter optimization, data cleaning, and
meta-learning. However, its huge computational cost presents a significant
challenge for its utilization in large-scale problems. This challenge arises
due to the nested structure of the bilevel formulation, where each
hyper-gradient computation necessitates a costly inner optimization procedure.
To address this issue, we propose a reformulation of bilevel optimization as a
minimax problem, effectively decoupling the outer-inner dependency. Under mild
conditions, we show these two problems are equivalent. Furthermore, we
introduce a multi-stage gradient descent and ascent (GDA) algorithm to solve
the resulting minimax problem with convergence guarantees. Extensive
experimental results demonstrate that our method outperforms state-of-the-art
bilevel methods while significantly reducing the computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1&quot;&gt;Rui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_R/0/1/0/all/0/1&quot;&gt;Renjie Pi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16300">
<title>Landmark Attention: Random-Access Infinite Context Length for Transformers. (arXiv:2305.16300v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16300</link>
<description rdf:parseType="Literal">&lt;p&gt;While Transformers have shown remarkable success in natural language
processing, their attention mechanism&apos;s large memory requirements have limited
their ability to handle longer contexts. Prior approaches, such as recurrent
memory or retrieval-based augmentation, have either compromised the
random-access flexibility of attention (i.e., the capability to select any
token in the entire context) or relied on separate mechanisms for relevant
context retrieval, which may not be compatible with the model&apos;s attention. In
this paper, we present a novel approach that allows access to the complete
context while retaining random-access flexibility, closely resembling running
attention on the entire context. Our method uses a landmark token to represent
each block of the input and trains the attention to use it for selecting
relevant blocks, enabling retrieval of blocks directly through the attention
mechanism instead of by relying on a separate mechanism. Our approach
seamlessly integrates with specialized data structures and the system&apos;s memory
hierarchy, enabling processing of arbitrarily long context lengths. We
demonstrate that our method can obtain comparable performance with
Transformer-XL while significantly reducing the number of retrieved tokens in
each step. Finally, we show that fine-tuning LLaMA 7B with our method
successfully extends its context length capacity to over 32k tokens, allowing
for inference at the context lengths of GPT-4. We release the implementation of
landmark attention and the code to reproduce our experiments at
https://github.com/epfml/landmark-attention/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohtashami_A/0/1/0/all/0/1&quot;&gt;Amirkeivan Mohtashami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16854">
<title>Channel and Gradient-Importance Aware Device Scheduling for Over-the-Air Federated Learning. (arXiv:2305.16854v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16854</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a popular privacy-preserving distributed training
scheme, where multiple devices collaborate to train machine learning models by
uploading local model updates. To improve communication efficiency,
over-the-air computation (AirComp) has been applied to FL, which leverages
analog modulation to harness the superposition property of radio waves such
that numerous devices can upload their model updates concurrently for
aggregation. However, the uplink channel noise incurs considerable model
aggregation distortion, which is critically determined by the device scheduling
and compromises the learned model performance. In this paper, we propose a
probabilistic device scheduling framework for over-the-air FL, named PO-FL, to
mitigate the negative impact of channel noise, where each device is scheduled
according to a certain probability and its model update is reweighted using
this probability in aggregation. We prove the unbiasedness of this aggregation
scheme and demonstrate the convergence of PO-FL on both convex and non-convex
loss functions. Our convergence bounds unveil that the device scheduling
affects the learning performance through the communication distortion and
global update variance. Based on the convergence analysis, we further develop a
channel and gradient-importance aware algorithm to optimize the device
scheduling probabilities in PO-FL. Extensive simulation results show that the
proposed PO-FL framework with channel and gradient-importance awareness
achieves faster convergence and produces better models than baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuchang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+lin_Z/0/1/0/all/0/1&quot;&gt;Zehong lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuyi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17010">
<title>Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Combinatorial optimization (CO) problems are often NP-hard and thus out of
reach for exact algorithms, making them a tempting domain to apply machine
learning methods. The highly structured constraints in these problems can
hinder either optimization or sampling directly in the solution space. On the
other hand, GFlowNets have recently emerged as a powerful machinery to
efficiently sample from composite unnormalized densities sequentially and have
the potential to amortize such solution-searching processes in CO, as well as
generate diverse solution candidates. In this paper, we design Markov decision
processes (MDPs) for different combinatorial problems and propose to train
conditional GFlowNets to sample from the solution space. Efficient training
techniques are also developed to benefit long-range credit assignment. Through
extensive experiments on a variety of different CO tasks with synthetic and
realistic data, we demonstrate that GFlowNet policies can efficiently find
high-quality solutions. Our implementation is open-sourced at
https://github.com/zdhNarsil/GFlowNet-CombOpt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dinghuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hanjun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1&quot;&gt;Nikolay Malkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Ling Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17403">
<title>Source-Free Domain Adaptation for SSVEP-based Brain-Computer Interfaces. (arXiv:2305.17403v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17403</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a source free domain adaptation method for steady-state
visually evoked potentials (SSVEP) based brain-computer interface (BCI)
spellers. SSVEP-based BCI spellers assist individuals experiencing speech
difficulties by enabling them to communicate at a fast rate. However, achieving
a high information transfer rate (ITR) in most prominent methods requires an
extensive calibration period before using the system, leading to discomfort for
new users. We address this issue by proposing a novel method that adapts a
powerful deep neural network (DNN) pre-trained on data from source domains
(data from former users or participants of previous experiments) to the new
user (target domain), based only on the unlabeled target data. This adaptation
is achieved by minimizing our proposed custom loss function composed of
self-adaptation and local-regularity terms. The self-adaptation term uses the
pseudo-label strategy, while the novel local-regularity term exploits the data
structure and forces the DNN to assign similar labels to adjacent instances.
The proposed method priorities user comfort by removing the burden of
calibration while maintaining an excellent character identification accuracy
and ITR. In particular, our method achieves striking 201.15 bits/min and 145.02
bits/min ITRs on the benchmark and BETA datasets, respectively, and outperforms
the state-of-the-art alternatives. Our code is available at
https://github.com/osmanberke/SFDA-SSVEP-BCI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guney_O/0/1/0/all/0/1&quot;&gt;Osman Berke Guney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kucukahmetler_D/0/1/0/all/0/1&quot;&gt;Deniz Kucukahmetler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozkan_H/0/1/0/all/0/1&quot;&gt;Huseyin Ozkan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18399">
<title>On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the structure of the penultimate Gram matrix in
deep neural networks, which contains the pairwise inner products of outputs
corresponding to a batch of inputs. In several architectures it has been
observed that this Gram matrix becomes degenerate with depth at initialization,
which dramatically slows training. Normalization layers, such as batch or layer
normalization, play a pivotal role in preventing the rank collapse issue.
Despite promising advances, the existing theoretical results do not extend to
layer normalization, which is widely used in transformers, and can not
quantitatively characterize the role of non-linear activations. To bridge this
gap, we prove that layer normalization, in conjunction with activation layers,
biases the Gram matrix of a multilayer perceptron towards the identity matrix
at an exponential rate with depth at initialization. We quantify this rate
using the Hermite expansion of the activation function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joudaki_A/0/1/0/all/0/1&quot;&gt;Amir Joudaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daneshmand_H/0/1/0/all/0/1&quot;&gt;Hadi Daneshmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1&quot;&gt;Francis Bach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18415">
<title>Geometric Algebra Transformer. (arXiv:2305.18415v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18415</link>
<description rdf:parseType="Literal">&lt;p&gt;Problems involving geometric data arise in physics, chemistry, robotics,
computer vision, and many other fields. Such data can take numerous forms, for
instance points, direction vectors, translations, or rotations, but to date
there is no single architecture that can be applied to such a wide variety of
geometric types while respecting their symmetries. In this paper we introduce
the Geometric Algebra Transformer (GATr), a general-purpose architecture for
geometric data. GATr represents inputs, outputs, and hidden states in the
projective geometric (or Clifford) algebra, which offers an efficient
16-dimensional vector-space representation of common geometric objects as well
as operators acting on them. GATr is equivariant with respect to E(3), the
symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile,
efficient, and scalable. We demonstrate GATr in problems from n-body modeling
to wall-shear-stress estimation on large arterial meshes to robotic motion
planning. GATr consistently outperforms both non-geometric and equivariant
baselines in terms of error, data efficiency, and scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brehmer_J/0/1/0/all/0/1&quot;&gt;Johann Brehmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1&quot;&gt;Pim de Haan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behrends_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;nke Behrends&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1&quot;&gt;Taco Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02080">
<title>Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02080</link>
<description rdf:parseType="Literal">&lt;p&gt;Various adaptation methods, such as LoRA, prompts, and adapters, have been
proposed to enhance the performance of pre-trained vision-language models in
specific domains. The robustness of these adaptation methods against
distribution shifts have not been studied. In this study, we assess the
robustness of 11 widely-used adaptation methods across 4 vision-language
datasets under multimodal corruptions. Concretely, we introduce 7 benchmark
datasets, including 96 visual and 87 textual corruptions, to investigate the
robustness of different adaptation methods, the impact of available adaptation
examples, and the influence of trainable parameter size during adaptation. Our
analysis reveals that: 1) Adaptation methods are more sensitive to text
corruptions than visual corruptions. 2) Full fine-tuning does not consistently
provide the highest robustness; instead, adapters can achieve better robustness
with comparable clean performance. 3) Contrary to expectations, our findings
indicate that increasing the number of adaptation data and parameters does not
guarantee enhanced robustness; instead it results in even lower robustness. We
hope this study could benefit future research in the development of robust
multimodal adaptation methods. The benchmark, code, and dataset used in this
study can be accessed at https://adarobustness.github.io .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02497">
<title>Learning on Bandwidth Constrained Multi-Source Data with MIMO-inspired DPP MAP Inference. (arXiv:2306.02497v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02497</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a distributed version of Determinant Point Processing
(DPP) inference to enhance multi-source data diversification under limited
communication bandwidth. DPP is a popular probabilistic approach that improves
data diversity by enforcing the repulsion of elements in the selected subsets.
The well-studied Maximum A Posteriori (MAP) inference in DPP aims to identify
the subset with the highest diversity quantified by DPP. However, this approach
is limited by the presumption that all data samples are available at one point,
which hinders its applicability to real-world applications such as traffic
datasets where data samples are distributed across sources and communication
between them is band-limited.
&lt;/p&gt;
&lt;p&gt;Inspired by the techniques used in Multiple-Input Multiple-Output (MIMO)
communication systems, we propose a strategy for performing MAP inference among
distributed sources. Specifically, we show that a lower bound of the
diversity-maximized distributed sample selection problem can be treated as a
power allocation problem in MIMO systems. A determinant-preserved sparse
representation of selected samples is used to perform sample precoding in local
sources to be processed by DPP. Our method does not require raw data exchange
among sources, but rather a band-limited feedback channel to send lightweight
diversity measures, analogous to the CSI message in MIMO systems, from the
center to data sources. The experiments show that our scalable approach can
outperform baseline methods, including random selection, uninformed individual
DPP with no feedback, and DPP with SVD-based feedback, in both i.i.d and
non-i.i.d setups. Specifically, it achieves 1 to 6 log-difference diversity
gain in the latent representation of CIFAR-10, CIFAR-100, StanfordCars, and
GTSRB datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amin_R/0/1/0/all/0/1&quot;&gt;Rahul Amin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Razi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06064">
<title>Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06064</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving NP-hard/complete combinatorial problems with neural networks is a
challenging research area that aims to surpass classical approximate
algorithms. The long-term objective is to outperform hand-designed heuristics
for NP-hard/complete problems by learning to generate superior solutions solely
from training data. Current neural-based methods for solving CO problems often
overlook the inherent &quot;algorithmic&quot; nature of the problems. In contrast,
heuristics designed for CO problems, e.g. TSP, frequently leverage
well-established algorithms, such as those for finding the minimum spanning
tree. In this paper, we propose leveraging recent advancements in neural
algorithmic reasoning to improve the learning of CO problems. Specifically, we
suggest pre-training our neural model on relevant algorithms before training it
on CO instances. Our results demonstrate that by using this learning setup, we
achieve superior performance compared to non-algorithmically informed deep
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1&quot;&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Numeroso_D/0/1/0/all/0/1&quot;&gt;Danilo Numeroso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1&quot;&gt;Davide Bacciu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06146">
<title>Hidden Classification Layers: Enhancing linear separability between classes in neural networks layers. (arXiv:2306.06146v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06146</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of classification problems, Deep Learning (DL) approaches
represent state of art. Many DL approaches are based on variations of standard
multi-layer feed-forward neural networks. These are also referred to as deep
networks. The basic idea is that each hidden neural layer accomplishes a data
transformation which is expected to make the data representation &quot;somewhat more
linearly separable&quot; than the previous one to obtain a final data representation
which is as linearly separable as possible. However, determining the
appropriate neural network parameters that can perform these transformations is
a critical problem. In this paper, we investigate the impact on deep network
classifier performances of a training approach favouring solutions where data
representations at the hidden layers have a higher degree of linear
separability between the classes with respect to standard methods. To this aim,
we propose a neural network architecture which induces an error function
involving the outputs of all the network layers. Although similar approaches
have already been partially discussed in the past literature, here we propose a
new architecture with a novel error function and an extensive experimental
analysis. This experimental analysis was made in the context of image
classification tasks considering four widely used datasets. The results show
that our approach improves the accuracy on the test set in all the considered
cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1&quot;&gt;Andrea Apicella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1&quot;&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1&quot;&gt;Roberto Prevete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06247">
<title>Online Learning with Set-Valued Feedback. (arXiv:2306.06247v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06247</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a variant of online multiclass classification where the learner
predicts a single label but receives a \textit{set of labels} as feedback. In
this model, the learner is penalized for not outputting a label contained in
the revealed set. We show that unlike online multiclass learning with
single-label feedback, deterministic and randomized online learnability are
\textit{not equivalent} in the realizable setting under set-valued feedback. In
addition, we show that deterministic and randomized realizable learnability are
equivalent if the Helly number of the collection of sets that can be revealed
as feedback is finite. In light of this separation, we give two new
combinatorial dimensions, named the Set Littlestone and Measure Shattering
dimension, whose finiteness characterizes deterministic and randomized
realizable learnability respectively. Additionally, these dimensions lower- and
upper bound the deterministic and randomized minimax regret in the realizable
setting. Going beyond the realizable setting, we prove that the Measure
shattering dimension continues to characterize learnability and quantify
minimax regret in the agnostic setting. Finally, we use our results to
establish bounds on the minimax regret for three practical learning settings:
online multilabel ranking, online multilabel classification, and real-valued
prediction with interval-valued response.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_V/0/1/0/all/0/1&quot;&gt;Vinod Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subedi_U/0/1/0/all/0/1&quot;&gt;Unique Subedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ambuj Tewari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07497">
<title>GQFedWAvg: Optimization-Based Quantized Federated Learning in General Edge Computing Systems. (arXiv:2306.07497v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07497</link>
<description rdf:parseType="Literal">&lt;p&gt;The optimal implementation of federated learning (FL) in practical edge
computing systems has been an outstanding problem. In this paper, we propose an
optimization-based quantized FL algorithm, which can appropriately fit a
general edge computing system with uniform or nonuniform computing and
communication resources at the workers. Specifically, we first present a new
random quantization scheme and analyze its properties. Then, we propose a
general quantized FL algorithm, namely GQFedWAvg. Specifically, GQFedWAvg
applies the proposed quantization scheme to quantize wisely chosen model
update-related vectors and adopts a generalized mini-batch stochastic gradient
descent (SGD) method with the weighted average local model updates in global
model aggregation. Besides, GQFedWAvg has several adjustable algorithm
parameters to flexibly adapt to the computing and communication resources at
the server and workers. We also analyze the convergence of GQFedWAvg. Next, we
optimize the algorithm parameters of GQFedWAvg to minimize the convergence
error under the time and energy constraints. We successfully tackle the
challenging non-convex problem using general inner approximation (GIA) and
multiple delicate tricks. Finally, we interpret GQFedWAvg&apos;s function principle
and show its considerable gains over existing FL algorithms using numerical
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Ying Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1&quot;&gt;Vincent Lau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07613">
<title>Revisiting and Advancing Adversarial Training Through A Simple Baseline. (arXiv:2306.07613v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07613</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we delve into the essential components of adversarial training
which is a pioneering defense technique against adversarial attacks. We
indicate that some factors such as the loss function, learning rate scheduler,
and data augmentation, which are independent of the model architecture, will
influence adversarial robustness and generalization. When these factors are
controlled for, we introduce a simple baseline approach, termed SimpleAT, that
performs competitively with recent methods and mitigates robust overfitting. We
conduct extensive experiments on CIFAR-10/100 and Tiny-ImageNet, which validate
the robustness of SimpleAT against state-of-the-art adversarial attackers such
as AutoAttack. Our results also demonstrate that SimpleAT exhibits good
performance in the presence of various image corruptions, such as those found
in the CIFAR-10-C. In addition, we empirically show that SimpleAT is capable of
reducing the variance in model predictions, which is considered the primary
contributor to robust overfitting. Our results also reveal the connections
between SimpleAT and many advanced state-of-the-art adversarial defense
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07691">
<title>StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07691</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that
leverages style diffusion and adversarial training with large speech language
models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its
predecessor by modeling styles as a latent random variable through diffusion
models to generate the most suitable style for the text without requiring
reference speech, achieving efficient latent diffusion while benefiting from
the diverse speech synthesis offered by diffusion models. Furthermore, we
employ large pre-trained SLMs, such as WavLM, as discriminators with our novel
differentiable duration modeling for end-to-end training, resulting in improved
speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker
LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by
native English speakers. Moreover, when trained on the LibriTTS dataset, our
model outperforms previous publicly available models for zero-shot speaker
adaptation. This work achieves the first human-level TTS on both single and
multispeaker datasets, showcasing the potential of style diffusion and
adversarial training with large SLMs. The audio demos and source code are
available at https://styletts2.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghao Aaron Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raghavan_V/0/1/0/all/0/1&quot;&gt;Vinay S. Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mischler_G/0/1/0/all/0/1&quot;&gt;Gavin Mischler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1&quot;&gt;Nima Mesgarani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08744">
<title>High-performance deep spiking neural networks with 0.3 spikes per neuron. (arXiv:2306.08744v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08744</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication by rare, binary spikes is a key factor for the energy
efficiency of biological brains. However, it is harder to train
biologically-inspired spiking neural networks (SNNs) than artificial neural
networks (ANNs). This is puzzling given that theoretical results provide exact
mapping algorithms from ANNs to SNNs with time-to-first-spike (TTFS) coding. In
this paper we analyze in theory and simulation the learning dynamics of
TTFS-networks and identify a specific instance of the vanishing-or-exploding
gradient problem. While two choices of SNN mappings solve this problem at
initialization, only the one with a constant slope of the neuron membrane
potential at threshold guarantees the equivalence of the training trajectory
between SNNs and ANNs with rectified linear units. We demonstrate that training
deep SNN models achieves the exact same performance as that of ANNs, surpassing
previous SNNs on image classification datasets such as MNIST/Fashion-MNIST,
CIFAR10/CIFAR100 and PLACES365. Our SNN accomplishes high-performance
classification with less than 0.3 spikes per neuron, lending itself for an
energy-efficient implementation. We show that fine-tuning SNNs with our robust
gradient descent algorithm enables their optimization for hardware
implementations with low latency and resilience to noise and quantization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanojevic_A/0/1/0/all/0/1&quot;&gt;Ana Stanojevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1&quot;&gt;Stanis&amp;#x142;aw Wo&amp;#x17a;niak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1&quot;&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherubini_G/0/1/0/all/0/1&quot;&gt;Giovanni Cherubini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pantazi_A/0/1/0/all/0/1&quot;&gt;Angeliki Pantazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1&quot;&gt;Wulfram Gerstner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10453">
<title>Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. (arXiv:2306.10453v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10453</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction attempts to predict whether an unseen edge exists based on
only a portion of edges of a graph. A flurry of methods have been introduced in
recent years that attempt to make use of graph neural networks (GNNs) for this
task. Furthermore, new and diverse datasets have also been created to better
evaluate the effectiveness of these new models. However, multiple pitfalls
currently exist that hinder our ability to properly evaluate these new methods.
These pitfalls mainly include: (1) Lower than actual performance on multiple
baselines, (2) A lack of a unified data split and evaluation metric on some
datasets, and (3) An unrealistic evaluation setting that uses easy negative
samples. To overcome these challenges, we first conduct a fair comparison
across prominent methods and datasets, utilizing the same dataset and
hyperparameter search settings. We then create a more practical evaluation
setting based on a Heuristic Related Sampling Technique (HeaRT), which samples
hard negative samples via multiple heuristics. The new evaluation setting helps
promote new challenges and opportunities in link prediction by aligning the
evaluation with real-world situations. Our implementation and data are
available at https://github.com/Juanhui28/HeaRT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shomer_H/0/1/0/all/0/1&quot;&gt;Harry Shomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shenglai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12045">
<title>Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v5 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12045</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing paradigm.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a {\it natural paradigm}. We show
that TeCoS-LVM models can produce more realistic spike activities and
accurately fit spike statistics than powerful alternatives. Additionally,
learned TeCoS-LVM models can generalize well to longer time scales. Overall,
while remaining computationally tractable, our model effectively captures key
features of neural coding systems. It thus provides a useful tool for building
accurate predictive computational accounts for various sensory perception
circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gehua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Runhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huajin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14069">
<title>Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets. (arXiv:2306.14069v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14069</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent advancements in offline reinforcement learning via
supervised learning (RvS) and the success of the decision transformer (DT)
architecture in various domains, DTs have fallen short in several challenging
benchmarks. The root cause of this underperformance lies in their inability to
seamlessly connect segments of suboptimal trajectories. To overcome this
limitation, we present a novel approach to enhance RvS methods by integrating
intermediate targets. We introduce the Waypoint Transformer (WT), using an
architecture that builds upon the DT framework and conditioned on
automatically-generated waypoints. The results show a significant increase in
the final return compared to existing RvS methods, with performance on par or
greater than existing state-of-the-art temporal difference learning-based
methods. Additionally, the performance and stability improvements are largest
in the most challenging environments and data configurations, including AntMaze
Large Play/Diverse and Kitchen Mixed/Partial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badrinath_A/0/1/0/all/0/1&quot;&gt;Anirudhan Badrinath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flet_Berliac_Y/0/1/0/all/0/1&quot;&gt;Yannis Flet-Berliac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_A/0/1/0/all/0/1&quot;&gt;Allen Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02842">
<title>Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02842</link>
<description rdf:parseType="Literal">&lt;p&gt;Risk-sensitive reinforcement learning (RL) aims to optimize policies that
balance the expected reward and risk. In this paper, we investigate a novel
risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR)
objective under linear and general function approximations. This new
formulation, named ICVaR-RL with function approximation, provides a principled
way to guarantee safety at each decision step. For ICVaR-RL with linear
function approximation, we propose a computationally efficient algorithm
ICVaR-L, which achieves an
$\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is
the risk level, $d$ is the dimension of state-action features, $H$ is the
length of each episode, and $K$ is the number of episodes. We also establish a
matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the
optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general
function approximation, we propose algorithm ICVaR-G, which achieves an
$\widetilde{O}(\sqrt{\alpha^{-(H+1)}DH^4K})$ regret, where $D$ is a dimensional
parameter that depends on the eluder dimension and covering number.
Furthermore, our analysis provides several novel techniques for risk-sensitive
RL, including an efficient approximation of the CVaR operator, a new ridge
regression with CVaR-adapted features, and a refined elliptical potential
lemma.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yihan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Pihe Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Desheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longbo Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04962">
<title>Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by nodes visited in the environment. We use these proposed
features as rewards for graph neural-network-based reinforcement learning. On
multiple classes of synthetically generated graphs, we find that trained agents
generalize to longer exploratory walks and larger environments than are seen
during training. Our method computes more efficiently than the greedy
evaluation of the relevant topological properties. The proposed intrinsic
motivations bear particular relevance for recommender systems. We demonstrate
that next-node recommendations considering curiosity are more predictive of
human choices than PageRank centrality in several real-world graph
environments, including MovieLens, Amazon Books, and Wikipedia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shubhankar P. Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1&quot;&gt;Mathieu Ouellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1&quot;&gt;Juan Cervino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kieran A. Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1&quot;&gt;Dani S. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05341">
<title>Tracking Most Significant Shifts in Nonparametric Contextual Bandits. (arXiv:2307.05341v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05341</link>
<description rdf:parseType="Literal">&lt;p&gt;We study nonparametric contextual bandits where Lipschitz mean reward
functions may change over time. We first establish the minimax dynamic regret
rate in this less understood setting in terms of number of changes $L$ and
total-variation $V$, both capturing all changes in distribution over context
space, and argue that state-of-the-art procedures are suboptimal in this
setting.
&lt;/p&gt;
&lt;p&gt;Next, we tend to the question of an adaptivity for this setting, i.e.
achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly,
we posit that the bandit problem, viewed locally at a given context $X_t$,
should not be affected by reward changes in other parts of context space $\cal
X$. We therefore propose a notion of change, which we term experienced
significant shifts, that better accounts for locality, and thus counts
considerably less changes than $L$ and $V$. Furthermore, similar to recent work
on non-stationary MAB (Suk &amp;amp; Kpotufe, 2022), experienced significant shifts
only count the most significant changes in mean rewards, e.g., severe best-arm
changes relevant to observed contexts.
&lt;/p&gt;
&lt;p&gt;Our main result is to show that this more tolerant notion of change can in
fact be adapted to.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suk_J/0/1/0/all/0/1&quot;&gt;Joe Suk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kpotufe_S/0/1/0/all/0/1&quot;&gt;Samory Kpotufe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07975">
<title>Finite element inspired networks: Learning interpretable deformable object dynamics from partial observations. (arXiv:2307.07975v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07975</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate simulation of deformable linear object (DLO) dynamics is challenging
if the task at hand requires a human-interpretable model that also yields fast
predictions. To arrive at such a model, we draw inspiration from the rigid
finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies
whose internal state is unrolled through time by a dynamics network. As this
state is not observed directly, the dynamics network is trained jointly with a
physics-informed encoder which maps observed motion variables to the DLO&apos;s
hidden state. To encourage that the state acquires a physically meaningful
representation, we leverage the forward kinematics of the underlying R-FEM
model as a decoder. Through robot experiments we demonstrate that the proposed
architecture provides an easy-to-handle, yet capable DLO dynamics model
yielding physically interpretable predictions from partial observations.
&lt;/p&gt;
&lt;p&gt;The project code is available at: \url{https://tinyurl.com/fei-networks}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamedov_S/0/1/0/all/0/1&quot;&gt;Shamil Mamedov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_A/0/1/0/all/0/1&quot;&gt;A. Ren&amp;#xe9; Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swevers_J/0/1/0/all/0/1&quot;&gt;Jan Swevers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09191">
<title>A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09191</link>
<description rdf:parseType="Literal">&lt;p&gt;Categorical encoders transform categorical features into numerical
representations that are indispensable for a wide range of machine learning
models. Existing encoder benchmark studies lack generalizability because of
their limited choice of (1) encoders, (2) experimental factors, and (3)
datasets. Additionally, inconsistencies arise from the adoption of varying
aggregation strategies. This paper is the most comprehensive benchmark of
categorical encoders to date, including an extensive evaluation of 32
configurations of encoders from diverse families, with 36 combinations of
experimental factors, and on 50 datasets. The study shows the profound
influence of dataset selection, experimental factors, and aggregation
strategies on the benchmark&apos;s conclusions -- aspects disregarded in previous
encoder benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matteucci_F/0/1/0/all/0/1&quot;&gt;Federico Matteucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arzamasov_V/0/1/0/all/0/1&quot;&gt;Vadim Arzamasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boehm_K/0/1/0/all/0/1&quot;&gt;Klemens Boehm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13831">
<title>Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13831</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD) is the simplest deep learning optimizer
with which to train deep neural networks. While SGD can use various learning
rates, such as constant or diminishing rates, the previous numerical results
showed that SGD performs better than other deep learning optimizers using when
it uses learning rates given by line search methods. In this paper, we perform
a convergence analysis on SGD with a learning rate given by an Armijo line
search for nonconvex optimization. The analysis indicates that the upper bound
of the expectation of the squared norm of the full gradient becomes small when
the number of steps and the batch size are large. Next, we show that, for SGD
with the Armijo-line-search learning rate, the number of steps needed for
nonconvex optimization is a monotone decreasing convex function of the batch
size; that is, the number of steps needed for nonconvex optimization decreases
as the batch size increases. Furthermore, we show that the stochastic
first-order oracle (SFO) complexity, which is the stochastic gradient
computation cost, is a convex function of the batch size; that is, there exists
a critical batch size that minimizes the SFO complexity. Finally, we provide
numerical results that support our theoretical results. The numerical results
indicate that the number of steps needed for training deep neural networks
decreases as the batch size increases and that there exist the critical batch
sizes that can be estimated from the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsukada_Y/0/1/0/all/0/1&quot;&gt;Yuki Tsukada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iiduka_H/0/1/0/all/0/1&quot;&gt;Hideaki Iiduka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09952">
<title>Finding emergence in data: causal emergence inspired dynamics learning. (arXiv:2308.09952v2 [physics.soc-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09952</link>
<description rdf:parseType="Literal">&lt;p&gt;Modelling complex dynamical systems in a data-driven manner is challenging
due to the presence of emergent behaviors and properties that cannot be
directly captured by micro-level observational data. Therefore, it is crucial
to develop a model that can effectively capture emergent dynamics at the
macro-level and quantify emergence based on the available data. Drawing
inspiration from the theory of causal emergence, this paper introduces a
machine learning framework aimed at learning macro-dynamics within an emergent
latent space. The framework achieves this by maximizing the effective
information (EI) to obtain a macro-dynamics model with stronger causal effects.
Experimental results on both simulated and real data demonstrate the
effectiveness of the proposed framework. Not only does it successfully capture
emergent patterns, but it also learns the coarse-graining strategy and
quantifies the degree of causal emergence in the data. Furthermore, experiments
conducted on environments different from the training dataset highlight the
superior generalization ability of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingzhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rong_Y/0/1/0/all/0/1&quot;&gt;Yingqi Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bing Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12634">
<title>Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cersovsky_J/0/1/0/all/0/1&quot;&gt;Josef Cersovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1&quot;&gt;Sadegh Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainmueller_D/0/1/0/all/0/1&quot;&gt;Dagmar Kainmueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoehne_J/0/1/0/all/0/1&quot;&gt;Johannes Hoehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15283">
<title>Structural Node Embeddings with Homomorphism Counts. (arXiv:2308.15283v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15283</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph homomorphism counts, first explored by Lov\&apos;asz in 1967, have recently
garnered interest as a powerful tool in graph-based machine learning. Grohe
(PODS 2020) proposed the theoretical foundations for using homomorphism counts
in machine learning on graph level as well as node level tasks. By their very
nature, these capture local structural information, which enables the creation
of robust structural embeddings. While a first approach for graph level tasks
has been made by Nguyen and Maehara (ICML 2020), we experimentally show the
effectiveness of homomorphism count based node embeddings. Enriched with node
labels, node weights, and edge weights, these offer an interpretable
representation of graph data, allowing for enhanced explainability of machine
learning models.
&lt;/p&gt;
&lt;p&gt;We propose a theoretical framework for isomorphism-invariant homomorphism
count based embeddings which lend themselves to a wide variety of downstream
tasks. Our approach capitalises on the efficient computability of graph
homomorphism counts for bounded treewidth graph classes, rendering it a
practical solution for real-world applications. We demonstrate their
expressivity through experiments on benchmark datasets. Although our results do
not match the accuracy of state-of-the-art neural architectures, they are
comparable to other advanced graph learning models. Remarkably, our approach
demarcates itself by ensuring explainability for each individual feature. By
integrating interpretable machine learning algorithms like SVMs or Random
Forests, we establish a seamless, end-to-end explainable pipeline. Our study
contributes to the advancement of graph-based techniques that offer both
performance and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_H/0/1/0/all/0/1&quot;&gt;Hinrikus Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oeljeklaus_L/0/1/0/all/0/1&quot;&gt;Luca Oeljeklaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhner_P/0/1/0/all/0/1&quot;&gt;Pascal K&amp;#xfc;hner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1&quot;&gt;Martin Grohe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15363">
<title>Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v4 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15363</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL
task. However, the absence of a systematical benchmark inhibits the development
of designing effective, efficient and economic LLM-based Text-to-SQL solutions.
To address this challenge, in this paper, we first conduct a systematical and
extensive comparison over existing prompt engineering methods, including
question representation, example selection and example organization, and with
these experimental results, we elaborate their pros and cons. Based on these
findings, we propose a new integrated solution, named DAIL-SQL, which refreshes
the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To
explore the potential of open-source LLM, we investigate them in various
scenarios, and further enhance their performance with supervised fine-tuning.
Our explorations highlight open-source LLMs&apos; potential in Text-to-SQL, as well
as the advantages and disadvantages of the supervised fine-tuning.
Additionally, towards an efficient and economic LLM-based Text-to-SQL solution,
we emphasize the token efficiency in prompt engineering and compare the prior
studies under this metric. We hope that our work provides a deeper
understanding of Text-to-SQL with LLMs, and inspires further investigations and
broad applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Dawei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yichen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bolin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01784">
<title>INTAGS: Interactive Agent-Guided Simulation. (arXiv:2309.01784v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01784</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications involving multi-agent system (MAS), it is imperative to
test an experimental (Exp) autonomous agent in a high-fidelity simulator prior
to its deployment to production, to avoid unexpected losses in the real-world.
Such a simulator acts as the environmental background (BG) agent(s), called
agent-based simulator (ABS), aiming to replicate the complex real MAS. However,
developing realistic ABS remains challenging, mainly due to the sequential and
dynamic nature of such systems. To fill this gap, we propose a metric to
distinguish between real and synthetic multi-agent systems, which is evaluated
through the live interaction between the Exp and BG agents to explicitly
account for the systems&apos; sequential nature. Specifically, we characterize the
system/environment by studying the effect of a sequence of BG agents&apos; responses
to the environment state evolution and take such effects&apos; differences as MAS
distance metric; The effect estimation is cast as a causal inference problem
since the environment evolution is confounded with the previous environment
state. Importantly, we propose the Interactive Agent-Guided Simulation (INTAGS)
framework to build a realistic ABS by optimizing over this novel metric. To
adapt to any environment with interactive sequential decision making agents,
INTAGS formulates the simulator as a stochastic policy in reinforcement
learning. Moreover, INTAGS utilizes the policy gradient update to bypass
differentiating the proposed metric such that it can support non-differentiable
operations of multi-agent environments. Through extensive experiments, we
demonstrate the effectiveness of INTAGS on an equity stock market simulation
example. We show that using INTAGS to calibrate the simulator can generate more
realistic market data compared to the state-of-the-art conditional Wasserstein
Generative Adversarial Network approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Song Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coletta_A/0/1/0/all/0/1&quot;&gt;Andrea Coletta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1&quot;&gt;Svitlana Vyetrenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1&quot;&gt;Tucker Balch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03307">
<title>Several fitness functions and entanglement gates in quantum kernel generation. (arXiv:2309.03307v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03307</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum machine learning (QML) represents a promising frontier in the quantum
technologies. In this pursuit of quantum advantage, the quantum kernel method
for support vector machine has emerged as a powerful approach. Entanglement, a
fundamental concept in quantum mechanics, assumes a central role in quantum
computing. In this paper, we investigate the optimal number of entanglement
gates in the quantum kernel feature maps by a multi-objective genetic
algorithm. We distinct the fitness functions of genetic algorithm for non-local
gates for entanglement and local gates to gain insights into the benefits of
employing entanglement gates. Our experiments reveal that the optimal
configuration of quantum circuits for the quantum kernel method incorporates a
proportional number of non-local gates for entanglement. The result complements
the prior literature on quantum kernel generation where non-local gates were
largely suppressed. Furthermore, we demonstrate that the separability indexes
of data can be leveraged to estimate the number of non-local gates required for
the quantum support vector machine&apos;s feature maps. This insight can be helpful
in selecting appropriate parameters, such as the entanglement parameter, in
various quantum programming packages like https://qiskit.org/ based on data
analysis. Our findings offer valuable guidance for enhancing the efficiency and
accuracy of quantum machine learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiyan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08387">
<title>Efficient Graphics Representation with Differentiable Indirection. (arXiv:2309.08387v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08387</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce differentiable indirection -- a novel learned primitive that
employs differentiable multi-scale lookup tables as an effective substitute for
traditional compute and data operations across the graphics pipeline. We
demonstrate its flexibility on a number of graphics tasks, i.e., geometric and
image representation, texture mapping, shading, and radiance field
representation. In all cases, differentiable indirection seamlessly integrates
into existing architectures, trains rapidly, and yields both versatile and
efficient results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1&quot;&gt;Sayantan Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshall_C/0/1/0/all/0/1&quot;&gt;Carl Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengqin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10003">
<title>A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10003</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes to measure the scope of a patent claim as the reciprocal
of the self-information contained in this claim. A probability of occurrence of
the claim is obtained from a language model and this probability is used to
compute the self-information. Grounded in information theory, this approach is
based on the assumption that an unlikely concept is more informative than a
usual concept, insofar as it is more surprising. In turn, the more surprising
the information required to defined the claim, the narrower its scope. Five
language models are considered, ranging from simplest models (each word or
character is assigned an identical probability) to intermediate models (using
average word or character frequencies), to a large language model (GPT2).
Interestingly, the scope resulting from the simplest language models is
proportional to the reciprocal of the number of words or characters involved in
the claim, a metric already used in previous works. Application is made to
multiple series of patent claims directed to distinct inventions, where each
series consists of claims devised to have a gradually decreasing scope. The
performance of the language models is assessed with respect to several ad hoc
tests. The more sophisticated the model, the better the results. I.e., the GPT2
probability model outperforms models based on word and character frequencies,
which themselves outdo the simplest models based on word or character counts.
Still, the character count appears to be a more reliable indicator than the
word count.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Ragot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12253">
<title>SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12253</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an extension to the CLRS algorithmic learning benchmark,
prioritizing scalability and the utilization of sparse representations. Many
algorithms in CLRS require global memory or information exchange, mirrored in
its execution model, which constructs fully connected (not sparse) graphs based
on the underlying problem. Despite CLRS&apos;s aim of assessing how effectively
learned algorithms can generalize to larger instances, the existing execution
model becomes a significant constraint due to its demanding memory requirements
and runtime (hard to scale). However, many important algorithms do not demand a
fully connected graph; these algorithms, primarily distributed in nature, align
closely with the message-passing paradigm employed by Graph Neural Networks.
Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark
specifically with scalability and sparseness in mind. Our approach includes
adapted algorithms from the original CLRS benchmark and introduces new problems
from distributed and randomized algorithms. Moreover, we perform a thorough
empirical evaluation of our benchmark. Code is publicly available at
https://github.com/jkminder/SALSA-CLRS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minder_J/0/1/0/all/0/1&quot;&gt;Julian Minder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1&quot;&gt;Florian Gr&amp;#xf6;tschla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l Mathys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13409">
<title>Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13409</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces a novel forecasting strategy that leverages the power
of fractional differencing (FD) to capture both short- and long-term
dependencies in time series data. Unlike traditional integer differencing
methods, FD preserves memory in series while stabilizing it for modeling
purposes. By applying FD to financial data from the SPY index and incorporating
sentiment analysis from news reports, this empirical analysis explores the
effectiveness of FD in conjunction with binary classification of target
variables. Supervised classification algorithms were employed to validate the
performance of FD series. The results demonstrate the superiority of FD over
integer differencing, as confirmed by Receiver Operating Characteristic/Area
Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maitra_S/0/1/0/all/0/1&quot;&gt;Sarit Maitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_V/0/1/0/all/0/1&quot;&gt;Vivek Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1&quot;&gt;Srashti Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Sukanya Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_G/0/1/0/all/0/1&quot;&gt;Goutam Kumar Kundu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17113">
<title>Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17113</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing multi-relational graph neural networks use one of two strategies for
identifying informative relations: either they reduce this problem to low-level
weight learning, or they rely on handcrafted chains of relational dependencies,
called meta-paths. However, the former approach faces challenges in the
presence of many relations (e.g., knowledge graphs), while the latter requires
substantial domain expertise to identify relevant meta-paths. In this work we
propose a novel approach to learn meta-paths and meta-path GNNs that are highly
accurate based on a small number of informative meta-paths. Key element of our
approach is a scoring function for measuring the potential informativeness of a
relation in the incremental construction of the meta-path. Our experimental
evaluation shows that the approach manages to correctly identify relevant
meta-paths even with a large number of relations, and substantially outperforms
existing multi-relational GNNs on synthetic and real-world experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrini_F/0/1/0/all/0/1&quot;&gt;Francesco Ferrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longa_A/0/1/0/all/0/1&quot;&gt;Antonio Longa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_M/0/1/0/all/0/1&quot;&gt;Manfred Jaeger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03358">
<title>Enhancing Robust Representation in Adversarial Training: Alignment and Exclusion Criteria. (arXiv:2310.03358v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03358</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are vulnerable to adversarial noise. Adversarial
Training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two criteria of robust
representation: (1) Exclusion: \emph{the feature of examples keeps away from
that of other classes}; (2) Alignment: \emph{the feature of natural and
corresponding adversarial examples is close to each other}. These motivate us
to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_N/0/1/0/all/0/1&quot;&gt;Nuoyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Decheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dawei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04741">
<title>Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04741</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthes_D/0/1/0/all/0/1&quot;&gt;Daniel Anthes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorat_S/0/1/0/all/0/1&quot;&gt;Sushrut Thorat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konig_P/0/1/0/all/0/1&quot;&gt;Peter K&amp;#xf6;nig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kietzmann_T/0/1/0/all/0/1&quot;&gt;Tim C. Kietzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04984">
<title>Model-adapted Fourier sampling for generative compressed sensing. (arXiv:2310.04984v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04984</link>
<description rdf:parseType="Literal">&lt;p&gt;We study generative compressed sensing when the measurement matrix is
randomly subsampled from a unitary matrix (with the DFT as an important special
case). It was recently shown that $\textit{O}(kdn\|
\boldsymbol{\alpha}\|_{\infty}^{2})$ uniformly random Fourier measurements are
sufficient to recover signals in the range of a neural network $G:\mathbb{R}^k
\to \mathbb{R}^n$ of depth $d$, where each component of the so-called local
coherence vector $\boldsymbol{\alpha}$ quantifies the alignment of a
corresponding Fourier vector with the range of $G$. We construct a
model-adapted sampling strategy with an improved sample complexity of
$\textit{O}(kd\| \boldsymbol{\alpha}\|_{2}^{2})$ measurements. This is enabled
by: (1) new theoretical recovery guarantees that we develop for nonuniformly
random sampling distributions and then (2) optimizing the sampling distribution
to minimize the number of measurements needed for these guarantees. This
development offers a sample complexity applicable to natural signal classes,
which are often almost maximally coherent with low Fourier frequencies.
Finally, we consider a surrogate sampling scheme, and validate its performance
in recovery experiments using the CelebA dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berk_A/0/1/0/all/0/1&quot;&gt;Aaron Berk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brugiapaglia_S/0/1/0/all/0/1&quot;&gt;Simone Brugiapaglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plan_Y/0/1/0/all/0/1&quot;&gt;Yaniv Plan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_M/0/1/0/all/0/1&quot;&gt;Matthew Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1&quot;&gt;Xia Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yilmaz_O/0/1/0/all/0/1&quot;&gt;Ozgur Yilmaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07048">
<title>FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07048</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal federated learning (FL) aims to enrich model training in FL
settings where devices are collecting measurements across multiple modalities
(e.g., sensors measuring pressure, motion, and other types of data). However,
key challenges to multimodal FL remain unaddressed, particularly in
heterogeneous network settings: (i) the set of modalities collected by each
device will be diverse, and (ii) communication limitations prevent devices from
uploading all their locally trained modality models to the server. In this
paper, we propose Federated Multimodal Fusion learning with Selective modality
communication (FedMFS), a new multimodal fusion FL methodology that can tackle
the above mentioned challenges. The key idea is the introduction of a modality
selection criterion for each device, which weighs (i) the impact of the
modality, gauged by Shapley value analysis, against (ii) the modality model
size as a gauge for communication overhead. This enables FedMFS to flexibly
balance performance against communication costs, depending on resource
constraints and application requirements. Experiments on the real-world
ActionSense dataset demonstrate the ability of FedMFS to achieve comparable
accuracy to several baselines while reducing the communication overhead by over
4x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Liangqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dong-Jun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellapandi_V/0/1/0/all/0/1&quot;&gt;Vishnu Pandi Chellapandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zak_S/0/1/0/all/0/1&quot;&gt;Stanislaw H. &amp;#x17b;ak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08176">
<title>Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08176</link>
<description rdf:parseType="Literal">&lt;p&gt;This work analyzes Graph Neural Networks, a generalization of Fully-Connected
Deep Neural Nets on Graph structured data, when their width, that is the number
of nodes in each fullyconnected layer is increasing to infinity. Infinite Width
Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels,
both Machine Learning Frameworks with long traditions and extensive theoretical
foundations. Gaussian Processes and Kernels have much less hyperparameters then
Neural Networks and can be used for uncertainty estimation, making them more
user friendly for applications. This works extends the increasing amount of
research connecting Gaussian Processes and Kernels to Neural Networks. The
Kernel and Gaussian Process closed forms are derived for a variety of
architectures, namely the standard Graph Neural Network, the Graph Neural
Network with Skip-Concatenate Connections and the Graph Attention Neural
Network. All architectures are evaluated on a variety of datasets on the task
of transductive Node Regression and Classification. Additionally, a Spectral
Sparsification method known as Effective Resistance is used to improve runtime
and memory requirements. Extending the setting to inductive graph learning
tasks (Graph Regression/ Classification) is straightforward and is briefly
discussed in 3.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobanoglu_Y/0/1/0/all/0/1&quot;&gt;Yunus Cobanoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08278">
<title>Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08278</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen &quot;out-of-distribution&quot; time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1&quot;&gt;Kashif Rasul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1&quot;&gt;Arjun Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Andrew Robert Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorasani_A/0/1/0/all/0/1&quot;&gt;Arian Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adamopoulos_G/0/1/0/all/0/1&quot;&gt;George Adamopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagwatkar_R/0/1/0/all/0/1&quot;&gt;Rishika Bhagwatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1&quot;&gt;Marin Bilo&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghonia_H/0/1/0/all/0/1&quot;&gt;Hena Ghonia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassen_N/0/1/0/all/0/1&quot;&gt;Nadhir Vincent Hassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1&quot;&gt;Anderson Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1&quot;&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapados_N/0/1/0/all/0/1&quot;&gt;Nicolas Chapados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevmyvaka_Y/0/1/0/all/0/1&quot;&gt;Yuriy Nevmyvaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08782">
<title>Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive data is often considered essential for deep learning applications,
but it also incurs significant computational and infrastructural costs.
Therefore, dataset pruning (DP) has emerged as an effective way to improve data
efficiency by identifying and removing redundant training samples without
sacrificing performance. In this work, we aim to address the problem of DP for
transfer learning, i.e., how to prune a source dataset for improved pretraining
efficiency and lossless finetuning accuracy on downstream target tasks. To our
best knowledge, the problem of DP for transfer learning remains open, as
previous studies have primarily addressed DP and transfer learning as separate
problems. By contrast, we establish a unified viewpoint to integrate DP with
transfer learning and find that existing DP methods are not suitable for the
transfer learning paradigm. We then propose two new DP methods, label mapping
and feature mapping, for supervised and self-supervised pretraining settings
respectively, by revisiting the DP problem through the lens of source-target
domain mapping. Furthermore, we demonstrate the effectiveness of our approach
on numerous transfer learning tasks. We show that source data classes can be
pruned by up to 40% ~ 80% without sacrificing downstream performance, resulting
in a significant 2 ~ 5 times speed-up during the pretraining stage. Besides,
our proposal exhibits broad applicability and can improve other computationally
intensive transfer learning techniques, such as adversarial pretraining. Codes
are available at https://github.com/OPTML-Group/DP4TL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yimeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Aochuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinghan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiancheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gaowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10348">
<title>Attribution Patching Outperforms Automated Circuit Discovery. (arXiv:2310.10348v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10348</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated interpretability research has recently attracted attention as a
potential research direction that could scale explanations of neural network
behavior to large models. Existing automated circuit discovery work applies
activation patching to identify subnetworks responsible for solving specific
tasks (circuits). In this work, we show that a simple method based on
attribution patching outperforms all existing methods while requiring just two
forward passes and a backward pass. We apply a linear approximation to
activation patching to estimate the importance of each edge in the
computational subgraph. Using this approximation, we prune the least important
edges of the network. We survey the performance and limitations of this method,
finding that averaged over all tasks our method has greater AUC from circuit
recovery than other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syed_A/0/1/0/all/0/1&quot;&gt;Aaquib Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rager_C/0/1/0/all/0/1&quot;&gt;Can Rager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conmy_A/0/1/0/all/0/1&quot;&gt;Arthur Conmy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11609">
<title>Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance. (arXiv:2310.11609v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11609</link>
<description rdf:parseType="Literal">&lt;p&gt;Structure determination is necessary to identify unknown organic molecules,
such as those in natural products, forensic samples, the interstellar medium,
and laboratory syntheses. Rotational spectroscopy enables structure
determination by providing accurate 3D information about small organic
molecules via their moments of inertia. Using these moments, Kraitchman
analysis determines isotopic substitution coordinates, which are the unsigned
$|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance,
including carbon, nitrogen, and oxygen. While unsigned substitution coordinates
can verify guesses of structures, the missing $+/-$ signs make it challenging
to determine the actual structure from the substitution coordinates alone. To
tackle this inverse problem, we develop KREED (Kraitchman
REflection-Equivariant Diffusion), a generative diffusion model that infers a
molecule&apos;s complete 3D structure from its molecular formula, moments of
inertia, and unsigned substitution coordinates of heavy atoms. KREED&apos;s top-1
predictions identify the correct 3D structure with &amp;gt;98% accuracy on the QM9 and
GEOM datasets when provided with substitution coordinates of all heavy atoms
with natural isotopic abundance. When substitution coordinates are restricted
to only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on
GEOM. On a test set of experimentally measured substitution coordinates
gathered from the literature, KREED predicts the correct all-atom 3D structure
in 25 of 33 cases, demonstrating experimental applicability for context-free 3D
structure determination with rotational spectroscopy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1&quot;&gt;Austin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_A/0/1/0/all/0/1&quot;&gt;Alston Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1&quot;&gt;Santiago Miret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pate_B/0/1/0/all/0/1&quot;&gt;Brooks Pate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1&quot;&gt;Al&amp;#xe1;n Aspuru-Guzik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11732">
<title>Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11732</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the significant progress made in practical applications of aligned
language models (LMs), they tend to be overconfident in output answers compared
to the corresponding pre-trained LMs. In this work, we systematically evaluate
the impact of the alignment process on logit-based uncertainty calibration of
LMs under the multiple-choice setting. We first conduct a thoughtful empirical
study on how aligned LMs differ in calibration from their pre-trained
counterparts. Experimental results reveal that there are two distinct
uncertainties in LMs under the multiple-choice setting, which are responsible
for the answer decision and the format preference of the LMs, respectively.
Then, we investigate the role of these two uncertainties on aligned LM&apos;s
calibration through fine-tuning in simple synthetic alignment schemes and
conclude that one reason for aligned LMs&apos; overconfidence is the conflation of
these two types of uncertainty. Furthermore, we examine the utility of common
post-hoc calibration methods for aligned LMs and propose an easy-to-implement
and sample-efficient method to calibrate aligned LMs. We hope our findings
could provide insights into the design of more reliable alignment processes for
LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1&quot;&gt;Guande He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12629">
<title>Online Combinatorial Linear Optimization via a Frank-Wolfe-based Metarounding Algorithm. (arXiv:2310.12629v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12629</link>
<description rdf:parseType="Literal">&lt;p&gt;Metarounding is an approach to convert an approximation algorithm for linear
optimization over some combinatorial classes to an online linear optimization
algorithm for the same class. We propose a new metarounding algorithm under a
natural assumption that a relax-based approximation algorithm exists for the
combinatorial class. Our algorithm is much more efficient in both theoretical
and practical aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsuboshi_R/0/1/0/all/0/1&quot;&gt;Ryotaro Mitsuboshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatano_K/0/1/0/all/0/1&quot;&gt;Kohei Hatano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takimoto_E/0/1/0/all/0/1&quot;&gt;Eiji Takimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13458">
<title>Correspondence learning between morphologically different robots via task demonstrations. (arXiv:2310.13458v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13458</link>
<description rdf:parseType="Literal">&lt;p&gt;We observe a large variety of robots in terms of their bodies, sensors, and
actuators. Given the commonalities in the skill sets, teaching each skill to
each different robot independently is inefficient and not scalable when the
large variety in the robotic landscape is considered. If we can learn the
correspondences between the sensorimotor spaces of different robots, we can
expect a skill that is learned in one robot can be more directly and easily
transferred to other robots. In this paper, we propose a method to learn
correspondences among two or more robots that may have different morphologies.
To be specific, besides robots with similar morphologies with different degrees
of freedom, we show that a fixed-based manipulator robot with joint control and
a differential drive mobile robot can be addressed within the proposed
framework. To set up the correspondence among the robots considered, an initial
base task is demonstrated to the robots to achieve the same goal. Then, a
common latent representation is learned along with the individual robot
policies for achieving the goal. After the initial learning stage, the
observation of a new task execution by one robot becomes sufficient to generate
a latent space representation pertaining to the other robots to achieve the
same task. We verified our system in a set of experiments where the
correspondence between robots is learned (1) when the robots need to follow the
same paths to achieve the same task, (2) when the robots need to follow
different trajectories to achieve the same task, and (3) when complexities of
the required sensorimotor trajectories are different for the robots. We also
provide a proof-of-the-concept realization of correspondence learning between a
real manipulator robot and a simulated mobile robot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aktas_H/0/1/0/all/0/1&quot;&gt;Hakan Aktas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagai_Y/0/1/0/all/0/1&quot;&gt;Yukie Nagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asada_M/0/1/0/all/0/1&quot;&gt;Minoru Asada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1&quot;&gt;Erhan Oztop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1&quot;&gt;Emre Ugur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14360">
<title>Is ChatGPT a game changer for geocoding -- a benchmark for geocoding address parsing techniques. (arXiv:2310.14360v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14360</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable success of GPT models across various tasks, including toponymy
recognition motivates us to assess the performance of the GPT-3 model in the
geocoding address parsing task. To ensure that the evaluation more accurately
mirrors performance in real-world scenarios with diverse user input qualities
and resolve the pressing need for a &apos;gold standard&apos; evaluation dataset for
geocoding systems, we introduce a benchmark dataset of low-quality address
descriptions synthesized based on human input patterns mining from actual input
logs of a geocoding system in production. This dataset has 21 different input
errors and variations; contains over 239,000 address records that are uniquely
selected from streets across all U.S. 50 states and D.C.; and consists of three
subsets to be used as training, validation, and testing sets. Building on this,
we train and gauge the performance of the GPT-3 model in extracting address
components, contrasting its performance with transformer-based and LSTM-based
models. The evaluation results indicate that Bidirectional LSTM-CRF model has
achieved the best performance over these transformer-based models and GPT-3
model. Transformer-based models demonstrate very comparable results compared to
the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in
performance, showcases potential in the address parsing task with few-shot
examples, exhibiting room for improvement with additional fine-tuning. We open
source the code and data of this presented benchmark so that researchers can
utilize it for future model development or extend it to evaluate similar tasks,
such as document geocoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhengcong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Diya Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_D/0/1/0/all/0/1&quot;&gt;Daniel W. Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15127">
<title>Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models. (arXiv:2310.15127v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15127</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained and frozen large language models (LLMs) can effectively map
simple scene rearrangement instructions to programs over a robot&apos;s visuomotor
functions through appropriate few-shot example prompting. To parse open-domain
natural language and adapt to a user&apos;s idiosyncratic procedures, not known
during prompt engineering time, fixed prompts fall short. In this paper, we
introduce HELPER, an embodied agent equipped with an external memory of
language-program pairs that parses free-form human-robot dialogue into action
programs through retrieval-augmented LLM prompting: relevant memories are
retrieved based on the current dialogue, instruction, correction, or VLM
description, and used as in-context prompt examples for LLM querying. The
memory is expanded during deployment to include pairs of user&apos;s language and
action plans, to assist future inferences and personalize them to the user&apos;s
language and routines. HELPER sets a new state-of-the-art in the TEACh
benchmark in both Execution from Dialog History (EDH) and Trajectory from
Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for
TfD. Our models, code, and video results can be found in our project&apos;s website:
https://helper-agent-llm.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1&quot;&gt;Gabriel Sarch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1&quot;&gt;Michael J. Tarr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1&quot;&gt;Katerina Fragkiadaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15308">
<title>SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15308</link>
<description rdf:parseType="Literal">&lt;p&gt;The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that absorbs their
expertise. Our method integrates techniques of multi-task learning, continual
learning, and distillation. Further, it demands significantly less
computational cost compared to traditional multi-task training from scratch,
and it only needs a small fraction of the pre-training datasets that were
initially used to train individual models. By applying our method to SAM and
CLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM
and CLIP into a single vision transformer. Compared with deploying SAM and CLIP
independently, our merged model, SAM-CLIP, reduces storage and compute costs
for inference, making it well-suited for edge device applications. We show that
SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also
introduces synergistic functionalities, notably in zero-shot semantic
segmentation, where SAM-CLIP establishes new state-of-the-art results on 5
benchmarks. It outperforms previous models that are specifically designed for
this task by a large margin, including +6.8% and +5.9% mean IoU improvement on
Pascal-VOC and COCO-Stuff datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasu_P/0/1/0/all/0/1&quot;&gt;Pavan Kumar Anasosalu Vasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1&quot;&gt;Fartash Faghri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemulapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Vemulapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1&quot;&gt;Mehrdad Farajtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Sachin Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1&quot;&gt;Mohammad Rastegari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1&quot;&gt;Oncel Tuzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pouransari_H/0/1/0/all/0/1&quot;&gt;Hadi Pouransari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15516">
<title>Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15516</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Deep reinforcement learning (DRL) models have shown promising
results in solving routing problems. However, most DRL solvers are commonly
proposed to solve node routing problems, such as the Traveling Salesman Problem
(TSP). Meanwhile, there has been limited research on applying neural methods to
arc routing problems, such as the Chinese Postman Problem (CPP), since they
often feature irregular and complex solution spaces compared to TSP. To fill
these gaps, this paper proposes a novel DRL framework to address the CPP with
load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc
routing problem with load constraints. The novelty of our method is two-fold.
First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential
model. Subsequently, we introduce an autoregressive model based on DRL, namely
Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge
effectively. Such a framework allows the DRL model to work efficiently and
scalably to arc routing problems. Furthermore, we propose a new bio-inspired
meta-heuristic solution based on Evolutionary Algorithm (EA) for CPP-LC.
Extensive experiments show that Arc-DRL outperforms existing meta-heuristic
methods such as Iterative Local Search (ILS) and Variable Neighborhood Search
(VNS) proposed by (Corberan et al., 2018) on large benchmark datasets for
CPP-LC regarding both solution quality and running time; while the EA gives the
best solution quality with much more running time. We release our C++
implementations for metaheuristics such as EA, ILS and VNS along with the code
for data generation and our generated data at
https://github.com/HySonLab/Chinese_Postman_Problem
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1&quot;&gt;Truong Son Hy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1&quot;&gt;Cong Dao Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15543">
<title>Symmetry-preserving graph attention network to solve routing problems at multiple resolutions. (arXiv:2310.15543v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15543</link>
<description rdf:parseType="Literal">&lt;p&gt;Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs)
have achieved reasonable improvement in accuracy and computation time with the
adaptation of Machine Learning (ML) methods. However, none of the previous
works completely respects the symmetries arising from TSPs and VRPs including
rotation, translation, permutation, and scaling. In this work, we introduce the
first-ever completely equivariant model and training to solve combinatorial
problems. Furthermore, it is essential to capture the multiscale structure
(i.e. from local to global information) of the input graph, especially for the
cases of large and long-range graphs, while previous methods are limited to
extracting only local information that can lead to a local or sub-optimal
solution. To tackle the above limitation, we propose a Multiresolution scheme
in combination with Equivariant Graph Attention network (mEGAT) architecture,
which can learn the optimal route based on low-level and high-level graph
resolutions in an efficient way. In particular, our approach constructs a
hierarchy of coarse-graining graphs from the input graph, in which we try to
solve the routing problems on simple low-level graphs first, then utilize that
knowledge for the more complex high-level graphs. Experimentally, we have shown
that our model outperforms existing baselines and proved that symmetry
preservation and multiresolution are important recipes for solving
combinatorial problems in a data-driven manner. Our source code is publicly
available at https://github.com/HySonLab/Multires-NP-hard
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1&quot;&gt;Cong Dao Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bach_T/0/1/0/all/0/1&quot;&gt;Thong Bach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1&quot;&gt;Truong Son Hy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16125">
<title>Online Two-stage Thermal History Prediction Method for Metal Additive Manufacturing of Thin Walls. (arXiv:2310.16125v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16125</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to propose an online two-stage thermal history prediction
method, which could be integrated into a metal AM process for performance
control. Based on the similarity of temperature curves (curve segments of a
temperature profile of one point) between any two successive layers, the first
stage of the proposed method designs a layer-to-layer prediction model to
estimate the temperature curves of the yet-to-print layer from measured
temperatures of certain points on the previously printed layer. With
measured/predicted temperature profiles of several points on the same layer,
the second stage proposes a reduced order model (ROM) (intra-layer prediction
model) to decompose and construct the temperature profiles of all points on the
same layer, which could be used to build the temperature field of the entire
layer. The training of ROM is performed with an extreme learning machine (ELM)
for computational efficiency. Fifteen wire arc AM experiments and nine
simulations are designed for thin walls with a fixed length and unidirectional
printing of each layer. The test results indicate that the proposed prediction
method could construct the thermal history of a yet-to-print layer within 0.1
seconds on a low-cost desktop computer. Meanwhile, the method has acceptable
generalization capability in most cases from lower layers to higher layers in
the same simulation, as well as from one simulation to a new simulation on
different AM process parameters. More importantly, after fine-tuning the
proposed method with limited experimental data, the relative errors of all
predicted temperature profiles on a new experiment are smaller than 0.09, which
demonstrates the applicability and generalization of the proposed two-stage
thermal history prediction method in online applications for metal AM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yifan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehaghani_M/0/1/0/all/0/1&quot;&gt;M. Rahmani Dehaghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajadi_P/0/1/0/all/0/1&quot;&gt;Pouyan Sajadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balani_S/0/1/0/all/0/1&quot;&gt;Shahriar Bakrani Balani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhalpe_A/0/1/0/all/0/1&quot;&gt;Akshay Dhalpe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panicker_S/0/1/0/all/0/1&quot;&gt;Suraj Panicker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coatanea_E/0/1/0/all/0/1&quot;&gt;Eric Coatanea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;G. Gary Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16597">
<title>Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16597</link>
<description rdf:parseType="Literal">&lt;p&gt;The infinitely wide neural network has been proven a useful and manageable
mathematical model that enables the understanding of many phenomena appearing
in deep learning. One example is the convergence of random deep networks to
Gaussian processes that allows a rigorous analysis of the way the choice of
activation function and network weights impacts the training dynamics. In this
paper, we extend the seminal proof of Matthews et al. (2018) to a larger class
of initial weight distributions (which we call PSEUDO-IID), including the
established cases of IID and orthogonal weights, as well as the emerging
low-rank and structured sparse settings celebrated for their computational
speed-up benefits. We show that fully-connected and convolutional networks
initialized with PSEUDO-IID distributions are all effectively equivalent up to
their variance. Using our results, one can identify the Edge-of-Chaos for a
broader class of neural networks and tune them at criticality in order to
enhance their training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nait_Saada_T/0/1/0/all/0/1&quot;&gt;Thiziri Nait-Saada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Naderi_A/0/1/0/all/0/1&quot;&gt;Alireza Naderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tanner_J/0/1/0/all/0/1&quot;&gt;Jared Tanner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18288">
<title>Sustainable Concrete via Bayesian Optimization. (arXiv:2310.18288v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18288</link>
<description rdf:parseType="Literal">&lt;p&gt;Eight percent of global carbon dioxide emissions can be attributed to the
production of cement, the main component of concrete, which is also the
dominant source of CO2 emissions in the construction of data centers. The
discovery of lower-carbon concrete formulae is therefore of high significance
for sustainability. However, experimenting with new concrete formulae is time
consuming and labor intensive, as one usually has to wait to record the
concrete&apos;s 28-day compressive strength, a quantity whose measurement can by its
definition not be accelerated. This provides an opportunity for experimental
design methodology like Bayesian Optimization (BO) to accelerate the search for
strong and sustainable concrete formulae. Herein, we 1) propose modeling steps
that make concrete strength amenable to be predicted accurately by a Gaussian
process model with relatively few measurements, 2) formulate the search for
sustainable concrete as a multi-objective optimization problem, and 3) leverage
the proposed model to carry out multi-objective BO with real-world strength
measurements of the algorithmically proposed mixes. Our experimental results
show improved trade-offs between the mixtures&apos; global warming potential (GWP)
and their associated compressive strengths, compared to mixes based on current
industry practices. Our methods are open-sourced at
github.com/facebookresearch/SustainableConcrete.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ament_S/0/1/0/all/0/1&quot;&gt;Sebastian Ament&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witte_A/0/1/0/all/0/1&quot;&gt;Andrew Witte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1&quot;&gt;Nishant Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusuma_J/0/1/0/all/0/1&quot;&gt;Julius Kusuma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18534">
<title>Multi Time Scale World Models. (arXiv:2310.18534v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18534</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent agents use internal world models to reason and make predictions
about different courses of their actions at many scales. Devising learning
paradigms and architectures that allow machines to learn world models that
operate at multiple levels of temporal abstractions while dealing with complex
uncertainty predictions is a major technical hurdle. In this work, we propose a
probabilistic formalism to learn multi-time scale world models which we call
the Multi Time Scale State Space (MTS3) model. Our model uses a computationally
efficient inference scheme on multiple time scales for highly accurate
long-horizon predictions and uncertainty estimates over several seconds into
the future. Our experiments, which focus on action conditional long horizon
future predictions, show that MTS3 outperforms recent methods on several system
identification benchmarks including complex simulated and real-world dynamical
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaj_V/0/1/0/all/0/1&quot;&gt;Vaisakh Shaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadeh_S/0/1/0/all/0/1&quot;&gt;Saleh Gholam Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_O/0/1/0/all/0/1&quot;&gt;Ozan Demir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douat_L/0/1/0/all/0/1&quot;&gt;Luiz Ricardo Douat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18936">
<title>Adversarial Examples Are Not Real Features. (arXiv:2310.18936v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18936</link>
<description rdf:parseType="Literal">&lt;p&gt;The existence of adversarial examples has been a mystery for years and
attracted much interest. A well-known theory by \citet{ilyas2019adversarial}
explains adversarial vulnerability from a data perspective by showing that one
can extract non-robust features from adversarial examples and these features
alone are useful for classification. However, the explanation remains quite
counter-intuitive since non-robust features are mostly noise features to
humans. In this paper, we re-examine the theory from a larger context by
incorporating multiple learning paradigms. Notably, we find that contrary to
their good usefulness under supervised learning, non-robust features attain
poor usefulness when transferred to other self-supervised learning paradigms,
such as contrastive learning, masked image modeling, and diffusion models. It
reveals that non-robust features are not really as useful as robust or natural
features that enjoy good transferability between these paradigms. Meanwhile,
for robustness, we also show that naturally trained encoders from robust
features are largely non-robust under AutoAttack. Our cross-paradigm
examination suggests that the non-robust features are not really useful but
more like paradigm-wise shortcuts, and robust features alone might be
insufficient to attain reliable model robustness. Code is available at
\url{https://github.com/PKU-ML/AdvNotRealFeatures}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18948">
<title>Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting. (arXiv:2310.18948v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18948</link>
<description rdf:parseType="Literal">&lt;p&gt;Maritime transportation is paramount in achieving global economic growth,
entailing concurrent ecological obligations in sustainability and safeguarding
endangered marine species, most notably preserving large whale populations. In
this regard, the Automatic Identification System (AIS) data plays a significant
role by offering real-time streaming data on vessel movement, allowing enhanced
traffic monitoring. This study explores using AIS data to prevent
vessel-to-whale collisions by forecasting long-term vessel trajectories from
engineered AIS data sequences. For such a task, we have developed an
encoder-decoder model architecture using Bidirectional Long Short-Term Memory
Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1
to 3 hours of AIS data as input. We feed the model with probabilistic features
engineered from historical AIS data that refer to each trajectory&apos;s potential
route and destination. The model then predicts the vessel&apos;s trajectory,
considering these additional features by leveraging convolutional layers for
spatial feature learning and a position-aware attention mechanism that
increases the importance of recent timesteps of a sequence during temporal
feature learning. The probabilistic features have an F1 Score of approximately
85% and 75% for each feature type, respectively, demonstrating their
effectiveness in augmenting information to the neural network. We test our
model on the Gulf of St. Lawrence, a region known to be the habitat of North
Atlantic Right Whales (NARW). Our model achieved a high R2 score of over 98%
using various techniques and features. It stands out among other approaches as
it can make complex decisions during turnings and path selection. Our study
highlights the potential of data engineering and trajectory forecasting models
for marine life species preservation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spadon_G/0/1/0/all/0/1&quot;&gt;Gabriel Spadon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1&quot;&gt;Jay Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Matthew Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vela_S/0/1/0/all/0/1&quot;&gt;Sarah Vela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrmann_R/0/1/0/all/0/1&quot;&gt;Romina Gehrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eden_D/0/1/0/all/0/1&quot;&gt;Derek Eden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkel_J/0/1/0/all/0/1&quot;&gt;Joshua van Berkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1&quot;&gt;Amilcar Soares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fablet_R/0/1/0/all/0/1&quot;&gt;Ronan Fablet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelot_R/0/1/0/all/0/1&quot;&gt;Ronald Pelot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19727">
<title>Generating Medical Prescriptions with Conditional Transformer. (arXiv:2310.19727v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19727</link>
<description rdf:parseType="Literal">&lt;p&gt;Access to real-world medication prescriptions is essential for medical
research and healthcare quality improvement. However, access to real medication
prescriptions is often limited due to the sensitive nature of the information
expressed. Additionally, manually labelling these instructions for training and
fine-tuning Natural Language Processing (NLP) models can be tedious and
expensive. We introduce a novel task-specific model architecture,
Label-To-Text-Transformer (\textbf{LT3}), tailored to generate synthetic
medication prescriptions based on provided labels, such as a vocabulary list of
medications and their attributes. LT3 is trained on a set of around 2K lines of
medication prescriptions extracted from the MIMIC-III database, allowing the
model to produce valuable synthetic medication prescriptions. We evaluate LT3&apos;s
performance by contrasting it with a state-of-the-art Pre-trained Language
Model (PLM), T5, analysing the quality and diversity of generated texts. We
deploy the generated synthetic data to train the SpacyNER model for the Named
Entity Recognition (NER) task over the n2c2-2018 dataset. The experiments show
that the model trained on synthetic data can achieve a 96-98\% F1 score at
Label Recognition on Drug, Frequency, Route, Strength, and Form. LT3 codes and
data will be shared at
\url{https://github.com/HECTA-UoM/Label-To-Text-Transformer}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkadi_S/0/1/0/all/0/1&quot;&gt;Samuel Belkadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheletti_N/0/1/0/all/0/1&quot;&gt;Nicolo Micheletti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Del_Pinto_W/0/1/0/all/0/1&quot;&gt;Warren Del-Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19909">
<title>Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks. (arXiv:2310.19909v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19909</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network based computer vision systems are typically built on a
backbone, a pretrained or randomly initialized feature extractor. Several years
ago, the default option was an ImageNet-trained convolutional neural network.
However, the recent past has seen the emergence of countless backbones
pretrained using various algorithms and datasets. While this abundance of
choice has led to performance increases for a range of systems, it is difficult
for practitioners to make informed decisions about which backbone to choose.
Battle of the Backbones (BoB) makes this choice easier by benchmarking a
diverse suite of pretrained models, including vision-language models, those
trained via self-supervised learning, and the Stable Diffusion backbone, across
a diverse set of computer vision tasks ranging from classification to object
detection to OOD generalization and more. Furthermore, BoB sheds light on
promising directions for the research community to advance computer vision by
illuminating strengths and weakness of existing approaches through a
comprehensive analysis conducted on more than 1500 training runs. While vision
transformers (ViTs) and self-supervised learning (SSL) are increasingly
popular, we find that convolutional neural networks pretrained in a supervised
fashion on large training sets still perform best on most tasks among the
models we consider. Moreover, in apples-to-apples comparisons on the same
architectures and similarly sized pretraining datasets, we find that SSL
backbones are highly competitive, indicating that future works should perform
SSL pretraining with advanced architectures and larger pretraining datasets. We
release the raw results of our experiments along with code that allows
researchers to put their own backbones through the gauntlet here:
https://github.com/hsouri/Battle-of-the-Backbones
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1&quot;&gt;Hossein Souri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Renkun Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1&quot;&gt;Manli Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1&quot;&gt;Viraj Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1&quot;&gt;Gowthami Somepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1&quot;&gt;Prithvijit Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mark Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bardes_A/0/1/0/all/0/1&quot;&gt;Adrien Bardes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20049">
<title>SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20049</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulating fluid dynamics is crucial for the design and development process,
ranging from simple valves to complex turbomachinery. Accurately solving the
underlying physical equations is computationally expensive. Therefore,
learning-based solvers that model interactions on meshes have gained interest
due to their promising speed-ups. However, it is unknown to what extent these
models truly understand the underlying physical principles and can generalize
rather than interpolate. Generalization is a key requirement for a
general-purpose fluid simulator, which should adapt to different topologies,
resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to
test the $\textit{generalization}$ of learned graph-based fluid simulators.
SURF comprises individual datasets and provides specific performance and
generalization metrics for evaluating and comparing different models. We
empirically demonstrate the applicability of SURF by thoroughly investigating
the two state-of-the-art graph-based models, yielding new insights into their
generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunzli_S/0/1/0/all/0/1&quot;&gt;Stefan K&amp;#xfc;nzli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grotschla_F/0/1/0/all/0/1&quot;&gt;Florian Gr&amp;#xf6;tschla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathys_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l Mathys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1&quot;&gt;Roger Wattenhofer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20258">
<title>Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20258</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization is a powerful method for optimizing black-box functions
with limited function evaluations. Recent works have shown that optimization in
a latent space through deep generative models such as variational autoencoders
leads to effective and efficient Bayesian optimization for structured or
discrete data. However, as the optimization does not take place in the input
space, it leads to an inherent gap that results in potentially suboptimal
solutions. To alleviate the discrepancy, we propose Correlated latent space
Bayesian Optimization (CoBO), which focuses on learning correlated latent
spaces characterized by a strong correlation between the distances in the
latent space and the distances within the objective function. Specifically, our
method introduces Lipschitz regularization, loss weighting, and trust region
recoordination to minimize the inherent gap around the promising areas. We
demonstrate the effectiveness of our approach on several optimization tasks in
discrete data, such as molecule design and arithmetic expression fitting, and
achieve high performance within a small budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seunghun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1&quot;&gt;Jaewon Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sihyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Juyeon Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01310">
<title>Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01310</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have gained significant attention and achieved
state-of-the-art performance in various computer vision tasks, including image
classification, instance segmentation, and object detection. However,
challenges remain in addressing attention complexity and effectively capturing
fine-grained information within images. Existing solutions often resort to
down-sampling operations, such as pooling, to reduce computational cost.
Unfortunately, such operations are non-invertible and can result in information
loss. In this paper, we present a novel approach called Scattering Vision
Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally
scattering network that enables the capture of intricate image details. SVT
overcomes the invertibility issue associated with down-sampling operations by
separating low-frequency and high-frequency components. Furthermore, SVT
introduces a unique spectral gating network utilizing Einstein multiplication
for token and channel mixing, effectively reducing complexity. We show that SVT
achieves state-of-the-art performance on the ImageNet dataset with a
significant reduction in a number of parameters and FLOPS. SVT shows 2\%
improvement over LiTv2 and iFormer. SVT-H-S reaches 84.2\% top-1 accuracy,
while SVT-H-B reaches 85.2\% (state-of-art for base versions) and SVT-H-L
reaches 85.7\% (again state-of-art for large versions). SVT also shows
comparable results in other vision tasks such as instance segmentation. SVT
also outperforms other transformers in transfer learning on standard datasets
such as CIFAR10, CIFAR100, Oxford Flower, and Stanford Car datasets. The
project page is available on this
webpage.\url{https://badripatro.github.io/svt/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patro_B/0/1/0/all/0/1&quot;&gt;Badri N. Patro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agneeswaran_V/0/1/0/all/0/1&quot;&gt;Vijay Srinivas Agneeswaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03197">
<title>Stable Linear Subspace Identification: A Machine Learning Approach. (arXiv:2311.03197v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03197</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
&lt;/p&gt;
&lt;p&gt;We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa&apos;s ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Natale_L/0/1/0/all/0/1&quot;&gt;Loris Di Natale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zakwan_M/0/1/0/all/0/1&quot;&gt;Muhammad Zakwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Svetozarevic_B/0/1/0/all/0/1&quot;&gt;Bratislav Svetozarevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heer_P/0/1/0/all/0/1&quot;&gt;Philipp Heer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trecate_G/0/1/0/all/0/1&quot;&gt;Giancarlo Ferrari Trecate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jones_C/0/1/0/all/0/1&quot;&gt;Colin N. Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04520">
<title>Adaptive Mirror Descent Bilevel Optimization. (arXiv:2311.04520v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04520</link>
<description rdf:parseType="Literal">&lt;p&gt;In the paper, we propose a class of efficient adaptive bilevel methods based
on mirror descent for nonconvex bilevel optimization, where its upper-level
problem is nonconvex possibly with nonsmooth regularization, and its
lower-level problem is also nonconvex while satisfies Polyak-{\L}ojasiewicz
(PL) condition. To solve these deterministic bilevel problems, we present an
efficient adaptive projection-aid gradient (i.e., AdaPAG) method based on
mirror descent, and prove that it obtains the best known gradient complexity of
$O(\epsilon^{-1})$ for finding an $\epsilon$-stationary solution of nonconvex
bilevel problems. To solve these stochastic bilevel problems, we propose an
efficient adaptive stochastic projection-aid gradient (i.e., AdaVSPAG) methods
based on mirror descent and variance-reduced techniques, and prove that it
obtains the best known gradient complexity of $O(\epsilon^{-3/2})$ for finding
an $\epsilon$-stationary solution. Since the PL condition relaxes the strongly
convex, our algorithms can be used to nonconvex strongly-convex bilevel
optimization. Theoretically, we provide a useful convergence analysis framework
for our methods under some mild conditions, and prove that our methods have a
fast convergence rate of $O(\frac{1}{T})$, where $T$ denotes the number of
iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Feihu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05659">
<title>Enhancing Instance-Level Image Classification with Set-Level Labels. (arXiv:2311.05659v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05659</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance-level image classification tasks have traditionally relied on
single-instance labels to train models, e.g., few-shot learning and transfer
learning. However, set-level coarse-grained labels that capture relationships
among instances can provide richer information in real-world scenarios. In this
paper, we present a novel approach to enhance instance-level image
classification by leveraging set-level labels. We provide a theoretical
analysis of the proposed method, including recognition conditions for fast
excess risk rate, shedding light on the theoretical foundations of our
approach. We conducted experiments on two distinct categories of datasets:
natural image datasets and histopathology image datasets. Our experimental
results demonstrate the effectiveness of our approach, showcasing improved
classification performance compared to traditional single-instance label-based
methods. Notably, our algorithm achieves 13% improvement in classification
accuracy compared to the strongest baseline on the histopathology image
classification benchmarks. Importantly, our experimental findings align with
the theoretical analysis, reinforcing the robustness and reliability of our
proposed method. This work bridges the gap between instance-level and set-level
image classification, offering a promising avenue for advancing the
capabilities of image classification models with set-level coarse-grained
labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aly A. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grossman_R/0/1/0/all/0/1&quot;&gt;Robert L. Grossman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06212">
<title>Differentiable VQ-VAE&apos;s for Robust White Matter Streamline Encodings. (arXiv:2311.06212v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06212</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the complex geometry of white matter streamlines, Autoencoders have
been proposed as a dimension-reduction tool to simplify the analysis
streamlines in a low-dimensional latent spaces. However, despite these recent
successes, the majority of encoder architectures only perform dimension
reduction on single streamlines as opposed to a full bundle of streamlines.
This is a severe limitation of the encoder architecture that completely
disregards the global geometric structure of streamlines at the expense of
individual fibers. Moreover, the latent space may not be well structured which
leads to doubt into their interpretability. In this paper we propose a novel
Differentiable Vector Quantized Variational Autoencoder, which are engineered
to ingest entire bundles of streamlines as single data-point and provides
reliable trustworthy encodings that can then be later used to analyze
streamlines in the latent space. Comparisons with several state of the art
Autoencoders demonstrate superior performance in both encoding and synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lizarraga_A/0/1/0/all/0/1&quot;&gt;Andrew Lizarraga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Taraku_B/0/1/0/all/0/1&quot;&gt;Brandon Taraku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Honig_E/0/1/0/all/0/1&quot;&gt;Edouardo Honig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Shantanu H. Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06444">
<title>Mitigating Pooling Bias in E-commerce Search via False Negative Estimation. (arXiv:2311.06444v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06444</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient and accurate product relevance assessment is critical for user
experiences and business success. Training a proficient relevance assessment
model requires high-quality query-product pairs, often obtained through
negative sampling strategies. Unfortunately, current methods introduce pooling
bias by mistakenly sampling false negatives, diminishing performance and
business impact. To address this, we present Bias-mitigating Hard Negative
Sampling (BHNS), a novel negative sampling strategy tailored to identify and
adjust for false negatives, building upon our original False Negative
Estimation algorithm. Our experiments in the Instacart search setting confirm
BHNS as effective for practical e-commerce use. Furthermore, comparative
analyses on public dataset showcase its domain-agnostic potential for diverse
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_T/0/1/0/all/0/1&quot;&gt;Taesik Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenneti_T/0/1/0/all/0/1&quot;&gt;Tejaswi Tenneti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haixun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fenglong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07073">
<title>Exposition on over-squashing problem on GNNs: Current Methods, Benchmarks and Challenges. (arXiv:2311.07073v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07073</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-based message-passing neural networks (MPNNs) have achieved remarkable
success in both node and graph-level learning tasks. However, several
identified problems, including over-smoothing (OSM), limited expressive power,
and over-squashing (OSQ), still limit the performance of MPNNs. In particular,
OSQ serves as the latest identified problem, where MPNNs gradually lose their
learning accuracy when long-range dependencies between graph nodes are
required. In this work, we provide an exposition on the OSQ problem by
summarizing different formulations of OSQ from current literature, as well as
the three different categories of approaches for addressing the OSQ problem. In
addition, we also discuss the alignment between OSQ and expressive power and
the trade-off between OSQ and OSM. Furthermore, we summarize the empirical
methods leveraged from existing works to verify the efficiency of OSQ
mitigation approaches, with illustrations of their computational complexities.
Lastly, we list some open questions that are of interest for further
exploration of the OSQ problem along with potential directions from the best of
our knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Dai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_A/0/1/0/all/0/1&quot;&gt;Andi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lequan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junbin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07355">
<title>ADAMM: Anomaly Detection of Attributed Multi-graphs with Metadata: A Unified Neural Network Approach. (arXiv:2311.07355v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07355</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a complex graph database of node- and edge-attributed multi-graphs as
well as associated metadata for each graph, how can we spot the anomalous
instances? Many real-world problems can be cast as graph inference tasks where
the graph representation could capture complex relational phenomena (e.g.,
transactions among financial accounts in a journal entry), along with metadata
reflecting tabular features (e.g. approver, effective date, etc.). While
numerous anomaly detectors based on Graph Neural Networks (GNNs) have been
proposed, none are capable of directly handling directed graphs with
multi-edges and self-loops. Furthermore, the simultaneous handling of
relational and tabular features remains an unexplored area. In this work we
propose ADAMM, a novel graph neural network model that handles directed
multi-graphs, providing a unified end-to-end architecture that fuses metadata
and graph-level representation learning through an unsupervised anomaly
detection objective. Experiments on datasets from two different domains,
namely, general-ledger journal entries from different firms (accounting) as
well as human GPS trajectories from thousands of individuals (urban mobility)
validate ADAMM&apos;s generality and detection effectiveness of expert-guided and
ground-truth anomalies. Notably, ADAMM outperforms existing baselines that
handle the two data modalities (graph and metadata) separately with post hoc
synthesis efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sotiropoulos_K/0/1/0/all/0/1&quot;&gt;Konstantinos Sotiropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lingxiao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Pierre Jinghong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1&quot;&gt;Leman Akoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07633">
<title>Rethinking and Benchmarking Predict-then-Optimize Paradigm for Combinatorial Optimization Problems. (arXiv:2311.07633v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07633</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous web applications rely on solving combinatorial optimization
problems, such as energy cost-aware scheduling, budget allocation on web
advertising, and graph matching on social networks. However, many optimization
problems involve unknown coefficients, and improper predictions of these
factors may lead to inferior decisions which may cause energy wastage,
inefficient resource allocation, inappropriate matching in social networks,
etc. Such a research topic is referred to as &quot;Predict-Then-Optimize (PTO)&quot;
which considers the performance of prediction and decision-making in a unified
system. A noteworthy recent development is the end-to-end methods by directly
optimizing the ultimate decision quality which claims to yield better results
in contrast to the traditional two-stage approach. However, the evaluation
benchmarks in this field are fragmented and the effectiveness of various models
in different scenarios remains unclear, hindering the comprehensive assessment
and fast deployment of these methods. To address these issues, we provide a
comprehensive categorization of current approaches and integrate existing
experimental scenarios to establish a unified benchmark, elucidating the
circumstances under which end-to-end training yields improvements, as well as
the contexts in which it performs ineffectively. We also introduce a new
dataset for the industrial combinatorial advertising problem for inclusive
finance to open-source. We hope the rethinking and benchmarking of PTO could
facilitate more convenient evaluation and deployment, and inspire further
improvements both in the academy and industry within this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1&quot;&gt;Haoyu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1&quot;&gt;Hang Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Runzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07723">
<title>Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07723</link>
<description rdf:parseType="Literal">&lt;p&gt;As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following&apos; by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models&apos;
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1&quot;&gt;Joshua Clymer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1&quot;&gt;Garrett Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1&quot;&gt;Rohan Subramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sam Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07750">
<title>SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification. (arXiv:2311.07750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07750</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the &quot;ChestX-ray14&quot; dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashraf_S/0/1/0/all/0/1&quot;&gt;S.M. Nabil Ashraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1&quot;&gt;Md. Adyelullahil Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1&quot;&gt;Hasnat Md. Abdullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md. Golam Rabiul Alam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07772">
<title>In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07772</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. Recent works
suggest that ICL can be thought of as a gradient descent (GD) based
optimization process. While promising, these results mainly focus on simplified
settings of ICL and provide only a preliminary evaluation of the similarities
between the two methods. In this work, we revisit the comparison between ICL
and GD-based finetuning and study what properties of ICL an equivalent process
must follow. We highlight a major difference in the flow of information between
ICL and standard finetuning. Namely, ICL can only rely on information from
lower layers at every point, while finetuning depends on loss gradients from
deeper layers. We refer to this discrepancy as Layer Causality and show that a
layer causal variant of the finetuning process aligns with ICL on par with
vanilla finetuning and is even better in most cases across relevant metrics. To
the best of our knowledge, this is the first work to discuss this discrepancy
explicitly and suggest a solution that tackles this problem with minimal
changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deutch_G/0/1/0/all/0/1&quot;&gt;Gilad Deutch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magar_N/0/1/0/all/0/1&quot;&gt;Nadav Magar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natan_T/0/1/0/all/0/1&quot;&gt;Tomer Bar Natan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1&quot;&gt;Guy Dar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07841">
<title>PEMS: Pre-trained Epidemic Time-series Models. (arXiv:2311.07841v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07841</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing accurate and reliable predictions about the future of an epidemic
is an important problem for enabling informed public health decisions. Recent
works have shown that leveraging data-driven solutions that utilize advances in
deep learning methods to learn from past data of an epidemic often outperform
traditional mechanistic models. However, in many cases, the past data is sparse
and may not sufficiently capture the underlying dynamics. While there exists a
large amount of data from past epidemics, leveraging prior knowledge from
time-series data of other diseases is a non-trivial challenge. Motivated by the
success of pre-trained models in language and vision tasks, we tackle the
problem of pre-training epidemic time-series models to learn from multiple
datasets from different diseases and epidemics. We introduce Pre-trained
Epidemic Time-Series Models (PEMS) that learn from diverse time-series datasets
of a variety of diseases by formulating pre-training as a set of
self-supervised learning (SSL) tasks. We tackle various important challenges
specific to pre-training for epidemic time-series such as dealing with
heterogeneous dynamics and efficiently capturing useful patterns from multiple
epidemic datasets by carefully designing the SSL tasks to learn important
priors about the epidemic dynamics that can be leveraged for fine-tuning to
multiple downstream tasks. The resultant PEM outperforms previous
state-of-the-art methods in various downstream time-series tasks across
datasets of varying seasonal patterns, geography, and mechanism of contagion
including the novel Covid-19 pandemic unseen in pre-trained data with better
efficiency using smaller fraction of datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamarthi_H/0/1/0/all/0/1&quot;&gt;Harshavardhan Kamarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1&quot;&gt;B. Aditya Prakash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08393">
<title>MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation. (arXiv:2311.08393v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08393</link>
<description rdf:parseType="Literal">&lt;p&gt;The learn-from-observation (LfO) paradigm is a human-inspired mode for a
robot to learn to perform a task simply by watching it being performed. LfO can
facilitate robot integration on factory floors by minimizing disruption and
reducing tedious programming. A key component of the LfO pipeline is a
transformation of the depth camera frames to the corresponding task state and
action pairs, which are then relayed to learning techniques such as imitation
or inverse reinforcement learning for understanding the task parameters. While
several existing computer vision models analyze videos for activity
recognition, SA-Net specifically targets robotic LfO from RGB-D data. However,
SA-Net and many other models analyze frame data captured from a single
viewpoint. Their analysis is therefore highly sensitive to occlusions of the
observed task, which are frequent in deployments. An obvious way of reducing
occlusions is to simultaneously observe the task from multiple viewpoints and
synchronously fuse the multiple streams in the model. Toward this, we present
multi-view SA-Net, which generalizes the SA-Net model to allow the perception
of multiple viewpoints of the task activity, integrate them, and better
recognize the state and action in each frame. Performance evaluations on two
distinct domains establish that MVSA-Net recognizes the state-action pairs
under occlusion more accurately compared to single-view MVSA-Net and other
baselines. Our ablation studies further evaluate its performance under
different ambient conditions and establish the contribution of the architecture
components. As such, MVSA-Net offers a significantly more robust and deployable
state-action trajectory generation compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asali_E/0/1/0/all/0/1&quot;&gt;Ehsan Asali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1&quot;&gt;Prashant Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jin Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08427">
<title>Towards a Transportable Causal Network Model Based on Observational Healthcare Data. (arXiv:2311.08427v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08427</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last decades, many prognostic models based on artificial
intelligence techniques have been used to provide detailed predictions in
healthcare. Unfortunately, the real-world observational data used to train and
validate these models are almost always affected by biases that can strongly
impact the outcomes validity: two examples are values missing not-at-random and
selection bias. Addressing them is a key element in achieving transportability
and in studying the causal relationships that are critical in clinical decision
making, going beyond simpler statistical approaches based on probabilistic
association.
&lt;/p&gt;
&lt;p&gt;In this context, we propose a novel approach that combines selection
diagrams, missingness graphs, causal discovery and prior knowledge into a
single graphical model to estimate the cardiovascular risk of adolescent and
young females who survived breast cancer. We learn this model from data
comprising two different cohorts of patients. The resulting causal network
model is validated by expert clinicians in terms of risk assessment, accuracy
and explainability, and provides a prognostic model that outperforms competing
machine learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernasconi_A/0/1/0/all/0/1&quot;&gt;Alice Bernasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanga_A/0/1/0/all/0/1&quot;&gt;Alessio Zanga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_P/0/1/0/all/0/1&quot;&gt;Peter J.F. Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1&quot;&gt;Marco Scutari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1&quot;&gt;Fabio Stella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08774">
<title>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation. (arXiv:2311.08774v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08774</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-assisted nuclei segmentation in histopathological images is a crucial task
in the diagnosis and treatment of cancer diseases. It decreases the time
required to manually screen microscopic tissue images and can resolve the
conflict between pathologists during diagnosis. Deep Learning has proven useful
in such a task. However, lack of labeled data is a significant barrier for deep
learning-based approaches. In this study, we propose a novel approach to nuclei
segmentation that leverages the available labelled and unlabelled data. The
proposed method combines the strengths of both transductive and inductive
learning, which have been previously attempted separately, into a single
framework. Inductive learning aims at approximating the general function and
generalizing to unseen test data, while transductive learning has the potential
of leveraging the unlabelled test data to improve the classification. To the
best of our knowledge, this is the first study to propose such a hybrid
approach for medical image segmentation. Moreover, we propose a novel two-stage
transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to
demonstrate the efficacy and potential of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1&quot;&gt;Hesham Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tondji_I/0/1/0/all/0/1&quot;&gt;Idriss Tondji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09115">
<title>HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data. (arXiv:2311.09115v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09115</link>
<description rdf:parseType="Literal">&lt;p&gt;Technological advances in medical data collection such as high-resolution
histopathology and high-throughput genomic sequencing have contributed to the
rising requirement for multi-modal biomedical modelling, specifically for
image, tabular, and graph data. Most multi-modal deep learning approaches use
modality-specific architectures that are trained separately and cannot capture
the crucial cross-modal information that motivates the integration of different
data sources. This paper presents the Hybrid Early-fusion Attention Learning
Network (HEALNet): a flexible multi-modal fusion architecture, which a)
preserves modality-specific structural information, b) captures the cross-modal
interactions and structural information in a shared latent space, c) can
effectively handle missing modalities during training and inference, and d)
enables intuitive model inspection by learning on the raw data input instead of
opaque embeddings. We conduct multi-modal survival analysis on Whole Slide
Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas
(TCGA). HEALNet achieves state-of-the-art performance, substantially improving
over both uni-modal and recent multi-modal baselines, whilst being robust in
scenarios with missing modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemker_K/0/1/0/all/0/1&quot;&gt;Konstantin Hemker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1&quot;&gt;Nikola Simidjievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamnik_M/0/1/0/all/0/1&quot;&gt;Mateja Jamnik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09574">
<title>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML&apos;s interpretable models, developed on a
limited volume of H&amp;amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vivek Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Vrishab Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Brent Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1&quot;&gt;Oscar Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1&quot;&gt;Rebecca Rojansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1&quot;&gt;Fabiola Valvert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1&quot;&gt;Edward Briercheck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1&quot;&gt;David Weinstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1&quot;&gt;Yasodha Natkunam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1&quot;&gt;Sebastian Fernandez-Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10090">
<title>JaxMARL: Multi-Agent RL Environments in JAX. (arXiv:2311.10090v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10090</link>
<description rdf:parseType="Literal">&lt;p&gt;Benchmarks play an important role in the development of machine learning
algorithms. For example, research in reinforcement learning (RL) has been
heavily influenced by available environments and benchmarks. However, RL
environments are traditionally run on the CPU, limiting their scalability with
typical academic compute. Recent advancements in JAX have enabled the wider use
of hardware acceleration to overcome these computational hurdles, enabling
massively parallel RL training pipelines and environments. This is particularly
useful for multi-agent reinforcement learning (MARL) research. First of all,
multiple agents must be considered at each environment step, adding
computational burden, and secondly, the sample complexity is increased due to
non-stationarity, decentralised partial observability, or other MARL
challenges. In this paper, we present JaxMARL, the first open-source code base
that combines ease-of-use with GPU enabled efficiency, and supports a large
number of commonly used MARL environments as well as popular baseline
algorithms. When considering wall clock time, our experiments show that per-run
our JAX-based training pipeline is up to 12500x faster than existing
approaches. This enables efficient and thorough evaluations, with the potential
to alleviate the evaluation crisis of the field. We also introduce and
benchmark SMAX, a vectorised, simplified version of the popular StarCraft
Multi-Agent Challenge, which removes the need to run the StarCraft II game
engine. This not only enables GPU acceleration, but also provides a more
flexible MARL environment, unlocking the potential for self-play,
meta-learning, and other future applications in MARL. We provide code at
https://github.com/flairox/jaxmarl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1&quot;&gt;Alexander Rutherford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_B/0/1/0/all/0/1&quot;&gt;Benjamin Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallici_M/0/1/0/all/0/1&quot;&gt;Matteo Gallici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cook_J/0/1/0/all/0/1&quot;&gt;Jonathan Cook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupu_A/0/1/0/all/0/1&quot;&gt;Andrei Lupu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingvarsson_G/0/1/0/all/0/1&quot;&gt;Gardar Ingvarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1&quot;&gt;Timon Willi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Akbir Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witt_C/0/1/0/all/0/1&quot;&gt;Christian Schroeder de Witt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souly_A/0/1/0/all/0/1&quot;&gt;Alexandra Souly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Saptarashmi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1&quot;&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_R/0/1/0/all/0/1&quot;&gt;Robert Tjarko Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacerda_B/0/1/0/all/0/1&quot;&gt;Bruno Lacerda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawes_N/0/1/0/all/0/1&quot;&gt;Nick Hawes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rocktaschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Nicolaus Foerster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10251">
<title>UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets. (arXiv:2311.10251v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10251</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Can Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Sheng Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Junyi Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pang_S/0/1/0/all/0/1&quot;&gt;Shuchao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Orgun_M/0/1/0/all/0/1&quot;&gt;Mehmet A. Orgun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10489">
<title>Handling Overlapping Asymmetric Datasets -- A Twice Penalized P-Spline Approach. (arXiv:2311.10489v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10489</link>
<description rdf:parseType="Literal">&lt;p&gt;Overlapping asymmetric datasets are common in data science and pose questions
of how they can be incorporated together into a predictive analysis. In
healthcare datasets there is often a small amount of information that is
available for a larger number of patients such as an electronic health record,
however a small number of patients may have had extensive further testing.
Common solutions such as missing imputation can often be unwise if the smaller
cohort is significantly different in scale to the larger sample, therefore the
aim of this research is to develop a new method which can model the smaller
cohort against a particular response, whilst considering the larger cohort
also. Motivated by non-parametric models, and specifically flexible smoothing
techniques via generalized additive models, we model a twice penalized P-Spline
approximation method to firstly prevent over/under-fitting of the smaller
cohort and secondly to consider the larger cohort. This second penalty is
created through discrepancies in the marginal value of covariates that exist in
both the smaller and larger cohorts. Through data simulations, parameter
tunings and model adaptations to consider a continuous and binary response, we
find our twice penalized approach offers an enhanced fit over a linear B-Spline
and once penalized P-Spline approximation. Applying to a real-life dataset
relating to a person&apos;s risk of developing Non-Alcoholic Steatohepatitis, we see
an improved model fit performance of over 65%. Areas for future work within
this space include adapting our method to not require dimensionality reduction
and also consider parametric modelling methods. However, to our knowledge this
is the first work to propose additional marginal penalties in a flexible
regression of which we can report a vastly improved model fit that is able to
consider asymmetric datasets, without the need for missing data imputation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McTeer_M/0/1/0/all/0/1&quot;&gt;Matthew McTeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Henderson_R/0/1/0/all/0/1&quot;&gt;Robin Henderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anstee_Q/0/1/0/all/0/1&quot;&gt;Quentin M Anstee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Missier_P/0/1/0/all/0/1&quot;&gt;Paolo Missier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19845">
<title>Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2310.19845</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, spam on online social networks has attracted attention in the
research and business world. Twitter has become the preferred medium to spread
spam content. Many research efforts attempted to encounter social networks
spam. Twitter brought extra challenges represented by the feature space size,
and imbalanced data distributions. Usually, the related research works focus on
part of these main challenges or produce black-box models. In this paper, we
propose a modified genetic algorithm for simultaneous dimensionality reduction
and hyper parameter optimization over imbalanced datasets. The algorithm
initialized an eXtreme Gradient Boosting classifier and reduced the features
space of tweets dataset; to generate a spam prediction model. The model is
validated using a 50 times repeated 10-fold stratified cross-validation, and
analyzed using nonparametric statistical tests. The resulted prediction model
attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy
respectively, utilizing less than 10\% of the total feature space. The
empirical results show that the modified genetic algorithm outperforms $Chi^2$
and $PCA$ feature selection methods. In addition, eXtreme Gradient Boosting
outperforms many machine learning algorithms, including BERT-based deep
learning model, in spam prediction. Furthermore, the proposed approach is
applied to SMS spam modeling and compared to related works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghatasheh_N/0/1/0/all/0/1&quot;&gt;Nazeeh Ghatasheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altaharwa_I/0/1/0/all/0/1&quot;&gt;Ismail Altaharwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aldebei_K/0/1/0/all/0/1&quot;&gt;Khaled Aldebei&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>