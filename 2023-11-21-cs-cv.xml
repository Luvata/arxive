<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.06905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04593" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.10099">
<title>Smart Traffic Management of Vehicles using Faster R-CNN based Deep Learning Method. (arXiv:2311.10099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10099</link>
<description rdf:parseType="Literal">&lt;p&gt;With constant growth of civilization and modernization of cities all across
the world since past few centuries smart traffic management of vehicles is one
of the most sorted after problem by research community. It is a challenging
problem in computer vision and artificial intelligence domain. Smart traffic
management basically involves segmentation of vehicles, estimation of traffic
density and tracking of vehicles. The vehicle segmentation from traffic videos
helps realization of niche applications such as monitoring of speed and
estimation of traffic. When occlusions, background with clutters and traffic
with density variations are present, this problem becomes more intractable in
nature. Keeping this motivation in this research work, we investigate Faster
R-CNN based deep learning method towards segmentation of vehicles. This problem
is addressed in four steps viz minimization with adaptive background model,
Faster R-CNN based subnet operation, Faster R-CNN initial refinement and result
optimization with extended topological active nets. The computational framework
uses ideas of adaptive background modeling. It also addresses shadow and
illumination related issues. Higher segmentation accuracy is achieved through
topological active net deformable models. The topological and extended
topological active nets help to achieve stated deformations. Mesh deformation
is achieved with minimization of energy. The segmentation accuracy is improved
with modified version of extended topological active net. The experimental
results demonstrate superiority of this computational framework
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_A/0/1/0/all/0/1&quot;&gt;Arindam Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10111">
<title>VideoCon: Robust Video-Language Alignment via Contrast Captions. (arXiv:2311.10111v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10111</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite being (pre)trained on a massive amount of data, state-of-the-art
video-language alignment models are not robust to semantically-plausible
contrastive changes in the video captions. Our work addresses this by
identifying a broad spectrum of contrast misalignments, such as replacing
entities, actions, and flipping event order, which alignment models should be
robust against. To this end, we introduce the VideoCon, a video-language
alignment dataset constructed by a large language model that generates
plausible contrast video captions and explanations for differences between
original and contrast video captions. Then, a generative video-language model
is finetuned with VideoCon to assess video-language entailment and generate
explanations. Our VideoCon-based alignment model significantly outperforms
current models. It exhibits a 12-point increase in AUC for the video-language
alignment task on human-generated contrast captions. Finally, our model sets
new state of the art zero-shot performance in temporally-extensive
video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video
question answering (ATP-Hard). Moreover, our model shows superior performance
on novel videos and human-crafted captions and explanations. Our code and data
are available at https://github.com/Hritikbansal/videocon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10116">
<title>Wildfire Smoke Detection with Cross Contrast Patch Embedding. (arXiv:2311.10116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10116</link>
<description rdf:parseType="Literal">&lt;p&gt;The Transformer-based deep networks have increasingly shown significant
advantages over CNNs. Some existing work has applied it in the field of
wildfire recognition or detection. However, we observed that the vanilla
Transformer is not friendly for extracting smoke features. Because low-level
information such as color, transparency and texture is very important for smoke
recognition, and transformer pays more attention to the semantic relevance
between middle- or high-level features, and is not sensitive to the subtle
changes of low-level features along the space. To solve this problem, we
propose the Cross Contrast Patch Embedding(CCPE) module based on the Swin
Transformer, which uses the multi-scales spatial frequency contrast information
in both vertical and horizontal directions to improve the discrimination of the
network on the underlying details. The fuzzy boundary of smoke makes the
positive and negative label assignment for instances in a dilemma, which is
another challenge for wildfires detection. To solve this problem, a Separable
Negative Sampling Mechanism(SNSM) is proposed. By using two different negative
instance sampling strategies on positive images and negative images
respectively, the problem of supervision signal confusion caused by label
diversity in the process of network training is alleviated. This paper also
releases the RealFire Test, the largest real wildfire test set so far, to
evaluate the proposed method and promote future research. It contains 50,535
images from 3,649 video clips. The proposed method has been extensively tested
and evaluated on RealFire Test dataset, and has a significant performance
improvement compared with the baseline detection models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akram_A/0/1/0/all/0/1&quot;&gt;Adeel Akram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1&quot;&gt;Zhilin Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qixing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10118">
<title>Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey. (arXiv:2311.10118v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10118</link>
<description rdf:parseType="Literal">&lt;p&gt;Since signet ring cells (SRCs) are associated with high peripheral metastasis
rate and dismal survival, they play an important role in determining surgical
approaches and prognosis, while they are easily missed by even experienced
pathologists. Although automatic diagnosis SRCs based on deep learning has
received increasing attention to assist pathologists in improving the
diagnostic efficiency and accuracy, the existing works have not been
systematically overviewed, which hindered the evaluation of the gap between
algorithms and clinical applications. In this paper, we provide a survey on SRC
analysis driven by deep learning from 2008 to August 2023. Specifically, the
biological characteristics of SRCs and the challenges of automatic
identification are systemically summarized. Then, the representative algorithms
are analyzed and compared via dividing them into classification, detection, and
segmentation. Finally, for comprehensive consideration to the performance of
existing methods and the requirements for clinical assistance, we discuss the
open issues and future trends of SRC analysis. The retrospect research will
help researchers in the related fields, particularly for who without medical
science background not only to clearly find the outline of SRC analysis, but
also gain the prospect of intelligent diagnosis, resulting in accelerating the
practice and application of intelligent algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junhao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Limei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_F/0/1/0/all/0/1&quot;&gt;Fei Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10121">
<title>Slide-SAM: Medical SAM Meets Sliding Window. (arXiv:2311.10121v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10121</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM) achieves remarkable results in 2D image
segmentation of natural images. However, the huge gap between medical images
and natural images prevents it directly applied to medical image segmentation
tasks. Especially in 3D medical image, SAM cannot learn the contextual
relationship between slices, which limites application in real scenarios. In
addition, recent research shows that applying 2D SAM to 3D images requires
prompting the entire volume, which is time and label comsuming. In order to
solve the above problems, we introduced Slide-SAM which extended SAM to 3D
medical images. Specifically, you only need to use a single slice prompt to
segement the entire volume, which greatly reduces the prompt workload for
professionals. Secondly, unlike traditional 3D medical image segmentation, we
are free from the influence of computing resources and can still use high
resolution (H$ \times $W = 1024$ \times $1024) for training in 3D images to
achieve optimal learning for small targets. This is to combine the entire 3D
volume is beyond the reach of training. Finally, we collected a large number of
3D images from large-scale 3D public and private datasets, and extended SAM to
3D medical image segmentation involving bounding box and point prompts.
Finally, we perform a comprehensive evaluation and analysis investigating the
performance of Slide-SAM in medical image segmentation of different modalities,
anatomy, and organs. We have verified Slide-SAM&apos;s segmentation capabilities on
multiple datasets, achieving the most advanced 3D segmentation performance
while maintaining the minimum prompt. Code will be open source soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Quan Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fenghe Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zikang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Heqin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S.Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10122">
<title>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. (arXiv:2311.10122v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10122</link>
<description rdf:parseType="Literal">&lt;p&gt;The Large Vision-Language Model (LVLM) has enhanced the performance of
various downstream tasks in visual-language understanding. Most existing
approaches encode images and videos into separate feature spaces, which are
then fed as inputs to large language models. However, due to the lack of
unified tokenization for images and videos, namely misalignment before
projection, it becomes challenging for a Large Language Model (LLM) to learn
multi-modal interactions from several poor projection layers. In this work, we
unify visual representation into the language feature space to advance the
foundational LLM towards a unified LVLM. As a result, we establish a simple but
robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images
and videos, mutually enhancing each other. Video-LLaVA achieves superior
performances on a broad range of 9 image benchmarks across 5 image
question-answering datasets and 4 image benchmark toolkits. Additionally, our
Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on
MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive
experiments demonstrate that Video-LLaVA mutually benefits images and videos
within a unified visual representation, outperforming models designed
specifically for images or videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10123">
<title>MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture. (arXiv:2311.10123v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models for 3D object synthesis have seen significant advancements
with the incorporation of prior knowledge distilled from 2D diffusion models.
Nevertheless, challenges persist in the form of multi-view geometric
inconsistencies and slow generation speeds within the existing 3D synthesis
frameworks. This can be attributed to two factors: firstly, the deficiency of
abundant geometric a priori knowledge in optimization, and secondly, the
entanglement issue between geometry and texture in conventional 3D generation
methods.In response, we introduce MetaDreammer, a two-stage optimization
approach that leverages rich 2D and 3D prior knowledge. In the first stage, our
emphasis is on optimizing the geometric representation to ensure multi-view
consistency and accuracy of 3D objects. In the second stage, we concentrate on
fine-tuning the geometry and optimizing the texture, thereby achieving a more
refined 3D object. Through leveraging 2D and 3D prior knowledge in two stages,
respectively, we effectively mitigate the interdependence between geometry and
texture. MetaDreamer establishes clear optimization objectives for each stage,
resulting in significant time savings in the 3D generation process. Ultimately,
MetaDreamer can generate high-quality 3D objects based on textual prompts
within 20 minutes, and to the best of our knowledge, it is the most efficient
text-to-3D generation method. Furthermore, we introduce image control into the
process, enhancing the controllability of 3D generation. Extensive empirical
evidence confirms that our method is not only highly efficient but also
achieves a quality level that is at the forefront of current state-of-the-art
3D generation techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lincong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Muyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Maoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoli Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10125">
<title>UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework. (arXiv:2311.10125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10125</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current landscape of artificial intelligence, foundation models serve
as the bedrock for advancements in both language and vision domains. OpenAI
GPT-4 has emerged as the pinnacle in large language models (LLMs), while the
computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models
such as Meta&apos;s SAM and DINO, and YOLOS. However, the financial and
computational burdens of training new models from scratch remain a significant
barrier to progress. In response to this challenge, we introduce
UnifiedVisionGPT, a novel framework designed to consolidate and automate the
integration of SOTA vision models, thereby facilitating the development of
vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key
features: (1) provides a versatile multimodal framework adaptable to a wide
range of applications, building upon the strengths of multimodal foundation
models; (2) seamlessly integrates various SOTA vision models to create a
comprehensive multimodal platform, capitalizing on the best components of each
model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in
the CV domain compared to the current trajectory of LLMs; and (4) introduces
automation in the selection of SOTA vision models, generating optimal results
based on diverse multimodal inputs such as text prompts and images. This paper
outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating
its potential to revolutionize the field of computer vision through enhanced
efficiency, versatility, generalization, and performance. Our implementation,
along with the unified multimodal framework and comprehensive dataset, is made
publicly available at https://github.com/LHBuilder/SA-Segment-Anything.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_C/0/1/0/all/0/1&quot;&gt;Chris Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Luhui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cindy Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Deshun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zaoshan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10126">
<title>I&amp;S-ViT: An Inclusive &amp; Stable Method for Pushing the Limit of Post-Training ViTs Quantization. (arXiv:2311.10126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10126</link>
<description rdf:parseType="Literal">&lt;p&gt;Albeit the scalable performance of vision transformers (ViTs), the dense
computational costs (training &amp;amp; inference) undermine their position in
industrial applications. Post-training quantization (PTQ), tuning ViTs with a
tiny dataset and running in a low-bit format, well addresses the cost issue but
unluckily bears more performance drops in lower-bit cases. In this paper, we
introduce I&amp;amp;S-ViT, a novel method that regulates the PTQ of ViTs in an
inclusive and stable fashion. I&amp;amp;S-ViT first identifies two issues in the PTQ of
ViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for
post-Softmax activations; (2) Rugged and magnified loss landscape in
coarse-grained quantization granularity for post-LayerNorm activations. Then,
I&amp;amp;S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2
quantizer (SULQ) that incorporates a shift mechanism followed by uniform
quantization to achieve both an inclusive domain representation and accurate
distribution approximation; (2) A three-stage smooth optimization strategy
(SOS) that amalgamates the strengths of channel-wise and layer-wise
quantization to enable stable learning. Comprehensive evaluations across
diverse vision tasks validate I&amp;amp;S-ViT&apos; superiority over existing PTQ of ViTs
methods, particularly in low-bit scenarios. For instance, I&amp;amp;S-ViT elevates the
performance of 3-bit ViT-B by an impressive 50.68%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yunshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiawei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingbao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mengzhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10162">
<title>K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise. (arXiv:2311.10162v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10162</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based MRI reconstruction models have achieved superior
performance these days. Most recently, diffusion models have shown remarkable
performance in image generation, in-painting, super-resolution, image editing
and more. As a generalized diffusion model, cold diffusion further broadens the
scope and considers models built around arbitrary image transformations such as
blurring, down-sampling, etc. In this paper, we propose a k-space cold
diffusion model that performs image degradation and restoration in k-space
without the need for Gaussian noise. We provide comparisons with multiple deep
learning-based MRI reconstruction models and perform tests on a well-known
large open-source MRI dataset. Our results show that this novel way of
performing degradation can generate high-quality reconstruction images for
accelerated MRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guoyao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farris_C/0/1/0/all/0/1&quot;&gt;Chad W. Farris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_S/0/1/0/all/0/1&quot;&gt;Stephan Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10177">
<title>Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts. (arXiv:2311.10177v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10177</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have demonstrated significant accuracy across various
domains, yet their vulnerability to subtle input alterations remains a
persistent challenge. Conventional methods like data augmentation, while
effective to some extent, fall short in addressing unforeseen corruptions,
limiting the adaptability of neural networks in real-world scenarios. In
response, this paper introduces a novel paradigm known as the Mixture of
Class-Specific Expert Architecture. The approach involves disentangling feature
learning for individual classes, offering a nuanced enhancement in scalability
and overall performance. By training dedicated network segments for each class
and subsequently aggregating their outputs, the proposed architecture aims to
mitigate vulnerabilities associated with common neural network structures. The
study underscores the importance of comprehensive evaluation methodologies,
advocating for the incorporation of benchmarks like the common corruptions
benchmark. This inclusion provides nuanced insights into the vulnerabilities of
neural networks, especially concerning their generalization capabilities and
robustness to unforeseen distortions. The research aligns with the broader
objective of advancing the development of highly robust learning systems
capable of nuanced reasoning across diverse and challenging real-world
scenarios. Through this contribution, the paper aims to foster a deeper
understanding of neural network limitations and proposes a practical approach
to enhance their resilience in the face of evolving and unpredictable
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1&quot;&gt;Shashank Kotyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1&quot;&gt;Danilo Vasconcellos Vargas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10207">
<title>Stella Nera: Achieving 161 TOp/s/W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication. (arXiv:2311.10207v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.10207</link>
<description rdf:parseType="Literal">&lt;p&gt;From classical HPC to deep learning, MatMul is at the heart of today&apos;s
computing. The recent Maddness method approximates MatMul without the need for
multiplication by using a hash-based version of product quantization (PQ)
indexing into a look-up table (LUT). Stella Nera is the first Maddness
accelerator and it achieves 15x higher area efficiency (GMAC/s/mm^2) and more
than 25x higher energy efficiency (TMAC/s/W) than direct MatMul accelerators
implemented in the same technology. The hash function is a decision tree, which
allows for an efficient hardware implementation as the multiply-accumulate
operations are replaced by decision tree passes and LUT lookups. The entire
Maddness MatMul can be broken down into parts that allow an effective
implementation with small computing units and memories, allowing it to reach
extreme efficiency while remaining generically applicable for MatMul tasks. In
a commercial 14nm technology and scaled to 3nm, we achieve an energy efficiency
of 161 TOp/s/W@0.55V with a Top-1 accuracy on CIFAR-10 of more than 92.5% using
ResNet9.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonleber_J/0/1/0/all/0/1&quot;&gt;Jannis Sch&amp;#xf6;nleber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavigelli_L/0/1/0/all/0/1&quot;&gt;Lukas Cavigelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andri_R/0/1/0/all/0/1&quot;&gt;Renzo Andri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perotti_M/0/1/0/all/0/1&quot;&gt;Matteo Perotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10224">
<title>CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images. (arXiv:2311.10224v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10224</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the lack of automated methods, to diagnose cerebrovascular disease,
time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually,
making it time-consuming. The commonly used encoder-decoder architectures for
cerebrovascular segmentation utilize redundant features, eventually leading to
the extraction of low-level features multiple times. Additionally,
convolutional neural networks (CNNs) suffer from performance degradation when
the batch size is small, and deeper networks experience the vanishing gradient
problem. Methods: In this paper, we attempt to solve these limitations and
propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet,
for precise extraction of brain vessel images. We proposed a sequence of
preprocessing techniques followed by deeply supervised UNet to improve the
accuracy of segmentation of the brain vessels leading to a stroke. To combine
the low and high semantics, we applied the attention mechanism. This mechanism
focuses on relevant associations and neglects irrelevant anatomical
information. Furthermore, the inclusion of deep supervision incorporates
different levels of features that prove to be beneficial for network
convergence. Results: We demonstrate the efficiency of the proposed method by
cross-validating with an unlabeled dataset, which was further labeled by us. We
believe that the novelty of this algorithm lies in its ability to perform well
on both labeled and unlabeled data with image processing-based enhancement. The
results indicate that our method performed better than the existing
state-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method
will help in accurate segmentation of cerebrovascular structure leading to
stroke
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abbas_S/0/1/0/all/0/1&quot;&gt;Syed Farhan Abbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duc_N/0/1/0/all/0/1&quot;&gt;Nguyen Thanh Duc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yoonguu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyungwon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Boreom Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10234">
<title>The Analysis and Extraction of Structure from Organizational Charts. (arXiv:2311.10234v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10234</link>
<description rdf:parseType="Literal">&lt;p&gt;Organizational charts, also known as org charts, are critical representations
of an organization&apos;s structure and the hierarchical relationships between its
components and positions. However, manually extracting information from org
charts can be error-prone and time-consuming. To solve this, we present an
automated and end-to-end approach that uses computer vision, deep learning, and
natural language processing techniques. Additionally, we propose a metric to
evaluate the completeness and hierarchical accuracy of the extracted
information. This approach has the potential to improve organizational
restructuring and resource utilization by providing a clear and concise
representation of the organizational structure. Our study lays a foundation for
further research on the topic of hierarchical chart analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manali_N/0/1/0/all/0/1&quot;&gt;Nikhil Manali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David Doermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desai_M/0/1/0/all/0/1&quot;&gt;Mahesh Desai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10245">
<title>Segment Anything in Defect Detection. (arXiv:2311.10245v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10245</link>
<description rdf:parseType="Literal">&lt;p&gt;Defect detection plays a crucial role in infrared non-destructive testing
systems, offering non-contact, safe, and efficient inspection capabilities.
However, challenges such as low resolution, high noise, and uneven heating in
infrared thermal images hinder comprehensive and accurate defect detection. In
this study, we propose DefectSAM, a novel approach for segmenting defects on
highly noisy thermal images based on the widely adopted model, Segment Anything
(SAM)\cite{kirillov2023segany}. Harnessing the power of a meticulously curated
dataset generated through labor-intensive lab experiments and valuable prompts
from experienced experts, DefectSAM surpasses existing state-of-the-art
segmentation algorithms and achieves significant improvements in defect
detection rates. Notably, DefectSAM excels in detecting weaker and smaller
defects on complex and irregular surfaces, reducing the occurrence of missed
detections and providing more accurate defect size estimations. Experimental
studies conducted on various materials have validated the effectiveness of our
solutions in defect detection, which hold significant potential to expedite the
evolution of defect detection tools, enabling enhanced inspection capabilities
and accuracy in defect identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bozhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1&quot;&gt;Bin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tongle Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10251">
<title>UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets. (arXiv:2311.10251v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10251</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models for medical images can help physicians diagnose and
manage diseases. However, due to the fact that medical image annotation
requires a great deal of manpower and expertise, as well as the fact that
clinical departments perform image annotation based on task orientation, there
is the problem of having fewer medical image annotation data with more
unlabeled data and having many datasets that annotate only a single organ. In
this paper, we present UniMOS, the first universal framework for achieving the
utilization of fully and partially labeled images as well as unlabeled images.
Specifically, we construct a Multi-Organ Segmentation (MOS) module over
fully/partially labeled data as the basenet and designed a new target adaptive
loss. Furthermore, we incorporate a semi-supervised training module that
combines consistent regularization and pseudolabeling techniques on unlabeled
data, which significantly improves the segmentation of unlabeled data.
Experiments show that the framework exhibits excellent performance in several
medical image segmentation tasks compared to other advanced methods, and also
significantly improves data utilization and reduces annotation cost. Code and
models are available at: https://github.com/lw8807001/UniMOS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Can Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_S/0/1/0/all/0/1&quot;&gt;Sheng Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Junyi Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pang_S/0/1/0/all/0/1&quot;&gt;Shuchao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Orgun_M/0/1/0/all/0/1&quot;&gt;Mehmet A. Orgun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10261">
<title>Vision meets mmWave Radar: 3D Object Perception Benchmark for Autonomous Driving. (arXiv:2311.10261v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10261</link>
<description rdf:parseType="Literal">&lt;p&gt;Sensor fusion is crucial for an accurate and robust perception system on
autonomous vehicles. Most existing datasets and perception solutions focus on
fusing cameras and LiDAR. However, the collaboration between camera and radar
is significantly under-exploited. The incorporation of rich semantic
information from the camera, and reliable 3D information from the radar can
potentially achieve an efficient, cheap, and portable solution for 3D object
perception tasks. It can also be robust to different lighting or all-weather
driving scenarios due to the capability of mmWave radars. In this paper, we
introduce the CRUW3D dataset, including 66K synchronized and well-calibrated
camera, radar, and LiDAR frames in various driving scenarios. Unlike other
large-scale autonomous driving datasets, our radar data is in the format of
radio frequency (RF) tensors that contain not only 3D location information but
also spatio-temporal semantic information. This kind of radar format can enable
machine learning models to generate more reliable object perception results
after interacting and fusing the information or features between the camera and
radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jen-Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jui-Te Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuan_S/0/1/0/all/0/1&quot;&gt;Sheng-Yao Kuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiqian Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_C/0/1/0/all/0/1&quot;&gt;Chiming Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1&quot;&gt;Shengyu Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gaoang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_G/0/1/0/all/0/1&quot;&gt;Guanbin Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10269">
<title>Interpretable pap smear cell representation for cervical cancer screening. (arXiv:2311.10269v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10269</link>
<description rdf:parseType="Literal">&lt;p&gt;Screening is critical for prevention and early detection of cervical cancer
but it is time-consuming and laborious. Supervised deep convolutional neural
networks have been developed to automate pap smear screening and the results
are promising. However, the interest in using only normal samples to train deep
neural networks has increased owing to class imbalance problems and
high-labeling costs that are both prevalent in healthcare. In this study, we
introduce a method to learn explainable deep cervical cell representations for
pap smear cytology images based on one class classification using variational
autoencoders. Findings demonstrate that a score can be calculated for cell
abnormality without training models with abnormal samples and localize
abnormality to interpret our results with a novel metric based on absolute
difference in cross entropy in agglomerative clustering. The best model that
discriminates squamous cell carcinoma (SCC) from normals gives 0.908 +- 0.003
area under operating characteristic curve (AUC) and one that discriminates
high-grade epithelial lesion (HSIL) 0.920 +- 0.002 AUC. Compared to other
clustering methods, our method enhances the V-measure and yields higher
homogeneity scores, which more effectively isolate different abnormality
regions, aiding in the interpretation of our results. Evaluation using in-house
and additional open dataset show that our model can discriminate abnormality
without the need of additional training of deep models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ando_Y/0/1/0/all/0/1&quot;&gt;Yu Ando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+and_N/0/1/0/all/0/1&quot;&gt;Nora Jee-Young Park and&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_G/0/1/0/all/0/1&quot;&gt;Gun Oh Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1&quot;&gt;Seokhwan Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Junghwan Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hyungsoo Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10278">
<title>Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint. (arXiv:2311.10278v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10278</link>
<description rdf:parseType="Literal">&lt;p&gt;Human fingerprints serve as one unique and powerful characteristic for each
person, from which policemen can recognize the identity. Similar to humans,
many natural bodies and intrinsic mechanical qualities can also be uniquely
identified from surface characteristics. To measure the elasto-plastic
properties of one material, one formally sharp indenter is pushed into the
measured body under constant force and retracted, leaving a unique residual
imprint of the minute size from several micrometers to nanometers. However, one
great challenge is how to map the optical image of this residual imprint into
the real wanted mechanical properties, i.e., the tensile force curve. In this
paper, we propose a novel method to use multi-fidelity neural networks (MFNN)
to solve this inverse problem. We first actively train the NN model via pure
simulation data, and then bridge the sim-to-real gap via transfer learning. The
most innovative part is that we use NN to dig out the unknown physics and also
implant the known physics into the transfer learning framework, thus highly
improving the model stability and decreasing the data requirement. This work
serves as one great example of applying machine learning into the real
experimental research, especially under the constraints of data limitation and
fidelity variance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongchao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10281">
<title>SSASS: Semi-Supervised Approach for Stenosis Segmentation. (arXiv:2311.10281v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10281</link>
<description rdf:parseType="Literal">&lt;p&gt;Coronary artery stenosis is a critical health risk, and its precise
identification in Coronary Angiography (CAG) can significantly aid medical
practitioners in accurately evaluating the severity of a patient&apos;s condition.
The complexity of coronary artery structures combined with the inherent noise
in X-ray images poses a considerable challenge to this task. To tackle these
obstacles, we introduce a semi-supervised approach for cardiovascular stenosis
segmentation. Our strategy begins with data augmentation, specifically tailored
to replicate the structural characteristics of coronary arteries. We then apply
a pseudo-label-based semi-supervised learning technique that leverages the data
generated through our augmentation process. Impressively, our approach
demonstrated an exceptional performance in the Automatic Region-based Coronary
Artery Disease diagnostics using x-ray angiography imagEs (ARCADE) Stenosis
Detection Algorithm challenge by utilizing a single model instead of relying on
an ensemble of multiple models. This success emphasizes our method&apos;s capability
and efficiency in providing an automated solution for accurately assessing
stenosis severity from medical imaging data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;In Kyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Junsup Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong-Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_J/0/1/0/all/0/1&quot;&gt;Jonghoe Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyun-Woo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10293">
<title>Hierarchical Pruning of Deep Ensembles with Focal Diversity. (arXiv:2311.10293v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10293</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural network ensembles combine the wisdom of multiple deep neural
networks to improve the generalizability and robustness over individual
networks. It has gained increasing popularity to study deep ensemble techniques
in the deep learning community. Some mission-critical applications utilize a
large number of deep neural networks to form deep ensembles to achieve desired
accuracy and resilience, which introduces high time and space costs for
ensemble execution. However, it still remains a critical challenge whether a
small subset of the entire deep ensemble can achieve the same or better
generalizability and how to effectively identify these small deep ensembles for
improving the space and time efficiency of ensemble execution. This paper
presents a novel deep ensemble pruning approach, which can efficiently identify
smaller deep ensembles and provide higher ensemble accuracy than the entire
deep ensemble of a large number of member networks. Our hierarchical ensemble
pruning approach (HQ) leverages three novel ensemble pruning techniques. First,
we show that the focal diversity metrics can accurately capture the
complementary capacity of the member networks of an ensemble, which can guide
ensemble pruning. Second, we design a focal diversity based hierarchical
pruning approach, which will iteratively find high quality deep ensembles with
low cost and high accuracy. Third, we develop a focal diversity consensus
method to integrate multiple focal diversity metrics to refine ensemble pruning
results, where smaller deep ensembles can be effectively identified to offer
high accuracy, high robustness and high efficiency. Evaluated using popular
benchmark datasets, we demonstrate that the proposed hierarchical ensemble
pruning approach can effectively identify high quality deep ensembles with
better generalizability while being more time and space efficient in ensemble
decision making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_K/0/1/0/all/0/1&quot;&gt;Ka-Ho Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenqi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Ling Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10296">
<title>BiHRNet: A Binary high-resolution network for Human Pose Estimation. (arXiv:2311.10296v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10296</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Pose Estimation (HPE) plays a crucial role in computer vision
applications. However, it is difficult to deploy state-of-the-art models on
resouce-limited devices due to the high computational costs of the networks. In
this work, a binary human pose estimator named BiHRNet(Binary HRNet) is
proposed, whose weights and activations are expressed as $\pm$1. BiHRNet
retains the keypoint extraction ability of HRNet, while using fewer computing
resources by adapting binary neural network (BNN). In order to reduce the
accuracy drop caused by network binarization, two categories of techniques are
proposed in this work. For optimizing the training process for binary pose
estimator, we propose a new loss function combining KL divergence loss with
AWing loss, which makes the binary network obtain more comprehensive output
distribution from its real-valued counterpart to reduce information loss caused
by binarization. For designing more binarization-friendly structures, we
propose a new information reconstruction bottleneck called IR Bottleneck to
retain more information in the initial stage of the network. In addition, we
also propose a multi-scale basic block called MS-Block for information
retention. Our work has less computation cost with few precision drop.
Experimental results demonstrate that BiHRNet achieves a PCKh of 87.9 on the
MPII dataset, which outperforms all binary pose estimation networks. On the
challenging of COCO dataset, the proposed method enables the binary neural
network to achieve 70.8 mAP, which is better than most tested lightweight
full-precision networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xueyao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1&quot;&gt;Yonghao Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10305">
<title>Semi-supervised ViT knowledge distillation network with style transfer normalization for colorectal liver metastases survival prediction. (arXiv:2311.10305v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10305</link>
<description rdf:parseType="Literal">&lt;p&gt;Colorectal liver metastases (CLM) significantly impact colon cancer patients,
influencing survival based on systemic chemotherapy response. Traditional
methods like tumor grading scores (e.g., tumor regression grade - TRG) for
prognosis suffer from subjectivity, time constraints, and expertise demands.
Current machine learning approaches often focus on radiological data, yet the
relevance of histological images for survival predictions, capturing intricate
tumor microenvironment characteristics, is gaining recognition. To address
these limitations, we propose an end-to-end approach for automated prognosis
prediction using histology slides stained with H&amp;amp;E and HPS. We first employ a
Generative Adversarial Network (GAN) for slide normalization to reduce staining
variations and improve the overall quality of the images that are used as input
to our prediction pipeline. We propose a semi-supervised model to perform
tissue classification from sparse annotations, producing feature maps. We use
an attention-based approach that weighs the importance of different slide
regions in producing the final classification results. We exploit the extracted
features for the metastatic nodules and surrounding tissue to train a prognosis
model. In parallel, we train a vision Transformer (ViT) in a knowledge
distillation framework to replicate and enhance the performance of the
prognosis prediction. In our evaluation on a clinical dataset of 258 patients,
our approach demonstrates superior performance with c-indexes of 0.804 (0.014)
for OS and 0.733 (0.014) for TTR. Achieving 86.9% to 90.3% accuracy in
predicting TRG dichotomization and 78.5% to 82.1% accuracy for the 3-class TRG
classification task, our approach outperforms comparative methods. Our proposed
pipeline can provide automated prognosis for pathologists and oncologists, and
can greatly promote precision medicine progress in managing CLM patients.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elforaici_M/0/1/0/all/0/1&quot;&gt;Mohamed El Amine Elforaici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Montagnon_E/0/1/0/all/0/1&quot;&gt;Emmanuel Montagnon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Romero_F/0/1/0/all/0/1&quot;&gt;Francisco Perdigon Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_W/0/1/0/all/0/1&quot;&gt;William Trung Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Azzi_F/0/1/0/all/0/1&quot;&gt;Feryel Azzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trudel_D/0/1/0/all/0/1&quot;&gt;Dominique Trudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Bich Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turcotte_S/0/1/0/all/0/1&quot;&gt;Simon Turcotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_A/0/1/0/all/0/1&quot;&gt;An Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kadoury_S/0/1/0/all/0/1&quot;&gt;Samuel Kadoury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10306">
<title>MPSeg : Multi-Phase strategy for coronary artery Segmentation. (arXiv:2311.10306v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10306</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate segmentation of coronary arteries is a pivotal process in assessing
cardiovascular diseases. However, the intricate structure of the cardiovascular
system presents significant challenges for automatic segmentation, especially
when utilizing methodologies like the SYNTAX Score, which relies extensively on
detailed structural information for precise risk stratification. To address
these difficulties and cater to this need, we present MPSeg, an innovative
multi-phase strategy designed for coronary artery segmentation. Our approach
specifically accommodates these structural complexities and adheres to the
principles of the SYNTAX Score. Initially, our method segregates vessels into
two categories based on their unique morphological characteristics: Left
Coronary Artery (LCA) and Right Coronary Artery (RCA). Specialized ensemble
models are then deployed for each category to execute the challenging
segmentation task. Due to LCA&apos;s higher complexity over RCA, a refinement model
is utilized to scrutinize and correct initial class predictions on segmented
areas. Notably, our approach demonstrated exceptional effectiveness when
evaluated in the Automatic Region-based Coronary Artery Disease diagnostics
using x-ray angiography imagEs (ARCADE) Segmentation Detection Algorithm
challenge at MICCAI 2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ku_J/0/1/0/all/0/1&quot;&gt;Jonghoe Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong-Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Junsup Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;In Kyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyun-Woo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10318">
<title>Nonparametric Teaching for Multiple Learners. (arXiv:2311.10318v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10318</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of teaching multiple learners simultaneously in the
nonparametric iterative teaching setting, where the teacher iteratively
provides examples to the learner for accelerating the acquisition of a target
concept. This problem is motivated by the gap between current single-learner
teaching setting and the real-world scenario of human instruction where a
teacher typically imparts knowledge to multiple students. Under the new problem
formulation, we introduce a novel framework -- Multi-learner Nonparametric
Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with
each learner focusing on learning a scalar-valued target model. To achieve
this, we frame the problem as teaching a vector-valued target model and extend
the target model space from a scalar-valued reproducing kernel Hilbert space
used in single-learner scenarios to a vector-valued space. Furthermore, we
demonstrate that MINT offers significant teaching speed-up over repeated
single-learner teaching, particularly when the multiple learners can
communicate with each other. Lastly, we conduct extensive experiments to
validate the practicality and efficiency of MINT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1&quot;&gt;James Kwok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10319">
<title>Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification. (arXiv:2311.10319v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10319</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in clinical treatment and research are limited by supervised
learning techniques that rely on large amounts of annotated data, an expensive
task requiring many hours of clinical specialists&apos; time. In this paper, we
propose using self-supervised and semi-supervised learning. These techniques
perform an auxiliary task that is label-free, scaling up machine-supervision is
easier compared with fully-supervised techniques. This paper proposes S4MI
(Self-Supervision and Semi-Supervision for Medical Imaging), our pipeline to
leverage advances in self and semi-supervision learning. We benchmark them on
three medical imaging datasets to analyze their efficacy for classification and
segmentation. This advancement in self-supervised learning with 10% annotation
performed better than 100% annotation for the classification of most datasets.
The semi-supervised approach yielded favorable outcomes for segmentation,
outperforming the fully-supervised approach by using 50% fewer labels in all
three datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pranav Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chukkapalli_R/0/1/0/all/0/1&quot;&gt;Raviteja Chukkapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1&quot;&gt;Shravan Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Luoyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jinqian Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smuda_C/0/1/0/all/0/1&quot;&gt;Craig Smuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1&quot;&gt;Jacopo Cirrone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10320">
<title>Learning transformer-based heterogeneously salient graph representation for multimodal fusion classification of hyperspectral image and LiDAR data. (arXiv:2311.10320v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10320</link>
<description rdf:parseType="Literal">&lt;p&gt;Data collected by different modalities can provide a wealth of complementary
information, such as hyperspectral image (HSI) to offer rich spectral-spatial
properties, synthetic aperture radar (SAR) to provide structural information
about the Earth&apos;s surface, and light detection and ranging (LiDAR) to cover
altitude information about ground elevation. Therefore, a natural idea is to
combine multimodal images for refined and accurate land-cover interpretation.
Although many efforts have been attempted to achieve multi-source remote
sensing image classification, there are still three issues as follows: 1)
indiscriminate feature representation without sufficiently considering modal
heterogeneity, 2) abundant features and complex computations associated with
modeling long-range dependencies, and 3) overfitting phenomenon caused by
sparsely labeled samples. To overcome the above barriers, a transformer-based
heterogeneously salient graph representation (THSGR) approach is proposed in
this paper. First, a multimodal heterogeneous graph encoder is presented to
encode distinctively non-Euclidean structural features from heterogeneous data.
Then, a self-attention-free multi-convolutional modulator is designed for
effective and efficient long-term dependency modeling. Finally, a mean forward
is put forward in order to avoid overfitting. Based on the above structures,
the proposed model is able to break through modal gaps to obtain differentiated
graph representation with competitive time cost, even for a small fraction of
training samples. Experiments and analyses on three benchmark datasets with
various state-of-the-art (SOTA) methods show the performance of the proposed
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liangpei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10328">
<title>TransONet: Automatic Segmentation of Vasculature in Computed Tomographic Angiograms Using Deep Learning. (arXiv:2311.10328v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10328</link>
<description rdf:parseType="Literal">&lt;p&gt;Pathological alterations in the human vascular system underlie many chronic
diseases, such as atherosclerosis and aneurysms. However, manually analyzing
diagnostic images of the vascular system, such as computed tomographic
angiograms (CTAs) is a time-consuming and tedious process. To address this
issue, we propose a deep learning model to segment the vascular system in CTA
images of patients undergoing surgery for peripheral arterial disease (PAD).
Our study focused on accurately segmenting the vascular system (1) from the
descending thoracic aorta to the iliac bifurcation and (2) from the descending
thoracic aorta to the knees in CTA images using deep learning techniques. Our
approach achieved average Dice accuracies of 93.5% and 80.64% in test dataset
for (1) and (2), respectively, highlighting its high accuracy and potential
clinical utility. These findings demonstrate the use of deep learning
techniques as a valuable tool for medical professionals to analyze the health
of the vascular system efficiently and accurately. Please visit the GitHub page
for this paper at https://github.com/pip-alireza/TransOnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rajeoni_A/0/1/0/all/0/1&quot;&gt;Alireza Bagheri Rajeoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pederson_B/0/1/0/all/0/1&quot;&gt;Breanna Pederson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Firooz_A/0/1/0/all/0/1&quot;&gt;Ali Firooz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abdollahi_H/0/1/0/all/0/1&quot;&gt;Hamed Abdollahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Smith_A/0/1/0/all/0/1&quot;&gt;Andrew K. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clair_D/0/1/0/all/0/1&quot;&gt;Daniel G. Clair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lessner_S/0/1/0/all/0/1&quot;&gt;Susan M. Lessner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Valafar_H/0/1/0/all/0/1&quot;&gt;Homayoun Valafar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10329">
<title>High-fidelity Person-centric Subject-to-Image Synthesis. (arXiv:2311.10329v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10329</link>
<description rdf:parseType="Literal">&lt;p&gt;Current subject-driven image generation methods encounter significant
challenges in person-centric image generation. The reason is that they learn
the semantic scene and person generation by fine-tuning a common pre-trained
diffusion, which involves an irreconcilable training imbalance. Precisely, to
generate realistic persons, they need to sufficiently tune the pre-trained
model, which inevitably causes the model to forget the rich semantic scene
prior and makes scene generation over-fit to the training data. Moreover, even
with sufficient fine-tuning, these methods can still not generate high-fidelity
persons since joint learning of the scene and person generation also lead to
quality compromise. In this paper, we propose Face-diffuser, an effective
collaborative generation pipeline to eliminate the above training imbalance and
quality compromise. Specifically, we first develop two specialized pre-trained
diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented
Diffusion Model (SDM), for scene and person generation, respectively. The
sampling process is divided into three sequential stages, i.e., semantic scene
construction, subject-scene fusion, and subject enhancement. The first and last
stages are performed by TDM and SDM respectively. The subject-scene fusion
stage, that is the collaboration achieved through a novel and highly effective
mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on
our key observation that there exists a robust link between classifier-free
guidance responses and the saliency of generated images. In each time step, SNF
leverages the unique strengths of each model and allows for the spatial
blending of predicted noises from both models automatically in a saliency-aware
manner. Extensive experiments confirm the impressive effectiveness and
robustness of the Face-diffuser.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10331">
<title>Leveraging Multimodal Fusion for Enhanced Diagnosis of Multiple Retinal Diseases in Ultra-wide OCTA. (arXiv:2311.10331v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10331</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultra-wide optical coherence tomography angiography (UW-OCTA) is an emerging
imaging technique that offers significant advantages over traditional OCTA by
providing an exceptionally wide scanning range of up to 24 x 20 $mm^{2}$,
covering both the anterior and posterior regions of the retina. However, the
currently accessible UW-OCTA datasets suffer from limited comprehensive
hierarchical information and corresponding disease annotations. To address this
limitation, we have curated the pioneering M3OCTA dataset, which is the first
multimodal (i.e., multilayer), multi-disease, and widest field-of-view UW-OCTA
dataset. Furthermore, the effective utilization of multi-layer ultra-wide
ocular vasculature information from UW-OCTA remains underdeveloped. To tackle
this challenge, we propose the first cross-modal fusion framework that
leverages multi-modal information for diagnosing multiple diseases. Through
extensive experiments conducted on our openly available M3OCTA dataset, we
demonstrate the effectiveness and superior performance of our method, both in
fixed and varying modalities settings. The construction of the M3OCTA dataset,
the first multimodal OCTA dataset encompassing multiple diseases, aims to
advance research in the ophthalmic image analysis community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_P/0/1/0/all/0/1&quot;&gt;Peilun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_G/0/1/0/all/0/1&quot;&gt;Guitao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuangle Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10336">
<title>Cooperative Perception with Learning-Based V2V communications. (arXiv:2311.10336v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.10336</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperative perception has been widely used in autonomous driving to
alleviate the inherent limitation of single automated vehicle perception. To
enable cooperation, vehicle-to-vehicle (V2V) communication plays an
indispensable role. This work analyzes the performance of cooperative
perception accounting for communications channel impairments. Different fusion
methods and channel impairments are evaluated. A new late fusion scheme is
proposed to leverage the robustness of intermediate features. In order to
compress the data size incurred by cooperation, a convolution neural
network-based autoencoder is adopted. Numerical results demonstrate that
intermediate fusion is more robust to channel impairments than early fusion and
late fusion, when the SNR is greater than 0 dB. Also, the proposed fusion
scheme outperforms the conventional late fusion using detection outputs, and
autoencoder provides a good compromise between detection accuracy and bandwidth
usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenguang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Payton_R/0/1/0/all/0/1&quot;&gt;Ryan Payton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Riley_M/0/1/0/all/0/1&quot;&gt;Michael Riley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuang-Hua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10339">
<title>A2XP: Towards Private Domain Generalization. (arXiv:2311.10339v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10339</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) have become pivotal in various fields, especially
in computer vision, outperforming previous methodologies. A critical challenge
in their deployment is the bias inherent in data across different domains, such
as image style, and environmental conditions, leading to domain gaps. This
necessitates techniques for learning general representations from biased
training data, known as domain generalization. This paper presents Attend to
eXpert Prompts (A2XP), a novel approach for domain generalization that
preserves the privacy and integrity of the network architecture. A2XP consists
of two phases: Expert Adaptation and Domain Generalization. In the first phase,
prompts for each source domain are optimized to guide the model towards the
optimal direction. In the second phase, two embedder networks are trained to
effectively amalgamate these expert prompts, aiming for an optimal output. Our
extensive experiments demonstrate that A2XP achieves state-of-the-art results
over existing non-private domain generalization methods. The experimental
results validate that the proposed approach not only tackles the domain
generalization challenge in DNNs but also offers a privacy-preserving,
efficient solution to the broader field of computer vision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Geunhyeok Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hyoseok Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10343">
<title>Enhancing Student Engagement in Online Learning through Facial Expression Analysis and Complex Emotion Recognition using Deep Learning. (arXiv:2311.10343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10343</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the COVID-19 pandemic, traditional physical classrooms have
transitioned to online environments, necessitating effective strategies to
ensure sustained student engagement. A significant challenge in online teaching
is the absence of real-time feedback from teachers on students learning
progress. This paper introduces a novel approach employing deep learning
techniques based on facial expressions to assess students engagement levels
during online learning sessions. Human emotions cannot be adequately conveyed
by a student using only the basic emotions, including anger, disgust, fear,
joy, sadness, surprise, and neutrality. To address this challenge, proposed a
generation of four complex emotions such as confusion, satisfaction,
disappointment, and frustration by combining the basic emotions. These complex
emotions are often experienced simultaneously by students during the learning
session. To depict these emotions dynamically,utilized a continuous stream of
image frames instead of discrete images. The proposed work utilized a
Convolutional Neural Network (CNN) model to categorize the fundamental
emotional states of learners accurately. The proposed CNN model demonstrates
strong performance, achieving a 95% accuracy in precise categorization of
learner emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1&quot;&gt;Rekha R Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_T/0/1/0/all/0/1&quot;&gt;Tina Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1&quot;&gt;Pavithra K&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10349">
<title>Pseudo Label-Guided Data Fusion and Output Consistency for Semi-Supervised Medical Image Segmentation. (arXiv:2311.10349v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10349</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning algorithms based on Convolutional Neural Networks have
become the benchmark for medical image segmentation tasks, but their
effectiveness heavily relies on a large amount of labeled data. However,
annotating medical image datasets is a laborious and time-consuming process.
Inspired by semi-supervised algorithms that use both labeled and unlabeled data
for training, we propose the PLGDF framework, which builds upon the mean
teacher network for segmenting medical images with less annotation. We propose
a novel pseudo-label utilization scheme, which combines labeled and unlabeled
data to augment the dataset effectively. Additionally, we enforce the
consistency between different scales in the decoder module of the segmentation
network and propose a loss function suitable for evaluating the consistency.
Moreover, we incorporate a sharpening operation on the predicted results,
further enhancing the accuracy of the segmentation.
&lt;/p&gt;
&lt;p&gt;Extensive experiments on three publicly available datasets demonstrate that
the PLGDF framework can largely improve performance by incorporating the
unlabeled data. Meanwhile, our framework yields superior performance compared
to six state-of-the-art semi-supervised learning methods. The codes of this
study are available at https://github.com/ortonwang/PLGDF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Junlin Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bizhe Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Min Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qinquan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tong_T/0/1/0/all/0/1&quot;&gt;Tong Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10356">
<title>Garment Recovery with Shape and Deformation Priors. (arXiv:2311.10356v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10356</link>
<description rdf:parseType="Literal">&lt;p&gt;While modeling people wearing tight-fitting clothing has made great strides
in recent years, loose-fitting clothing remains a challenge. We propose a
method that delivers realistic garment models from real-world images,
regardless of garment shape or deformation. To this end, we introduce a fitting
approach that utilizes shape and deformation priors learned from synthetic data
to accurately capture garment shapes and deformations, including large ones.
Not only does our approach recover the garment geometry accurately, it also
yields models that can be directly used by downstream applications such as
animation and simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ren Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumery_C/0/1/0/all/0/1&quot;&gt;Corentin Dumery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Guillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1&quot;&gt;Pascal Fua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10361">
<title>Video-based Sequential Bayesian Homography Estimation for Soccer Field Registration. (arXiv:2311.10361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10361</link>
<description rdf:parseType="Literal">&lt;p&gt;A novel Bayesian framework is proposed, which explicitly relates the
homography of one video frame to the next through an affine transformation
while explicitly modelling keypoint uncertainty. The literature has previously
used differential homography between subsequent frames, but not in a Bayesian
setting. In cases where Bayesian methods have been applied, camera motion is
not adequately modelled, and keypoints are treated as deterministic. The
proposed method, Bayesian Homography Inference from Tracked Keypoints (BHITK),
employs a two-stage Kalman filter and significantly improves existing methods.
Existing keypoint detection methods may be easily augmented with BHITK. It
enables less sophisticated and less computationally expensive methods to
outperform the state-of-the-art approaches in most homography evaluation
metrics. Furthermore, the homography annotations of the WorldCup and
TS-WorldCup datasets have been refined using a custom homography annotation
tool released for public use. The refined datasets are consolidated and
released as the consolidated and refined WorldCup (CARWC) dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claasen_P/0/1/0/all/0/1&quot;&gt;Paul J. Claasen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villiers_J/0/1/0/all/0/1&quot;&gt;J.P. de Villiers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10365">
<title>Dates Fruit Disease Recognition using Machine Learning. (arXiv:2311.10365v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10365</link>
<description rdf:parseType="Literal">&lt;p&gt;Many countries such as Saudi Arabia, Morocco and Tunisia are among the top
exporters and consumers of palm date fruits. Date fruit production plays a
major role in the economies of the date fruit exporting countries. Date fruits
are susceptible to disease just like any fruit and early detection and
intervention can end up saving the produce. However, with the vast farming
lands, it is nearly impossible for farmers to observe date trees on a frequent
basis for early disease detection. In addition, even with human observation the
process is prone to human error and increases the date fruit cost. With the
recent advances in computer vision, machine learning, drone technology, and
other technologies; an integrated solution can be proposed for the automatic
detection of date fruit disease. In this paper, a hybrid features based method
with the standard classifiers is proposed based on the extraction of L*a*b
color features, statistical features, and Discrete Wavelet Transform (DWT)
texture features for the early detection and classification of date fruit
disease. A dataset was developed for this work consisting of 871 images divided
into the following classes; Healthy date, Initial stage of disease,
Malnourished date, and Parasite infected. The extracted features were input to
common classifiers such as the Random Forest (RF), Multilayer Perceptron (MLP),
Na\&quot;ive Bayes (NB), and Fuzzy Decision Trees (FDT). The highest average
accuracy was achieved when combining the L*a*b, Statistical, and DWT Features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahim_G/0/1/0/all/0/1&quot;&gt;Ghassen Ben Brahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alghazo_J/0/1/0/all/0/1&quot;&gt;Jaafar Alghazo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_G/0/1/0/all/0/1&quot;&gt;Ghazanfar Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alnujaidi_K/0/1/0/all/0/1&quot;&gt;Khalid Alnujaidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10366">
<title>Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models. (arXiv:2311.10366v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10366</link>
<description rdf:parseType="Literal">&lt;p&gt;As video analysis using deep learning models becomes more widespread, the
vulnerability of such models to adversarial attacks is becoming a pressing
concern. In particular, Universal Adversarial Perturbation (UAP) poses a
significant threat, as a single perturbation can mislead deep learning models
on entire datasets. We propose a novel video UAP using image data and image
model. This enables us to take advantage of the rich image data and image
model-based studies available for video applications. However, there is a
challenge that image models are limited in their ability to analyze the
temporal aspects of videos, which is crucial for a successful video attack. To
address this challenge, we introduce the Breaking Temporal Consistency (BTC)
method, which is the first attempt to incorporate temporal information into
video attacks using image models. We aim to generate adversarial videos that
have opposite patterns to the original. Specifically, BTC-UAP minimizes the
feature similarity between neighboring frames in videos. Our approach is simple
but effective at attacking unseen video models. Additionally, it is applicable
to videos of varying lengths and invariant to temporal shifts. Our approach
surpasses existing methods in terms of effectiveness on various datasets,
including ImageNet, UCF-101, and Kinetics-400.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hee-Seon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_M/0/1/0/all/0/1&quot;&gt;Minji Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minbeom Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1&quot;&gt;Myung-Joon Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Changick Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10380">
<title>MSE-Nets: Multi-annotated Semi-supervised Ensemble Networks for Improving Segmentation of Medical Image with Ambiguous Boundaries. (arXiv:2311.10380v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10380</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation annotations exhibit variations among experts due
to the ambiguous boundaries of segmented objects and backgrounds in medical
images. Although using multiple annotations for each image in the
fully-supervised has been extensively studied for training deep models,
obtaining a large amount of multi-annotated data is challenging due to the
substantial time and manpower costs required for segmentation annotations,
resulting in most images lacking any annotations. To address this, we propose
Multi-annotated Semi-supervised Ensemble Networks (MSE-Nets) for learning
segmentation from limited multi-annotated and abundant unannotated data.
Specifically, we introduce the Network Pairwise Consistency Enhancement (NPCE)
module and Multi-Network Pseudo Supervised (MNPS) module to enhance MSE-Nets
for the segmentation task by considering two major factors: (1) to optimize the
utilization of all accessible multi-annotated data, the NPCE separates
(dis)agreement annotations of multi-annotated data at the pixel level and
handles agreement and disagreement annotations in different ways, (2) to
mitigate the introduction of imprecise pseudo-labels, the MNPS extends the
training data by leveraging consistent pseudo-labels from unannotated data.
Finally, we improve confidence calibration by averaging the predictions of base
networks. Experiments on the ISIC dataset show that we reduced the demand for
multi-annotated data by 97.75\% and narrowed the gap with the best
fully-supervised baseline to just a Jaccard index of 4\%. Furthermore, compared
to other semi-supervised methods that rely only on a single annotation or a
combined fusion approach, the comprehensive experimental results on ISIC and
RIGA datasets demonstrate the superior performance of our proposed method in
medical image segmentation with ambiguous boundaries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1&quot;&gt;Tengjin Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhidong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixiu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_P/0/1/0/all/0/1&quot;&gt;Pengfei Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhiming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10382">
<title>Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking. (arXiv:2311.10382v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10382</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Object Tracking (MOT) remains a vital component of intelligent video
analysis, which aims to locate targets and maintain a consistent identity for
each target throughout a video sequence. Existing works usually learn a
discriminative feature representation, such as motion and appearance, to
associate the detections across frames, which are easily affected by mutual
occlusion and background clutter in practice. In this paper, we propose a
simple yet effective two-stage feature learning paradigm to jointly learn
single-shot and multi-shot features for different targets, so as to achieve
robust data association in the tracking process. For the detections without
being associated, we design a novel single-shot feature learning module to
extract discriminative features of each detection, which can efficiently
associate targets between adjacent frames. For the tracklets being lost several
frames, we design a novel multi-shot feature learning module to extract
discriminative features of each tracklet, which can accurately refind these
lost targets after a long period. Once equipped with a simple data association
logic, the resulting VisualTracker can perform robust MOT based on the
single-shot and multi-shot feature representations. Extensive experimental
results demonstrate that our method has achieved significant improvements on
MOT17 and MOT20 datasets while reaching state-of-the-art performance on
DanceTrack dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sanping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zheng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Le Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10389">
<title>Two-Factor Authentication Approach Based on Behavior Patterns for Defeating Puppet Attacks. (arXiv:2311.10389v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10389</link>
<description rdf:parseType="Literal">&lt;p&gt;Fingerprint traits are widely recognized for their unique qualities and
security benefits. Despite their extensive use, fingerprint features can be
vulnerable to puppet attacks, where attackers manipulate a reluctant but
genuine user into completing the authentication process. Defending against such
attacks is challenging due to the coexistence of a legitimate identity and an
illegitimate intent. In this paper, we propose PUPGUARD, a solution designed to
guard against puppet attacks. This method is based on user behavioral patterns,
specifically, the user needs to press the capture device twice successively
with different fingers during the authentication process. PUPGUARD leverages
both the image features of fingerprints and the timing characteristics of the
pressing intervals to establish two-factor authentication. More specifically,
after extracting image features and timing characteristics, and performing
feature selection on the image features, PUPGUARD fuses these two features into
a one-dimensional feature vector, and feeds it into a one-class classifier to
obtain the classification result. This two-factor authentication method
emphasizes dynamic behavioral patterns during the authentication process,
thereby enhancing security against puppet attacks. To assess PUPGUARD&apos;s
effectiveness, we conducted experiments on datasets collected from 31 subjects,
including image features and timing characteristics. Our experimental results
demonstrate that PUPGUARD achieves an impressive accuracy rate of 97.87% and a
remarkably low false positive rate (FPR) of 1.89%. Furthermore, we conducted
comparative experiments to validate the superiority of combining image features
and timing characteristics within PUPGUARD for enhancing resistance against
puppet attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zhiming Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haobo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faccio_D/0/1/0/all/0/1&quot;&gt;Daniele Faccio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10399">
<title>Optimized Deep Learning Models for AUV Seabed Image Analysis. (arXiv:2311.10399v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10399</link>
<description rdf:parseType="Literal">&lt;p&gt;Using autonomous underwater vehicles, or AUVs, has completely changed how we
gather data from the ocean floor. AUV innovation has advanced significantly,
especially in the analysis of images, due to the increasing need for accurate
and efficient seafloor mapping. This blog post provides a detailed summary and
comparison of the most current advancements in AUV seafloor image processing.
We will go into the realm of undersea technology, covering everything through
computer and algorithmic advancements to advances in sensors and cameras. After
reading this page through to the end, you will have a solid understanding of
the most up-to-date techniques and tools for using AUVs to process seabed
photos and how they could further our comprehension of the ocean floor
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_R/0/1/0/all/0/1&quot;&gt;Rajesh Sharma R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sungheetha_A/0/1/0/all/0/1&quot;&gt;Akey Sungheetha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_C/0/1/0/all/0/1&quot;&gt;Chinnaiyan R&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10408">
<title>Deep Learning based CNN Model for Classification and Detection of Individuals Wearing Face Mask. (arXiv:2311.10408v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10408</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the global COVID-19 pandemic, there has been a critical demand
for protective measures, with face masks emerging as a primary safeguard. The
approach involves a two-fold strategy: first, recognizing the presence of a
face by detecting faces, and second, identifying masks on those faces. This
project utilizes deep learning to create a model that can detect face masks in
real-time streaming video as well as images. Face detection, a facet of object
detection, finds applications in diverse fields such as security, biometrics,
and law enforcement. Various detector systems worldwide have been developed and
implemented, with convolutional neural networks chosen for their superior
performance accuracy and speed in object detection. Experimental results attest
to the model&apos;s excellent accuracy on test data. The primary focus of this
research is to enhance security, particularly in sensitive areas. The research
paper proposes a rapid image pre-processing method with masks centred on faces.
Employing feature extraction and Convolutional Neural Network, the system
classifies and detects individuals wearing masks. The research unfolds in three
stages: image pre-processing, image cropping, and image classification,
collectively contributing to the identification of masked faces. Continuous
surveillance through webcams or CCTV cameras ensures constant monitoring,
triggering a security alert if a person is detected without a mask.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinnaiyan_R/0/1/0/all/0/1&quot;&gt;R. Chinnaiyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+M_I/0/1/0/all/0/1&quot;&gt;Iyyappan M&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+A_A/0/1/0/all/0/1&quot;&gt;Al Raiyan Shariff A&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sai_K/0/1/0/all/0/1&quot;&gt;Kondaveeti Sai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+M_M/0/1/0/all/0/1&quot;&gt;Mallikarjunaiah B M&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharath_P/0/1/0/all/0/1&quot;&gt;P Bharath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10430">
<title>Deep Residual CNN for Multi-Class Chest Infection Diagnosis. (arXiv:2311.10430v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10430</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of deep learning has significantly propelled the capabilities of
automated medical image diagnosis, providing valuable tools and resources in
the realm of healthcare and medical diagnostics. This research delves into the
development and evaluation of a Deep Residual Convolutional Neural Network
(CNN) for the multi-class diagnosis of chest infections, utilizing chest X-ray
images. The implemented model, trained and validated on a dataset amalgamated
from diverse sources, demonstrated a robust overall accuracy of 93%. However,
nuanced disparities in performance across different classes, particularly
Fibrosis, underscored the complexity and challenges inherent in automated
medical image diagnosis. The insights derived pave the way for future research,
focusing on enhancing the model&apos;s proficiency in classifying conditions that
present more subtle and nuanced visual features in the images, as well as
optimizing and refining the model architecture and training process. This paper
provides a comprehensive exploration into the development, implementation, and
evaluation of the model, offering insights and directions for future research
and development in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kwon_R/0/1/0/all/0/1&quot;&gt;Ryan Donghan Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lim_D/0/1/0/all/0/1&quot;&gt;Dohyun Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yoonha Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung Won Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10437">
<title>DUA-DA: Distillation-based Unbiased Alignment for Domain Adaptive Object Detection. (arXiv:2311.10437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10437</link>
<description rdf:parseType="Literal">&lt;p&gt;Though feature-alignment based Domain Adaptive Object Detection (DAOD) have
achieved remarkable progress, they ignore the source bias issue, i.e. the
aligned features are more favorable towards the source domain, leading to a
sub-optimal adaptation. Furthermore, the presence of domain shift between the
source and target domains exacerbates the problem of inconsistent
classification and localization in general detection pipelines. To overcome
these challenges, we propose a novel Distillation-based Unbiased Alignment
(DUA) framework for DAOD, which can distill the source features towards a more
balanced position via a pre-trained teacher model during the training process,
alleviating the problem of source bias effectively. In addition, we design a
Target-Relevant Object Localization Network (TROLN), which can mine
target-related knowledge to produce two classification-free metrics (IoU and
centerness). Accordingly, we implement a Domain-aware Consistency Enhancing
(DCE) strategy that utilizes these two metrics to further refine classification
confidences, achieving a harmonization between classification and localization
in cross-domain scenarios. Extensive experiments have been conducted to
manifest the effectiveness of this method, which consistently improves the
strong baseline by large margins, outperforming existing alignment-based works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yongchao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yingjie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10448">
<title>DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal. (arXiv:2311.10448v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10448</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models trained on sensitive or private data can
inadvertently memorize and leak that information. Machine unlearning seeks to
retroactively remove such details from model weights to protect privacy. We
contribute a lightweight unlearning algorithm that leverages the Fisher
Information Matrix (FIM) for selective forgetting. Prior work in this area
requires full retraining or large matrix inversions, which are computationally
expensive. Our key insight is that the diagonal elements of the FIM, which
measure the sensitivity of log-likelihood to changes in weights, contain
sufficient information for effective forgetting. Specifically, we compute the
FIM diagonal over two subsets -- the data to retain and forget -- for all
trainable weights. This diagonal representation approximates the complete FIM
while dramatically reducing computation. We then use it to selectively update
weights to maximize forgetting of the sensitive subset while minimizing impact
on the retained subset. Experiments show that our algorithm can successfully
forget any randomly selected subsets of training data across neural network
architectures. By leveraging the FIM diagonal, our approach provides an
interpretable, lightweight, and efficient solution for machine unlearning with
practical privacy benefits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaeli Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghalyan_N/0/1/0/all/0/1&quot;&gt;Najah Ghalyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gourgoulias_K/0/1/0/all/0/1&quot;&gt;Kostis Gourgoulias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buford_J/0/1/0/all/0/1&quot;&gt;John Buford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1&quot;&gt;Sean Moran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10463">
<title>Correlation-Distance Graph Learning for Treatment Response Prediction from rs-fMRI. (arXiv:2311.10463v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10463</link>
<description rdf:parseType="Literal">&lt;p&gt;Resting-state fMRI (rs-fMRI) functional connectivity (FC) analysis provides
valuable insights into the relationships between different brain regions and
their potential implications for neurological or psychiatric disorders.
However, specific design efforts to predict treatment response from rs-fMRI
remain limited due to difficulties in understanding the current brain state and
the underlying mechanisms driving the observed patterns, which limited the
clinical application of rs-fMRI. To overcome that, we propose a graph learning
framework that captures comprehensive features by integrating both correlation
and distance-based similarity measures under a contrastive loss. This approach
results in a more expressive framework that captures brain dynamic features at
different scales and enables more accurate prediction of treatment response.
Our experiments on the chronic pain and depersonalization disorder datasets
demonstrate that our proposed method outperforms current methods in different
scenarios. To the best of our knowledge, we are the first to explore the
integration of distance-based and correlation-based neural similarity into
graph learning for treatment response prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sisi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P. H. Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haozheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_N/0/1/0/all/0/1&quot;&gt;Nan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingkang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_H/0/1/0/all/0/1&quot;&gt;Hongxiao Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10472">
<title>End-to-end autoencoding architecture for the simultaneous generation of medical images and corresponding segmentation masks. (arXiv:2311.10472v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10472</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the increasing use of deep learning in medical image segmentation,
acquiring sufficient training data remains a challenge in the medical field. In
response, data augmentation techniques have been proposed; however, the
generation of diverse and realistic medical images and their corresponding
masks remains a difficult task, especially when working with insufficient
training sets. To address these limitations, we present an end-to-end
architecture based on the Hamiltonian Variational Autoencoder (HVAE). This
approach yields an improved posterior distribution approximation compared to
traditional Variational Autoencoders (VAE), resulting in higher image
generation quality. Our method outperforms generative adversarial architectures
under data-scarce conditions, showcasing enhancements in image quality and
precise tumor mask synthesis. We conduct experiments on two publicly available
datasets, MICCAI&apos;s Brain Tumor Segmentation Challenge (BRATS), and Head and
Neck Tumor Segmentation Challenge (HECKTOR), demonstrating the effectiveness of
our method on different medical imaging modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kebaili_A/0/1/0/all/0/1&quot;&gt;Aghiles Kebaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lapuyade_Lahorgue_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Lapuyade-Lahorgue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vera_P/0/1/0/all/0/1&quot;&gt;Pierre Vera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Su Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10476">
<title>FRCSyn Challenge at WACV 2024:Face Recognition Challenge in the Era of Synthetic Data. (arXiv:2311.10476v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10476</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the widespread adoption of face recognition technology around the
world, and its remarkable performance on current benchmarks, there are still
several challenges that must be covered in more detail. This paper offers an
overview of the Face Recognition Challenge in the Era of Synthetic Data
(FRCSyn) organized at WACV 2024. This is the first international challenge
aiming to explore the use of synthetic data in face recognition to address
existing limitations in the technology. Specifically, the FRCSyn Challenge
targets concerns related to data privacy issues, demographic biases,
generalization to unseen scenarios, and performance limitations in challenging
scenarios, including significant age disparities between enrollment and
testing, pose variations, and occlusions. The results achieved in the FRCSyn
Challenge, together with the proposed benchmark, contribute significantly to
the application of synthetic data to improve face recognition technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melzi_P/0/1/0/all/0/1&quot;&gt;Pietro Melzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1&quot;&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeAndres_Tame_I/0/1/0/all/0/1&quot;&gt;Ivan DeAndres-Tame&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weisong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zheyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao-Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinlin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1&quot;&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothari_M/0/1/0/all/0/1&quot;&gt;Mahak Kothari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zama_M/0/1/0/all/0/1&quot;&gt;Md Haider Zama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_D/0/1/0/all/0/1&quot;&gt;Debayan Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biesseck_B/0/1/0/all/0/1&quot;&gt;Bernardo Biesseck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_P/0/1/0/all/0/1&quot;&gt;Pedro Vidal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granada_R/0/1/0/all/0/1&quot;&gt;Roger Granada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fickel_G/0/1/0/all/0/1&quot;&gt;Guilherme Fickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuhr_G/0/1/0/all/0/1&quot;&gt;Gustavo F&amp;#xfc;hr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1&quot;&gt;David Menotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unnervik_A/0/1/0/all/0/1&quot;&gt;Alexander Unnervik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1&quot;&gt;Anjith George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecabert_C/0/1/0/all/0/1&quot;&gt;Christophe Ecabert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahreza_H/0/1/0/all/0/1&quot;&gt;Hatef Otroshi Shahreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_P/0/1/0/all/0/1&quot;&gt;Parsa Rahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Marcel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarridis_I/0/1/0/all/0/1&quot;&gt;Ioannis Sarridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1&quot;&gt;Christos Koutlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baltsou_G/0/1/0/all/0/1&quot;&gt;Georgia Baltsou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1&quot;&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diou_C/0/1/0/all/0/1&quot;&gt;Christos Diou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Domenico_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xf2; Di Domenico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1&quot;&gt;Guido Borghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_L/0/1/0/all/0/1&quot;&gt;Lorenzo Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mas_Candela_E/0/1/0/all/0/1&quot;&gt;Enrique Mas-Candela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Perez_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngela S&amp;#xe1;nchez-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzori_A/0/1/0/all/0/1&quot;&gt;Andrea Atzori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1&quot;&gt;Fadi Boutros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1&quot;&gt;Gianni Fenu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1&quot;&gt;Mirko Marras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10492">
<title>A Relay System for Semantic Image Transmission based on Shared Feature Extraction and Hyperprior Entropy Compression. (arXiv:2311.10492v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10492</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, the need for high-quality image reconstruction and restoration is
more and more urgent. However, most image transmission systems may suffer from
image quality degradation or transmission interruption in the face of
interference such as channel noise and link fading. To solve this problem, a
relay communication network for semantic image transmission based on shared
feature extraction and hyperprior entropy compression (HEC) is proposed, where
the shared feature extraction technology based on Pearson correlation is
proposed to eliminate partial shared feature of extracted semantic latent
feature. In addition, the HEC technology is used to resist the effect of
channel noise and link fading and carried out respectively at the source node
and the relay node. Experimental results demonstrate that compared with other
recent research methods, the proposed system has lower transmission overhead
and higher semantic image transmission performance. Particularly, under the
same conditions, the multi-scale structural similarity (MS-SSIM) of this system
is superior to the comparison method by approximately 0.2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1&quot;&gt;Wannian An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Haotai Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiaodong/0/1/0/all/0/1&quot;&gt;Xiaodong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10513">
<title>A Framework of Landsat-8 Band Selection based on UMDA for Deforestation Detection. (arXiv:2311.10513v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10513</link>
<description rdf:parseType="Literal">&lt;p&gt;The conservation of tropical forests is a current subject of social and
ecological relevance due to their crucial role in the global ecosystem.
Unfortunately, millions of hectares are deforested and degraded each year.
Therefore, government or private initiatives are needed for monitoring tropical
forests. In this sense, this work proposes a novel framework, which uses of
distribution estimation algorithm (UMDA) to select spectral bands from
Landsat-8 that yield a better representation of deforestation areas to guide a
semantic segmentation architecture called DeepLabv3+. In performed experiments,
it was possible to find several compositions that reach balanced accuracy
superior to 90% in segment classification tasks. Furthermore, the best
composition (651) found by UMDA algorithm fed the DeepLabv3+ architecture and
surpassed in efficiency and effectiveness all compositions compared in this
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_E/0/1/0/all/0/1&quot;&gt;Eduardo B. Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedro_P/0/1/0/all/0/1&quot;&gt;Paulo R. C. Pedro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazenda_A/0/1/0/all/0/1&quot;&gt;Alvaro Fazenda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faria_F/0/1/0/all/0/1&quot;&gt;Fabio A. Faria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10517">
<title>Mind the map! Accounting for existing map information when estimating online HDMaps from sensor data. (arXiv:2311.10517v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10517</link>
<description rdf:parseType="Literal">&lt;p&gt;Online High Definition Map (HDMap) estimation from sensors offers a low-cost
alternative to manually acquired HDMaps. As such, it promises to lighten costs
for already HDMap-reliant Autonomous Driving systems, and potentially even
spread their use to new systems. In this paper, we propose to improve online
HDMap estimation by accounting for already existing maps. We identify 3
reasonable types of useful existing maps (minimalist, noisy, and outdated). We
also introduce MapEX, a novel online HDMap estimation framework that accounts
for existing maps. MapEX achieves this by encoding map elements into query
tokens and by refining the matching algorithm used to train classic query based
map estimation models. We demonstrate that MapEX brings significant
improvements on the nuScenes dataset. For instance, MapEX - given noisy maps -
improves by 38% over the MapTRv2 detector it is based on and by 16% over the
current SOTA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Li Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lingrand_D/0/1/0/all/0/1&quot;&gt;Diane Lingrand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Precioso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10522">
<title>Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel&apos;s generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10523">
<title>Removing Adverse Volumetric Effects From Trained Neural Radiance Fields. (arXiv:2311.10523v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10523</link>
<description rdf:parseType="Literal">&lt;p&gt;While the use of neural radiance fields (NeRFs) in different challenging
settings has been explored, only very recently have there been any
contributions that focus on the use of NeRF in foggy environments. We argue
that the traditional NeRF models are able to replicate scenes filled with fog
and propose a method to remove the fog when synthesizing novel views. By
calculating the global contrast of a scene, we can estimate a density threshold
that, when applied, removes all visible fog. This makes it possible to use NeRF
as a way of rendering clear views of objects of interest located in fog-filled
environments. Additionally, to benchmark performance on such scenes, we
introduce a new dataset that expands some of the original synthetic NeRF scenes
through the addition of fog and natural environments. The code, dataset, and
video results can be found on our project page: https://vegardskui.com/fognerf/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teigen_A/0/1/0/all/0/1&quot;&gt;Andreas L. Teigen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1&quot;&gt;Mauhing Yip&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamran_V/0/1/0/all/0/1&quot;&gt;Victor P. Hamran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skui_V/0/1/0/all/0/1&quot;&gt;Vegard Skui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahl_A/0/1/0/all/0/1&quot;&gt;Annette Stahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mester_R/0/1/0/all/0/1&quot;&gt;Rudolf Mester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10529">
<title>Segment Anything Model with Uncertainty Rectification for Auto-Prompting Medical Image Segmentation. (arXiv:2311.10529v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10529</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of the Segment Anything Model (SAM) has marked a significant
advancement in prompt-driven image segmentation. However, SAM&apos;s application to
medical image segmentation requires manual prompting of target structures to
obtain acceptable performance, which is still labor-intensive. Despite attempts
of auto-prompting to turn SAM into a fully automatic manner, it still exhibits
subpar performance and lacks of reliability in the field of medical imaging. In
this paper, we propose UR-SAM, an uncertainty rectified SAM framework to
enhance the robustness and reliability for auto-prompting medical image
segmentation. Our method incorporates a prompt augmentation module to estimate
the distribution of predictions and generate uncertainty maps, and an
uncertainty-based rectification module to further enhance the performance of
SAM. Extensive experiments on two public 3D medical datasets covering the
segmentation of 35 organs demonstrate that without supplementary training or
fine-tuning, our method further improves the segmentation performance with up
to 10.7 % and 13.8 % in dice similarity coefficient, demonstrating efficiency
and broad capabilities for medical image segmentation without manual prompting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shiyao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10543">
<title>Joint covariance property under geometric image transformations for spatio-temporal receptive fields according to generalized Gaussian model for receptive fields. (arXiv:2311.10543v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10543</link>
<description rdf:parseType="Literal">&lt;p&gt;The influence of natural image transformations on receptive field responses
is crucial for modelling visual operations in computer vision and biological
vision. In this regard, covariance properties with respect to geometric image
transformations in the earliest layers of the visual hierarchy are essential
for expressing robust image operations and for formulating invariant visual
operations at higher levels. This paper defines and proves a joint covariance
property under compositions of spatial scaling transformations, spatial affine
transformations, Galilean transformations and temporal scaling transformations,
which makes it possible to characterize how different types of image
transformations interact with each other. Specifically, the derived relations
show the receptive field parameters need to be transformed, in order to match
the output from spatio-temporal receptive fields with the underlying
spatio-temporal image transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1&quot;&gt;Tony Lindeberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10549">
<title>Archtree: on-the-fly tree-structured exploration for latency-aware pruning of deep neural networks. (arXiv:2311.10549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10549</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have become ubiquitous in addressing a number of
problems, particularly in computer vision. However, DNN inference is
computationally intensive, which can be prohibitive e.g. when considering edge
devices. To solve this problem, a popular solution is DNN pruning, and more so
structured pruning, where coherent computational blocks (e.g. channels for
convolutional networks) are removed: as an exhaustive search of the space of
pruned sub-models is intractable in practice, channels are typically removed
iteratively based on an importance estimation heuristic. Recently, promising
latency-aware pruning methods were proposed, where channels are removed until
the network reaches a target budget of wall-clock latency pre-emptively
estimated on specific hardware. In this paper, we present Archtree, a novel
method for latency-driven structured pruning of DNNs. Archtree explores
multiple candidate pruned sub-models in parallel in a tree-like fashion,
allowing for a better exploration of the search space. Furthermore, it involves
on-the-fly latency estimation on the target hardware, accounting for closer
latencies as compared to the specified budget. Empirical results on several DNN
architectures and target hardware show that Archtree better preserves the
original model accuracy while better fitting the latency budget as compared to
existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reboul_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Ouazan Reboul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1&quot;&gt;Edouard Yvinec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1&quot;&gt;Arnaud Dapogny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1&quot;&gt;Kevin Bailly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10568">
<title>Phase Guided Light Field for Spatial-Depth High Resolution 3D Imaging. (arXiv:2311.10568v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.10568</link>
<description rdf:parseType="Literal">&lt;p&gt;On 3D imaging, light field cameras typically are of single shot, and however,
they heavily suffer from low spatial resolution and depth accuracy. In this
paper, by employing an optical projector to project a group of single
high-frequency phase-shifted sinusoid patterns, we propose a phase guided light
field algorithm to significantly improve both the spatial and depth resolutions
for off-the-shelf light field cameras. First, for correcting the axial
aberrations caused by the main lens of our light field camera, we propose a
deformed cone model to calibrate our structured light field system. Second,
over wrapped phases computed from patterned images, we propose a stereo
matching algorithm, i.e. phase guided sum of absolute difference, to robustly
obtain the correspondence for each pair of neighbored two lenslets. Finally, by
introducing a virtual camera according to the basic geometrical optics of light
field imaging, we propose a reorganization strategy to reconstruct 3D point
clouds with spatial-depth high resolution. Experimental results show that,
compared with the state-of-the-art active light field methods, the proposed
reconstructs 3D point clouds with a spatial resolution of 1280$\times$720 with
factors 10$\times$ increased, while maintaining the same high depth resolution
and needing merely a single group of high-frequency patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Geyou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Ce Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10572">
<title>SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning. (arXiv:2311.10572v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10572</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) methods effectively leverage unlabeled data to
improve model generalization. However, SSL models often underperform in
open-set scenarios, where unlabeled data contain outliers from novel categories
that do not appear in the labeled set. In this paper, we study the challenging
and realistic open-set SSL setting, where the goal is to both correctly
classify inliers and to detect outliers. Intuitively, the inlier classifier
should be trained on inlier data only. However, we find that inlier
classification performance can be largely improved by incorporating
high-confidence pseudo-labeled data, regardless of whether they are inliers or
outliers. Also, we propose to utilize non-linear transformations to separate
the features used for inlier classification and outlier detection in the
multi-task learning framework, preventing adverse effects between them.
Additionally, we introduce pseudo-negative mining, which further boosts outlier
detection performance. The three ingredients lead to what we call Simple but
Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves
both inlier classification and outlier detection performance, outperforming
existing methods by a large margin. Our code will be released at
https://github.com/YUE-FAN/SSB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yue Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kukleva_A/0/1/0/all/0/1&quot;&gt;Anna Kukleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Dengxin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10582">
<title>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications. (arXiv:2311.10582v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.10582</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion trajectory prediction is a very important functionality for
human-robot collaboration, specifically in accompanying, guiding, or
approaching tasks, but also in social robotics, self-driving vehicles, or
security systems. In this paper, a novel trajectory prediction model, Social
Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a
Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate
different plausible people trajectories reducing collisions in a scene.
Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to
emphasize the destination learning. We show that our method is more accurate in
making predictions in UCY or BIWI datasets than most of the current
state-of-the-art models and also reduces collisions in comparison to other
approaches. Through real-life experiments, we demonstrate that the model can be
used in real-time without GPU&apos;s to perform good quality predictions with a low
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gil_O/0/1/0/all/0/1&quot;&gt;Oscar Gil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1&quot;&gt;Alberto Sanfeliu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10591">
<title>FOCAL: A Cost-Aware Video Dataset for Active Learning. (arXiv:2311.10591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10591</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the FOCAL (Ford-OLIVES Collaboration on Active
Learning) dataset which enables the study of the impact of annotation-cost
within a video active learning setting. Annotation-cost refers to the time it
takes an annotator to label and quality-assure a given video sequence. A
practical motivation for active learning research is to minimize
annotation-cost by selectively labeling informative samples that will maximize
performance within a given budget constraint. However, previous work in video
active learning lacks real-time annotation labels for accurately assessing cost
minimization and instead operates under the assumption that annotation-cost
scales linearly with the amount of data to annotate. This assumption does not
take into account a variety of real-world confounding factors that contribute
to a nonlinear cost such as the effect of an assistive labeling tool and the
variety of interactions within a scene such as occluded objects, weather, and
motion of objects. FOCAL addresses this discrepancy by providing real
annotation-cost labels for 126 video sequences across 69 unique city scenes
with a variety of weather, lighting, and seasonal conditions. We also introduce
a set of conformal active learning algorithms that take advantage of the
sequential structure of video data in order to achieve a better trade-off
between annotation-cost and performance while also reducing floating point
operations (FLOPS) overhead by at least 77.67%. We show how these approaches
better reflect how annotations on videos are done in practice through a
sequence selection framework. We further demonstrate the advantage of these
approaches by introducing two performance-cost metrics and show that the best
conformal active learning method is cheaper than the best traditional active
learning method by 113 hours.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokilepersaud_K/0/1/0/all/0/1&quot;&gt;Kiran Kokilepersaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logan_Y/0/1/0/all/0/1&quot;&gt;Yash-Yee Logan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benkert_R/0/1/0/all/0/1&quot;&gt;Ryan Benkert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1&quot;&gt;Mohit Prabhushankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1&quot;&gt;Ghassan AlRegib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1&quot;&gt;Enrique Corona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kunjan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1&quot;&gt;Mostafa Parchami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10592">
<title>D\&apos;etection d&apos;objets c\&apos;elestes dans des images astronomiques par IA explicable. (arXiv:2311.10592v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10592</link>
<description rdf:parseType="Literal">&lt;p&gt;Amateur and professional astronomers can easily capture a large number of
deep sky images with recent smart telescopes. However, afterwards verification
is still required to check whether the celestial objects targeted are actually
visible in the images produced. Depending on the magnitude of the targets, the
observation conditions and the time during which the data is captured, it is
possible that only stars are present in the images. In this study, we propose
an approach based on explainable Artificial Intelligence to automatically
detect the presence and position of captured objects. -- --
&lt;/p&gt;
&lt;p&gt;Gr\^ace \`a l&apos;apport des t\&apos;elescopes automatis\&apos;es grand public, les
astronomes amateurs et professionnels peuvent capturer facilement une grande
quantit\&apos;e d&apos;images du ciel profond (comme par exemple les galaxies,
n\&apos;ebuleuses, ou amas globulaires). N\&apos;eanmoins, une v\&apos;erification reste
n\&apos;ecessaire \`a post\&apos;eriori pour v\&apos;erifier si les objets c\&apos;elestes vis\&apos;es
sont effectivement visibles dans les images produites: cela d\&apos;epend notamment
de la magnitude des cibles, des conditions d&apos;observation mais aussi de la
dur\&apos;ee pendant laquelle les donn\&apos;ees sont captur\&apos;ees. Dans cette \&apos;etude,
nous proposons une approche bas\&apos;ee sur l&apos;IA explicable pour d\&apos;etecter
automatiquement la pr\&apos;esence et la position des objets captur\&apos;es.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parisot_O/0/1/0/all/0/1&quot;&gt;Olivier Parisot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaziri_M/0/1/0/all/0/1&quot;&gt;Mahmoud Jaziri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10601">
<title>Multimodal Indoor Localization Using Crowdsourced Radio Maps. (arXiv:2311.10601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10601</link>
<description rdf:parseType="Literal">&lt;p&gt;Indoor Positioning Systems (IPS) traditionally rely on odometry and building
infrastructures like WiFi, often supplemented by building floor plans for
increased accuracy. However, the limitation of floor plans in terms of
availability and timeliness of updates challenges their wide applicability. In
contrast, the proliferation of smartphones and WiFi-enabled robots has made
crowdsourced radio maps - databases pairing locations with their corresponding
Received Signal Strengths (RSS) - increasingly accessible. These radio maps not
only provide WiFi fingerprint-location pairs but encode movement regularities
akin to the constraints imposed by floor plans. This work investigates the
possibility of leveraging these radio maps as a substitute for floor plans in
multimodal IPS. We introduce a new framework to address the challenges of radio
map inaccuracies and sparse coverage. Our proposed system integrates an
uncertainty-aware neural network model for WiFi localization and a bespoken
Bayesian fusion technique for optimal fusion. Extensive evaluations on multiple
real-world sites indicate a significant performance enhancement, with results
showing ~ 25% improvement over the best baseline
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1&quot;&gt;Zhaoguang Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xiangyu Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1&quot;&gt;Qiyue Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peize Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zampella_F/0/1/0/all/0/1&quot;&gt;Francisco Zampella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsehly_F/0/1/0/all/0/1&quot;&gt;Firas Alsehly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10605">
<title>CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification. (arXiv:2311.10605v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10605</link>
<description rdf:parseType="Literal">&lt;p&gt;Person re-identification (re-ID) is a challenging task that aims to learn
discriminative features for person retrieval. In person re-ID, Jaccard distance
is a widely used distance metric, especially in re-ranking and clustering
scenarios. However, we discover that camera variation has a significant
negative impact on the reliability of Jaccard distance. In particular, Jaccard
distance calculates the distance based on the overlap of relevant neighbors.
Due to camera variation, intra-camera samples dominate the relevant neighbors,
which reduces the reliability of the neighbors by introducing intra-camera
negative samples and excluding inter-camera positive samples. To overcome this
problem, we propose a novel camera-aware Jaccard (CA-Jaccard) distance that
leverages camera information to enhance the reliability of Jaccard distance.
Specifically, we introduce camera-aware k-reciprocal nearest neighbors (CKRNNs)
to find k-reciprocal nearest neighbors on the intra-camera and inter-camera
ranking lists, which improves the reliability of relevant neighbors and
guarantees the contribution of inter-camera samples in the overlap. Moreover,
we propose a camera-aware local query expansion (CLQE) to exploit camera
variation as a strong constraint to mine reliable samples in relevant neighbors
and assign these samples higher weights in overlap to further improve the
reliability. Our CA-Jaccard distance is simple yet effective and can serve as a
general distance metric for person re-ID methods with high reliability and low
computational cost. Extensive experiments demonstrate the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zheyi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixuan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10617">
<title>Astronomical Images Quality Assessment with Automated Machine Learning. (arXiv:2311.10617v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2311.10617</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronically Assisted Astronomy consists in capturing deep sky images with
a digital camera coupled to a telescope to display views of celestial objects
that would have been invisible through direct observation. This practice
generates a large quantity of data, which may then be enhanced with dedicated
image editing software after observation sessions. In this study, we show how
Image Quality Assessment can be useful for automatically rating astronomical
images, and we also develop a dedicated model by using Automated Machine
Learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Parisot_O/0/1/0/all/0/1&quot;&gt;Olivier Parisot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bruneau_P/0/1/0/all/0/1&quot;&gt;Pierrick Bruneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hitzelberger_P/0/1/0/all/0/1&quot;&gt;Patrik Hitzelberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10648">
<title>Self-trained Panoptic Segmentation. (arXiv:2311.10648v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10648</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoptic segmentation is an important computer vision task which combines
semantic and instance segmentation. It plays a crucial role in domains of
medical image analysis, self-driving vehicles, and robotics by providing a
comprehensive understanding of visual environments. Traditionally, deep
learning panoptic segmentation models have relied on dense and accurately
annotated training data, which is expensive and time consuming to obtain.
Recent advancements in self-supervised learning approaches have shown great
potential in leveraging synthetic and unlabelled data to generate pseudo-labels
using self-training to improve the performance of instance and semantic
segmentation models. The three available methods for self-supervised panoptic
segmentation use proposal-based transformer architectures which are
computationally expensive, complicated and engineered for specific tasks. The
aim of this work is to develop a framework to perform embedding-based
self-supervised panoptic segmentation using self-training in a
synthetic-to-real domain adaptation problem setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Shourya Verma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10651">
<title>3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual Transformer Learning. (arXiv:2311.10651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10651</link>
<description rdf:parseType="Literal">&lt;p&gt;Analysis of the 3D Texture is indispensable for various tasks, such as
retrieval, segmentation, classification, and inspection of sculptures, knitted
fabrics, and biological tissues. A 3D texture is a locally repeated surface
variation independent of the surface&apos;s overall shape and can be determined
using the local neighborhood and its characteristics. Existing techniques
typically employ computer vision techniques that analyze a 3D mesh globally,
derive features, and then utilize the obtained features for retrieval or
classification. Several traditional and learning-based methods exist in the
literature, however, only a few are on 3D texture, and nothing yet, to the best
of our knowledge, on the unsupervised schemes. This paper presents an original
framework for the unsupervised segmentation of the 3D texture on the mesh
manifold. We approach this problem as binary surface segmentation, partitioning
the mesh surface into textured and non-textured regions without prior
annotation. We devise a mutual transformer-based system comprising a label
generator and a cleaner. The two models take geometric image representations of
the surface mesh facets and label them as texture or non-texture across an
iterative mutual learning scheme. Extensive experiments on three publicly
available datasets with diverse texture patterns demonstrate that the proposed
framework outperforms standard and SOTA unsupervised techniques and competes
reasonably with supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganapathi_I/0/1/0/all/0/1&quot;&gt;Iyyakutti Iyappan Ganapathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_F/0/1/0/all/0/1&quot;&gt;Fayaz Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1&quot;&gt;Sajid Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Syed Sadaf Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1&quot;&gt;Naoufel Werghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10696">
<title>Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation. (arXiv:2311.10696v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10696</link>
<description rdf:parseType="Literal">&lt;p&gt;A versatile medical image segmentation model applicable to imaging data
collected with diverse equipment and protocols can facilitate model deployment
and maintenance. However, building such a model typically requires a large,
diverse, and fully annotated dataset, which is rarely available due to the
labor-intensive and costly data curation. In this study, we develop a
cost-efficient method by harnessing readily available data with partially or
even sparsely annotated segmentation labels. We devise strategies for model
self-disambiguation, prior knowledge incorporation, and imbalance mitigation to
address challenges associated with inconsistently labeled data from various
sources, including label ambiguity and imbalances across modalities, datasets,
and segmentation labels. Experimental results on a multi-modal dataset compiled
from eight different sources for abdominal organ segmentation have demonstrated
our method&apos;s effectiveness and superior performance over alternative
state-of-the-art methods, highlighting its potential for optimizing the use of
existing annotated data and reducing the annotation efforts for new data to
further enhance model capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuemeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuncong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yong Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10699">
<title>Using linear initialisation to improve speed of convergence and fully-trained error in Autoencoders. (arXiv:2311.10699v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10699</link>
<description rdf:parseType="Literal">&lt;p&gt;Good weight initialisation is an important step in successful training of
Artificial Neural Networks. Over time a number of improvements have been
proposed to this process. In this paper we introduce a novel weight
initialisation technique called the Straddled Matrix Initialiser. This
initialisation technique is motivated by our assumption that major,
global-scale relationships in data are linear with only smaller effects
requiring complex non-linearities. Combination of Straddled Matrix and ReLU
activation function initialises a Neural Network as a de facto linear model,
which we postulate should be a better starting point for optimisation given our
assumptions. We test this by training autoencoders on three datasets using
Straddled Matrix and seven other state-of-the-art weight initialisation
techniques. In all our experiments the Straddeled Matrix Initialiser clearly
outperforms all other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marais_M/0/1/0/all/0/1&quot;&gt;Marcel Marais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartstein_M/0/1/0/all/0/1&quot;&gt;Mate Hartstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevora_G/0/1/0/all/0/1&quot;&gt;George Cevora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10701">
<title>SpACNN-LDVAE: Spatial Attention Convolutional Latent Dirichlet Variational Autoencoder for Hyperspectral Pixel Unmixing. (arXiv:2311.10701v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10701</link>
<description rdf:parseType="Literal">&lt;p&gt;The Hyperspectral Unxming problem is to find the pure spectral signal of the
underlying materials (endmembers) and their proportions (abundances). The
proposed method builds upon the recently proposed method, Latent Dirichlet
Variational Autoencoder (LDVAE). It assumes that abundances can be encoded as
Dirichlet Distributions while mixed pixels and endmembers are represented by
Multivariate Normal Distributions. However, LDVAE does not leverage spatial
information present in an HSI; we propose an Isotropic CNN encoder with spatial
attention to solve the hyperspectral unmixing problem. We evaluated our model
on Samson, Hydice Urban, Cuprite, and OnTech-HSI-Syn-21 datasets. Our model
also leverages the transfer learning paradigm for Cuprite Dataset, where we
train the model on synthetic data and evaluate it on real-world data. We are
able to observe the improvement in the results for the endmember extraction and
abundance estimation by incorporating the spatial information. Code can be
found at https://github.com/faisalqureshi/cnn-ldvae
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_S/0/1/0/all/0/1&quot;&gt;Soham Chitnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mantripragada_K/0/1/0/all/0/1&quot;&gt;Kiran Mantripragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_F/0/1/0/all/0/1&quot;&gt;Faisal Z. Qureshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10707">
<title>Multimodal Representation Learning by Alternating Unimodal Adaptation. (arXiv:2311.10707v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.10707</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning, which integrates data from diverse sensory modes, plays
a pivotal role in artificial intelligence. However, existing multimodal
learning methods often struggle with challenges where some modalities appear
more dominant than others during multimodal learning, resulting in suboptimal
performance. To address this challenge, we propose MLA (Multimodal Learning
with Alternating Unimodal Adaptation). MLA reframes the conventional joint
multimodal learning process by transforming it into an alternating unimodal
learning process, thereby minimizing interference between modalities.
Simultaneously, it captures cross-modal interactions through a shared head,
which undergoes continuous optimization across different modalities. This
optimization process is controlled by a gradient modification mechanism to
prevent the shared head from losing previously acquired information. During the
inference phase, MLA utilizes a test-time uncertainty-based model fusion
mechanism to integrate multimodal information. Extensive experiments are
conducted on five diverse datasets, encompassing scenarios with complete
modalities and scenarios with missing modalities. These experiments demonstrate
the superiority of MLA over competing prior approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10708">
<title>SelfEval: Leveraging the discriminative nature of generative models for evaluation. (arXiv:2311.10708v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10708</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we show that text-to-image generative models can be &apos;inverted&apos;
to assess their own text-image understanding capabilities in a completely
automated manner.
&lt;/p&gt;
&lt;p&gt;Our method, called SelfEval, uses the generative model to compute the
likelihood of real images given text prompts, making the generative model
directly applicable to discriminative tasks.
&lt;/p&gt;
&lt;p&gt;Using SelfEval, we repurpose standard datasets created for evaluating
multimodal text-image discriminative models to evaluate generative models in a
fine-grained manner: assessing their performance on attribute binding, color
recognition, counting, shape recognition, spatial understanding.
&lt;/p&gt;
&lt;p&gt;To the best of our knowledge SelfEval is the first automated metric to show a
high degree of agreement for measuring text-faithfulness with the gold-standard
human evaluations across multiple models and benchmarks.
&lt;/p&gt;
&lt;p&gt;Moreover, SelfEval enables us to evaluate generative models on challenging
tasks such as Winoground image-score where they demonstrate competitive
performance to discriminative models.
&lt;/p&gt;
&lt;p&gt;We also show severe drawbacks of standard automated metrics such as
CLIP-score to measure text faithfulness on benchmarks such as DrawBench, and
how SelfEval sidesteps these issues.
&lt;/p&gt;
&lt;p&gt;We hope SelfEval enables easy and reliable automated evaluation for diffusion
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1&quot;&gt;Sai Saketh Rambhatla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1&quot;&gt;Ishan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10709">
<title>Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. (arXiv:2311.10709v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.10709</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google&apos;s Imagen Video, 90% vs. Nvidia&apos;s
PYOCO, and 96% vs. Meta&apos;s Make-A-Video. Our model outperforms commercial
solutions such as RunwayML&apos;s Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user&apos;s text
prompt, where our generations are preferred 96% over prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1&quot;&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mannat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1&quot;&gt;Andrew Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1&quot;&gt;Quentin Duval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azadi_S/0/1/0/all/0/1&quot;&gt;Samaneh Azadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1&quot;&gt;Sai Saketh Rambhatla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Akbar Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1&quot;&gt;Devi Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1&quot;&gt;Ishan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03574">
<title>Structured Prediction Problem Archive. (arXiv:2202.03574v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03574</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured prediction problems are one of the fundamental tools in machine
learning. In order to facilitate algorithm development for their numerical
solution, we collect in one place a large number of datasets in easy to read
formats for a diverse set of problem classes. We provide archival links to
datasets, description of the considered problems and problem formats, and a
short summary of problem characteristics including size, number of instances
etc. For reference we also give a non-exhaustive selection of algorithms
proposed in the literature for their solution. We hope that this central
repository will make benchmarking and comparison to established works easier.
We welcome submission of interesting new datasets and algorithms for inclusion
in our archive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1&quot;&gt;Paul Swoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andres_B/0/1/0/all/0/1&quot;&gt;Bjoern Andres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hornakova_A/0/1/0/all/0/1&quot;&gt;Andrea Hornakova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1&quot;&gt;Florian Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irmai_J/0/1/0/all/0/1&quot;&gt;Jannik Irmai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roetzer_P/0/1/0/all/0/1&quot;&gt;Paul Roetzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1&quot;&gt;Bogdan Savchynskyy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_D/0/1/0/all/0/1&quot;&gt;David Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1&quot;&gt;Ahmed Abbas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08387">
<title>A Semantic-aware Attention and Visual Shielding Network for Cloth-changing Person Re-identification. (arXiv:2207.08387v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08387</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloth-changing person reidentification (ReID) is a newly emerging research
topic that aims to retrieve pedestrians whose clothes are changed. Since the
human appearance with different clothes exhibits large variations, it is very
difficult for existing approaches to extract discriminative and robust feature
representations. Current works mainly focus on body shape or contour sketches,
but the human semantic information and the potential consistency of pedestrian
features before and after changing clothes are not fully explored or are
ignored. To solve these issues, in this work, a novel semantic-aware attention
and visual shielding network for cloth-changing person ReID (abbreviated as
SAVS) is proposed where the key idea is to shield clues related to the
appearance of clothes and only focus on visual semantic information that is not
sensitive to view/posture changes. Specifically, a visual semantic encoder is
first employed to locate the human body and clothing regions based on human
semantic segmentation information. Then, a human semantic attention module
(HSA) is proposed to highlight the human semantic information and reweight the
visual feature map. In addition, a visual clothes shielding module (VCS) is
also designed to extract a more robust feature representation for the
cloth-changing task by covering the clothing regions and focusing the model on
the visual semantic information unrelated to the clothes. Most importantly,
these two modules are jointly explored in an end-to-end unified framework.
Extensive experiments demonstrate that the proposed method can significantly
outperform state-of-the-art methods, and more robust features can be extracted
for cloth-changing persons. Compared with FSAM (published in CVPR 2021), this
method can achieve improvements of 32.7% (16.5%) and 14.9% (-) on the LTCC and
PRCC datasets in terms of mAP (rank-1), respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hongwei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1&quot;&gt;Weili Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jie Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shenyong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05231">
<title>NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction. (arXiv:2212.05231v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05231</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent methods for neural surface representation and rendering, for example
NeuS, have demonstrated the remarkably high-quality reconstruction of static
scenes. However, the training of NeuS takes an extremely long time (8 hours),
which makes it almost impossible to apply them to dynamic scenes with thousands
of frames. We propose a fast neural surface reconstruction approach, called
NeuS2, which achieves two orders of magnitude improvement in terms of
acceleration without compromising reconstruction quality. To accelerate the
training process, we parameterize a neural surface representation by
multi-resolution hash encodings and present a novel lightweight calculation of
second-order derivatives tailored to our networks to leverage CUDA parallelism,
achieving a factor two speed up. To further stabilize and expedite training, a
progressive learning strategy is proposed to optimize multi-resolution hash
encodings from coarse to fine. We extend our method for fast training of
dynamic scenes, with a proposed incremental training strategy and a novel
global transformation prediction component, which allow our method to handle
challenging long sequences with large movements and deformations. Our
experiments on various datasets demonstrate that NeuS2 significantly
outperforms the state-of-the-arts in both surface reconstruction accuracy and
training speed for both static and dynamic scenes. The code is available at our
website: https://vcai.mpi-inf.mpg.de/projects/NeuS2/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1&quot;&gt;Marc Habermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1&quot;&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01913">
<title>Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment. (arXiv:2303.01913v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01913</link>
<description rdf:parseType="Literal">&lt;p&gt;As deep learning models become popular, there is a lot of need for deploying
them to diverse device environments. Because it is costly to develop and
optimize a neural network for every single environment, there is a line of
research to search neural networks for multiple target environments
efficiently. However, existing works for such a situation still suffer from
requiring many GPUs and expensive costs. Motivated by this, we propose a novel
neural network optimization framework named Bespoke for low-cost deployment.
Our framework searches for a lightweight model by replacing parts of an
original model with randomly selected alternatives, each of which comes from a
pretrained neural network or the original model. In the practical sense,
Bespoke has two significant merits. One is that it requires near zero cost for
designing the search space of neural networks. The other merit is that it
exploits the sub-networks of public pretrained neural networks, so the total
cost is minimal compared to the existing works. We conduct experiments
exploring Bespoke&apos;s the merits, and the results show that it finds efficient
models for multiple targets with meager cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong-Ryul Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1&quot;&gt;Yong-Hyuk Moon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07945">
<title>Edit-A-Video: Single Video Editing with Object-Aware Consistency. (arXiv:2303.07945v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07945</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the fact that text-to-video (TTV) model has recently achieved
remarkable success, there have been few approaches on TTV for its extension to
video editing. Motivated by approaches on TTV models adapting from
diffusion-based text-to-image (TTI) models, we suggest the video editing
framework given only a pretrained TTI model and a single &amp;lt;text, video&amp;gt; pair,
which we term Edit-A-Video. The framework consists of two stages: (1) inflating
the 2D model into the 3D model by appending temporal modules and tuning on the
source video (2) inverting the source video into the noise and editing with
target text prompt and attention map injection. Each stage enables the temporal
modeling and preservation of semantic attributes of the source video. One of
the key challenges for video editing include a background inconsistency
problem, where the regions not included for the edit suffer from undesirable
and inconsistent temporal alterations. To mitigate this issue, we also
introduce a novel mask blending method, termed as sparse-causal blending (SC
Blending). We improve previous mask blending methods to reflect the temporal
consistency so that the area where the editing is applied exhibits smooth
transition while also achieving spatio-temporal consistency of the unedited
regions. We present extensive experimental results over various types of text
and videos, and demonstrate the superiority of the proposed method compared to
baselines in terms of background consistency, text alignment, and video editing
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Chaehun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heeseung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Che Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-gil Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10076">
<title>A Simple Framework for 3D Occupancy Estimation in Autonomous Driving. (arXiv:2303.10076v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10076</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of estimating 3D occupancy from surrounding-view images is an
exciting development in the field of autonomous driving, following the success
of Bird&apos;s Eye View (BEV) perception. This task provides crucial 3D attributes
of the driving environment, enhancing the overall understanding and perception
of the surrounding space. In this work, we present a simple framework for 3D
occupancy estimation, which is a CNN-based framework designed to reveal several
key factors for 3D occupancy estimation, such as network design, optimization,
and evaluation. In addition, we explore the relationship between 3D occupancy
estimation and other related tasks, such as monocular depth estimation and 3D
reconstruction, which could advance the study of 3D perception in autonomous
driving. For evaluation, we propose a simple sampling strategy to define the
metric for occupancy evaluation, which is flexible for current public datasets.
Moreover, we establish the benchmark in terms of the depth estimation metric,
where we compare our proposed method with monocular depth estimation methods on
the DDAD and Nuscenes datasets and achieve competitive performance. The
relevant code will be updated in https://github.com/GANWANSHUI/SimpleOccupancy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Wanshui Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_N/0/1/0/all/0/1&quot;&gt;Ningkai Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1&quot;&gt;Naoto Yokoya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11573">
<title>BigSmall: Efficient Multi-Task Learning for Disparate Spatial and Temporal Physiological Measurements. (arXiv:2303.11573v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11573</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding of human visual perception has historically inspired the design
of computer vision architectures. As an example, perception occurs at different
scales both spatially and temporally, suggesting that the extraction of salient
visual information may be made more effective by paying attention to specific
features at varying scales. Visual changes in the body due to physiological
processes also occur at different scales and with modality-specific
characteristic properties. Inspired by this, we present BigSmall, an efficient
architecture for physiological and behavioral measurement. We present the first
joint camera-based facial action, cardiac, and pulmonary measurement model. We
propose a multi-branch network with wrapping temporal shift modules that yields
both accuracy and efficiency gains. We observe that fusing low-level features
leads to suboptimal performance, but that fusing high level features enables
efficiency gains with negligible loss in accuracy. Experimental results
demonstrate that BigSmall significantly reduces the computational costs.
Furthermore, compared to existing task-specific models, BigSmall achieves
comparable or better results on multiple physiological measurement tasks
simultaneously with a unified model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanswamy_G/0/1/0/all/0/1&quot;&gt;Girish Narayanswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yujia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chengqian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17245">
<title>Investigating and Mitigating the Side Effects of Noisy Views for Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios. (arXiv:2303.17245v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17245</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view clustering (MVC) aims at exploring category structures among
multi-view data in self-supervised manners. Multiple views provide more
information than single views and thus existing MVC methods can achieve
satisfactory performance. However, their performance might seriously degenerate
when the views are noisy in practical multi-view scenarios. In this paper, we
first formally investigate the drawback of noisy views and then propose a
theoretically grounded deep MVC method (namely MVCAN) to address this issue.
Specifically, we propose a novel MVC objective that enables un-shared
parameters and inconsistent clustering predictions across multiple views to
reduce the side effects of noisy views. Furthermore, a two-level multi-view
iterative optimization is designed to generate robust learning targets for
refining individual views&apos; representation learning. Theoretical analysis
reveals that MVCAN works by achieving the multi-view consistency,
complementarity, and noise robustness. Finally, experiments on extensive public
datasets demonstrate that MVCAN outperforms state-of-the-art methods and is
robust against the existence of noisy views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yazhou Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04400">
<title>Identity-Guided Collaborative Learning for Cloth-Changing Person Reidentification. (arXiv:2304.04400v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04400</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloth-changing person reidentification (ReID) is a newly emerging research
topic that is aimed at addressing the issues of large feature variations due to
cloth-changing and pedestrian view/pose changes. Although significant progress
has been achieved by introducing extra information (e.g., human contour
sketching information, human body keypoints, and 3D human information),
cloth-changing person ReID is still challenging due to impressionable
pedestrian representations. Moreover, human semantic information and pedestrian
identity information are not fully explored. To solve these issues, we propose
a novel identity-guided collaborative learning scheme (IGCL) for cloth-changing
person ReID, where the human semantic is fully utilized and the identity is
unchangeable to guide collaborative learning. First, we design a novel clothing
attention degradation stream to reasonably reduce the interference caused by
clothing information where clothing attention and mid-level collaborative
learning are employed. Second, we propose a human semantic attention and body
jigsaw stream to highlight the human semantic information and simulate
different poses of the same identity. In this way, the extraction features not
only focus on human semantic information that is unrelated to the background
but also are suitable for pedestrian pose variations. Moreover, a pedestrian
identity enhancement stream is further proposed to enhance the identity
importance and extract more favorable identity robust features. Most
importantly, all these streams are jointly explored in an end-to-end unified
framework, and the identity is utilized to guide the optimization. Extensive
experiments on five public clothing person ReID datasets demonstrate that the
proposed IGCL significantly outperforms SOTA methods and that the extracted
feature is more robust, discriminative, and clothing-irrelevant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shenxun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1&quot;&gt;Weili Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shenyong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06866">
<title>PMI Sampler: Patch Similarity Guided Frame Selection for Aerial Action Recognition. (arXiv:2304.06866v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06866</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new algorithm for selection of informative frames in video
action recognition. Our approach is designed for aerial videos captured using a
moving camera where human actors occupy a small spatial resolution of video
frames. Our algorithm utilizes the motion bias within aerial videos, which
enables the selection of motion-salient frames. We introduce the concept of
patch mutual information (PMI) score to quantify the motion bias between
adjacent frames, by measuring the similarity of patches. We use this score to
assess the amount of discriminative motion information contained in one frame
relative to another. We present an adaptive frame selection strategy using
shifted leaky ReLu and cumulative distribution function, which ensures that the
sampled frames comprehensively cover all the essential segments with high
motion salience. Our approach can be integrated with any action recognition
model to enhance its accuracy. In practice, our method achieves a relative
improvement of 2.2 - 13.8% in top-1 accuracy on UAV-Human, 6.8% on NEC Drone,
and 9.0% on Diving48 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1&quot;&gt;Ruiqi Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1&quot;&gt;Divya Kothandaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05370">
<title>MSVQ: Self-Supervised Learning with Multiple Sample Views and Queues. (arXiv:2305.05370v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05370</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised methods based on contrastive learning have achieved great
success in unsupervised visual representation learning. However, most methods
under this framework suffer from the problem of false negative samples.
Inspired by the mean shift for self-supervised learning, we propose a new
simple framework, namely Multiple Sample Views and Queues (MSVQ). We jointly
construct three soft labels on-the-fly by utilizing two complementary and
symmetric approaches: multiple augmented positive views and two momentum
encoders that generate various semantic features for negative samples. Two
teacher networks perform similarity relationship calculations with negative
samples and then transfer this knowledge to the student network. Let the
student network mimic the similarity relationships between the samples, thus
giving the student network a more flexible ability to identify false negative
samples in the dataset. The classification results on four benchmark image
datasets demonstrate the high effectiveness and efficiency of our approach
compared to some classical methods. Source code and pretrained models are
available \href{https://github.com/pc-cp/MSVQ}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xianzhong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04226">
<title>Normalization Layers Are All That Sharpness-Aware Minimization Needs. (arXiv:2306.04226v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04226</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima
and has been shown to enhance generalization performance in various settings.
In this work we show that perturbing only the affine normalization parameters
(typically comprising 0.1% of the total parameters) in the adversarial step of
SAM can outperform perturbing all of the parameters.This finding generalizes to
different SAM variants and both ResNet (Batch Normalization) and Vision
Transformer (Layer Normalization) architectures. We consider alternative sparse
perturbation approaches and find that these do not achieve similar performance
enhancement at such extreme sparsity levels, showing that this behaviour is
unique to the normalization layers. Although our findings reaffirm the
effectiveness of SAM in improving generalization performance, they cast doubt
on whether this is solely caused by reduced sharpness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1&quot;&gt;Maximilian Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1&quot;&gt;Tiffany Vlaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05399">
<title>Matting Anything. (arXiv:2306.05399v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose the Matting Anything Model (MAM), an efficient and
versatile framework for estimating the alpha matte of any instance in an image
with flexible and interactive visual or linguistic user prompt guidance. MAM
offers several significant advantages over previous specialized image matting
networks: (i) MAM is capable of dealing with various types of image matting,
including semantic, instance, and referring image matting with only a single
model; (ii) MAM leverages the feature maps from the Segment Anything Model
(SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha
matte through iterative refinement, which has only 2.7 million trainable
parameters. (iii) By incorporating SAM, MAM simplifies the user intervention
required for the interactive use of image matting from the trimap to the box,
point, or text prompt. We evaluate the performance of MAM on various image
matting benchmarks, and the experimental results demonstrate that MAM achieves
comparable performance to the state-of-the-art specialized image matting models
under different metrics on each benchmark. Overall, MAM shows superior
generalization ability and can effectively handle various image matting tasks
with fewer parameters, making it a practical solution for unified image
matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1&quot;&gt;Jitesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06048">
<title>How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?. (arXiv:2306.06048v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06048</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1&quot;&gt;Yifei Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixuan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06354">
<title>EventCLIP: Adapting CLIP for Event-based Object Recognition. (arXiv:2306.06354v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06354</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in zero-shot and few-shot classification heavily rely on the
success of pre-trained vision-language models (VLMs) such as CLIP. Due to a
shortage of large-scale datasets, training such models for event camera data
remains infeasible. Thus, adapting existing VLMs across modalities to event
vision is an important research challenge. In this work, we introduce
EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot
event-based object recognition. We first generalize CLIP&apos;s image encoder to
event data by converting raw events to 2D grid-based representations. To
further enhance performance, we propose a feature adapter to aggregate temporal
information over event frames and refine text embeddings to better align with
the visual inputs. We evaluate EventCLIP on N-Caltech, N-Cars, and N-ImageNet
datasets, achieving state-of-the-art few-shot performance. When fine-tuned on
the entire dataset, our method outperforms all existing event classifiers.
Moreover, we explore practical applications of EventCLIP including robust event
classification and label-free event recognition, where our approach surpasses
previous baselines designed specifically for these tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xudong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1&quot;&gt;Igor Gilitschenski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08984">
<title>Tree Variational Autoencoders. (arXiv:2306.08984v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Tree Variational Autoencoder (TreeVAE), a new generative
hierarchical clustering model that learns a flexible tree-based posterior
distribution over latent variables. TreeVAE hierarchically divides samples
according to their intrinsic characteristics, shedding light on hidden
structures in the data. It adapts its architecture to discover the optimal tree
for encoding dependencies between latent variables. The proposed tree-based
generative architecture enables lightweight conditional inference and improves
generative performance by utilizing specialized leaf decoders. We show that
TreeVAE uncovers underlying clusters in the data and finds meaningful
hierarchical relations between the different groups on a variety of datasets,
including real-world imaging data. We present empirically that TreeVAE provides
a more competitive log-likelihood lower bound than the sequential counterparts.
Finally, due to its generative nature, TreeVAE is able to generate new samples
from the discovered clusters via conditional sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1&quot;&gt;Laura Manduchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandenhirtz_M/0/1/0/all/0/1&quot;&gt;Moritz Vandenhirtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryser_A/0/1/0/all/0/1&quot;&gt;Alain Ryser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11719">
<title>Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision. (arXiv:2306.11719v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11719</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models are a powerful type of generative models used to
capture complex distributions of real-world signals. However, their
applicability is limited to scenarios where training samples are readily
available, which is not always the case in real-world applications. For
example, in inverse graphics, the goal is to generate samples from a
distribution of 3D scenes that align with a given image, but ground-truth 3D
scenes are unavailable and only 2D images are accessible. To address this
limitation, we propose a novel class of denoising diffusion probabilistic
models that learn to sample from distributions of signals that are never
directly observed. Instead, these signals are measured indirectly through a
known differentiable forward model, which produces partial observations of the
unknown signal. Our approach involves integrating the forward model directly
into the denoising process. This integration effectively connects the
generative modeling of observations with the generative modeling of the
underlying signals, allowing for end-to-end training of a conditional
generative model over signals. During inference, our approach enables sampling
from the distribution of underlying signals that are consistent with a given
partial observation. We demonstrate the effectiveness of our method on three
challenging computer vision tasks. For instance, in the context of inverse
graphics, our model enables direct sampling from the distribution of 3D scenes
that align with a single 2D input image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1&quot;&gt;Ayush Tewari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1&quot;&gt;Tianwei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1&quot;&gt;George Cazenavette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1&quot;&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;do Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1&quot;&gt;William T. Freeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1&quot;&gt;Vincent Sitzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07482">
<title>Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based Tumor Classification. (arXiv:2307.07482v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07482</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole slide image (WSI) assessment is a challenging and crucial step in
cancer diagnosis and treatment planning. WSIs require high magnifications to
facilitate sub-cellular analysis. Precise annotations for patch- or even
pixel-level classifications in the context of gigapixel WSIs are tedious to
acquire and require domain experts. Coarse-grained labels, on the other hand,
are easily accessible, which makes WSI classification an ideal use case for
multiple instance learning (MIL). In our work, we propose a novel
embedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both the
embedding and aggregation steps. Since all-purpose visual feature
representations are not yet available, embedding models are currently limited
in terms of generalizability. With our work, we explore the potential of
dynamic meta-embedding based on cutting-edge self-supervised pre-trained models
in the context of MIL. Moreover, we propose a new MIL architecture capable of
combining MIL-attention with correlated self-attention. The Dual-Query
Perceiver design of our approach allows us to leverage the concept of
self-distillation and to combine the advantages of a small model in the context
of a low data regime with the rich feature representation of a larger model. We
demonstrate the superior performance of our approach on three histopathological
datasets, where we show improvement of up to 10% over state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holdenried_Krafft_S/0/1/0/all/0/1&quot;&gt;Simon Holdenried-Krafft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somers_P/0/1/0/all/0/1&quot;&gt;Peter Somers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montes_Majarro_I/0/1/0/all/0/1&quot;&gt;Ivonne A. Montes-Majarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silimon_D/0/1/0/all/0/1&quot;&gt;Diana Silimon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarin_C/0/1/0/all/0/1&quot;&gt;Cristina Tar&amp;#xed;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fend_F/0/1/0/all/0/1&quot;&gt;Falko Fend&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1&quot;&gt;Hendrik P. A. Lensch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08003">
<title>SHAMSUL: Systematic Holistic Analysis to investigate Medical Significance Utilizing Local interpretability methods in deep learning for chest radiography pathology prediction. (arXiv:2307.08003v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08003</link>
<description rdf:parseType="Literal">&lt;p&gt;The interpretability of deep neural networks has become a subject of great
interest within the medical and healthcare domain. This attention stems from
concerns regarding transparency, legal and ethical considerations, and the
medical significance of predictions generated by these deep neural networks in
clinical decision support systems. To address this matter, our study delves
into the application of four well-established interpretability methods: Local
Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations
(SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise
Relevance Propagation (LRP). Leveraging the approach of transfer learning with
a multi-label-multi-class chest radiography dataset, we aim to interpret
predictions pertaining to specific pathology classes. Our analysis encompasses
both single-label and multi-label predictions, providing a comprehensive and
unbiased assessment through quantitative and qualitative investigations, which
are compared against human expert annotation. Notably, Grad-CAM demonstrates
the most favorable performance in quantitative evaluation, while the LIME
heatmap score segmentation visualization exhibits the highest level of medical
significance. Our research underscores both the outcomes and the challenges
faced in the holistic approach adopted for assessing these interpretability
methods and suggests that a multimodal-based approach, incorporating diverse
sources of information beyond chest radiography images, could offer additional
insights for enhancing interpretability in the medical domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Mahbub Ul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollmen_J/0/1/0/all/0/1&quot;&gt;Jaakko Hollm&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baldvinsson_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf3;n R&amp;#xfa;nar Baldvinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahmani_R/0/1/0/all/0/1&quot;&gt;Rahim Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06595">
<title>VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for
evaluation of instruction-following vision-language models for real-world use.
Our starting point is curating 70 &apos;instruction families&apos; that we envision
instruction tuned vision-language models should be able to address. Extending
beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to
game playing and creative generation. Following curation, our dataset comprises
592 test queries, each with a human-authored instruction-conditioned caption.
These descriptions surface instruction-specific factors, e.g., for an
instruction asking about the accessibility of a storefront for wheelchair
users, the instruction-conditioned caption describes ramps/potential obstacles.
These descriptions enable 1) collecting human-verified reference outputs for
each instance; and 2) automatic evaluation of candidate multimodal generations
using a text-only LLM, aligning with human judgment. We quantify quality gaps
between models and references using both human and automatic evaluations; e.g.,
the top-performing instruction-following model wins against the GPT-4 reference
in just 27% of the comparison. VisIT-Bench is dynamic to participate,
practitioners simply submit their model&apos;s response on the project website;
Data, code and leaderboard is available at visit-bench.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11239">
<title>LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training. (arXiv:2308.11239v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11239</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning object segmentation in image and video datasets without human
supervision is a challenging problem. Humans easily identify moving salient
objects in videos using the gestalt principle of common fate, which suggests
that what moves together belongs together. Building upon this idea, we propose
a self-supervised object discovery approach that leverages motion and
appearance information to produce high-quality object segmentation masks.
Specifically, we redesign the traditional graph cut on images to include motion
information in a linear combination with appearance information to produce edge
weights. Remarkably, this step produces object segmentation masks comparable to
the current state-of-the-art on multiple benchmarks. To further improve
performance, we bootstrap a segmentation network trained on these preliminary
masks as pseudo-ground truths to learn from its own outputs via self-training.
We demonstrate the effectiveness of our approach, named LOCATE, on multiple
standard video object segmentation, image saliency detection, and object
segmentation benchmarks, achieving results on par with and, in many cases
surpassing state-of-the-art methods. We also demonstrate the transferability of
our approach to novel domains through a qualitative study on in-the-wild
images. Additionally, we present extensive ablation analysis to support our
design choices and highlight the contribution of each component of our proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Silky Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1&quot;&gt;Shripad Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1&quot;&gt;Mausoom Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14113">
<title>Semantic-aware Consistency Network for Cloth-changing Person Re-Identification. (arXiv:2308.14113v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14113</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloth-changing Person Re-Identification (CC-ReID) is a challenging task that
aims to retrieve the target person across multiple surveillance cameras when
clothing changes might happen. Despite recent progress in CC-ReID, existing
approaches are still hindered by the interference of clothing variations since
they lack effective constraints to keep the model consistently focused on
clothing-irrelevant regions. To address this issue, we present a Semantic-aware
Consistency Network (SCNet) to learn identity-related semantic features by
proposing effective consistency constraints. Specifically, we generate the
black-clothing image by erasing pixels in the clothing area, which explicitly
mitigates the interference from clothing variations. In addition, to fully
exploit the fine-grained identity information, a head-enhanced attention module
is introduced, which learns soft attention maps by utilizing the proposed
part-based matching loss to highlight head information. We further design a
semantic consistency loss to facilitate the learning of high-level
identity-related semantic features, forcing the model to focus on semantically
consistent cloth-irrelevant regions. By using the consistency constraint, our
model does not require any extra auxiliary segmentation module to generate the
black-clothing image or locate the head region during the inference stage.
Extensive experiments on four cloth-changing person Re-ID datasets (LTCC, PRCC,
Vc-Clothes, and DeepChange) demonstrate that our proposed SCNet makes
significant improvements over prior state-of-the-art approaches. Our code is
available at: https://github.com/Gpn-star/SCNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Peini Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianbing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoquan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04447">
<title>Impact of Blur and Resolution on Demographic Disparities in 1-to-Many Facial Identification. (arXiv:2309.04447v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04447</link>
<description rdf:parseType="Literal">&lt;p&gt;Most studies to date that have examined demographic variations in face
recognition accuracy have analyzed 1-to-1 matching accuracy, using images that
could be described as &quot;government ID quality&quot;. This paper analyzes the accuracy
of 1-to-many facial identification across demographic groups, and in the
presence of blur and reduced resolution in the probe image as might occur in
&quot;surveillance camera quality&quot; images. Cumulative match characteristic curves
(CMC) are not appropriate for comparing propensity for rank-one recognition
errors across demographics, and so we use three metrics for our analysis: (1)
the well-known d&apos; metric between mated and non-mated score distributions, and
introduced in this work, (2) absolute score difference between thresholds in
the high-similarity tail of the non-mated and the low-similarity tail of the
mated distribution, and (3) distribution of (mated - non-mated rank-one scores)
across the set of probe images. We find that demographic variation in 1-to-many
accuracy does not entirely follow what has been observed in 1-to-1 matching
accuracy. Also, different from 1-to-1 accuracy, demographic comparison of
1-to-many accuracy can be affected by different numbers of identities and
images across demographics. More importantly, we show that increased blur in
the probe image, or reduced resolution of the face in the probe image, can
significantly increase the false positive identification rate. And we show that
the demographic variation in these high blur or low resolution conditions is
much larger for male / female than for African-American / Caucasian. The point
that 1-to-many accuracy can potentially collapse in the context of processing
&quot;surveillance camera quality&quot; probe images against a &quot;government ID quality&quot;
gallery is an important one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatta_A/0/1/0/all/0/1&quot;&gt;Aman Bhatta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pangelinan_G/0/1/0/all/0/1&quot;&gt;Gabriella Pangelinan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1&quot;&gt;Michael C. King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1&quot;&gt;Kevin W. Bowyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15991">
<title>Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness. (arXiv:2309.15991v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15991</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial neural networks typically struggle in generalizing to
out-of-context examples. One reason for this limitation is caused by having
datasets that incorporate only partial information regarding the potential
correlational structure of the world. In this work, we propose TIDA (Targeted
Image-editing Data Augmentation), a targeted data augmentation method focused
on improving models&apos; human-like abilities (e.g., gender recognition) by filling
the correlational structure gap using a text-to-image generative model. More
specifically, TIDA identifies specific skills in captions describing images
(e.g., the presence of a specific gender in the image), changes the caption
(e.g., &quot;woman&quot; to &quot;man&quot;), and then uses a text-to-image model to edit the image
in order to match the novel caption (e.g., uniquely changing a woman to a man
while maintaining the context identical). Based on the Flickr30K benchmark, we
show that, compared with the original data set, a TIDA-enhanced dataset related
to gender, color, and counting abilities induces better performance in several
image captioning metrics. Furthermore, on top of relying on the classical BLEU
metric, we conduct a fine-grained analysis of the improvements of our models
against the baseline in different ways. We compared text-to-image generative
models and found different behaviors of the image captioning models in terms of
encoding visual encoding and textual decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barriere_V/0/1/0/all/0/1&quot;&gt;Valentin Barriere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rio_F/0/1/0/all/0/1&quot;&gt;Felipe del Rio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferari_A/0/1/0/all/0/1&quot;&gt;Andres Carvallo De Ferari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aspillaga_C/0/1/0/all/0/1&quot;&gt;Carlos Aspillaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrera_Berg_E/0/1/0/all/0/1&quot;&gt;Eugenio Herrera-Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calderon_C/0/1/0/all/0/1&quot;&gt;Cristian Buc Calderon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00022">
<title>CtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image Understanding. (arXiv:2310.00022v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00022</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning representations through self-supervision on unlabeled data has
proven highly effective for understanding diverse images. However, remote
sensing images often have complex and densely populated scenes with multiple
land objects and no clear foreground objects. This intrinsic property generates
high object density, resulting in false positive pairs or missing contextual
information in self-supervised learning. To address these problems, we propose
a context-enhanced masked image modeling method (CtxMIM), a simple yet
efficient MIM-based self-supervised learning for remote sensing image
understanding. CtxMIM formulates original image patches as a reconstructive
template and employs a Siamese framework to operate on two sets of image
patches. A context-enhanced generative branch is introduced to provide
contextual information through context consistency constraints in the
reconstruction. With the simple and elegant design, CtxMIM encourages the
pre-training model to learn object-level or pixel-level features on a
large-scale dataset without specific temporal or geographical constraints.
Finally, extensive experiments show that features learned by CtxMIM outperform
fully supervised and state-of-the-art self-supervised learning methods on
various downstream tasks, including land cover classification, semantic
segmentation, object detection, and instance segmentation. These results
demonstrate that CtxMIM learns impressive remote sensing representations with
high generalization and transferability. Code and data will be made public
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15105">
<title>FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning. (arXiv:2310.15105v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15105</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the limited availability of data, existing few-shot learning methods
trained from scratch fail to achieve satisfactory performance. In contrast,
large-scale pre-trained models such as CLIP demonstrate remarkable few-shot and
zero-shot capabilities. To enhance the performance of pre-trained models for
downstream tasks, fine-tuning the model on downstream data is frequently
necessary. However, fine-tuning the pre-trained model leads to a decrease in
its generalizability in the presence of distribution shift, while the limited
number of samples in few-shot learning makes the model highly susceptible to
overfitting. Consequently, existing methods for fine-tuning few-shot learning
primarily focus on fine-tuning the model&apos;s classification head or introducing
additional structure. In this paper, we introduce a fine-tuning approach termed
Feature Discrimination Alignment (FD-Align). Our method aims to bolster the
model&apos;s generalizability by preserving the consistency of spurious features
across the fine-tuning process. Extensive experimental results validate the
efficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model
can seamlessly integrate with existing methods, leading to performance
improvements. Our code can be found in https://github.com/skingorz/FD-Align.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huimin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Bochao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15200">
<title>Open-Set Image Tagging with Multi-Grained Text Supervision. (arXiv:2310.15200v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15200</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the Recognize Anything Plus Model (RAM++), an
open-set image tagging model effectively leveraging multi-grained text
supervision. Previous approaches (e.g., CLIP) primarily utilize global text
supervision paired with images, leading to sub-optimal performance in
recognizing multiple individual semantic tags. In contrast, RAM++ seamlessly
integrates individual tag supervision with global text supervision, all within
a unified alignment framework. This integration not only ensures efficient
recognition of predefined tag categories, but also enhances generalization
capabilities for diverse open-set categories. Furthermore, RAM++ employs large
language models (LLMs) to convert semantically constrained tag supervision into
more expansive tag description supervision, thereby enriching the scope of
open-set visual description concepts. Comprehensive evaluations on various
image recognition benchmarks demonstrate RAM++ exceeds existing
state-of-the-art (SOTA) open-set image tagging models on most aspects.
Specifically, for predefined commonly used tag categories, RAM++ showcases 10.2
mAP and 15.4 mAP enhancements over CLIP on OpenImages and ImageNet. For
open-set categories beyond predefined, RAM++ records improvements of 5.0 mAP
and 6.4 mAP over CLIP and RAM respectively on OpenImages. For diverse
human-object interaction phrases, RAM++ achieves 7.8 mAP and 4.7 mAP
improvements on the HICO benchmark. Code, datasets and pre-trained models are
available at \url{https://github.com/xinyu1205/recognize-anything}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi-Jie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youcai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1&quot;&gt;Weiwei Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Rui Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuejie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yanchun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18849">
<title>Deep Learning-based Compressed Domain Multimedia for Man and Machine: A Taxonomy and Application to Point Cloud Classification. (arXiv:2310.18849v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18849</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current golden age of multimedia, human visualization is no longer the
single main target, with the final consumer often being a machine which
performs some processing or computer vision tasks. In both cases, deep learning
plays a undamental role in extracting features from the multimedia
representation data, usually producing a compressed representation referred to
as latent representation. The increasing development and adoption of deep
learning-based solutions in a wide area of multimedia applications have opened
an exciting new vision where a common compressed multimedia representation is
used for both man and machine. The main benefits of this vision are two-fold:
i) improved performance for the computer vision tasks, since the effects of
coding artifacts are mitigated; and ii) reduced computational complexity, since
prior decoding is not required. This paper proposes the first taxonomy for
designing compressed domain computer vision solutions driven by the
architecture and weights compatibility with an available spatio-temporal
computer vision processor. The potential of the proposed taxonomy is
demonstrated for the specific case of point cloud classification by designing
novel compressed domain processors using the JPEG Pleno Point Cloud Coding
standard under development and adaptations of the PointGrid classifier.
Experimental results show that the designed compressed domain point cloud
classification solutions can significantly outperform the spatial-temporal
domain classification benchmarks when applied to the decompressed data,
containing coding artifacts, and even surpass their performance when applied to
the original uncompressed data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seleem_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Seleem&lt;/a&gt; (1, 2, 4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guarda_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; F. R. Guarda&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_N/0/1/0/all/0/1&quot;&gt;Nuno M. M. Rodrigues&lt;/a&gt; (2, 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1&quot;&gt;Fernando Pereira&lt;/a&gt; (1, 2) ((1) Instituto Superior T&amp;#xe9;cnico - Universidade de Lisboa, Lisbon, Portugal, (2) Instituto de Telecomunica&amp;#xe7;&amp;#xf5;es, Portugal, (3) ESTG, Polit&amp;#xe9;cnico de Leiria, Leiria, Portugal, (4) Faculty of Computers and Information, South Valley University, Qena, Egypt)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00689">
<title>Collaboration in Immersive Environments: Challenges and Solutions. (arXiv:2311.00689v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00689</link>
<description rdf:parseType="Literal">&lt;p&gt;Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doroudian_S/0/1/0/all/0/1&quot;&gt;Shahin Doroudian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00690">
<title>What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00690</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of visual analytics process can benefit visualization
researchers from multiple aspects, including improving visual designs and
developing advanced interaction functions. However, the log files of user
behaviors are still hard to analyze due to the complexity of sensemaking and
our lack of knowledge on the related user behaviors. This work presents a study
on a comprehensive data collection of user behaviors, and our analysis approach
with time-series classification methods. We have chosen a classical
visualization application, Covid-19 data analysis, with common analysis tasks
covering geo-spatial, time-series and multi-attributes. Our user study collects
user behaviors on a diverse set of visualization tasks with two comparable
systems, desktop and immersive visualizations. We summarize the classification
results with three time-series machine learning algorithms at two scales, and
explore the influences of behavior features. Our results reveal that user
behaviors can be distinguished during the process of visual analytics and there
is a potentially strong association between the physical behaviors of users and
the visualization tasks they perform. We also demonstrate the usage of our
models by interpreting open sessions of visual analytics, which provides an
automatic way to study sensemaking without tedious manual annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doroudian_S/0/1/0/all/0/1&quot;&gt;Shahin Doroudian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zekun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1&quot;&gt;Aidong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02058">
<title>LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02058</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce LOTUS, a continual imitation learning algorithm that empowers a
physical robot to continuously and efficiently learn to solve new manipulation
tasks throughout its lifespan. The core idea behind LOTUS is constructing an
ever-growing skill library from a sequence of new tasks with a small number of
human demonstrations. LOTUS starts with a continual skill discovery process
using an open-vocabulary vision model, which extracts skills as recurring
patterns presented in unsegmented demonstrations. Continual skill discovery
updates existing skills to avoid catastrophic forgetting of previous tasks and
adds new skills to solve novel tasks. LOTUS trains a meta-controller that
flexibly composes various skills to tackle vision-based manipulation tasks in
the lifelong learning process. Our comprehensive experiments show that LOTUS
outperforms state-of-the-art baselines by over 11% in success rate, showing its
superior knowledge transfer ability compared to prior methods. More results and
videos can be found on the project website:
https://ut-austin-rpl.github.io/Lotus/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Weikang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yifeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rutav Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02256">
<title>Image Recognition of Oil Leakage Area Based on Logical Semantic Discrimination. (arXiv:2311.02256v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02256</link>
<description rdf:parseType="Literal">&lt;p&gt;Implementing precise detection of oil leaks in peak load equipment through
image analysis can significantly enhance inspection quality and ensure the
system&apos;s safety and reliability. However, challenges such as varying shapes of
oil-stained regions, background noise, and fluctuating lighting conditions
complicate the detection process. To address this, the integration of logical
rule-based discrimination into image recognition has been proposed. This
approach involves recognizing the spatial relationships among objects to
semantically segment images of oil spills using a Mask RCNN network. The
process begins with histogram equalization to enhance the original image,
followed by the use of Mask RCNN to identify the preliminary positions and
outlines of oil tanks, the ground, and areas of potential oil contamination.
Subsequent to this identification, the spatial relationships between these
objects are analyzed. Logical rules are then applied to ascertain whether the
suspected areas are indeed oil spills. This method&apos;s effectiveness has been
confirmed by testing on images captured from peak power equipment in the field.
The results indicate that this approach can adeptly tackle the challenges in
identifying oil-contaminated areas, showing a substantial improvement in
accuracy compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weiying Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xun Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02738">
<title>Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion. (arXiv:2311.02738v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02738</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated creation of synthetic traffic scenarios is a key part of validating
the safety of autonomous vehicles (AVs). In this paper, we propose Scenario
Diffusion, a novel diffusion-based architecture for generating traffic
scenarios that enables controllable scenario generation. We combine latent
diffusion, object detection and trajectory regression to generate distributions
of synthetic agent poses, orientations and trajectories simultaneously. To
provide additional control over the generated scenario, this distribution is
conditioned on a map and sets of tokens describing the desired scenario. We
show that our approach has sufficient expressive capacity to model diverse
traffic patterns and generalizes to different geographical regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pronovost_E/0/1/0/all/0/1&quot;&gt;Ethan Pronovost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesina_M/0/1/0/all/0/1&quot;&gt;Meghana Reddy Ganesina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendy_N/0/1/0/all/0/1&quot;&gt;Noureldin Hendy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Andres Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1&quot;&gt;Nicholas Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05146">
<title>OW-SLR: Overlapping Windows on Semi-Local Region for Image Super-Resolution. (arXiv:2311.05146v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05146</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been considerable progress in implicit neural representation to
upscale an image to any arbitrary resolution. However, existing methods are
based on defining a function to predict the Red, Green and Blue (RGB) value
from just four specific loci. Relying on just four loci is insufficient as it
leads to losing fine details from the neighboring region(s). We show that by
taking into account the semi-local region leads to an improvement in
performance. In this paper, we propose applying a new technique called
Overlapping Windows on Semi-Local Region (OW-SLR) to an image to obtain any
arbitrary resolution by taking the coordinates of the semi-local region around
a point in the latent space. This extracted detail is used to predict the RGB
value of a point. We illustrate the technique by applying the algorithm to the
Optical Coherence Tomography-Angiography (OCT-A) images and show that it can
upscale them to random resolution. This technique outperforms the existing
state-of-the-art methods when applied to the OCT500 dataset. OW-SLR provides
better results for classifying healthy and diseased retinal images such as
diabetic retinopathy and normals from the given set of OCT-A images. The
project page is available at https://rishavbb.github.io/ow-slr/index.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1&quot;&gt;Rishav Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaji_J/0/1/0/all/0/1&quot;&gt;Janarthanam Jothi Balaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_V/0/1/0/all/0/1&quot;&gt;Vasudevan Lakshminarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05836">
<title>UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05836</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qinrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07623">
<title>PadChannel: Improving CNN Performance through Explicit Padding Encoding. (arXiv:2311.07623v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07623</link>
<description rdf:parseType="Literal">&lt;p&gt;In convolutional neural networks (CNNs), padding plays a pivotal role in
preserving spatial dimensions throughout the layers. Traditional padding
techniques do not explicitly distinguish between the actual image content and
the padded regions, potentially causing CNNs to incorrectly interpret the
boundary pixels or regions that resemble boundaries. This ambiguity can lead to
suboptimal feature extraction. To address this, we propose PadChannel, a novel
padding method that encodes padding statuses as an additional input channel,
enabling CNNs to easily distinguish genuine pixels from padded ones. By
incorporating PadChannel into several prominent CNN architectures, we observed
small performance improvements and notable reductions in the variances on the
ImageNet-1K image classification task at marginal increases in the
computational cost. The source code is available at
https://github.com/AussieSeaweed/pad-channel
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juho Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09178">
<title>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution. (arXiv:2311.09178v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09178</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1&quot;&gt;Israa Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1&quot;&gt;Marwah Sulaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehabeldin_Z/0/1/0/all/0/1&quot;&gt;Zahraa Shehabeldin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barakat_M/0/1/0/all/0/1&quot;&gt;Mohammed Barakat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussein_D/0/1/0/all/0/1&quot;&gt;Dareen Hussein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Naggar_M/0/1/0/all/0/1&quot;&gt;Mohammed El-Naggar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1&quot;&gt;Hesham Eraqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1&quot;&gt;Moustafa Youssef&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09574">
<title>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype. (arXiv:2311.09574v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09574</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate classification of lymphoma subtypes using hematoxylin and eosin
(H&amp;amp;E)-stained tissue is complicated by the wide range of morphological features
these cancers can exhibit. We present LymphoML - an interpretable machine
learning method that identifies morphologic features that correlate with
lymphoma subtypes. Our method applies steps to process H&amp;amp;E-stained tissue
microarray cores, segment nuclei and cells, compute features encompassing
morphology, texture, and architecture, and train gradient-boosted models to
make diagnostic predictions. LymphoML&apos;s interpretable models, developed on a
limited volume of H&amp;amp;E-stained tissue, achieve non-inferior diagnostic accuracy
to pathologists using whole-slide images and outperform black box deep-learning
on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using
SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each
feature on model prediction and find that nuclear shape features are most
discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma
(F1-score: 74.5%). Finally, we provide the first demonstration that a model
combining features from H&amp;amp;E-stained tissue with features from a standardized
panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a
46-stain panel (86.1%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vivek Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Vrishab Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Brent Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_O/0/1/0/all/0/1&quot;&gt;Oscar Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rojansky_R/0/1/0/all/0/1&quot;&gt;Rebecca Rojansky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1&quot;&gt;Andrew Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvert_F/0/1/0/all/0/1&quot;&gt;Fabiola Valvert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briercheck_E/0/1/0/all/0/1&quot;&gt;Edward Briercheck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinstock_D/0/1/0/all/0/1&quot;&gt;David Weinstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natkunam_Y/0/1/0/all/0/1&quot;&gt;Yasodha Natkunam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Pol_S/0/1/0/all/0/1&quot;&gt;Sebastian Fernandez-Pol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1&quot;&gt;Pranav Rajpurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10065">
<title>Visual Environment Assessment for Safe Autonomous Quadrotor Landing. (arXiv:2311.10065v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10065</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous identification and evaluation of safe landing zones are of
paramount importance for ensuring the safety and effectiveness of aerial robots
in the event of system failures, low battery, or the successful completion of
specific tasks. In this paper, we present a novel approach for detection and
assessment of potential landing sites for safe quadrotor landing. Our solution
efficiently integrates 2D and 3D environmental information, eliminating the
need for external aids such as GPS and computationally intensive elevation
maps. The proposed pipeline combines semantic data derived from a Neural
Network (NN), to extract environmental features, with geometric data obtained
from a disparity map, to extract critical geometric attributes such as slope,
flatness, and roughness. We define several cost metrics based on these
attributes to evaluate safety, stability, and suitability of regions in the
environments and identify the most suitable landing area. Our approach runs in
real-time on quadrotors equipped with limited computational capabilities.
Experimental results conducted in diverse environments demonstrate that the
proposed method can effectively assess and identify suitable landing areas,
enabling the safe and autonomous landing of a quadrotor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Secchiero_M/0/1/0/all/0/1&quot;&gt;Mattia Secchiero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobbili_N/0/1/0/all/0/1&quot;&gt;Nishanth Bobbili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1&quot;&gt;Giuseppe Loianno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.06905">
<title>Less is More: Proxy Datasets in NAS approaches. (arXiv:2203.06905v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2203.06905</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) defines the design of Neural Networks as a
search problem. Unfortunately, NAS is computationally intensive because of
various possibilities depending on the number of elements in the design and the
possible connections between them. In this work, we extensively analyze the
role of the dataset size based on several sampling approaches for reducing the
dataset size (unsupervised and supervised cases) as an agnostic approach to
reduce search time. We compared these techniques with four common NAS
approaches in NAS-Bench-201 in roughly 1,400 experiments on CIFAR-100. One of
our surprising findings is that in most cases we can reduce the amount of
training data to 25\%, consequently reducing search time to 25\%, while at the
same time maintaining the same accuracy as if training on the full dataset.
Additionally, some designs derived from subsets out-perform designs derived
from the full dataset by up to 22 p.p. accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn Hees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04593">
<title>DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2307.04593v1 [eess.IV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.04593</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces Differential Wavelet Amplifier (DWA), a drop-in module
for wavelet-based image Super-Resolution (SR). DWA invigorates an approach
recently receiving less attention, namely Discrete Wavelet Transformation
(DWT). DWT enables an efficient image representation for SR and reduces the
spatial area of its input by a factor of 4, the overall model size, and
computation cost, framing it as an attractive approach for sustainable ML. Our
proposed DWA model improves wavelet-based SR models by leveraging the
difference between two convolutional filters to refine relevant feature
extraction in the wavelet domain, emphasizing local contrasts and suppressing
common noise in the input signals. We show its effectiveness by integrating it
into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear
improvement in classical SR tasks. Moreover, DWA enables a direct application
of DWSR and MWCNN to input image space, reducing the DWT representation
channel-wise since it omits traditional DWT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Brian B. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Frolov_S/0/1/0/all/0/1&quot;&gt;Stanislav Frolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raue_F/0/1/0/all/0/1&quot;&gt;Federico Raue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Palacio_S/0/1/0/all/0/1&quot;&gt;Sebastian Palacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>