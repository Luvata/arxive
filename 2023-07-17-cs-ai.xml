<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.12319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06913" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.06950">
<title>Pathway toward prior knowledge-integrated machine learning in engineering. (arXiv:2307.06950v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06950</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the digitalization trend and data volume surge, first-principles
models (also known as logic-driven, physics-based, rule-based, or
knowledge-based models) and data-driven approaches have existed in parallel,
mirroring the ongoing AI debate on symbolism versus connectionism. Research for
process development to integrate both sides to transfer and utilize domain
knowledge in the data-driven process is rare. This study emphasizes efforts and
prevailing trends to integrate multidisciplinary domain professions into
machine acknowledgeable, data-driven processes in a two-fold organization:
examining information uncertainty sources in knowledge representation and
exploring knowledge decomposition with a three-tier knowledge-integrated
machine learning paradigm. This approach balances holist and reductionist
perspectives in the engineering domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_P/0/1/0/all/0/1&quot;&gt;Philipp Geyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06951">
<title>AI For Global Climate Cooperation 2023 Competition Proceedings. (arXiv:2307.06951v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06951</link>
<description rdf:parseType="Literal">&lt;p&gt;The international community must collaborate to mitigate climate change and
sustain economic growth. However, collaboration is hard to achieve, partly
because no global authority can ensure compliance with international climate
agreements. Combining AI with climate-economic simulations offers a promising
solution to design international frameworks, including negotiation protocols
and climate agreements, that promote and incentivize collaboration. In
addition, these frameworks should also have policy goals fulfillment, and
sustained commitment, taking into account climate-economic dynamics and
strategic behaviors. These challenges require an interdisciplinary approach
across machine learning, economics, climate science, law, policy, ethics, and
other fields.
&lt;/p&gt;
&lt;p&gt;Towards this objective, we organized AI for Global Climate Cooperation, a
Mila competition in which teams submitted proposals and analyses of
international frameworks, based on (modifications of) RICE-N, an AI-driven
integrated assessment model (IAM). In particular, RICE-N supports modeling
regional decision-making using AI agents. Furthermore, the IAM then models the
climate-economic impact of those decisions into the future.
&lt;/p&gt;
&lt;p&gt;Whereas the first track focused only on performance metrics, the proposals
submitted to the second track were evaluated both quantitatively and
qualitatively. The quantitative evaluation focused on a combination of (i) the
degree of mitigation of global temperature rise and (ii) the increase in
economic productivity. On the other hand, an interdisciplinary panel of human
experts in law, policy, sociology, economics and environmental science,
evaluated the solutions qualitatively. In particular, the panel considered the
effectiveness, simplicity, feasibility, ethics, and notions of climate justice
of the protocols. In the third track, the participants were asked to critique
and improve RICE-N.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Prateek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phade_S/0/1/0/all/0/1&quot;&gt;Soham Phade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1&quot;&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Andrew Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Stephan Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06954">
<title>ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06954</link>
<description rdf:parseType="Literal">&lt;p&gt;Conspiracy Theory Identication task is a new shared task proposed for the
first time at the Evalita 2023. The ACTI challenge, based exclusively on
comments published on conspiratorial channels of telegram, is divided into two
subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial
content and (ii) Conspiratorial Category Classification about specific
conspiracy theory classification. A total of fifteen teams participated in the
task for a total of 81 submissions. We illustrate the best performing
approaches were based on the utilization of large language models. We finally
draw conclusions about the utilization of these models for counteracting the
spreading of misinformation in online platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1&quot;&gt;Giuseppe Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1&quot;&gt;Niklas Stoehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1&quot;&gt;Manoel Horta Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06962">
<title>Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06962</link>
<description rdf:parseType="Literal">&lt;p&gt;The dominant text generation models compose the output by sequentially
selecting words from a fixed vocabulary. In this paper, we formulate text
generation as progressively copying text segments (e.g., words or phrases) from
an existing text collection. We compute the contextualized representations of
meaningful text segments and index them using efficient vector search toolkits.
The task of text generation is then decomposed into a series of copy-and-paste
operations: at each time step, we seek suitable text spans from the text
collection rather than selecting from a standalone vocabulary. Experiments on
the standard language modeling benchmark (WikiText-103) show that our approach
achieves better generation quality according to both automatic and human
evaluations. Besides, its inference efficiency is comparable to token-level
autoregressive models thanks to the reduction of decoding steps. We also show
that our approach allows for effective domain adaptation by simply switching to
domain-specific text collection without extra training. Finally, we observe
that our approach attains additional performance gains by simply scaling up to
larger text collections, again without further training.\footnote{Our source
codes are publicly available at
\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xian-Ling Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06963">
<title>Is Task-Agnostic Explainable AI a Myth?. (arXiv:2307.06963v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06963</link>
<description rdf:parseType="Literal">&lt;p&gt;Our work serves as a framework for unifying the challenges of contemporary
explainable AI (XAI). We demonstrate that while XAI methods provide
supplementary and potentially useful output for machine learning models,
researchers and decision-makers should be mindful of their conceptual and
technical limitations, which frequently result in these methods themselves
becoming black boxes. We examine three XAI research avenues spanning image,
textual, and graph data, covering saliency, attention, and graph-type
explainers. Despite the varying contexts and timeframes of the mentioned cases,
the same persistent roadblocks emerge, highlighting the need for a conceptual
breakthrough in the field to address the challenge of compatibility between XAI
methods and application tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaszczewicz_A/0/1/0/all/0/1&quot;&gt;Alicja Chaszczewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06970">
<title>Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens. (arXiv:2307.06970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06970</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the application of supervised machine learning
algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic
Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM)
process. A total of 31 PLA specimens were prepared, with Infill Percentage,
Layer Height, Print Speed, and Extrusion Temperature serving as input
parameters. The primary objective was to assess the accuracy and effectiveness
of four distinct supervised classification algorithms, namely Logistic
Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest
Neighbor, in predicting the UTS of the specimens. The results revealed that
while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1
score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC)
score of 0.79, outperforming the other algorithms. This demonstrates the
superior ability of the KNN algorithm in differentiating between the two
classes of ultimate tensile strength within the dataset, rendering it the most
favorable choice for classification in the context of this research. This study
represents the first attempt to estimate the UTS of PLA specimens using machine
learning-based classification algorithms, and the findings offer valuable
insights into the potential of these techniques in improving the performance
and accuracy of predictive models in the domain of additive manufacturing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Akshansh Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1&quot;&gt;Vijaykumar S Jatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06971">
<title>Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2307.06971</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate explainability via short Boolean formulas in the data model
based on unary relations. As an explanation of length k, we take a Boolean
formula of length k that minimizes the error with respect to the target
attribute to be explained. We first provide novel quantitative bounds for the
expected error in this scenario. We then also demonstrate how the setting works
in practice by studying three concrete data sets. In each case, we calculate
explanation formulas of different lengths using an encoding in Answer Set
Programming. The most accurate formulas we obtain achieve errors similar to
other methods on the same data sets. However, due to overfitting, these
formulas are not necessarily ideal explanations, so we use cross validation to
identify a suitable length for explanations. By limiting to shorter formulas,
we obtain explanations that avoid overfitting but are still reasonably accurate
and also, importantly, human interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_R/0/1/0/all/0/1&quot;&gt;Reijo Jaakkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janhunen_T/0/1/0/all/0/1&quot;&gt;Tomi Janhunen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuusisto_A/0/1/0/all/0/1&quot;&gt;Antti Kuusisto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rankooh_M/0/1/0/all/0/1&quot;&gt;Masood Feyzbakhsh Rankooh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilander_M/0/1/0/all/0/1&quot;&gt;Miikka Vilander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06983">
<title>IR Design for Application-Specific Natural Language: A Case Study on Traffic Data. (arXiv:2307.06983v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.06983</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of software applications in the transportation industry,
Domain-Specific Languages (DSLs) have enjoyed widespread adoption due to their
ease of use and various other benefits. With the ceaseless progress in computer
performance and the rapid development of large-scale models, the possibility of
programming using natural language in specified applications - referred to as
Application-Specific Natural Language (ASNL) - has emerged. ASNL exhibits
greater flexibility and freedom, which, in turn, leads to an increase in
computational complexity for parsing and a decrease in processing performance.
To tackle this issue, our paper advances a design for an intermediate
representation (IR) that caters to ASNL and can uniformly process
transportation data into graph data format, improving data processing
performance. Experimental comparisons reveal that in standard data query
operations, our proposed IR design can achieve a speed improvement of over
forty times compared to direct usage of standard XML format data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Ding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shengyue Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zuqiu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei-Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yilun Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07002">
<title>Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks. (arXiv:2307.07002v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07002</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art models can perform well in controlled environments, but they
often struggle when presented with out-of-distribution (OOD) examples, making
OOD detection a critical component of NLP systems. In this paper, we focus on
highlighting the limitations of existing approaches to OOD detection in NLP.
Specifically, we evaluated eight OOD detection methods that are easily
integrable into existing NLP systems and require no additional OOD data or
model modifications. One of our contributions is providing a well-structured
research environment that allows for full reproducibility of the results.
Additionally, our analysis shows that existing OOD detection methods for NLP
tasks are not yet sufficiently sensitive to capture all samples characterized
by various types of distributional shifts. Particularly challenging testing
scenarios arise in cases of background shift and randomly shuffled word order
within in domain texts. This highlights the need for future work to develop
more effective OOD detection approaches for the NLP problems, and our work
provides a well-defined foundation for further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1&quot;&gt;Mateusz Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baran_J/0/1/0/all/0/1&quot;&gt;Joanna Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1&quot;&gt;Mateusz W&amp;#xf3;jcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1&quot;&gt;Maciej Zi&amp;#x119;ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonczarek_A/0/1/0/all/0/1&quot;&gt;Adam Gonczarek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07014">
<title>Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07014</link>
<description rdf:parseType="Literal">&lt;p&gt;Off-policy evaluation (OPE) aims to estimate the benefit of following a
counterfactual sequence of actions, given data collected from executed
sequences. However, existing OPE estimators often exhibit high bias and high
variance in problems involving large, combinatorial action spaces. We
investigate how to mitigate this issue using factored action spaces i.e.
expressing each action as a combination of independent sub-actions from smaller
action spaces. This approach facilitates a finer-grained analysis of how
actions differ in their effects. In this work, we propose a new family of
&quot;decomposed&quot; importance sampling (IS) estimators based on factored action
spaces. Given certain assumptions on the underlying problem structure, we prove
that the decomposed IS estimators have less variance than their original
non-decomposed versions, while preserving the property of zero bias. Through
simulations, we empirically verify our theoretical results, probing the
validity of various assumptions. Provided with a technique that can derive the
action space factorisation for a given problem, our work shows that OPE can be
improved &quot;for free&quot; by utilising this inherent problem structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebello_A/0/1/0/all/0/1&quot;&gt;Aaman Rebello&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shengpu Tang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parbhoo_S/0/1/0/all/0/1&quot;&gt;Sonali Parbhoo&lt;/a&gt; (1) ((1) Department of Engineering, Imperial College London, (2) Division of Computer Science &amp;amp; Engineering, University of Michigan)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07046">
<title>A metric learning approach for endoscopic kidney stone identification. (arXiv:2307.07046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07046</link>
<description rdf:parseType="Literal">&lt;p&gt;Several Deep Learning (DL) methods have recently been proposed for an
automated identification of kidney stones during an ureteroscopy to enable
rapid therapeutic decisions. Even if these DL approaches led to promising
results, they are mainly appropriate for kidney stone types for which numerous
labelled data are available. However, only few labelled images are available
for some rare kidney stone types. This contribution exploits Deep Metric
Learning (DML) methods i) to handle such classes with few samples, ii) to
generalize well to out of distribution samples, and iii) to cope better with
new classes which are added to the database. The proposed Guided Deep Metric
Learning approach is based on a novel architecture which was designed to learn
data representations in an improved way. The solution was inspired by Few-Shot
Learning (FSL) and makes use of a teacher-student approach. The teacher model
(GEMINI) generates a reduced hypothesis space based on prior knowledge from the
labeled data, and is used it as a guide to a student model (i.e., ResNet50)
through a Knowledge Distillation scheme. Extensive tests were first performed
on two datasets separately used for the recognition, namely a set of images
acquired for the surfaces of the kidney stone fragments, and a set of images of
the fragment sections. The proposed DML-approach improved the identification
accuracy by 10% and 12% in comparison to DL-methods and other DML-approaches,
respectively. Moreover, model embeddings from the two dataset types were merged
in an organized way through a multi-view scheme to simultaneously exploit the
information of surface and section fragments. Test with the resulting mixed
model improves the identification accuracy by at least 3% and up to 30% with
respect to DL-models and shallow machine learning methods, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_J/0/1/0/all/0/1&quot;&gt;Jorge Gonzalez-Zapata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Tiro_F/0/1/0/all/0/1&quot;&gt;Francisco Lopez-Tiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villalvazo_Avila_E/0/1/0/all/0/1&quot;&gt;Elias Villalvazo-Avila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flores_Araiza_D/0/1/0/all/0/1&quot;&gt;Daniel Flores-Araiza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubert_J/0/1/0/all/0/1&quot;&gt;Jacques Hubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1&quot;&gt;Andres Mendez-Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1&quot;&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daul_C/0/1/0/all/0/1&quot;&gt;Christian Daul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07059">
<title>Vertex-based Networks to Accelerate Path Planning Algorithms. (arXiv:2307.07059v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07059</link>
<description rdf:parseType="Literal">&lt;p&gt;Path planning plays a crucial role in various autonomy applications, and RRT*
is one of the leading solutions in this field. In this paper, we propose the
utilization of vertex-based networks to enhance the sampling process of RRT*,
leading to more efficient path planning. Our approach focuses on critical
vertices along the optimal paths, which provide essential yet sparser
abstractions of the paths. We employ focal loss to address the associated data
imbalance issue, and explore different masking configurations to determine
practical tradeoffs in system performance. Through experiments conducted on
randomly generated floor maps, our solutions demonstrate significant speed
improvements, achieving over a 400% enhancement compared to the baseline model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jundong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07084">
<title>Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07084</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning or optimal control can provide effective reasoning for
sequential decision-making problems with variable dynamics. Such reasoning in
practical implementation, however, poses a persistent challenge in interpreting
the reward function and corresponding optimal policy. Consequently, formalizing
the sequential decision-making problems as inference has a considerable value,
as probabilistic inference in principle offers diverse and powerful
mathematical tools to infer the stochastic dynamics whilst suggesting a
probabilistic interpretation of the reward design and policy convergence. In
this study, we propose a novel Adaptive Wasserstein Variational Optimization
(AWaVO) to tackle these challenges in sequential decision-making. Our approach
utilizes formal methods to provide interpretations of reward design,
transparency of training convergence, and probabilistic interpretation of
sequential decisions. To demonstrate practicality, we show convergent training
with guaranteed global convergence rates not only in simulation but also in
real robot tasks, and empirically verify a reasonable tradeoff between high
performance and conservative interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyle_D/0/1/0/all/0/1&quot;&gt;David Boyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07085">
<title>Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/2307.07085</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular mechanics (MM) force fields -- the models that characterize the
energy landscape of molecular systems via simple pairwise and polynomial terms
-- have traditionally relied on human expert-curated, inflexible, and poorly
extensible discrete chemical parameter assignment rules, namely atom or valence
types. Recently, there has been significant interest in using graph neural
networks to replace this process, while enabling the parametrization scheme to
be learned in an end-to-end differentiable manner directly from quantum
chemical calculations or condensed-phase data. In this paper, we extend the
Espaloma end-to-end differentiable force field construction approach by
incorporating both energy and force fitting directly to quantum chemical data
into the training process. Building on the OpenMM SPICE dataset, we curate a
dataset containing chemical spaces highly relevant to the broad interest of
biomolecular modeling, covering small molecules, proteins, and RNA. The
resulting force field, espaloma 0.3.0, self-consistently parametrizes these
diverse biomolecular species, accurately predicts quantum chemical energies and
forces, and maintains stable quantum chemical energy-minimized geometries.
Surprisingly, this simple approach produces highly accurate protein-ligand
binding free energies when self-consistently parametrizing protein and ligand.
This approach -- capable of fitting new force fields to large quantum chemical
datasets in one GPU-day -- shows significant promise as a path forward for
building systematically more accurate force fields that can be easily extended
to new chemical domains of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Takaba_K/0/1/0/all/0/1&quot;&gt;Kenichiro Takaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pulido_I/0/1/0/all/0/1&quot;&gt;Iv&amp;#xe1;n Pulido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Henry_M/0/1/0/all/0/1&quot;&gt;Mike Henry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+MacDermott_Opeskin_H/0/1/0/all/0/1&quot;&gt;Hugo MacDermott-Opeskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chodera_J/0/1/0/all/0/1&quot;&gt;John D. Chodera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanqing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07091">
<title>Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07091</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is a promising direction that allows RL
agents to pre-train on large datasets, avoiding the recurrence of expensive
data collection. To advance the field, it is crucial to generate large-scale
datasets. Compositional RL is particularly appealing for generating such large
datasets, since 1) it permits creating many tasks from few components, 2) the
task structure may enable trained agents to solve new tasks by combining
relevant learned components, and 3) the compositional dimensions provide a
notion of task relatedness. This paper provides four offline RL datasets for
simulated robotic manipulation created using the 256 tasks from CompoSuite
[Mendez et al., 2022a]. Each dataset is collected from an agent with a
different degree of performance, and consists of 256 million transitions. We
provide training and evaluation settings for assessing an agent&apos;s ability to
learn compositional task policies. Our benchmarking experiments on each setting
show that current offline RL methods can learn the training tasks to some
extent and that compositional methods significantly outperform
non-compositional methods. However, current methods are still unable to extract
the tasks&apos; compositional structure to generalize to unseen tasks, showing a
need for further research in offline compositional RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussing_M/0/1/0/all/0/1&quot;&gt;Marcel Hussing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_J/0/1/0/all/0/1&quot;&gt;Jorge A. Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singrodia_A/0/1/0/all/0/1&quot;&gt;Anisha Singrodia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_C/0/1/0/all/0/1&quot;&gt;Cassandra Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1&quot;&gt;Eric Eaton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07119">
<title>DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07119</link>
<description rdf:parseType="Literal">&lt;p&gt;Current automated machine learning (ML) tools are model-centric, focusing on
model selection and parameter optimization. However, the majority of the time
in data analysis is devoted to data cleaning and wrangling, for which limited
tools are available. Here we present DataAssist, an automated data preparation
and cleaning platform that enhances dataset quality using ML-informed methods.
We show that DataAssist provides a pipeline for exploratory data analysis and
data cleaning, including generating visualization for user-selected variables,
unifying data annotation, suggesting anomaly removal, and preprocessing data.
The exported dataset can be readily integrated with other autoML tools or
user-specified model for downstream analysis. Our data-centric tool is
applicable to a variety of fields, including economics, business, and
forecasting applications saving over 50\% time of the time spent on data
cleansing and preparation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1&quot;&gt;Kartikay Goyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1&quot;&gt;Vakul Goyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07134">
<title>Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms. (arXiv:2307.07134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07134</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms have become ubiquitous in a number of
applications (e.g. image classification). However, due to the insufficient
measurement of traditional metrics (e.g. the coarse-grained Accuracy of each
classifier), substantial gaps are usually observed between the real-world
performance of these algorithms and their scores in standardized evaluations.
In this paper, inspired by the psychometric theories from human measurement, we
propose a task-agnostic evaluation framework Camilla, where a multi-dimensional
diagnostic metric Ability is defined for collaboratively measuring the
multifaceted strength of each machine learning algorithm. Specifically, given
the response logs from different algorithms to data samples, we leverage
cognitive diagnosis assumptions and neural networks to learn the complex
interactions among algorithms, samples and the skills (explicitly or implicitly
pre-defined) of each sample. In this way, both the abilities of each algorithm
on multiple skills and some of the sample factors (e.g. sample difficulty) can
be simultaneously quantified. We conduct extensive experiments with hundreds of
machine learning algorithms on four public datasets, and our experimental
results demonstrate that Camilla not only can capture the pros and cons of each
algorithm more precisely, but also outperforms state-of-the-art baselines on
the metric reliability, rank consistency and rank stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Zheng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhenya Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuanren Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07146">
<title>Federated Learning-Empowered AI-Generated Content in Wireless Networks. (arXiv:2307.07146v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.07146</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence generated content (AIGC) has emerged as a promising
technology to improve the efficiency, quality, diversity and flexibility of the
content creation process by adopting a variety of generative AI models.
Deploying AIGC services in wireless networks has been expected to enhance the
user experience. However, the existing AIGC service provision suffers from
several limitations, e.g., the centralized training in the pre-training,
fine-tuning and inference processes, especially their implementations in
wireless networks with privacy preservation. Federated learning (FL), as a
collaborative learning framework where the model training is distributed to
cooperative data owners without the need for data sharing, can be leveraged to
simultaneously improve learning efficiency and achieve privacy protection for
AIGC. To this end, we present FL-based techniques for empowering AIGC, and aim
to enable users to generate diverse, personalized, and high-quality content.
Furthermore, we conduct a case study of FL-aided AIGC fine-tuning by using the
state-of-the-art AIGC model, i.e., stable diffusion model. Numerical results
show that our scheme achieves advantages in effectively reducing the
communication cost and training latency and privacy protection. Finally, we
highlight several major research directions and open issues for the convergence
of FL and AIGC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xumin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peichun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hongyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiawen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong In Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07167">
<title>Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07167</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial Training (AT) has been found to substantially improve the
robustness of deep learning classifiers against adversarial attacks. AT
involves obtaining robustness by including adversarial examples in training a
classifier. Most variants of AT algorithms treat every training example
equally. However, recent works have shown that better performance is achievable
by treating them unequally. In addition, it has been observed that AT exerts an
uneven influence on different classes in a training set and unfairly hurts
examples corresponding to classes that are inherently harder to classify.
Consequently, various reweighting schemes have been proposed that assign
unequal weights to robust losses of individual examples in a training set. In
this work, we propose a novel instance-wise reweighting scheme. It considers
the vulnerability of each natural example and the resulting information loss on
its adversarial counterpart occasioned by adversarial attacks. Through
extensive experiments, we show that our proposed method significantly improves
over existing reweighting schemes, especially against strong white and
black-box attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakorede_O/0/1/0/all/0/1&quot;&gt;Olukorede Fakorede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nirala_A/0/1/0/all/0/1&quot;&gt;Ashutosh Kumar Nirala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atsague_M/0/1/0/all/0/1&quot;&gt;Modeste Atsague&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jin Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07176">
<title>Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07176</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread application of Reinforcement Learning (RL) in real-world
situations is yet to come to fruition, largely as a result of its failure to
satisfy the essential safety demands of such systems. Existing safe
reinforcement learning (SafeRL) methods, employing cost functions to enhance
safety, fail to achieve zero-cost in complex scenarios, including vision-only
tasks, even with comprehensive data sampling and training. To address this, we
introduce Safe DreamerV3, a novel algorithm that integrates both
Lagrangian-based and planning-based methods within a world model. Our
methodology represents a significant advancement in SafeRL as the first
algorithm to achieve nearly zero-cost in both low-dimensional and vision-only
tasks within the Safety-Gymnasium benchmark. Our project website can be found
in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weidong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chunhe Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07177">
<title>TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction. (arXiv:2307.07177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07177</link>
<description rdf:parseType="Literal">&lt;p&gt;The prediction of mild cognitive impairment (MCI) conversion to Alzheimer&apos;s
disease (AD) is important for early treatment to prevent or slow the
progression of AD. To accurately predict the MCI conversion to stable MCI or
progressive MCI, we propose Triformer, a novel transformer-based framework with
three specialized transformers to incorporate multi-model data. Triformer uses
I) an image transformer to extract multi-view image features from medical
scans, II) a clinical transformer to embed and correlate multi-modal clinical
data, and III) a modality fusion transformer that produces an accurate
prediction based on fusing the outputs from the image and clinical
transformers. Triformer is evaluated on the Alzheimer&apos;s Disease Neuroimaging
Initiative (ANDI)1 and ADNI2 datasets and outperforms previous state-of-the-art
single and multi-modal methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Linfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Junyan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaoying Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1&quot;&gt;Shekhar S. Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrallah_F/0/1/0/all/0/1&quot;&gt;Fatima A. Nasrallah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07189">
<title>Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07189</link>
<description rdf:parseType="Literal">&lt;p&gt;Even nowadays, where Deep Learning (DL) has achieved state-of-the-art
performance in a wide range of research domains, accelerating training and
building robust DL models remains a challenging task. To this end, generations
of researchers have pursued to develop robust methods for training DL
architectures that can be less sensitive to weight distributions, model
architectures and loss landscapes. However, such methods are limited to
adaptive learning rate optimizers, initialization schemes, and clipping
gradients without investigating the fundamental rule of parameters update.
Although multiplicative updates have contributed significantly to the early
development of machine learning and hold strong theoretical claims, to best of
our knowledge, this is the first work that investigate them in context of DL
training acceleration and robustness. In this work, we propose an optimization
framework that fits to a wide range of optimization algorithms and enables one
to apply alternative update rules. To this end, we propose a novel
multiplicative update rule and we extend their capabilities by combining it
with a traditional additive update term, under a novel hybrid update method. We
claim that the proposed framework accelerates training, while leading to more
robust models in contrast to traditionally used additive update rule and we
experimentally demonstrate their effectiveness in a wide range of task and
optimization methods. Such tasks ranging from convex and non-convex
optimization to difficult image classification benchmarks applying a wide range
of traditionally used optimization methods and Deep Neural Network (DNN)
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirtas_M/0/1/0/all/0/1&quot;&gt;Manos Kirtas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Passalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1&quot;&gt;Anastasios Tefas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07248">
<title>Rigorous Runtime Analysis of Diversity Optimization with GSEMO on OneMinMax. (arXiv:2307.07248v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.07248</link>
<description rdf:parseType="Literal">&lt;p&gt;The evolutionary diversity optimization aims at finding a diverse set of
solutions which satisfy some constraint on their fitness. In the context of
multi-objective optimization this constraint can require solutions to be
Pareto-optimal. In this paper we study how the GSEMO algorithm with additional
diversity-enhancing heuristic optimizes a diversity of its population on a
bi-objective benchmark problem OneMinMax, for which all solutions are
Pareto-optimal.
&lt;/p&gt;
&lt;p&gt;We provide a rigorous runtime analysis of the last step of the optimization,
when the algorithm starts with a population with a second-best diversity, and
prove that it finds a population with optimal diversity in expected time
$O(n^2)$, when the problem size $n$ is odd. For reaching our goal, we analyse
the random walk of the population, which reflects the frequency of changes in
the population and their outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antipov_D/0/1/0/all/0/1&quot;&gt;Denis Antipov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_A/0/1/0/all/0/1&quot;&gt;Aneta Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_F/0/1/0/all/0/1&quot;&gt;Frank Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07250">
<title>Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byung-Kwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07255">
<title>Dialogue Agents 101: A Beginner&apos;s Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07255</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharing ideas through communication with peers is the primary mode of human
interaction. Consequently, extensive research has been conducted in the area of
conversational AI, leading to an increase in the availability and diversity of
conversational tasks, datasets, and methods. However, with numerous tasks being
explored simultaneously, the current landscape of conversational AI becomes
fragmented. Therefore, initiating a well-thought-out model for a dialogue agent
can pose significant challenges for a practitioner. Towards highlighting the
critical ingredients needed for a practitioner to design a dialogue agent from
scratch, the current study provides a comprehensive overview of the primary
characteristics of a dialogue agent, the supporting tasks, their corresponding
open-domain datasets, and the methods used to benchmark these datasets. We
observe that different methods have been used to tackle distinct dialogue
tasks. However, building separate models for each task is costly and does not
leverage the correlation among the several tasks of a dialogue agent. As a
result, recent trends suggest a shift towards building unified foundation
models. To this end, we propose UNIT, a UNified dIalogue dataseT constructed
from conversations of existing datasets for different dialogue tasks capturing
the nuances for each of them. We also examine the evaluation strategies used to
measure the performance of dialogue agents and highlight the scope for future
research in the area of conversational AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Shivani Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1&quot;&gt;Sumit Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1&quot;&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07260">
<title>A Dynamic Points Removal Benchmark in Point Cloud Maps. (arXiv:2307.07260v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07260</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of robotics, the point cloud has become an essential map
representation. From the perspective of downstream tasks like localization and
global path planning, points corresponding to dynamic objects will adversely
affect their performance. Existing methods for removing dynamic points in point
clouds often lack clarity in comparative evaluations and comprehensive
analysis. Therefore, we propose an easy-to-extend unified benchmarking
framework for evaluating techniques for removing dynamic points in maps. It
includes refactored state-of-art methods and novel metrics to analyze the
limitations of these approaches. This enables researchers to dive deep into the
underlying reasons behind these limitations. The benchmark makes use of several
datasets with different sensor types. All the code and datasets related to our
study are publicly available for further development and utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duberg_D/0/1/0/all/0/1&quot;&gt;Daniel Duberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1&quot;&gt;Ruoyu Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1&quot;&gt;Mingkai Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lujia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensfelt_P/0/1/0/all/0/1&quot;&gt;Patric Jensfelt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07265">
<title>AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023. (arXiv:2307.07265v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.07265</link>
<description rdf:parseType="Literal">&lt;p&gt;This report presents the technical details of our submission to the 2023
Epic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. The
task is to learn the mapping from audio samples to their corresponding action
labels. To achieve this goal, we propose a simple yet effective single-stream
CNN-based architecture called AudioInceptionNeXt that operates on the
time-frequency log-mel-spectrogram of the audio samples. Motivated by the
design of the InceptionNeXt, we propose parallel multi-scale depthwise
separable convolutional kernels in the AudioInceptionNeXt block, which enable
the model to learn the time and frequency information more effectively. The
large-scale separable kernels capture the long duration of activities and the
global frequency semantic information, while the small-scale separable kernels
capture the short duration of activities and local details of frequency
information. Our approach achieved 55.43% of top-1 accuracy on the challenge
test set, ranked as 1st on the public leaderboard. Codes are available
anonymously at https://github.com/StevenLauHKHK/AudioInceptionNeXt.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_K/0/1/0/all/0/1&quot;&gt;Kin Wai Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehman_Y/0/1/0/all/0/1&quot;&gt;Yasar Abbas Ur Rehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lan Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07286">
<title>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching. (arXiv:2307.07286v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07286</link>
<description rdf:parseType="Literal">&lt;p&gt;One-shot skeleton action recognition, which aims to learn a skeleton action
recognition model with a single training sample, has attracted increasing
interest due to the challenge of collecting and annotating large-scale skeleton
action data. However, most existing studies match skeleton sequences by
comparing their feature vectors directly which neglects spatial structures and
temporal orders of skeleton data. This paper presents a novel one-shot skeleton
action recognition technique that handles skeleton action recognition via
multi-scale spatial-temporal feature matching. We represent skeleton data at
multiple spatial and temporal scales and achieve optimal feature matching from
two perspectives. The first is multi-scale matching which captures the
scale-wise semantic relevance of skeleton data at multiple spatial and temporal
scales simultaneously. The second is cross-scale matching which handles
different motion magnitudes and speeds by capturing sample-wise relevance
across multiple scales. Extensive experiments over three large-scale datasets
(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior
one-shot skeleton action recognition, and it outperforms the state-of-the-art
consistently by large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Siyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwa_E/0/1/0/all/0/1&quot;&gt;Er Meng Hwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex C. Kot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07306">
<title>C3: Zero-shot Text-to-SQL with ChatGPT. (arXiv:2307.07306v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07306</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,
which achieves 82.3\% in terms of execution accuracy on the holdout test set of
Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the
Spider Challenge. C3 consists of three key components: Clear Prompting (CP),
Calibration with Hints (CH), and Consistent Output (CO), which are
corresponding to the model input, model bias and model output respectively. It
provides a systematic treatment for zero-shot Text-to-SQL. Extensive
experiments have been conducted to verify the effectiveness and efficiency of
our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xuemei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuhang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuren Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunjun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_l/0/1/0/all/0/1&quot;&gt;lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jinshu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1&quot;&gt;Dongfang Lou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07325">
<title>Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.07325</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation learning of speech, without textual resources, is an area
of significant interest for many low resource speech applications. In this
paper, we describe an approach to self-supervised representation learning from
raw audio using a hidden unit clustering (HUC) framework. The input to the
model consists of audio samples that are windowed and processed with 1-D
convolutional layers. The learned &quot;time-frequency&quot; representations from the
convolutional neural network (CNN) module are further processed with long short
term memory (LSTM) layers which generate a contextual vector representation for
every windowed segment. The HUC framework, allowing the categorization of the
representations into a small number of phoneme-like units, is used to train the
model for learning semantically rich speech representations. The targets
consist of phoneme-like pseudo labels for each audio segment and these are
generated with an iterative k-means algorithm. We explore techniques that
improve the speaker invariance of the learned representations and illustrate
the effectiveness of the proposed approach on two settings, i) completely
unsupervised speech applications on the sub-tasks described as part of the
ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition
(ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi
dataset. In these experiments, we achieve state-of-art results for various
ZeroSpeech tasks. Further, on the ASR experiments, the HUC representations are
shown to improve significantly over other established benchmarks based on
Wav2vec, HuBERT and Best-RQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Varun Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sai_T/0/1/0/all/0/1&quot;&gt;Tarun Sai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1&quot;&gt;Sriram Ganapathy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07367">
<title>Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow. (arXiv:2307.07367v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.07367</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models like ChatGPT efficiently provide users with information
about various topics, presenting a potential substitute for searching the web
and asking people for help online. But since users interact privately with the
model, these models may drastically reduce the amount of publicly available
human-generated data and knowledge resources. This substitution can present a
significant problem in securing training data for future models. In this work,
we investigate how the release of ChatGPT changed human-generated open data on
the web by analyzing the activity on Stack Overflow, the leading online Q\&amp;amp;A
platform for computer programming. We find that relative to its Russian and
Chinese counterparts, where access to ChatGPT is limited, and to similar forums
for mathematics, where ChatGPT is less capable, activity on Stack Overflow
significantly decreased. A difference-in-differences model estimates a 16\%
decrease in weekly posts on Stack Overflow. This effect increases in magnitude
over time, and is larger for posts related to the most widely used programming
languages. Posts made after ChatGPT get similar voting scores than before,
suggesting that ChatGPT is not merely displacing duplicate or low-quality
content. These results suggest that more users are adopting large language
models to answer questions and they are better substitutes for Stack Overflow
for languages for which they have more training data. Using models like ChatGPT
may be more efficient for solving certain programming problems, but its
widespread adoption and the resulting shift away from public exchange on the
web will limit the open data people and models can learn from in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rio_Chanona_M/0/1/0/all/0/1&quot;&gt;Maria del Rio-Chanona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurentsyeva_N/0/1/0/all/0/1&quot;&gt;Nadzeya Laurentsyeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wachs_J/0/1/0/all/0/1&quot;&gt;Johannes Wachs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07370">
<title>AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes. (arXiv:2307.07370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07370</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning is a significant field across computer vision and natural
language processing. We propose and present AIC-AB NET, a novel
Attribute-Information-Combined Attention-Based Network that combines spatial
attention architecture and text attributes in an encoder-decoder. For caption
generation, adaptive spatial attention determines which image region best
represents the image and whether to attend to the visual features or the visual
sentinel. Text attribute information is synchronously fed into the decoder to
help image recognition and reduce uncertainty. We have tested and evaluated our
AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The
Fashion dataset is employed as a benchmark of single-object images. The results
show the superior performance of the proposed model compared to the
state-of-the-art baseline and ablated models on both the images from MSCOCO and
our single-object images. Our AIC-AB NET outperforms the baseline adaptive
attention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095
(CIDEr score) on the Fashion dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_G/0/1/0/all/0/1&quot;&gt;Guoyun Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlassov_V/0/1/0/all/0/1&quot;&gt;Vladimir Vlassov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07392">
<title>Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing need for text summarization techniques that are both
efficient and accurate, it becomes crucial to explore avenues that enhance the
quality and precision of pre-trained models specifically tailored for
summarizing Bengali texts. When it comes to text summarization tasks, there are
numerous pre-trained transformer models at one&apos;s disposal. Consequently, it
becomes quite a challenge to discern the most informative and relevant summary
for a given text among the various options generated by these pre-trained
summarization models. This paper aims to identify the most accurate and
informative summary for a given text by utilizing a simple but effective
ranking-based approach that compares the output of four different pre-trained
Bengali text summarization models. The process begins by carrying out
preprocessing of the input text that involves eliminating unnecessary elements
such as special characters and punctuation marks. Next, we utilize four
pre-trained summarization models to generate summaries, followed by applying a
text ranking algorithm to identify the most suitable summary. Ultimately, the
summary with the highest ranking score is chosen as the final one. To evaluate
the effectiveness of this approach, the generated summaries are compared
against human-annotated summaries using standard NLG metrics such as BLEU,
ROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by
leveraging the strengths of each pre-trained transformer model and combining
them using a ranking-based approach, our methodology significantly improves the
accuracy and effectiveness of the Bengali text summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1&quot;&gt;G. M. Shahariar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukder_T/0/1/0/all/0/1&quot;&gt;Tonmoy Talukder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sotez_R/0/1/0/all/0/1&quot;&gt;Rafin Alam Khan Sotez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1&quot;&gt;Md. Tanvir Rouf Shawon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07409">
<title>KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07409</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce CheXOFA, a new pre-trained vision-language model
(VLM) for the chest X-ray domain. Our model is initially pre-trained on various
multimodal datasets within the general domain before being transferred to the
chest X-ray domain. Following a prominent VLM, we unify various domain-specific
tasks into a simple sequence-to-sequence schema. It enables the model to
effectively learn the required knowledge and skills from limited resources in
the domain. Demonstrating superior performance on the benchmark datasets
provided by the BioNLP shared task, our model benefits from its training across
multiple tasks and domains. With subtle techniques including ensemble and
factual calibration, our system achieves first place on the RadSum23
leaderboard for the hidden test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gangwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hajung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seongsu Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chanhwi Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1&quot;&gt;Mujeen Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Eric Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07413">
<title>Exploiting Counter-Examples for Active Learning with Partial labels. (arXiv:2307.07413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07413</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a new problem, \emph{active learning with partial labels}
(ALPL). In this setting, an oracle annotates the query samples with partial
labels, relaxing the oracle from the demanding accurate labeling process. To
address ALPL, we first build an intuitive baseline that can be seamlessly
incorporated into existing AL frameworks. Though effective, this baseline is
still susceptible to the \emph{overfitting}, and falls short of the
representative partial-label-based samples during the query process. Drawing
inspiration from human inference in cognitive science, where accurate
inferences can be explicitly derived from \emph{counter-examples} (CEs), our
objective is to leverage this human-like learning pattern to tackle the
\emph{overfitting} while enhancing the process of selecting representative
samples in ALPL. Specifically, we construct CEs by reversing the partial labels
for each instance, and then we propose a simple but effective WorseNet to
directly learn from this complementary pattern. By leveraging the distribution
gap between WorseNet and the predictor, this adversarial evaluation manner
could enhance both the performance of the predictor itself and the sample
selection process, allowing the predictor to capture more accurate patterns in
the data. Experimental results on five real-world datasets and four benchmark
datasets show that our proposed method achieves comprehensive improvements over
ten representative AL frameworks, highlighting the superiority of WorseNet. The
source code will be available at \url{https://github.com/Ferenas/APLL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jieming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalander_M/0/1/0/all/0/1&quot;&gt;Marcus Kalander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07415">
<title>AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07415</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents AutoHint, a novel framework for automatic prompt
engineering and optimization for Large Language Models (LLM). While LLMs have
demonstrated remarkable ability in achieving high-quality annotation in various
tasks, the key to applying this ability to specific tasks lies in developing
high-quality prompts. Thus we propose a framework to inherit the merits of both
in-context learning and zero-shot learning by incorporating enriched
instructions derived from input-output demonstrations to optimize original
prompt. We refer to the enrichment as the hint and propose a framework to
automatically generate the hint from labeled data. More concretely, starting
from an initial prompt, our method first instructs a LLM to deduce new hints
for selected samples from incorrect predictions, and then summarizes from
per-sample hints and adds the results back to the initial prompt to form a new,
enriched instruction. The proposed method is evaluated on the BIG-Bench
Instruction Induction dataset for both zero-shot and few-short prompts, where
experiments demonstrate our method is able to significantly boost accuracy for
multiple tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinchuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homma_Y/0/1/0/all/0/1&quot;&gt;Youkow Homma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jian Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1&quot;&gt;Denis Charles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07417">
<title>RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07417</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation has been widely used in low-resource NER tasks to tackle
the problem of data sparsity. However, previous data augmentation methods have
the disadvantages of disrupted syntactic structures, token-label mismatch, and
requirement for external knowledge or manual effort. To address these issues,
we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata
\textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained
language models (PLMs) with continuous prompt, RoPDA performs entity
augmentation and context augmentation through five fundamental augmentation
operations to generate label-flipping and label-preserving examples. To
optimize the utilization of the augmented samples, we present two techniques:
Self-Consistency Filtering and mixup. The former effectively eliminates
low-quality samples, while the latter prevents performance degradation arising
from the direct utilization of label-flipping samples. Extensive experiments on
three benchmarks from different domains demonstrate that RoPDA significantly
improves upon strong baselines, and also outperforms state-of-the-art
semi-supervised learning methods when unlabeled data is included.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sihan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Furao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07420">
<title>Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07420</link>
<description rdf:parseType="Literal">&lt;p&gt;For both public and private firms, comparable companies analysis is widely
used as a method for company valuation. In particular, the method is of great
value for valuation of private equity companies. The several approaches to the
comparable companies method usually rely on a qualitative approach to
identifying similar peer companies, which tends to use established industry
classification schemes and/or analyst intuition and knowledge. However, more
quantitative methods have started being used in the literature and in the
private equity industry, in particular, machine learning clustering, and
natural language processing (NLP). For NLP methods, the process consists of
extracting product entities from e.g., the company&apos;s website or company
descriptions from some financial database system and then to perform similarity
analysis. Here, using companies descriptions/summaries from publicly available
companies&apos; Wikipedia websites, we show that using large language models (LLMs),
such as GPT from openaAI, has a much higher precision and success rate than
using the standard named entity recognition (NER) which uses manual annotation.
We demonstrate quantitatively a higher precision rate, and show that,
qualitatively, it can be used to create appropriate comparable companies peer
groups which can then be used for equity valuation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Covas_E/0/1/0/all/0/1&quot;&gt;Eurico Covas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07426">
<title>Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar. (arXiv:2307.07426v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.07426</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time music information retrieval (RT-MIR) has much potential to augment
the capabilities of traditional acoustic instruments. We develop RT-MIR
techniques aimed at augmenting percussive fingerstyle, which blends acoustic
guitar playing with guitar body percussion. We formulate several design
objectives for RT-MIR systems for augmented instrument performance: (i) causal
constraint, (ii) perceptually negligible action-to-sound latency, (iii) control
intimacy support, (iv) synthesis control support. We present and evaluate
real-time guitar body percussion recognition and embedding learning techniques
based on convolutional neural networks (CNNs) and CNNs jointly trained with
variational autoencoders (VAEs). We introduce a taxonomy of guitar body
percussion based on hand part and location. We follow a cross-dataset
evaluation approach by collecting three datasets labelled according to the
taxonomy. The embedding quality of the models is assessed using KL-Divergence
across distributions corresponding to different taxonomic classes. Results
indicate that the networks are strong classifiers especially in a simplified
2-class recognition task, and the VAEs yield improved class separation compared
to CNNs as evidenced by increased KL-Divergence across distributions. We argue
that the VAE embedding quality could support control intimacy and rich
interaction when the latent space&apos;s parameters are used to control an external
synthesis engine. Further design challenges around generalisation to different
datasets have been identified.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martelloni_A/0/1/0/all/0/1&quot;&gt;Andrea Martelloni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McPherson_A/0/1/0/all/0/1&quot;&gt;Andrew P McPherson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1&quot;&gt;Mathieu Barthet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07443">
<title>Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07443</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular property prediction has gained significant attention due to its
transformative potential in multiple scientific disciplines. Conventionally, a
molecule graph can be represented either as a graph-structured data or a SMILES
text. Recently, the rapid development of Large Language Models (LLMs) has
revolutionized the field of NLP. Although it is natural to utilize LLMs to
assist in understanding molecules represented by SMILES, the exploration of how
LLMs will impact molecular property prediction is still in its early stage. In
this work, we advance towards this objective through two perspectives:
zero/few-shot molecular classification, and using the new explanations
generated by LLMs as representations of molecules. To be specific, we first
prompt LLMs to do in-context molecular classification and evaluate their
performance. After that, we employ LLMs to generate semantically enriched
explanations for the original SMILES and then leverage that to fine-tune a
small-scale LM model for multiple downstream tasks. The experimental results
highlight the superiority of text explanations as molecular representations
across multiple benchmark datasets, and confirm the immense potential of LLMs
in molecular property prediction tasks. Codes are available at
\url{https://github.com/ChnQ/LLM4Mol}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huayi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07445">
<title>TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling. (arXiv:2307.07445v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.07445</link>
<description rdf:parseType="Literal">&lt;p&gt;In future 6G Mobile Edge Computing (MEC), autopilot systems require the
capability of processing multimodal data with strong interdependencies.
However, traditional heuristic algorithms are inadequate for real-time
scheduling due to their requirement for multiple iterations to derive the
optimal scheme. We propose a novel TSNet-SAC based on Transformer, that
utilizes heuristic algorithms solely to guide the training of TSNet.
Additionally, a Sliding Augment Component (SAC) is introduced to enhance the
robustness and resolve algorithm defects. Furthermore, the Extender component
is designed to handle multi-scale training data and provide network
scalability, enabling TSNet to adapt to different access scenarios. Simulation
demonstrates that TSNet-SAC outperforms existing networks in accuracy and
robustness, achieving superior scheduling-making latency compared to heuristic
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1&quot;&gt;Ke Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haohan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Desheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07448">
<title>Depth-bounded Epistemic Logic. (arXiv:2307.07448v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07448</link>
<description rdf:parseType="Literal">&lt;p&gt;Epistemic logics model how agents reason about their beliefs and the beliefs
of other agents. Existing logics typically assume the ability of agents to
reason perfectly about propositions of unbounded modal depth. We present DBEL,
an extension of S5 that models agents that can reason about epistemic formulas
only up to a specific modal depth. To support explicit reasoning about agent
depths, DBEL includes depth atoms Ead (agent a has depth exactly d) and Pad
(agent a has depth at least d). We provide a sound and complete axiomatization
of DBEL.
&lt;/p&gt;
&lt;p&gt;We extend DBEL to support public announcements for bounded depth agents and
show how the resulting DPAL logic generalizes standard axioms from public
announcement logic. We present two alternate extensions and identify two
undesirable properties, amnesia and knowledge leakage, that these extensions
have but DPAL does not. We provide axiomatizations of these logics as well as
complexity results for satisfiability and model checking.
&lt;/p&gt;
&lt;p&gt;Finally, we use these logics to illustrate how agents with bounded modal
depth reason in the classical muddy children problem, including upper and lower
bounds on the depth knowledge necessary for agents to successfully solve the
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arthaud_F/0/1/0/all/0/1&quot;&gt;Farid Arthaud&lt;/a&gt; (Massachusetts Institute of Technology), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinard_M/0/1/0/all/0/1&quot;&gt;Martin Rinard&lt;/a&gt; (Massachusetts Institute of Technology)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07457">
<title>Structured Pruning of Neural Networks for Constraints Learning. (arXiv:2307.07457v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07457</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the integration of Machine Learning (ML) models with
Operation Research (OR) tools has gained popularity across diverse
applications, including cancer treatment, algorithmic configuration, and
chemical process optimization. In this domain, the combination of ML and OR
often relies on representing the ML model output using Mixed Integer
Programming (MIP) formulations. Numerous studies in the literature have
developed such formulations for many ML predictors, with a particular emphasis
on Artificial Neural Networks (ANNs) due to their significant interest in many
applications. However, ANNs frequently contain a large number of parameters,
resulting in MIP formulations that are impractical to solve, thereby impeding
scalability. In fact, the ML community has already introduced several
techniques to reduce the parameter count of ANNs without compromising their
performance, since the substantial size of modern ANNs presents challenges for
ML applications as it significantly impacts computational efforts during
training and necessitates significant memory resources for storage. In this
paper, we showcase the effectiveness of pruning, one of these techniques, when
applied to ANNs prior to their integration into MIPs. By pruning the ANN, we
achieve significant improvements in the speed of the solution process. We
discuss why pruning is more suitable in this context compared to other ML
compression techniques, and we identify the most appropriate pruning
strategies. To highlight the potential of this approach, we conduct experiments
using feed-forward neural networks with multiple layers to construct
adversarial examples. Our results demonstrate that pruning offers remarkable
reductions in solution times without hindering the quality of the final
decision, enabling the resolution of previously unsolvable instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cacciola_M/0/1/0/all/0/1&quot;&gt;Matteo Cacciola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frangioni_A/0/1/0/all/0/1&quot;&gt;Antonio Frangioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1&quot;&gt;Andrea Lodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07469">
<title>Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition. (arXiv:2307.07469v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07469</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing interactive action plays an important role in human-robot
interaction and collaboration. Previous methods use late fusion and
co-attention mechanism to capture interactive relations, which have limited
learning capability or inefficiency to adapt to more interacting entities. With
assumption that priors of each entity are already known, they also lack
evaluations on a more general setting addressing the diversity of subjects. To
address these problems, we propose an Interactive Spatiotemporal Token
Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and
interactive relations. Specifically, our network contains a tokenizer to
partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to
represent motions of multiple diverse entities. By extending the entity
dimension, ISTs provide better interactive representations. To jointly learn
along three dimensions in ISTs, multi-head self-attention blocks integrated
with 3D convolutions are designed to capture inter-token correlations. When
modeling correlations, a strict entity ordering is usually irrelevant for
recognizing interactive actions. To this end, Entity Rearrangement is proposed
to eliminate the orderliness in ISTs for interchangeable entities. Extensive
experiments on four datasets verify the effectiveness of ISTA-Net by
outperforming state-of-the-art methods. Our code is publicly available at
https://github.com/Necolizer/ISTA-Net
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Beichen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07508">
<title>Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07508</link>
<description rdf:parseType="Literal">&lt;p&gt;The dynamic vehicle dispatching problem corresponds to deciding which
vehicles to assign to requests that arise stochastically over time and space.
It emerges in diverse areas, such as in the assignment of trucks to loads to be
transported; in emergency systems; and in ride-hailing services. In this paper,
we model the problem as a semi-Markov decision process, which allows us to
treat time as continuous. In this setting, decision epochs coincide with
discrete events whose time intervals are random. We argue that an event-based
approach substantially reduces the combinatorial complexity of the decision
space and overcomes other limitations of discrete-time models often proposed in
the literature. In order to test our approach, we develop a new discrete-event
simulator and use double deep q-learning to train our decision agents.
Numerical experiments are carried out in realistic scenarios using data from
New York City. We compare the policies obtained through our approach with
heuristic policies often used in practice. Results show that our policies
exhibit better average waiting times, cancellation rates and total service
times, with reduction in average waiting times of up to 50% relative to the
other tested heuristic policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordeiro_E/0/1/0/all/0/1&quot;&gt;Edyvalberty Alenquer Cordeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitombeira_Neto_A/0/1/0/all/0/1&quot;&gt;Anselmo Ramalho Pitombeira-Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03443">
<title>Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03443</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how to optimally regulate prediction algorithms in a world where an
agent uses complex &apos;black-box&apos; prediction functions to make decisions such as
lending, medical testing, or hiring, and where a principal is limited in how
much she can learn about the agent&apos;s black-box model. We show that limiting
agents to prediction functions that are simple enough to be fully transparent
is inefficient as long as the misalignment is limited and first-best prediction
functions are sufficiently complex. Algorithmic audits can improve welfare, but
the gains depend on the design of the audit tools. Tools that focus on
minimizing overall information loss, the focus of many explainer tools, will
generally be inefficient since they focus on explaining the average behavior of
the prediction function. Targeted tools that focus on the source of incentive
misalignment, e.g., excess false positives or racial disparities, can provide
second-best solutions. We provide empirical support for our theoretical
findings using an application in consumer lending, where we document that
complex models regulated based on context-specific explanation tools outperform
simple, fully transparent models. This gain from complex models represents a
Pareto improvement across our empirical applications that are preferred both by
the lender and from the perspective of the financial regulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Blattner_L/0/1/0/all/0/1&quot;&gt;Laura Blattner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Nelson_S/0/1/0/all/0/1&quot;&gt;Scott Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Spiess_J/0/1/0/all/0/1&quot;&gt;Jann Spiess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.12319">
<title>Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2202.12319</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor networks, widely used for providing efficient representations of
low-energy states of local quantum many-body systems, have been recently
proposed as machine learning architectures which could present advantages with
respect to traditional ones. In this work we show that tensor network
architectures have especially prospective properties for privacy-preserving
machine learning, which is important in tasks such as the processing of medical
records. First, we describe a new privacy vulnerability that is present in
feedforward neural networks, illustrating it in synthetic and real-world
datasets. Then, we develop well-defined conditions to guarantee robustness to
such vulnerability, which involve the characterization of models equivalent
under gauge symmetry. We rigorously prove that such conditions are satisfied by
tensor-network architectures. In doing so, we define a novel canonical form for
matrix product states, which has a high degree of regularity and fixes the
residual gauge that is left in the canonical forms based on singular value
decompositions. We supplement the analytical findings with practical examples
where matrix product states are trained on datasets of medical records, which
show large reductions on the probability of an attacker extracting information
about the training dataset from the model&apos;s parameters. Given the growing
expertise in training tensor-network architectures, these results imply that
one may not have to be forced to make a choice between accuracy in prediction
and ensuring the privacy of the information processed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pozas_Kerstjens_A/0/1/0/all/0/1&quot;&gt;Alejandro Pozas-Kerstjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Santana_S/0/1/0/all/0/1&quot;&gt;Senaida Hern&amp;#xe1;ndez-Santana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monturiol_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Ram&amp;#xf3;n Pareja Monturiol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1&quot;&gt;Marco Castrill&amp;#xf3;n L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1&quot;&gt;Giannicola Scarpa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Guillen_C/0/1/0/all/0/1&quot;&gt;Carlos E. Gonz&amp;#xe1;lez-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Garcia_D/0/1/0/all/0/1&quot;&gt;David P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02762">
<title>Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02762</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Story-Telling is the process of forming a multi-sentence story from a
set of images. Appropriately including visual variation and contextual
information captured inside the input images is one of the most challenging
aspects of visual storytelling. Consequently, stories developed from a set of
images often lack cohesiveness, relevance, and semantic relationship. In this
paper, we propose a novel Vision Transformer Based Model for describing a set
of images as a story. The proposed method extracts the distinct features of the
input images using a Vision Transformer (ViT). Firstly, input images are
divided into 16X16 patches and bundled into a linear projection of flattened
patches. The transformation from a single image to multiple image patches
captures the visual variety of the input visual patterns. These features are
used as input to a Bidirectional-LSTM which is part of the sequence encoder.
This captures the past and future image context of all image patches. Then, an
attention mechanism is implemented and used to increase the discriminatory
capacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The
performance of our proposed model is evaluated using the Visual Story-Telling
dataset (VIST), and the results show that our model outperforms the current
state of the art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakan_Z/0/1/0/all/0/1&quot;&gt;Zainy M. Malakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1&quot;&gt;Ghulam Mubashar Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1&quot;&gt;Ajmal Mian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00543">
<title>DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00543</link>
<description rdf:parseType="Literal">&lt;p&gt;Many compression techniques have been proposed to reduce the communication
overhead of Federated Learning training procedures. However, these are
typically designed for compressing model updates, which are expected to decay
throughout training. As a result, such methods are inapplicable to downlink
(i.e., from the parameter server to clients) compression in the cross-device
setting, where heterogeneous clients $\textit{may appear only once}$ during
training and thus must download the model parameters. Accordingly, we propose
$\textsf{DoCoFL}$ -- a new framework for downlink compression in the
cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined
with many uplink compression schemes, rendering it suitable for bi-directional
compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$
offers significant bi-directional bandwidth reduction while achieving
competitive accuracy to that of a baseline without any compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorfman_R/0/1/0/all/0/1&quot;&gt;Ron Dorfman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1&quot;&gt;Shay Vargaftik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1&quot;&gt;Yaniv Ben-Itzhak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Y. Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02401">
<title>Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02401</link>
<description rdf:parseType="Literal">&lt;p&gt;Affordance detection is a challenging problem with a wide variety of robotic
applications. Traditional affordance detection methods are limited to a
predefined set of affordance labels, hence potentially restricting the
adaptability of intelligent robots in complex and dynamic environments. In this
paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,
which is capable of detecting an unbounded number of affordances in 3D point
clouds. By simultaneously learning the affordance text and the point feature,
OpenAD successfully exploits the semantic relationships between affordances.
Therefore, our proposed method enables zero-shot detection and can be able to
detect previously unseen affordances without a single annotation example.
Intensive experimental results show that OpenAD works effectively on a wide
range of affordance detection setups and outperforms other baselines by a large
margin. Additionally, we demonstrate the practicality of the proposed OpenAD in
real-world robotic applications with a fast inference speed (~100ms). Our
project is available at https://openad2023.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Toan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Minh Nhat Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1&quot;&gt;An Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dzung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1&quot;&gt;Thieu Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07274">
<title>Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07274</link>
<description rdf:parseType="Literal">&lt;p&gt;Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Guetta_N/0/1/0/all/0/1&quot;&gt;Nitzan Bitton-Guetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1&quot;&gt;Gabriel Stanovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1&quot;&gt;Roy Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14863">
<title>DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion. (arXiv:2303.14863v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14863</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https://github.com/sauradip/DiffusionTAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08349">
<title>Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08349</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite numerous successes in Deep Reinforcement Learning (DRL), the learned
policies are not interpretable. Moreover, since DRL does not exploit symbolic
relational representations, it has difficulties in coping with structural
changes in its environment (such as increasing the number of objects).
Relational Reinforcement Learning, on the other hand, inherits the relational
representations from symbolic planning to learn reusable policies. However, it
has so far been unable to scale up and exploit the power of deep neural
networks. We propose Deep Explainable Relational Reinforcement Learning
(DERRL), a framework that exploits the best of both -- neural and symbolic
worlds. By resorting to a neuro-symbolic approach, DERRL combines relational
representations and constraints from symbolic planning with deep learning to
extract interpretable policies. These policies are in the form of logical rules
that explain how each decision (or action) is arrived at. Through several
experiments, in setups like the Countdown Game, Blocks World, Gridworld, and
Traffic, we show that the policies learned by DERRL can be applied to different
configurations and contexts, hence generalizing to environmental modifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1&quot;&gt;Rishi Hazra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01486">
<title>ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01486</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a framework ARBEx, a novel attentive feature
extraction framework driven by Vision Transformer with reliability balancing to
cope against poor class distributions, bias, and uncertainty in the facial
expression learning (FEL) task. We reinforce several data pre-processing and
refinement methods along with a window-based cross-attention ViT to squeeze the
best of the data. We also employ learnable anchor points in the embedding space
with label distributions and multi-head self-attention mechanism to optimize
performance against weak predictions with reliability balancing, which is a
strategy that leverages anchor points, attention scores, and confidence values
to enhance the resilience of label predictions. To ensure correct label
classification and improve the models&apos; discriminative power, we introduce
anchor loss, which encourages large margins between anchor points.
Additionally, the multi-head self-attention mechanism, which is also trainable,
plays an integral role in identifying accurate labels. This approach provides
critical elements for improving the reliability of predictions and has a
substantial positive effect on final prediction capabilities. Our adaptive
model can be integrated with any deep neural network to forestall challenges in
various recognition tasks. Our strategy outperforms current state-of-the-art
methodologies, according to extensive experiments conducted in a variety of
contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasi_A/0/1/0/all/0/1&quot;&gt;Azmine Toushik Wasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serbetar_K/0/1/0/all/0/1&quot;&gt;Karlo &amp;#x160;erbetar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Raima Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafi_T/0/1/0/all/0/1&quot;&gt;Taki Hasan Rafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_D/0/1/0/all/0/1&quot;&gt;Dong-Kyu Chae&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14724">
<title>I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14724</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual metaphors are powerful rhetorical devices used to persuade or
communicate creative ideas through images. Similar to linguistic metaphors,
they convey meaning implicitly through symbolism and juxtaposition of the
symbols. We propose a new task of generating visual metaphors from linguistic
metaphors. This is a challenging task for diffusion-based text-to-image models,
such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning
and compositionality. We propose to solve the task through the collaboration
between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3
(davinci-002) with Chain-of-Thought prompting generates text that represents a
visual elaboration of the linguistic metaphor containing the implicit meaning
and relevant objects, which is then used as input to the diffusion-based
text-to-image models.Using a human-AI collaboration framework, where humans
interact both with the LLM and the top-performing diffusion model, we create a
high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic
metaphors and their associated visual elaborations. Evaluation by professional
illustrators shows the promise of LLM-Diffusion Model collaboration for this
task . To evaluate the utility of our Human-AI collaboration framework and the
quality of our dataset, we perform both an intrinsic human-based evaluation and
an extrinsic evaluation using visual entailment as a downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1&quot;&gt;Tuhin Chakrabarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1&quot;&gt;Arkadiy Saakyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winn_O/0/1/0/all/0/1&quot;&gt;Olivia Winn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1&quot;&gt;Artemis Panagopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1&quot;&gt;Marianna Apidianaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1&quot;&gt;Smaranda Muresan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17375">
<title>Attention Schema in Neural Agents. (arXiv:2305.17375v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17375</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention has become a common ingredient in deep learning architectures. It
adds a dynamical selection of information on top of the static selection of
information supported by weights. In the same way, we can imagine a
higher-order informational filter built on top of attention: an Attention
Schema (AS), namely, a descriptive and predictive model of attention. In
cognitive neuroscience, Attention Schema Theory (AST) supports this idea of
distinguishing attention from AS. A strong prediction of this theory is that an
agent can use its own AS to also infer the states of other agents&apos; attention
and consequently enhance coordination with other agents. As such, multi-agent
reinforcement learning would be an ideal setting to experimentally test the
validity of AST. We explore different ways in which attention and AS interact
with each other. Our preliminary results indicate that agents that implement
the AS as a recurrent internal control achieve the best performance. In
general, these exploratory experiments suggest that equipping artificial agents
with a model of attention can enhance their social intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dianbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolotta_S/0/1/0/all/0/1&quot;&gt;Samuele Bolotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;He Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1&quot;&gt;Guillaume Dumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18405">
<title>Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18405</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep graph clustering, which aims to group the nodes of a graph into disjoint
clusters with deep neural networks, has achieved promising progress in recent
years. However, the existing methods fail to scale to the large graph with
million nodes. To solve this problem, a scalable deep graph clustering method
(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by
discriminating nodes, whether being corrupted by augmentations, representations
are learned in a self-supervised manner. Meanwhile, the cluster centres are
initialized as learnable neural parameters. Subsequently, the clustering
distribution is optimized by minimizing the proposed cluster dilation loss and
cluster shrink loss in an adversarial manner. By these settings, we unify the
two-step clustering, i.e., representation learning and clustering optimization,
into an end-to-end framework, guiding the network to learn clustering-friendly
features. Besides, Dink-Net scales well to large graphs since the designed loss
functions adopt the mini-batch data to optimize the clustering distribution
even without performance drops. Both experimental results and theoretical
analyses demonstrate the superiority of our method. Compared to the runner-up,
Dink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111
million nodes and 1.6 billion edges. The source code is released at
https://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,
and datasets) of deep graph clustering is shared at
https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10756">
<title>A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10756</link>
<description rdf:parseType="Literal">&lt;p&gt;The rehabilitation treatment helps to heal minor sports and occupational
injuries. In a traditional rehabilitation process, a therapist will assign
certain actions to a patient to perform in between hospital visits, and it will
rely on the patient to remember actions correctly and the schedule to perform
them. Unfortunately, many patients forget to perform actions or fail to recall
actions in detail. As a consequence, the rehabilitation treatment is hampered
or, in the worst case, the patient may suffer from additional injury caused by
performing incorrect actions. To resolve these issues, we propose a HRNet-based
rehabilitation monitoring system, which can remind a patient when to perform
the actions and display the actions for the patient to follow via the patient&apos;s
smartphone. In addition, it helps the therapist to monitor the progress of the
rehabilitation for the patient. Our system consists of an iOS app and several
components at the server side. The app is in charge of displaying and
collecting action videos. The server computes the similarity score between the
therapist&apos;s actions and the patient&apos;s in the videos to keep track of the number
of repetitions of each action. Theses stats will be shown to both of the
patient and therapist. The extensive experiments show that the F1-Score of the
similarity calculation is as high as 0.9 and the soft accuracy of the number of
repetitions is higher than 90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_Y/0/1/0/all/0/1&quot;&gt;Yi-Ching Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liou_F/0/1/0/all/0/1&quot;&gt;Fong-Syuan Liou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsuan Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Z/0/1/0/all/0/1&quot;&gt;Zi-Cing Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;MIn-Te Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14369">
<title>Few-Shot Continual Learning via Flat-to-Wide Approaches. (arXiv:2306.14369v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14369</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing approaches on continual learning call for a lot of samples in their
training processes. Such approaches are impractical for many real-world
problems having limited samples because of the overfitting problem. This paper
proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach
(FLOWER), where a flat-to-wide learning process finding the flat-wide minima is
proposed to address the catastrophic forgetting problem. The issue of data
scarcity is overcome with a data augmentation approach making use of a ball
generator concept to restrict the sampling space into the smallest enclosing
ball. Our numerical studies demonstrate the advantage of FLOWER achieving
significantly improved performances over prior arts notably in the small base
tasks. For further study, source codes of FLOWER, competitor algorithms and
experimental logs are shared publicly in
\url{https://github.com/anwarmaxsum/FLOWER}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masum_M/0/1/0/all/0/1&quot;&gt;Muhammad Anwar Ma&amp;#x27;sum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1&quot;&gt;Edwin Lughofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibullah/0/1/0/all/0/1&quot;&gt;Habibullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowalczyk_R/0/1/0/all/0/1&quot;&gt;Ryszard Kowalczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03380">
<title>On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jinqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04962">
<title>Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by the visited nodes in the environment. We use these
proposed features as rewards for graph neural-network-based reinforcement
learning. On multiple classes of synthetically generated graphs, we find that
trained agents generalize to larger environments and to longer exploratory
walks than are seen during training. Our method computes more efficiently than
the greedy evaluation of the relevant topological properties. The proposed
intrinsic motivations bear particular relevance for recommender systems. We
demonstrate that curiosity-based recommendations are more predictive of human
behavior than PageRank centrality for several real-world graph datasets,
including MovieLens, Amazon Books, and Wikispeedia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shubhankar P. Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1&quot;&gt;Mathieu Ouellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1&quot;&gt;Juan Cervino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kieran A. Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1&quot;&gt;Dani S. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05300">
<title>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence thrives on the concept of cognitive synergy, where
collaboration and information integration among different cognitive processes
yield superior outcomes compared to individual cognitive processes in
isolation. Although Large Language Models (LLMs) have demonstrated promising
performance as general task-solving agents, they still struggle with tasks that
require intensive domain knowledge and complex reasoning. In this work, we
propose Solo Performance Prompting (SPP), which transforms a single LLM into a
cognitive synergist by engaging in multi-turn self-collaboration with multiple
personas. A cognitive synergist refers to an intelligent agent that
collaborates with multiple minds, combining their individual strengths and
knowledge, to enhance problem-solving and overall performance in complex tasks.
By dynamically identifying and simulating different personas based on task
inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have
discovered that assigning multiple, fine-grained personas in LLMs elicits
better problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP
effectively elicits internal knowledge acquisition abilities, reduces
hallucination, and maintains strong reasoning capabilities. Code, data, and
prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenhailong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shaoguang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05832">
<title>Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has been experiencing an increasing surge of interest due to
the recent advancements in image processing and deep learning-based techniques.
View planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish view planning for active
reconstruction; the former approach poses a challenge for complex or newly
identified targets while the latter is computationally expensive. In this work,
we present Bag-of-Views (BoV), a fully appearance-based model used to assign
utility to the captured views for both offline dataset refinement and online
next-best-view (NBV) planning applications targeting the task of 3D
reconstruction. With this contribution, we also developed the View Planning
Toolbox (VPT), a lightweight package for training and testing machine
learning-based view planning frameworks, custom view dataset generation of
arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a
BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of
our model in reducing the number of required views for high-quality
reconstructions in dataset refinement and NBV planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazani_S/0/1/0/all/0/1&quot;&gt;Sara Hatami Gazani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucsok_M/0/1/0/all/0/1&quot;&gt;Matthew Tucsok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mantegh_I/0/1/0/all/0/1&quot;&gt;Iraj Mantegh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06501">
<title>Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06501</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: The artificial pancreas (AP) has shown promising potential in
achieving closed-loop glucose control for individuals with type 1 diabetes
mellitus (T1DM). However, designing an effective control policy for the AP
remains challenging due to the complex physiological processes, delayed insulin
response, and inaccurate glucose measurements. While model predictive control
(MPC) offers safety and stability through the dynamic model and safety
constraints, it lacks individualization and is adversely affected by
unannounced meals. Conversely, deep reinforcement learning (DRL) provides
personalized and adaptive strategies but faces challenges with distribution
shifts and substantial data requirements. Methods: We propose a hybrid control
policy for the artificial pancreas (HyCPAP) to address the above challenges.
HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the
strengths of both policies while compensating for their respective limitations.
To facilitate faster deployment of AP systems in real-world settings, we
further incorporate meta-learning techniques into HyCPAP, leveraging previous
experience and patient-shared knowledge to enable fast adaptation to new
patients with limited available data. Results: We conduct extensive experiments
using the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Our
approaches achieve the highest percentage of time spent in the desired
euglycemic range and the lowest occurrences of hypoglycemia. Conclusion: The
results clearly demonstrate the superiority of our methods for closed-loop
glucose management in individuals with T1DM. Significance: The study presents
novel control policies for AP systems, affirming the great potential of
proposed methods for efficient closed-loop glucose control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Wenzhou Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Luolin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_F/0/1/0/all/0/1&quot;&gt;Feng Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06913">
<title>Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting the inner workings of deep learning models is crucial for
establishing trust and ensuring model safety. Concept-based explanations have
emerged as a superior approach that is more interpretable than feature
attribution estimates such as pixel saliency. However, defining the concepts
for the interpretability analysis biases the explanations by the user&apos;s
expectations on the concepts. To address this, we propose a novel post-hoc
unsupervised method that automatically uncovers the concepts learned by deep
models during training. By decomposing the latent space of a layer in singular
vectors and refining them by unsupervised clustering, we uncover concept
vectors aligned with directions of high variance that are relevant to the model
prediction, and that point to semantically distinct concepts. Our extensive
experiments reveal that the majority of our concepts are readily understandable
to humans, exhibit coherency, and bear relevance to the task at hand. Moreover,
we showcase the practical utility of our method in dataset exploration, where
our concept vectors successfully identify outlier training samples affected by
various confounding factors. This novel exploration technique has remarkable
versatility to data types and model architectures and it will facilitate the
identification of biases and the discovery of sources of error within training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziani_M/0/1/0/all/0/1&quot;&gt;Mara Graziani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahony_L/0/1/0/all/0/1&quot;&gt;Laura O&amp;#x27; Mahony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;An-Phi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>