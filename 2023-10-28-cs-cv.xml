<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1908.10907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.05435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.02206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.07368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.14251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.01646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.09249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.01992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00990" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.16851">
<title>Deep Learning Models for Classification of COVID-19 Cases by Medical Images. (arXiv:2310.16851v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.16851</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, the use of chest Computed Tomography (CT) images for
detecting coronavirus infections has gained significant attention, owing to
their ability to reveal bilateral changes in affected individuals. However,
classifying patients from medical images presents a formidable challenge,
particularly in identifying such bilateral changes. To tackle this challenge,
our study harnesses the power of deep learning models for the precise
classification of infected patients. Our research involves a comparative
analysis of deep transfer learning-based classification models, including
DenseNet201, GoogleNet, and AlexNet, against carefully chosen supervised
learning models. Additionally, our work encompasses Covid-19 classification,
which involves the identification and differentiation of medical images, such
as X-rays and electrocardiograms, that exhibit telltale signs of Covid-19
infection. This comprehensive approach ensures that our models can handle a
wide range of medical image types and effectively identify characteristic
patterns indicative of Covid-19. By conducting meticulous research and
employing advanced deep learning techniques, we have made significant strides
in enhancing the accuracy and speed of Covid-19 diagnosis. Our results
demonstrate the effectiveness of these models and their potential to make
substantial contributions to the global effort to combat COVID-19.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Amir Ali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16856">
<title>GraFT: Gradual Fusion Transformer for Multimodal Re-Identification. (arXiv:2310.16856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16856</link>
<description rdf:parseType="Literal">&lt;p&gt;Object Re-Identification (ReID) is pivotal in computer vision, witnessing an
escalating demand for adept multimodal representation learning. Current models,
although promising, reveal scalability limitations with increasing modalities
as they rely heavily on late fusion, which postpones the integration of
specific modality insights. Addressing this, we introduce the \textbf{Gradual
Fusion Transformer (GraFT)} for multimodal ReID. At its core, GraFT employs
learnable fusion tokens that guide self-attention across encoders, adeptly
capturing both modality-specific and object-specific features. Further
bolstering its efficacy, we introduce a novel training paradigm combined with
an augmented triplet loss, optimizing the ReID feature embedding space. We
demonstrate these enhancements through extensive ablation studies and show that
GraFT consistently surpasses established multimodal ReID benchmarks.
Additionally, aiming for deployment versatility, we&apos;ve integrated neural
network pruning into GraFT, offering a balance between model size and
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haoli Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayao Li&lt;/a&gt; (Emily), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiller_E/0/1/0/all/0/1&quot;&gt;Eva Schiller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDermott_L/0/1/0/all/0/1&quot;&gt;Luke McDermott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1&quot;&gt;Daniel Cummings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16858">
<title>4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation. (arXiv:2310.16858v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16858</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper targets interactive object-level editing(e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in dynamic NeRF based on user
strokes on a single frame. Our dynamic scene representation is built upon
hybrid semantic feature fields so that the spatial-temporal consistency can be
maintained after editing. In addition, we design recursive selection refinement
that significantly boosts segmentation accuracy in a dynamic NeRF to aid the
editing process. Moreover, we develop multi-view reprojection inpainting to
fill holes caused by incomplete scene capture after editing. Extensive
experiments and editing examples on real-world demonstrate that 4D-Editor
achieves photo-realistic dynamic NeRF editing. Project page:
https://patrickddj.github.io/4D-Editor
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dadong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1&quot;&gt;Zhihui Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaobo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xidong Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16861">
<title>General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.16861</link>
<description rdf:parseType="Literal">&lt;p&gt;The pre-training architectures of large language models encompass various
types, including autoencoding models, autoregressive models, and
encoder-decoder models. We posit that any modality can potentially benefit from
a large language model, as long as it undergoes vector quantization to become
discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which
seamlessly integrates autoencoding and autoregressive tasks in point cloud
transformer. This model is versatile, allowing fine-tuning for downstream point
cloud representation tasks, as well as unconditional and conditional generation
tasks. GPM enhances masked prediction in autoencoding through various forms of
mask padding tasks, leading to improved performance in point cloud
understanding. Additionally, GPM demonstrates highly competitive results in
unconditional point cloud generation tasks, even exhibiting the potential for
conditional generation tasks by modifying the input&apos;s conditional information.
Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves
superior performance in point cloud understanding tasks. Furthermore, the
integration of autoregressive and autoencoding within the same transformer
underscores its versatility across different downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Laurence T. Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16870">
<title>MACP: Efficient Model Adaptation for Cooperative Perception. (arXiv:2310.16870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16870</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception
capabilities of connected and automated vehicles (CAVs) by enabling information
sharing to &quot;see through the occlusions&quot;, resulting in significant performance
improvements. However, developing and training complex multi-agent perception
models from scratch can be expensive and unnecessary when existing single-agent
models show remarkable generalization capabilities. In this paper, we propose a
new framework termed MACP, which equips a single-agent pre-trained model with
cooperation capabilities. We approach this objective by identifying the key
challenges of shifting from single-agent to cooperative settings, adapting the
model by freezing most of its parameters and adding a few lightweight modules.
We demonstrate in our experiments that the proposed framework can effectively
utilize cooperative observations and outperform other state-of-the-art
approaches in both simulated and real-world cooperative perception benchmarks
while requiring substantially fewer tunable parameters with reduced
communication costs. Our source code is available at
https://github.com/PurdueDigitalTwin/MACP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juanwu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ZHao_S/0/1/0/all/0/1&quot;&gt;Sicheng ZHao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenqian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziran Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16872">
<title>SonoSAM -- Segment Anything on Ultrasound Images. (arXiv:2310.16872v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.16872</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present SonoSAM - a promptable foundational model for
segmenting objects of interest on ultrasound images. Fine-tuned exclusively on
a rich, diverse set of objects from roughly 200k ultrasound image-mask pairs,
SonoSAM demonstrates state-of-the-art performance on 8 unseen ultrasound
data-sets, outperforming competing methods by a significant margin on all
metrics of interest. SonoSAM achieves average dice similarity score of more
than 90% on almost all test datasets within 2-6 clicks on an average, making it
a valuable tool for annotating ultrasound images. We also extend SonoSAM to 3-D
(2-D +t) applications and demonstrate superior performance making it a valuable
tool for generating dense annotations from ultrasound cine-loops. Further, to
increase practical utility of SonoSAM, we propose a two-step process of
fine-tuning followed by knowledge distillation to a smaller footprint model
without comprising the performance. We present detailed qualitative and
quantitative comparisons of SonoSAM with state-of-the art methods showcasing
efficacy of SonoSAM as one of the first reliable, generic foundational model
for ultrasound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravishankar_H/0/1/0/all/0/1&quot;&gt;Hariharan Ravishankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Patil_R/0/1/0/all/0/1&quot;&gt;Rohan Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Melapudi_V/0/1/0/all/0/1&quot;&gt;Vikram Melapudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhatia_P/0/1/0/all/0/1&quot;&gt;Parminder Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taha_K/0/1/0/all/0/1&quot;&gt;Kass-Hout Taha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Annangi_P/0/1/0/all/0/1&quot;&gt;Pavan Annangi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16898">
<title>MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory. (arXiv:2310.16898v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16898</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the high price and heavy energy consumption of GPUs, deploying deep
models on IoT devices such as microcontrollers makes significant contributions
for ecological AI. Conventional methods successfully enable convolutional
neural network inference of high resolution images on microcontrollers, while
the framework for vision transformers that achieve the state-of-the-art
performance in many vision applications still remains unexplored. In this
paper, we propose a hardware-algorithm co-optimizations method called MCUFormer
to deploy vision transformers on microcontrollers with extremely limited
memory, where we jointly design transformer architecture and construct the
inference operator library to fit the memory resource constraint. More
specifically, we generalize the one-shot network architecture search (NAS) to
discover the optimal architecture with highest task performance given the
memory budget from the microcontrollers, where we enlarge the existing search
space of vision transformers by considering the low-rank decomposition
dimensions and patch resolution for memory reduction. For the construction of
the inference operator library of vision transformers, we schedule the memory
buffer during inference through operator integration, patch embedding
decomposition, and token overwriting, allowing the memory buffer to be fully
utilized to adapt to the forward pass of the vision transformer. Experimental
results demonstrate that our MCUFormer achieves 73.62\% top-1 accuracy on
ImageNet for image classification with 320KB memory on STM32F746
microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yinan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1&quot;&gt;Zhou Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16919">
<title>Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs. (arXiv:2310.16919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16919</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel multi-bit box-free watermarking method for the protection
of Intellectual Property Rights (IPR) of GANs with improved robustness against
white-box attacks like fine-tuning, pruning, quantization, and surrogate model
attacks. The watermark is embedded by adding an extra watermarking loss term
during GAN training, ensuring that the images generated by the GAN contain an
invisible watermark that can be retrieved by a pre-trained watermark decoder.
In order to improve the robustness against white-box model-level attacks, we
make sure that the model converges to a wide flat minimum of the watermarking
loss term, in such a way that any modification of the model parameters does not
erase the watermark. To do so, we add random noise vectors to the parameters of
the generator and require that the watermarking loss term is as invariant as
possible with respect to the presence of noise. This procedure forces the
generator to converge to a wide flat minimum of the watermarking loss. The
proposed method is architectureand dataset-agnostic, thus being applicable to
many different generation tasks and models, as well as to CNN-based image
processing architectures. We present the results of extensive experiments
showing that the presence of the watermark has a negligible impact on the
quality of the generated images, and proving the superior robustness of the
watermark against model modification and surrogate model attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1&quot;&gt;Jianwei Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhihua Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1&quot;&gt;Benedetta Tondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1&quot;&gt;Mauro Barni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16936">
<title>Diagnosing Alzheimer&apos;s Disease using Early-Late Multimodal Data Fusion with Jacobian Maps. (arXiv:2310.16936v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16936</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease (AD) is a prevalent and debilitating neurodegenerative
disorder impacting a large aging population. Detecting AD in all its
presymptomatic and symptomatic stages is crucial for early intervention and
treatment. An active research direction is to explore machine learning methods
that harness multimodal data fusion to outperform human inspection of medical
scans. However, existing multimodal fusion models have limitations, including
redundant computation, complex architecture, and simplistic handling of missing
data. Moreover, the preprocessing pipelines of medical scans remain
inadequately detailed and are seldom optimized for individual subjects. In this
paper, we propose an efficient early-late fusion (ELF) approach, which
leverages a convolutional neural network for automated feature extraction and
random forests for their competitive performance on small datasets.
Additionally, we introduce a robust preprocessing pipeline that adapts to the
unique characteristics of individual subjects and makes use of whole brain
images rather than slices or patches. Moreover, to tackle the challenge of
detecting subtle changes in brain volume, we transform images into the Jacobian
domain (JD) to enhance both accuracy and robustness in our classification.
Using MRI and CT images from the OASIS-3 dataset, our experiments demonstrate
the effectiveness of the ELF approach in classifying AD into four stages with
an accuracy of 97.19%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_Y/0/1/0/all/0/1&quot;&gt;Yasmine Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16954">
<title>Improving Performance in Colorectal Cancer Histology Decomposition using Deep and Ensemble Machine Learning. (arXiv:2310.16954v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2310.16954</link>
<description rdf:parseType="Literal">&lt;p&gt;In routine colorectal cancer management, histologic samples stained with
hematoxylin and eosin are commonly used. Nonetheless, their potential for
defining objective biomarkers for patient stratification and treatment
selection is still being explored. The current gold standard relies on
expensive and time-consuming genetic tests. However, recent research highlights
the potential of convolutional neural networks (CNNs) in facilitating the
extraction of clinically relevant biomarkers from these readily available
images. These CNN-based biomarkers can predict patient outcomes comparably to
golden standards, with the added advantages of speed, automation, and minimal
cost. The predictive potential of CNN-based biomarkers fundamentally relies on
the ability of convolutional neural networks (CNNs) to classify diverse tissue
types from whole slide microscope images accurately. Consequently, enhancing
the accuracy of tissue class decomposition is critical to amplifying the
prognostic potential of imaging-based biomarkers. This study introduces a
hybrid Deep and ensemble machine learning model that surpassed all preceding
solutions for this classification task. Our model achieved 96.74% accuracy on
the external test set and 99.89% on the internal test set. Recognizing the
potential of these models in advancing the task, we have made them publicly
available for further research and development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Prezja_F/0/1/0/all/0/1&quot;&gt;Fabi Prezja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Annala_L/0/1/0/all/0/1&quot;&gt;Leevi Annala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kiiskinen_S/0/1/0/all/0/1&quot;&gt;Sampsa Kiiskinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lahtinen_S/0/1/0/all/0/1&quot;&gt;Suvi Lahtinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ojala_T/0/1/0/all/0/1&quot;&gt;Timo Ojala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ruusuvuori_P/0/1/0/all/0/1&quot;&gt;Pekka Ruusuvuori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kuopio_T/0/1/0/all/0/1&quot;&gt;Teijo Kuopio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16978">
<title>The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.16978</link>
<description rdf:parseType="Literal">&lt;p&gt;The global need for effective disease diagnosis remains substantial, given
the complexities of various disease mechanisms and diverse patient symptoms. To
tackle these challenges, researchers, physicians, and patients are turning to
machine learning (ML), an artificial intelligence (AI) discipline, to develop
solutions. By leveraging sophisticated ML and AI methods, healthcare
stakeholders gain enhanced diagnostic and treatment capabilities. However,
there is a scarcity of research focused on ML algorithms for enhancing the
accuracy and computational efficiency. This research investigates the capacity
of machine learning algorithms to improve the transmission of heart rate data
in time series healthcare metrics, concentrating particularly on optimizing
accuracy and efficiency. By exploring various ML algorithms used in healthcare
applications, the review presents the latest trends and approaches in ML-based
disease diagnosis (MLBDD). The factors under consideration include the
algorithm utilized, the types of diseases targeted, the data types employed,
the applications, and the evaluation metrics. This review aims to shed light on
the prospects of ML in healthcare, particularly in disease diagnosis. By
analyzing the current literature, the study provides insights into
state-of-the-art methodologies and their performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1&quot;&gt;S M Atikur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibtisum_S/0/1/0/all/0/1&quot;&gt;Sifat Ibtisum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazgir_E/0/1/0/all/0/1&quot;&gt;Ehsan Bazgir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barai_T/0/1/0/all/0/1&quot;&gt;Tumpa Barai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16979">
<title>Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16979</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based solutions for semantic segmentation suffer from
significant performance degradation when tested on data with different
characteristics than what was used during the training. Adapting the models
using annotated data from the new domain is not always practical. Unsupervised
Domain Adaptation (UDA) approaches are crucial in deploying these models in the
actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ
a teacher-student self-training approach, where a teacher model is used to
generate pseudo-labels for the new data which in turn guide the training
process of the student model. Though this approach has seen a lot of success,
it suffers from the issue of noisy pseudo-labels being propagated in the
training process. To address this issue, we propose an auxiliary pseudo-label
refinement network (PRN) for online refining of the pseudo labels and also
localizing the pixels whose predicted labels are likely to be noisy. Being able
to improve the quality of pseudo labels and select highly reliable ones, PRN
helps self-training of segmentation models to be robust against pseudo label
noise propagation during different stages of adaptation. We evaluate our
approach on benchmark datasets with three different domain shifts, and our
approach consistently performs significantly better than the previous
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1&quot;&gt;Niluthpol Chowdhury Mithun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajvanshi_A/0/1/0/all/0/1&quot;&gt;Abhinav Rajvanshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1&quot;&gt;Han-Pang Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1&quot;&gt;Supun Samarasekera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16991">
<title>An Efficient Deep Learning-based approach for Recognizing Agricultural Pests in the Wild. (arXiv:2310.16991v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16991</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the biggest challenges that the farmers go through is to fight insect
pests during agricultural product yields. The problem can be solved easily and
avoid economic losses by taking timely preventive measures. This requires
identifying insect pests in an easy and effective manner. Most of the insect
species have similarities between them. Without proper help from the
agriculturist academician it is very challenging for the farmers to identify
the crop pests accurately. To address this issue we have done extensive
experiments considering different methods to find out the best method among
all. This paper presents a detailed overview of the experiments done on mainly
a robust dataset named IP102 including transfer learning with finetuning,
attention mechanism and custom architecture. Some example from another dataset
D0 is also shown to show robustness of our experimented techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafi_M/0/1/0/all/0/1&quot;&gt;Mohtasim Hadi Rafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjabin_M/0/1/0/all/0/1&quot;&gt;Mohammad Ratul Mahjabin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Sabbir Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16999">
<title>Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.16999</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a method for verifying the output of a deep neural network for
medical image segmentation that is robust to several classes of random as well
as worst-case perturbations i.e. adversarial attacks. This method is based on a
general approach recently developed by the authors called ``Trust, but Verify&quot;
wherein an auxiliary verification network produces predictions about certain
masked features in the input image using the segmentation as an input. A
well-designed auxiliary network will produce high-quality predictions when the
input segmentations are accurate, but will produce low-quality predictions when
the segmentations are incorrect. Checking the predictions of such a network
with the original image allows us to detect bad segmentations. However, to
ensure the verification method is truly robust, we need a method for checking
the quality of the predictions that does not itself rely on a black-box neural
network. Indeed, we show that previous methods for segmentation evaluation that
do use deep neural regression networks are vulnerable to false negatives i.e.
can inaccurately label bad segmentations as good. We describe the design of a
verification network that avoids such vulnerability and present results to
demonstrate its robustness compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_F/0/1/0/all/0/1&quot;&gt;Fahim Ahmed Zaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weiyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1&quot;&gt;Milan Sonka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudumbai_R/0/1/0/all/0/1&quot;&gt;Raghuraman Mudumbai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17042">
<title>StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17042</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly advancing domain of deep learning optimization, this paper
unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded
Adam algorithm. Central to StochGradAdam is its gradient sampling technique.
This method not only ensures stable convergence but also leverages the
advantages of selective gradient consideration, fostering robust training by
potentially mitigating the effects of noisy or outlier data and enhancing the
exploration of the loss landscape for more dependable convergence. In both
image classification and segmentation tasks, StochGradAdam has demonstrated
superior performance compared to the traditional Adam optimizer. By judiciously
sampling a subset of gradients at each iteration, the optimizer is optimized
for managing intricate models. The paper provides a comprehensive exploration
of StochGradAdam&apos;s methodology, from its mathematical foundations to bias
correction strategies, heralding a promising advancement in deep learning
training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17050">
<title>Exploring Question Decomposition for Zero-Shot VQA. (arXiv:2310.17050v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17050</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering (VQA) has traditionally been treated as a
single-step task where each question receives the same amount of effort, unlike
natural human question-answering strategies. We explore a question
decomposition strategy for VQA to overcome this limitation. We probe the
ability of recently developed large vision-language models to use human-written
decompositions and produce their own decompositions of visual questions,
finding they are capable of learning both tasks from demonstrations alone.
However, we show that naive application of model-written decompositions can
hurt performance. We introduce a model-driven selective decomposition approach
for second-guessing predictions and correcting errors, and validate its
effectiveness on eight VQA tasks across three domains, showing consistent
improvements in accuracy, including improvements of &amp;gt;20% on medical VQA
datasets and boosting the zero-shot performance of BLIP-2 above chance on a VQA
reformulation of the challenging Winoground task. Project Site:
https://zaidkhan.me/decomposition-0shot-vqa/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1&quot;&gt;Zaid Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+BG_V/0/1/0/all/0/1&quot;&gt;Vijay Kumar BG&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1&quot;&gt;Samuel Schulter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yun Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17075">
<title>HyperFields: Towards Zero-Shot Generation of NeRFs from Text. (arXiv:2310.17075v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17075</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce HyperFields, a method for generating text-conditioned Neural
Radiance Fields (NeRFs) with a single forward pass and (optionally) some
fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns
a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF
distillation training, which distills scenes encoded in individual NeRFs into
one dynamic hypernetwork. These techniques enable a single network to fit over
a hundred unique scenes. We further demonstrate that HyperFields learns a more
general map between text and NeRFs, and consequently is capable of predicting
novel in-distribution and out-of-distribution scenes -- either zero-shot or
with a few finetuning steps. Finetuning HyperFields benefits from accelerated
convergence thanks to the learned general map, and is capable of synthesizing
novel scenes 5 to 10 times faster than existing neural optimization-based
methods. Our ablation experiments show that both the dynamic architecture and
NeRF distillation are critical to the expressivity of HyperFields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_S/0/1/0/all/0/1&quot;&gt;Sudarshan Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Richard Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Avery Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1&quot;&gt;Michael Maire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1&quot;&gt;Greg Shakhnarovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1&quot;&gt;Rana Hanocka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17078">
<title>HCT: Hybrid Convnet-Transformer for Parkinson&apos;s disease detection and severity prediction from gait. (arXiv:2310.17078v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17078</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel deep learning method based on a new Hybrid
ConvNet-Transformer architecture to detect and stage Parkinson&apos;s disease (PD)
from gait data. We adopt a two-step approach by dividing the problem into two
sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy
versus parkinsonian patients. If the patient is parkinsonian, a multi-class
Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&amp;amp;Y) score to
assess the PD severity stage. Our hybrid architecture exploits the strengths of
both Convolutional Neural Networks (ConvNets) and Transformers to accurately
detect PD and determine the severity stage. In particular, we take advantage of
ConvNets to capture local patterns and correlations in the data, while we
exploit Transformers for handling long-term dependencies in the input signal.
We show that our hybrid method achieves superior performance when compared to
other state-of-the-art methods, with a PD detection accuracy of 97% and a
severity staging accuracy of 87%. Our source code is available at:
https://github.com/SafwenNaimi
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naimi_S/0/1/0/all/0/1&quot;&gt;Safwen Naimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouachir_W/0/1/0/all/0/1&quot;&gt;Wassim Bouachir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1&quot;&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17080">
<title>Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images. (arXiv:2310.17080v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17080</link>
<description rdf:parseType="Literal">&lt;p&gt;Lichens are symbiotic organisms composed of fungi, algae, and/or
cyanobacteria that thrive in a variety of environments. They play important
roles in carbon and nitrogen cycling, and contribute directly and indirectly to
biodiversity. Ecologists typically monitor lichens by using them as indicators
to assess air quality and habitat conditions. In particular, epiphytic lichens,
which live on trees, are key markers of air quality and environmental health. A
new method of monitoring epiphytic lichens involves using time-lapse cameras to
gather images of lichen populations. These cameras are used by ecologists in
Newfoundland and Labrador to subsequently analyze and manually segment the
images to determine lichen thalli condition and change. These methods are
time-consuming and susceptible to observer bias. In this work, we aim to
automate the monitoring of lichens over extended periods and to estimate their
biomass and condition to facilitate the task of ecologists. To accomplish this,
our proposed framework uses semantic segmentation with an effective training
approach to automate monitoring and biomass estimation of epiphytic lichens on
time-lapse images. We show that our method has the potential to significantly
improve the accuracy and efficiency of lichen population monitoring, making it
a valuable tool for forest ecologists and environmental scientists to evaluate
the impact of climate change on Canada&apos;s forests. To the best of our knowledge,
this is the first time that such an approach has been used to assist ecologists
in monitoring and analyzing epiphytic lichens.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naimi_S/0/1/0/all/0/1&quot;&gt;Safwen Naimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koubaa_O/0/1/0/all/0/1&quot;&gt;Olfa Koubaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouachir_W/0/1/0/all/0/1&quot;&gt;Wassim Bouachir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1&quot;&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeddore_G/0/1/0/all/0/1&quot;&gt;Gregory Jeddore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baines_P/0/1/0/all/0/1&quot;&gt;Patricia Baines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Correia_D/0/1/0/all/0/1&quot;&gt;David Correia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arsenault_A/0/1/0/all/0/1&quot;&gt;Andre Arsenault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17097">
<title>Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Approach for Object Detection. (arXiv:2310.17097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17097</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has emerged as a potent framework for training models
across distributed data sources while maintaining data privacy. Nevertheless,
it faces challenges with limited high-quality labels and non-IID client data,
particularly in applications like autonomous driving. To address these hurdles,
we navigate the uncharted waters of Semi-Supervised Federated Object Detection
(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where
labeled data reside only at the server while clients possess unlabeled data.
Notably, our method represents the inaugural implementation of SSFOD for
clients with 0% labeled non-IID data, a stark contrast to previous studies that
maintain some subset of labels at each client. We propose FedSTO, a two-stage
strategy encompassing Selective Training followed by Orthogonally enhanced
full-parameter training, to effectively address data shift (e.g. weather
conditions) between server and clients. Our contributions include selectively
refining the backbone of the detector to avert overfitting, orthogonality
regularization to boost representation divergence, and local EMA-driven pseudo
label assignment to yield high-quality pseudo labels. Extensive validation on
prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)
attests to the efficacy of our approach, demonstrating state-of-the-art
results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as
well as fully-supervised centralized training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1&quot;&gt;Eric Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Christian Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mugunthan_V/0/1/0/all/0/1&quot;&gt;Vaikkunth Mugunthan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17109">
<title>LP-OVOD: Open-Vocabulary Object Detection by Linear Probing. (arXiv:2310.17109v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17109</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the challenging problem of open-vocabulary object
detection (OVOD) where an object detector must identify both seen and unseen
classes in test images without labeled examples of the unseen classes in
training. A typical approach for OVOD is to use joint text-image embeddings of
CLIP to assign box proposals to their closest text label. However, this method
has a critical issue: many low-quality boxes, such as over- and
under-covered-object boxes, have the same similarity score as high-quality
boxes since CLIP is not trained on exact object location information. To
address this issue, we propose a novel method, LP-OVOD, that discards
low-quality boxes by training a sigmoid linear classifier on pseudo labels
retrieved from the top relevant region proposals to the novel text.
Experimental results on COCO affirm the superior performance of our approach
over the state of the art, achieving $\textbf{40.5}$ in $\text{AP}_{novel}$
using ResNet50 as the backbone and without external datasets or knowing novel
classes during training. Our code will be available at
https://github.com/VinAIResearch/LP-OVOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Chau Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Truong Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khoi Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17122">
<title>Enhancing sea ice segmentation in Sentinel-1 images with atrous convolutions. (arXiv:2310.17122v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.17122</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the growing volume of remote sensing data and the low latency required
for safe marine navigation, machine learning (ML) algorithms are being
developed to accelerate sea ice chart generation, currently a manual
interpretation task. However, the low signal-to-noise ratio of the freely
available Sentinel-1 Synthetic Aperture Radar (SAR) imagery, the ambiguity of
backscatter signals for ice types, and the scarcity of open-source
high-resolution labelled data makes automating sea ice mapping challenging. We
use Extreme Earth version 2, a high-resolution benchmark dataset generated for
ML training and evaluation, to investigate the effectiveness of ML for
automated sea ice mapping. Our customized pipeline combines ResNets and Atrous
Spatial Pyramid Pooling for SAR image segmentation. We investigate the
performance of our model for: i) binary classification of sea ice and open
water in a segmentation framework; and ii) a multiclass segmentation of five
sea ice types. For binary ice-water classification, models trained with our
largest training set have weighted F1 scores all greater than 0.95 for January
and July test scenes. Specifically, the median weighted F1 score was 0.98,
indicating high performance for both months. By comparison, a competitive
baseline U-Net has a weighted average F1 score of ranging from 0.92 to 0.94
(median 0.93) for July, and 0.97 to 0.98 (median 0.97) for January. Multiclass
ice type classification is more challenging, and even though our models achieve
2% improvement in weighted F1 average compared to the baseline U-Net, test
weighted F1 is generally between 0.6 and 0.80. Our approach can efficiently
segment full SAR scenes in one run, is faster than the baseline U-Net, retains
spatial resolution and dimension, and is more robust against noise compared to
approaches that rely on patch classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lima_R/0/1/0/all/0/1&quot;&gt;Rafael Pires de Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vahedi_B/0/1/0/all/0/1&quot;&gt;Behzad Vahedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hughes_N/0/1/0/all/0/1&quot;&gt;Nick Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barrett_A/0/1/0/all/0/1&quot;&gt;Andrew P. Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_W/0/1/0/all/0/1&quot;&gt;Walter Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karimzadeh_M/0/1/0/all/0/1&quot;&gt;Morteza Karimzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17126">
<title>Deep Learning on SAR Imagery: Transfer Learning Versus Randomly Initialized Weights. (arXiv:2310.17126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17126</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying deep learning on Synthetic Aperture Radar (SAR) data is becoming
more common for mapping purposes. One such case is sea ice, which is highly
dynamic and rapidly changes as a result of the combined effect of wind,
temperature, and ocean currents. Therefore, frequent mapping of sea ice is
necessary to ensure safe marine navigation. However, there is a general
shortage of expert-labeled data to train deep learning algorithms. Fine-tuning
a pre-trained model on SAR imagery is a potential solution. In this paper, we
compare the performance of deep learning models trained from scratch using
randomly initialized weights against pre-trained models that we fine-tune for
this purpose. Our results show that pre-trained models lead to better results,
especially on test samples from the melt season.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimzadeh_M/0/1/0/all/0/1&quot;&gt;Morteza Karimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_R/0/1/0/all/0/1&quot;&gt;Rafael Pires de Lima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17128">
<title>Task-driven Prompt Evolution for Foundation Models. (arXiv:2310.17128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17128</link>
<description rdf:parseType="Literal">&lt;p&gt;Promptable foundation models, particularly Segment Anything Model (SAM), have
emerged as a promising alternative to the traditional task-specific supervised
learning for image segmentation. However, many evaluation studies have found
that their performance on medical imaging modalities to be underwhelming
compared to conventional deep learning methods. In the world of large
pre-trained language and vision-language models, learning prompt from
downstream tasks has achieved considerable success in improving performance. In
this work, we propose a plug-and-play Prompt Optimization Technique for
foundation models like SAM (SAMPOT) that utilizes the downstream segmentation
task to optimize the human-provided prompt to obtain improved performance. We
demonstrate the utility of SAMPOT on lung segmentation in chest X-ray images
and obtain an improvement on a significant number of cases ($\sim75\%$) over
human-provided initial prompts. We hope this work will lead to further
investigations in the nascent field of automatic visual prompt-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1&quot;&gt;Rachana Sathish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramani_R/0/1/0/all/0/1&quot;&gt;Rahul Venkataramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shriram_K/0/1/0/all/0/1&quot;&gt;K S Shriram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sudhakar_P/0/1/0/all/0/1&quot;&gt;Prasad Sudhakar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17131">
<title>Virtual Accessory Try-On via Keypoint Hallucination. (arXiv:2310.17131v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17131</link>
<description rdf:parseType="Literal">&lt;p&gt;The virtual try-on task refers to fitting the clothes from one image onto
another portrait image. In this paper, we focus on virtual accessory try-on,
which fits accessory (e.g., glasses, ties) onto a face or portrait image.
Unlike clothing try-on, which relies on human silhouette as guidance, accessory
try-on warps the accessory into an appropriate location and shape to generate a
plausible composite image. In contrast to previous try-on methods that treat
foreground (i.e., accessories) and background (i.e., human faces or bodies)
equally, we propose a background-oriented network to utilize the prior
knowledge of human bodies and accessories. Specifically, our approach learns
the human body priors and hallucinates the target locations of specified
foreground keypoints in the background. Then our approach will inject
foreground information with accessory priors into the background UNet. Based on
the hallucinated target locations, the warping parameters are calculated to
warp the foreground. Moreover, this background-oriented network can also easily
incorporate auxiliary human face/body semantic segmentation supervision to
further boost performance. Experiments conducted on STRAT dataset validate the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gou_J/0/1/0/all/0/1&quot;&gt;Junhong Gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Li Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1&quot;&gt;Jianlou Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17135">
<title>Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type Segmentation. (arXiv:2310.17135v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17135</link>
<description rdf:parseType="Literal">&lt;p&gt;Up-to-date sea ice charts are crucial for safer navigation in ice-infested
waters. Recently, Convolutional Neural Network (CNN) models show the potential
to accelerate the generation of ice maps for large regions. However, results
from CNN models still need to undergo scrutiny as higher metrics performance
not always translate to adequate outputs. Sea ice type classes are imbalanced,
requiring special treatment during training. We evaluate how three different
loss functions, some developed for imbalanced class problems, affect the
performance of CNN models trained to predict the dominant ice type in
Sentinel-1 images. Despite the fact that Dice and Focal loss produce higher
metrics, results from cross-entropy seem generally more physically consistent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_R/0/1/0/all/0/1&quot;&gt;Rafael Pires de Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahedi_B/0/1/0/all/0/1&quot;&gt;Behzad Vahedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimzadeh_M/0/1/0/all/0/1&quot;&gt;Morteza Karimzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17138">
<title>A Classifier Using Global Character Level and Local Sub-unit Level Features for Hindi Online Handwritten Character Recognition. (arXiv:2310.17138v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17138</link>
<description rdf:parseType="Literal">&lt;p&gt;A classifier is developed that defines a joint distribution of global
character features, number of sub-units and local sub-unit features to model
Hindi online handwritten characters. The classifier uses latent variables to
model the structure of sub-units. The classifier uses histograms of points,
orientations, and dynamics of orientations (HPOD) features to represent
characters at global character level and local sub-unit level and is
independent of character stroke order and stroke direction variations. The
parameters of the classifier is estimated using maximum likelihood method.
Different classifiers and features used in other studies are considered in this
study for classification performance comparison with the developed classifier.
The classifiers considered are Second Order Statistics (SOS), Sub-space (SS),
Fisher Discriminant (FD), Feedforward Neural Network (FFN) and Support Vector
Machines (SVM) and the features considered are Spatio Temporal (ST), Discrete
Fourier Transform (DFT), Discrete Cosine Transform (SCT), Discrete Wavelet
Transform (DWT), Spatial (SP) and Histograms of Oriented Gradients (HOG). Hindi
character datasets used for training and testing the developed classifier
consist of samples of handwritten characters from 96 different character
classes. There are 12832 samples with an average of 133 samples per character
class in the training set and 2821 samples with an average of 29 samples per
character class in the testing set. The developed classifier has the highest
accuracy of 93.5\% on the testing set compared to that of the classifiers
trained on different features extracted from the same training set and
evaluated on the same testing set considered in this study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anand Sharma&lt;/a&gt; (MIET, Meerut), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1&quot;&gt;A. G. Ramakrishnan&lt;/a&gt; (IISc, Bengaluru)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17147">
<title>Simple Baselines for Projection-based Full-reference and No-reference Point Cloud Quality Assessment. (arXiv:2310.17147v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17147</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are widely used in 3D content representation and have various
applications in multimedia. However, compression and simplification processes
inevitably result in the loss of quality-aware information under storage and
bandwidth constraints. Therefore, there is an increasing need for effective
methods to quantify the degree of distortion in point clouds. In this paper, we
propose simple baselines for projection-based point cloud quality assessment
(PCQA) to tackle this challenge. We use multi-projections obtained via a common
cube-like projection process from the point clouds for both full-reference (FR)
and no-reference (NR) PCQA tasks. Quality-aware features are extracted with
popular vision backbones. The FR quality representation is computed as the
similarity between the feature maps of reference and distorted projections
while the NR quality representation is obtained by simply squeezing the feature
maps of distorted projections with average pooling The corresponding quality
representations are regressed into visual quality scores by fully-connected
layers. Taking part in the ICIP 2023 PCVQA Challenge, we succeeded in achieving
the top spot in four out of the five competition tracks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingjie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17152">
<title>Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17152</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current study, our purpose is to evaluate the feasibility of applying
deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in
healthy controls scanned at 0.55T and compared with 3.0T. The current study
assesses the performance of standard in-practice bone, and cartilage
segmentation algorithms at 0.55T, both qualitatively and quantitatively, in
terms of comparing segmentation performance, areas of improvement, and
compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial
results demonstrate a usable to good technical feasibility of translating
existing quantitative deep-learning-based image segmentation techniques,
trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition
environment. Especially in terms of segmenting cartilage compartments, the
models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T
low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be
utilized for evaluating knee cartilage thickness and bone segmentations aided
by established DL algorithms trained at higher-field strengths out-of-the-box
initially. This could be utilized at the far-spread point-of-care locations
with a lack of radiologists available to manually segment low-field images, at
least till a decent base of low-field data pool is collated. With further
fine-tuning with manual labeling of low-field data or utilizing synthesized
higher SNR images from low-field images, OA biomarker quantification
performance is potentially guaranteed to be further improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_R/0/1/0/all/0/1&quot;&gt;Rupsa Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkaya_Z/0/1/0/all/0/1&quot;&gt;Zehra Akkaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luitjens_J/0/1/0/all/0/1&quot;&gt;Johanna Luitjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Pan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1&quot;&gt;Valentina Pedoia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Sharmila Majumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17154">
<title>Deep Imbalanced Regression via Hierarchical Classification Adjustment. (arXiv:2310.17154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17154</link>
<description rdf:parseType="Literal">&lt;p&gt;Regression tasks in computer vision, such as age estimation or counting, are
often formulated into classification by quantizing the target space into
classes. Yet real-world data is often imbalanced -- the majority of training
samples lie in a head range of target values, while a minority of samples span
a usually larger tail range. By selecting the class quantization, one can
adjust imbalanced regression targets into balanced classification outputs,
though there are trade-offs in balancing classification accuracy and
quantization error. To improve regression performance over the entire range of
data, we propose to construct hierarchical classifiers for solving imbalanced
regression tasks. The fine-grained classifiers limit the quantization error
while being modulated by the coarse predictions to ensure high accuracy.
Standard hierarchical classification approaches, however, when applied to the
regression problem, fail to ensure that predicted ranges remain consistent
across the hierarchy. As such, we propose a range-preserving distillation
process that can effectively learn a single classifier from the set of
hierarchical classifiers. Our novel hierarchical classification adjustment
(HCA) for imbalanced regression shows superior results on three diverse tasks:
age estimation, crowd counting and depth estimation. We will release the source
code upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haipeng Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Angela Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17156">
<title>Learning depth from monocular video sequences. (arXiv:2310.17156v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17156</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning single image depth estimation model from monocular video sequence is
a very challenging problem. In this paper, we propose a novel training loss
which enables us to include more images for supervision during the training
process. We propose a simple yet effective model to account the frame to frame
pixel motion. We also design a novel network architecture for single image
estimation. When combined, our method produces state of the art results for
monocular depth estimation on the KITTI dataset in the self-supervised setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17158">
<title>CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter. (arXiv:2310.17158v1 [astro-ph.EP])</title>
<link>http://arxiv.org/abs/2310.17158</link>
<description rdf:parseType="Literal">&lt;p&gt;The Kessler syndrome refers to the escalating space debris from frequent
space activities, threatening future space exploration. Addressing this issue
is vital. Several AI models, including Convolutional Neural Networks (CNN),
Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning
(MAML), have been assessed with various data types. Earlier studies highlighted
the combination of the YOLO object detector and a linear Kalman filter for
object detection and tracking. Building on this, our project introduces
CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter
for tracking satellites in sequential images, compared to a linear Kalman
filter. Using the SPARK dataset from the University of Luxembourg for training
and testing, the YOLOv3 precisely detected and classified all satellite
categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237).
Both CosmosDSR and the LKF tracked satellites accurately (UKF:
MSE=2.83/RMSE=1.66, LKF: MSE=2.84/RMSE=1.66). Despite concerns of class
imbalance and the absence of real images, the model shows promise. Future work
should address these limitations, increase tracking sample size, and improve
metrics. This research suggests the algorithm&apos;s potential in detecting and
tracking satellites, paving the way for solutions to the Kessler syndrome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Roll_D/0/1/0/all/0/1&quot;&gt;Daniel S. Roll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kurt_Z/0/1/0/all/0/1&quot;&gt;Zeyneb Kurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Woo_W/0/1/0/all/0/1&quot;&gt;Wai Lok Woo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17163">
<title>Low-Dimensional Gradient Helps Out-of-Distribution Detection. (arXiv:2310.17163v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17163</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting out-of-distribution (OOD) samples is essential for ensuring the
reliability of deep neural networks (DNNs) in real-world scenarios. While
previous research has predominantly investigated the disparity between
in-distribution (ID) and OOD data through forward information analysis, the
discrepancy in parameter gradients during the backward process of DNNs has
received insufficient attention. Existing studies on gradient disparities
mainly focus on the utilization of gradient norms, neglecting the wealth of
information embedded in gradient directions. To bridge this gap, in this paper,
we conduct a comprehensive investigation into leveraging the entirety of
gradient information for OOD detection. The primary challenge arises from the
high dimensionality of gradients due to the large number of network parameters.
To solve this problem, we propose performing linear dimension reduction on the
gradient using a designated subspace that comprises principal components. This
innovative technique enables us to obtain a low-dimensional representation of
the gradient with minimal information loss. Subsequently, by integrating the
reduced gradient with various existing detection score functions, our approach
demonstrates superior performance across a wide range of detection tasks. For
instance, on the ImageNet benchmark, our method achieves an average reduction
of 11.15% in the false positive rate at 95% recall (FPR95) compared to the
current state-of-the-art approach. The code would be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xinwen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17164">
<title>Bridging Phylogeny and Taxonomy with Protein-protein Interaction Networks. (arXiv:2310.17164v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17164</link>
<description rdf:parseType="Literal">&lt;p&gt;The protein-protein interaction (PPI) network provides an overview of the
complex biological reactions vital to an organism&apos;s metabolism and survival.
Even though in the past PPI network were compared across organisms in detail,
there has not been large-scale research on how individual PPI networks reflect
on the species relationships. In this study we aim to increase our
understanding of the tree of life and taxonomy by gleaming information from the
PPI networks. We successful created (1) a predictor of network statistics based
on known traits of existing species in the phylogeny, and (2) a taxonomic
classifier of organism using the known protein network statistics, whether
experimentally determined or predicted de novo. With the knowledge of protein
interactions at its core, our two models effectively connects two field with
widely diverging methodologies - the phylogeny and taxonomy of species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long-Huei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moorthy_M/0/1/0/all/0/1&quot;&gt;Mohana Prasad Sathya Moorthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pratyaksh Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17167">
<title>Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17167</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces two key contributions aimed at improving the speed and
quality of images generated through inverse diffusion processes. The first
contribution involves reparameterizing the diffusion process in terms of the
angle on a quarter-circular arc between the image and noise, specifically
setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This
reparameterization eliminates two singularities and allows for the expression
of diffusion evolution as a well-behaved ordinary differential equation (ODE).
In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be
used effectively. The second contribution is to directly estimate both the
image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which
enables more stable calculations of the update step in the inverse diffusion
steps, as accurate estimation of both the image and noise are crucial at
different stages of the process. Together with these changes, our model
achieves faster generation, with the ability to converge on high-quality images
more quickly, and higher quality of the generated images, as measured by
metrics such as Frechet Inception Distance (FID), spatial Frechet Inception
Distance (sFID), precision, and recall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenkai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1&quot;&gt;Krista A. Ehinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1&quot;&gt;Tom Drummond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17170">
<title>MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR. (arXiv:2310.17170v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17170</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to address critical issues in the field of Multi-Object
Tracking (MOT) by proposing an efficient and computationally resource-efficient
end-to-end multi-object tracking model, named MO-YOLO. Traditional MOT methods
typically involve two separate steps: object detection and object tracking,
leading to computational complexity and error propagation issues. Recent
research has demonstrated outstanding performance in end-to-end MOT models
based on Transformer architectures, but they require substantial hardware
support. MO-YOLO combines the strengths of YOLO and RT-DETR models to construct
a high-efficiency, lightweight, and resource-efficient end-to-end multi-object
tracking network, offering new opportunities in the multi-object tracking
domain. On the MOT17 dataset, MOTR\cite{zeng2022motr} requires training with 8
GeForce 2080 Ti GPUs for 4 days to achieve satisfactory results, while MO-YOLO
only requires 1 GeForce 2080 Ti GPU and 12 hours of training to achieve
comparable performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_W/0/1/0/all/0/1&quot;&gt;Wu Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liu Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xingle_Z/0/1/0/all/0/1&quot;&gt;Zhang Xingle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17176">
<title>A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17176</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate teeth segmentation and orientation are fundamental in modern oral
healthcare, enabling precise diagnosis, treatment planning, and dental implant
design. In this study, we present a comprehensive approach to teeth
segmentation and orientation from panoramic X-ray images, leveraging deep
learning techniques. We build our model based on FUSegNet, a popular model
originally developed for wound segmentation, and introduce modifications by
incorporating grid-based attention gates into the skip connections. We
introduce oriented bounding box (OBB) generation through principal component
analysis (PCA) for precise tooth orientation estimation. Evaluating our
approach on the publicly available DNS dataset, comprising 543 panoramic X-ray
images, we achieve the highest Intersection-over-Union (IoU) score of 82.43%
and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in
teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU)
score of 82.82%. We also conduct detailed analyses of individual tooth labels
and categorical performance, shedding light on strengths and weaknesses. The
proposed model&apos;s accuracy and versatility offer promising prospects for
improving dental diagnoses, treatment planning, and personalized healthcare in
the oral domain. Our generated OBB coordinates and codes are available at
https://github.com/mrinal054/Instance_teeth_segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhar_M/0/1/0/all/0/1&quot;&gt;Mrinal Kanti Dhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1&quot;&gt;Mou Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhab_D/0/1/0/all/0/1&quot;&gt;D. Madhab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zeyun Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17177">
<title>Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17177</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of transformers on various computer vision tasks, they
suffer from excessive memory and computational cost. Some works present dynamic
vision transformers to accelerate inference by pruning redundant tokens. A key
to improving token pruning is using well-trained models as initialization for
faster convergence and better performance. However, current base models usually
adopt full image training, i.e., using full images as inputs and keeping the
whole feature maps through the forward process, which causes inconsistencies
with dynamic models that gradually reduce tokens, including calculation
pattern, information amount and token selection strategy inconsistencies.
Inspired by MAE which performs masking and reconstruction self-supervised task,
we devise masked fine-tuning to bridge the gaps between pre-trained base models
used for initialization and token pruning based dynamic vision transformers, by
masking image patches and predicting the image class label based on left
unmasked patches. Extensive experiments on ImageNet demonstrate that base
models via masked fine-tuning gain strong occlusion robustness and ability
against information loss. With this better initialization, Dynamic ViT achieves
higher accuracies, especially under large token pruning ratios (e.g., 81.9% vs.
81.3%, and 62.3% vs. 58.9% for DeiT based Dynamic ViT/0.8 and Dynamic ViT/0.3).
Moreover, we apply our method into different token pruning based dynamic vision
transformers, different pre-trained models and randomly initialized models to
demonstrate the generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Fengyuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17183">
<title>Understanding the Effects of Projectors in Knowledge Distillation. (arXiv:2310.17183v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17183</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventionally, during the knowledge distillation process (e.g. feature
distillation), an additional projector is often required to perform feature
transformation due to the dimension mismatch between the teacher and the
student networks. Interestingly, we discovered that even if the student and the
teacher have the same feature dimensions, adding a projector still helps to
improve the distillation performance. In addition, projectors even improve
logit distillation if we add them to the architecture too. Inspired by these
surprising findings and the general lack of understanding of the projectors in
the knowledge distillation process from existing literature, this paper
investigates the implicit role that projectors play but so far have been
overlooked. Our empirical study shows that the student with a projector (1)
obtains a better trade-off between the training accuracy and the testing
accuracy compared to the student without a projector when it has the same
feature dimensions as the teacher, (2) better preserves its similarity to the
teacher beyond shallow and numeric resemblance, from the view of Centered
Kernel Alignment (CKA), and (3) avoids being over-confident as the teacher does
at the testing phase. Motivated by the positive effects of projectors, we
propose a projector ensemble-based feature distillation method to further
improve distillation performance. Despite the simplicity of the proposed
strategy, empirical results from the evaluation of classification tasks on
benchmark datasets demonstrate the superior classification performance of our
method on a broad range of teacher-student pairs and verify from the aspects of
CKA and model calibration that the student&apos;s features are of improved quality
with the projector ensemble design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yudong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoog_F/0/1/0/all/0/1&quot;&gt;Frank de Hoog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusy_B/0/1/0/all/0/1&quot;&gt;Brano Kusy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17188">
<title>Blind Image Super-resolution with Rich Texture-Aware Codebooks. (arXiv:2310.17188v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17188</link>
<description rdf:parseType="Literal">&lt;p&gt;Blind super-resolution (BSR) methods based on high-resolution (HR)
reconstruction codebooks have achieved promising results in recent years.
However, we find that a codebook based on HR reconstruction may not effectively
capture the complex correlations between low-resolution (LR) and HR images. In
detail, multiple HR images may produce similar LR versions due to complex blind
degradations, causing the HR-dependent only codebooks having limited texture
diversity when faced with confusing LR inputs. To alleviate this problem, we
propose the Rich Texture-aware Codebook-based Network (RTCNet), which consists
of the Degradation-robust Texture Prior Module (DTPM) and the Patch-aware
Texture Prior Module (PTPM). DTPM effectively mines the cross-resolution
correlation of textures between LR and HR images by exploiting the
cross-resolution correspondence of textures. PTPM uses patch-wise semantic
pre-training to correct the misperception of texture similarity in the
high-level semantic regularization. By taking advantage of this, RTCNet
effectively gets rid of the misalignment of confusing textures between HR and
LR in the BSR scenarios. Experiments show that RTCNet outperforms
state-of-the-art methods on various benchmarks by up to 0.16 ~ 0.46dB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1&quot;&gt;Rui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Ming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fangyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xing Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17189">
<title>Exploring Iterative Refinement with Diffusion Models for Video Grounding. (arXiv:2310.17189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17189</link>
<description rdf:parseType="Literal">&lt;p&gt;Video grounding aims to localize the target moment in an untrimmed video
corresponding to a given sentence query. Existing methods typically select the
best prediction from a set of predefined proposals or directly regress the
target span in a single-shot manner, resulting in the absence of a systematical
prediction refinement process. In this paper, we propose DiffusionVG, a novel
framework with diffusion models that formulates video grounding as a
conditional generation task, where the target span is generated from Gaussian
noise inputs and interatively refined in the reverse diffusion process. During
training, DiffusionVG progressively adds noise to the target span with a fixed
forward diffusion process and learns to recover the target span in the reverse
diffusion process. In inference, DiffusionVG can generate the target span from
Gaussian noise inputs by the learned reverse diffusion process conditioned on
the video-sentence representations. Our DiffusionVG follows the encoder-decoder
architecture, which firstly encodes the video-sentence features and iteratively
denoises the predicted spans in its specialized span refining decoder. Without
bells and whistles, our DiffusionVG demonstrates competitive or even superior
performance compared to existing well-crafted models on mainstream Charades-STA
and ActivityNet Captions benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Tao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yaoyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1&quot;&gt;Te Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shao-Lun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17190">
<title>Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping. (arXiv:2310.17190v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17190</link>
<description rdf:parseType="Literal">&lt;p&gt;Tone mapping aims to convert high dynamic range (HDR) images to low dynamic
range (LDR) representations, a critical task in the camera imaging pipeline. In
recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained
attention due to their ability to strike a favorable balance between
enhancement performance and computational efficiency. However, these methods
often fail to deliver satisfactory results in local areas since the look-up
table is a global operator for tone mapping, which works based on pixel values
and fails to incorporate crucial local information. To this end, this paper
aims to address this issue by exploring a novel strategy that integrates global
and local operators by utilizing closed-form Laplacian pyramid decomposition
and reconstruction. Specifically, we employ image-adaptive 3D LUTs to
manipulate the tone in the low-frequency image by leveraging the specific
characteristics of the frequency information. Furthermore, we utilize local
Laplacian filters to refine the edge details in the high-frequency components
in an adaptive manner. Local Laplacian filters are widely used to preserve edge
details in photographs, but their conventional usage involves manual tuning and
fixed implementation within camera imaging pipelines or photo editing tools. We
propose to learn parameter value maps progressively for local Laplacian filters
from annotated data using a lightweight network. Our model achieves
simultaneous global tone manipulation and local edge detail preservation in an
end-to-end manner. Extensive experimental results on two benchmark datasets
demonstrate that the proposed method performs favorably against
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Feng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1&quot;&gt;Ming Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qingbo Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Changxin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1&quot;&gt;Nong Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17209">
<title>Weakly-Supervised Surgical Phase Recognition. (arXiv:2310.17209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17209</link>
<description rdf:parseType="Literal">&lt;p&gt;A key element of computer-assisted surgery systems is phase recognition of
surgical videos. Existing phase recognition algorithms require frame-wise
annotation of a large number of videos, which is time and money consuming. In
this work we join concepts of graph segmentation with self-supervised learning
to derive a random-walk solution for per-frame phase prediction. Furthermore,
we utilize within our method two forms of weak supervision: sparse timestamps
or few-shot learning. The proposed algorithm enjoys low complexity and can
operate in lowdata regimes. We validate our method by running experiments with
the public Cholec80 dataset of laparoscopic cholecystectomy videos,
demonstrating promising performance in multiple setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirsch_R/0/1/0/all/0/1&quot;&gt;Roy Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Regev Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1&quot;&gt;Mathilde Caron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golany_T/0/1/0/all/0/1&quot;&gt;Tomer Golany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_D/0/1/0/all/0/1&quot;&gt;Daniel Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1&quot;&gt;Ehud Rivlin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17212">
<title>Emotion Recognition by Video: A review. (arXiv:2310.17212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17212</link>
<description rdf:parseType="Literal">&lt;p&gt;Video emotion recognition is an important branch of affective computing, and
its solutions can be applied in different fields such as human-computer
interaction (HCI) and intelligent medical treatment. Although the number of
papers published in the field of emotion recognition is increasing, there are
few comprehensive literature reviews covering related research on video emotion
recognition. Therefore, this paper selects articles published from 2015 to 2023
to systematize the existing trends in video emotion recognition in related
studies. In this paper, we first talk about two typical emotion models, then we
talk about databases that are frequently utilized for video emotion
recognition, including unimodal databases and multimodal databases. Next, we
look at and classify the specific structure and performance of modern unimodal
and multimodal video emotion recognition methods, talk about the benefits and
drawbacks of each, and then we compare them in detail in the tables. Further,
we sum up the primary difficulties right now looked by video emotion
recognition undertakings and point out probably the most encouraging future
headings, such as establishing an open benchmark database and better multimodal
fusion strategys. The essential objective of this paper is to assist scholarly
and modern scientists with keeping up to date with the most recent advances and
new improvements in this speedy, high-influence field of video emotion
recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Junxiao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuecheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Liangyu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17216">
<title>Three-dimensional Bone Image Synthesis with Generative Adversarial Networks. (arXiv:2310.17216v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2310.17216</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image processing has been highlighted as an area where deep
learning-based models have the greatest potential. However, in the medical
field in particular, problems of data availability and privacy are hampering
research progress and thus rapid implementation in clinical routine. The
generation of synthetic data not only ensures privacy, but also allows to
\textit{draw} new patients with specific characteristics, enabling the
development of data-driven models on a much larger scale. This work
demonstrates that three-dimensional generative adversarial networks (GANs) can
be efficiently trained to generate high-resolution medical volumes with finely
detailed voxel-based architectures. In addition, GAN inversion is successfully
implemented for the three-dimensional setting and used for extensive research
on model interpretability and applications such as image morphing, attribute
editing and style mixing. The results are comprehensively validated on a
database of three-dimensional HR-pQCT instances representing the bone
micro-architecture of the distal radius.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Angermann_C/0/1/0/all/0/1&quot;&gt;Christoph Angermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bereiter_Payr_J/0/1/0/all/0/1&quot;&gt;Johannes Bereiter-Payr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stock_K/0/1/0/all/0/1&quot;&gt;Kerstin Stock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haltmeier_M/0/1/0/all/0/1&quot;&gt;Markus Haltmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Degenhart_G/0/1/0/all/0/1&quot;&gt;Gerald Degenhart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17218">
<title>Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification. (arXiv:2310.17218v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17218</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to adapt large-scale pre-trained vision-language models, such
as contrastive language-image pretraining (CLIP), to enhance the performance of
object reidentification (Re-ID) across various supervision settings. Although
prompt learning has enabled a recent work named CLIP-ReID to achieve promising
performance, the underlying mechanisms and the necessity of prompt learning
remain unclear due to the absence of semantic labels in ReID tasks. In this
work, we first analyze the role prompt learning in CLIP-ReID and identify its
limitations. Based on our investigations, we propose a simple yet effective
approach to adapt CLIP for supervised object Re-ID. Our approach directly
fine-tunes the image encoder of CLIP using a prototypical contrastive learning
(PCL) loss, eliminating the need for prompt learning. Experimental results on
both person and vehicle Re-ID datasets demonstrate the competitiveness of our
method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP
fine-tuning approach to unsupervised scenarios, where we achieve state-of-the
art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xiaojin Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17255">
<title>Generalizing to Unseen Domains in Diabetic Retinopathy Classification. (arXiv:2310.17255v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17255</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetic retinopathy (DR). is caused by long-standing diabetes and is among
the fifth leading cause for visual impairments. The process of early diagnosis
and treatments could be helpful in curing the disease, however, the detection
procedure is rather challenging and mostly tedious. Therefore, automated
diabetic retinopathy classification using deep learning techniques has gained
interest in the medical imaging community. Akin to several other real-world
applications of deep learning, the typical assumption of i.i.d data is also
violated in DR classification that relies on deep learning. Therefore,
developing DR classification methods robust to unseen distributions is of great
value. In this paper, we study the problem of generalizing a model to unseen
distributions or domains (a.k.a domain generalization) in DR classification. To
this end, we propose a simple and effective domain generalization (DG) approach
that achieves self-distillation in vision transformers (ViT) via a novel
prediction softening mechanism. This prediction softening is an adaptive convex
combination one-hot labels with the model&apos;s own knowledge. We perform extensive
experiments on challenging open-source DR classification datasets under both
multi-source and single-source DG settings with three different ViT backbones
to establish the efficacy and applicability of our approach against competing
methods. For the first time, we report the performance of several
state-of-the-art DG methods on open-source DR classification datasets after
conducting thorough experiments. Finally, our method is also capable of
delivering improved calibration performance than other methods, showing its
suitability for safety-critical applications, including healthcare. We hope
that our contributions would investigate more DG research across the medical
imaging community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galappaththige_C/0/1/0/all/0/1&quot;&gt;Chamuditha Jayanga Galappaththige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuruppu_G/0/1/0/all/0/1&quot;&gt;Gayal Kuruppu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Haris Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17261">
<title>Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17261</link>
<description rdf:parseType="Literal">&lt;p&gt;When the training dataset comprises a 1:1 proportion of dogs to cats, a
generative model that produces 1:1 dogs and cats better resembles the training
species distribution than another model with 3:1 dogs and cats. Can we capture
this phenomenon using existing metrics? Unfortunately, we cannot, because these
metrics do not provide any interpretability beyond &quot;diversity&quot;. In this
context, we propose a new evaluation protocol that measures the divergence of a
set of generated images from the training set regarding the distribution of
attribute strengths as follows. Single-attribute Divergence (SaD) measures the
divergence regarding PDFs of a single attribute. Paired-attribute Divergence
(PaD) measures the divergence regarding joint PDFs of a pair of attributes.
They provide which attributes the models struggle. For measuring the attribute
strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures
the cosine similarity between image and text vectors with heterogeneous initial
points. With SaD and PaD, we reveal the following about existing generative
models. ProjectedGAN generates implausible attribute relationships such as a
baby with a beard even though it has competitive scores of existing metrics.
Diffusion models struggle to capture diverse colors in the datasets. The larger
sampling timesteps of latent diffusion model generate the more minor objects
including earrings and necklaces. Stable Diffusion v1.5 better captures the
attributes than v2.1. Our metrics lay a foundation for explainable evaluations
of generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongkyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_M/0/1/0/all/0/1&quot;&gt;Mingi Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17281">
<title>BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds. (arXiv:2310.17281v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17281</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a surprisingly simple and efficient method for self-supervision of
3D backbone on automotive Lidar point clouds. We design a contrastive loss
between features of Lidar scans captured in the same scene. Several such
approaches have been proposed in the literature from PointConstrast, which uses
a contrast at the level of points, to the state-of-the-art TARL, which uses a
contrast at the level of segments, roughly corresponding to objects. While the
former enjoys a great simplicity of implementation, it is surpassed by the
latter, which however requires a costly pre-processing. In BEVContrast, we
define our contrast at the level of 2D cells in the Bird&apos;s Eye View plane.
Resulting cell-level representations offer a good trade-off between the
point-level representations exploited in PointContrast and segment-level
representations exploited in TARL: we retain the simplicity of PointContrast
(cell representations are cheap to compute) while surpassing the performance of
TARL in downstream semantic segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sautier_C/0/1/0/all/0/1&quot;&gt;Corentin Sautier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1&quot;&gt;Gilles Puy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1&quot;&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1&quot;&gt;Renaud Marlet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1&quot;&gt;Vincent Lepetit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17290">
<title>RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments. (arXiv:2310.17290v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17290</link>
<description rdf:parseType="Literal">&lt;p&gt;Intention-oriented object detection aims to detect desired objects based on
specific intentions or requirements. For instance, when we desire to &quot;lie down
and rest&quot;, we instinctively seek out a suitable option such as a &quot;bed&quot; or a
&quot;sofa&quot; that can fulfill our needs. Previous work in this area is limited either
by the number of intention descriptions or by the affordance vocabulary
available for intention objects. These limitations make it challenging to
handle intentions in open environments effectively. To facilitate this
research, we construct a comprehensive dataset called Reasoning
Intention-Oriented Objects (RIO). In particular, RIO is specifically designed
to incorporate diverse real-world scenarios and a wide range of object
categories. It offers the following key features: 1) intention descriptions in
RIO are represented as natural sentences rather than a mere word or verb
phrase, making them more practical and meaningful; 2) the intention
descriptions are contextually relevant to the scene, enabling a broader range
of potential functionalities associated with the objects; 3) the dataset
comprises a total of 40,214 images and 130,585 intention-object pairs. With the
proposed RIO, we evaluate the ability of some existing models to reason
intention-oriented objects in open environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1&quot;&gt;Mengxue Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17294">
<title>Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution. (arXiv:2310.17294v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17294</link>
<description rdf:parseType="Literal">&lt;p&gt;The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual
quality of videos, by simultaneously performing video frame interpolation (VFI)
and video super-resolution (VSR). However, facing the challenge of the
additional temporal dimension and scale inconsistency, most existing STVSR
methods are complex and inflexible in dynamically modeling different motion
amplitudes. In this work, we find that choosing an appropriate processing scale
achieves remarkable benefits in flow-based feature propagation. We propose a
novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects
sub-networks with different processing scales for individual samples.
Experiments on four public STVSR benchmarks demonstrate that SAFA achieves
state-of-the-art performance. Our SAFA network outperforms recent
state-of-the-art methods such as TMNet and VideoINR by an average improvement
of over 0.5dB on PSNR, while requiring less than half the number of parameters
and only 1/3 computational costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Ailin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaotao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuchang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17316">
<title>Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics. (arXiv:2310.17316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17316</link>
<description rdf:parseType="Literal">&lt;p&gt;Defect inspection is paramount within the closed-loop manufacturing system.
However, existing datasets for defect inspection often lack precision and
semantic granularity required for practical applications. In this paper, we
introduce the Defect Spectrum, a comprehensive benchmark that offers precise,
semantic-abundant, and large-scale annotations for a wide range of industrial
defects. Building on four key industrial benchmarks, our dataset refines
existing annotations and introduces rich semantic details, distinguishing
multiple defect types within a single image. Furthermore, we introduce
Defect-Gen, a two-stage diffusion-based generator designed to create
high-quality and diverse defective images, even when working with limited
datasets. The synthetic images generated by Defect-Gen significantly enhance
the efficacy of defect inspection models. Overall, The Defect Spectrum dataset
demonstrates its potential in defect inspection research, offering a solid
platform for testing and refining advanced models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pengguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingcong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17323">
<title>IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting. (arXiv:2310.17323v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17323</link>
<description rdf:parseType="Literal">&lt;p&gt;Although action recognition for procedural tasks has received notable
attention, it has a fundamental flaw in that no measure of success for actions
is provided. This limits the applicability of such systems especially within
the industrial domain, since the outcome of procedural actions is often
significantly more important than the mere execution. To address this
limitation, we define the novel task of procedure step recognition (PSR),
focusing on recognizing the correct completion and order of procedural steps.
Alongside the new task, we also present the multi-modal IndustReal dataset.
Unlike currently available datasets, IndustReal contains procedural errors
(such as omissions) as well as execution errors. A significant part of these
errors are exclusively present in the validation and test sets, making
IndustReal suitable to evaluate robustness of algorithms to new, unseen
mistakes. Additionally, to encourage reproducibility and allow for scalable
approaches trained on synthetic data, the 3D models of all parts are publicly
available. Annotations and benchmark performance are provided for action
recognition and assembly state detection, as well as the new PSR task.
IndustReal, along with the code and model weights, is available at:
https://github.com/TimSchoonbeek/IndustReal .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoonbeek_T/0/1/0/all/0/1&quot;&gt;Tim J. Schoonbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houben_T/0/1/0/all/0/1&quot;&gt;Tim Houben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onvlee_H/0/1/0/all/0/1&quot;&gt;Hans Onvlee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1&quot;&gt;Peter H.N. de With&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1&quot;&gt;Fons van der Sommen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17325">
<title>C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.17325</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation learning assumes that real-world data is generated by a few
semantically meaningful generative factors (i.e., sources of variation) and
aims to discover them in the latent space. These factors are expected to be
causally disentangled, meaning that distinct factors are encoded into separate
latent variables, and changes in one factor will not affect the values of the
others. Compared to statistical independence, causal disentanglement allows
more controllable data generation, improved robustness, and better
generalization. However, most existing work assumes unconfoundedness in the
discovery process, that there are no common causes to the generative factors
and thus obtain only statistical independence. In this paper, we recognize the
importance of modeling confounders in discovering causal generative factors.
Unfortunately, such factors are not identifiable without proper inductive bias.
We fill the gap by introducing a framework entitled Confounded-Disentanglement
(C-Disentanglement), the first framework that explicitly introduces the
inductive bias of confounder via labels from domain expertise. In addition, we
accordingly propose an approach to sufficiently identify the causally
disentangled factors under any inductive bias of the confounder. We conduct
extensive experiments on both synthetic and real-world datasets. Our method
demonstrates competitive results compared to various SOTA baselines in
obtaining causally disentangled features and downstream tasks under domain
shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiaxin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17347">
<title>CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling. (arXiv:2310.17347v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17347</link>
<description rdf:parseType="Literal">&lt;p&gt;While conditional diffusion models are known to have good coverage of the
data distribution, they still face limitations in output diversity,
particularly when sampled with a high classifier-free guidance scale for
optimal image quality or when trained on small datasets. We attribute this
problem to the role of the conditioning signal in inference and offer an
improved sampling strategy for diffusion models that can increase generation
diversity, especially at high guidance scales, with minimal loss of sample
quality. Our sampling strategy anneals the conditioning signal by adding
scheduled, monotonically decreasing Gaussian noise to the conditioning vector
during inference to balance diversity and condition alignment. Our
Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained
model and sampling algorithm, and we show that it boosts the diversity of
diffusion models in various conditional generation tasks. Further, using an
existing pretrained diffusion model, CADS achieves a new state-of-the-art FID
of 1.70 and 2.31 for class-conditional ImageNet generation at 256$\times$256
and 512$\times$512 respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadat_S/0/1/0/all/0/1&quot;&gt;Seyedmorteza Sadat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1&quot;&gt;Jakob Buhmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradely_D/0/1/0/all/0/1&quot;&gt;Derek Bradely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1&quot;&gt;Romann M. Weber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17356">
<title>Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning. (arXiv:2310.17356v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17356</link>
<description rdf:parseType="Literal">&lt;p&gt;Ahead-of-time forecasting of the output power of power plants is essential
for the stability of the electricity grid and ensuring uninterrupted service.
However, forecasting renewable energy sources is difficult due to the chaotic
behavior of natural energy sources. This paper presents a new approach to
estimate short-term solar irradiance from sky images. The~proposed algorithm
extracts features from sky images and use learning-based techniques to estimate
the solar irradiance. The~performance of proposed machine learning (ML)
algorithm is evaluated using two publicly available datasets of sky images.
The~datasets contain over 350,000 images for an interval of 16 years, from 2004
to 2020, with the corresponding global horizontal irradiance (GHI) of each
image as the ground truth. Compared to the state-of-the-art computationally
heavy algorithms proposed in the literature, our approach achieves competitive
results with much less computational complexity for both nowcasting and
forecasting up to 4 h ahead of time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_lahham_A/0/1/0/all/0/1&quot;&gt;Anas Al-lahham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theeb_O/0/1/0/all/0/1&quot;&gt;Obaidah Theeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elalem_K/0/1/0/all/0/1&quot;&gt;Khaled Elalem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshawi_T/0/1/0/all/0/1&quot;&gt;Tariq A. Alshawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshebeili_S/0/1/0/all/0/1&quot;&gt;Saleh A. Alshebeili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17359">
<title>SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation. (arXiv:2310.17359v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17359</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce an SE(3) diffusion model-based point cloud
registration framework for 6D object pose estimation in real-world scenarios.
Our approach formulates the 3D registration task as a denoising diffusion
process, which progressively refines the pose of the source point cloud to
obtain a precise alignment with the model point cloud. Training our framework
involves two operations: An SE(3) diffusion process and an SE(3) reverse
process. The SE(3) diffusion process gradually perturbs the optimal rigid
transformation of a pair of point clouds by continuously injecting noise
(perturbation transformation). By contrast, the SE(3) reverse process focuses
on learning a denoising network that refines the noisy transformation
step-by-step, bringing it closer to the optimal transformation for accurate
pose estimation. Unlike standard diffusion models used in linear Euclidean
spaces, our diffusion model operates on the SE(3) manifold. This requires
exploiting the linear Lie algebra $\mathfrak{se}(3)$ associated with SE(3) to
constrain the transformation transitions during the diffusion and reverse
processes. Additionally, to effectively train our denoising network, we derive
a registration-specific variational lower bound as the optimization objective
for model learning. Furthermore, we show that our denoising network can be
constructed with a surrogate registration model, making our approach applicable
to different deep registration networks. Extensive experiments demonstrate that
our diffusion registration framework presents outstanding pose estimation
performance on the real-world TUD-L, LINEMOD, and Occluded-LINEMOD datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haobo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1&quot;&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1&quot;&gt;Zheng Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17379">
<title>YOLO-BEV: Generating Bird&apos;s-Eye View in the Same Way as 2D Object Detection. (arXiv:2310.17379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17379</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle perception systems strive to achieve comprehensive and rapid visual
interpretation of their surroundings for improved safety and navigation. We
introduce YOLO-BEV, an efficient framework that harnesses a unique surrounding
cameras setup to generate a 2D bird&apos;s-eye view of the vehicular environment. By
strategically positioning eight cameras, each at a 45-degree interval, our
system captures and integrates imagery into a coherent 3x3 grid format, leaving
the center blank, providing an enriched spatial representation that facilitates
efficient processing. In our approach, we employ YOLO&apos;s detection mechanism,
favoring its inherent advantages of swift response and compact model structure.
Instead of leveraging the conventional YOLO detection head, we augment it with
a custom-designed detection head, translating the panoramically captured data
into a unified bird&apos;s-eye view map of ego car. Preliminary results validate the
feasibility of YOLO-BEV in real-time vehicular perception tasks. With its
streamlined architecture and potential for rapid deployment due to minimized
parameters, YOLO-BEV poses as a promising tool that may reshape future
perspectives in autonomous driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liguo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanliang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17395">
<title>Learning Temporal Sentence Grounding From Narrated EgoVideos. (arXiv:2310.17395v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17395</link>
<description rdf:parseType="Literal">&lt;p&gt;The onset of long-form egocentric datasets such as Ego4D and EPIC-Kitchens
presents a new challenge for the task of Temporal Sentence Grounding (TSG).
Compared to traditional benchmarks on which this task is evaluated, these
datasets offer finer-grained sentences to ground in notably longer videos. In
this paper, we develop an approach for learning to ground sentences in these
datasets using only narrations and their corresponding rough narration
timestamps. We propose to artificially merge clips to train for temporal
grounding in a contrastive manner using text-conditioning attention. This Clip
Merging (CliMer) approach is shown to be effective when compared with a high
performing TSG method -- e.g. mean R@1 improves from 3.9 to 5.7 on Ego4D and
from 10.7 to 13.0 on EPIC-Kitchens. Code and data splits available from:
https://github.com/keflanagan/CliMer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flanagan_K/0/1/0/all/0/1&quot;&gt;Kevin Flanagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1&quot;&gt;Dima Damen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1&quot;&gt;Michael Wray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17403">
<title>Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17403</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial patches undermine the reliability of optical flow predictions
when placed in arbitrary scene locations. Therefore, they pose a realistic
threat to real-world motion detection and its downstream applications.
Potential remedies are defense strategies that detect and remove adversarial
patches, but their influence on the underlying motion prediction has not been
investigated. In this paper, we thoroughly examine the currently available
detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art
optical flow methods, and illuminate their side effects on the quality and
robustness of the final flow predictions. In particular, we implement
defense-aware attacks to investigate whether current defenses are able to
withstand attacks that take the defense mechanism into account. Our experiments
yield two surprising results: Detect-and-remove defenses do not only lower the
optical flow quality on benign scenes, in doing so, they also harm the
robustness under patch attacks for all tested optical flow methods except
FlowNetC. As currently employed detect-and-remove defenses fail to deliver the
promised adversarial robustness for optical flow, they evoke a false sense of
security. The code is available at
https://github.com/cv-stuttgart/DetectionDefenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheurer_E/0/1/0/all/0/1&quot;&gt;Erik Scheurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1&quot;&gt;Jenny Schmalfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lis_A/0/1/0/all/0/1&quot;&gt;Alexander Lis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Bruhn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17418">
<title>Circuit as Set of Points. (arXiv:2310.17418v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17418</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of circuit designs continues to grow rapidly, artificial
intelligence technologies are being extensively used in Electronic Design
Automation (EDA) to assist with circuit design. Placement and routing are the
most time-consuming parts of the physical design process, and how to quickly
evaluate the placement has become a hot research topic. Prior works either
transformed circuit designs into images using hand-crafted methods and then
used Convolutional Neural Networks (CNN) to extract features, which are limited
by the quality of the hand-crafted methods and could not achieve end-to-end
training, or treated the circuit design as a graph structure and used Graph
Neural Networks (GNN) to extract features, which require time-consuming
preprocessing. In our work, we propose a novel perspective for circuit design
by treating circuit components as point clouds and using Transformer-based
point cloud perception methods to extract features from the circuit. This
approach enables direct feature extraction from raw data without any
preprocessing, allows for end-to-end training, and results in high performance.
Experimental results show that our method achieves state-of-the-art performance
in congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as
well as in design rule check (DRC) violation prediction tasks on the CircuitNet
dataset. Our method establishes a bridge between the relatively mature point
cloud perception methods and the fast-developing EDA algorithms, enabling us to
leverage more collective intelligence to solve this task. To facilitate the
research of open EDA design, source codes and pre-trained models are released
at https://github.com/hustvl/circuitformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Jialv Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiahao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chang Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17419">
<title>AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors. (arXiv:2310.17419v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17419</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models can create remarkably photorealistic fake images while
raising concerns about misinformation and copyright infringement, known as
deepfake threats. Deepfake detection technique is developed to distinguish
between real and fake images, where the existing methods typically learn
classifiers in the image domain or various feature domains. However, the
generalizability of deepfake detection against emerging and more advanced
generative models remains challenging. In this paper, being inspired by the
zero-shot advantages of Vision-Language Models (VLMs), we propose a novel
approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve
the deepfake detection accuracy over unseen data. We formulate deepfake
detection as a visual question answering problem, and tune soft prompts for
InstructBLIP to answer the real/fake information of a query image. We conduct
full-spectrum experiments on datasets from 3 held-in and 13 held-out generative
models, covering modern text-to-image generation, image editing and image
attacks. Results demonstrate that (1) the deepfake detection accuracy can be
significantly and consistently improved (from 58.8% to 91.31%, in average
accuracy over unseen data) using pretrained vision-language models with prompt
tuning; (2) our superior performance is at less cost of trainable parameters,
resulting in an effective and efficient solution for deepfake detection. Code
and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;You-Ming Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chen Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1&quot;&gt;Wei-Chen Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Ning Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17421">
<title>Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition. (arXiv:2310.17421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17421</link>
<description rdf:parseType="Literal">&lt;p&gt;Human action recognition from skeletal data is an important and active area
of research in which the state of the art has not yet achieved near-perfect
accuracy on many well-known datasets. In this paper, we introduce the
Distribution of Action Movements Descriptor, a novel action descriptor based on
the distribution of the directions of the motions of the joints between frames,
over the set of all possible motions in the dataset. The descriptor is computed
as a normalized histogram over a set of representative directions of the
joints, which are in turn obtained via clustering. While the descriptor is
global in the sense that it represents the overall distribution of movement
directions of an action, it is able to partially retain its temporal structure
by applying a windowing scheme.
&lt;/p&gt;
&lt;p&gt;The descriptor, together with a standard classifier, outperforms several
state-of-the-art techniques on many well-known datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eestrebou_C/0/1/0/all/0/1&quot;&gt;Cesar Eestrebou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17427">
<title>Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17427</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic sign language recognition is an important topic within the areas of
human-computer interaction and machine learning. On the one hand, it poses a
complex challenge that requires the intervention of various knowledge areas,
such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people.
&lt;/p&gt;
&lt;p&gt;This paper offers two main contributions: first, the creation of a database
of handshapes for the Argentinian Sign Language (LSA), which is a topic that
has barely been discussed so far. Secondly, a technique for image processing,
descriptor extraction and subsequent handshape classification using a
supervised adaptation of self-organizing maps that is called ProbSom. This
technique is compared to others in the state of the art, such as Support Vector
Machines (SVM), Random Forests, and Neural Networks.
&lt;/p&gt;
&lt;p&gt;The database that was built contains 800 images with 16 LSA handshapes, and
is a first step towards building a comprehensive database of Argentinian signs.
The ProbSom-based neural classifier, using the proposed descriptor, achieved an
accuracy rate above 90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Estrebou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17429">
<title>LSA64: An Argentinian Sign Language Dataset. (arXiv:2310.17429v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17429</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic sign language recognition is a research area that encompasses
human-computer interaction, computer vision and machine learning. Robust
automatic recognition of sign language could assist in the translation process
and the integration of hearing-impaired people, as well as the teaching of sign
language to the hearing population. Sign languages differ significantly in
different countries and even regions, and their syntax and semantics are
different as well from those of written languages. While the techniques for
automatic sign language recognition are mostly the same for different
languages, training a recognition system for a new language requires having an
entire dataset for that language. This paper presents a dataset of 64 signs
from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains
3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first
step towards building a comprehensive research-level dataset of Argentinian
signs, specifically tailored to sign language recognition or other machine
learning tasks. The subjects that performed the signs wore colored gloves to
ease the hand tracking and segmentation steps, allowing experiments on the
dataset to focus specifically on the recognition of signs. We also present a
pre-processed version of the dataset, from which we computed statistics of
movement, position and handshape of the signs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Estrebou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosete_A/0/1/0/all/0/1&quot;&gt;Alejandro Rosete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17436">
<title>Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation. (arXiv:2310.17436v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17436</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art deep neural networks have been shown to be extremely
powerful in a variety of perceptual tasks like semantic segmentation. However,
these networks are vulnerable to adversarial perturbations of the input which
are imperceptible for humans but lead to incorrect predictions. Treating image
segmentation as a sum of pixel-wise classifications, adversarial attacks
developed for classification models were shown to be applicable to segmentation
models as well. In this work, we present simple uncertainty-based weighting
schemes for the loss functions of such attacks that (i) put higher weights on
pixel classifications which can more easily perturbed and (ii) zero-out the
pixel-wise losses corresponding to those pixels that are already confidently
misclassified. The weighting schemes can be easily integrated into the loss
function of a range of well-known adversarial attackers with minimal additional
computational overhead, but lead to significant improved perturbation
performance, as we demonstrate in our empirical analysis on several datasets
and models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1&quot;&gt;Kira Maag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17437">
<title>Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language. (arXiv:2310.17437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17437</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic sign language recognition (SLR) is an important topic within the
areas of human-computer interaction and machine learning. On the one hand, it
poses a complex challenge that requires the intervention of various knowledge
areas, such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people, as well as the teaching of sign language for the hearing population.
&lt;/p&gt;
&lt;p&gt;SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or
similar models to recognize signs. Such techniques exploit the sequential
ordering of frames to reduce the number of hypothesis. This paper presents a
general probabilistic model for sign classification that combines
sub-classifiers based on different types of features such as position, movement
and handshape. The model employs a bag-of-words approach in all classification
steps, to explore the hypothesis that ordering is not essential for
recognition. The proposed model achieved an accuracy rate of 97% on an
Argentinian Sign Language dataset containing 64 classes of signs and 3200
samples, providing some evidence that indeed recognition without ordering is
possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_F/0/1/0/all/0/1&quot;&gt;Franco Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiroga_F/0/1/0/all/0/1&quot;&gt;Facundo Manuel Quiroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estrebou_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;sar Estrebou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanzarini_L/0/1/0/all/0/1&quot;&gt;Laura Lanzarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosete_A/0/1/0/all/0/1&quot;&gt;Alejandro Rosete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17451">
<title>Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.17451</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the great success of neural visual generative models in recent years,
integrating them with strong symbolic knowledge reasoning systems remains a
challenging task. The main challenges are two-fold: one is symbol assignment,
i.e. bonding latent factors of neural visual generators with meaningful symbols
from knowledge reasoning systems. Another is rule learning, i.e. learning new
rules, which govern the generative process of the data, to augment the
knowledge reasoning systems. To deal with these symbol grounding problems, we
propose a neural-symbolic learning approach, Abductive Visual Generation
(AbdGen), for integrating logic programming systems with neural visual
generative models based on the abductive learning framework. To achieve
reliable and efficient symbol assignment, the quantized abduction method is
introduced for generating abduction proposals by the nearest-neighbor lookups
within semantic codebooks. To achieve precise rule learning, the contrastive
meta-abduction method is proposed to eliminate wrong rules with positive cases
and avoid less-informative rules with negative cases simultaneously.
Experimental results on various benchmark datasets show that compared to the
baselines, AbdGen requires significantly fewer instance-level labeling
information for symbol assignment. Furthermore, our approach can effectively
learn underlying logical generative rules from data, which is out of the
capability of existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhexu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yao-Xiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wang-Zhou Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17455">
<title>OTMatch: Improving Semi-Supervised Learning with Optimal Transport. (arXiv:2310.17455v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17455</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning has made remarkable strides by effectively utilizing
a limited amount of labeled data while capitalizing on the abundant information
present in unlabeled data. However, current algorithms often prioritize
aligning image predictions with specific classes generated through
self-training techniques, thereby neglecting the inherent relationships that
exist within these classes. In this paper, we present a new approach called
OTMatch, which leverages semantic relationships among classes by employing an
optimal transport loss function. By utilizing optimal transport, our proposed
method consistently outperforms established state-of-the-art methods. Notably,
we observed a substantial improvement of a certain percentage in accuracy
compared to the current state-of-the-art method, FreeMatch. OTMatch achieves
3.18%, 3.46%, and 1.28% error rate reduction over FreeMatch on CIFAR-10 with 1
label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels
per class, respectively. This demonstrates the effectiveness and superiority of
our approach in harnessing semantic relationships to enhance learning
performance in a semi-supervised setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhiquan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17462">
<title>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17462</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object&apos;s
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method&apos;s potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1&quot;&gt;Daniel Kienzle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1&quot;&gt;Julian Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1&quot;&gt;Katja Ludwig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1&quot;&gt;Rainer Lienhart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17468">
<title>Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17468</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Peng Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17493">
<title>A Hybrid Graph Network for Complex Activity Detection in Video. (arXiv:2310.17493v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17493</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretation and understanding of video presents a challenging computer
vision task in numerous fields - e.g. autonomous driving and sports analytics.
Existing approaches to interpreting the actions taking place within a video
clip are based upon Temporal Action Localisation (TAL), which typically
identifies short-term actions. The emerging field of Complex Activity Detection
(CompAD) extends this analysis to long-term activities, with a deeper
understanding obtained by modelling the internal structure of a complex
activity taking place within the video. We address the CompAD problem using a
hybrid graph neural network which combines attention applied to a graph
encoding the local (short-term) dynamic scene with a temporal graph modelling
the overall long-duration activity. Our approach is as follows: i) Firstly, we
propose a novel feature extraction technique which, for each video snippet,
generates spatiotemporal `tubes&apos; for the active elements (`agents&apos;) in the
(local) scene by detecting individual objects, tracking them and then
extracting 3D features from all the agent tubes as well as the overall scene.
ii) Next, we construct a local scene graph where each node (representing either
an agent tube or the scene) is connected to all other nodes. Attention is then
applied to this graph to obtain an overall representation of the local dynamic
scene. iii) Finally, all local scene graph representations are interconnected
via a temporal graph, to estimate the complex activity class together with its
start and end time. The proposed framework outperforms all previous
state-of-the-art methods on all three datasets including ActivityNet-1.3,
Thumos-14, and ROAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1&quot;&gt;Izzeddin Teeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1&quot;&gt;Andrew Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17504">
<title>Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving. (arXiv:2310.17504v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17504</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised image networks can be used to address complex 2D tasks (e.g.,
semantic segmentation, object discovery) very efficiently and with little or no
downstream supervision. However, self-supervised 3D networks on lidar data do
not perform as well for now. A few methods therefore propose to distill
high-quality self-supervised 2D features into 3D networks. The most recent ones
doing so on autonomous driving data show promising results. Yet, a performance
gap persists between these distilled features and fully-supervised ones. In
this work, we revisit 2D-to-3D distillation. First, we propose, for semantic
segmentation, a simple approach that leads to a significant improvement
compared to prior 3D distillation methods. Second, we show that distillation in
high capacity 3D networks is key to reach high quality 3D features. This
actually allows us to significantly close the gap between unsupervised
distilled 3D features and fully-supervised ones. Last, we show that our
high-quality distilled representations can also be used for open-vocabulary
segmentation and background/foreground discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puy_G/0/1/0/all/0/1&quot;&gt;Gilles Puy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1&quot;&gt;Spyros Gidaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1&quot;&gt;Alexandre Boulch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1&quot;&gt;Oriane Sim&amp;#xe9;oni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sautier_C/0/1/0/all/0/1&quot;&gt;Corentin Sautier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1&quot;&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1&quot;&gt;Renaud Marlet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17519">
<title>FLARE: Fast Learning of Animatable and Relightable Mesh Avatars. (arXiv:2310.17519v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17519</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is to efficiently learn personalized animatable 3D head avatars from
videos that are geometrically accurate, realistic, relightable, and compatible
with current rendering systems. While 3D meshes enable efficient processing and
are highly portable, they lack realism in terms of shape and appearance. Neural
representations, on the other hand, are realistic but lack compatibility and
are slow to train and render. Our key insight is that it is possible to
efficiently learn high-fidelity 3D mesh representations via differentiable
rendering by exploiting highly-optimized methods from traditional computer
graphics and approximating some of the components with neural networks. To that
end, we introduce \moniker, a technique that enables the creation of animatable
and relightable mesh avatars from a single monocular video. First, we learn a
canonical geometry using a mesh representation, enabling efficient
differentiable rasterization and straightforward animation via learned
blendshapes and linear blend skinning weights. Second, we follow
physically-based rendering and factor observed colors into intrinsic albedo,
roughness, and a neural representation of the illumination, allowing the
learned avatars to be relit in novel scenes. Since our input videos are
captured on a single device with a narrow field of view, modeling the
surrounding environment light is non-trivial. Based on the split-sum
approximation for modeling specular reflections, we address this by
approximating the pre-filtered environment map with a multi-layer perceptron
(MLP) modulated by the surface roughness, eliminating the need to explicitly
model the light. We demonstrate that our mesh-based avatar formulation,
combined with learned deformation, material, and lighting MLPs, produces
avatars with high-quality geometry and appearance, while also being efficient
to train and render compared to existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1&quot;&gt;Shrisha Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yufeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1&quot;&gt;Michael J. Black&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Abrevaya_V/0/1/0/all/0/1&quot;&gt;Victoria Fernandez-Abrevaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17527">
<title>Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction. (arXiv:2310.17527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17527</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel
method for efficiently reconstructing dynamic 3D scenes from multi-view or
monocular videos. Based on the observation that dynamic scenes often contain
substantial static areas that result in redundancy in storage and computations,
MSTH represents a dynamic scene as a weighted combination of a 3D hash encoding
and a 4D hash encoding. The weights for the two components are represented by a
learnable mask which is guided by an uncertainty-based objective to reflect the
spatial and temporal importance of each 3D position. With this design, our
method can reduce the hash collision rate by avoiding redundant queries and
modifications on static areas, making it feasible to represent a large number
of space-time voxels by hash tables with small size.Besides, without the
requirements to fit the large numbers of temporally redundant features
independently, our method is easier to optimize and converge rapidly with only
twenty minutes of training for a 300-frame dynamic scene.As a result, MSTH
obtains consistently better results than previous methods with only 20 minutes
of training time and 130 MB of memory storage. Code is available at
https://github.com/masked-spacetime-hashing/msth
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guokang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yafei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17530">
<title>Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17530</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained machine learning models are known to perpetuate and even amplify
existing biases in data, which can result in unfair outcomes that ultimately
impact user experience. Therefore, it is crucial to understand the mechanisms
behind those prejudicial biases to ensure that model performance does not
result in discriminatory behaviour toward certain groups or populations. In
this work, we define gender bias as our case study. We quantify bias
amplification in pretraining and after fine-tuning on three families of
vision-and-language models. We investigate the connection, if any, between the
two learning stages, and evaluate how bias amplification reflects on model
performance. Overall, we find that bias amplification in pretraining and after
fine-tuning are independent. We then examine the effect of continued
pretraining on gender-neutral data, finding that this reduces group
disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without
significantly compromising task performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabello_L/0/1/0/all/0/1&quot;&gt;Laura Cabello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1&quot;&gt;Emanuele Bugliarello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1&quot;&gt;Stephanie Brandl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1&quot;&gt;Desmond Elliott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17534">
<title>SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.17534</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous works study black-box attacks on image classifiers. However, these
works make different assumptions on the adversary&apos;s knowledge and current
literature lacks a cohesive organization centered around the threat model. To
systematize knowledge in this area, we propose a taxonomy over the threat space
spanning the axes of feedback granularity, the access of interactive queries,
and the quality and quantity of the auxiliary data available to the attacker.
Our new taxonomy provides three key insights. 1) Despite extensive literature,
numerous under-explored threat spaces exist, which cannot be trivially solved
by adapting techniques from well-explored settings. We demonstrate this by
establishing a new state-of-the-art in the less-studied setting of access to
top-k confidence scores by adapting techniques from well-explored settings of
accessing the complete confidence vector, but show how it still falls short of
the more restrictive setting that only obtains the prediction label,
highlighting the need for more research. 2) Identification the threat model of
different attacks uncovers stronger baselines that challenge prior
state-of-the-art claims. We demonstrate this by enhancing an initially weaker
baseline (under interactive query access) via surrogate models, effectively
overturning claims in the respective paper. 3) Our taxonomy reveals
interactions between attacker knowledge that connect well to related areas,
such as model inversion and extraction attacks. We discuss how advances in
other areas can enable potentially stronger black-box attacks. Finally, we
emphasize the need for a more realistic assessment of attack success by
factoring in local attack runtime. This approach reveals the potential for
certain attacks to achieve notably higher success rates and the need to
evaluate attacks in diverse and harder settings, highlighting the need for
better selection criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suya_F/0/1/0/all/0/1&quot;&gt;Fnu Suya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suri_A/0/1/0/all/0/1&quot;&gt;Anshuman Suri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jingtao Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_D/0/1/0/all/0/1&quot;&gt;David Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17559">
<title>Instability of computer vision models is a necessary result of the task itself. (arXiv:2310.17559v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.17559</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples resulting from instability of current computer vision
models are an extremely important topic due to their potential to compromise
any application. In this paper we demonstrate that instability is inevitable
due to a) symmetries (translational invariance) of the data, b) the categorical
nature of the classification task, and c) the fundamental discrepancy of
classifying images as objects themselves. The issue is further exacerbated by
non-exhaustive labelling of the training data. Therefore we conclude that
instability is a necessary result of how the problem of computer vision is
currently formulated. While the problem cannot be eliminated, through the
analysis of the causes, we have arrived at ways how it can be partially
alleviated. These include i) increasing the resolution of images, ii) providing
contextual information for the image, iii) exhaustive labelling of training
data, and iv) preventing attackers from frequent access to the computer vision
system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turnbull_O/0/1/0/all/0/1&quot;&gt;Oliver Turnbull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cevora_G/0/1/0/all/0/1&quot;&gt;George Cevora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1908.10907">
<title>DFPENet-geology: A Deep Learning Framework for High Precision Recognition and Segmentation of Co-seismic Landslides. (arXiv:1908.10907v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1908.10907</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic recognition and segmentation methods now become the essential
requirement in identifying co-seismic landslides, which are fundamental for
disaster assessment and mitigation in large-scale earthquakes. This approach
used to be carried out through pixel-based or object-oriented methods. However,
due to the massive amount of remote sensing data, variations in different
earthquake scenarios, and the efficiency requirement for post-earthquake
rescue, these methods are difficult to develop into an accurate, rapid,
comprehensive, and general (cross-scene) solution for co-seismic landslide
recognition. This paper develops a robust model, Dense Feature Pyramid with
Encoder-decoder Network (DFPENet), to understand and fuse the multi-scale
features of objects in remote sensing images. The proposed method achieves a
competitive segmentation accuracy on the public ISPRS 2D Semantic. Furthermore,
a comprehensive and widely-used scheme is proposed for co-seismic landslide
recognition, which integrates image features extracted from the DFPENet model,
geologic features, temporal resolution, landslide spatial analysis, and
transfer learning, while only RGB images are used. To corroborate its
feasibility and applicability, the proposed scheme is applied to two
earthquake-triggered landslides in Jiuzhaigou (China) and Hokkaido (Japan),
using available pre- and post-earthquake remote sensing images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingsong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1&quot;&gt;Chaojun Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tianhai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xuanmei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Duoxiang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.05435">
<title>Optimization-Inspired Learning with Architecture Augmentations and Control Mechanisms for Low-Level Vision. (arXiv:2012.05435v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.05435</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been a growing interest in combining learnable
modules with numerical optimization to solve low-level vision tasks. However,
most existing approaches focus on designing specialized schemes to generate
image/feature propagation. There is a lack of unified consideration to
construct propagative modules, provide theoretical analysis tools, and design
effective learning mechanisms. To mitigate the above issues, this paper
proposes a unified optimization-inspired learning framework to aggregate
Generative, Discriminative, and Corrective (GDC for short) principles with
strong generalization for diverse optimization models. Specifically, by
introducing a general energy minimization model and formulating its descent
direction from different viewpoints (i.e., in a generative manner, based on the
discriminative metric and with optimality-based correction), we construct three
propagative modules to effectively solve the optimization models with flexible
combinations. We design two control mechanisms that provide the non-trivial
theoretical guarantees for both fully- and partially-defined optimization
formulations. Under the support of theoretical guarantees, we can introduce
diverse architecture augmentation strategies such as normalization and search
to ensure stable propagation with convergence and seamlessly integrate the
suitable modules into the propagation respectively. Extensive experiments
across varied low-level vision tasks validate the efficacy and adaptability of
GDC. The codes are available at
https://github.com/LiuZhu-CV/GDC-OptimizationLearning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1&quot;&gt;Pan Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongxuan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.02206">
<title>Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.02206</link>
<description rdf:parseType="Literal">&lt;p&gt;Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close: when
tasked with learning to classify objects by training on non-repeating video
frames in temporal order (online stream learning), models that learn well from
shuffled datasets catastrophically forget old knowledge upon learning new
stimuli. We propose a new continual learning algorithm, Compositional Replay
Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature
maps reconstructed by recombining generic parts. CRUMB concatenates trainable
and re-usable &quot;memory block&quot; vectors to compositionally reconstruct feature map
tensors in convolutional neural networks, like crumbs forming a loaf of bread.
CRUMB stores the indices of memory blocks used to reconstruct new stimuli,
enabling replay of specific memories during later tasks. This reconstruction
mechanism also primes the neural network to minimize catastrophic forgetting by
forcing it to attend to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With about 4% as much memory and 30% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the prior state-of-the-art. Our
code is available on GitHub at https://github.com/MorganBDT/crumb.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talbot_M/0/1/0/all/0/1&quot;&gt;Morgan B. Talbot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zawar_R/0/1/0/all/0/1&quot;&gt;Rushikesh Zawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badkundri_R/0/1/0/all/0/1&quot;&gt;Rohil Badkundri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.07368">
<title>Quality-Aware Network for Face Parsing. (arXiv:2106.07368v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.07368</link>
<description rdf:parseType="Literal">&lt;p&gt;This is a very short technical report, which introduces the solution of the
Team BUPT-CASIA for Short-video Face Parsing Track of The 3rd Person in Context
(PIC) Workshop and Challenge at CVPR 2021.
&lt;/p&gt;
&lt;p&gt;Face parsing has recently attracted increasing interest due to its numerous
application potentials. Generally speaking, it has a lot in common with human
parsing, such as task setting, data characteristics, number of categories and
so on. Therefore, this work applies state-of-the-art human parsing method to
face parsing task to explore the similarities and differences between them. Our
submission achieves 86.84% score and wins the 2nd place in the challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1&quot;&gt;Xueshi Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Wenhe Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.14251">
<title>Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2109.14251</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate inference of fine-grained traffic flow from coarse-grained one is an
emerging yet crucial problem, which can help greatly reduce the number of the
required traffic monitoring sensors for cost savings. In this work, we notice
that traffic flow has a high correlation with road network, which was either
completely ignored or simply treated as an external factor in previous works.
To facilitate this problem, we propose a novel Road-Aware Traffic Flow
Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks
to fully learn the road-aware spatial distribution of fine-grained traffic
flow. Specifically, a multi-directional 1D convolutional layer is first
introduced to extract the semantic feature of the road network. Subsequently,
we incorporate the road network feature and coarse-grained flow feature to
regularize the short-range spatial distribution modeling of road-relative
traffic flow. Furthermore, we take the road network feature as a query to
capture the long-range spatial distribution of traffic flow with a transformer
architecture. Benefiting from the road-aware inference mechanism, our method
can generate high-quality fine-grained traffic flow maps. Extensive experiments
on three real-world datasets show that the proposed RATFM outperforms
state-of-the-art models under various scenarios. Our code and datasets are
released at {\url{https://github.com/luimoli/RATFM}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengmeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junfan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.01646">
<title>Investigating the usefulness of Quantum Blur. (arXiv:2112.01646v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.01646</link>
<description rdf:parseType="Literal">&lt;p&gt;Though some years remain before quantum computation can fully outperform
conventional computation, it already provides resources that can be used for
exploratory purposes in various fields. This includes certain tasks for
procedural generation in computer games, music and art. The so-called `Quantum
Blur&apos; method represents the first step on this journey, providing a simple
proof-of-principle example of how quantum software can be useful in these areas
today. Here we analyse the `Quantum Blur&apos; method and compare it to conventional
blur effects. This investigation was guided by discussions with the most
prominent user of the method, to determine which features were found most
useful. In particular we determine how these features depend on the quantum
phenomena of superposition and entanglement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wootton_J/0/1/0/all/0/1&quot;&gt;James R. Wootton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfaffhauser_M/0/1/0/all/0/1&quot;&gt;Marcel Pfaffhauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.09249">
<title>Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.09249</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is an emerging distributed learning paradigm under
privacy constraint. Data heterogeneity is one of the main challenges in FL,
which results in slow convergence and degraded performance. Most existing
approaches only tackle the heterogeneity challenge by restricting the local
model update in client, ignoring the performance drop caused by direct global
model aggregation. Instead, we propose a data-free knowledge distillation
method to fine-tune the global model in the server (FedFTG), which relieves the
issue of direct model aggregation. Concretely, FedFTG explores the input space
of local models through a generator, and uses it to transfer the knowledge from
local models to the global model. Besides, we propose a hard sample mining
scheme to achieve effective knowledge distillation throughout the training. In
addition, we develop customized label sampling and class-level ensemble to
derive maximum utilization of knowledge, which implicitly mitigates the
distribution discrepancy across clients. Extensive experiments show that our
FedFTG significantly outperforms the state-of-the-art (SOTA) FL algorithms and
can serve as a strong plugin for enhancing FedAvg, FedProx, FedDyn, and
SCAFFOLD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1&quot;&gt;Ling-Yu Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11699">
<title>Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11699</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant progress has been witnessed in learning-based Multi-view Stereo
(MVS) under supervised and unsupervised settings. To combine their respective
merits in accuracy and completeness, meantime reducing the demand for expensive
labeled data, this paper explores the problem of learning-based MVS in a
semi-supervised setting that only a tiny part of the MVS data is attached with
dense depth ground truth. However, due to huge variation of scenarios and
flexible settings in views, it may break the basic assumption in classic
semi-supervised learning, that unlabeled data and labeled data share the same
label space and data distribution, named as semi-supervised distribution-gap
ambiguity in the MVS problem. To handle these issues, we propose a novel
semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the
simple case that the basic assumption works in MVS data, consistency
regularization encourages the model predictions to be consistent between
original sample and randomly augmented sample. For further troublesome case
that the basic assumption is conflicted in MVS data, we propose a novel style
consistency loss to alleviate the negative effect caused by the distribution
gap. The visual style of unlabeled sample is transferred to labeled sample to
shrink the gap, and the model prediction of generated sample is further
supervised with the label in original labeled sample. The experimental results
in semi-supervised settings of multiple MVS datasets show the superior
performance of the proposed method. With the same settings in backbone network,
our proposed SDA-MVS outperforms its fully-supervised and unsupervised
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Haihong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wenxiong Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01254">
<title>A Robust Morphological Approach for Semantic Segmentation of Very High Resolution Images. (arXiv:2208.01254v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01254</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art methods for semantic segmentation of images involve
computationally intensive neural network architectures. Most of these methods
are not adaptable to high-resolution image segmentation due to memory and other
computational issues. Typical approaches in literature involve design of neural
network architectures that can fuse global information from low-resolution
images and local information from the high-resolution counterparts. However,
architectures designed for processing high resolution images are unnecessarily
complex and involve a lot of hyper parameters that can be difficult to tune.
Also, most of these architectures require ground truth annotations of the high
resolution images to train, which can be hard to obtain. In this article, we
develop a robust pipeline based on mathematical morphological (MM) operators
that can seamlessly extend any existing semantic segmentation algorithm to high
resolution images. Our method does not require the ground truth annotations of
the high resolution images. It is based on efficiently utilizing information
from the low-resolution counterparts, and gradient information on the
high-resolution images. We obtain high quality seeds from the inferred labels
on low-resolution images using traditional morphological operators and
propagate seed labels using a random walker to refine the semantic labels at
the boundaries. We show that the semantic segmentation results obtained by our
method beat the existing state-of-the-art algorithms on high-resolution images.
We empirically prove the robustness of our approach to the hyper parameters
used in our pipeline. Further, we characterize some necessary conditions under
which our pipeline is applicable and provide an in-depth analysis of the
proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saravanan_S/0/1/0/all/0/1&quot;&gt;Siddharth Saravanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1&quot;&gt;Aditya Challa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1&quot;&gt;Sravan Danda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13959">
<title>Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding. (arXiv:2209.13959v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13959</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal transformer exhibits high capacity and flexibility to align image
and text for visual grounding. However, the existing encoder-only grounding
framework (e.g., TransVG) suffers from heavy computation due to the
self-attention operation with quadratic time complexity. To address this issue,
we present a new multimodal transformer architecture, coined as Dynamic
Mutilmodal DETR (Dynamic MDETR), by decoupling the whole grounding process into
encoding and decoding phases. The key observation is that there exists high
spatial redundancy in images. Thus, we devise a new dynamic multimodal
transformer decoder by exploiting this sparsity prior to speed up the visual
grounding process. Specifically, our dynamic decoder is composed of a 2D
adaptive sampling module and a text guided decoding module. The sampling module
aims to select these informative patches by predicting the offsets with respect
to a reference point, while the decoding module works for extracting the
grounded object information by performing cross attention between image
features and text features. These two modules are stacked alternatively to
gradually bridge the modality gap and iteratively refine the reference point of
grounded object, eventually realizing the objective of visual grounding.
Extensive experiments on five benchmarks demonstrate that our proposed Dynamic
MDETR achieves competitive trade-offs between computation and accuracy.
Notably, using only 9% feature points in the decoder, we can reduce ~44% GFLOPs
of the multimodal transformer, but still get higher accuracy than the
encoder-only counterpart. In addition, to verify its generalization ability and
scale up our Dynamic MDETR, we build the first one-stage CLIP empowered visual
grounding framework, and achieve the state-of-the-art performance on these
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Fengyuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruopeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weilin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00157">
<title>Ponder: Point Cloud Pre-training via Neural Rendering. (arXiv:2301.00157v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00157</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel approach to self-supervised learning of point cloud
representations by differentiable neural rendering. Motivated by the fact that
informative point cloud features should be able to encode rich geometry and
appearance cues and render realistic images, we train a point-cloud encoder
within a devised point-based neural renderer by comparing the rendered images
with real images on massive RGB-D data. The learned point-cloud encoder can be
easily integrated into various downstream tasks, including not only high-level
tasks like 3D detection and segmentation, but low-level tasks like 3D
reconstruction and image synthesis. Extensive experiments on various tasks
demonstrate the superiority of our approach compared to existing pre-training
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Di Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Sida Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Honghui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06874">
<title>Training Methods of Multi-label Prediction Classifiers for Hyperspectral Remote Sensing Images. (arXiv:2301.06874v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06874</link>
<description rdf:parseType="Literal">&lt;p&gt;With their combined spectral depth and geometric resolution, hyperspectral
remote sensing images embed a wealth of complex, non-linear information that
challenges traditional computer vision techniques. Yet, deep learning methods
known for their representation learning capabilities prove more suitable for
handling such complexities. Unlike applications that focus on single-label,
pixel-level classification methods for hyperspectral remote sensing images, we
propose a multi-label, patch-level classification method based on a
two-component deep-learning network. We use patches of reduced spatial
dimension and a complete spectral depth extracted from the remote sensing
images. Additionally, we investigate three training schemes for our network:
Iterative, Joint, and Cascade. Experiments suggest that the Joint scheme is the
best-performing scheme; however, its application requires an expensive search
for the best weight combination of the loss constituents. The Iterative scheme
enables the sharing of features between the two parts of the network at the
early stages of training. It performs better on complex data with multi-labels.
Further experiments showed that methods designed with different architectures
performed well when trained on patches extracted and labeled according to our
sampling method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haidar_S/0/1/0/all/0/1&quot;&gt;Salma Haidar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Oramas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08951">
<title>Time-Conditioned Generative Modeling of Object-Centric Representations for Video Decomposition and Prediction. (arXiv:2301.08951v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08951</link>
<description rdf:parseType="Literal">&lt;p&gt;When perceiving the world from multiple viewpoints, humans have the ability
to reason about the complete objects in a compositional manner even when an
object is completely occluded from certain viewpoints. Meanwhile, humans are
able to imagine novel views after observing multiple viewpoints. Recent
remarkable advances in multi-view object-centric learning still leaves some
unresolved problems: 1) The shapes of partially or completely occluded objects
can not be well reconstructed. 2) The novel viewpoint prediction depends on
expensive viewpoint annotations rather than implicit rules in view
representations. In this paper, we introduce a time-conditioned generative
model for videos. To reconstruct the complete shape of an object accurately, we
enhance the disentanglement between the latent representations of objects and
views, where the latent representations of time-conditioned views are jointly
inferred with a Transformer and then are input to a sequential extension of
Slot Attention to learn object-centric representations. In addition, Gaussian
processes are employed as priors of view latent variables for video generation
and novel-view prediction without viewpoint annotations. Experiments on
multiple datasets demonstrate that the proposed model can make object-centric
video decomposition, reconstruct the complete shapes of occluded objects, and
make novel-view predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chengmin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04824">
<title>Lithium Metal Battery Quality Control via Transformer-CNN Segmentation. (arXiv:2302.04824v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04824</link>
<description rdf:parseType="Literal">&lt;p&gt;Lithium metal battery (LMB) has the potential to be the next-generation
battery system because of its high theoretical energy density. However, defects
known as dendrites are formed by heterogeneous lithium (Li) plating, which
hinders the development and utilization of LMBs. Non-destructive techniques to
observe the dendrite morphology often use X-ray computed tomography (XCT) to
provide cross-sectional views. To retrieve three-dimensional structures inside
a battery, image segmentation becomes essential to quantitatively analyze XCT
images. This work proposes a new semantic segmentation approach using a
transformer-based neural network called TransforCNN that is capable of
segmenting out dendrites from XCT data. In addition, we compare the performance
of the proposed TransforCNN with three other algorithms, such as U-Net, Y-Net,
and E-Net, consisting of an Ensemble Network model for XCT analysis. Our
results show the advantages of using TransforCNN when evaluating
over-segmentation metrics, such as mean Intersection over Union (mIoU) and mean
Dice Similarity Coefficient (mDSC) as well as through several qualitatively
comparative visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quenum_J/0/1/0/all/0/1&quot;&gt;Jerome Quenum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenyuk_I/0/1/0/all/0/1&quot;&gt;Iryna Zenyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushizima_D/0/1/0/all/0/1&quot;&gt;Daniela Ushizima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06961">
<title>DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06961</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate fovea localization is essential for analyzing retinal diseases to
prevent irreversible vision loss. While current deep learning-based methods
outperform traditional ones, they still face challenges such as the lack of
local anatomical landmarks around the fovea, the inability to robustly handle
diseased retinal images, and the variations in image conditions. In this paper,
we propose a novel transformer-based architecture called DualStreamFoveaNet
(DSFN) for multi-cue fusion. This architecture explicitly incorporates
long-range connections and global features using retina and vessel
distributions for robust fovea localization. We introduce a spatial attention
mechanism in the dual-stream encoder to extract and fuse self-learned
anatomical information, focusing more on features distributed along blood
vessels and significantly reducing computational costs by decreasing token
numbers. Our extensive experiments show that the proposed architecture achieves
state-of-the-art performance on two public datasets and one large-scale private
dataset. Furthermore, we demonstrate that the DSFN is more robust on both
normal and diseased retina images and has better generalization capacity in
cross-dataset experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sifan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaopeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jionglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00198">
<title>Convolutional Visual Prompt for Robust Visual Perception. (arXiv:2303.00198v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00198</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision models are often vulnerable to out-of-distribution (OOD) samples
without adapting. While visual prompts offer a lightweight method of
input-space adaptation for large-scale vision models, they rely on a
high-dimensional additive vector and labeled data. This leads to overfitting
when adapting models in a self-supervised test-time setting without labels. We
introduce convolutional visual prompts (CVP) for label-free test-time
adaptation for robust visual perception. The structured nature of CVP demands
fewer trainable parameters, less than 1\% compared to standard visual prompts,
combating overfitting. Extensive experiments and analysis on a wide variety of
OOD visual perception tasks show that our approach is effective, improving
robustness by up to 5.87% over several large-scale models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yun-Yun Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chengzhi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junfeng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00905">
<title>Open-World Object Manipulation using Pre-trained Vision-Language Models. (arXiv:2303.00905v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00905</link>
<description rdf:parseType="Literal">&lt;p&gt;For robots to follow instructions from people, they must be able to connect
the rich semantic information in human vocabulary, e.g. &quot;can you get me the
pink stuffed whale?&quot; to their sensory observations and actions. This brings up
a notably difficult challenge for robots: while robot learning approaches allow
robots to learn many different behaviors from first-hand experience, it is
impractical for robots to have first-hand experiences that span all of this
semantic information. We would like a robot&apos;s policy to be able to perceive and
pick up the pink stuffed whale, even if it has never seen any data interacting
with a stuffed whale before. Fortunately, static data on the internet has vast
semantic information, and this information is captured in pre-trained
vision-language models. In this paper, we study whether we can interface robot
policies with these pre-trained models, with the aim of allowing robots to
complete instructions involving object categories that the robot has never seen
first-hand. We develop a simple approach, which we call Manipulation of
Open-World Objects (MOO), which leverages a pre-trained vision-language model
to extract object-identifying information from the language command and image,
and conditions the robot policy on the current image, the instruction, and the
extracted object information. In a variety of experiments on a real mobile
manipulator, we find that MOO generalizes zero-shot to a wide range of novel
object categories and environments. In addition, we show how MOO generalizes to
other, non-language-based input modalities to specify the object of interest
such as finger pointing, and how it can be further extended to enable
open-world navigation and manipulation. The project&apos;s website and evaluation
videos can be found at https://robot-moo.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1&quot;&gt;Austin Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wohlhart_P/0/1/0/all/0/1&quot;&gt;Paul Wohlhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirmani_S/0/1/0/all/0/1&quot;&gt;Sean Kirmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1&quot;&gt;Brianna Zitkovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04909">
<title>Robotic Fabric Flattening with Wrinkle Direction Detection. (arXiv:2303.04909v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04909</link>
<description rdf:parseType="Literal">&lt;p&gt;Deformable Object Manipulation (DOM) is an important field of research as it
contributes to practical tasks such as automatic cloth handling, cable routing,
surgical operation, etc. Perception is considered one of the major challenges
in DOM due to the complex dynamics and high degree of freedom of deformable
objects. In this paper, we develop a novel image-processing algorithm based on
Gabor filters to extract useful features from cloth, and based on this, devise
a strategy for cloth flattening tasks. We also evaluate the overall framework
experimentally and compare it with three human operators. The results show that
our algorithm can determine the direction of wrinkles on the cloth accurately
in simulation as well as in real robot experiments. Furthermore, our
dewrinkling strategy compares favorably to baseline methods. The experiment
video is available on
https://sites.google.com/view/robotic-fabric-flattening/home
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yulei Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jihong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santina_C/0/1/0/all/0/1&quot;&gt;Cosimo Della Santina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gienger_M/0/1/0/all/0/1&quot;&gt;Michael Gienger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1&quot;&gt;Jens Kober&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07963">
<title>RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning. (arXiv:2303.07963v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07963</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new method for 3D point cloud registration based on
deep learning. The architecture is composed of three distinct blocs: (i) an
encoder composed of a convolutional graph-based descriptor that encodes the
immediate neighbourhood of each point and an attention mechanism that encodes
the variations of the surface normals. Such descriptors are refined by
highlighting attention between the points of the same set and then between the
points of the two sets. (ii) a matching process that estimates a matrix of
correspondences using the Sinkhorn algorithm. (iii) Finally, the rigid
transformation between the two point clouds is calculated by RANSAC using the
Kc best scores from the correspondence matrix. We conduct experiments on the
ModelNet40 dataset, and our proposed architecture shows very promising results,
outperforming state-of-the-art methods in most of the simulated configurations,
including partial overlap and data augmentation with Gaussian noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slimani_K/0/1/0/all/0/1&quot;&gt;Karim Slimani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamadazte_B/0/1/0/all/0/1&quot;&gt;Brahim Tamadazte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achard_C/0/1/0/all/0/1&quot;&gt;Catherine Achard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09412">
<title>NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters. (arXiv:2303.09412v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09412</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis using neural radiance fields (NeRF) is the
state-of-the-art technique for generating high-quality images from novel
viewpoints. Existing methods require a priori knowledge about extrinsic and
intrinsic camera parameters. This limits their applicability to synthetic
scenes, or real-world scenarios with the necessity of a preprocessing step.
Current research on the joint optimization of camera parameters and NeRF
focuses on refining noisy extrinsic camera parameters and often relies on the
preprocessing of intrinsic camera parameters. Further approaches are limited to
cover only one single camera intrinsic. To address these limitations, we
propose a novel end-to-end trainable approach called NeRFtrinsic Four. We
utilize Gaussian Fourier features to estimate extrinsic camera parameters and
dynamically predict varying intrinsic camera parameters through the supervision
of the projection error. Our approach outperforms existing joint optimization
methods on LLFF and BLEFF. In addition to these existing datasets, we introduce
a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic
Four is a step forward in joint optimization NeRF-based view synthesis and
enables more realistic and flexible rendering in real-world scenarios with
varying camera parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schieber_H/0/1/0/all/0/1&quot;&gt;Hannah Schieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuser_F/0/1/0/all/0/1&quot;&gt;Fabian Deuser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1&quot;&gt;Bernhard Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1&quot;&gt;Norbert Oswald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Daniel Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13497">
<title>TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13497</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those applied to 3D GANs may fail to extrapolate the result onto the
novel view, whereas optimization-based 3D GAN inversion methods are
time-consuming and can require at least several minutes per image. Fast
encoder-based techniques, such as those developed for StyleGAN, may also be
less appealing due to the lack of identity preservation. Our work introduces a
fast technique that bridges the gap between the two approaches by directly
utilizing the tri-plane representation presented for the EG3D generative model.
In particular, we build upon a feed-forward convolutional encoder for the
latent code and extend it with a fully-convolutional predictor of tri-plane
numerical offsets. The renderings are similar in quality to the ones produced
by optimization-based techniques and outperform the ones by encoder-based
methods. As we empirically prove, this is a consequence of directly operating
in the tri-plane space, not in the GAN parameter space, while making use of an
encoder-based trainable approach. Finally, we demonstrate significantly more
correct embedding of a face image in 3D than for all the baselines, further
strengthened by a probably symmetric prior enabled during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattarai_A/0/1/0/all/0/1&quot;&gt;Ananta R. Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sevastopolsky_A/0/1/0/all/0/1&quot;&gt;Artem Sevastopolsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01091">
<title>Changes to Captions: An Attentive Network for Remote Sensing Change Captioning. (arXiv:2304.01091v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01091</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, advanced research has focused on the direct learning and
analysis of remote sensing images using natural language processing (NLP)
techniques. The ability to accurately describe changes occurring in
multi-temporal remote sensing images is becoming increasingly important for
geospatial understanding and land planning. Unlike natural image change
captioning tasks, remote sensing change captioning aims to capture the most
significant changes, irrespective of various influential factors such as
illumination, seasonal effects, and complex land covers. In this study, we
highlight the significance of accurately describing changes in remote sensing
images and present a comparison of the change captioning task for natural and
synthetic images and remote sensing images. To address the challenge of
generating accurate captions, we propose an attentive changes-to-captions
network, called Chg2Cap for short, for bi-temporal remote sensing images. The
network comprises three main components: 1) a Siamese CNN-based feature
extractor to collect high-level representations for each image pair; 2) an
attentive decoder that includes a hierarchical self-attention block to locate
change-related features and a residual block to generate the image embedding;
and 3) a transformer-based caption generator to decode the relationship between
the image embedding and the word embedding into a description. The proposed
Chg2Cap network is evaluated on two representative remote sensing datasets, and
a comprehensive experimental analysis is provided. The code and pre-trained
models will be available online at https://github.com/ShizhenChang/Chg2Cap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shizhen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1&quot;&gt;Pedram Ghamisi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03659">
<title>Probing Conceptual Understanding of Large Visual-Language Models. (arXiv:2304.03659v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03659</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years large visual-language (V+L) models have achieved great
success in various downstream tasks. However, it is not well studied whether
these models have a conceptual grasp of the visual content. In this work we
focus on conceptual understanding of these large V+L models.To facilitate this
study, we propose novel benchmarking datasets for probing three different
aspects of content understanding, 1) relations, 2) composition and 3) context.
Our probes are grounded in cognitive science and help determine if a V+L model
can, for example, determine if ``snow garnished with a man&apos;&apos; is implausible, or
if it can identify beach furniture by knowing it is located on a beach. We
experimented with five different state-of-the-art V+L models and observe that
these models mostly fail to demonstrate a conceptual understanding. This study
reveals several interesting insights such as cross-attention helps learning
conceptual understanding, and that CNNs are better with texture and patterns,
while Transformers are better at color and shape. We further utilize some of
these insights and propose a baseline for improving performance by a simple
finetuning technique that rewards the three conceptual understanding measures
with promising initial results. We believe that the proposed benchmarks will
help the community assess and improve the conceptual understanding capabilities
of large V+L models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1&quot;&gt;Madeline Chantry Schiappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1&quot;&gt;Michael Cogswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1&quot;&gt;Ajay Divakaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh Singh Rawat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06700">
<title>Control3Diff: Learning Controllable 3D Diffusion Models from Single-view Images. (arXiv:2304.06700v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06700</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently become the de-facto approach for generative
modeling in the 2D domain. However, extending diffusion models to 3D is
challenging due to the difficulties in acquiring 3D ground truth data for
training. On the other hand, 3D GANs that integrate implicit 3D representations
into GANs have shown remarkable 3D-aware generation when trained only on
single-view image datasets. However, 3D GANs do not provide straightforward
ways to precisely control image synthesis. To address these challenges, We
present Control3Diff, a 3D diffusion model that combines the strengths of
diffusion models and 3D GANs for versatile, controllable 3D-aware image
synthesis for single-view datasets. Control3Diff explicitly models the
underlying latent distribution (optionally conditioned on external inputs),
thus enabling direct control during the diffusion process. Moreover, our
approach is general and applicable to any type of controlling input, allowing
us to train it with the same diffusion objective without any auxiliary
supervision. We validate the efficacy of Control3Diff on standard image
generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various
conditioning inputs such as images, sketches, and text prompts. Please see the
project website (\url{https://jiataogu.me/control3diff}) for video comparisons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Qingzhe Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shuangfei Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1&quot;&gt;Josh Susskind&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08965">
<title>PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via Cross-modal Distillation and Super-Voxel Clustering. (arXiv:2304.08965v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08965</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of point clouds usually requires exhausting efforts of
human annotations, hence it attracts wide attention to the challenging topic of
learning from unlabeled or weaker forms of annotations. In this paper, we take
the first attempt for fully unsupervised semantic segmentation of point clouds,
which aims to delineate semantically meaningful objects without any form of
annotations. Previous works of unsupervised pipeline on 2D images fails in this
task of point clouds, due to: 1) Clustering Ambiguity caused by limited
magnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity
caused by the irregular sparsity of point cloud. Therefore, we propose a novel
framework, PointDC, which is comprised of two steps that handle the
aforementioned problems respectively: Cross-Modal Distillation (CMD) and
Super-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual
features are back-projected to the 3D space and aggregated to a unified point
feature to distill the training of the point representation. In the second
stage of SVC, the point features are aggregated to super-voxels and then fed to
the iterative clustering process for excavating semantic classes. PointDC
yields a significant improvement over the prior state-of-the-art unsupervised
methods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic
segmentation benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zisheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1&quot;&gt;Haihong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wenxiong Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12406">
<title>AutoFocusFormer: Image Segmentation off the Grid. (arXiv:2304.12406v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12406</link>
<description rdf:parseType="Literal">&lt;p&gt;Real world images often have highly imbalanced content density. Some areas
are very uniform, e.g., large patches of blue sky, while other areas are
scattered with many small objects. Yet, the commonly used successive grid
downsampling strategy in convolutional deep networks treats all areas equally.
Hence, small objects are represented in very few spatial locations, leading to
worse results in tasks such as segmentation. Intuitively, retaining more pixels
representing small objects during downsampling helps to preserve important
information. To achieve this, we propose AutoFocusFormer (AFF), a
local-attention transformer image recognition backbone, which performs adaptive
downsampling by learning to retain the most important pixels for the task.
Since adaptive downsampling generates a set of pixels irregularly distributed
on the image plane, we abandon the classic grid structure. Instead, we develop
a novel point-based local attention block, facilitated by a balanced clustering
module and a learnable neighborhood merging module, which yields
representations for our point-based versions of state-of-the-art segmentation
heads. Experiments show that our AutoFocusFormer (AFF) improves significantly
over baseline models of similar sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziwen_C/0/1/0/all/0/1&quot;&gt;Chen Ziwen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patnaik_K/0/1/0/all/0/1&quot;&gt;Kaushik Patnaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shuangfei Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_A/0/1/0/all/0/1&quot;&gt;Alvin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhile Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1&quot;&gt;Alex Schwing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1&quot;&gt;Alex Colburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuxin_L/0/1/0/all/0/1&quot;&gt;Li Fuxin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12470">
<title>Recurrent Transformer Encoders for Vision-based Estimation of Fatigue and Engagement in Cognitive Training Sessions. (arXiv:2304.12470v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12470</link>
<description rdf:parseType="Literal">&lt;p&gt;Computerized cognitive training (CCT) is a scalable, well-tolerated
intervention that has promise for slowing cognitive decline. Outcomes from CCT
are limited by a lack of effective engagement, which is decreased by factors
such as mental fatigue, particularly in older adults at risk for dementia.
There is a need for scalable, automated measures that can monitor mental
fatigue during CCT. Here, we develop and validate a novel Recurrent Video
Transformer (RVT) method for monitoring real-time mental fatigue in older
adults with mild cognitive impairment from video-recorded facial gestures
during CCT. The RVT model achieved the highest balanced accuracy(78%) and
precision (0.82) compared to the prior state-of-the-art models for binary and
multi-class classification of mental fatigue and was additionally validated via
significant association (p=0.023) with CCT reaction time. By leveraging dynamic
temporal information, the RVT model demonstrates the potential to accurately
measure real-time mental fatigue, laying the foundation for future personalized
CCT that increase effective engagement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yunlong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Feng Vankee Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1&quot;&gt;Ehsan Adeli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12760">
<title>Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies. (arXiv:2304.12760v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12760</link>
<description rdf:parseType="Literal">&lt;p&gt;Vanilla spiking neurons in Spiking Neural Networks (SNNs) use
charge-fire-reset neuronal dynamics, which can only be simulated serially and
can hardly learn long-time dependencies. We find that when removing reset, the
neuronal dynamics can be reformulated in a non-iterative form and parallelized.
By rewriting neuronal dynamics without reset to a general formulation, we
propose the Parallel Spiking Neuron (PSN), which generates hidden states that
are independent of their predecessors, resulting in parallelizable neuronal
dynamics and extremely high simulation speed. The weights of inputs in the PSN
are fully connected, which maximizes the utilization of temporal information.
To avoid the use of future inputs for step-by-step inference, the weights of
the PSN can be masked, resulting in the masked PSN. By sharing weights across
time-steps based on the masked PSN, the sliding PSN is proposed to handle
sequences of varying lengths. We evaluate the PSN family on simulation speed
and temporal/static data classification, and the results show the overwhelming
advantage of the PSN family in efficiency and accuracy. To the best of our
knowledge, this is the first study about parallelizing spiking neurons and can
be a cornerstone for the spiking deep learning research. Our codes are
available at \url{https://github.com/fangwei123456/Parallel-Spiking-Neuron}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1&quot;&gt;Wei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhaokun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Ding Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02324">
<title>Cross-Stream Contrastive Learning for Self-Supervised Skeleton-Based Action Recognition. (arXiv:2305.02324v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02324</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised skeleton-based action recognition enjoys a rapid growth along
with the development of contrastive learning. The existing methods rely on
imposing invariance to augmentations of 3D skeleton within a single data
stream, which merely leverages the easy positive pairs and limits the ability
to explore the complicated movement patterns. In this paper, we advocate that
the defect of single-stream contrast and the lack of necessary feature
transformation are responsible for easy positives, and therefore propose a
Cross-Stream Contrastive Learning framework for skeleton-based action
Representation learning (CSCLR). Specifically, the proposed CSCLR not only
utilizes intra-stream contrast pairs, but introduces inter-stream contrast
pairs as hard samples to formulate a better representation learning. Besides,
to further exploit the potential of positive pairs and increase the robustness
of self-supervised representation learning, we propose a Positive Feature
Transformation (PFT) strategy which adopts feature-level manipulation to
increase the variance of positive pairs. To validate the effectiveness of our
method, we conduct extensive experiments on three benchmark datasets NTU-RGB+D
60, NTU-RGB+D 120 and PKU-MMD. Experimental results show that our proposed
CSCLR exceeds the state-of-the-art methods on a diverse range of evaluation
protocols.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Ding Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wensheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04247">
<title>Estimation of control area in badminton doubles with pose information from top and back view drone videos. (arXiv:2305.04247v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of visual tracking to the performance analysis of sports
players in dynamic competitions is vital for effective coaching. In doubles
matches, coordinated positioning is crucial for maintaining control of the
court and minimizing opponents&apos; scoring opportunities. The analysis of such
teamwork plays a vital role in understanding the dynamics of the game. However,
previous studies have primarily focused on analyzing and assessing singles
players without considering occlusion in broadcast videos. These studies have
relied on discrete representations, which involve the analysis and
representation of specific actions (e.g., strokes) or events that occur during
the game while overlooking the meaningful spatial distribution. In this work,
we present the first annotated drone dataset from top and back views in
badminton doubles and propose a framework to estimate the control area
probability map, which can be used to evaluate teamwork performance. We present
an efficient framework of deep neural networks that enables the calculation of
full probability surfaces. This framework utilizes the embedding of a Gaussian
mixture map of players&apos; positions and employs graph convolution on their poses.
In the experiment, we verify our approach by comparing various baselines and
discovering the correlations between the score and control area. Additionally,
we propose a practical application for assessing optimal positioning to provide
instructions during a game. Our approach offers both visual and quantitative
evaluations of players&apos; movements, thereby providing valuable insights into
doubles teamwork. The dataset and related project code is available at
https://github.com/Ning-D/Drone_BD_ControlArea
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wenhui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bei_Y/0/1/0/all/0/1&quot;&gt;Yingjiu Bei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09758">
<title>A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot. (arXiv:2305.09758v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09758</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimedia content, such as advertisements and story videos, exhibit a rich
blend of creativity and multiple modalities. They incorporate elements like
text, visuals, audio, and storytelling techniques, employing devices like
emotions, symbolism, and slogans to convey meaning. There is a dearth of large
annotated training datasets in the multimedia domain hindering the development
of supervised learning models with satisfactory performance for real-world
applications. On the other hand, the rise of large language models (LLMs) has
witnessed remarkable zero-shot performance in various natural language
processing (NLP) tasks, such as emotion classification, question-answering, and
topic classification. To leverage such advanced techniques to bridge this
performance gap in multimedia understanding, we propose verbalizing long videos
to generate their descriptions in natural language, followed by performing
video-understanding tasks on the generated story as opposed to the original
video. Through extensive experiments on fifteen video-understanding tasks, we
demonstrate that our method, despite being zero-shot, achieves significantly
better results than supervised baselines for video understanding. Furthermore,
to alleviate a lack of story understanding benchmarks, we publicly release the
first dataset on a crucial task in computational social science on persuasion
strategy identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1&quot;&gt;Aanisha Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1&quot;&gt;Yaman K Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajiv Ratn Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10355">
<title>Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10355</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the superior language abilities of large language models (LLM),
large vision-language models (LVLM) have been recently explored by integrating
powerful LLMs for improving the performance on complex multimodal tasks.
Despite the promising progress on LVLMs, we find that LVLMs suffer from the
hallucination problem, i.e. they tend to generate objects that are inconsistent
with the target images in the descriptions. To investigate it, this work
presents the first systematic study on object hallucination of LVLMs. We
conduct the evaluation experiments on several representative LVLMs, and show
that they mostly suffer from severe object hallucination issue. We further
discuss that the visual instructions may influence the hallucination, and find
that: objects that frequently occur in the visual instructions or co-occur with
the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we
find that existing evaluation methods might be affected by the input
instructions and generation styles of LVLMs. Thus, we further design an
improved evaluation method for object hallucination by proposing a
polling-based query method called POPE. Experiment results demonstrate that our
POPE can evaluate the object hallucination in a more stable and flexible way.
Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yifan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13507">
<title>Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13507</link>
<description rdf:parseType="Literal">&lt;p&gt;Misinformation is often conveyed in multiple modalities, e.g. a miscaptioned
image. Multimodal misinformation is perceived as more credible by humans, and
spreads faster than its text-only counterparts. While an increasing body of
research investigates automated fact-checking (AFC), previous surveys mostly
focus on text. In this survey, we conceptualise a framework for AFC including
subtasks unique to multimodal misinformation. Furthermore, we discuss related
terms used in different communities and map them to our framework. We focus on
four modalities prevalent in real-world fact-checking: text, image, audio, and
video. We survey benchmarks and models, and discuss limitations and promising
directions for future research
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1&quot;&gt;Mubashara Akhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1&quot;&gt;Michael Schlichtkrull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhijiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1&quot;&gt;Oana Cocarascu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1&quot;&gt;Elena Simperl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1&quot;&gt;Andreas Vlachos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14267">
<title>SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models. (arXiv:2305.14267v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14267</link>
<description rdf:parseType="Literal">&lt;p&gt;A potent class of generative models known as Diffusion Probabilistic Models
(DPMs) has become prominent. A forward diffusion process adds gradually noise
to data, while a model learns to gradually denoise. Sampling from pre-trained
DPMs is obtained by solving differential equations (DE) defined by the learnt
model, a process which has shown to be prohibitively slow. Numerous efforts on
speeding-up this process have consisted on crafting powerful ODE solvers.
Despite being quick, such solvers do not usually reach the optimal quality
achieved by available slow SDE solvers. Our goal is to propose SDE solvers that
reach optimal quality without requiring several hundreds or thousands of NFEs
to achieve that goal. We propose Stochastic Explicit Exponential
Derivative-free Solvers (SEEDS), improving and generalizing Exponential
Integrator approaches to the stochastic case on several frameworks. After
carefully analyzing the formulation of exact solutions of diffusion SDEs, we
craft SEEDS to analytically compute the linear part of such solutions. Inspired
by the Exponential Time-Differencing method, SEEDS use a novel treatment of the
stochastic components of solutions, enabling the analytical computation of
their variance, and contains high-order terms allowing to reach optimal quality
sampling $\sim3$-$5\times$ faster than previous SDE methods. We validate our
approach on several image generation benchmarks, showing that SEEDS outperform
or are competitive with previous SDE solvers. Contrary to the latter, SEEDS are
derivative and training free, and we fully prove strong convergence guarantees
for them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1&quot;&gt;Martin Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_N/0/1/0/all/0/1&quot;&gt;Nelson Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thuy Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gherbi_E/0/1/0/all/0/1&quot;&gt;Elies Gherbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1&quot;&gt;Hatem Hajri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masmoudi_N/0/1/0/all/0/1&quot;&gt;Nader Masmoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15134">
<title>Networks are Slacking Off: Understanding Generalization Problem in Image Deraining. (arXiv:2305.15134v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15134</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep deraining networks consistently encounter substantial generalization
issues when deployed in real-world applications, although they are successful
in laboratory benchmarks. A prevailing perspective in deep learning encourages
using highly complex data for training, with the expectation that richer image
background content will facilitate overcoming the generalization problem.
However, through comprehensive and systematic experimentation, we discover that
this strategy does not enhance the generalization capability of these networks.
On the contrary, it exacerbates the tendency of networks to overfit specific
degradations. Our experiments reveal that better generalization in a deraining
network can be achieved by simplifying the complexity of the training
background images. This is because that the networks are ``slacking off&apos;&apos;
during training, that is, learning the least complex elements in the image
background and degradation to minimize training loss. When the background
images are less complex than the rain streaks, the network will prioritize the
background reconstruction, thereby suppressing overfitting the rain patterns
and leading to improved generalization performance. Our research offers a
valuable perspective and methodology for better understanding the
generalization problem in low-level vision tasks and displays promising
potential for practical application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianzheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xiangtao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16150">
<title>Unifying GANs and Score-Based Diffusion as Generative Particle Models. (arXiv:2305.16150v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16150</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle-based deep generative models, such as gradient flows and score-based
diffusion models, have recently gained traction thanks to their striking
performance. Their principle of displacing particle distributions using
differential equations is conventionally seen as opposed to the previously
widespread generative adversarial networks (GANs), which involve training a
pushforward generator network. In this paper we challenge this interpretation,
and propose a novel framework that unifies particle and adversarial generative
models by framing generator training as a generalization of particle models.
This suggests that a generator is an optional addition to any such generative
model. Consequently, integrating a generator into a score-based diffusion model
and training a GAN without a generator naturally emerge from our framework. We
empirically test the viability of these original models as proofs of concepts
of potential applications of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franceschi_J/0/1/0/all/0/1&quot;&gt;Jean-Yves Franceschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gartrell_M/0/1/0/all/0/1&quot;&gt;Mike Gartrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_L/0/1/0/all/0/1&quot;&gt;Ludovic Dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1&quot;&gt;Thibaut Issenhuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezenac_E/0/1/0/all/0/1&quot;&gt;Emmanuel de B&amp;#xe9;zenac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Micka&amp;#xeb;l Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1&quot;&gt;Alain Rakotomamonjy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19693">
<title>Spontaneous Symmetry Breaking in Generative Diffusion Models. (arXiv:2305.19693v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19693</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative diffusion models have recently emerged as a leading approach for
generating high-dimensional data. In this paper, we show that the dynamics of
these models exhibit a spontaneous symmetry breaking that divides the
generative dynamics into two distinct phases: 1) A linear steady-state dynamics
around a central fixed-point and 2) an attractor dynamics directed towards the
data manifold. These two &quot;phases&quot; are separated by the change in stability of
the central fixed-point, with the resulting window of instability being
responsible for the diversity of the generated samples. Using both theoretical
and empirical evidence, we show that an accurate simulation of the early
dynamics does not significantly contribute to the final generation, since early
fluctuations are reverted to the central fixed point. To leverage this insight,
we propose a Gaussian late initialization scheme, which significantly improves
model performance, achieving up to 3x FID improvements on fast samplers, while
also increasing sample diversity (e.g., racial composition of generated CelebA
images). Our work offers a new way to understand the generative dynamics of
diffusion models that has the potential to bring about higher performance and
less biased fast-samplers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raya_G/0/1/0/all/0/1&quot;&gt;Gabriel Raya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambrogioni_L/0/1/0/all/0/1&quot;&gt;Luca Ambrogioni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00595">
<title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective. (arXiv:2306.00595v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00595</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on the weakly-supervised audio-visual video parsing task (AVVP),
which aims to identify and locate all the events in audio/visual modalities.
Previous works only concentrate on video-level overall label denoising across
modalities, but overlook the segment-level label noise, where adjacent video
segments (i.e., 1-second video clips) may contain different events. However,
recognizing events in the segment is challenging because its label could be any
combination of events that occur in the video. To address this issue, we
consider tackling AVVP from the language perspective, since language could
freely describe how various events appear in each segment beyond fixed labels.
Specifically, we design language prompts to describe all cases of event
appearance for each video. Then, the similarity between language prompts and
segments is calculated, where the event of the most similar prompt is regarded
as the segment-level label. In addition, to deal with the mislabeled segments,
we propose to perform dynamic re-weighting on the unreliable segments to adjust
their labels. Experiments show that our simple yet effective approach
outperforms state-of-the-art methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yingying Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yutian Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00612">
<title>AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset. (arXiv:2306.00612v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00612</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a long-term vision for Autonomous Driving (AD) community that the
perception models can learn from a large-scale point cloud dataset, to obtain
unified representations that can achieve promising results on different tasks
or benchmarks. Previous works mainly focus on the self-supervised pre-training
pipeline, meaning that they perform the pre-training and fine-tuning on the
same benchmark, which is difficult to attain the performance scalability and
cross-dataset application for the pre-training checkpoint. In this paper, for
the first time, we are committed to building a large-scale pre-training
point-cloud dataset with diverse data distribution, and meanwhile learning
generalizable representations from such a diverse pre-training dataset. We
formulate the point-cloud pre-training task as a semi-supervised problem, which
leverages the few-shot labeled and massive unlabeled point-cloud data to
generate the unified backbone representations that can be directly applied to
many baseline models and benchmarks, decoupling the AD-related pre-training
process and downstream fine-tuning task. During the period of backbone
pre-training, by enhancing the scene- and instance-level distribution diversity
and exploiting the backbone&apos;s ability to learn from unknown instances, we
achieve significant performance gains on a series of downstream perception
benchmarks including Waymo, nuScenes, and KITTI, under different baseline
models like PV-RCNN++, SECOND, CenterPoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiakang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiangchao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00650">
<title>Universal Test-time Adaptation through Weight Ensembling, Diversity Weighting, and Prior Correction. (arXiv:2306.00650v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00650</link>
<description rdf:parseType="Literal">&lt;p&gt;Since distribution shifts are likely to occur during test-time and can
drastically decrease the model&apos;s performance, online test-time adaptation (TTA)
continues to update the model after deployment, leveraging the current test
data. Clearly, a method proposed for online TTA has to perform well for all
kinds of environmental conditions. By introducing the variable factors domain
non-stationarity and temporal correlation, we first unfold all practically
relevant settings and define the entity as universal TTA. We want to highlight
that this is the first work that covers such a broad spectrum, which is
indispensable for the use in practice. To tackle the problem of universal TTA,
we identify and highlight several challenges a self-training based method has
to deal with: 1) model bias and the occurrence of trivial solutions when
performing entropy minimization on varying sequence lengths with and without
multiple domain shifts, 2) loss of generalization which exacerbates the
adaptation to multiple domain shifts and the occurrence of catastrophic
forgetting, and 3) performance degradation due to shifts in class prior. To
prevent the model from becoming biased, we leverage a dataset and
model-agnostic certainty and diversity weighting. In order to maintain
generalization and prevent catastrophic forgetting, we propose to continually
weight-average the source and adapted model. To compensate for disparities in
the class prior during test-time, we propose an adaptive prior correction
scheme that reweights the model&apos;s predictions. We evaluate our approach, named
ROID, on a wide range of settings, datasets, and models, setting new standards
in the field of universal TTA. Code is available at:
https://github.com/mariodoebler/test-time-adaptation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marsden_R/0/1/0/all/0/1&quot;&gt;Robert A. Marsden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1&quot;&gt;Mario D&amp;#xf6;bler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00984">
<title>StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners. (arXiv:2306.00984v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00984</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the potential of learning visual representations using
synthetic images generated by text-to-image models. This is a natural question
in the light of the excellent performance of such models in generating
high-quality images. We consider specifically the Stable Diffusion, one of the
leading open source text-to-image models. We show that (1) when the generative
model is configured with proper classifier-free guidance scale, training
self-supervised methods on synthetic images can match or beat the real image
counterpart; (2) by treating the multiple images generated from the same text
prompt as positives for each other, we develop a multi-positive contrastive
learning method, which we call StableRep. With solely synthetic images, the
representations learned by StableRep surpass the performance of representations
learned by SimCLR and CLIP using the same set of text prompts and corresponding
real images, on large scale datasets. When we further add language supervision,
StableRep trained with 20M synthetic images achieves better accuracy than CLIP
trained with 50M real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonglong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lijie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Huiwen Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1&quot;&gt;Dilip Krishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01874">
<title>SACSoN: Scalable Autonomous Control for Social Navigation. (arXiv:2306.01874v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01874</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning provides a powerful tool for building socially compliant
robotic systems that go beyond simple predictive models of human behavior. By
observing and understanding human interactions from past experiences, learning
can enable effective social navigation behaviors directly from data. In this
paper, our goal is to develop methods for training policies for socially
unobtrusive navigation, such that robots can navigate among humans in ways that
don&apos;t disturb human behavior. We introduce a definition for such behavior based
on the counterfactual perturbation of the human: if the robot had not intruded
into the space, would the human have acted in the same way? By minimizing this
counterfactual perturbation, we can induce robots to behave in ways that do not
alter the natural behavior of humans in the shared space. Instantiating this
principle requires training policies to minimize their effect on human
behavior, and this in turn requires data that allows us to model the behavior
of humans in the presence of robots. Therefore, our approach is based on two
key contributions. First, we collect a large dataset where an indoor mobile
robot interacts with human bystanders. Second, we utilize this dataset to train
policies that minimize counterfactual perturbation. We provide supplementary
videos and make publicly available the largest-of-its-kind visual navigation
dataset on our project page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirose_N/0/1/0/all/0/1&quot;&gt;Noriaki Hirose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1&quot;&gt;Dhruv Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Ajay Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04249">
<title>DEMIST: A deep-learning-based task-specific denoising approach for myocardial perfusion SPECT. (arXiv:2306.04249v3 [physics.med-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04249</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an important need for methods to process myocardial perfusion
imaging (MPI) SPECT images acquired at lower radiation dose and/or acquisition
time such that the processed images improve observer performance on the
clinical task of detecting perfusion defects. To address this need, we build
upon concepts from model-observer theory and our understanding of the human
visual system to propose a Detection task-specific deep-learning-based approach
for denoising MPI SPECT images (DEMIST). The approach, while performing
denoising, is designed to preserve features that influence observer performance
on detection tasks. We objectively evaluated DEMIST on the task of detecting
perfusion defects using a retrospective study with anonymized clinical data in
patients who underwent MPI studies across two scanners (N = 338). The
evaluation was performed at low-dose levels of 6.25%, 12.5% and 25% and using
an anthropomorphic channelized Hotelling observer. Performance was quantified
using area under the receiver operating characteristics curve (AUC). Images
denoised with DEMIST yielded significantly higher AUC compared to corresponding
low-dose images and images denoised with a commonly used task-agnostic DL-based
denoising method. Similar results were observed with stratified analysis based
on patient sex and defect type. Additionally, DEMIST improved visual fidelity
of the low-dose images as quantified using root mean squared error and
structural similarity index metric. A mathematical analysis revealed that
DEMIST preserved features that assist in detection tasks while improving the
noise properties, resulting in improved observer performance. The results
provide strong evidence for further clinical evaluation of DEMIST to denoise
low-count images in MPI SPECT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md Ashequr Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zitong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Laforest_R/0/1/0/all/0/1&quot;&gt;Richard Laforest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Abbey_C/0/1/0/all/0/1&quot;&gt;Craig K. Abbey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Siegel_B/0/1/0/all/0/1&quot;&gt;Barry A. Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jha_A/0/1/0/all/0/1&quot;&gt;Abhinav K. Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08247">
<title>Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation. (arXiv:2306.08247v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08247</link>
<description rdf:parseType="Literal">&lt;p&gt;Originating from the diffusion phenomenon in physics that describes particle
movement, the diffusion generative models inherit the characteristics of
stochastic random walk in the data space along the denoising trajectory.
However, the intrinsic mutual interference among image regions contradicts the
need for practical downstream application scenarios where the preservation of
low-level pixel information from given conditioning is desired (e.g.,
customization tasks like personalized generation and inpainting based on a
user-provided single image). In this work, we investigate the diffusion
(physics) in diffusion (machine learning) properties and propose our Cyclic
One-Way Diffusion (COW) method to control the direction of diffusion phenomenon
given a pre-trained frozen diffusion model for versatile customization
application scenarios, where the low-level pixel information from the
conditioning needs to be preserved. Notably, unlike most current methods that
incorporate additional conditions by fine-tuning the base text-to-image
diffusion model or learning auxiliary networks, our method provides a novel
perspective to understand the task needs and is applicable to a wider range of
customization scenarios in a learning-free manner. Extensive experiment results
show that our proposed COW can achieve more flexible customization based on
strict visual conditions in different application settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhihao Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Ye Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08645">
<title>Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis. (arXiv:2306.08645v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08645</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models (DMs) have recently gained attention with state-of-the-art
performance in text-to-image synthesis. Abiding by the tradition in deep
learning, DMs are trained and evaluated on the images with fixed sizes.
However, users are demanding for various images with specific sizes and various
aspect ratio. This paper focuses on adapting text-to-image diffusion models to
handle such variety while maintaining visual fidelity. First we observe that,
during the synthesis, lower resolution images suffer from incomplete object
portrayal, while higher resolution images exhibit repetitively disordered
presentation. Next, we establish a statistical relationship indicating that
attention entropy changes with token quantity, suggesting that models aggregate
spatial information in proportion to image resolution. The subsequent
interpretation on our observations is that objects are incompletely depicted
due to limited spatial information for low resolutions, while repetitively
disorganized presentation arises from redundant spatial information for high
resolutions. From this perspective, we propose a scaling factor to alleviate
the change of attention entropy and mitigate the defective pattern observed.
Extensive experimental results validate the efficacy of the proposed scaling
factor, enabling models to achieve better visual effects, image quality, and
text alignment. Notably, these improvements are achieved without additional
training or fine-tuning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xuli Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14685">
<title>DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14685</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though trained mainly on images, we discover that pretrained diffusion
models show impressive power in guiding sketch synthesis. In this paper, we
present DiffSketcher, an innovative algorithm that creates \textit{vectorized}
free-hand sketches using natural language input. DiffSketcher is developed
based on a pre-trained text-to-image diffusion model. It performs the task by
directly optimizing a set of B\&apos;ezier curves with an extended version of the
score distillation sampling (SDS) loss, which allows us to use a raster-level
diffusion model as a prior for optimizing a parametric vectorized sketch
generator. Furthermore, we explore attention maps embedded in the diffusion
model for effective stroke initialization to speed up the generation process.
The generated sketches demonstrate multiple levels of abstraction while
maintaining recognizability, underlying structure, and essential visual details
of the subject drawn. Our experiments show that DiffSketcher achieves greater
quality than prior work. The code and demo of DiffSketcher can be found at
https://ximinng.github.io/DiffSketcher-project/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Ximing Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haitao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07063">
<title>Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07063</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel methodology aimed at optimizing the application of frozen
large language models (LLMs) for resource-intensive vision-language (VL)
pre-training. The current paradigm uses visual features as prompts to guide
language models, with a focus on determining the most relevant visual features
for corresponding text. Our approach diverges by concentrating on the language
component, specifically identifying the optimal prompts to align with visual
features. We introduce the Prompt-Transformer (P-Former), a model that predicts
these ideal prompts, which is trained exclusively on linguistic data, bypassing
the need for image-text pairings. This strategy subtly bifurcates the
end-to-end VL training process into an additional, separate stage. Our
experiments reveal that our framework significantly enhances the performance of
a robust image-to-text baseline (BLIP-2), and effectively narrows the
performance gap between models trained with either 4M or 129M image-text pairs.
Importantly, our framework is modality-agnostic and flexible in terms of
architectural design, as validated by its successful application in a video
learning task using varied base modules. The code will be made available at
https://github.com/yiren-jian/BLIText.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1&quot;&gt;Yiren Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chongyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1&quot;&gt;Soroush Vosoughi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07813">
<title>TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation. (arXiv:2307.07813v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07813</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent edge vision tasks encounter the critical challenge of ensuring
power and latency efficiency due to the typically heavy computational load they
impose on edge platforms.This work leverages one of the first &quot;AI in sensor&quot;
vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power
end-to-end edge vision applications. We evaluate the IMX500 and compare it to
other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by
exploring gaze estimation as a case study. We propose TinyTracker, a highly
efficient, fully quantized model for 2D gaze estimation designed to maximize
the performance of the edge vision systems considered in this study.
TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]
without significant loss in gaze estimation accuracy (maximum of 0.16 cm when
fully quantized). TinyTracker&apos;s deployment on the Sony IMX500 vision sensor
results in end-to-end latency of around 19ms. The camera takes around 17.9ms to
read, process and transmit the pixels to the accelerator. The inference time of
the network is 0.86ms with an additional 0.24 ms for retrieving the results
from the sensor. The overall energy consumption of the end-to-end system is 4.9
mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is
1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ
VS 34.2mJ)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1&quot;&gt;Thomas Ruegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10350">
<title>Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10350</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive web datasets play a key role in the success of large vision-language
models like CLIP and Flamingo. However, the raw web data is noisy, and existing
filtering methods to reduce noise often come at the expense of data diversity.
Our work focuses on caption quality as one major source of noise, and studies
how generated captions can increase the utility of web-scraped datapoints with
nondescript text. Through exploring different mixing strategies for raw and
generated captions, we outperform the best filtering method proposed by the
DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a
candidate pool of 128M image-text pairs. Our best approach is also 2x better at
Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an
effective source of text supervision. In experimenting with different image
captioning models, we also demonstrate that the performance of a model on
standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable
indicator of the utility of the captions it generates for multimodal training.
Finally, our experiments with using generated captions at DataComp&apos;s large
scale (1.28B image-text pairs) offer insights into the limitations of synthetic
text, as well as the importance of image curation with increasing training data
quantity. The synthetic captions used in our experiments are now available on
HuggingFace.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10922">
<title>Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10922</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent contrastive language image pre-training has led to learning highly
transferable and robust image representations. However, adapting these models
to video domains with minimal supervision remains an open problem. We explore a
simple step in that direction, using language tied self-supervised learning to
adapt an image CLIP model to the video domain. A backbone modified for temporal
modeling is trained under self-distillation settings with train objectives
operating in an action concept space. Feature vectors of various action
concepts extracted from a language encoder using relevant textual prompts
construct this space. We introduce two train objectives, concept distillation
and concept alignment, that retain generality of original representations while
enforcing relations between actions and their attributes. Our approach improves
zero-shot and linear probing performance on three action recognition
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranasinghe_K/0/1/0/all/0/1&quot;&gt;Kanchana Ranasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04826">
<title>WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields. (arXiv:2308.04826v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04826</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has shown impressive performance in novel view
synthesis via implicit scene representation. However, it usually suffers from
poor scalability as requiring densely sampled images for each new scene.
Several studies have attempted to mitigate this problem by integrating
Multi-View Stereo (MVS) technique into NeRF while they still entail a
cumbersome fine-tuning process for new scenes. Notably, the rendering quality
will drop severely without this fine-tuning process and the errors mainly
appear around the high-frequency features. In the light of this observation, we
design WaveNeRF, which integrates wavelet frequency decomposition into MVS and
NeRF to achieve generalizable yet high-quality synthesis without any per-scene
optimization. To preserve high-frequency information when generating 3D feature
volumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating
the discrete wavelet transform into the classical cascade MVS, which
disentangles high-frequency information explicitly. With that, disentangled
frequency features can be injected into classic NeRF via a novel hybrid neural
renderer to yield faithful high-frequency details, and an intuitive
frequency-guided sampling strategy can be designed to suppress artifacts around
high-frequency regions. Extensive experiments over three widely studied
benchmarks show that WaveNeRF achieves superior generalizable radiance field
modeling when only given three images as input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Muyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1&quot;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Ling Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08231">
<title>DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field. (arXiv:2308.08231v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08231</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing hand-held objects from a single RGB image is an important and
challenging problem. Existing works utilizing Signed Distance Fields (SDF)
reveal limitations in comprehensively capturing the complex hand-object
interactions, since SDF is only reliable within the proximity of the target,
and hence, infeasible to simultaneously encode local hand and object cues. To
address this issue, we propose DDF-HO, a novel approach leveraging Directed
Distance Field (DDF) as the shape representation. Unlike SDF, DDF maps a ray in
3D space, consisting of an origin and a direction, to corresponding DDF values,
including a binary visibility signal determining whether the ray intersects the
objects and a distance value measuring the distance from origin to target in
the given direction. We randomly sample multiple rays and collect local to
global geometric features for them by introducing a novel 2D ray-based feature
aggregation scheme and a 3D intersection-aware hand pose embedding, combining
2D-3D features to model hand-object interactions. Extensive experiments on
synthetic and real-world datasets demonstrate that DDF-HO consistently
outperforms all baseline methods by a large margin, especially under Chamfer
Distance, with about 80% leap forward. Codes are available at
https://github.com/ZhangCYG/DDFHO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyangguang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1&quot;&gt;Yan Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruida Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangyao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1&quot;&gt;Fabian Manhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00359">
<title>Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00359</link>
<description rdf:parseType="Literal">&lt;p&gt;Shannon, in his seminal paper introducing information theory, divided the
communication into three levels: technical, semantic, and effectivenss. While
the technical level is concerned with accurate reconstruction of transmitted
symbols, the semantic and effectiveness levels deal with the inferred meaning
and its effect on the receiver. Thanks to telecommunications, the first level
problem has produced great advances like the internet. Large Language Models
(LLMs) make some progress towards the second goal, but the third level still
remains largely untouched. The third problem deals with predicting and
optimizing communication for desired receiver behavior. LLMs, while showing
wide generalization capabilities across a wide range of tasks, are unable to
solve for this. One reason for the underperformance could be a lack of
``behavior tokens&apos;&apos; in LLMs&apos; training corpora. Behavior tokens define receiver
behavior over a communication, such as shares, likes, clicks, purchases,
retweets, etc. While preprocessing data for LLM training, behavior tokens are
often removed from the corpora as noise. Therefore, in this paper, we make some
initial progress towards reintroducing behavior tokens in LLM training. The
trained models, other than showing similar performance to LLMs on content
understanding tasks, show generalization capabilities on behavior simulation,
content simulation, behavior understanding, and behavior domain adaptation.
Using a wide range of tasks on two corpora, we show results on all these
capabilities. We call these models Large Content and Behavior Models (LCBMs).
Further, to spur more research on LCBMs, we release our new Content Behavior
Corpus (CBC), a repository containing communicator, message, and corresponding
receiver behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1&quot;&gt;Ashmit Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Aditya Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1&quot;&gt;Aanisha Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1&quot;&gt;Yaman K Singla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Somesh Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1&quot;&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1&quot;&gt;Ishita Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrangeli_S/0/1/0/all/0/1&quot;&gt;Stefano Petrangeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajiv Ratn Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changyou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1&quot;&gt;Balaji Krishnamurthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01270">
<title>COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01270</link>
<description rdf:parseType="Literal">&lt;p&gt;We present COMEDIAN, a novel pipeline to initialize spatiotemporal
transformers for action spotting, which involves self-supervised learning and
knowledge distillation. Action spotting is a timestamp-level temporal action
detection task. Our pipeline consists of three steps, with two initialization
stages. First, we perform self-supervised initialization of a spatial
transformer using short videos as input. Additionally, we initialize a temporal
transformer that enhances the spatial transformer&apos;s outputs with global context
through knowledge distillation from a pre-computed feature bank aligned with
each short video segment. In the final step, we fine-tune the transformers to
the action spotting task. The experiments, conducted on the SoccerNet-v2
dataset, demonstrate state-of-the-art performance and validate the
effectiveness of COMEDIAN&apos;s pretraining paradigm. Our results highlight several
advantages of our pretraining pipeline, including improved performance and
faster convergence compared to non-pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denize_J/0/1/0/all/0/1&quot;&gt;Julien Denize&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liashuha_M/0/1/0/all/0/1&quot;&gt;Mykola Liashuha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1&quot;&gt;Jaonary Rabarisoa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orcesi_A/0/1/0/all/0/1&quot;&gt;Astrid Orcesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herault_R/0/1/0/all/0/1&quot;&gt;Romain H&amp;#xe9;rault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06118">
<title>CHITNet: A Complementary to Harmonious Information Transfer Network for Infrared and Visible Image Fusion. (arXiv:2309.06118v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06118</link>
<description rdf:parseType="Literal">&lt;p&gt;Current infrared and visible image fusion (IVIF) methods go to great lengths
to excavate complementary features and design complex fusion strategies, which
is extremely challenging. To this end, we rethink the IVIF outside the box,
proposing a complementary to harmonious information transfer network (CHITNet).
It reasonably transfers complementary information into harmonious one, which
integrates both the shared and complementary features from two modalities.
Specifically, to skillfully sidestep aggregating complementary information in
IVIF, we design a mutual information transfer (MIT) module to mutually
represent features from two modalities, roughly transferring complementary
information into harmonious one. Then, a harmonious information acquisition
supervised by source image (HIASSI) module is devised to further ensure the
complementary to harmonious information transfer after MIT. Meanwhile, we also
propose a structure information preservation (SIP) module to guarantee that the
edge structure information of the source images can be transferred to the
fusion results. Moreover, a mutual promotion training paradigm (MPTP) with
interaction loss is adopted to facilitate better collaboration among MIT,
HIASSI and SIP. In this way, the proposed method is able to generate fused
images with higher qualities. Extensive experimental results demonstrate the
superiority of our CHITNet over state-of-the-art algorithms in terms of visual
quality and quantitative evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yafei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1&quot;&gt;Keying Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huafeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07254">
<title>Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement. (arXiv:2309.07254v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07254</link>
<description rdf:parseType="Literal">&lt;p&gt;While diffusion models demonstrate a remarkable capability for generating
high-quality images, their tendency to `replicate&apos; training data raises privacy
concerns. Although recent research suggests that this replication may stem from
the insufficient generalization of training data captions and duplication of
training images, effective mitigation strategies remain elusive. To address
this gap, our paper first introduces a generality score that measures the
caption generality and employ large language model (LLM) to generalize training
captions. Subsequently, we leverage generalized captions and propose a novel
dual fusion enhancement approach to mitigate the replication of diffusion
models. Our empirical results demonstrate that our proposed methods can
significantly reduce replication by 43.5% compared to the original diffusion
model while maintaining the diversity and quality of generations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dake Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1&quot;&gt;Peter A. Beerel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07510">
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent&apos;s morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
Project page at https://chengkaiacademycity.github.io/EnvAwareAfford/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Guanqi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13847">
<title>Tuning Multi-mode Token-level Prompt Alignment across Modalities. (arXiv:2309.13847v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13847</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in prompt tuning of vision-language models have underscored
their potential in enhancing open-world visual concept comprehension. However,
prior works only primarily focus on single-mode (only one prompt for each
modality) and holistic level (image or sentence) semantic alignment, which
fails to capture the sample diversity, leading to sub-optimal prompt discovery.
To address the limitation, we propose a multi-mode token-level tuning framework
that leverages the optimal transportation to learn and align a set of prompt
tokens across modalities. Specifically, we rely on two essential factors: 1)
multi-mode prompts discovery, which guarantees diverse semantic
representations, and 2) token-level alignment, which helps explore fine-grained
similarity. Consequently, the similarity can be calculated as a hierarchical
transportation problem between the modality-specific sets. Extensive
experiments on popular image recognition benchmarks show the superior
generalization and few-shot abilities of our approach. The qualitative analysis
demonstrates that the learned prompt tokens have the ability to capture diverse
visual concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dongsheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Miaoge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;MingSheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14660">
<title>CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-to-point cloud (I2P) registration is a fundamental task in the field of
autonomous vehicles and transportation systems for cross-modality data fusion
and localization. Existing I2P registration methods estimate correspondences at
the point/pixel level, often overlooking global alignment. However, I2P
matching can easily converge to a local optimum when performed without
high-level guidance from global constraints. To address this issue, this paper
introduces CoFiI2P, a novel I2P registration network that extracts
correspondences in a coarse-to-fine manner to achieve the globally optimal
solution. First, the image and point cloud data are processed through a Siamese
encoder-decoder network for hierarchical feature extraction. Second, a
coarse-to-fine matching module is designed to leverage these features and
establish robust feature correspondences. Specifically, In the coarse matching
phase, a novel I2P transformer module is employed to capture both homogeneous
and heterogeneous global information from the image and point cloud data. This
enables the estimation of coarse super-point/super-pixel matching pairs with
discriminative descriptors. In the fine matching module, point/pixel pairs are
established with the guidance of super-point/super-pixel correspondences.
Finally, based on matching pairs, the transform matrix is estimated with the
EPnP-RANSAC algorithm. Extensive experiments conducted on the KITTI dataset
demonstrate that CoFiI2P achieves impressive results, with a relative rotation
error (RRE) of 1.14 degrees and a relative translation error (RTE) of 0.29
meters. These results represent a significant improvement of 84\% in RRE and
89\% in RTE compared to the current state-of-the-art (SOTA) method. Qualitative
results are available at https://youtu.be/ovbedasXuZE. The source code will be
publicly released at https://github.com/kang-1-2-3/CoFiI2P.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shuhao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Youqi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Fuxun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bisheng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00917">
<title>Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards Enhancing Text Spotting Performance. (arXiv:2310.00917v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;The adaptation capability to a wide range of domains is crucial for scene
text spotting models when deployed to real-world conditions. However, existing
state-of-the-art (SOTA) approaches usually incorporate scene text detection and
recognition simply by pretraining on natural scene text datasets, which do not
directly exploit the intermediate feature representations between multiple
domains. Here, we investigate the problem of domain-adaptive scene text
spotting, i.e., training a model on multi-domain source data such that it can
directly adapt to target domains rather than being specialized for a specific
domain or scenario. Further, we investigate a transformer baseline called
Swin-TESTR to focus on solving scene-text spotting for both regular and
arbitrary-shaped scene text along with an exhaustive evaluation. The results
clearly demonstrate the potential of intermediate representations to achieve
significant performance on text spotting benchmarks across multiple domains
(e.g. language, synth-to-real, and documents). both in terms of accuracy and
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Alloy Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sanket Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ayan Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1&quot;&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1&quot;&gt;Umapada Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01164">
<title>Segment Any Building. (arXiv:2310.01164v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01164</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of identifying and segmenting buildings within remote sensing
imagery has perennially stood at the forefront of scholarly investigations.
This manuscript accentuates the potency of harnessing diversified datasets in
tandem with cutting-edge representation learning paradigms for building
segmentation in such images. Through the strategic amalgamation of disparate
datasets, we have not only expanded the informational horizon accessible for
model training but also manifested unparalleled performance metrics across
multiple datasets. Our avant-garde joint training regimen underscores the merit
of our approach, bearing significant implications in pivotal domains such as
urban infrastructural development, disaster mitigation strategies, and
ecological surveillance. Our methodology, predicated upon the fusion of
datasets and gleaning insights from pre-trained models, carves a new benchmark
in the annals of building segmentation endeavors. The outcomes of this research
both fortify the foundations for ensuing scholarly pursuits and presage a
horizon replete with innovative applications in the discipline of building
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02255">
<title>MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02255</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit
impressive problem-solving skills in many tasks and domains, but their ability
in mathematical reasoning in visual contexts has not been systematically
studied. To bridge this gap, we present MathVista, a benchmark designed to
combine challenges from diverse mathematical and visual tasks. It consists of
6,141 examples, derived from 28 existing multimodal datasets involving
mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and
PaperQA). Completing these tasks requires fine-grained, deep visual
understanding and compositional reasoning, which all state-of-the-art
foundation models find challenging. With MathVista, we have conducted a
comprehensive, quantitative evaluation of 12 prominent foundation models. The
best-performing GPT-4V model achieves an overall accuracy of 49.9%,
substantially outperforming Bard, the second-best performer, by 15.1%. Our
in-depth analysis reveals that the superiority of GPT-4V is mainly attributed
to its enhanced visual perception and mathematical reasoning. However, GPT-4V
still falls short of human performance by 10.4%, as it often struggles to
understand complex figures and perform rigorous reasoning. This significant gap
underscores the critical role that MathVista will play in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. We further explore the new ability of
self-verification, the application of self-consistency, and the interactive
chatbot capabilities of GPT-4V, highlighting its promising potential for future
research. The project is available at https://mathvista.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tony Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiacheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1&quot;&gt;Michel Galley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08872">
<title>R&amp;B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation. (arXiv:2310.08872v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08872</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image (T2I) diffusion models have achieved remarkable progress
in generating high-quality images given text-prompts as input. However, these
models fail to convey appropriate spatial composition specified by a layout
instruction. In this work, we probe into zero-shot grounded T2I generation with
diffusion models, that is, generating images corresponding to the input layout
information without training auxiliary modules or finetuning diffusion models.
We propose a Region and Boundary (R&amp;amp;B) aware cross-attention guidance approach
that gradually modulates the attention maps of diffusion model during
generative process, and assists the model to synthesize images (1) with high
fidelity, (2) highly compatible with textual input, and (3) interpreting layout
instructions accurately. Specifically, we leverage the discrete sampling to
bridge the gap between consecutive attention maps and discrete layout
constraints, and design a region-aware loss to refine the generative layout
during diffusion process. We further propose a boundary-aware loss to
strengthen object discriminability within the corresponding regions.
Experimental results show that our method outperforms existing state-of-the-art
zero-shot grounded T2I generation methods by a large margin both qualitatively
and quantitatively on several benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jiayu Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1&quot;&gt;Henglei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09478">
<title>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning. (arXiv:2310.09478v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09478</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have shown their remarkable capabilities as a general
interface for various language-related applications. Motivated by this, we
target to build a unified interface for completing many vision-language tasks
including image description, visual question answering, and visual grounding,
among others. The challenge is to use a single model for performing diverse
vision-language tasks effectively with simple multi-modal instructions. Towards
this objective, we introduce MiniGPT-v2, a model that can be treated as a
unified interface for better handling various vision-language tasks. We propose
using unique identifiers for different tasks when training the model. These
identifiers enable our model to better distinguish each task instruction
effortlessly and also improve the model learning efficiency for each task.
After the three-stage training, the experimental results show that MiniGPT-v2
achieves strong performance on many visual question-answering and visual
grounding benchmarks compared to other vision-language generalist models. Our
model and codes are available at https://minigpt-v2.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Deyao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zechun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1&quot;&gt;Vikas Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1&quot;&gt;Mohamed Elhoseiny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13268">
<title>DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics. (arXiv:2310.13268v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13268</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion probabilistic models (DPMs) have exhibited excellent performance
for high-fidelity image generation while suffering from inefficient sampling.
Recent works accelerate the sampling procedure by proposing fast ODE solvers
that leverage the specific ODE form of DPMs. However, they highly rely on
specific parameterization during inference (such as noise/data prediction),
which might not be the optimal choice. In this work, we propose a novel
formulation towards the optimal parameterization during sampling that minimizes
the first-order discretization error of the ODE solution. Based on such
formulation, we propose \textit{DPM-Solver-v3}, a new fast ODE solver for DPMs
by introducing several coefficients efficiently computed on the pretrained
model, which we call \textit{empirical model statistics}. We further
incorporate multistep methods and a predictor-corrector framework, and propose
some techniques for improving sample quality at small numbers of function
evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3
achieves consistently better or comparable performance in both unconditional
and conditional sampling with both pixel-space and latent-space DPMs,
especially in 5$\sim$10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE)
on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable
Diffusion, bringing a speed-up of 15\%$\sim$30\% compared to previous
state-of-the-art training-free methods. Code is available at
\url{https://github.com/thu-ml/DPM-Solver-v3}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kaiwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianfei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14159">
<title>Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14159</link>
<description rdf:parseType="Literal">&lt;p&gt;As short-form funny videos on social networks are gaining popularity, it
becomes demanding for AI models to understand them for better communication
with humans. Unfortunately, previous video humor datasets target specific
domains, such as speeches or sitcoms, and mostly focus on verbal cues. We
curate a user-generated dataset of 10K multimodal funny videos from YouTube,
called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both
verbal and visual elements contributing to humor. After filtering, we annotate
each video with timestamps and text explanations for funny moments. Our
ExFunTube is unique over existing datasets in that our videos cover a wide
range of domains with various types of humor that necessitate a multimodal
understanding of the content. Also, we develop a zero-shot video-to-text
prompting to maximize video humor understanding of large language models
(LLMs). With three different evaluation methods using automatic scores,
rationale quality experiments, and human evaluations, we show that our
prompting significantly improves LLMs&apos; ability for humor explanation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1&quot;&gt;Dayoon Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gunhee Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15105">
<title>FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning. (arXiv:2310.15105v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15105</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the limited availability of data, existing few-shot learning methods
trained from scratch fail to achieve satisfactory performance. In contrast,
large-scale pre-trained models such as CLIP demonstrate remarkable few-shot and
zero-shot capabilities. To enhance the performance of pre-trained models for
downstream tasks, fine-tuning the model on downstream data is frequently
necessary. However, fine-tuning the pre-trained model leads to a decrease in
its generalizability in the presence of distribution shift, while the limited
number of samples in few-shot learning makes the model highly susceptible to
overfitting. Consequently, existing methods for fine-tuning few-shot learning
primarily focus on fine-tuning the model&apos;s classification head or introducing
additional structure. In this paper, we introduce a fine-tuning approach termed
Feature Discrimination Alignment (FD-Align). Our method aims to bolster the
model&apos;s generalizability by preserving the consistency of spurious features
across the fine-tuning process. Extensive experimental results validate the
efficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model
can seamlessly integrate with existing methods, leading to performance
improvements. Our code can be found in https://github.com/skingorz/FD-Align.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huimin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Bochao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15447">
<title>DeepIron: Predicting Unwarped Garment Texture from a Single Image. (arXiv:2310.15447v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15447</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic reconstruction of 3D clothing from an image has wide applications,
such as avatar creation and virtual try-on. This paper presents a novel
framework that reconstructs the texture map for 3D garments from a single image
with pose. Assuming that 3D garments are modeled by stitching 2D garment sewing
patterns, our specific goal is to generate a texture image for the sewing
patterns. A key component of our framework, the Texture Unwarper, infers the
original texture image from the input clothing image, which exhibits warping
and occlusion of texture due to the user&apos;s body shape and pose. The Texture
Unwarper effectively transforms between the input and output images by mapping
the latent spaces of the two images. By inferring the unwarped original texture
of the input garment, our method helps reconstruct 3D garment models that can
show high-quality texture images realistically deformed for new poses. We
validate the effectiveness of our approach through a comparison with other
methods and ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Hyun-Song Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sung-Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15712">
<title>GNeSF: Generalizable Neural Semantic Fields. (arXiv:2310.15712v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15712</link>
<description rdf:parseType="Literal">&lt;p&gt;3D scene segmentation based on neural implicit representation has emerged
recently with the advantage of training only on 2D supervision. However,
existing approaches still requires expensive per-scene optimization that
prohibits generalization to novel scenes during inference. To circumvent this
problem, we introduce a generalizable 3D segmentation framework based on
implicit representation. Specifically, our framework takes in multi-view image
features and semantic maps as the inputs instead of only spatial information to
avoid overfitting to scene-specific geometric and semantic information. We
propose a novel soft voting mechanism to aggregate the 2D semantic information
from different views for each 3D point. In addition to the image features, view
difference information is also encoded in our framework to predict the voting
scores. Intuitively, this allows the semantic information from nearby views to
contribute more compared to distant ones. Furthermore, a visibility module is
also designed to detect and filter out detrimental information from occluded
views. Due to the generalizability of our proposed method, we can synthesize
semantic maps or conduct 3D semantic segmentation for novel scenes with solely
2D semantic supervision. Experimental results show that our approach achieves
comparable performance with scene-specific approaches. More importantly, our
approach can even outperform existing strong supervision-based approaches with
only 2D annotations. Our source code is available at:
https://github.com/HLinChen/GNeSF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanlin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mengqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16002">
<title>Integrating View Conditions for Image Synthesis. (arXiv:2310.16002v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16002</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of image processing, applying intricate semantic modifications
within existing images remains an enduring challenge. This paper introduces a
pioneering framework that integrates viewpoint information to enhance the
control of image editing tasks. By surveying existing object editing
methodologies, we distill three essential criteria, consistency,
controllability, and harmony, that should be met for an image editing method.
In contrast to previous approaches, our method takes the lead in satisfying all
three requirements for addressing the challenge of image synthesis. Through
comprehensive experiments, encompassing both quantitative assessments and
qualitative comparisons with contemporary state-of-the-art methods, we present
compelling evidence of our framework&apos;s superior performance across multiple
dimensions. This work establishes a promising avenue for advancing image
synthesis techniques and empowering precise object modifications while
preserving the visual coherence of the entire composition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinbin Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1&quot;&gt;Aosong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaicheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16020">
<title>ConvBKI: Real-Time Probabilistic Semantic Mapping Network with Quantifiable Uncertainty. (arXiv:2310.16020v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16020</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a modular neural network for real-time semantic
mapping in uncertain environments, which explicitly updates per-voxel
probabilistic distributions within a neural network layer. Our approach
combines the reliability of classical probabilistic algorithms with the
performance and efficiency of modern neural networks. Although robotic
perception is often divided between modern differentiable methods and classical
explicit methods, a union of both is necessary for real-time and trustworthy
performance. We introduce a novel Convolutional Bayesian Kernel Inference
(ConvBKI) layer which incorporates semantic segmentation predictions online
into a 3D map through a depthwise convolution layer by leveraging conjugate
priors. We compare ConvBKI against state-of-the-art deep learning approaches
and probabilistic algorithms for mapping to evaluate reliability and
performance. We also create a Robot Operating System (ROS) package of ConvBKI
and test it on real-world perceptually challenging off-road driving data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1&quot;&gt;Joey Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yuewei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friesen_J/0/1/0/all/0/1&quot;&gt;Joshua Friesen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewen_P/0/1/0/all/0/1&quot;&gt;Parker Ewen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1&quot;&gt;Andrew Capodieci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1&quot;&gt;Paramsothy Jayakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1&quot;&gt;Kira Barton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1&quot;&gt;Maani Ghaffari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16436">
<title>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. (arXiv:2310.16436v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16436</link>
<description rdf:parseType="Literal">&lt;p&gt;A long-standing goal of AI systems is to perform complex multimodal reasoning
like humans. Recently, large language models (LLMs) have made remarkable
strides in such multi-step reasoning on the language modality solely by
leveraging the chain of thought (CoT) to mimic human thinking. However, the
transfer of these advancements to multimodal contexts introduces heightened
challenges, including but not limited to the impractical need for
labor-intensive annotation and the limitations in terms of flexibility,
generalizability, and explainability. To evoke CoT reasoning in multimodality,
this work first conducts an in-depth analysis of these challenges posed by
multimodality and presents two key insights: &quot;keeping critical thinking&quot; and
&quot;letting everyone do their jobs&quot; in multimodal CoT reasoning. Furthermore, this
study proposes a novel DDCoT prompting that maintains a critical attitude
through negative-space prompting and incorporates multimodality into reasoning
by first dividing the reasoning responsibility of LLMs into reasoning and
recognition and then integrating the visual recognition capability of visual
models into the joint reasoning process. The rationales generated by DDCoT not
only improve the reasoning abilities of both large and small language models in
zero-shot prompting and fine-tuning learning, significantly outperforming
state-of-the-art methods but also exhibit impressive generalizability and
explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Ge Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiajin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16639">
<title>Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving. (arXiv:2310.16639v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16639</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept bottleneck models have been successfully used for explainable machine
learning by encoding information within the model with a set of human-defined
concepts. In the context of human-assisted or autonomous driving,
explainability models can help user acceptance and understanding of decisions
made by the autonomous vehicle, which can be used to rationalize and explain
driver or vehicle behavior. We propose a new approach using concept bottlenecks
as visual features for control command predictions and explanations of user and
vehicle behavior. We learn a human-understandable concept layer that we use to
explain sequential driving scenes while learning vehicle control commands. This
approach can then be used to determine whether a change in a preferred gap or
steering commands from a human (or autonomous vehicle) is led by an external
stimulus or change in preferences. We achieve competitive performance to latent
visual features while gaining interpretability within our model setup.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echterhoff_J/0/1/0/all/0/1&quot;&gt;Jessica Echterhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1&quot;&gt;An Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kyungtae Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelraouf_A/0/1/0/all/0/1&quot;&gt;Amr Abdelraouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1&quot;&gt;Rohit Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1&quot;&gt;Julian McAuley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16818">
<title>DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior. (arXiv:2310.16818v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16818</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DreamCraft3D, a hierarchical 3D content generation method that
produces high-fidelity and coherent 3D objects. We tackle the problem by
leveraging a 2D reference image to guide the stages of geometry sculpting and
texture boosting. A central focus of this work is to address the consistency
issue that existing works encounter. To sculpt geometries that render
coherently, we perform score distillation sampling via a view-dependent
diffusion model. This 3D prior, alongside several training strategies,
prioritizes the geometry consistency but compromises the texture fidelity. We
further propose Bootstrapped Score Distillation to specifically boost the
texture. We train a personalized diffusion model, Dreambooth, on the augmented
renderings of the scene, imbuing it with 3D knowledge of the scene being
optimized. The score distillation from this 3D-aware diffusion prior provides
view-consistent guidance for the scene. Notably, through an alternating
optimization of the diffusion prior and 3D scene representation, we achieve
mutually reinforcing improvements: the optimized 3D scene aids in training the
scene-specific diffusion model, which offers increasingly view-consistent
guidance for 3D optimization. The optimization is thus bootstrapped and leads
to substantial texture boosting. With tailored 3D priors throughout the
hierarchical generation, DreamCraft3D generates coherent 3D objects with
photorealistic renderings, advancing the state-of-the-art in 3D content
generation. Code available at https://github.com/deepseek-ai/DreamCraft3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Ruizhi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhenda Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yebin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.01992">
<title>Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2010.01992</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the framework of multi-task representation (MTR)
learning where the goal is to use source tasks to learn a representation that
reduces the sample complexity of solving a target task. We start by reviewing
recent advances in MTR theory and show that they can provide novel insights for
popular meta-learning algorithms when analyzed within this framework. In
particular, we highlight a fundamental difference between gradient-based and
metric-based algorithms in practice and put forward a theoretical analysis to
explain it. Finally, we use the derived insights to improve the performance of
meta-learning methods via a new spectral-based regularization term and confirm
its efficiency through experimental studies on few-shot classification
benchmarks. To the best of our knowledge, this is the first contribution that
puts the most recent learning bounds of MTR theory into practice for the task
of few-shot classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouniot_Q/0/1/0/all/0/1&quot;&gt;Quentin Bouniot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1&quot;&gt;Ievgen Redko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1&quot;&gt;Romaric Audigier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1&quot;&gt;Ang&amp;#xe9;lique Loesch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1&quot;&gt;Amaury Habrard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00990">
<title>A weighted-variance variational autoencoder model for speech enhancement. (arXiv:2211.00990v2 [cs.SD] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2211.00990</link>
<description rdf:parseType="Literal">&lt;p&gt;We address speech enhancement based on variational autoencoders, which
involves learning a speech prior distribution in the time-frequency (TF)
domain. A zero-mean complex-valued Gaussian distribution is usually assumed for
the generative model, where the speech information is encoded in the variance
as a function of a latent variable. In contrast to this commonly used approach,
we propose a weighted variance generative model, where the contribution of each
spectrogram time-frame in parameter learning is weighted. We impose a Gamma
prior distribution on the weights, which would effectively lead to a Student&apos;s
t-distribution instead of Gaussian for speech generative modeling. We develop
efficient training and speech enhancement algorithms based on the proposed
generative model. Our experimental results on spectrogram auto-encoding and
speech enhancement demonstrate the effectiveness and robustness of the proposed
approach compared to the standard unweighted variance model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golmakani_A/0/1/0/all/0/1&quot;&gt;Ali Golmakani&lt;/a&gt; (MULTISPEECH), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1&quot;&gt;Mostafa Sadeghi&lt;/a&gt; (MULTISPEECH), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1&quot;&gt;Xavier Alameda-Pineda&lt;/a&gt; (ROBOTLEARN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serizel_R/0/1/0/all/0/1&quot;&gt;Romain Serizel&lt;/a&gt; (MULTISPEECH)</dc:creator>
</item>
</rdf:RDF>