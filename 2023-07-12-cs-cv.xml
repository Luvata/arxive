<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2005.01344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.08756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04390" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.04771">
<title>Invariant Scattering Transform for Medical Imaging. (arXiv:2307.04771v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.04771</link>
<description rdf:parseType="Literal">&lt;p&gt;Invariant scattering transform introduces new area of research that merges
the signal processing with deep learning for computer vision. Nowadays, Deep
Learning algorithms are able to solve a variety of problems in medical sector.
Medical images are used to detect diseases brain cancer or tumor, Alzheimer&apos;s
disease, breast cancer, Parkinson&apos;s disease and many others. During pandemic
back in 2020, machine learning and deep learning has played a critical role to
detect COVID-19 which included mutation analysis, prediction, diagnosis and
decision making. Medical images like X-ray, MRI known as magnetic resonance
imaging, CT scans are used for detecting diseases. There is another method in
deep learning for medical imaging which is scattering transform. It builds
useful signal representation for image classification. It is a wavelet
technique; which is impactful for medical image classification problems. This
research article discusses scattering transform as the efficient system for
medical image analysis where it&apos;s figured by scattering the signal information
implemented in a deep convolutional network. A step by step case study is
manifested at this research work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huda_N/0/1/0/all/0/1&quot;&gt;Nafisa Labiba Ishrat Huda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biswas_A/0/1/0/all/0/1&quot;&gt;Angona Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nasim_M/0/1/0/all/0/1&quot;&gt;MD Abdullah Al Nasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md. Fahim Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Shoaib Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04787">
<title>Collaborative Score Distillation for Consistent Visual Synthesis. (arXiv:2307.04787v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04787</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative priors of large-scale text-to-image diffusion models enable a wide
range of new generation and editing applications on diverse visual modalities.
However, when adapting these priors to complex visual modalities, often
represented as multiple images (e.g., video), achieving consistency across a
set of images is challenging. In this paper, we address this challenge with a
novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein
Variational Gradient Descent (SVGD). Specifically, we propose to consider
multiple samples as &quot;particles&quot; in the SVGD update and combine their score
functions to distill generative priors over a set of images synchronously.
Thus, CSD facilitates seamless integration of information across 2D images,
leading to a consistent visual synthesis across multiple samples. We show the
effectiveness of CSD in a variety of tasks, encompassing the visual editing of
panorama images, videos, and 3D scenes. Our results underline the competency of
CSD as a versatile method for enhancing inter-sample consistency, thereby
broadening the applicability of text-to-image diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Subin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyungmin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;June Suk Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Jongheon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04816">
<title>Q-YOLO: Efficient Inference for Real-time Object Detection. (arXiv:2307.04816v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04816</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time object detection plays a vital role in various computer vision
applications. However, deploying real-time object detectors on
resource-constrained platforms poses challenges due to high computational and
memory requirements. This paper describes a low-bit quantization method to
build a highly efficient one-stage detector, dubbed as Q-YOLO, which can
effectively address the performance degradation problem caused by activation
distribution imbalance in traditional quantized YOLO models. Q-YOLO introduces
a fully end-to-end Post-Training Quantization (PTQ) pipeline with a
well-designed Unilateral Histogram-based (UH) activation quantization scheme,
which determines the maximum truncation values through histogram analysis by
minimizing the Mean Squared Error (MSE) quantization errors. Extensive
experiments on the COCO dataset demonstrate the effectiveness of Q-YOLO,
outperforming other PTQ methods while achieving a more favorable balance
between accuracy and computational cost. This research contributes to advancing
the efficient deployment of object detection models on resource-limited edge
devices, enabling real-time detection with reduced computational and memory
overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huixin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuhui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xianbin Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04838">
<title>CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04838</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the potential of Vision-Language Models (VLMs),
specifically CLIP, in predicting visual object relationships, which involves
interpreting visual features from images into language-based relations. Current
state-of-the-art methods use complex graphical models that utilize language
cues and visual features to address this challenge. We hypothesize that the
strong language priors in CLIP embeddings can simplify these graphical models
paving for a simpler approach. We adopt the UVTransE relation prediction
framework, which learns the relation as a translational embedding with subject,
object, and union box embeddings from a scene. We systematically explore the
design of CLIP-based subject, object, and union-box representations within the
UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate
Estimation). CREPE utilizes text-based representations for all three bounding
boxes and introduces a novel contrastive training strategy to automatically
infer the text prompt for union-box. Our approach achieves state-of-the-art
performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual
Genome benchmark, achieving a 15.3\% gain in performance over recent
state-of-the-art at mR@20. This work demonstrates CLIP&apos;s effectiveness in
object relation prediction and encourages further research on VLMs in this
challenging domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanyam_R/0/1/0/all/0/1&quot;&gt;Rakshith Subramanyam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayram_T/0/1/0/all/0/1&quot;&gt;T. S. Jayram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04858">
<title>AmadeusGPT: a natural language interface for interactive animal behavioral analysis. (arXiv:2307.04858v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.04858</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of quantifying and analyzing animal behavior involves translating
the naturally occurring descriptive language of their actions into
machine-readable code. Yet, codifying behavior analysis is often challenging
without deep understanding of animal behavior and technical machine learning
knowledge. To limit this gap, we introduce AmadeusGPT: a natural language
interface that turns natural language descriptions of behaviors into
machine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4
allow for interactive language-based queries that are potentially well suited
for making interactive behavior analysis. However, the comprehension capability
of these LLMs is limited by the context window size, which prevents it from
remembering distant conversations. To overcome the context window limitation,
we implement a novel dual-memory mechanism to allow communication between
short-term and long-term memory using symbols as context pointers for retrieval
and saving. Concretely, users directly use language-based definitions of
behavior and our augmented GPT develops code based on the core AmadeusGPT API,
which contains machine learning, computer vision, spatio-temporal reasoning,
and visualization modules. Users then can interactively refine results, and
seamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and
show we can produce state-of-the-art performance on the MABE 2022 behavior
challenge tasks. Note, an end-user would not need to write any code to achieve
this. Thus, collectively AmadeusGPT presents a novel way to merge deep
biological knowledge, large-language models, and core computer vision modules
into a more naturally intelligent system. Code and demos can be found at:
https://github.com/AdaptiveMotorControlLab/AmadeusGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Shaokai Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauer_J/0/1/0/all/0/1&quot;&gt;Jessy Lauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_A/0/1/0/all/0/1&quot;&gt;Alexander Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathis_M/0/1/0/all/0/1&quot;&gt;Mackenzie W. Mathis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04859">
<title>Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04859</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to generate diverse 3D articulated head avatars is vital to a
plethora of applications, including augmented reality, cinematography, and
education. Recent work on text-guided 3D object generation has shown great
promise in addressing these needs. These methods directly leverage pre-trained
2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance
fields of generic objects. However, due to the lack of geometry and texture
priors, these methods have limited control over the generated 3D objects,
making it difficult to operate inside a specific domain, e.g., human heads. In
this work, we develop a new approach to text-guided 3D head avatar generation
to address this limitation. Our framework directly operates on the geometry and
texture of an articulable 3D morphable model (3DMM) of a head, and introduces
novel optimization procedures to update the geometry and texture while keeping
the 2D and 3D facial features aligned. The result is a 3D head avatar that is
consistent with the text description and can be readily articulated using the
deformation model of the 3DMM. We show that our diffusion-based articulated
head avatars outperform state-of-the-art approaches for this task. The latter
are typically based on CLIP, which is known to provide limited diversity of
generation and accuracy for 3D object generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1&quot;&gt;Alexander W. Bergman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1&quot;&gt;Wang Yifan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04869">
<title>Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04869</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated continual learning (FCL) learns incremental tasks over time from
confidential datasets distributed across clients. This paper focuses on
rehearsal-free FCL, which has severe forgetting issues when learning new tasks
due to the lack of access to historical task data. To address this issue, we
propose Fed-CPrompt based on prompt learning techniques to obtain task-specific
prompts in a communication-efficient way. Fed-CPrompt introduces two key
components, asynchronous prompt learning, and contrastive continual loss, to
handle asynchronous task arrival and heterogeneous data distributions in FCL,
respectively. Extensive experiments demonstrate the effectiveness of
Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagwe_G/0/1/0/all/0/1&quot;&gt;Gaurav Bagwe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiaoyong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Miao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04909">
<title>Planar Curve Registration using Bayesian Inversion. (arXiv:2307.04909v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04909</link>
<description rdf:parseType="Literal">&lt;p&gt;We study parameterisation-independent closed planar curve matching as a
Bayesian inverse problem. The motion of the curve is modelled via a curve on
the diffeomorphism group acting on the ambient space, leading to a large
deformation diffeomorphic metric mapping (LDDMM) functional penalising the
kinetic energy of the deformation. We solve Hamilton&apos;s equations for the curve
matching problem using the Wu-Xu element [S. Wu, J. Xu, Nonconforming finite
element spaces for $2m^\text{th}$ order partial differential equations on
$\mathbb{R}^n$ simplicial grids when $m=n+1$, Mathematics of Computation 88
(316) (2019) 531-551] which provides mesh-independent Lipschitz constants for
the forward motion of the curve, and solve the inverse problem for the momentum
using Bayesian inversion. Since this element is not affine-equivalent we
provide a pullback theory which expedites the implementation and efficiency of
the forward map. We adopt ensemble Kalman inversion using a negative Sobolev
norm mismatch penalty to measure the discrepancy between the target and the
ensemble mean shape. We provide several numerical examples to validate the
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bock_A/0/1/0/all/0/1&quot;&gt;Andreas Bock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotter_C/0/1/0/all/0/1&quot;&gt;Colin J. Cotter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1&quot;&gt;Robert C. Kirby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04916">
<title>Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery. (arXiv:2307.04916v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04916</link>
<description rdf:parseType="Literal">&lt;p&gt;Deforestation estimation and fire detection in the Amazon forest poses a
significant challenge due to the vast size of the area and the limited
accessibility. However, these are crucial problems that lead to severe
environmental consequences, including climate change, global warming, and
biodiversity loss. To effectively address this problem, multimodal satellite
imagery and remote sensing offer a promising solution for estimating
deforestation and detecting wildfire in the Amazonia region. This research
paper introduces a new curated dataset and a deep learning-based approach to
solve these problems using convolutional neural networks (CNNs) and
comprehensive data processing techniques. Our dataset includes curated images
and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.
We design the dataset considering different spatial and temporal resolution
requirements. Our method successfully achieves high-precision deforestation
estimation and burned area detection on unseen images from the region. Our
code, models and dataset are open source:
https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fodor_G/0/1/0/all/0/1&quot;&gt;Gabor Fodor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1&quot;&gt;Marcos V. Conde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04918">
<title>Kinematically-Decoupled Impedance Control for Fast Object Visual Servoing and Grasping on Quadruped Manipulators. (arXiv:2307.04918v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.04918</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a control pipeline for SAG (Searching, Approaching, and Grasping)
of objects, based on a decoupled arm kinematic chain and impedance control,
which integrates image-based visual servoing (IBVS). The kinematic decoupling
allows for fast end-effector motions and recovery that leads to robust visual
servoing. The whole approach and pipeline can be generalized for any mobile
platform (wheeled or tracked vehicles), but is most suitable for dynamically
moving quadruped manipulators thanks to their reactivity against disturbances.
The compliance of the impedance controller makes the robot safer for
interactions with humans and the environment. We demonstrate the performance
and robustness of the proposed approach with various experiments on our 140 kg
HyQReal quadruped robot equipped with a 7-DoF manipulator arm. The experiments
consider dynamic locomotion, tracking under external disturbances, and fast
motions of the target object.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parosi_R/0/1/0/all/0/1&quot;&gt;Riccardo Parosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risiglione_M/0/1/0/all/0/1&quot;&gt;Mattia Risiglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caldwell_D/0/1/0/all/0/1&quot;&gt;Darwin G. Caldwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semini_C/0/1/0/all/0/1&quot;&gt;Claudio Semini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barasuol_V/0/1/0/all/0/1&quot;&gt;Victor Barasuol&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04924">
<title>Count-Free Single-Photon 3D Imaging with Race Logic. (arXiv:2307.04924v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.04924</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-photon cameras (SPCs) have emerged as a promising technology for
high-resolution 3D imaging. A single-photon 3D camera determines the round-trip
time of a laser pulse by capturing the arrival of individual photons at each
camera pixel. Constructing photon-timestamp histograms is a fundamental
operation for a single-photon 3D camera. However, in-pixel histogram processing
is computationally expensive and requires large amount of memory per pixel.
Digitizing and transferring photon timestamps to an off-sensor histogramming
module is bandwidth and power hungry. Here we present an online approach for
distance estimation without explicitly storing photon counts. The two key
ingredients of our approach are (a) processing photon streams using race logic,
which maintains photon data in the time-delay domain, and (b) constructing
count-free equi-depth histograms. Equi-depth histograms are a succinct
representation for ``peaky&apos;&apos; distributions, such as those obtained by an SPC
pixel from a laser pulse reflected by a surface. Our approach uses a binner
element that converges on the median (or, more generally, to another quantile)
of a distribution. We cascade multiple binners to form an equi-depth
histogrammer that produces multi-bin histograms. Our evaluation shows that this
method can provide an order of magnitude reduction in bandwidth and power
consumption while maintaining similar distance reconstruction accuracy as
conventional processing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ingle_A/0/1/0/all/0/1&quot;&gt;Atul Ingle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_D/0/1/0/all/0/1&quot;&gt;David Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04946">
<title>DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization. (arXiv:2307.04946v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04946</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems generally require a regularizer or prior for a good
solution. A recent trend is to train a convolutional net to denoise images, and
use this net as a prior when solving the inverse problem. Several proposals
depend on a singular value decomposition of the forward operator, and several
others backpropagate through the denoising net at runtime. Here we propose a
simpler approach that combines the traditional gradient-based minimization of
reconstruction error with denoising. Noise is also added at each step, so the
iterative dynamics resembles a Langevin or diffusion process. Both the level of
added noise and the size of the denoising step decay exponentially with time.
We apply our method to the problem of tomographic reconstruction from electron
micrographs acquired at multiple tilt angles. With empirical studies using
simulated tilt views, we find parameter settings for our method that produce
good results. We show that high accuracy can be achieved with as few as 50
denoising steps. We also compare with DDRM and DPS, more complex diffusion
methods of the kinds mentioned above. These methods are less accurate (as
measured by MSE and SSIM) for our tomography problem, even after the generation
hyperparameters are optimized. Finally we extend our method to reconstruction
of arbitrary-sized images and show results on 128 $\times$ 1568 pixel images
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1&quot;&gt;Kyle Luther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1&quot;&gt;H. Sebastian Seung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04952">
<title>Compact Twice Fusion Network for Edge Detection. (arXiv:2307.04952v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04952</link>
<description rdf:parseType="Literal">&lt;p&gt;The significance of multi-scale features has been gradually recognized by the
edge detection community. However, the fusion of multi-scale features increases
the complexity of the model, which is not friendly to practical application. In
this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate
multi-scale features while maintaining the compactness of the model. CTFN
includes two lightweight multi-scale feature fusion modules: a Semantic
Enhancement Module (SEM) that can utilize the semantic information contained in
coarse-scale features to guide the learning of fine-scale features, and a
Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary
merits of multi-scale features by assigning weights to all features.
Notwithstanding all this, the interference of texture noise makes the correct
classification of some pixels still a challenge. For these hard samples, we
propose a novel loss function, coined Dynamic Focal Loss, which reshapes the
standard cross-entropy loss and dynamically adjusts the weights to correct the
distribution of hard samples. We evaluate our method on three datasets, i.e.,
BSDS500, NYUDv2, and BIPEDv2. Compared with state-of-the-art methods, CTFN
achieves competitive accuracy with less parameters and computational cost.
Apart from the backbone, CTFN requires only 0.1M additional parameters, which
reduces its computation cost to just 60% of other state-of-the-art methods. The
codes are available at https://github.com/Li-yachuan/CTFN-pytorch-master.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yachuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongmin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+P%2E_X/0/1/0/all/0/1&quot;&gt;Xavier Soria P.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chaozhi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1&quot;&gt;Qian Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yun Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangdong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04956">
<title>PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation. (arXiv:2307.04956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04956</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual anomaly detection is essential and commonly used for many tasks in the
field of computer vision. Recent anomaly detection datasets mainly focus on
industrial automated inspection, medical image analysis and video surveillance.
In order to broaden the application and research of anomaly detection in
unmanned supermarkets and smart manufacturing, we introduce the supermarket
goods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolution
images of 484 different appearance goods divided into 6 categories. Each
category contains several common different types of anomalies such as
deformation, surface damage and opened. Anomalies contain both texture changes
and structural changes. It follows the unsupervised setting and only normal
(defect-free) images are used for training. Pixel-precise ground truth regions
are provided for all anomalies. Moreover, we also conduct a thorough evaluation
of current state-of-the-art unsupervised anomaly detection methods. This
initial benchmark indicates that some methods which perform well on the
industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on
our dataset. This is a comprehensive, multi-object dataset for supermarket
goods anomaly detection that focuses on real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Ge Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_M/0/1/0/all/0/1&quot;&gt;Miaoju Ban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Runwei Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04973">
<title>SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image. (arXiv:2307.04973v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04973</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Segmenting Anything has taken an important step towards general
artificial intelligence. At the same time, its reliability and fairness have
also attracted great attention, especially in the field of health care. In this
study, we propose multi-box prompts triggered uncertainty estimation for SAM
cues to demonstrate the reliability of segmented lesions or tissues. We
estimate the distribution of SAM predictions via Monte Carlo with prior
distribution parameters, which employs different prompts as formulation of
test-time augmentation. Our experimental results found that multi-box prompts
augmentation improve the SAM performance, and endowed each pixel with
uncertainty. This provides the first paradigm for a reliable SAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1&quot;&gt;Guoyao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1&quot;&gt;Ke Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xuedong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1&quot;&gt;Sancong Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04978">
<title>Diffusion idea exploration for art generation. (arXiv:2307.04978v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04978</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-Modal learning tasks have picked up pace in recent times. With plethora
of applications in diverse areas, generation of novel content using multiple
modalities of data has remained a challenging problem. To address the same,
various generative modelling techniques have been proposed for specific tasks.
Novel and creative image generation is one important aspect for industrial
application which could help as an arm for novel content generation. Techniques
proposed previously used Generative Adversarial Network(GAN), autoregressive
models and Variational Autoencoders (VAE) for accomplishing similar tasks.
These approaches are limited in their capability to produce images guided by
either text instructions or rough sketch images decreasing the overall
performance of image generator. We used state of the art diffusion models to
generate creative art by primarily leveraging text with additional support of
rough sketches. Diffusion starts with a pattern of random dots and slowly
converts that pattern into a design image using the guiding information fed
into the model. Diffusion models have recently outperformed other generative
models in image generation tasks using cross modal data as guiding information.
The initial experiments for this task of novel image generation demonstrated
promising qualitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1&quot;&gt;Nikhil Verma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04981">
<title>A Multi-view Impartial Decision Network for Frontotemporal Dementia Diagnosis. (arXiv:2307.04981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04981</link>
<description rdf:parseType="Literal">&lt;p&gt;Frontotemporal Dementia (FTD) diagnosis has been successfully progress using
deep learning techniques. However, current FTD identification methods suffer
from two limitations. Firstly, they do not exploit the potential of multi-view
functional magnetic resonance imaging (fMRI) for classifying FTD. Secondly,
they do not consider the reliability of the multi-view FTD diagnosis. To
address these limitations, we propose a reliable multi-view impartial decision
network (MID-Net) for FTD diagnosis in fMRI. Our MID-Net provides confidence
for each view and generates a reliable prediction without any conflict. To
achieve this, we employ multiple expert models to extract evidence from the
abundant neural network information contained in fMRI images. We then introduce
the Dirichlet Distribution to characterize the expert class probability
distribution from an evidence level. Additionally, a novel Impartial Decision
Maker (IDer) is proposed to combine the different opinions inductively to
arrive at an unbiased prediction without additional computation cost. Overall,
our MID-Net dynamically integrates the decisions of different experts on FTD
disease, especially when dealing with multi-view high-conflict cases. Extensive
experiments on a high-quality FTD fMRI dataset demonstrate that our model
outperforms previous methods and provides high uncertainty for hard-to-classify
examples. We believe that our approach represents a significant step toward the
deployment of reliable FTD decision-making under multi-expert conditions. We
will release the codes for reproduction after acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1&quot;&gt;Guoyao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1&quot;&gt;Ke Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xuedong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1&quot;&gt;Sancong Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05000">
<title>Neural Point-based Volumetric Avatar: Surface-guided Neural Points for Efficient and Photorealistic Volumetric Head Avatar. (arXiv:2307.05000v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05000</link>
<description rdf:parseType="Literal">&lt;p&gt;Rendering photorealistic and dynamically moving human heads is crucial for
ensuring a pleasant and immersive experience in AR/VR and video conferencing
applications. However, existing methods often struggle to model challenging
facial regions (e.g., mouth interior, eyes, hair/beard), resulting in
unrealistic and blurry results. In this paper, we propose {\fullname}
({\name}), a method that adopts the neural point representation as well as the
neural volume rendering process and discards the predefined connectivity and
hard correspondence imposed by mesh-based approaches. Specifically, the neural
points are strategically constrained around the surface of the target
expression via a high-resolution UV displacement map, achieving increased
modeling capacity and more accurate control. We introduce three technical
innovations to improve the rendering and training efficiency: a patch-wise
depth-guided (shading point) sampling strategy, a lightweight radiance decoding
process, and a Grid-Error-Patch (GEP) ray sampling strategy during training. By
design, our {\name} is better equipped to handle topologically changing regions
and thin structures while also ensuring accurate expression control when
animating avatars. Experiments conducted on three subjects from the Multiface
dataset demonstrate the effectiveness of our designs, outperforming previous
state-of-the-art methods, especially in handling challenging facial regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Di Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yanpei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1&quot;&gt;Linchao Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Song-Hai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05014">
<title>Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1&quot;&gt;Yossi Gandelsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05016">
<title>TRansPose: Large-Scale Multispectral Dataset for Transparent Object. (arXiv:2307.05016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05016</link>
<description rdf:parseType="Literal">&lt;p&gt;Transparent objects are encountered frequently in our daily lives, yet
recognizing them poses challenges for conventional vision sensors due to their
unique material properties, not being well perceived from RGB or depth cameras.
Overcoming this limitation, thermal infrared cameras have emerged as a
solution, offering improved visibility and shape information for transparent
objects. In this paper, we present TRansPose, the first large-scale
multispectral dataset that combines stereo RGB-D, thermal infrared (TIR)
images, and object poses to promote transparent object research. The dataset
includes 99 transparent objects, encompassing 43 household items, 27 recyclable
trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It
comprises a vast collection of 333,819 images and 4,000,056 annotations,
providing instance-level segmentation masks, ground-truth poses, and completed
depth information. The data was acquired using a FLIR A65 thermal infrared
(TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda
robot manipulator. Spanning 87 sequences, TRansPose covers various challenging
real-life scenarios, including objects filled with water, diverse lighting
conditions, heavy clutter, non-transparent or translucent containers, objects
in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed
from the following link: https://sites.google.com/view/transpose-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jeongyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1&quot;&gt;Myung-Hwan Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Sangwoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wooseong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Minwoo Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jaeho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Ayoung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05017">
<title>Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification. (arXiv:2307.05017v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05017</link>
<description rdf:parseType="Literal">&lt;p&gt;Decisions made by convolutional neural networks(CNN) can be understood and
explained by visualizing discriminative regions on images. To this end, Class
Activation Map (CAM) based methods were proposed as powerful interpretation
tools, making the prediction of deep learning models more explainable,
transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,
Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with
fully-connected (FC) layers as a classifier. It is worth noting that many deep
learning models classify images without FC layers, e.g., few-shot learning
image classification, contrastive learning image classification, and image
retrieval tasks. In this work, a post-hoc interpretation tool named feature
activation map (FAM) is proposed, which can interpret deep learning models
without FC layers as a classifier. In the proposed FAM algorithm, the
channel-wise contribution weights are derived from the similarity scores
between two image embeddings. The activation maps are linearly combined with
the corresponding normalized contribution weights, forming the explanation map
for visualization. The quantitative and qualitative experiments conducted on
ten deep learning models for few-shot image classification, contrastive
learning image classification and image retrieval tasks demonstrate the
effectiveness of the proposed FAM algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weichuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05025">
<title>Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05025</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, research on learning with noisy labels has focused on
devising novel algorithms that can achieve robustness to noisy training labels
while generalizing to clean data. These algorithms often incorporate
sophisticated techniques, such as noise modeling, label correction, and
co-training. In this study, we demonstrate that a simple baseline using
cross-entropy loss, combined with widely used regularization strategies like
learning rate decay, model weights average, and data augmentations, can
outperform state-of-the-art methods. Our findings suggest that employing a
combination of regularization strategies can be more effective than intricate
algorithms in tackling the challenges of learning with noisy labels. While some
of these regularization strategies have been utilized in previous noisy label
learning research, their full potential has not been thoroughly explored. Our
results encourage a reevaluation of benchmarks for learning with noisy labels
and prompt reconsideration of the role of specialized learning algorithms
designed for training with noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hui Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaxi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dadong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05033">
<title>Towards Anytime Optical Flow Estimation with Event Cameras. (arXiv:2307.05033v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05033</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras are capable of responding to log-brightness changes in
microseconds. Its characteristic of producing responses only to the changing
region is particularly suitable for optical flow estimation. In contrast to the
super low-latency response speed of event cameras, existing datasets collected
via event cameras, however, only provide limited frame rate optical flow ground
truth, (e.g., at 10Hz), greatly restricting the potential of event-driven
optical flow. To address this challenge, we put forward a high-frame-rate,
low-latency event representation Unified Voxel Grid, sequentially fed into the
network bin by bin. We then propose EVA-Flow, an EVent-based Anytime Flow
estimation network to produce high-frame-rate event optical flow with only
low-frame-rate optical flow ground truth for supervision. The key component of
our EVA-Flow is the stacked Spatiotemporal Motion Refinement (SMR) module,
which predicts temporally-dense optical flow and enhances the accuracy via
spatial-temporal motion refinement. The time-dense feature warping utilized in
the SMR module provides implicit supervision for the intermediate optical flow.
Additionally, we introduce the Rectified Flow Warp Loss (RFWL) for the
unsupervised evaluation of intermediate optical flow in the absence of ground
truth. This is, to the best of our knowledge, the first work focusing on
anytime optical flow estimation via event cameras. A comprehensive variety of
experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow
achieves competitive performance, super-low-latency (5ms), fastest inference
(9.2ms), time-dense motion estimation (200Hz), and strong generalization. Our
code will be available at https://github.com/Yaozhuwa/EVA-Flow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yaozu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05038">
<title>Disentangled Contrastive Image Translation for Nighttime Surveillance. (arXiv:2307.05038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05038</link>
<description rdf:parseType="Literal">&lt;p&gt;Nighttime surveillance suffers from degradation due to poor illumination and
arduous human annotations. It is challengable and remains a security risk at
night. Existing methods rely on multi-spectral images to perceive objects in
the dark, which are troubled by low resolution and color absence. We argue that
the ultimate solution for nighttime surveillance is night-to-day translation,
or Night2Day, which aims to translate a surveillance scene from nighttime to
the daytime while maintaining semantic consistency. To achieve this, this paper
presents a Disentangled Contrastive (DiCo) learning method. Specifically, to
address the poor and complex illumination in the nighttime scenes, we propose a
learnable physical prior, i.e., the color invariant, which provides a stable
perception of a highly dynamic night environment and can be incorporated into
the learning pipeline of neural networks. Targeting the surveillance scenes, we
develop a disentangled representation, which is an auxiliary pretext task that
separates surveillance scenes into the foreground and background with
contrastive learning. Such a strategy can extract the semantics without
supervision and boost our model to achieve instance-aware translation. Finally,
we incorporate all the modules above into generative adversarial networks and
achieve high-fidelity translation. This paper also contributes a new
surveillance dataset called NightSuR. It includes six scenes to support the
study on nighttime surveillance. This dataset collects nighttime images with
different properties of nighttime environments, such as flare and extreme
darkness. Extensive experiments demonstrate that our method outperforms
existing works significantly. The dataset and source code will be released on
GitHub soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_G/0/1/0/all/0/1&quot;&gt;Guanzhou Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05075">
<title>Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images. (arXiv:2307.05075v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05075</link>
<description rdf:parseType="Literal">&lt;p&gt;Removing multiple degradations, such as haze, rain, and blur, from real-world
images poses a challenging and illposed problem. Recently, unified models that
can handle different degradations have been proposed and yield promising
results. However, these approaches focus on synthetic images and experience a
significant performance drop when applied to realworld images. In this paper,
we introduce Uni-Removal, a twostage semi-supervised framework for addressing
the removal of multiple degradations in real-world images using a unified model
and parameters. In the knowledge transfer stage, Uni-Removal leverages a
supervised multi-teacher and student architecture in the knowledge transfer
stage to facilitate learning from pretrained teacher networks specialized in
different degradation types. A multi-grained contrastive loss is introduced to
enhance learning from feature and image spaces. In the domain adaptation stage,
unsupervised fine-tuning is performed by incorporating an adversarial
discriminator on real-world images. The integration of an extended
multi-grained contrastive loss and generative adversarial loss enables the
adaptation of the student network from synthetic to real-world domains.
Extensive experiments on real-world degraded datasets demonstrate the
effectiveness of our proposed method. We compare our Uni-Removal framework with
state-of-the-art supervised and unsupervised methods, showcasing its promising
results in real-world image dehazing, deraining, and deblurring simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Danfeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuanqiang Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05080">
<title>Estimating label quality and errors in semantic segmentation data via any model. (arXiv:2307.05080v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05080</link>
<description rdf:parseType="Literal">&lt;p&gt;The labor-intensive annotation process of semantic segmentation datasets is
often prone to errors, since humans struggle to label every pixel correctly. We
study algorithms to automatically detect such annotation errors, in particular
methods to score label quality, such that the images with the lowest scores are
least likely to be correctly labeled. This helps prioritize what data to review
in order to ensure a high-quality training/evaluation dataset, which is
critical in sensitive applications such as medical imaging and autonomous
vehicles. Widely applicable, our label quality scores rely on probabilistic
predictions from a trained segmentation model -- any model architecture and
training procedure can be utilized. Here we study 7 different label quality
scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation
model to detect annotation errors in a version of the SYNTHIA dataset.
Precision-recall evaluations reveal a score -- the soft-minimum of the
model-estimated likelihoods of each pixel&apos;s annotated class -- that is
particularly effective to identify images that are mislabeled, across multiple
types of annotation error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lad_V/0/1/0/all/0/1&quot;&gt;Vedang Lad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1&quot;&gt;Jonas Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05087">
<title>SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View Representation. (arXiv:2307.05087v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05087</link>
<description rdf:parseType="Literal">&lt;p&gt;SAR images are highly sensitive to observation configurations, and they
exhibit significant variations across different viewing angles, making it
challenging to represent and learn their anisotropic features. As a result,
deep learning methods often generalize poorly across different view angles.
Inspired by the concept of neural radiance fields (NeRF), this study combines
SAR imaging mechanisms with neural networks to propose a novel NeRF model for
SAR image generation. Following the mapping and projection pinciples, a set of
SAR images is modeled implicitly as a function of attenuation coefficients and
scattering intensities in the 3D imaging space through a differentiable
rendering equation. SAR-NeRF is then constructed to learn the distribution of
attenuation coefficients and scattering intensities of voxels, where the
vectorized form of 3D voxel SAR rendering equation and the sampling
relationship between the 3D space voxels and the 2D view ray grids are
analytically derived. Through quantitative experiments on various datasets, we
thoroughly assess the multi-view representation and generalization capabilities
of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can
significantly improve SAR target classification performance under few-shot
learning setup, where a 10-type classification accuracy of 91.6\% can be
achieved by using only 12 images per class.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhengxin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Feng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiangtao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1&quot;&gt;Feng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Ya-Qiu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05092">
<title>Offline and Online Optical Flow Enhancement for Deep Video Compression. (arXiv:2307.05092v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05092</link>
<description rdf:parseType="Literal">&lt;p&gt;Video compression relies heavily on exploiting the temporal redundancy
between video frames, which is usually achieved by estimating and using the
motion information. The motion information is represented as optical flows in
most of the existing deep video compression networks. Indeed, these networks
often adopt pre-trained optical flow estimation networks for motion estimation.
The optical flows, however, may be less suitable for video compression due to
the following two factors. First, the optical flow estimation networks were
trained to perform inter-frame prediction as accurately as possible, but the
optical flows themselves may cost too many bits to encode. Second, the optical
flow estimation networks were trained on synthetic data, and may not generalize
well enough to real-world videos. We address the twofold limitations by
enhancing the optical flows in two stages: offline and online. In the offline
stage, we fine-tune a trained optical flow estimation network with the motion
information provided by a traditional (non-deep) video compression scheme, e.g.
H.266/VVC, as we believe the motion information of H.266/VVC achieves a better
rate-distortion trade-off. In the online stage, we further optimize the latent
features of the optical flows with a gradient descent-based algorithm for the
video to be compressed, so as to enhance the adaptivity of the optical flows.
We conduct experiments on a state-of-the-art deep video compression scheme,
DCVC. Experimental results demonstrate that the proposed offline and online
enhancement together achieves on average 12.8% bitrate saving on the tested
videos, without increasing the model or computational complexity of the decoder
side.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chuanbo Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_X/0/1/0/all/0/1&quot;&gt;Xihua Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haotian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05128">
<title>One-Shot Learning for Periocular Recognition: Exploring the Effect of Domain Adaptation and Data Bias on Deep Representations. (arXiv:2307.05128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05128</link>
<description rdf:parseType="Literal">&lt;p&gt;One weakness of machine-learning algorithms is the need to train the models
for a new task. This presents a specific challenge for biometric recognition
due to the dynamic nature of databases and, in some instances, the reliance on
subject collaboration for data collection. In this paper, we investigate the
behavior of deep representations in widely used CNN models under extreme data
scarcity for One-Shot periocular recognition, a biometric recognition task. We
analyze the outputs of CNN layers as identity-representing feature vectors. We
examine the impact of Domain Adaptation on the network layers&apos; output for
unseen data and evaluate the method&apos;s robustness concerning data normalization
and generalization of the best-performing layer. We improved state-of-the-art
results that made use of networks trained with biometric datasets with millions
of images and fine-tuned for the target periocular dataset by utilizing
out-of-the-box CNNs trained for the ImageNet Recognition Challenge and standard
computer vision algorithms. For example, for the Cross-Eyed dataset, we could
reduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in the
Close-World and Open-World protocols, respectively, for the periocular case. We
also demonstrate that traditional algorithms like SIFT can outperform CNNs in
situations with limited data or scenarios where the network has not been
trained with the test classes like the Open-World mode. SIFT alone was able to
reduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) for
Cross-Eyed in the Close-World and Open-World protocols, respectively, and a
reduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for the
Open-World and single biometric case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1&quot;&gt;Kevin Hernandez-Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05129">
<title>DFR: Depth from Rotation by Uncalibrated Image Rectification with Latitudinal Motion Assumption. (arXiv:2307.05129v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05129</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the increasing prevalence of rotating-style capture (e.g.,
surveillance cameras), conventional stereo rectification techniques frequently
fail due to the rotation-dominant motion and small baseline between views. In
this paper, we tackle the challenge of performing stereo rectification for
uncalibrated rotating cameras. To that end, we propose Depth-from-Rotation
(DfR), a novel image rectification solution that analytically rectifies two
images with two-point correspondences and serves for further depth estimation.
Specifically, we model the motion of a rotating camera as the camera rotates on
a sphere with fixed latitude. The camera&apos;s optical axis lies perpendicular to
the sphere&apos;s surface. We call this latitudinal motion assumption. Then we
derive a 2-point analytical solver from directly computing the rectified
transformations on the two images. We also present a self-adaptive strategy to
reduce the geometric distortion after rectification. Extensive synthetic and
real data experiments demonstrate that the proposed method outperforms existing
works in effectiveness and efficiency by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongcong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yifei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1&quot;&gt;Ming Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huiqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_Y/0/1/0/all/0/1&quot;&gt;Yizhen Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05134">
<title>TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1&quot;&gt;Paul Grimal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Le Borgne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1&quot;&gt;Olivier Ferret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1&quot;&gt;Julien Tourille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05136">
<title>Unveiling the invisible: Enhanced detection and analysis deteriorated areas in solar PV modules using unsupervised sensing algorithms and 3D augmented reality. (arXiv:2307.05136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05136</link>
<description rdf:parseType="Literal">&lt;p&gt;Solar Photovoltaic (PV) is increasingly being used to address the global
concern of energy security. However, hot spot and snail trails in PV modules
caused mostly by crakes reduce their efficiency and power capacity. This
article presents a groundbreaking methodology for automatically identifying and
analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)
modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality
(AR) visualization. By transforming the traditional methods of diagnosis and
repair, our approach not only enhances efficiency but also substantially cuts
down the cost of PV system maintenance. Validated through computer simulations
and real-world image datasets, the proposed framework accurately identifies
dirty regions, emphasizing the critical role of regular maintenance in
optimizing the power capacity of solar PV modules. Our immediate objective is
to leverage drone technology for real-time, automatic solar panel detection,
significantly boosting the efficacy of PV maintenance. The proposed methodology
could revolutionize solar PV maintenance, enabling swift, precise anomaly
detection without human intervention. This could result in significant cost
savings, heightened energy production, and improved overall performance of
solar PV systems. Moreover, the novel combination of unsupervised sensing
algorithms with 3D AR visualization heralds new opportunities for further
research and development in solar PV maintenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oulefki_A/0/1/0/all/0/1&quot;&gt;Adel Oulefki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trongtiraku_T/0/1/0/all/0/1&quot;&gt;Thaweesak Trongtiraku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1&quot;&gt;Kahina Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1&quot;&gt;Sos Agaian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samir/0/1/0/all/0/1&quot;&gt;Samir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benbelkacem/0/1/0/all/0/1&quot;&gt;Benbelkacem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerroudji_M/0/1/0/all/0/1&quot;&gt;Mohamed Amine Guerroudji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemmouri_M/0/1/0/all/0/1&quot;&gt;Mohamed Zemmouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferhat_S/0/1/0/all/0/1&quot;&gt;Sahla Ferhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenati_N/0/1/0/all/0/1&quot;&gt;Nadia Zenati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atalla_S/0/1/0/all/0/1&quot;&gt;Shadi Atalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1&quot;&gt;Wathiq Mansoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05151">
<title>ExFaceGAN: Exploring Identity Directions in GAN&apos;s Learned Latent Space for Synthetic Identity Generation. (arXiv:2307.05151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05151</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have recently presented impressive results in
generating realistic face images of random synthetic identities. To generate
multiple samples of a certain synthetic identity, several previous works
proposed to disentangle the latent space of GANs by incorporating additional
supervision or regularization, enabling the manipulation of certain attributes,
e.g. identity, hairstyle, pose, or expression. Most of these works require
designing special loss functions and training dedicated network architectures.
Others proposed to disentangle specific factors in unconditional pretrained
GANs latent spaces to control their output, which also requires supervision by
attribute classifiers. Moreover, these attributes are entangled in GAN&apos;s latent
space, making it difficult to manipulate them without affecting the identity
information. We propose in this work a framework, ExFaceGAN, to disentangle
identity information in state-of-the-art pretrained GANs latent spaces,
enabling the generation of multiple samples of any synthetic identity. The
variations in our generated images are not limited to specific attributes as
ExFaceGAN explicitly aims at disentangling identity information, while other
visual attributes are randomly drawn from a learned GAN latent space. As an
example of the practical benefit of our ExFaceGAN, we empirically prove that
data generated by ExFaceGAN can be successfully used to train face recognition
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1&quot;&gt;Fadi Boutros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemt_M/0/1/0/all/0/1&quot;&gt;Marcel Klemt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meiling Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1&quot;&gt;Arjan Kuijper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05158">
<title>A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings. (arXiv:2307.05158v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05158</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting where a person is looking is a complex task, requiring to
understand not only the person&apos;s gaze and scene content, but also the 3D scene
structure and the person&apos;s situation (are they manipulating? interacting or
observing others? attentive?) to detect obstructions in the line of sight or
apply attention priors that humans typically have when observing others. In
this paper, we hypothesize that identifying and leveraging such priors can be
better achieved through the exploitation of explicitly derived multimodal cues
such as depth and pose. We thus propose a modular multimodal architecture
allowing to combine these cues using an attention mechanism. The architecture
can naturally be exploited in privacy-sensitive situations such as surveillance
and health, where personally identifiable information cannot be released. We
perform extensive experiments on the GazeFollow and VideoAttentionTarget public
datasets, obtaining state-of-the-art performance and demonstrating very
competitive results in the privacy setting case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Anshul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tafasca_S/0/1/0/all/0/1&quot;&gt;Samy Tafasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odobez_J/0/1/0/all/0/1&quot;&gt;Jean-Marc Odobez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05180">
<title>ResMatch: Residual Attention Learning for Local Feature Matching. (arXiv:2307.05180v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05180</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention-based graph neural networks have made great progress in feature
matching learning. However, insight of how attention mechanism works for
feature matching is lacked in the literature. In this paper, we rethink cross-
and self-attention from the viewpoint of traditional feature matching and
filtering. In order to facilitate the learning of matching and filtering, we
inject the similarity of descriptors and relative positions into cross- and
self-attention score, respectively. In this way, the attention can focus on
learning residual matching and filtering functions with reference to the basic
functions of measuring visual and spatial correlation. Moreover, we mine intra-
and inter-neighbors according to the similarity of descriptors and relative
positions. Then sparse attention for each point can be performed only within
its neighborhoods to acquire higher computation efficiency. Feature matching
networks equipped with our full and sparse residual attention learning
strategies are termed ResMatch and sResMatch respectively. Extensive
experiments, including feature matching, pose estimation and visual
localization, confirm the superiority of our networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yuxin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiayi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05182">
<title>Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05182</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical students and junior surgeons often rely on senior surgeons and
specialists to answer their questions when learning surgery. However, experts
are often busy with clinical and academic work, and have little time to give
guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question
Answering (VQA) systems can only provide simple answers without the location of
the answers. In addition, vision-language (ViL) embedding is still a less
explored research in these kinds of tasks. Therefore, a surgical Visual
Question Localized-Answering (VQLA) system would be helpful for medical
students and junior surgeons to learn and understand from recorded surgical
videos. We propose an end-to-end Transformer with Co-Attention gaTed
Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not
require feature extraction through detection models. The CAT-ViL embedding
module is designed to fuse heterogeneous features from visual and textual
sources. The fused embedding will feed a standard Data-Efficient Image
Transformer (DeiT) module, before the parallel classifier and detector for
joint prediction. We conduct the experimental validation on public surgical
videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results
highlight the superior performance and robustness of our proposed model
compared to the state-of-the-art approaches. Ablation studies further prove the
outstanding performance of all the proposed components. The proposed method
provides a promising solution for surgical scene understanding, and opens up a
primary step in the Artificial Intelligence (AI)-based VQLA system for surgical
training. Our code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Mobarakol Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05201">
<title>The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework. (arXiv:2307.05201v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05201</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of label-efficient learning on video data, the distillation
method and the structural design of the teacher-student architecture have a
significant impact on knowledge distillation. However, the relationship between
these factors has been overlooked in previous research. To address this gap, we
propose a new weakly supervised learning framework for knowledge distillation
in video classification that is designed to improve the efficiency and accuracy
of the student model. Our approach leverages the concept of substage-based
learning to distill knowledge based on the combination of student substages and
the correlation of corresponding substages. We also employ the progressive
cascade training method to address the accuracy loss caused by the large
capacity gap between the teacher and the student. Additionally, we propose a
pseudo-label optimization strategy to improve the initial data label. To
optimize the loss functions of different distillation substages during the
training process, we introduce a new loss method based on feature distribution.
We conduct extensive experiments on both real and simulated data sets,
demonstrating that our proposed approach outperforms existing distillation
methods in terms of knowledge distillation for video classification tasks. Our
proposed substage-based distillation approach has the potential to inform
future research on label-efficient learning for video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zheng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05222">
<title>Generative Pretraining in Multimodality. (arXiv:2307.05222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05222</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Emu, a Transformer-based multimodal foundation model, which can
seamlessly generate images and texts in multimodal context. This omnivore model
can take in any single-modality or multimodal data input indiscriminately
(e.g., interleaved image, text and video) through a one-model-for-all
autoregressive training process. First, visual signals are encoded into
embeddings, and together with text tokens form an interleaved input sequence.
Emu is then end-to-end trained with a unified objective of classifying the next
text token or regressing the next visual embedding in the multimodal sequence.
This versatile multimodality empowers the exploration of diverse pretraining
data sources at scale, such as videos with interleaved frames and text,
webpages with interleaved images and text, as well as web-scale image-text
pairs and video-text pairs. Emu can serve as a generalist multimodal interface
for both image-to-text and text-to-image tasks, and supports in-context image
and text generation. Across a broad range of zero-shot/few-shot tasks including
image captioning, visual question answering, video question answering and
text-to-image generation, Emu demonstrates superb performance compared to
state-of-the-art large multimodal models. Extended capabilities such as
multimodal assistants via instruction tuning are also demonstrated with
impressive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Quan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qiying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yufeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaosong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yueze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongcheng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinlong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05241">
<title>Does pre-training on brain-related tasks results in better deep-learning-based brain age biomarkers?. (arXiv:2307.05241v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05241</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain age prediction using neuroimaging data has shown great potential as an
indicator of overall brain health and successful aging, as well as a disease
biomarker. Deep learning models have been established as reliable and efficient
brain age estimators, being trained to predict the chronological age of healthy
subjects. In this paper, we investigate the impact of a pre-training step on
deep learning models for brain age prediction. More precisely, instead of the
common approach of pre-training on natural imaging classification, we propose
pre-training the models on brain-related tasks, which led to state-of-the-art
results in our experiments on ADNI data. Furthermore, we validate the resulting
brain age biomarker on images of patients with mild cognitive impairment and
Alzheimer&apos;s disease. Interestingly, our results indicate that better-performing
deep learning models in terms of brain age prediction on healthy patients do
not result in more reliable biomarkers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacheco_B/0/1/0/all/0/1&quot;&gt;Bruno Machado Pacheco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_V/0/1/0/all/0/1&quot;&gt;Victor Hugo Rocha de Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antunes_A/0/1/0/all/0/1&quot;&gt;Augusto Braga Fernandes Antunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedro_S/0/1/0/all/0/1&quot;&gt;Saulo Domingos de Souza Pedro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1&quot;&gt;Danilo Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05249">
<title>DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis. (arXiv:2307.05249v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05249</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-center positron emission tomography (PET) image synthesis aims at
recovering low-dose PET images from multiple different centers. The
generalizability of existing methods can still be suboptimal for a multi-center
study due to domain shifts, which result from non-identical data distribution
among centers with different imaging systems/protocols. While some approaches
address domain shifts by training specialized models for each center, they are
parameter inefficient and do not well exploit the shared knowledge across
centers. To address this, we develop a generalist model that shares
architecture and parameters across centers to utilize the shared knowledge.
However, the generalist model can suffer from the center interference issue,
\textit{i.e.} the gradient directions of different centers can be inconsistent
or even opposite owing to the non-identical data distribution. To mitigate such
interference, we introduce a novel dynamic routing strategy with cross-layer
connections that routes data from different centers to different experts.
Experiments show that our generalist model with dynamic routing (DRMC) exhibits
excellent generalizability across centers. Code and data are available at:
https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_B/0/1/0/all/0/1&quot;&gt;Bingzheng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yubo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05254">
<title>OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification. (arXiv:2307.05254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05254</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning (AL) is an effective approach to select the most informative
samples to label so as to reduce the annotation cost. Existing AL methods
typically work under the closed-set assumption, i.e., all classes existing in
the unlabeled sample pool need to be classified by the target model. However,
in some practical clinical tasks, the unlabeled pool may contain not only the
target classes that need to be fine-grainedly classified, but also non-target
classes that are irrelevant to the clinical tasks. Existing AL methods cannot
work well in this scenario because they tend to select a large number of
non-target samples. In this paper, we formulate this scenario as an open-set AL
problem and propose an efficient framework, OpenAL, to address the challenge of
querying samples from an unlabeled pool with both target class and non-target
class samples. Experiments on fine-grained classification of pathology images
show that OpenAL can significantly improve the query quality of target class
samples and achieve higher performance than current state-of-the-art AL
methods. Code is available at https://github.com/miccaiif/OpenAL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Linhao Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yingfan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Manning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhijian Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05256">
<title>Towards exploring adversarial learning for anomaly detection in complex driving scenes. (arXiv:2307.05256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05256</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the many Autonomous Systems (ASs), such as autonomous driving cars,
performs various safety-critical functions. Many of these autonomous systems
take advantage of Artificial Intelligence (AI) techniques to perceive their
environment. But these perceiving components could not be formally verified,
since, the accuracy of such AI-based components has a high dependency on the
quality of training data. So Machine learning (ML) based anomaly detection, a
technique to identify data that does not belong to the training data could be
used as a safety measuring indicator during the development and operational
time of such AI-based components. Adversarial learning, a sub-field of machine
learning has proven its ability to detect anomalies in images and videos with
impressive results on simple data sets. Therefore, in this work, we investigate
and provide insight into the performance of such techniques on a highly complex
driving scenes dataset called Berkeley DeepDrive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habib_N/0/1/0/all/0/1&quot;&gt;Nour Habib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yunsu Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buragohain_A/0/1/0/all/0/1&quot;&gt;Abhishek Buragohain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rausch_A/0/1/0/all/0/1&quot;&gt;Andreas Rausch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05270">
<title>APRF: Anti-Aliasing Projection Representation Field for Inverse Problem in Imaging. (arXiv:2307.05270v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05270</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse-view Computed Tomography (SVCT) reconstruction is an ill-posed inverse
problem in imaging that aims to acquire high-quality CT images based on
sparsely-sampled measurements. Recent works use Implicit Neural Representations
(INRs) to build the coordinate-based mapping between sinograms and CT images.
However, these methods have not considered the correlation between adjacent
projection views, resulting in aliasing artifacts on SV sinograms. To address
this issue, we propose a self-supervised SVCT reconstruction method --
Anti-Aliasing Projection Representation Field (APRF), which can build the
continuous representation between adjacent projection views via the spatial
constraints. Specifically, APRF only needs SV sinograms for training, which
first employs a line-segment sampling module to estimate the distribution of
projection views in a local region, and then synthesizes the corresponding
sinogram values using center-based line integral module. After training APRF on
a single SV sinogram itself, it can synthesize the corresponding dense-view
(DV) sinogram with consistent continuity. High-quality CT images can be
obtained by applying re-projection techniques on the predicted DV sinograms.
Extensive experiments on CT images demonstrate that APRF outperforms
state-of-the-art methods, yielding more accurate details and fewer artifacts.
Our code will be publicly available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohua Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05276">
<title>Unbiased Scene Graph Generation via Two-stage Causal Modeling. (arXiv:2307.05276v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05276</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive performance of recent unbiased Scene Graph Generation
(SGG) methods, the current debiasing literature mainly focuses on the
long-tailed distribution problem, whereas it overlooks another source of bias,
i.e., semantic confusion, which makes the SGG model prone to yield false
predictions for similar relationships. In this paper, we explore a debiasing
procedure for the SGG task leveraging causal inference. Our central insight is
that the Sparse Mechanism Shift (SMS) in causality allows independent
intervention on multiple biases, thereby potentially preserving head category
performance while pursuing the prediction of high-informative tail
relationships. However, the noisy datasets lead to unobserved confounders for
the SGG task, and thus the constructed causal models are always
causal-insufficient to benefit from SMS. To remedy this, we propose Two-stage
Causal Modeling (TsCM) for the SGG task, which takes the long-tailed
distribution and semantic confusion as confounders to the Structural Causal
Model (SCM) and then decouples the causal intervention into two stages. The
first stage is causal representation learning, where we use a novel Population
Loss (P-Loss) to intervene in the semantic confusion confounder. The second
stage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the
long-tailed distribution confounder to complete causal calibration learning.
These two stages are model agnostic and thus can be used in any SGG model that
seeks unbiased predictions. Comprehensive experiments conducted on the popular
SGG backbones and benchmarks show that our TsCM can achieve state-of-the-art
performance in terms of mean recall rate. Furthermore, TsCM can maintain a
higher recall rate than other debiasing methods, which indicates that our
method can achieve a better tradeoff between head and tail relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuzhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_S/0/1/0/all/0/1&quot;&gt;Shuaifeng Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1&quot;&gt;Janne Heikkil&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05288">
<title>Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle Safety. (arXiv:2307.05288v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05288</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles require accurate and reliable short-term trajectory
predictions for safe and efficient driving. While most commercial automated
vehicles currently use state machine-based algorithms for trajectory
forecasting, recent efforts have focused on end-to-end data-driven systems.
Often, the design of these models is limited by the availability of datasets,
which are typically restricted to generic scenarios. To address this
limitation, we have developed a synthetic dataset for short-term trajectory
prediction tasks using the CARLA simulator. This dataset is extensive and
incorporates what is considered complex scenarios - pedestrians crossing the
road, vehicles overtaking - and comprises 6000 perspective view images with
corresponding IMU and odometry information for each frame. Furthermore, an
end-to-end short-term trajectory prediction model using convolutional neural
networks (CNN) and long short-term memory (LSTM) networks has also been
developed. This model can handle corner cases, such as slowing down near zebra
crossings and stopping when pedestrians cross the road, without the need for
explicit encoding of the surrounding environment. In an effort to accelerate
this research and assist others, we are releasing our dataset and model to the
research community. Our datasets are publicly available on
https://github.com/navigatinguncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Sushil Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahiaoui_L/0/1/0/all/0/1&quot;&gt;Lucie Yahiaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arindam Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halton_M/0/1/0/all/0/1&quot;&gt;Mark Halton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05314">
<title>Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering. (arXiv:2307.05314v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05314</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical visual question answering (VQA) is a challenging task that requires
answering clinical questions of a given medical image, by taking consider of
both visual and language information. However, due to the small scale of
training data for medical VQA, pre-training fine-tuning paradigms have been a
commonly used solution to improve model generalization performance. In this
paper, we present a novel self-supervised approach that learns unimodal and
multimodal feature representations of input images and text using medical image
caption datasets, by leveraging both unimodal and multimodal contrastive
losses, along with masked language modeling and image text matching as
pretraining objectives. The pre-trained model is then transferred to downstream
medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA)
performance on three publicly available medical VQA datasets with significant
accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we
conduct a comprehensive analysis to validate the effectiveness of different
components of the approach and study different pre-training settings. Our codes
and models are available at https://github.com/pengfeiliHEU/MUMC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jinlong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zixu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shenjun Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05317">
<title>Automatic Generation of Semantic Parts for Face Image Synthesis. (arXiv:2307.05317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05317</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image synthesis (SIS) refers to the problem of generating realistic
imagery given a semantic segmentation mask that defines the spatial layout of
object classes. Most of the approaches in the literature, other than the
quality of the generated images, put effort in finding solutions to increase
the generation diversity in terms of style i.e. texture. However, they all
neglect a different feature, which is the possibility of manipulating the
layout provided by the mask. Currently, the only way to do so is manually by
means of graphical users interfaces. In this paper, we describe a network
architecture to address the problem of automatically manipulating or generating
the shape of object classes in semantic segmentation masks, with specific focus
on human faces. Our proposed model allows embedding the mask class-wise into a
latent space where each class embedding can be independently edited. Then, a
bi-directional LSTM block and a convolutional decoder output a new, locally
manipulated mask. We report quantitative and qualitative results on the
CelebMask-HQ dataset, which show our model can both faithfully reconstruct and
modify a segmentation mask at the class level. Also, we show our model can be
put before a SIS generator, opening the way to a fully automatic generation
control of both shape and texture. Code available at
https://github.com/TFonta/Semantic-VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fontanini_T/0/1/0/all/0/1&quot;&gt;Tomaso Fontanini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1&quot;&gt;Claudio Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1&quot;&gt;Massimo Bertozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1&quot;&gt;Andrea Prati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05322">
<title>Class Instance Balanced Learning for Long-Tailed Classification. (arXiv:2307.05322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05322</link>
<description rdf:parseType="Literal">&lt;p&gt;The long-tailed image classification task remains important in the
development of deep neural networks as it explicitly deals with large
imbalances in the class frequencies of the training data. While uncommon in
engineered datasets, this imbalance is almost always present in real-world
data. Previous approaches have shown that combining cross-entropy and
contrastive learning can improve performance on the long-tailed task, but they
do not explore the tradeoff between head and tail classes. We propose a novel
class instance balanced loss (CIBL), which reweights the relative contributions
of a cross-entropy and a contrastive loss as a function of the frequency of
class instances in the training batch. This balancing favours the contrastive
loss for more common classes, leading to a learned classifier with a more
balanced performance across all class frequencies. Furthermore, increasing the
relative weight on the contrastive head shifts performance from common (head)
to rare (tail) classes, allowing the user to skew the performance towards these
classes if desired. We also show that changing the linear classifier head with
a cosine classifier yields a network that can be trained to similar performance
in substantially fewer epochs. We obtain competitive results on both
CIFAR-100-LT and ImageNet-LT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavoie_M/0/1/0/all/0/1&quot;&gt;Marc-Antoine Lavoie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1&quot;&gt;Steven Waslander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05325">
<title>Self-supervised adversarial masking for 3D point cloud representation learning. (arXiv:2307.05325v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05325</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised methods have been proven effective for learning deep
representations of 3D point cloud data. Although recent methods in this domain
often rely on random masking of inputs, the results of this approach can be
improved. We introduce PointCAM, a novel adversarial method for learning a
masking function for point clouds. Our model utilizes a self-distillation
framework with an online tokenizer for 3D point clouds. Compared to previous
techniques that optimize patch-level and object-level objectives, we postulate
applying an auxiliary network that learns how to select masks instead of
choosing them randomly. Our results show that the learned masking function
achieves state-of-the-art or competitive performance on various downstream
tasks. The source code is available at https://github.com/szacho/pointcam.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szachniewicz_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Szachniewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozlowski_W/0/1/0/all/0/1&quot;&gt;Wojciech Koz&amp;#x142;owski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Stypu&amp;#x142;kowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1&quot;&gt;Maciej Zi&amp;#x119;ba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05350">
<title>Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05350</link>
<description rdf:parseType="Literal">&lt;p&gt;The current approach to ML model design is either to choose a flexible
Blackbox model and explain it post hoc or to start with an interpretable model.
Blackbox models are flexible but difficult to explain, whereas interpretable
models are designed to be explainable. However, developing interpretable models
necessitates extensive ML knowledge, and the resulting models tend to be less
flexible, offering potentially subpar performance compared to their Blackbox
equivalents. This paper aims to blur the distinction between a post hoc
explanation of a BlackBox and constructing interpretable models. We propose
beginning with a flexible BlackBox model and gradually \emph{carving out} a
mixture of interpretable models and a \emph{residual network}. Our design
identifies a subset of samples and \emph{routes} them through the interpretable
models. The remaining samples are routed through a flexible residual network.
We adopt First Order Logic (FOL) as the interpretable model&apos;s backbone, which
provides basic reasoning on concepts retrieved from the BlackBox model. On the
residual network, we repeat the method until the proportion of data explained
by the residual network falls below a desired threshold. Our approach offers
several advantages. First, the mixture of interpretable and flexible residual
networks results in almost no compromise in performance. Second, the route,
interpret, and repeat approach yields a highly flexible interpretable model.
Our extensive experiment demonstrates the performance of the model on various
datasets. We show that by editing the FOL model, we can fix the shortcut
learned by the original BlackBox model. Finally, our method provides a
framework for a hybrid symbolic-connectionist network that is simple to train
and adaptable to many applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05356">
<title>VisText: A Benchmark for Semantically Rich Chart Captioning. (arXiv:2307.05356v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05356</link>
<description rdf:parseType="Literal">&lt;p&gt;Captions that describe or explain charts help improve recall and
comprehension of the depicted data and provide a more accessible medium for
people with visual disabilities. However, current approaches for automatically
generating such captions struggle to articulate the perceptual or cognitive
features that are the hallmark of charts (e.g., complex trends and patterns).
In response, we introduce VisText: a dataset of 12,441 pairs of charts and
captions that describe the charts&apos; construction, report key statistics, and
identify perceptual and cognitive phenomena. In VisText, a chart is available
as three representations: a rasterized image, a backing data table, and a scene
graph -- a hierarchical representation of a chart&apos;s visual elements akin to a
web page&apos;s Document Object Model (DOM). To evaluate the impact of VisText, we
fine-tune state-of-the-art language models on our chart captioning task and
apply prefix-tuning to produce captions that vary the semantic content they
convey. Our models generate coherent, semantically rich captions and perform on
par with state-of-the-art chart captioning models across machine translation
and text generation metrics. Through qualitative analysis, we identify six
broad categories of errors that our models make that can inform future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Benny J. Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1&quot;&gt;Angie Boggust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1&quot;&gt;Arvind Satyanarayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05372">
<title>Food Recognition and Nutritional Apps. (arXiv:2307.05372v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05372</link>
<description rdf:parseType="Literal">&lt;p&gt;Food recognition and nutritional apps are trending technologies that may
revolutionise the way people with diabetes manage their diet. Such apps can
monitor food intake as a digital diary and even employ artificial intelligence
to assess the diet automatically. Although these apps offer a promising
solution for managing diabetes, they are rarely used by patients. This chapter
aims to provide an in-depth assessment of the current status of apps for food
recognition and nutrition, to identify factors that may inhibit or facilitate
their use, while it is accompanied by an outline of relevant research and
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_L/0/1/0/all/0/1&quot;&gt;Lubnaa Abdur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papathanail_I/0/1/0/all/0/1&quot;&gt;Ioannis Papathanail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brigato_L/0/1/0/all/0/1&quot;&gt;Lorenzo Brigato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spanakis_E/0/1/0/all/0/1&quot;&gt;Elias K. Spanakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mougiakakou_S/0/1/0/all/0/1&quot;&gt;Stavroula Mougiakakou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05377">
<title>The Role of Schwartz Measures in Human Tri-Color Vision. (arXiv:2307.05377v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2307.05377</link>
<description rdf:parseType="Literal">&lt;p&gt;The human tri-color vision process may be characterized as follows:
&lt;/p&gt;
&lt;p&gt;1. A requirement of three scalar quantities to fully define a color (for
example, intensity, hue, and purity), with
&lt;/p&gt;
&lt;p&gt;2. These scalar measures linear in the intensity of the incident light,
allowing in general any specific color to be duplicated by an additive mixture
of light from three standardized (basis) colors,
&lt;/p&gt;
&lt;p&gt;3. The exception being that the spectral colors are unique, in that they
cannot be duplicated by any positive mixture of other colors.
&lt;/p&gt;
&lt;p&gt;These characteristics strongly suggest that human color vision makes use of
Schwartz measures in processing color data. This hypothesis is subject to test.
In this brief paper, the results of this hypothesis are shown to be in good
agreement with measured data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sloan_M/0/1/0/all/0/1&quot;&gt;M. L. Sloan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05387">
<title>TBSS++: A novel computational method for Tract-Based Spatial Statistics. (arXiv:2307.05387v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05387</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-weighted magnetic resonance imaging (dMRI) is widely used to assess
the brain white matter. One of the most common computations in dMRI involves
cross-subject tract-specific analysis, whereby dMRI-derived biomarkers are
compared between cohorts of subjects. The accuracy and reliability of these
studies hinges on the ability to compare precisely the same white matter tracts
across subjects. This is an intricate and error-prone computation. Existing
computational methods such as Tract-Based Spatial Statistics (TBSS) suffer from
a host of shortcomings and limitations that can seriously undermine the
validity of the results. We present a new computational framework that
overcomes the limitations of existing methods via (i) accurate segmentation of
the tracts, and (ii) precise registration of data from different
subjects/scans. The registration is based on fiber orientation distributions.
To further improve the alignment of cross-subject data, we create detailed
atlases of white matter tracts. These atlases serve as an unbiased reference
space where the data from all subjects is registered for comparison. Extensive
evaluations show that, compared with TBSS, our proposed framework offers
significantly higher reproducibility and robustness to data perturbations. Our
method promises a drastic improvement in accuracy and reproducibility of
cross-subject dMRI studies that are routinely used in neuroscience and medical
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1&quot;&gt;Davood Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kebiri_H/0/1/0/all/0/1&quot;&gt;Hamza Kebiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1&quot;&gt;Ali Gholipour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05396">
<title>Handwritten Text Recognition Using Convolutional Neural Network. (arXiv:2307.05396v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05396</link>
<description rdf:parseType="Literal">&lt;p&gt;OCR (Optical Character Recognition) is a technology that offers comprehensive
alphanumeric recognition of handwritten and printed characters at electronic
speed by merely scanning the document. Recently, the understanding of visual
data has been termed Intelligent Character Recognition (ICR). Intelligent
Character Recognition (ICR) is the OCR module that can convert scans of
handwritten or printed characters into ASCII text. ASCII data is the standard
format for data encoding in electronic communication. ASCII assigns standard
numeric values to letters, numeral, symbols, white-spaces and other characters.
In more technical terms, OCR is the process of using an electronic device to
transform 2-Dimensional textual information into machine-encoded text. Anything
that contains text both machine written or handwritten can be scanned either
through a scanner or just simply a picture of the text is enough for the
recognition system to distinguish the text. The goal of this papers is to show
the results of a Convolutional Neural Network model which has been trained on
National Institute of Science and Technology (NIST) dataset containing over a
100,000 images. The network learns from the features extracted from the images
and use it to generate the probability of each class to which the picture
belongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Atman Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_A/0/1/0/all/0/1&quot;&gt;A. Sharath Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+C_K/0/1/0/all/0/1&quot;&gt;Kavyashree C&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05397">
<title>On the Vulnerability of DeepFake Detectors to Attacks Generated by Denoising Diffusion Models. (arXiv:2307.05397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05397</link>
<description rdf:parseType="Literal">&lt;p&gt;The detection of malicious Deepfakes is a constantly evolving problem, that
requires continuous monitoring of detectors, to ensure they are able to detect
image manipulations generated by the latest emerging models. In this paper, we
present a preliminary study that investigates the vulnerability of single-image
Deepfake detectors to attacks created by a representative of the newest
generation of generative methods, i.e. Denoising Diffusion Models (DDMs). Our
experiments are run on FaceForensics++, a commonly used benchmark dataset,
consisting of Deepfakes generated with various techniques for face swapping and
face reenactment. The analysis shows, that reconstructing existing Deepfakes
with only one denoising diffusion step significantly decreases the accuracy of
all tested detectors, without introducing visually perceptible image changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanovska_M/0/1/0/all/0/1&quot;&gt;Marija Ivanovska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1&quot;&gt;Vitomir &amp;#x160;truc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05409">
<title>3D detection of roof sections from a single satellite image and application to LOD2-building reconstruction. (arXiv:2307.05409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05409</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing urban areas in 3D out of satellite raster images has been a
long-standing and challenging goal of both academical and industrial research.
The rare methods today achieving this objective at a Level Of Details $2$ rely
on procedural approaches based on geometry, and need stereo images and/or LIDAR
data as input. We here propose a method for urban 3D reconstruction named
KIBS(\textit{Keypoints Inference By Segmentation}), which comprises two novel
features: i) a full deep learning approach for the 3D detection of the roof
sections, and ii) only one single (non-orthogonal) satellite raster image as
model input. This is achieved in two steps: i) by a Mask R-CNN model performing
a 2D segmentation of the buildings&apos; roof sections, and after blending these
latter segmented pixels within the RGB satellite raster image, ii) by another
identical Mask R-CNN model inferring the heights-to-ground of the roof
sections&apos; corners via panoptic segmentation, unto full 3D reconstruction of the
buildings and city. We demonstrate the potential of the KIBS method by
reconstructing different urban areas in a few minutes, with a Jaccard index for
the 2D segmentation of individual roof sections of $88.55\%$ and $75.21\%$ on
our two data sets resp., and a height&apos;s mean error of such correctly segmented
pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets
resp., hence within the LOD2 precision range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lussange_J/0/1/0/all/0/1&quot;&gt;Johann Lussange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mulin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarabalka_Y/0/1/0/all/0/1&quot;&gt;Yuliya Tarabalka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafarge_F/0/1/0/all/0/1&quot;&gt;Florent Lafarge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05445">
<title>AutoDecoding Latent 3D Diffusion Models. (arXiv:2307.05445v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05445</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to the generation of static and articulated 3D
assets that has a 3D autodecoder at its core. The 3D autodecoder framework
embeds properties learned from the target dataset in the latent space, which
can then be decoded into a volumetric representation for rendering
view-consistent appearance and geometry. We then identify the appropriate
intermediate volumetric latent space, and introduce robust normalization and
de-normalization operations to learn a 3D diffusion from 2D images or monocular
videos of rigid or articulated objects. Our approach is flexible enough to use
either existing camera supervision or no camera information at all -- instead
efficiently learning it during training. Our evaluations demonstrate that our
generation results outperform state-of-the-art alternatives on various
benchmark datasets and metrics, including multi-view image datasets of
synthetic objects, real in-the-wild videos of moving people, and a large-scale,
real video dataset of static objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ntavelis_E/0/1/0/all/0/1&quot;&gt;Evangelos Ntavelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1&quot;&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1&quot;&gt;Kyle Olszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1&quot;&gt;Sergey Tulyakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05447">
<title>Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and Denoising. (arXiv:2307.05447v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05447</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the low accuracy of object detection and recognition in many
intelligent surveillance systems at nighttime, the quality of night images is
crucial. Compared with the corresponding daytime image, nighttime image is
characterized as low brightness, low contrast and high noise. In this paper, a
bio-inspired image enhancement algorithm is proposed to convert a low
illuminance image to a brighter and clear one. Different from existing
bio-inspired algorithm, the proposed method doesn&apos;t use any training sequences,
we depend on a novel chain of contrast enhancement and denoising algorithms
without using any forms of recursive functions. Our method can largely improve
the brightness and contrast of night images, besides, suppress noise. Then we
implement on real experiment, and simulation experiment to test our algorithms.
Both results show the advantages of proposed algorithm over contrast pair,
Meylan and Retinex.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xinyi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priyanka_S/0/1/0/all/0/1&quot;&gt;Steffi Agino Priyanka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1&quot;&gt;Hsiao-Jung Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuankai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05462">
<title>Efficient 3D Articulated Human Generation with Layered Surface Volumes. (arXiv:2307.05462v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05462</link>
<description rdf:parseType="Literal">&lt;p&gt;Access to high-quality and diverse 3D articulated digital human assets is
crucial in various applications, ranging from virtual reality to social
platforms. Generative approaches, such as 3D generative adversarial networks
(GANs), are rapidly replacing laborious manual content creation tools. However,
existing 3D GAN frameworks typically rely on scene representations that
leverage either template meshes, which are fast but offer limited quality, or
volumes, which offer high capacity but are slow to render, thereby limiting the
3D fidelity in GAN settings. In this work, we introduce layered surface volumes
(LSVs) as a new 3D object representation for articulated digital humans. LSVs
represent a human body using multiple textured mesh layers around a
conventional template. These layers are rendered using alpha compositing with
fast differentiable rasterization, and they can be interpreted as a volumetric
representation that allocates its capacity to a manifold of finite thickness
around the template. Unlike conventional single-layer templates that struggle
with representing fine off-surface details like hair or accessories, our
surface volumes naturally capture such details. LSVs can be articulated, and
they exhibit exceptional efficiency in GAN settings, where a 2D generator
learns to synthesize the RGBA textures for the individual layers. Trained on
unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality
and view-consistent 3D articulated digital humans without the need for
view-inconsistent 2D upsampling networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1&quot;&gt;Wang Yifan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1&quot;&gt;Alexander W. Bergman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1&quot;&gt;Menglei Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bolei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05463">
<title>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone. (arXiv:2307.05463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05463</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-language pre-training (VLP) has become increasingly important due to
its ability to generalize to various vision and language tasks. However,
existing egocentric VLP frameworks utilize separate video and language encoders
and learn task-specific cross-modal information only during fine-tuning,
limiting the development of a unified system. In this work, we introduce the
second generation of egocentric video-language pre-training (EgoVLPv2), a
significant improvement from the previous generation, by incorporating
cross-modal fusion directly into the video and language backbones. EgoVLPv2
learns strong video-text representation during pre-training and reuses the
cross-modal attention modules to support different downstream tasks in a
flexible and efficient manner, reducing fine-tuning costs. Moreover, our
proposed fusion in the backbone strategy is more lightweight and
compute-efficient than stacking additional fusion-specific layers. Extensive
experiments on a wide range of VL tasks demonstrate the effectiveness of
EgoVLPv2 by achieving consistent state-of-the-art performance over strong
baselines across all downstream. Our project page can be found at
https://shramanpramanick.github.io/EgoVLPv2/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1&quot;&gt;Shraman Pramanick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yale Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sayan Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Qinghong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1&quot;&gt;Hardik Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1&quot;&gt;Rama Chellappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05468">
<title>My3DGen: Building Lightweight Personalized 3D Generative Model. (arXiv:2307.05468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05468</link>
<description rdf:parseType="Literal">&lt;p&gt;Our paper presents My3DGen, a practical system for creating a personalized
and lightweight 3D generative prior using as few as 10 images. My3DGen can
reconstruct multi-view consistent images from an input test image, and generate
novel appearances by interpolating between any two images of the same
individual. While recent studies have demonstrated the effectiveness of
personalized generative priors in producing high-quality 2D portrait
reconstructions and syntheses, to the best of our knowledge, we are the first
to develop a personalized 3D generative prior. Instead of fine-tuning a large
pre-trained generative model with millions of parameters to achieve
personalization, we propose a parameter-efficient approach. Our method involves
utilizing a pre-trained model with fixed weights as a generic prior, while
training a separate personalized prior through low-rank decomposition of the
weights in each convolution and fully connected layer. However,
parameter-efficient few-shot fine-tuning on its own often leads to overfitting.
To address this, we introduce a regularization technique based on symmetry of
human faces. This regularization enforces that novel view renderings of a
training sample, rendered from symmetric poses, exhibit the same identity. By
incorporating this symmetry prior, we enhance the quality of reconstruction and
synthesis, particularly for non-frontal (profile) faces. Our final system
combines low-rank fine-tuning with symmetry regularization and significantly
surpasses the performance of pre-trained models, e.g. EG3D. It introduces only
approximately 0.6 million additional parameters per identity compared to 31
million for full finetuning of the original model. As a result, our system
achieves a 50-fold reduction in model size without sacrificing the quality of
the generated 3D faces. Code will be available at our project page:
https://luchaoqi.github.io/my3dgen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Luchao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaye Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05471">
<title>Scale Alone Does not Improve Mechanistic Interpretability in Vision Models. (arXiv:2307.05471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05471</link>
<description rdf:parseType="Literal">&lt;p&gt;In light of the recent widespread adoption of AI systems, understanding the
internal information processing of neural networks has become increasingly
critical. Most recently, machine vision has seen remarkable progress by scaling
neural networks to unprecedented levels in dataset and model size. We here ask
whether this extraordinary increase in scale also positively impacts the field
of mechanistic interpretability. In other words, has our understanding of the
inner workings of scaled neural networks improved as well? We here use a
psychophysical paradigm to quantify mechanistic interpretability for a diverse
suite of models and find no scaling effect for interpretability - neither for
model nor dataset size. Specifically, none of the nine investigated
state-of-the-art models are easier to interpret than the GoogLeNet model from
almost a decade ago. Latest-generation vision models appear even less
interpretable than older architectures, hinting at a regression rather than
improvement, with modern models sacrificing interpretability for accuracy.
These results highlight the need for models explicitly designed to be
mechanistically interpretable and the need for more helpful interpretability
methods to increase our understanding of networks at an atomic level. We
release a dataset containing more than 120&apos;000 human responses from our
psychophysical evaluation of 767 units across nine models. This dataset is
meant to facilitate research on automated instead of human-based
interpretability evaluations that can ultimately be leveraged to directly
optimize the mechanistic interpretability of models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1&quot;&gt;Thomas Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05473">
<title>Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives. (arXiv:2307.05473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05473</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a set of calibrated images of a scene, we present an approach that
produces a simple, compact, and actionable 3D world representation by means of
3D primitives. While many approaches focus on recovering high-fidelity 3D
scenes, we focus on parsing a scene into mid-level 3D representations made of a
small set of textured primitives. Such representations are interpretable, easy
to manipulate and suited for physics-based simulations. Moreover, unlike
existing primitive decomposition methods that rely on 3D input data, our
approach operates directly on images through differentiable rendering.
Specifically, we model primitives as textured superquadric meshes and optimize
their parameters from scratch with an image rendering loss. We highlight the
importance of modeling transparency for each primitive, which is critical for
optimization and also enables handling varying numbers of primitives. We show
that the resulting textured primitives faithfully reconstruct the input images
and accurately model the visible 3D points, while providing amodal shape
completions of unseen object regions. We compare our approach to the state of
the art on diverse scenes from DTU, and demonstrate its robustness on real-life
captures from BlendedMVS and Nerfstudio. We also showcase how our results can
be used to effortlessly edit a scene or perform physical simulations. Code and
video results are available at https://www.tmonnier.com/DBW .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1&quot;&gt;Tom Monnier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1&quot;&gt;Jake Austin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1&quot;&gt;Angjoo Kanazawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1&quot;&gt;Mathieu Aubry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2005.01344">
<title>Tamed Warping Network for High-Resolution Semantic Video Segmentation. (arXiv:2005.01344v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2005.01344</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent approaches for fast semantic video segmentation have reduced
redundancy by warping feature maps across adjacent frames, greatly speeding up
the inference phase. However, the accuracy drops seriously owing to the errors
incurred by warping. In this paper, we propose a novel framework and design a
simple and effective correction stage after warping. Specifically, we build a
non-key-frame CNN, fusing warped context features with current spatial details.
Based on the feature fusion, our Context Feature Rectification~(CFR) module
learns the model&apos;s difference from a per-frame model to correct the warped
features. Furthermore, our Residual-Guided Attention~(RGA) module utilizes the
residual maps in the compressed domain to help CRF focus on error-prone
regions. Results on Cityscapes show that the accuracy significantly increases
from $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$
FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g.,
``human&apos;&apos; and ``object&apos;&apos;, the improvements are even higher than 18 percentage
points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Songyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Junyi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.08756">
<title>Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.08756</link>
<description rdf:parseType="Literal">&lt;p&gt;It is no secret amongst deep learning researchers that finding the optimal
data augmentation strategy during training can mean the difference between
state-of-the-art performance and a run-of-the-mill result. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve equivalent performance using just 6 trainings with Random
Unidimensional Augmentation. Source code is available at
https://github.com/fastestimator/RUA/tree/v1.0
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1&quot;&gt;Michael Potter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1&quot;&gt;V. Ratna Saripalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1&quot;&gt;Theodore Trafalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04044">
<title>kMaX-DeepLab: k-means Mask Transformer. (arXiv:2207.04044v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04044</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of transformers in vision tasks not only advances network backbone
designs, but also starts a brand-new page to achieve end-to-end image
recognition (e.g., object detection and panoptic segmentation). Originated from
Natural Language Processing (NLP), transformer architectures, consisting of
self-attention and cross-attention, effectively learn long-range interactions
between elements in a sequence. However, we observe that most existing
transformer-based vision models simply borrow the idea from NLP, neglecting the
crucial difference between languages and images, particularly the extremely
large sequence length of spatially flattened pixel features. This subsequently
impedes the learning in cross-attention between pixel features and object
queries. In this paper, we rethink the relationship between pixels and object
queries and propose to reformulate the cross-attention learning as a clustering
process. Inspired by the traditional k-means clustering algorithm, we develop a
k-means Mask Xformer (kMaX-DeepLab) for segmentation tasks, which not only
improves the state-of-the-art, but also enjoys a simple and elegant design. As
a result, our kMaX-DeepLab achieves a new state-of-the-art performance on COCO
val set with 58.0% PQ, Cityscapes val set with 68.4% PQ, 44.0% AP, and 83.5%
mIoU, and ADE20K val set with 50.9% PQ and 55.2% mIoU without test-time
augmentation or external dataset. We hope our work can shed some light on
designing transformers tailored for vision tasks. TensorFlow code and models
are available at https://github.com/google-research/deeplab2 A PyTorch
re-implementation is also available at
https://github.com/bytedance/kmax-deeplab
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qihang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1&quot;&gt;Maxwell Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yukun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang-Chieh Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05379">
<title>Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological
disorder. Its main symptoms include difficulty in (verbal and/or non-verbal)
communication, and rigid/repetitive behavior. These symptoms are often
indistinguishable from a normal (control) individual, due to which this
disorder remains undiagnosed in early childhood leading to delayed treatment.
Since the learning curve is steep during the initial age, an early diagnosis of
autism could allow to take adequate interventions at the right time, which
might positively affect the growth of an autistic child. Further, the
traditional methods of autism diagnosis require multiple visits to a
specialized psychiatrist, however this process can be time-consuming. In this
paper, we present a learning based approach to automate autism diagnosis using
simple and small action video clips of subjects. This task is particularly
challenging because the amount of annotated data available is small, and the
variations among samples from the two categories (ASD and control) are
generally indistinguishable. This is also evident from poor performance of a
binary classifier learned using the cross-entropy loss on top of a baseline
encoder. To address this, we adopt contrastive feature learning in both self
supervised and supervised learning frameworks, and show that these can lead to
a significant increase in the prediction accuracy of a binary classifier on
this task. We further validate this by conducting thorough experimental
analyses under different set-ups on two publicly available datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1&quot;&gt;Asha Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Pankaj Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_Y/0/1/0/all/0/1&quot;&gt;Yashaswi Verma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07336">
<title>An Inter-observer consistent deep adversarial training for visual scanpath prediction. (arXiv:2211.07336v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07336</link>
<description rdf:parseType="Literal">&lt;p&gt;The visual scanpath is a sequence of points through which the human gaze
moves while exploring a scene. It represents the fundamental concepts upon
which visual attention research is based. As a result, the ability to predict
them has emerged as an important task in recent years. In this paper, we
propose an inter-observer consistent adversarial training approach for scanpath
prediction through a lightweight deep neural network. The adversarial method
employs a discriminative neural network as a dynamic loss that is better suited
to model the natural stochastic phenomenon while maintaining consistency
between the distributions related to the subjective nature of scanpaths
traversed by different observers. Through extensive testing, we show the
competitiveness of our approach in regard to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerkouri_M/0/1/0/all/0/1&quot;&gt;Mohamed Amine Kerkouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tliba_M/0/1/0/all/0/1&quot;&gt;Marouane Tliba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1&quot;&gt;Aladine Chetouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruno_A/0/1/0/all/0/1&quot;&gt;Alessandro Bruno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16211">
<title>ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene Novel View Synthesis. (arXiv:2211.16211v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16211</link>
<description rdf:parseType="Literal">&lt;p&gt;We represent the ResNeRF, a novel geometry-guided two-stage framework for
indoor scene novel view synthesis. Be aware of that a good geometry would
greatly boost the performance of novel view synthesis, and to avoid the
geometry ambiguity issue, we propose to characterize the density distribution
of the scene based on a base density estimated from scene geometry and a
residual density parameterized by the geometry. In the first stage, we focus on
geometry reconstruction based on SDF representation, which would lead to a good
geometry surface of the scene and also a sharp density. In the second stage,
the residual density is learned based on the SDF learned in the first stage for
encoding more details about the appearance. In this way, our method can better
learn the density distribution with the geometry prior for high-fidelity novel
view synthesis while preserving the 3D structures. Experiments on large-scale
indoor scenes with many less-observed and textureless areas show that with the
good 3D surface, our method achieves state-of-the-art performance for novel
view synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuting Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiqun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shenghua Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12636">
<title>Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays. (arXiv:2301.12636v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12636</link>
<description rdf:parseType="Literal">&lt;p&gt;Image augmentations are quintessential for effective visual representation
learning across self-supervised learning techniques. While augmentation
strategies for natural imaging have been studied extensively, medical images
are vastly different from their natural counterparts. Thus, it is unknown
whether common augmentation strategies employed in Siamese representation
learning generalize to medical images and to what extent. To address this
challenge, in this study, we systematically assess the effect of various
augmentations on the quality and robustness of the learned representations. We
train and evaluate Siamese Networks for abnormality detection on chest X-Rays
across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate
the efficacy of the learned representations through experiments involving
linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally,
we identify a set of augmentations that yield robust representations that
generalize well to both out-of-distribution data and diseases, while
outperforming supervised baselines using just zero-shot transfer and linear
probes by up to 20%. Our code is available at
https://github.com/StanfordMIMI/siaug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sluijs_R/0/1/0/all/0/1&quot;&gt;Rogier van der Sluijs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhaskhar_N/0/1/0/all/0/1&quot;&gt;Nandita Bhaskhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1&quot;&gt;Daniel Rubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Langlotz_C/0/1/0/all/0/1&quot;&gt;Curtis Langlotz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1&quot;&gt;Akshay Chaudhari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13153">
<title>Directed Diffusion: Direct Control of Object Placement through Attention Guidance. (arXiv:2302.13153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13153</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion
are able to generate an effectively endless variety of images given only a
short text prompt describing the desired image content. In many cases the
images are of very high quality. However, these models often struggle to
compose scenes containing several key objects such as characters in specified
positional relationships. The missing capability to &quot;direct&quot; the placement of
characters and objects both within and across images is crucial in
storytelling, as recognized in the literature on film and animation theory. In
this work, we take a particularly straightforward approach to providing the
needed direction. Drawing on the observation that the cross-attention maps for
prompt words reflect the spatial layout of objects denoted by those words, we
introduce an optimization objective that produces ``activation&apos;&apos; at desired
positions in these cross-attention maps. The resulting approach is a step
toward generalizing the applicability of text-guided diffusion models beyond
single images to collections of related images, as in storybooks. To the best
of our knowledge, our Directed Diffusion method is the first diffusion
technique that provides positional control over multiple objects, while making
use of an existing pre-trained model and maintaining a coherent blend between
the positioned objects and the background. Moreover, it requires only a few
lines to implement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wan-Duo Kurt Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;J.P. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1&quot;&gt;Avisek Lahiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_T/0/1/0/all/0/1&quot;&gt;Thomas Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13668">
<title>Contrastive Video Question Answering via Video Graph Transformer. (arXiv:2302.13668v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13668</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to perform video question answering (VideoQA) in a Contrastive
manner via a Video Graph Transformer model (CoVGT). CoVGT&apos;s uniqueness and
superiority are three-fold: 1) It proposes a dynamic graph transformer module
which encodes video by explicitly capturing the visual objects, their relations
and dynamics, for complex spatio-temporal reasoning. 2) It designs separate
video and text transformers for contrastive learning between the video and text
to perform QA, instead of multi-modal transformer for answer classification.
Fine-grained video-text communication is done by additional cross-modal
interaction modules. 3) It is optimized by the joint fully- and self-supervised
contrastive objectives between the correct and incorrect answers, as well as
the relevant and irrelevant questions respectively. With superior video
encoding and QA solution, we show that CoVGT can achieve much better
performances than previous arts on video reasoning tasks. Its performances even
surpass those models that are pretrained with millions of external data. We
further show that CoVGT can also benefit from cross-modal pretraining, yet with
orders of magnitude smaller data. The results demonstrate the effectiveness and
superiority of CoVGT, and additionally reveal its potential for more
data-efficient pretraining. We hope our success can advance VideoQA beyond
coarse recognition/description towards fine-grained relation reasoning of video
contents. Our code is available at https://github.com/doc-doc/CoVGT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Junbin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1&quot;&gt;Angela Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yicong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14302">
<title>Improving Model Generalization by On-manifold Adversarial Augmentation in the Frequency Domain. (arXiv:2302.14302v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14302</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) may suffer from significantly degenerated
performance when the training and test data are of different underlying
distributions. Despite the importance of model generalization to
out-of-distribution (OOD) data, the accuracy of state-of-the-art (SOTA) models
on OOD data can plummet. Recent work has demonstrated that regular or
off-manifold adversarial examples, as a special case of data augmentation, can
be used to improve OOD generalization. Inspired by this, we theoretically prove
that on-manifold adversarial examples can better benefit OOD generalization.
Nevertheless, it is nontrivial to generate on-manifold adversarial examples
because the real manifold is generally complex. To address this issue, we
proposed a novel method of Augmenting data with Adversarial examples via a
Wavelet module (AdvWavAug), an on-manifold adversarial data augmentation
technique that is simple to implement. In particular, we project a benign image
into a wavelet domain. With the assistance of the sparsity characteristic of
wavelet transformation, we can modify an image on the estimated data manifold.
We conduct adversarial augmentation based on AdvProp training framework.
Extensive experiments on different models and different datasets, including
ImageNet and its distorted versions, demonstrate that our method can improve
model generalization, especially on OOD data. By integrating AdvWavAug into the
training process, we have achieved SOTA results on some recent
transformer-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wenzhao Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shibao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05066">
<title>Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05066</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning is well known for its remarkable performance in
representation learning and various downstream computer vision tasks. Recently,
Positive-pair-Only Contrastive Learning (POCL) has achieved reliable
performance without the need to construct positive-negative training sets. It
reduces memory requirements by lessening the dependency on the batch size. The
POCL method typically uses a single loss function to extract the distortion
invariant representation (DIR) which describes the proximity of positive-pair
representations affected by different distortions. This loss function
implicitly enables the model to filter out or ignore the distortion variant
representation (DVR) affected by different distortions. However, existing POCL
methods do not explicitly enforce the disentanglement and exploitation of the
actually valuable DVR. In addition, these POCL methods have been observed to be
sensitive to augmentation strategies. To address these limitations, we propose
a novel POCL framework named Distortion-Disentangled Contrastive Learning
(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to
explicitly disentangle and exploit the DVR inside the model and feature stream
to improve the overall representation utilization efficiency, robustness and
representation ability. Experiments carried out demonstrate the superiority of
our framework to Barlow Twins and Simsiam in terms of convergence,
representation quality, and robustness on several benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sifan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jionglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06388">
<title>FAC: 3D Representation Learning via Foreground Aware Feature Contrast. (arXiv:2303.06388v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06388</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has recently demonstrated great potential for
unsupervised pre-training in 3D scene understanding tasks. However, most
existing work randomly selects point features as anchors while building
contrast, leading to a clear bias toward background points that often dominate
in 3D scenes. Also, object awareness and foreground-to-background
discrimination are neglected, making contrastive learning less effective. To
tackle these issues, we propose a general foreground-aware feature contrast
(FAC) framework to learn more effective point cloud representations in
pre-training. FAC consists of two novel contrast designs to construct more
effective and informative contrast pairs. The first is building positive pairs
within the same foreground segment where points tend to have the same
semantics. The second is that we prevent over-discrimination between 3D
segments/objects and encourage foreground-to-background distinctions at the
segment level with adaptive feature learning in a Siamese correspondence
network, which adaptively learns feature correlations within and across point
cloud views effectively. Visualization with point activation maps shows that
our contrast pairs capture clear correspondences among foreground regions
during pre-training. Quantitative experiments also show that FAC achieves
superior knowledge transfer and data efficiency in various downstream 3D
semantic segmentation and object detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1&quot;&gt;Aoran Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Ling Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06681">
<title>Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction. (arXiv:2303.06681v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06681</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse-view cone-beam CT (CBCT) reconstruction is an important direction to
reduce radiation dose and benefit clinical applications. Previous voxel-based
generation methods represent the CT as discrete voxels, resulting in high
memory requirements and limited spatial resolution due to the use of 3D
decoders. In this paper, we formulate the CT volume as a continuous intensity
field and develop a novel DIF-Net to perform high-quality CBCT reconstruction
from extremely sparse (fewer than 10) projection views at an ultrafast speed.
The intensity field of a CT can be regarded as a continuous function of 3D
spatial points. Therefore, the reconstruction can be reformulated as regressing
the intensity value of an arbitrary 3D point from given sparse projections.
Specifically, for a point, DIF-Net extracts its view-specific features from
different 2D projection views. These features are subsequently aggregated by a
fusion module for intensity estimation. Notably, thousands of points can be
processed in parallel to improve efficiency during training and testing. In
practice, we collect a knee CBCT dataset to train and evaluate DIF-Net.
Extensive experiments show that our approach can reconstruct CBCT with high
image quality and high spatial resolution from extremely sparse views within
1.6 seconds, significantly outperforming state-of-the-art methods. Our code
will be available at https://github.com/xmed-lab/DIF-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongjin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10610">
<title>DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification. (arXiv:2303.10610v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10610</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion Probabilistic Models have recently shown remarkable performance in
generative image modeling, attracting significant attention in the computer
vision community. However, while a substantial amount of diffusion-based
research has focused on generative tasks, few studies have applied diffusion
models to general medical image classification. In this paper, we propose the
first diffusion-based model (named DiffMIC) to address general medical image
classification by eliminating unexpected noise and perturbations in medical
images and robustly capturing semantic representation. To achieve this goal, we
devise a dual conditional guidance strategy that conditions each diffusion step
with multiple granularities to improve step-wise regional attention.
Furthermore, we propose learning the mutual information in each granularity by
enforcing Maximum-Mean Discrepancy regularization during the diffusion forward
process. We evaluate the effectiveness of our DiffMIC on three medical
classification tasks with different image modalities, including placental
maturity grading on ultrasound images, skin lesion classification using
dermatoscopic images, and diabetic retinopathy grading using fundus images. Our
experimental results demonstrate that DiffMIC outperforms state-of-the-art
methods by a significant margin, indicating the universality and effectiveness
of the proposed model. Our code will be publicly available at
https://github.com/scott-yjyang/DiffMIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yijun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1&quot;&gt;Angelica I. Aviles-Rivero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13223">
<title>Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels. (arXiv:2303.13223v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13223</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label recognition (MLR) with incomplete labels is very challenging.
Recent works strive to explore the image-to-label correspondence in the
vision-language model, \ie, CLIP, to compensate for insufficient annotations.
In spite of promising performance, they generally overlook the valuable prior
about the label-to-label correspondence. In this paper, we advocate remedying
the deficiency of label supervision for the MLR with incomplete labels by
deriving a structured semantic prior about the label-to-label correspondence
via a semantic prior prompter. We then present a novel Semantic Correspondence
Prompt Network (SCPNet), which can thoroughly explore the structured semantic
prior. A Prior-Enhanced Self-Supervised Learning method is further introduced
to enhance the use of the prior. Comprehensive experiments and analyses on
several widely used benchmark datasets show that our method significantly
outperforms existing methods on all datasets, well demonstrating the
effectiveness and the superiority of our method. Our code will be available at
https://github.com/jameslahm/SCPNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zixuan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Ao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengzhang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yongjun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Weipeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jungong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16739">
<title>Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimziation. (arXiv:2303.16739v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16739</link>
<description rdf:parseType="Literal">&lt;p&gt;Actively planning sensor views during object reconstruction is crucial for
autonomous mobile robots. An effective method should be able to strike a
balance between accuracy and efficiency. In this paper, we propose a seamless
integration of the emerging implicit representation with the active
reconstruction task. We build an implicit occupancy field as our geometry
proxy. While training, the prior object bounding box is utilized as auxiliary
information to generate clean and detailed reconstructions. To evaluate view
uncertainty, we employ a sampling-based approach that directly extracts entropy
from the reconstructed occupancy probability field as our measure of view
information gain. This eliminates the need for additional uncertainty maps or
learning. Unlike previous methods that compare view uncertainty within a finite
set of candidates, we aim to find the next-best-view (NBV) on a continuous
manifold. Leveraging the differentiability of the implicit representation, the
NBV can be optimized directly by maximizing the view uncertainty using gradient
descent. It significantly enhances the method&apos;s adaptability to different
scenarios. Simulation and real-world experiments demonstrate that our approach
effectively improves reconstruction accuracy and efficiency of view planning in
active reconstruction tasks. The proposed system will open source at
https://github.com/HITSZ-NRSL/ActiveImplicitRecon.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Dongyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_F/0/1/0/all/0/1&quot;&gt;Fengyu Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1&quot;&gt;Mengmeng Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02168">
<title>I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02168</link>
<description rdf:parseType="Literal">&lt;p&gt;Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks&apos; Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1&quot;&gt;Tejas Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Furong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1&quot;&gt;Mohammad Rostami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06470">
<title>Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06470</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability of image and video generation models to create photorealistic
images has reached unprecedented heights, making it difficult to distinguish
between real and fake images in many cases. However, despite this progress, a
gap remains between the quality of generated images and those found in the real
world. To address this, we have reviewed a vast body of literature from both
academic publications and social media to identify qualitative shortcomings in
image generation models, which we have classified into five categories. By
understanding these failures, we can identify areas where these models need
improvement, as well as develop strategies for detecting deep fakes. The
prevalence of deep fakes in today&apos;s society is a serious concern, and our
findings can help mitigate their negative impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10671">
<title>Point-supervised Single-cell Segmentation via Collaborative Knowledge Sharing. (arXiv:2304.10671v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10671</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their superior performance, deep-learning methods often suffer from
the disadvantage of needing large-scale well-annotated training data. In
response, recent literature has seen a proliferation of efforts aimed at
reducing the annotation burden. This paper focuses on a weakly-supervised
training setting for single-cell segmentation models, where the only available
training label is the rough locations of individual cells. The specific problem
is of practical interest due to the widely available nuclei counter-stain data
in biomedical literature, from which the cell locations can be derived
programmatically. Of more general interest is a proposed self-learning method
called collaborative knowledge sharing, which is related to but distinct from
the more well-known consistency learning methods. This strategy achieves
self-learning by sharing knowledge between a principal model and a very
light-weight collaborator model. Importantly, the two models are entirely
different in their architectures, capacities, and model outputs: In our case,
the principal model approaches the segmentation problem from an
object-detection perspective, whereas the collaborator model a sematic
segmentation perspective. We assessed the effectiveness of this strategy by
conducting experiments on LIVECell, a large single-cell segmentation dataset of
bright-field images, and on A431 dataset, a fluorescence image dataset in which
the location labels are generated automatically from nuclei counter-stain data.
Implementing code is available at https://github.com/jiyuuchc/lacss
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Ji Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00679">
<title>Enhanced Multi-level Features for Very High Resolution Remote Sensing Scene Classification. (arXiv:2305.00679v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Very high-resolution (VHR) remote sensing (RS) scene classification is a
challenging task due to the higher inter-class similarity and intra-class
variability problems. Recently, the existing deep learning (DL)-based methods
have shown great promise in VHR RS scene classification. However, they still
provide an unstable classification performance. To address such a problem, we,
in this letter, propose a novel DL-based approach. For this, we devise an
enhanced VHR attention module (EAM), followed by the atrous spatial pyramid
pooling (ASPP) and global average pooling (GAP). This procedure imparts the
enhanced features from the corresponding level. Then, the multi-level feature
fusion is performed. Experimental results on two widely-used VHR RS datasets
show that the proposed approach yields a competitive and stable/robust
classification performance with the least standard deviation of 0.001. Further,
the highest overall accuracies on the AID and the NWPU datasets are 95.39% and
93.04%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1&quot;&gt;Chiranjibi Sitaula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+KC_S/0/1/0/all/0/1&quot;&gt;Sumesh KC&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aryal_J/0/1/0/all/0/1&quot;&gt;Jagannath Aryal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11427">
<title>RGB-D And Thermal Sensor Fusion: A Systematic Literature Review. (arXiv:2305.11427v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11427</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last decade, the computer vision field has seen significant progress
in multimodal data fusion and learning, where multiple sensors, including
depth, infrared, and visual, are used to capture the environment across diverse
spectral ranges. Despite these advancements, there has been no systematic and
comprehensive evaluation of fusing RGB-D and thermal modalities to date. While
autonomous driving using LiDAR, radar, RGB, and other sensors has garnered
substantial research interest, along with the fusion of RGB and depth
modalities, the integration of thermal cameras and, specifically, the fusion of
RGB-D and thermal data, has received comparatively less attention. This might
be partly due to the limited number of publicly available datasets for such
applications. This paper provides a comprehensive review of both,
state-of-the-art and traditional methods used in fusing RGB-D and thermal
camera data for various applications, such as site inspection, human tracking,
fault detection, and others. The reviewed literature has been categorised into
technical areas, such as 3D reconstruction, segmentation, object detection,
available datasets, and other related topics. Following a brief introduction
and an overview of the methodology, the study delves into calibration and
registration techniques, then examines thermal visualisation and 3D
reconstruction, before discussing the application of classic feature-based
techniques as well as modern deep learning approaches. The paper concludes with
a discourse on current limitations and potential future research directions. It
is hoped that this survey will serve as a valuable reference for researchers
looking to familiarise themselves with the latest advancements and contribute
to the RGB-DT research field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1&quot;&gt;Martin Brenner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyes_N/0/1/0/all/0/1&quot;&gt;Napoleon H. Reyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susnjak_T/0/1/0/all/0/1&quot;&gt;Teo Susnjak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barczak_A/0/1/0/all/0/1&quot;&gt;Andre L.C. Barczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15542">
<title>TOAST: Transfer Learning via Attention Steering. (arXiv:2305.15542v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15542</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we explore refocusing model
attention for transfer learning. We introduce Top-Down Attention Steering
(TOAST), a novel transfer learning algorithm that keeps the pre-trained
backbone frozen, selects task-relevant features in the output, and feeds those
features back to the model to steer the attention to the task-specific
features. By refocusing the attention only, TOAST achieves state-of-the-art
results on a number of transfer learning benchmarks, while having a small
number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt
tuning, TOAST substantially improves performance across a range of fine-grained
visual classification datasets (e.g., 81.1% -&amp;gt; 86.2% on FGVC). TOAST also
outperforms the fully fine-tuned Alpaca and Vicuna models on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Baifeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_S/0/1/0/all/0/1&quot;&gt;Siyu Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17303">
<title>Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17303</link>
<description rdf:parseType="Literal">&lt;p&gt;Building generalizable AI models is one of the primary challenges in the
healthcare domain. While radiologists rely on generalizable descriptive rules
of abnormality, Neural Network (NN) models suffer even with a slight shift in
input distribution (e.g., scanner type). Fine-tuning a model to transfer
knowledge from one domain to another requires a significant amount of labeled
data in the target domain. In this paper, we develop an interpretable model
that can be efficiently fine-tuned to an unseen target domain with minimal
computational cost. We assume the interpretable component of NN to be
approximately domain-invariant. However, interpretable models typically
underperform compared to their Blackbox (BB) variants. We start with a BB in
the source domain and distill it into a \emph{mixture} of shallow interpretable
models using human-understandable concepts. As each interpretable model covers
a subset of data, a mixture of interpretable models achieves comparable
performance as BB. Further, we use the pseudo-labeling technique from
semi-supervised learning (SSL) to learn the concept classifier in the target
domain, followed by fine-tuning the interpretable models in the target domain.
We evaluate our model using a real-life large-scale chest-X-ray (CXR)
classification dataset. The code is available at:
\url{https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18295">
<title>RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths. (arXiv:2305.18295v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18295</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generation has recently witnessed remarkable achievements. We
introduce a text-conditional image diffusion model, termed RAPHAEL, to generate
highly artistic images, which accurately portray the text prompts, encompassing
multiple nouns, adjectives, and verbs. This is achieved by stacking tens of
mixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling
billions of diffusion paths (routes) from the network input to the output. Each
path intuitively functions as a &quot;painter&quot; for depicting a particular textual
concept onto a specified image region at a diffusion timestep. Comprehensive
experiments reveal that RAPHAEL outperforms recent cutting-edge models, such as
Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both
image quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior
performance in switching images across diverse styles, such as Japanese comics,
realism, cyberpunk, and ink illustration. Secondly, a single model with three
billion parameters, trained on 1,000 A100 GPUs for two months, achieves a
state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,
RAPHAEL significantly surpasses its counterparts in human evaluation on the
ViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the
frontiers of image generation research in both academia and industry, paving
the way for future breakthroughs in this rapidly evolving field. More details
can be found on a webpage: https://miaohua.sensetime.com/en.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zeyue Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guanglu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiushan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Boxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03430">
<title>Revisiting the Trade-off between Accuracy and Robustness via Weight Distribution of Filters. (arXiv:2306.03430v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03430</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks have been proven to be potential threats to Deep Neural
Networks (DNNs), and many methods are proposed to defend against adversarial
attacks. However, while enhancing the robustness, the clean accuracy will
decline to a certain extent, implying a trade-off existed between the accuracy
and robustness. In this paper, we firstly empirically find an obvious
distinction between standard and robust models in the filters&apos; weight
distribution of the same architecture, and then theoretically explain this
phenomenon in terms of the gradient regularization, which shows this difference
is an intrinsic property for DNNs, and thus a static network architecture is
difficult to improve the accuracy and robustness at the same time. Secondly,
based on this observation, we propose a sample-wise dynamic network
architecture named Adversarial Weight-Varied Network (AW-Net), which focuses on
dealing with clean and adversarial examples with a ``divide and rule&quot; weight
strategy. The AW-Net dynamically adjusts network&apos;s weights based on regulation
signals generated by an adversarial detector, which is directly influenced by
the input sample. Benefiting from the dynamic network architecture, clean and
adversarial examples can be processed with different network weights, which
provides the potentiality to enhance the accuracy and robustness
simultaneously. A series of experiments demonstrate that our AW-Net is
architecture-friendly to handle both clean and adversarial examples and can
achieve better trade-off performance than state-of-the-art robust models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiji Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06210">
<title>Single-Model Attribution of Generative Models Through Final-Layer Inversion. (arXiv:2306.06210v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06210</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent groundbreaking developments on generative modeling have sparked
interest in practical single-model attribution. Such methods predict whether a
sample was generated by a specific generator or not, for instance, to prove
intellectual property theft. However, previous works are either limited to the
closed-world setting or require undesirable changes of the generative model. We
address these shortcomings by proposing FLIPAD, a new approach for single-model
attribution in the open-world setting based on final-layer inversion and
anomaly detection. We show that the utilized final-layer inversion can be
reduced to a convex lasso optimization problem, making our approach
theoretically sound and computationally efficient. The theoretical findings are
accompanied by an experimental study demonstrating the effectiveness of our
approach, outperforming the existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1&quot;&gt;Mike Laszkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricker_J/0/1/0/all/0/1&quot;&gt;Jonas Ricker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1&quot;&gt;Johannes Lederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07532">
<title>Referring Camouflaged Object Detection. (arXiv:2306.07532v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07532</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of referring camouflaged object detection (Ref-COD),
a new task that aims to segment specified camouflaged objects based on a small
set of referring images with salient target objects. We first assemble a
large-scale dataset, called R2C7K, which consists of 7K images covering 64
object categories in real-world scenarios. Then, we develop a simple but strong
dual-branch framework, dubbed R2CNet, with a reference branch embedding the
common representations of target objects from referring images and a
segmentation branch identifying and segmenting camouflaged objects under the
guidance of the common representations. In particular, we design a Referring
Mask Generation module to generate pixel-level prior mask and a Referring
Feature Enrichment module to enhance the capability of identifying specified
camouflaged objects. Extensive experiments show the superiority of our Ref-COD
methods over their COD counterparts in segmenting specified camouflaged objects
and identifying the main body of target objects. Our code and dataset are
publicly available at https://github.com/zhangxuying1004/RefCOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bowen Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1&quot;&gt;Qibin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08249">
<title>Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition. (arXiv:2306.08249v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08249</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked autoencoder (MAE) has attracted unprecedented attention and achieves
remarkable performance in many vision tasks. It reconstructs random masked
image patches (known as proxy task) during pretraining and learns meaningful
semantic representations that can be transferred to downstream tasks. However,
MAE has not been thoroughly explored in ultrasound imaging. In this work, we
investigate the potential of MAE for ultrasound image recognition. Motivated by
the unique property of ultrasound imaging in high noise-to-signal ratio, we
propose a novel deblurring MAE approach that incorporates deblurring into the
proxy task during pretraining. The addition of deblurring facilitates the
pretraining to better recover the subtle details presented in the ultrasound
images, thus improving the performance of the downstream classification task.
Our experimental results demonstrate the effectiveness of our deblurring MAE,
achieving state-of-the-art performance in ultrasound image classification.
Overall, our work highlights the potential of MAE for ultrasound image
recognition and presents a novel approach that incorporates deblurring to
further improve its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qingbo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1&quot;&gt;Qicheng Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12045">
<title>Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12045</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing flow.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a natural paradigm. We show that
TeCoS-LVM models can produce more realistic spike activities and accurately fit
spike statistics than powerful alternatives. Additionally, learned TeCoS-LVM
models can generalize well to longer time scales. Overall, while remaining
computationally tractable, our model effectively captures key features of
neural coding systems. It thus provides a useful tool for building accurate
predictive computational accounts for various sensory perception circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gehua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Runhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huajin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16170">
<title>Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16170</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training is a practical approach for improving the robustness of
deep neural networks against adversarial attacks. Although bringing reliable
robustness, the performance toward clean examples is negatively affected after
adversarial training, which means a trade-off exists between accuracy and
robustness. Recently, some studies have tried to use knowledge distillation
methods in adversarial training, achieving competitive performance in improving
the robustness but the accuracy for clean samples is still limited. In this
paper, to mitigate the accuracy-robustness trade-off, we introduce the
Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model&apos;s
adversarial training process by applying a strong clean teacher and a strong
robust teacher to handle the clean examples and adversarial examples,
respectively. During the optimization process, to ensure that different
teachers show similar knowledge scales, we design the Entropy-Based Balance
algorithm to adjust the teacher&apos;s temperature and keep the teachers&apos;
information entropy consistent. Besides, to ensure that the student has a
relatively consistent learning speed from multiple teachers, we propose the
Normalization Loss Balance algorithm to adjust the learning weights of
different types of knowledge. A series of experiments conducted on public
datasets demonstrate that MTARD outperforms the state-of-the-art adversarial
training and distillation methods against various adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiji Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xizhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02858">
<title>Deep Ensemble Learning with Frame Skipping for Face Anti-Spoofing. (arXiv:2307.02858v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02858</link>
<description rdf:parseType="Literal">&lt;p&gt;Face presentation attacks (PA), also known as spoofing attacks, pose a
substantial threat to biometric systems that rely on facial recognition
systems, such as access control systems, mobile payments, and identity
verification systems. To mitigate the spoofing risk, several video-based
methods have been presented in the literature that analyze facial motion in
successive video frames. However, estimating the motion between adjacent frames
is a challenging task and requires high computational cost. In this paper, we
rephrase the face anti-spoofing task as a motion prediction problem and
introduce a deep ensemble learning model with a frame skipping mechanism. In
particular, the proposed frame skipping adopts a uniform sampling approach by
dividing the original video into video clips of fixed size. By doing so, every
nth frame of the clip is selected to ensure that the temporal patterns can
easily be perceived during the training of three different recurrent neural
networks (RNNs). Motivated by the performance of individual RNNs, a meta-model
is developed to improve the overall detection performance by combining the
prediction of individual RNNs. Extensive experiments were performed on four
datasets, and state-of-the-art performance is reported on MSU-MFSD (3.12%),
Replay-Attack (11.19%), and OULU-NPU (12.23%) databases by using half total
error rates (HTERs) in the most challenging cross-dataset testing scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhammad_U/0/1/0/all/0/1&quot;&gt;Usman Muhammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1&quot;&gt;Md Ziaul Hoque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1&quot;&gt;Mourad Oussalah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1&quot;&gt;Jorma Laaksonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03273">
<title>ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images. (arXiv:2307.03273v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03273</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistical shape models (SSM) have been well-established as an excellent
tool for identifying variations in the morphology of anatomy across the
underlying population. Shape models use consistent shape representation across
all the samples in a given cohort, which helps to compare shapes and identify
the variations that can detect pathologies and help in formulating treatment
plans. In medical imaging, computing these shape representations from CT/MRI
scans requires time-intensive preprocessing operations, including but not
limited to anatomy segmentation annotations, registration, and texture
denoising. Deep learning models have demonstrated exceptional capabilities in
learning shape representations directly from volumetric images, giving rise to
highly effective and efficient Image-to-SSM. Nevertheless, these models are
data-hungry and due to the limited availability of medical data, deep learning
models tend to overfit. Offline data augmentation techniques, that use kernel
density estimation based (KDE) methods for generating shape-augmented samples,
have successfully aided Image-to-SSM networks in achieving comparable accuracy
to traditional SSM methods. However, these augmentation methods focus on shape
augmentation, whereas deep learning models exhibit image-based texture bias
results in sub-optimal models. This paper introduces a novel strategy for
on-the-fly data augmentation for the Image-to-SSM framework by leveraging
data-dependent noise generation or texture augmentation. The proposed framework
is trained as an adversary to the Image-to-SSM network, augmenting diverse and
challenging noisy samples. Our approach achieves improved accuracy by
encouraging the model to focus on the underlying geometry rather than relying
solely on pixel values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanam_M/0/1/0/all/0/1&quot;&gt;Mokshagna Sai Teja Karanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kataria_T/0/1/0/all/0/1&quot;&gt;Tushar Kataria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04014">
<title>Novel Pipeline for Diagnosing Acute Lymphoblastic Leukemia Sensitive to Related Biomarkers. (arXiv:2307.04014v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04014</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Lymphoblastic Leukemia (ALL) is one of the most common types of
childhood blood cancer. The quick start of the treatment process is critical to
saving the patient&apos;s life, and for this reason, early diagnosis of this disease
is essential. Examining the blood smear images of these patients is one of the
methods used by expert doctors to diagnose this disease. Deep learning-based
methods have numerous applications in medical fields, as they have
significantly advanced in recent years. ALL diagnosis is not an exception in
this field, and several machine learning-based methods for this problem have
been proposed. In previous methods, high diagnostic accuracy was reported, but
our work showed that this alone is not sufficient, as it can lead to models
taking shortcuts and not making meaningful decisions. This issue arises due to
the small size of medical training datasets. To address this, we constrained
our model to follow a pipeline inspired by experts&apos; work. We also demonstrated
that, since a judgement based on only one image is insufficient, redefining the
problem as a multiple-instance learning problem is necessary for achieving a
practical result. Our model is the first to provide a solution to this problem
in a multiple-instance learning setup. We introduced a novel pipeline for
diagnosing ALL that approximates the process used by hematologists, is
sensitive to disease biomarkers, and achieves an accuracy of 96.15%, an
F1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL
IDB 1. Our method was further evaluated on an out-of-distribution dataset,
which posed a challenging test and had acceptable performance. Notably, our
model was trained on a relatively small dataset, highlighting the potential for
our approach to be applied to other medical datasets with limited data
availability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askari_Farsangi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Askari-Farsangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifi_Zarchi_A/0/1/0/all/0/1&quot;&gt;Ali Sharifi-Zarchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1&quot;&gt;Mohammad Hossein Rohban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04157">
<title>DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer. (arXiv:2307.04157v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04157</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Style Transfer (NST) is the field of study applying neural techniques
to modify the artistic appearance of a content image to match the style of a
reference style image. Traditionally, NST methods have focused on texture-based
image edits, affecting mostly low level information and keeping most image
structures the same. However, style-based deformation of the content is
desirable for some styles, especially in cases where the style is abstract or
the primary concept of the style is in its deformed rendition of some content.
With the recent introduction of diffusion models, such as Stable Diffusion, we
can access far more powerful image generation techniques, enabling new
possibilities. In our work, we propose using this new class of models to
perform style transfer while enabling deformable style transfer, an elusive
capability in previous models. We show how leveraging the priors of these
models can expose new artistic controls at inference time, and we document our
findings in exploring this new direction for the field of style transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruta_D/0/1/0/all/0/1&quot;&gt;Dan Ruta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarres_G/0/1/0/all/0/1&quot;&gt;Gemma Canet Tarr&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1&quot;&gt;Nicholas Kolkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1&quot;&gt;John Collomosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04390">
<title>CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning. (arXiv:2307.04390v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04390</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: MR-based subchondral bone effectively predicts knee
osteoarthritis. However, its clinical application is limited by the cost and
time of MR. Purpose: We aim to develop a novel distillation-learning-based
method named SRRD for subchondral bone microstructural analysis using
easily-acquired CT images, which leverages paired MR images to enhance the
CT-based analysis model during training. Materials and Methods: Knee joint
images of both CT and MR modalities were collected from October 2020 to May
2021. Firstly, we developed a GAN-based generative model to transform MR images
into CT images, which was used to establish the anatomical correspondence
between the two modalities. Next, we obtained numerous patches of subchondral
bone regions of MR images, together with their trabecular parameters (BV / TV,
Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression.
The distillation-learning technique was used to train the regression model and
transfer MR structural information to the CT-based model. The regressed
trabecular parameters were further used for knee osteoarthritis classification.
Results: A total of 80 participants were evaluated. CT-based regression results
of trabecular parameters achieved intra-class correlation coefficients (ICCs)
of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N,
respectively. The use of distillation learning significantly improved the
performance of the CT-based knee osteoarthritis classification method using the
CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of
0.658 (95% CI, 0.574-0.742) (p&amp;lt;.001). Conclusions: The proposed SRRD method
showed high reliability and validity in MR-CT registration, regression, and
knee osteoarthritis classification, indicating the feasibility of subchondral
bone microstructural analysis based on CT images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuqi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qing_G/0/1/0/all/0/1&quot;&gt;Gaowei Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_K/0/1/0/all/0/1&quot;&gt;Kai Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenglei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lichi Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>