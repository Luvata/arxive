<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.11104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.09559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.12645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.04317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.13803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10711" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.13699">
<title>EFL Students&apos; Attitudes and Contradictions in a Machine-in-the-loop Activity System. (arXiv:2307.13699v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.13699</link>
<description rdf:parseType="Literal">&lt;p&gt;This study applies Activity Theory and investigates the attitudes and
contradictions of 67 English as a foreign language (EFL) students from four
Hong Kong secondary schools towards machine-in-the-loop writing, where
artificial intelligence (AI) suggests ideas during composition. Students
answered an open-ended question about their feelings on writing with AI.
Results revealed mostly positive attitudes, with some negative or mixed
feelings. From a thematic analysis, contradictions or points of tension between
students and AI stemmed from AI inadequacies, students&apos; balancing enthusiasm
with preference, and their striving for language autonomy. The research
highlights the benefits and challenges of implementing machine-in-the-loop
writing in EFL classrooms, suggesting educators align activity goals with
students&apos; values, language abilities, and AI capabilities to enhance students&apos;
activity systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1&quot;&gt;David James Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1&quot;&gt;Hengky Susanto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Kai Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13700">
<title>CAMP: A Context-Aware Cricket Players Performance Metric. (arXiv:2307.13700v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13700</link>
<description rdf:parseType="Literal">&lt;p&gt;Cricket is the second most popular sport after soccer in terms of viewership.
However, the assessment of individual player performance, a fundamental task in
team sports, is currently primarily based on aggregate performance statistics,
including average runs and wickets taken. We propose Context-Aware Metric of
player Performance, CAMP, to quantify individual players&apos; contributions toward
a cricket match outcome. CAMP employs data mining methods and enables effective
data-driven decision-making for selection and drafting, coaching and training,
team line-ups, and strategy development. CAMP incorporates the exact context of
performance, such as opponents&apos; strengths and specific circumstances of games,
such as pressure situations. We empirically evaluate CAMP on data of
limited-over cricket matches between 2001 and 2019. In every match, a committee
of experts declares one player as the best player, called Man of the M}atch
(MoM). The top two rated players by CAMP match with MoM in 83\% of the 961
games. Thus, the CAMP rating of the best player closely matches that of the
domain experts. By this measure, CAMP significantly outperforms the current
best-known players&apos; contribution measure based on the Duckworth-Lewis-Stern
(DLS) method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1&quot;&gt;Muhammad Sohaib Ayub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_N/0/1/0/all/0/1&quot;&gt;Naimat Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Sarwan Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1&quot;&gt;Imdad Ullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awais_M/0/1/0/all/0/1&quot;&gt;Mian Muhammad Awais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Asad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faizullah_S/0/1/0/all/0/1&quot;&gt;Safiullah Faizullah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13701">
<title>$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation. (arXiv:2307.13701v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13701</link>
<description rdf:parseType="Literal">&lt;p&gt;To answer complex queries on knowledge graphs, logical reasoning over
incomplete knowledge is required due to the open-world assumption.
Learning-based methods are essential because they are capable of generalizing
over unobserved knowledge. Therefore, an appropriate dataset is fundamental to
both obtaining and evaluating such methods under this paradigm. In this paper,
we propose a comprehensive framework for data generation, model training, and
method evaluation that covers the combinatorial space of Existential
First-order Queries with multiple variables ($\text{EFO}_{k}$). The
combinatorial query space in our framework significantly extends those defined
by set operations in the existing literature. Additionally, we construct a
dataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empirical
evaluation, and our benchmark results provide new insights into how query
hardness affects the results. Furthermore, we demonstrate that the existing
dataset construction process is systematically biased that hinders the
appropriate development of query-answering methods, highlighting the importance
of our work. Our code and data are provided
in~\url{https://github.com/HKUST-KnowComp/EFOK-CQA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_W/0/1/0/all/0/1&quot;&gt;Weizhi Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13702">
<title>Measuring Faithfulness in Chain-of-Thought Reasoning. (arXiv:2307.13702v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13702</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) perform better when they produce step-by-step,
&quot;Chain-of-Thought&quot; (CoT) reasoning before answering a question, but it is
unclear if the stated reasoning is a faithful explanation of the model&apos;s actual
reasoning (i.e., its process for answering the question). We investigate
hypotheses for how CoT reasoning may be unfaithful, by examining how the model
predictions change when we intervene on the CoT (e.g., by adding mistakes or
paraphrasing it). Models show large variation across tasks in how strongly they
condition on the CoT when predicting their answer, sometimes relying heavily on
the CoT and other times primarily ignoring it. CoT&apos;s performance boost does not
seem to come from CoT&apos;s added test-time compute alone or from information
encoded via the particular phrasing of the CoT. As models become larger and
more capable, they produce less faithful reasoning on most tasks we study.
Overall, our results suggest that CoT can be faithful if the circumstances such
as the model size and task are carefully chosen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1&quot;&gt;Tamera Lanham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anna Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1&quot;&gt;Ansh Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1&quot;&gt;Benoit Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1&quot;&gt;Carson Denison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1&quot;&gt;Danny Hernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dustin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1&quot;&gt;Esin Durmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1&quot;&gt;Jackson Kernion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukosiute_K/0/1/0/all/0/1&quot;&gt;Kamil&amp;#x117; Luko&amp;#x161;i&amp;#x16b;t&amp;#x117;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Karina Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Newton Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1&quot;&gt;Nicholas Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1&quot;&gt;Nicholas Schiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rausch_O/0/1/0/all/0/1&quot;&gt;Oliver Rausch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larson_R/0/1/0/all/0/1&quot;&gt;Robin Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1&quot;&gt;Sam McCandlish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Sandipan Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1&quot;&gt;Saurav Kadavath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shannon Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1&quot;&gt;Thomas Henighan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1&quot;&gt;Timothy Maxwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telleen_Lawton_T/0/1/0/all/0/1&quot;&gt;Timothy Telleen-Lawton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1&quot;&gt;Tristan Hume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatfield_Dodds_Z/0/1/0/all/0/1&quot;&gt;Zac Hatfield-Dodds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1&quot;&gt;Jared Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1&quot;&gt;Jan Brauner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13704">
<title>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13704</link>
<description rdf:parseType="Literal">&lt;p&gt;eXplainable Artificial Intelligence (XAI) is now an important and essential
part of machine learning, allowing to explain the predictions of complex
models. XAI is especially required in risky applications, particularly in
health care, where human lives depend on the decisions of AI systems. One area
of medical research is age prediction and identification of biomarkers of aging
and age-related diseases. However, the role of XAI in the age prediction task
has not previously been explored directly. In this review, we discuss the
application of XAI approaches to age prediction tasks. We give a systematic
review of the works organized by body systems, and discuss the benefits of XAI
in medical applications and, in particular, in the age prediction domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalyakulina_A/0/1/0/all/0/1&quot;&gt;Alena Kalyakulina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yusipov_I/0/1/0/all/0/1&quot;&gt;Igor Yusipov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13705">
<title>Control and Monitoring of Artificial Intelligence Algorithms. (arXiv:2307.13705v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13705</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper elucidates the importance of governing an artificial intelligence
model post-deployment and overseeing potential fluctuations in the distribution
of present data in contrast to the training data. The concepts of data drift
and concept drift are explicated, along with their respective foundational
distributions. Furthermore, a range of metrics is introduced, which can be
utilized to scrutinize the model&apos;s performance concerning potential temporal
variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortuno_C/0/1/0/all/0/1&quot;&gt;Carlos Mario Braga Ortu&amp;#xf1;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donoso_B/0/1/0/all/0/1&quot;&gt;Blanza Martinez Donoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villanueva_B/0/1/0/all/0/1&quot;&gt;Bel&amp;#xe9;n Mu&amp;#xf1;iz Villanueva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13706">
<title>Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism. (arXiv:2307.13706v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.13706</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Emotion Detection (ED) aims to build systems to identify users&apos;
emotions automatically. This field has the potential to enhance HCI, creating
an individualised experience for the user. However, ED systems tend to perform
poorly on people with Autism Spectrum Disorder (ASD). Hence, the need to create
ED systems tailored to how people with autism express emotions. Previous works
have created ED systems tailored for children with ASD but did not share the
resulting dataset. Sharing annotated datasets is essential to enable the
development of more advanced computer models for ED within the research
community. In this paper, we describe our experience establishing a process to
create a multimodal annotated dataset featuring children with a level 1
diagnosis of autism. In addition, we introduce CALMED (Children, Autism,
Multimodal, Emotion, Detection), the resulting multimodal emotion detection
dataset featuring children with autism aged 8-12. CALMED includes audio and
video features extracted from recording files of study sessions with
participants, together with annotations provided by their parents into four
target classes. The generated dataset includes a total of 57,012 examples, with
each example representing a time window of 200ms (0.2s). Our experience and
methods described here, together with the dataset shared, aim to contribute to
future research applications of affective computing in ASD, which has the
potential to create systems to improve the lives of people with ASD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sousa_A/0/1/0/all/0/1&quot;&gt;Annanda Sousa&lt;/a&gt; (NUI Galway), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_K/0/1/0/all/0/1&quot;&gt;Karen Young&lt;/a&gt; (NUI Galway), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daquin_M/0/1/0/all/0/1&quot;&gt;Mathieu D&amp;#x27;aquin&lt;/a&gt; (Data Science, Knowledge, Reasoning and Engineering, LORIA, LORIA - NLPKD), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zarrouk_M/0/1/0/all/0/1&quot;&gt;Manel Zarrouk&lt;/a&gt; (LIPN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holloway_J/0/1/0/all/0/1&quot;&gt;Jennifer Holloway&lt;/a&gt; (ASK)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13709">
<title>Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13709</link>
<description rdf:parseType="Literal">&lt;p&gt;Many properties in real world, such as desirability or strength in
competitive environment, can&apos;t be directly observed, which makes them difficult
to evaluate. To deal with this challenging problem, prior work has primarily
focused on estimating those properties of known items, especially the strength
of sports players, only of those who appears in paired comparison dataset. In
this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework
to evaluate any properties of unknown items, not necessarily present in
dataset. Our method seamlessly integrates traditional Bradley-Terry model with
a neural network structure. We also generalizes this architecture further for
asymmetric environment with unfairness, which is much more common in real world
settings. In our experimental analysis, DBTR successfully learned desired
quantification of those properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_S/0/1/0/all/0/1&quot;&gt;Satoru Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13715">
<title>Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions. (arXiv:2307.13715v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13715</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, our objective is to improve the performance of the existing
framework ShuttleNet in predicting badminton shot types and locations by
leveraging past strokes. We participated in the CoachAI Badminton Challenge at
IJCAI 2023 and achieved significantly better results compared to the baseline.
Ultimately, our team achieved the first position in the competition and we made
our code available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shih-Hong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_P/0/1/0/all/0/1&quot;&gt;Pin-Hsuan Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Fu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chien-An Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13716">
<title>FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13716</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional federated learning uses the number of samples to calculate the
weights of each client model and uses this fixed weight value to fusion the
global model. However, in practical scenarios, each client&apos;s device and data
heterogeneity leads to differences in the quality of each client&apos;s model. Thus
the contribution to the global model is not wholly determined by the sample
size. In addition, if clients intentionally upload low-quality or malicious
models, using these models for aggregation will lead to a severe decrease in
global model accuracy. Traditional federated learning algorithms do not address
these issues. To solve this probelm, we propose FedDRL, a model fusion approach
using reinforcement learning based on a two staged approach. In the first
stage, Our method could filter out malicious models and selects trusted client
models to participate in the model fusion. In the second stage, the FedDRL
algorithm adaptively adjusts the weights of the trusted client models and
aggregates the optimal global model. We also define five model fusion scenarios
and compare our method with two baseline algorithms in those scenarios. The
experimental results show that our algorithm has higher reliability than other
algorithms while maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Leiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Cihao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Sibo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yuming Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheewei Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13720">
<title>Composite Diffusion | whole &gt;= \Sigma parts. (arXiv:2307.13720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13720</link>
<description rdf:parseType="Literal">&lt;p&gt;For an artist or a graphic designer, the spatial layout of a scene is a
critical design choice. However, existing text-to-image diffusion models
provide limited support for incorporating spatial information. This paper
introduces Composite Diffusion as a means for artists to generate high-quality
images by composing from the sub-scenes. The artists can specify the
arrangement of these sub-scenes through a flexible free-form segment layout.
They can describe the content of each sub-scene primarily using natural text
and additionally by utilizing reference images or control inputs such as line
art, scribbles, human pose, canny edges, and more.
&lt;/p&gt;
&lt;p&gt;We provide a comprehensive and modular method for Composite Diffusion that
enables alternative ways of generating, composing, and harmonizing sub-scenes.
Further, we wish to evaluate the composite image for effectiveness in both
image quality and achieving the artist&apos;s intent. We argue that existing image
quality metrics lack a holistic evaluation of image composites. To address
this, we propose novel quality criteria especially relevant to composite
generation.
&lt;/p&gt;
&lt;p&gt;We believe that our approach provides an intuitive method of art creation.
Through extensive user surveys, quantitative and qualitative analysis, we show
how it achieves greater spatial, semantic, and creative control over image
generation. In addition, our methods do not need to retrain or modify the
architecture of the base diffusion models and can work in a plug-and-play
manner with the fine-tuned models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamwal_V/0/1/0/all/0/1&quot;&gt;Vikram Jamwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_R/0/1/0/all/0/1&quot;&gt;Ramaneswaran S&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13721">
<title>Foundational Models Defining a New Era in Vision: A Survey and Outlook. (arXiv:2307.13721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13721</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision systems to see and reason about the compositional nature of visual
scenes are fundamental to understanding our world. The complex relations
between objects and their locations, ambiguities, and variations in the
real-world environment can be better described in human language, naturally
governed by grammatical rules and other modalities such as audio and depth. The
models learned to bridge the gap between such modalities coupled with
large-scale training data facilitate contextual reasoning, generalization, and
prompt capabilities at test time. These models are referred to as foundational
models. The output of such models can be modified through human-provided
prompts without retraining, e.g., segmenting a particular object by providing a
bounding box, having interactive dialogues by asking questions about an image
or video scene or manipulating the robot&apos;s behavior through language
instructions. In this survey, we provide a comprehensive review of such
emerging foundational models, including typical architecture designs to combine
different modalities (vision, text, audio, etc), training objectives
(contrastive, generative), pre-training datasets, fine-tuning mechanisms, and
the common prompting patterns; textual, visual, and heterogeneous. We discuss
the open challenges and research directions for foundational models in computer
vision, including difficulties in their evaluations and benchmarking, gaps in
their real-world understanding, limitations of their contextual understanding,
biases, vulnerability to adversarial attacks, and interpretability issues. We
review recent developments in this field, covering a wide range of applications
of foundation models systematically and comprehensively. A comprehensive list
of foundational models studied in this work is available at
\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awais_M/0/1/0/all/0/1&quot;&gt;Muhammad Awais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13755">
<title>TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13755</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised object detection (SSOD) can incorporate limited labeled data
and large amounts of unlabeled data to improve the performance and
generalization of existing object detectors. Despite many advances, recent SSOD
methods are still challenged by noisy/misleading pseudo-labels, classical
exponential moving average (EMA) strategy, and the consensus of Teacher-Student
models in the latter stages of training. This paper proposes a novel
training-based model refinement (TMR) stage and a simple yet effective
representation disagreement (RD) strategy to address the limitations of
classical EMA and the consensus problem. The TMR stage of Teacher-Student
models optimizes the lightweight scaling operation to refine the model&apos;s
weights and prevent overfitting or forgetting learned patterns from unlabeled
data. Meanwhile, the RD strategy helps keep these models diverged to encourage
the student model to explore complementary representations. In addition, we use
cascade regression to generate more reliable pseudo-labels for supervising the
student model. Extensive experiments demonstrate the superior performance of
our approach over state-of-the-art SSOD methods. Specifically, the proposed
approach outperforms the Unbiased-Teacher method by an average mAP margin of
4.6% and 5.3% when using partially-labeled and fully-labeled data on the
MS-COCO dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1&quot;&gt;Nadir Erbilgin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13763">
<title>Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.13763</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new approach to non-parametric density estimation, that is based
on regularizing a Sobolev norm of the density. This method is provably
different from Kernel Density Estimation, and makes the bias of the model clear
and interpretable. While there is no closed analytic form for the associated
kernel, we show that one can approximate it using sampling. The optimization
problem needed to determine the density is non-convex, and standard gradient
methods do not perform well. However, we show that with an appropriate
initialization and using natural gradients, one can obtain well performing
solutions. Finally, while the approach provides unnormalized densities, which
prevents the use of log-likelihood for cross validation, we show that one can
instead adapt Fisher Divergence based Score Matching methods for this task. We
evaluate the resulting method on the comprehensive recent Anomaly Detection
benchmark suite, ADBench, and find that it ranks second best, among more than
15 algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kozdoba_M/0/1/0/all/0/1&quot;&gt;Mark Kozdoba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perets_B/0/1/0/all/0/1&quot;&gt;Binyamin Perets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13766">
<title>ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning. (arXiv:2307.13766v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.13766</link>
<description rdf:parseType="Literal">&lt;p&gt;In practical scenarios, the effectiveness of sequential recommendation
systems is hindered by the user cold-start problem, which arises due to limited
interactions for accurately determining user preferences. Previous studies have
attempted to address this issue by combining meta-learning with user and
item-side information. However, these approaches face inherent challenges in
modeling user preference dynamics, particularly for &quot;minor users&quot; who exhibit
distinct preferences compared to more common or &quot;major users.&quot; To overcome
these limitations, we present a novel approach called ClusterSeq, a
Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq
leverages dynamic information in the user sequence to enhance item prediction
accuracy, even in the absence of side information. This model preserves the
preferences of minor users without being overshadowed by major users, and it
capitalizes on the collective knowledge of users within the same cluster.
Extensive experiments conducted on various benchmark datasets validate the
effectiveness of ClusterSeq. Empirical results consistently demonstrate that
ClusterSeq outperforms several state-of-the-art meta-learning recommenders.
Notably, compared to existing meta-learning methods, our proposed approach
achieves a substantial improvement of 16-39% in Mean Reciprocal Rank (MRR).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheri_M/0/1/0/all/0/1&quot;&gt;Mohammmadmahdi Maheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdollahzadeh_R/0/1/0/all/0/1&quot;&gt;Reza Abdollahzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_B/0/1/0/all/0/1&quot;&gt;Bardia Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafiei_M/0/1/0/all/0/1&quot;&gt;Mina Rafiei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibi_J/0/1/0/all/0/1&quot;&gt;Jafar Habibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabiee_H/0/1/0/all/0/1&quot;&gt;Hamid R. Rabiee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13770">
<title>E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning. (arXiv:2307.13770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13770</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of transformer-based models continues to grow, fine-tuning these
large-scale pretrained vision models for new tasks has become increasingly
parameter-intensive. Parameter-efficient learning has been developed to reduce
the number of tunable parameters during fine-tuning. Although these methods
show promising results, there is still a significant performance gap compared
to full fine-tuning. To address this challenge, we propose an Effective and
Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale
transformer-based model adaptation. Specifically, we introduce a set of
learnable key-value prompts and visual prompts into self-attention and input
layers, respectively, to improve the effectiveness of model fine-tuning.
Moreover, we design a prompt pruning procedure to systematically prune low
importance prompts while preserving model performance, which largely enhances
the model&apos;s efficiency. Empirical results demonstrate that our approach
outperforms several state-of-the-art baselines on two benchmarks, with
considerably low parameter usage (e.g., 0.32% of model parameters on VTAB-1k).
Our code is available at https://github.com/ChengHan111/E2VPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13776">
<title>Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations. (arXiv:2307.13776v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.13776</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we advocate for using large pre-trained monolingual language
models in cross lingual zero-shot word sense disambiguation (WSD) coupled with
a contextualized mapping mechanism. We also report rigorous experiments that
illustrate the effectiveness of employing sparse contextualized word
representations obtained via a dictionary learning procedure. Our experimental
results demonstrate that the above modifications yield a significant
improvement of nearly 6.5 points of increase in the average F-score (from 62.0
to 68.5) over a collection of 17 typologically diverse set of target languages.
We release our source code for replicating our experiments at
https://github.com/begab/sparsity_makes_sense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berend_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe1;bor Berend&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13777">
<title>An Empirical Study on Bugs Inside PyTorch: A Replication Study. (arXiv:2307.13777v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.13777</link>
<description rdf:parseType="Literal">&lt;p&gt;Software systems are increasingly relying on deep learning components, due to
their remarkable capability of identifying complex data patterns and powering
intelligent behaviour. A core enabler of this change in software development is
the availability of easy-to-use deep learning libraries. Libraries like PyTorch
and TensorFlow empower a large variety of intelligent systems, offering a
multitude of algorithms and configuration options, applicable to numerous
domains of systems. However, bugs in those popular deep learning libraries also
may have dire consequences for the quality of systems they enable; thus, it is
important to understand how bugs are identified and fixed in those libraries.
&lt;/p&gt;
&lt;p&gt;Inspired by a study of Jia et al., which investigates the bug identification
and fixing process at TensorFlow, we characterize bugs in the PyTorch library,
a very popular deep learning framework. We investigate the causes and symptoms
of bugs identified during PyTorch&apos;s development, and assess their locality
within the project, and extract patterns of bug fixes. Our results highlight
that PyTorch bugs are more like traditional software projects bugs, than
related to deep learning characteristics. Finally, we also compare our results
with the study on TensorFlow, highlighting similarities and differences across
the bug identification and fixing process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majdinasab_V/0/1/0/all/0/1&quot;&gt;Vahid Majdinasab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_S/0/1/0/all/0/1&quot;&gt;Sharon Chee Yin Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Mohayeminul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_D/0/1/0/all/0/1&quot;&gt;Diego Elias Costa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shihab_E/0/1/0/all/0/1&quot;&gt;Emad Shihab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1&quot;&gt;Foutse Khomh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadi_S/0/1/0/all/0/1&quot;&gt;Sarah Nadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_M/0/1/0/all/0/1&quot;&gt;Muhammad Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13779">
<title>Is GPT a Computational Model of Emotion? Detailed Analysis. (arXiv:2307.13779v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.13779</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the emotional reasoning abilities of the GPT family
of large language models via a component perspective. The paper first examines
how the model reasons about autobiographical memories. Second, it
systematically varies aspects of situations to impact emotion intensity and
coping tendencies. Even without the use of prompt engineering, it is shown that
GPT&apos;s predictions align significantly with human-provided appraisals and
emotional labels. However, GPT faces difficulties predicting emotion intensity
and coping responses. GPT-4 showed the highest performance in the initial study
but fell short in the second, despite providing superior results after minor
prompt engineering. This assessment brings up questions on how to effectively
employ the strong points and address the weak areas of these models,
particularly concerning response variability. These studies underscore the
merits of evaluating models from a componential perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tak_A/0/1/0/all/0/1&quot;&gt;Ala N. Tak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1&quot;&gt;Jonathan Gratch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13813">
<title>How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.13813</link>
<description rdf:parseType="Literal">&lt;p&gt;Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important tool for practical machine
learning is the model Exponential Moving Average (EMA), which is a model copy
that does not receive gradient information, but instead follows its target
model with some momentum. This model EMA can improve the robustness and
generalization properties of supervised learning, stabilize pseudo-labeling,
and provide a learning signal for Self-Supervised Learning (SSL). Prior works
have treated the model EMA separately from optimization, leading to different
training dynamics across batch sizes and lower model performance. In this work,
we provide a scaling rule for optimization in the presence of model EMAs and
demonstrate its validity across a range of architectures, optimizers, and data
modalities. We also show the rule&apos;s validity where the model EMA contributes to
the optimization of the target model, enabling us to train EMA-based
pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we
enable training of BYOL up to batch size 24,576 without sacrificing
performance, optimally a 6$\times$ wall-clock time reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1&quot;&gt;Pierre Ablin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1&quot;&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1&quot;&gt;Eeshan Gunesh Dhekane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1&quot;&gt;Xavier Suau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1&quot;&gt;Russ Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13815">
<title>ForestMonkey: Toolkit for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.13815v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13815</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have
gained popularity recently, enabling users to explain the predictions or
decision processes of AI models. This paper introduces Forest Monkey (FM), a
toolkit designed to reason the outputs of any AI-based defect detection and/or
classification model with data explainability. Implemented as a Python package,
FM takes input in the form of dataset folder paths (including original images,
ground truth labels, and predicted labels) and provides a set of charts and a
text file to illustrate the reasoning results and suggest possible
improvements. The FM toolkit consists of processes such as feature extraction
from predictions to reasoning targets, feature extraction from images to defect
characteristics, and a decision tree-based AI-Reasoner. Additionally, this
paper investigates the time performance of the FM toolkit when applied to four
AI models with different datasets. Lastly, a tutorial is provided to guide
users in performing reasoning tasks using the FM toolkit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosma_G/0/1/0/all/0/1&quot;&gt;Georgina Cosma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bugby_S/0/1/0/all/0/1&quot;&gt;Sarah Bugby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_J/0/1/0/all/0/1&quot;&gt;Jason Watkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13821">
<title>Fitting Auditory Filterbanks with Multiresolution Neural Networks. (arXiv:2307.13821v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.13821</link>
<description rdf:parseType="Literal">&lt;p&gt;Waveform-based deep learning faces a dilemma between nonparametric and
parametric approaches. On one hand, convolutional neural networks (convnets)
may approximate any linear time-invariant system; yet, in practice, their
frequency responses become more irregular as their receptive fields grow. On
the other hand, a parametric model such as LEAF is guaranteed to yield Gabor
filters, hence an optimal time-frequency localization; yet, this strong
inductive bias comes at the detriment of representational capacity. In this
paper, we aim to overcome this dilemma by introducing a neural audio model,
named multiresolution neural network (MuReNN). The key idea behind MuReNN is to
train separate convolutional operators over the octave subbands of a discrete
wavelet transform (DWT). Since the scale of DWT atoms grows exponentially
between octaves, the receptive fields of the subsequent learnable convolutions
in MuReNN are dilated accordingly. For a given real-world dataset, we fit the
magnitude response of MuReNN to that of a well-established auditory filterbank:
Gammatone for speech, CQT for music, and third-octave for urban sounds,
respectively. This is a form of knowledge distillation (KD), in which the
filterbank &apos;&apos;teacher&apos;&apos; is engineered by domain knowledge while the neural
network &apos;&apos;student&apos;&apos; is optimized from data. We compare MuReNN to the state of
the art in terms of goodness of fit after KD on a hold-out set and in terms of
Heisenberg time-frequency localization. Compared to convnets and Gabor
convolutions, we find that MuReNN reaches state-of-the-art performance on all
three optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lostanlen_V/0/1/0/all/0/1&quot;&gt;Vincent Lostanlen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haider_D/0/1/0/all/0/1&quot;&gt;Daniel Haider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Han Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagrange_M/0/1/0/all/0/1&quot;&gt;Mathieu Lagrange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balazs_P/0/1/0/all/0/1&quot;&gt;Peter Balazs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehler_M/0/1/0/all/0/1&quot;&gt;Martin Ehler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13824">
<title>Offline Reinforcement Learning with On-Policy Q-Function Regularization. (arXiv:2307.13824v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13824</link>
<description rdf:parseType="Literal">&lt;p&gt;The core challenge of offline reinforcement learning (RL) is dealing with the
(potentially catastrophic) extrapolation error induced by the distribution
shift between the history dataset and the desired policy. A large portion of
prior work tackles this challenge by implicitly/explicitly regularizing the
learning policy towards the behavior policy, which is hard to estimate reliably
in practice. In this work, we propose to regularize towards the Q-function of
the behavior policy instead of the behavior policy itself, under the premise
that the Q-function can be estimated more reliably and easily by a SARSA-style
estimate and handles the extrapolation error more straightforwardly. We propose
two algorithms taking advantage of the estimated Q-function through
regularizations, and demonstrate they exhibit strong performance on the D4RL
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Laixi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1&quot;&gt;Robert Dadashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yuejie Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1&quot;&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13837">
<title>Scaling Integer Arithmetic in Probabilistic Programs. (arXiv:2307.13837v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13837</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributions on integers are ubiquitous in probabilistic modeling but remain
challenging for many of today&apos;s probabilistic programming languages (PPLs). The
core challenge comes from discrete structure: many of today&apos;s PPL inference
strategies rely on enumeration, sampling, or differentiation in order to scale,
which fail for high-dimensional complex discrete distributions involving
integers. Our insight is that there is structure in arithmetic that these
approaches are not using. We present a binary encoding strategy for discrete
distributions that exploits the rich logical structure of integer operations
like summation and comparison. We leverage this structured encoding with
knowledge compilation to perform exact probabilistic inference, and show that
this approach scales to much larger integer distributions with arithmetic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;William X. Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_P/0/1/0/all/0/1&quot;&gt;Poorva Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjoa_R/0/1/0/all/0/1&quot;&gt;Ryan Tjoa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holtzen_S/0/1/0/all/0/1&quot;&gt;Steven Holtzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Millstein_T/0/1/0/all/0/1&quot;&gt;Todd Millstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1&quot;&gt;Guy Van den Broeck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13850">
<title>MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13850</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding multimodal perception for embodied AI is an open question
because such inputs may contain highly complementary as well as redundant
information for the task. A relevant direction for multimodal policies is
understanding the global trends of each modality at the fusion layer. To this
end, we disentangle the attributions for visual, language, and previous action
inputs across different policies trained on the ALFRED dataset. Attribution
analysis can be utilized to rank and group the failure scenarios, investigate
modeling and dataset biases, and critically analyze multimodal EAI policies for
robustness and user trust before deployment. We present MAEA, a framework to
compute global attributions per modality of any differentiable policy. In
addition, we show how attributions enable lower-level behavior analysis in EAI
policies for language and visual attributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamarapalli_J/0/1/0/all/0/1&quot;&gt;Jayant Sravan Tamarapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerramilli_S/0/1/0/all/0/1&quot;&gt;Sahiti Yerramilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13854">
<title>WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.13854</link>
<description rdf:parseType="Literal">&lt;p&gt;With generative AI advances, the exciting potential for autonomous agents to
manage daily tasks via natural language commands has emerged. However, cur rent
agents are primarily created and tested in simplified synthetic environments,
substantially limiting real-world scenario representation. In this paper, we
build an environment for agent command and control that is highly realistic and
reproducible. Specifically, we focus on agents that perform tasks on websites,
and we create an environment with fully functional websites from four common
domains: e-commerce, social forum discussions, collaborative software
development, and content management. Our environment is enriched with tools
(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage
human-like task-solving. Building upon our environment, we release a set of
benchmark tasks focusing on evaluating the functional correctness of task
completions. The tasks in our benchmark are diverse, long-horizon, and are
designed to emulate tasks that humans routinely perform on the internet. We
design and implement several autonomous agents, integrating recent techniques
such as reasoning before acting. The results demonstrate that solving complex
tasks is challenging: our best GPT-4-based agent only achieves an end-to-end
task success rate of 10.59%. These results highlight the need for further
development of robust agents, that current state-of-the-art LMs are far from
perfect performance in these real-life tasks, and that WebArena can be used to
measure such progress. Our code, data, environment reproduction resources, and
video demonstrations are publicly available at https://webarena.dev/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Frank F. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuhui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_R/0/1/0/all/0/1&quot;&gt;Robert Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Abishek Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xianyi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1&quot;&gt;Daniel Fried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1&quot;&gt;Uri Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13886">
<title>Dynamic Grouping for Climate Change Negotiation: Facilitating Cooperation and Balancing Interests through Effective Strategies. (arXiv:2307.13886v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.13886</link>
<description rdf:parseType="Literal">&lt;p&gt;The current framework for climate change negotiation models presents several
limitations that warrant further research and development. In this track, we
discuss mainly two key areas for improvement, focusing on the geographical
impacts and utility framework. In the aspects of geographical impacts, We
explore five critical aspects: (1) the shift from local to global impact, (2)
variability in climate change effects across regions, (3) heterogeneity in
geographical location and political structures, and (4) collaborations between
adjacent nations, (5) the importance of including historical and cultural
factors influencing climate negotiations. Furthermore, we emphasize the need to
refine the utility and rewards framework to reduce the homogeneity and the
level of overestimating the climate mitigation by integrating the positive
effects of saving rates into the reward function and heterogeneity among all
regions. By addressing these limitations, we hope to enhance the accuracy and
effectiveness of climate change negotiation models, enabling policymakers and
stakeholders to devise targeted and appropriate strategies to tackle climate
change at both regional and global levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Duo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yuren Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yu Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13892">
<title>AI4GCC - Team: Below Sea Level: Score and Real World Relevance. (arXiv:2307.13892v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.13892</link>
<description rdf:parseType="Literal">&lt;p&gt;As our submission for track three of the AI for Global Climate Cooperation
(AI4GCC) competition, we propose a negotiation protocol for use in the RICE-N
climate-economic simulation. Our proposal seeks to address the challenges of
carbon leakage through methods inspired by the Carbon Border Adjustment
Mechanism (CBAM) and Climate Clubs (CC). We demonstrate the effectiveness of
our approach by comparing simulated outcomes to representative concentration
pathways (RCP) and shared socioeconomic pathways (SSP). Our protocol results in
a temperature rise comparable to RCP 3.4/4.5 and SSP 2. Furthermore, we provide
an analysis of our protocol&apos;s World Trade Organization compliance,
administrative and political feasibility, and ethical concerns. We recognize
that our proposal risks hurting the least developing countries, and we suggest
specific corrective measures to avoid exacerbating existing inequalities, such
as technology sharing and wealth redistribution. Future research should improve
the RICE-N tariff mechanism and implement actions allowing for the
aforementioned corrective measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozny_P/0/1/0/all/0/1&quot;&gt;Phillip Wozny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renting_B/0/1/0/all/0/1&quot;&gt;Bram Renting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftin_R/0/1/0/all/0/1&quot;&gt;Robert Loftin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wieners_C/0/1/0/all/0/1&quot;&gt;Claudia Wieners&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acar_E/0/1/0/all/0/1&quot;&gt;Erman Acar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13893">
<title>Dynamic Grouping for Climate Change Negotiation: Facilitating Cooperation and Balancing Interests through Effective Strategies. (arXiv:2307.13893v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.13893</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a dynamic grouping negotiation model for climate
mitigation based on real-world business and political negotiation protocols.
Within the AI4GCC competition framework, we develop a three-stage process:
group formation and updates, intra-group negotiation, and inter-group
negotiation. Our model promotes efficient and effective cooperation between
various stakeholders to achieve global climate change objectives. By
implementing a group-forming method and group updating strategy, we address the
complexities and imbalances in multi-region climate negotiations. Intra-group
negotiations ensure that all members contribute to mitigation efforts, while
inter-group negotiations use the proposal-evaluation framework to set
mitigation and savings rates. We demonstrate our negotiation model within the
RICE-N framework, illustrating a promising approach for facilitating
international cooperation on climate change mitigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yu Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Duo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yuren Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13894">
<title>AI4GCC - Team: Below Sea Level: Critiques and Improvements. (arXiv:2307.13894v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.13894</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a critical analysis of the simulation framework RICE-N, an
integrated assessment model (IAM) for evaluating the impacts of climate change
on the economy. We identify key issues with RICE-N, including action masking
and irrelevant actions, and suggest improvements such as utilizing tariff
revenue and penalizing overproduction. We also critically engage with features
of IAMs in general, namely overly optimistic damage functions and unrealistic
abatement cost functions. Our findings contribute to the ongoing efforts to
further develop the RICE-N framework in an effort to improve the simulation,
making it more useful as an inspiration for policymakers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renting_B/0/1/0/all/0/1&quot;&gt;Bram Renting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozny_P/0/1/0/all/0/1&quot;&gt;Phillip Wozny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftin_R/0/1/0/all/0/1&quot;&gt;Robert Loftin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wieners_C/0/1/0/all/0/1&quot;&gt;Claudia Wieners&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acar_E/0/1/0/all/0/1&quot;&gt;Erman Acar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13899">
<title>Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13899</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates methods for improving generative data augmentation
for deep learning. Generative data augmentation leverages the synthetic samples
produced by generative models as an additional dataset for classification with
small dataset settings. A key challenge of generative data augmentation is that
the synthetic data contain uninformative samples that degrade accuracy. This is
because the synthetic samples do not perfectly represent class categories in
real data and uniform sampling does not necessarily provide useful samples for
tasks. In this paper, we present a novel strategy for generative data
augmentation called meta generative regularization (MGR). To avoid the
degradation of generative data augmentation, MGR utilizes synthetic samples in
the regularization term for feature extractors instead of in the loss function,
e.g., cross-entropy. These synthetic samples are dynamically determined to
minimize the validation losses through meta-learning. We observed that MGR can
avoid the performance degradation of na\&quot;ive generative data augmentation and
boost the baselines. Experiments on six datasets showed that MGR is effective
particularly when datasets are smaller and stably outperforms baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1&quot;&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chijiwa_D/0/1/0/all/0/1&quot;&gt;Daiki Chijiwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1&quot;&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumagai_A/0/1/0/all/0/1&quot;&gt;Atsutoshi Kumagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1&quot;&gt;Hisashi Kashima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13900">
<title>FinTree: Financial Dataset Pretrain Transformer Encoder for Relation Extraction. (arXiv:2307.13900v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.13900</link>
<description rdf:parseType="Literal">&lt;p&gt;We present FinTree, Financial Dataset Pretrain Transformer Encoder for
Relation Extraction. Utilizing an encoder language model, we further pretrain
FinTree on the financial dataset, adapting the model in financial domain tasks.
FinTree stands out with its novel structure that predicts a masked token
instead of the conventional [CLS] token, inspired by the Pattern Exploiting
Training methodology. This structure allows for more accurate relation
predictions between two given entities. The model is trained with a unique
input pattern to provide contextual and positional information about the
entities of interest, and a post-processing step ensures accurate predictions
in line with the entity types. Our experiments demonstrate that FinTree
outperforms on the REFinD, a large-scale financial relation extraction dataset.
The code and pretrained models are available at
https://github.com/HJ-Ok/FinTree.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ok_H/0/1/0/all/0/1&quot;&gt;Hyunjong Ok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13907">
<title>Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input. (arXiv:2307.13907v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13907</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven, neural network (NN) based anomaly detection and predictive
maintenance are emerging research areas. NN-based analytics of time-series data
offer valuable insights into past behaviors and estimates of critical
parameters like remaining useful life (RUL) of equipment and state-of-charge
(SOC) of batteries. However, input time series data can be exposed to
intentional or unintentional noise when passing through sensors, necessitating
robust validation and verification of these NNs. This paper presents a case
study of the robustness verification approach for time series regression NNs
(TSRegNN) using set-based formal methods. It focuses on utilizing
variable-length input data to streamline input manipulation and enhance network
architecture generalizability. The method is applied to two data sets in the
Prognostics and Health Management (PHM) application areas: (1) SOC estimation
of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs&apos;
robustness is checked using star-based reachability analysis, and several
performance measures evaluate the effect of bounded perturbations in the input
on network outputs, i.e., future outcomes. Overall, the paper offers a
comprehensive case study for validating and verifying NN-based analytics of
time-series data in real-world applications, emphasizing the importance of
robustness testing for accurate and reliable predictions, especially
considering the impact of noise on future outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_N/0/1/0/all/0/1&quot;&gt;Neelanjana Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_D/0/1/0/all/0/1&quot;&gt;Diego Manzanas Lopez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1&quot;&gt;Taylor T Johnson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13912">
<title>Embedding Democratic Values into Social Media AIs via Societal Objective Functions. (arXiv:2307.13912v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.13912</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we design artificial intelligence (AI) systems that rank our social media
feeds to consider democratic values such as mitigating partisan animosity as
part of their objective functions? We introduce a method for translating
established, vetted social scientific constructs into AI objective functions,
which we term societal objective functions, and demonstrate the method with
application to the political science construct of anti-democratic attitudes.
Traditionally, we have lacked observable outcomes to use to train such models,
however, the social sciences have developed survey instruments and qualitative
codebooks for these constructs, and their precision facilitates translation
into detailed prompts for large language models. We apply this method to create
a democratic attitude model that estimates the extent to which a social media
post promotes anti-democratic attitudes, and test this democratic attitude
model across three studies. In Study 1, we first test the attitudinal and
behavioral effectiveness of the intervention among US partisans (N=1,380) by
manually annotating (alpha=.895) social media posts with anti-democratic
attitude scores and testing several feed ranking conditions based on these
scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants&apos;
partisan animosity without compromising their experience and engagement. In
Study 2, we scale up the manual labels by creating the democratic attitude
model, finding strong agreement with manual labels (rho=.75). Finally, in Study
3, we replicate Study 1 using the democratic attitude model instead of manual
labels to test its attitudinal and behavioral impact (N=558), and again find
that the feed downranking using the societal objective function reduced
partisan animosity (d=.25). This method presents a novel strategy to draw on
social science theory and methods to mitigate societal harms in social media
AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chenyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1&quot;&gt;Michelle S. Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_M/0/1/0/all/0/1&quot;&gt;Minh Chau Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hancock_J/0/1/0/all/0/1&quot;&gt;Jeff Hancock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1&quot;&gt;Michael S. Bernstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13922">
<title>Stability of Multi-Agent Learning: Convergence in Network Games with Many Players. (arXiv:2307.13922v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.13922</link>
<description rdf:parseType="Literal">&lt;p&gt;The behaviour of multi-agent learning in many player games has been shown to
display complex dynamics outside of restrictive examples such as network
zero-sum games. In addition, it has been shown that convergent behaviour is
less likely to occur as the number of players increase. To make progress in
resolving this problem, we study Q-Learning dynamics and determine a sufficient
condition for the dynamics to converge to a unique equilibrium in any network
game. We find that this condition depends on the nature of pairwise
interactions and on the network structure, but is explicitly independent of the
total number of agents in the game. We evaluate this result on a number of
representative network games and show that, under suitable network conditions,
stable learning dynamics can be achieved with an arbitrary number of agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Aamal Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonte_D/0/1/0/all/0/1&quot;&gt;Dan Leonte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belardinelli_F/0/1/0/all/0/1&quot;&gt;Francesco Belardinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1&quot;&gt;Georgios Piliouras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13944">
<title>Entropy Neural Estimation for Graph Contrastive Learning. (arXiv:2307.13944v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13944</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning on graphs aims at extracting distinguishable high-level
representations of nodes. In this paper, we theoretically illustrate that the
entropy of a dataset can be approximated by maximizing the lower bound of the
mutual information across different views of a graph, \ie, entropy is estimated
by a neural network. Based on this finding, we propose a simple yet effective
subset sampling strategy to contrast pairwise representations between views of
a dataset. In particular, we randomly sample nodes and edges from a given graph
to build the input subset for a view. Two views are fed into a parameter-shared
Siamese network to extract the high-dimensional embeddings and estimate the
information entropy of the entire graph. For the learning process, we propose
to optimize the network using two objectives, simultaneously. Concretely, the
input of the contrastive loss function consists of positive and negative pairs.
Our selection strategy of pairs is different from previous works and we present
a novel strategy to enhance the representation ability of the graph encoder by
selecting nodes based on cross-view similarities. We enrich the diversity of
the positive and negative pairs by selecting highly similar samples and totally
different data with the guidance of cross-view similarity scores, respectively.
We also introduce a cross-view consistency constraint on the representations
generated from the different views. This objective guarantees the learned
representations are consistent across views from the perspective of the entire
graph. We conduct extensive experiments on seven graph benchmarks, and the
proposed approach achieves competitive performance compared to the current
state-of-the-art methods. The source code will be publicly released once this
paper is accepted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yixuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1&quot;&gt;Kun Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13945">
<title>Learning-based Control for PMSM Using Distributed Gaussian Processes with Optimal Aggregation Strategy. (arXiv:2307.13945v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.13945</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing demand for accurate control in varying and unknown environments
has sparked a corresponding increase in the requirements for power supply
components, including permanent magnet synchronous motors (PMSMs). To infer the
unknown part of the system, machine learning techniques are widely employed,
especially Gaussian process regression (GPR) due to its flexibility of
continuous system modeling and its guaranteed performance. For practical
implementation, distributed GPR is adopted to alleviate the high computational
complexity. However, the study of distributed GPR from a control perspective
remains an open problem. In this paper, a control-aware optimal aggregation
strategy of distributed GPR for PMSMs is proposed based on the Lyapunov
stability theory. This strategy exclusively leverages the posterior mean,
thereby obviating the need for computationally intensive calculations
associated with posterior variance in alternative approaches. Moreover, the
straightforward calculation process of our proposed strategy lends itself to
seamless implementation in high-frequency PMSM control. The effectiveness of
the proposed strategy is demonstrated in the simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhenxiao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiaobing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zewen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hattab_G/0/1/0/all/0/1&quot;&gt;Georges Hattab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13949">
<title>How Does Diffusion Influence Pretrained Language Models on Out-of-Distribution Data?. (arXiv:2307.13949v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.13949</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based pretrained language models (PLMs) have achieved great
success in modern NLP. An important advantage of PLMs is good
out-of-distribution (OOD) robustness. Recently, diffusion models have attracted
a lot of work to apply diffusion to PLMs. It remains under-explored how
diffusion influences PLMs on OOD data. The core of diffusion models is a
forward diffusion process which gradually applies Gaussian noise to inputs, and
a reverse denoising process which removes noise. The noised input
reconstruction is a fundamental ability of diffusion models. We directly
analyze OOD robustness by measuring the reconstruction loss, including testing
the abilities to reconstruct OOD data, and to detect OOD samples. Experiments
are conducted by analyzing different training parameters and data statistical
features on eight datasets. It shows that finetuning PLMs with diffusion
degrades the reconstruction ability on OOD data. The comparison also shows that
diffusion models can effectively detect OOD samples, achieving state-of-the-art
performance in most of the datasets with an absolute accuracy improvement up to
18%. These results indicate that diffusion reduces OOD robustness of PLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huazheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Daixuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haifeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1&quot;&gt;Qi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jianxin Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13962">
<title>Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13962</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we measure the linear separability of hidden layer outputs to
study the characteristics of deep neural networks. In particular, we first
propose Minkowski difference based linear separability measures (MD-LSMs) to
evaluate the linear separability degree of two points sets. Then, we
demonstrate that there is a synchronicity between the linear separability
degree of hidden layer outputs and the network training performance, i.e., if
the updated weights can enhance the linear separability degree of hidden layer
outputs, the updated network will achieve a better training performance, and
vice versa. Moreover, we study the effect of activation function and network
size (including width and depth) on the linear separability of hidden layers.
Finally, we conduct the numerical experiments to validate our findings on some
popular deep networks including multilayer perceptron (MLP), convolutional
neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet,
vision transformer (ViT) and GoogLeNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wensheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lixue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13978">
<title>Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation. (arXiv:2307.13978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13978</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to
generate realistic outputs based on training datasets. However, the challenge
of exerting control over the generation process of GANs remains a significant
hurdle. In this paper, we propose a novel methodology to address this issue by
integrating a reinforcement learning (RL) agent with a latent-space GAN
(l-GAN), thereby facilitating the generation of desired outputs. More
specifically, we have developed an actor-critic RL agent with a meticulously
designed reward policy, enabling it to acquire proficiency in navigating the
latent space of the l-GAN and generating outputs based on specified tasks. To
substantiate the efficacy of our approach, we have conducted a series of
experiments employing the MNIST dataset, including arithmetic addition as an
illustrative task. The outcomes of these experiments serve to validate our
methodology. Our pioneering integration of an RL agent with a GAN model
represents a novel advancement, holding great potential for enhancing
generative networks in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasian_M/0/1/0/all/0/1&quot;&gt;Mahyar Abbasian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabzadeh_T/0/1/0/all/0/1&quot;&gt;Taha Rajabzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradipari_A/0/1/0/all/0/1&quot;&gt;Ahmadreza Moradipari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1&quot;&gt;Seyed Amir Hossein Aqajari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongsheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1&quot;&gt;Amir Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14010">
<title>ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution. (arXiv:2307.14010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14010</link>
<description rdf:parseType="Literal">&lt;p&gt;Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a
high-resolution hyperspectral image from a low-resolution observation. However,
the prevailing CNN-based approaches have shown limitations in building
long-range dependencies and capturing interaction information between spectral
features. This results in inadequate utilization of spectral information and
artifacts after upsampling. To address this issue, we propose ESSAformer, an
ESSA attention-embedded Transformer network for single-HSI-SR with an iterative
refining structure. Specifically, we first introduce a robust and
spectral-friendly similarity metric, \ie, the spectral correlation coefficient
of the spectrum (SCC), to replace the original attention matrix and
incorporates inductive biases into the model to facilitate training. Built upon
it, we further utilize the kernelizable attention technique with theoretical
support to form a novel efficient SCC-kernel-based self-attention (ESSA) and
reduce attention computation to linear complexity. ESSA enlarges the receptive
field for features after upsampling without bringing much computation and
allows the model to effectively utilize spatial-spectral information from
different scales, resulting in the generation of more natural high-resolution
images. Without the need for pretraining on large-scale datasets, our
experiments demonstrate ESSA&apos;s effectiveness in both visual quality and
quantitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingjin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14019">
<title>One-Nearest Neighborhood Guides Inlier Estimation for Unsupervised Point Cloud Registration. (arXiv:2307.14019v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14019</link>
<description rdf:parseType="Literal">&lt;p&gt;The precision of unsupervised point cloud registration methods is typically
limited by the lack of reliable inlier estimation and self-supervised signal,
especially in partially overlapping scenarios. In this paper, we propose an
effective inlier estimation method for unsupervised point cloud registration by
capturing geometric structure consistency between the source point cloud and
its corresponding reference point cloud copy. Specifically, to obtain a high
quality reference point cloud copy, an One-Nearest Neighborhood (1-NN) point
cloud is generated by input point cloud. This facilitates matching map
construction and allows for integrating dual neighborhood matching scores of
1-NN point cloud and input point cloud to improve matching confidence.
Benefiting from the high quality reference copy, we argue that the neighborhood
graph formed by inlier and its neighborhood should have consistency between
source point cloud and its corresponding reference copy. Based on this
observation, we construct transformation-invariant geometric structure
representations and capture geometric structure consistency to score the inlier
confidence for estimated correspondences between source point cloud and its
reference copy. This strategy can simultaneously provide the reliable
self-supervised signal for model optimization. Finally, we further calculate
transformation estimation by the weighted SVD algorithm with the estimated
correspondences and corresponding inlier confidence. We train the proposed
model in an unsupervised manner, and extensive experiments on synthetic and
real-world datasets illustrate the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yongzhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Maoguo Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1&quot;&gt;Qiguang Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1&quot;&gt;A. K. Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14057">
<title>Open Image Content Disarm And Reconstruction. (arXiv:2307.14057v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.14057</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advance in malware technology, attackers create new ways to hide
their malicious code from antivirus services. One way to obfuscate an attack is
to use common files as cover to hide the malicious scripts, so the malware will
look like a legitimate file. Although cutting-edge Artificial Intelligence and
content signature exist, evasive malware successfully bypasses next-generation
malware detection using advanced methods like steganography. Some of the files
commonly used to hide malware are image files (e.g., JPEG). In addition, some
malware use steganography to hide malicious scripts or sensitive data in
images. Steganography in images is difficult to detect even with specialized
tools. Image-based attacks try to attack the user&apos;s device using malicious
payloads or utilize image steganography to hide sensitive data inside
legitimate images and leak it outside the user&apos;s device. Therefore in this
paper, we present a novel Image Content Disarm and Reconstruction (ICDR). Our
ICDR system removes potential malware, with a zero trust approach, while
maintaining high image quality and file usability. By extracting the image
data, removing it from the rest of the file, and manipulating the image pixels,
it is possible to disable or remove the hidden malware inside the file.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belkind_E/0/1/0/all/0/1&quot;&gt;Eli Belkind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubin_R/0/1/0/all/0/1&quot;&gt;Ran Dubin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvir_A/0/1/0/all/0/1&quot;&gt;Amit Dvir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14085">
<title>Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks. (arXiv:2307.14085v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14085</link>
<description rdf:parseType="Literal">&lt;p&gt;We study reinforcement learning (RL) for learning a Quantal Stackelberg
Equilibrium (QSE) in an episodic Markov game with a leader-follower structure.
In specific, at the outset of the game, the leader announces her policy to the
follower and commits to it. The follower observes the leader&apos;s policy and, in
turn, adopts a quantal response policy by solving an entropy-regularized policy
optimization problem induced by leader&apos;s policy. The goal of the leader is to
find her optimal policy, which yields the optimal expected total return, by
interacting with the follower and learning from data. A key challenge of this
problem is that the leader cannot observe the follower&apos;s reward, and needs to
infer the follower&apos;s quantal response model from his actions against leader&apos;s
policies. We propose sample-efficient algorithms for both the online and
offline settings, in the context of function approximation. Our algorithms are
based on (i) learning the quantal response model via maximum likelihood
estimation and (ii) model-free or model-based RL for solving the leader&apos;s
decision making problem, and we show that they achieve sublinear regret upper
bounds. Moreover, we quantify the uncertainty of these estimators and leverage
the uncertainty to implement optimistic and pessimistic algorithms for online
and offline settings. Besides, when specialized to the linear and myopic
setting, our algorithms are also computationally efficient. Our theoretical
analysis features a novel performance-difference lemma which incorporates the
error of quantal response model, which might be of independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14109">
<title>GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs. (arXiv:2307.14109v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14109</link>
<description rdf:parseType="Literal">&lt;p&gt;GraphRNN is a deep learning-based architecture proposed by You et al. for
learning generative models for graphs. We replicate the results of You et al.
using a reproduced implementation of the GraphRNN architecture and evaluate
this against baseline models using new metrics. Through an ablation study, we
find that the BFS traversal suggested by You et al. to collapse representations
of isomorphic graphs contributes significantly to model performance.
Additionally, we extend GraphRNN to generate directed acyclic graphs by
replacing the BFS traversal with a topological sort. We demonstrate that this
method improves significantly over a directed-multiclass variant of GraphRNN on
a real-world dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1&quot;&gt;Taniya Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_M/0/1/0/all/0/1&quot;&gt;Mark Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravichandran_M/0/1/0/all/0/1&quot;&gt;Maya Ravichandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khatri_N/0/1/0/all/0/1&quot;&gt;Nikhil Khatri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14119">
<title>A semantics-driven methodology for high-quality image annotation. (arXiv:2307.14119v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14119</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in Machine Learning and Computer Vision has highlighted the
presence of various types of systematic flaws inside ground truth object
recognition benchmark datasets. Our basic tenet is that these flaws are rooted
in the many-to-many mappings which exist between the visual information encoded
in images and the intended semantics of the labels annotating them. The net
consequence is that the current annotation process is largely under-specified,
thus leaving too much freedom to the subjective judgment of annotators. In this
paper, we propose vTelos, an integrated Natural Language Processing, Knowledge
Representation, and Computer Vision methodology whose main goal is to make
explicit the (otherwise implicit) intended annotation semantics, thus
minimizing the number and role of subjective choices. A key element of vTelos
is the exploitation of the WordNet lexico-semantic hierarchy as the main means
for providing the meaning of natural language labels and, as a consequence, for
driving the annotation of images based on the objects and the visual properties
they depict. The methodology is validated on images populating a subset of the
ImageNet hierarchy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1&quot;&gt;Mayukh Bagchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1&quot;&gt;Xiaolei Diao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14134">
<title>Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14134</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces and evaluates tiny, mini, small, and medium-sized
uncased Turkish BERT models, aiming to bridge the research gap in
less-resourced languages. We trained these models on a diverse dataset
encompassing over 75GB of text from multiple sources and tested them on several
tasks, including mask prediction, sentiment analysis, news classification, and,
zero-shot classification. Despite their smaller size, our models exhibited
robust performance, including zero-shot task, while ensuring computational
efficiency and faster execution times. Our findings provide valuable insights
into the development and application of smaller language models, especially in
the context of the Turkish language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1&quot;&gt;Himmet Toprak Kesgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuce_M/0/1/0/all/0/1&quot;&gt;Muzaffer Kaan Yuce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1&quot;&gt;Mehmet Fatih Amasyali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14138">
<title>Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.14138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14138</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the piecewise stationary combinatorial semi-bandit problem with
causally related rewards. In our nonstationary environment, variations in the
base arms&apos; distributions, causal relationships between rewards, or both, change
the reward generation process. In such an environment, an optimal
decision-maker must follow both sources of change and adapt accordingly. The
problem becomes aggravated in the combinatorial semi-bandit setting, where the
decision-maker only observes the outcome of the selected bundle of arms. The
core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We
assume the agent relies on an adaptive approach to overcome the challenge. More
specifically, it employs a change-point detector based on the Generalized
Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart
as a new alternative restarting strategy in the decision making process in
structured environments. Finally, our algorithm integrates a mechanism to trace
the variations of the underlying graph structure, which captures the causal
relationships between the rewards in the bandit setting. Theoretically, we
establish a regret upper bound that reflects the effects of the number of
structural- and distribution changes on the performance. The outcome of our
numerical experiments in real-world scenarios exhibits applicability and
superior performance of our proposal compared to the state-of-the-art
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourani_Koliji_B/0/1/0/all/0/1&quot;&gt;Behzad Nourani-Koliji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilaj_S/0/1/0/all/0/1&quot;&gt;Steven Bilaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balef_A/0/1/0/all/0/1&quot;&gt;Amir Rezaei Balef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1&quot;&gt;Setareh Maghsudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14142">
<title>LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14142</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering (VQA) has been intensively studied as a multimodal
task that requires effort in bridging vision and language to infer answers
correctly. Recent attempts have developed various attention-based modules for
solving VQA tasks. However, the performance of model inference is largely
bottlenecked by visual processing for semantics understanding. Most existing
detection methods rely on bounding boxes, remaining a serious challenge for VQA
models to understand the causal nexus of object semantics in images and
correctly infer contextual information. To this end, we propose a finer model
framework without bounding boxes in this work, termed Looking Out of Instance
Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained
feature descriptions to produce visual facts. Furthermore, to overcome the
label ambiguity caused by instance masks, two types of relation attention
modules: 1) intra-modality and 2) inter-modality, are devised to infer the
correct answers from the different multi-view features. Specifically, we
implement a mutual relation attention module to model sophisticated and deeper
visual semantic relations between instance objects and background information.
In addition, our proposed attention model can further analyze salient image
regions by focusing on important word-related questions. Experimental results
on four benchmark VQA datasets prove that our proposed method has favorable
performance in improving visual reasoning capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yeming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yaoru Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haibo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14192">
<title>Unveiling Security, Privacy, and Ethical Concerns of ChatGPT. (arXiv:2307.14192v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.14192</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into the realm of ChatGPT, an AI-powered chatbot that
utilizes topic modeling and reinforcement learning to generate natural
responses. Although ChatGPT holds immense promise across various industries,
such as customer service, education, mental health treatment, personal
productivity, and content creation, it is essential to address its security,
privacy, and ethical implications. By exploring the upgrade path from GPT-1 to
GPT-4, discussing the model&apos;s features, limitations, and potential
applications, this study aims to shed light on the potential risks of
integrating ChatGPT into our daily lives. Focusing on security, privacy, and
ethics issues, we highlight the challenges these concerns pose for widespread
adoption. Finally, we analyze the open problems in these areas, calling for
concerted efforts to ensure the development of secure and ethically sound large
language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1&quot;&gt;Ran Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1&quot;&gt;Jianbing Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14206">
<title>AI and Education: An Investigation into the Use of ChatGPT for Systems Thinking. (arXiv:2307.14206v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.14206</link>
<description rdf:parseType="Literal">&lt;p&gt;This exploratory study investigates the potential of the artificial
intelligence tool, ChatGPT, to support systems thinking (ST) in various
subjects. Using both general and subject specific prompts, the study assesses
the accuracy, helpfulness, and reliability of ChatGPT&apos;s responses across
different versions of the tool. The results indicate that ChatGPT can provide
largely correct and very helpful responses in various subjects, demonstrating
its potential as a tool for enhancing ST skills. However, occasional
inaccuracies highlight the need for users to remain critical of ChatGPT&apos;s
responses. Despite some limitations, this study suggests that with careful use
and attention to its idiosyncrasies, ChatGPT can be a valuable tool for
teaching and learning ST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arndt_H/0/1/0/all/0/1&quot;&gt;Holger Arndt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14226">
<title>Explore the possibility of advancing climate negotiations on the basis of regional trade organizations: A study based on RICE-N. (arXiv:2307.14226v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.14226</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate issues have become more and more important now. Although global
governments have made some progress, we are still facing the truth that the
prospect of international cooperation is not clear at present. Due to the
limitations of the Integrated assessment models (IAMs) model, it is difficult
to simulate the dynamic negotiation process. Therefore, using deep learning to
build a new agents based model (ABM) might can provide new theoretical support
for climate negotiations. Building on the RICE-N model, this work proposed an
approach to climate negotiations based on existing trade groups. Simulation
results show that the scheme has a good prospect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wubo Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14232">
<title>Sources of Opacity in Computer Systems: Towards a Comprehensive Taxonomy. (arXiv:2307.14232v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.14232</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern computer systems are ubiquitous in contemporary life yet many of them
remain opaque. This poses significant challenges in domains where desiderata
such as fairness or accountability are crucial. We suggest that the best
strategy for achieving system transparency varies depending on the specific
source of opacity prevalent in a given context. Synthesizing and extending
existing discussions, we propose a taxonomy consisting of eight sources of
opacity that fall into three main categories: architectural, analytical, and
socio-technical. For each source, we provide initial suggestions as to how to
address the resulting opacity in practice. The taxonomy provides a starting
point for requirements engineers and other practitioners to understand
contextually prevalent sources of opacity, and to select or develop appropriate
strategies for overcoming them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_S/0/1/0/all/0/1&quot;&gt;Sara Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crook_B/0/1/0/all/0/1&quot;&gt;Barnaby Crook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kastner_L/0/1/0/all/0/1&quot;&gt;Lena K&amp;#xe4;stner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schomacker_A/0/1/0/all/0/1&quot;&gt;Astrid Schom&amp;#xe4;cker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speith_T/0/1/0/all/0/1&quot;&gt;Timo Speith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14236">
<title>UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text. (arXiv:2307.14236v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14236</link>
<description rdf:parseType="Literal">&lt;p&gt;This demo paper presents UnScientify, an interactive system designed to
detect scientific uncertainty in scholarly full text. The system utilizes a
weakly supervised technique that employs a fine-grained annotation scheme to
identify verbally formulated uncertainty at the sentence level in scientific
texts. The pipeline for the system includes a combination of pattern matching,
complex sentence checking, and authorial reference checking. Our approach
automates labeling and annotation tasks for scientific uncertainty
identification, taking into account different types of scientific uncertainty,
that can serve various applications such as information retrieval, text mining,
and scholarly document processing. Additionally, UnScientify provides
interpretable results, aiding in the comprehension of identified instances of
scientific uncertainty in text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ningrum_P/0/1/0/all/0/1&quot;&gt;Panggih Kusuma Ningrum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1&quot;&gt;Philipp Mayr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanassova_I/0/1/0/all/0/1&quot;&gt;Iana Atanassova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14239">
<title>Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI). (arXiv:2307.14239v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14239</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crook_B/0/1/0/all/0/1&quot;&gt;Barnaby Crook&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schluter_M/0/1/0/all/0/1&quot;&gt;Maximilian Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speith_T/0/1/0/all/0/1&quot;&gt;Timo Speith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14246">
<title>A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI). (arXiv:2307.14246v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14246</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speith_T/0/1/0/all/0/1&quot;&gt;Timo Speith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langer_M/0/1/0/all/0/1&quot;&gt;Markus Langer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14266">
<title>Improving International Climate Policy via Mutually Conditional Binding Commitments. (arXiv:2307.14266v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.14266</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes enhancements to the RICE-N simulation and multi-agent
reinforcement learning framework to improve the realism of international
climate policy negotiations. Acknowledging the framework&apos;s value, we highlight
the necessity of significant enhancements to address the diverse array of
factors in modeling climate negotiations. Building upon our previous work on
the &quot;Conditional Commitments Mechanism&quot; (CCF mechanism) we discuss ways to
bridge the gap between simulation and reality. We suggest the inclusion of a
recommender or planner agent to enhance coordination, address the Real2Sim gap
by incorporating social factors and non-party stakeholder sub-agents, and
propose enhancements to the underlying Reinforcement Learning solution
algorithm. These proposed improvements aim to advance the evaluation and
formulation of negotiation protocols for more effective international climate
policy decision-making in Rice-N. However, further experimentation and testing
are required to determine the implications and effectiveness of these
suggestions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heitzig_J/0/1/0/all/0/1&quot;&gt;Jobst Heitzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oechssler_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Oechssler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Proschel_C/0/1/0/all/0/1&quot;&gt;Christoph Pr&amp;#xf6;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragavan_N/0/1/0/all/0/1&quot;&gt;Niranjana Ragavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_R/0/1/0/all/0/1&quot;&gt;Richie YatLong Lo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14267">
<title>Improving International Climate Policy via Mutually Conditional Binding Commitments. (arXiv:2307.14267v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.14267</link>
<description rdf:parseType="Literal">&lt;p&gt;The Paris Agreement, considered a significant milestone in climate
negotiations, has faced challenges in effectively addressing climate change due
to the unconditional nature of most Nationally Determined Contributions (NDCs).
This has resulted in a prevalence of free-riding behavior among major polluters
and a lack of concrete conditionality in NDCs. To address this issue, we
propose the implementation of a decentralized, bottom-up approach called the
Conditional Commitment Mechanism. This mechanism, inspired by the National
Popular Vote Interstate Compact, offers flexibility and incentives for early
adopters, aiming to formalize conditional cooperation in international climate
policy. In this paper, we provide an overview of the mechanism, its performance
in the AI4ClimateCooperation challenge, and discuss potential real-world
implementation aspects. Prior knowledge of the climate mitigation collective
action problem, basic economic principles, and game theory concepts are
assumed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heitzig_J/0/1/0/all/0/1&quot;&gt;Jobst Heitzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oechssler_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Oechssler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Proschel_C/0/1/0/all/0/1&quot;&gt;Christoph Pr&amp;#xf6;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragavan_N/0/1/0/all/0/1&quot;&gt;Niranjana Ragavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1&quot;&gt;Yat Long Lo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14283">
<title>General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14283</link>
<description rdf:parseType="Literal">&lt;p&gt;Most applications of Artificial Intelligence (AI) are designed for a confined
and specific task. However, there are many scenarios that call for a more
general AI, capable of solving a wide array of tasks without being specifically
designed for them. The term General-Purpose Artificial Intelligence Systems
(GPAIS) has been defined to refer to these AI systems. To date, the possibility
of an Artificial General Intelligence, powerful enough to perform any
intellectual task as if it were human, or even improve it, has remained an
aspiration, fiction, and considered a risk for our society. Whilst we might
still be far from achieving that, GPAIS is a reality and sitting at the
forefront of AI research.
&lt;/p&gt;
&lt;p&gt;This work discusses existing definitions for GPAIS and proposes a new
definition that allows for a gradual differentiation among types of GPAIS
according to their properties and limitations. We distinguish between
closed-world and open-world GPAIS, characterising their degree of autonomy and
ability based on several factors such as adaptation to new tasks, competence in
domains not intentionally trained for, ability to learn from few data, or
proactive acknowledgment of their own limitations. We then propose a taxonomy
of approaches to realise GPAIS, describing research trends such as the use of
AI techniques to improve another AI or foundation models. As a prime example,
we delve into generative AI, aligning them with the terms and concepts
presented in the taxonomy. Through the proposed definition and taxonomy, our
aim is to facilitate research collaboration across different areas that are
tackling general-purpose tasks, as they share many common aspects. Finally, we
discuss the current state of GPAIS, its challenges and prospects, implications
for our society, and the need for responsible and trustworthy AI systems and
regulation, with the goal of providing a holistic view of GPAIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triguero_I/0/1/0/all/0/1&quot;&gt;Isaac Triguero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_D/0/1/0/all/0/1&quot;&gt;Daniel Molina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poyatos_J/0/1/0/all/0/1&quot;&gt;Javier Poyatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1&quot;&gt;Francisco Herrera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14294">
<title>Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis. (arXiv:2307.14294v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14294</link>
<description rdf:parseType="Literal">&lt;p&gt;Splitting of sequential data, such as videos and time series, is an essential
step in various data analysis tasks, including object tracking and anomaly
detection. However, splitting sequential data presents a variety of challenges
that can impact the accuracy and reliability of subsequent analyses. This
concept article examines the challenges associated with splitting sequential
data, including data acquisition, data representation, split ratio selection,
setting up quality criteria, and choosing suitable selection strategies. We
explore these challenges through two real-world examples: motor test benches
and particle tracking in liquids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botache_D/0/1/0/all/0/1&quot;&gt;Diego Botache&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dingel_K/0/1/0/all/0/1&quot;&gt;Kristina Dingel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huhnstock_R/0/1/0/all/0/1&quot;&gt;Rico Huhnstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehresmann_A/0/1/0/all/0/1&quot;&gt;Arno Ehresmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14298">
<title>ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality. (arXiv:2307.14298v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.14298</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems have become indispensable tools in the hotel hospitality
industry, enabling personalized and tailored experiences for guests. Recent
advancements in large language models (LLMs), such as ChatGPT, and persuasive
technologies, have opened new avenues for enhancing the effectiveness of those
systems. This paper explores the potential of integrating ChatGPT and
persuasive technologies for automating and improving hotel hospitality
recommender systems. First, we delve into the capabilities of ChatGPT, which
can understand and generate human-like text, enabling more accurate and
context-aware recommendations. We discuss the integration of ChatGPT into
recommender systems, highlighting the ability to analyze user preferences,
extract valuable insights from online reviews, and generate personalized
recommendations based on guest profiles. Second, we investigate the role of
persuasive technology in influencing user behavior and enhancing the persuasive
impact of hotel recommendations. By incorporating persuasive techniques, such
as social proof, scarcity and personalization, recommender systems can
effectively influence user decision-making and encourage desired actions, such
as booking a specific hotel or upgrading their room. To investigate the
efficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment
with a case study involving a hotel recommender system. We aim to study the
impact of integrating ChatGPT and persua-sive techniques on user engagement,
satisfaction, and conversion rates. The preliminary results demonstrate the
potential of these technologies in enhancing the overall guest experience and
business performance. Overall, this paper contributes to the field of hotel
hospitality by exploring the synergistic relationship between LLMs and
persuasive technology in recommender systems, ultimately influencing guest
satisfaction and hotel revenue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remountakis_M/0/1/0/all/0/1&quot;&gt;Manolis Remountakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Kotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kourtzis_B/0/1/0/all/0/1&quot;&gt;Babis Kourtzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsekouras_G/0/1/0/all/0/1&quot;&gt;George E. Tsekouras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14316">
<title>Reinforcement Learning by Guided Safe Exploration. (arXiv:2307.14316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14316</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety is critical to broadening the application of reinforcement learning
(RL). Often, we train RL agents in a controlled environment, such as a
laboratory, before deploying them in the real world. However, the real-world
target task might be unknown prior to deployment. Reward-free RL trains an
agent without the reward to adapt quickly once the reward is revealed. We
consider the constrained reward-free setting, where an agent (the guide) learns
to explore safely without the reward signal. This agent is trained in a
controlled environment, which allows unsafe interactions and still provides the
safety signal. After the target task is revealed, safety violations are not
allowed anymore. Thus, the guide is leveraged to compose a safe behaviour
policy. Drawing from transfer learning, we also regularize a target policy (the
student) towards the guide while the student is unreliable and gradually
eliminate the influence of the guide as training progresses. The empirical
analysis shows that this method can achieve safe transfer learning and helps
the student solve the target task faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qisong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simao_T/0/1/0/all/0/1&quot;&gt;Thiago D. Sim&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_N/0/1/0/all/0/1&quot;&gt;Nils Jansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tindemans_S/0/1/0/all/0/1&quot;&gt;Simon H. Tindemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spaan_M/0/1/0/all/0/1&quot;&gt;Matthijs T. J. Spaan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14324">
<title>Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14324</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a case study on the design, administration,
post-processing, and evaluation of surveys on large language models (LLMs). It
comprises two components: (1) A statistical method for eliciting beliefs
encoded in LLMs. We introduce statistical measures and evaluation metrics that
quantify the probability of an LLM &quot;making a choice&quot;, the associated
uncertainty, and the consistency of that choice. (2) We apply this method to
study what moral beliefs are encoded in different LLMs, especially in ambiguous
cases where the right choice is not obvious. We design a large-scale survey
comprising 680 high-ambiguity moral scenarios (e.g., &quot;Should I tell a white
lie?&quot;) and 687 low-ambiguity moral scenarios (e.g., &quot;Should I stop for a
pedestrian on the road?&quot;). Each scenario includes a description, two possible
actions, and auxiliary labels indicating violated rules (e.g., &quot;do not kill&quot;).
We administer the survey to 28 open- and closed-source LLMs. We find that (a)
in unambiguous scenarios, most models &quot;choose&quot; actions that align with
commonsense. In ambiguous cases, most models express uncertainty. (b) Some
models are uncertain about choosing the commonsense action because their
responses are sensitive to the question-wording. (c) Some models reflect clear
preferences in ambiguous scenarios. Specifically, closed-source models tend to
agree with each other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherrer_N/0/1/0/all/0/1&quot;&gt;Nino Scherrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Claudia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1&quot;&gt;Amir Feder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14326">
<title>Waypoint-Based Imitation Learning for Robotic Manipulation. (arXiv:2307.14326v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.14326</link>
<description rdf:parseType="Literal">&lt;p&gt;While imitation learning methods have seen a resurgent interest for robotic
manipulation, the well-known problem of compounding errors continues to afflict
behavioral cloning (BC). Waypoints can help address this problem by reducing
the horizon of the learning problem for BC, and thus, the errors compounded
over time. However, waypoint labeling is underspecified, and requires
additional human supervision. Can we generate waypoints automatically without
any additional human supervision? Our key insight is that if a trajectory
segment can be approximated by linear motion, the endpoints can be used as
waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation
learning, a preprocessing module to decompose a demonstration into a minimal
set of waypoints which when interpolated linearly can approximate the
trajectory up to a specified error threshold. AWE can be combined with any BC
algorithm, and we find that AWE can increase the success rate of
state-of-the-art algorithms by up to 25% in simulation and by 4-28% on
real-world bimanual manipulation tasks, reducing the decision making horizon by
up to a factor of 10. Videos and code are available at
https://lucys0.github.io/awe/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lucy Xiaoyang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Archit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tony Z. Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14332">
<title>Event-based Vision for Early Prediction of Manipulation Actions. (arXiv:2307.14332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14332</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic visual sensors are artificial retinas that output sequences of
asynchronous events when brightness changes occur in the scene. These sensors
offer many advantages including very high temporal resolution, no motion blur
and smart data compression ideal for real-time processing. In this study, we
introduce an event-based dataset on fine-grained manipulation actions and
perform an experimental study on the use of transformers for action prediction
with events. There is enormous interest in the fields of cognitive robotics and
human-robot interaction on understanding and predicting human actions as early
as possible. Early prediction allows anticipating complex stages for planning,
enabling effective and real-time interaction. Our Transformer network uses
events to predict manipulation actions as they occur, using online inference.
The model succeeds at predicting actions early on, building up confidence over
time and achieving state-of-the-art classification. Moreover, the
attention-based transformer architecture allows us to study the role of the
spatio-temporal patterns selected by the model. Our experiments show that the
Transformer network captures action dynamic features outperforming video-based
approaches and succeeding with scenarios where the differences between actions
lie in very subtle cues. Finally, we release the new event dataset, which is
the first in the literature for manipulation action recognition. Code will be
available at https://github.com/DaniDeniz/EventVisionTransformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deniz_D/0/1/0/all/0/1&quot;&gt;Daniel Deniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1&quot;&gt;Cornelia Fermuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ros_E/0/1/0/all/0/1&quot;&gt;Eduardo Ros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Alvarez_M/0/1/0/all/0/1&quot;&gt;Manuel Rodriguez-Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barranco_F/0/1/0/all/0/1&quot;&gt;Francisco Barranco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14335">
<title>WavJourney: Compositional Audio Creation with Large Language Models. (arXiv:2307.14335v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.14335</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown great promise in integrating diverse
expert models to tackle intricate language and vision tasks. Despite their
significance in advancing the field of Artificial Intelligence Generated
Content (AIGC), their potential in intelligent audio content creation remains
unexplored. In this work, we tackle the problem of creating audio content with
storylines encompassing speech, music, and sound effects, guided by text
instructions. We present WavJourney, a system that leverages LLMs to connect
various audio models for audio content generation. Given a text description of
an auditory scene, WavJourney first prompts LLMs to generate a structured
script dedicated to audio storytelling. The audio script incorporates diverse
audio elements, organized based on their spatio-temporal relationships. As a
conceptual representation of audio, the audio script provides an interactive
and interpretable rationale for human engagement. Afterward, the audio script
is fed into a script compiler, converting it into a computer program. Each line
of the program calls a task-specific audio generation model or computational
operation function (e.g., concatenate, mix). The computer program is then
executed to obtain an explainable solution for audio generation. We demonstrate
the practicality of WavJourney across diverse real-world scenarios, including
science fiction, education, and radio play. The explainable and interactive
design of WavJourney fosters human-machine co-creation in multi-round
dialogues, enhancing creative control and adaptability in audio production.
WavJourney audiolizes the human imagination, opening up new avenues for
creativity in multimedia content creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xubo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhongkai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haohe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Meng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiushi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jinhua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qiuqiang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1&quot;&gt;Mark D. Plumbley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenwu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.11104">
<title>Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.11104</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding optimal paths in connected graphs requires determining the smallest
total cost for traveling along the graph&apos;s edges. This problem can be solved by
several classical algorithms where, usually, costs are predefined for all
edges. Conventional planning methods can, thus, normally not be used when
wanting to change costs in an adaptive way following the requirements of some
task. Here we show that one can define a neural network representation of path
finding problems by transforming cost values into synaptic weights, which
allows for online weight adaptation using network learning mechanisms. When
starting with an initial activity value of one, activity propagation in this
network will lead to solutions, which are identical to those found by the
Bellman-Ford algorithm. The neural network has the same algorithmic complexity
as Bellman-Ford and, in addition, we can show that network learning mechanisms
(such as Hebbian learning) can adapt the weights in the network augmenting the
resulting paths according to some task at hand. We demonstrate this by learning
to navigate in an environment with obstacles as well as by learning to follow
certain sequences of path nodes. Hence, the here-presented novel algorithm may
open up a different regime of applications where path-augmentation (by
learning) is directly coupled with path finding in a natural way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulvicius_T/0/1/0/all/0/1&quot;&gt;Tomas Kulvicius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamosiunaite_M/0/1/0/all/0/1&quot;&gt;Minija Tamosiunaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worgotter_F/0/1/0/all/0/1&quot;&gt;Florentin W&amp;#xf6;rg&amp;#xf6;tter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.09559">
<title>Priming Cross-Session Motor Imagery Classification with A Universal Deep Domain Adaptation Framework. (arXiv:2202.09559v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.09559</link>
<description rdf:parseType="Literal">&lt;p&gt;Motor imagery (MI) is a common brain computer interface (BCI) paradigm. EEG
is non-stationary with low signal-to-noise, classifying motor imagery tasks of
the same participant from different EEG recording sessions is generally
challenging, as EEG data distribution may vary tremendously among different
acquisition sessions. Although it is intuitive to consider the cross-session MI
classification as a domain adaptation problem, the rationale and feasible
approach is not elucidated. In this paper, we propose a Siamese deep domain
adaptation (SDDA) framework for cross-session MI classification based on
mathematical models in domain adaptation theory. The proposed framework can be
easily applied to most existing artificial neural networks without altering the
network structure, which facilitates our method with great flexibility and
transferability. In the proposed framework, domain invariants were firstly
constructed jointly with channel normalization and Euclidean alignment. Then,
embedding features from source and target domain were mapped into the
Reproducing Kernel Hilbert Space (RKHS) and aligned accordingly. A cosine-based
center loss was also integrated into the framework to improve the
generalizability of the SDDA. The proposed framework was validated with two
classic and popular convolutional neural networks from BCI research field
(EEGNet and ConvNet) in two MI-EEG public datasets (BCI Competition IV IIA,
IIB). Compared to the vanilla EEGNet and ConvNet, the proposed SDDA framework
was able to boost the MI classification accuracy by 15.2%, 10.2% respectively
in IIA dataset, and 5.5%, 4.2% in IIB dataset. The final MI classification
accuracy reached 82.01% in IIA dataset and 87.52% in IIB, which outperformed
the state-of-the-art methods in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_C/0/1/0/all/0/1&quot;&gt;Carlo Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yelong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Meirong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_D/0/1/0/all/0/1&quot;&gt;Dong Ming&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.12645">
<title>Exploring Multi-Modal Representations for Ambiguity Detection &amp; Coreference Resolution in the SIMMC 2.0 Challenge. (arXiv:2202.12645v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2202.12645</link>
<description rdf:parseType="Literal">&lt;p&gt;Anaphoric expressions, such as pronouns and referential descriptions, are
situated with respect to the linguistic context of prior turns, as well as, the
immediate visual environment. However, a speaker&apos;s referential descriptions do
not always uniquely identify the referent, leading to ambiguities in need of
resolution through subsequent clarificational exchanges. Thus, effective
Ambiguity Detection and Coreference Resolution are key to task success in
Conversational AI. In this paper, we present models for these two tasks as part
of the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT
and LXMERT based models, compare them to a number of baselines and provide
ablation experiments. Our results show that (1) language models are able to
exploit correlations in the data to detect ambiguity; and (2) unimodal
coreference resolution models can avoid the need for a vision component,
through the use of smart object representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiyah_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Chiyah-Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1&quot;&gt;Alessandro Suglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopes_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Lopes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eshghi_A/0/1/0/all/0/1&quot;&gt;Arash Eshghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1&quot;&gt;Helen Hastie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.04317">
<title>MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.04317</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration is the process of bringing different images into a common
coordinate system - a technique widely used in various applications of computer
vision, such as remote sensing, image retrieval, and, most commonly, medical
imaging. Deep learning based techniques have been applied successfully to
tackle various complex medical image processing problems, including medical
image registration. Over the years, several image registration techniques have
been proposed using deep learning. Deformable image registration techniques
such as Voxelmorph have been successful in capturing finer changes and
providing smoother deformations. However, Voxelmorph, as well as ICNet and
FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical
view of the supplied image) and, therefore, cannot track large deformations. In
order to tackle the aforementioned problems, this paper extends the Voxelmorph
approach in three different ways. To improve the performance in case of small
as well as large deformations, supervision of the model at different
resolutions has been integrated using a multi-scale UNet. To support the
network to learn and encode the minute structural co-relations of the given
image-pairs, a self-constructing graph network (SCGNet) has been used as the
latent of the multi-scale UNet - which can improve the learning process of the
model and help the model to generalise better. And finally, to make the
deformations inverse-consistent, cycle consistency loss has been employed. On
the task of registration of brain MRIs, the proposed method achieved
significant improvements over ANTs and VoxelMorph, obtaining a Dice score of
0.8013 \pm 0.0243 for intramodal and 0.6211 \pm 0.0309 for intermodal, while
VoxelMorph achieved 0.7747 \pm 0.0260 and 0.6071 \pm 0.0510, respectively
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Soumick Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bajaj_H/0/1/0/all/0/1&quot;&gt;Himanshi Bajaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siddiquee_I/0/1/0/all/0/1&quot;&gt;Istiyak H. Siddiquee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Subbarayappa_N/0/1/0/all/0/1&quot;&gt;Nandish Bandi Subbarayappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Simon_S/0/1/0/all/0/1&quot;&gt;Steve Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shashidhar_S/0/1/0/all/0/1&quot;&gt;Suraj Bangalore Shashidhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1&quot;&gt;Oliver Speck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nurnberge_A/0/1/0/all/0/1&quot;&gt;Andreas N&amp;#xfc;rnberge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10848">
<title>Robust Quantity-Aware Aggregation for Federated Learning. (arXiv:2205.10848v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10848</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their local data, and becomes an important
privacy-preserving machine learning framework. However, classical FL faces
serious security and robustness problem, e.g., malicious clients can poison
model updates and at the same time claim large quantities to amplify the impact
of their model updates in the model aggregation. Existing defense methods for
FL, while all handling malicious model updates, either treat all quantities
benign or simply ignore/truncate the quantities of all clients. The former is
vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal
performance since the local data on different clients is usually in
significantly different sizes. In this paper, we propose a robust
quantity-aware aggregation algorithm for federated learning, called FedRA, to
perform the aggregation with awareness of local data quantities while being
able to defend against quantity-enhanced attacks. More specifically, we propose
a method to filter malicious clients by jointly considering the uploaded model
updates and data quantities from different clients, and performing
quantity-aware weighted averaging on model updates from remaining clients.
Moreover, as the number of malicious clients participating in the federated
learning may dynamically change in different rounds, we also propose a
malicious client number estimator to predict how many suspicious clients should
be filtered in each round. Experiments on four public datasets demonstrate the
effectiveness of our FedRA method in defending FL against quantity-enhanced
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jingwei Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1&quot;&gt;Tao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guangzhong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13619">
<title>Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13619</link>
<description rdf:parseType="Literal">&lt;p&gt;As one of the most pervasive applications of machine learning, recommender
systems are playing an important role on assisting human decision making. The
satisfaction of users and the interests of platforms are closely related to the
quality of the generated recommendation results. However, as a highly
data-driven system, recommender system could be affected by data or algorithmic
bias and thus generate unfair results, which could weaken the reliance of the
systems. As a result, it is crucial to address the potential unfairness
problems in recommendation settings. Recently, there has been growing attention
on fairness considerations in recommender systems with more and more literature
on approaches to promote fairness in recommendation. However, the studies are
rather fragmented and lack a systematic organization, thus making it difficult
to penetrate for new researchers to the domain. This motivates us to provide a
systematic survey of existing works on fairness in recommendation. This survey
focuses on the foundations for fairness in recommendation literature. It first
presents a brief introduction about fairness in basic machine learning tasks
such as classification and ranking in order to provide a general overview of
fairness research, as well as introduce the more complex situations and
challenges that need to be considered when studying fairness in recommender
systems. After that, the survey will introduce fairness in recommendation with
a focus on the taxonomies of current fairness definitions, the typical
techniques for improving fairness, as well as the datasets for fairness studies
in recommendation. The survey also talks about the challenges and opportunities
in fairness research with the hope of promoting the fair recommendation
research area and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanxiong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuchang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02070">
<title>Priors in Deep Image Restoration and Enhancement: A Survey. (arXiv:2206.02070v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02070</link>
<description rdf:parseType="Literal">&lt;p&gt;Image restoration and enhancement is a process of improving the image quality
by removing degradations, such as noise, blur, and resolution degradation. Deep
learning (DL) has recently been applied to image restoration and enhancement.
Due to its ill-posed property, plenty of works have been explored priors to
facilitate training deep neural networks (DNNs). However, the importance of
priors has not been systematically studied and analyzed by far in the research
community. Therefore, this paper serves as the first study that provides a
comprehensive overview of recent advancements in priors for deep image
restoration and enhancement. Our work covers five primary contents: (1) A
theoretical analysis of priors for deep image restoration and enhancement; (2)
A hierarchical and structural taxonomy of priors commonly used in the DL-based
methods; (3) An insightful discussion on each prior regarding its principle,
potential, and applications; (4) A summary of crucial problems by highlighting
the potential future directions, especially adopting the large-scale foundation
models as prior, to spark more research in the community; (5) An open-source
repository that provides a taxonomy of all mentioned works and code links.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yunfan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yunhao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.13803">
<title>FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification. (arXiv:2206.13803v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.13803</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL), training deep models from decentralized data without
privacy leakage, has shown great potential in medical image computing recently.
However, considering the ubiquitous class imbalance in medical data, FL can
exhibit performance degradation, especially for minority classes (e.g. rare
diseases). Existing methods towards this problem mainly focus on training a
balanced classifier to eliminate class prior bias among classes, but neglect to
explore better representation to facilitate classification performance. In this
paper, we present a privacy-preserving FL method named FedIIC to combat class
imbalance from two perspectives: feature learning and classifier learning. In
feature learning, two levels of contrastive learning are designed to extract
better class-specific features with imbalanced data in FL. In classifier
learning, per-class margins are dynamically set according to real-time
difficulty and class priors, which helps the model learn classes equally.
Experimental results on publicly-available datasets demonstrate the superior
performance of FedIIC in dealing with both real-world and simulated
multi-source medical imaging data under class imbalance. Code is available at
https://github.com/wnn2000/FedIIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1&quot;&gt;Nannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zengqiang Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03829">
<title>Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03829</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive review of past and current advances in
the early detection of bark beetle-induced tree mortality from three primary
perspectives: bark beetle &amp;amp; host interactions, RS, and ML/DL. In contrast to
prior efforts, this review encompasses all RS systems and emphasizes ML/DL
methods to investigate their strengths and weaknesses. We parse existing
literature based on multi- or hyper-spectral analyses and distill their
knowledge based on: bark beetle species &amp;amp; attack phases with a primary emphasis
on early stages of attacks, host trees, study regions, RS platforms &amp;amp; sensors,
spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation
indices (SVIs), ML approaches, learning schemes, task categories, models,
algorithms, classes/clusters, features, and DL networks &amp;amp; architectures.
Although DL-based methods and the random forest (RF) algorithm showed promising
results, highlighting their potential to detect subtle changes across visible,
thermal, and short-wave infrared (SWIR) spectral regions, they still have
limited effectiveness and high uncertainties. To inspire novel solutions to
these shortcomings, we delve into the principal challenges &amp;amp; opportunities from
different perspectives, enabling a deeper understanding of the current state of
research and guiding future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1&quot;&gt;Devin Goodsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1&quot;&gt;Nadir Erbilgin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15595">
<title>FsaNet: Frequency Self-attention for Semantic Segmentation. (arXiv:2211.15595v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15595</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering the spectral properties of images, we propose a new
self-attention mechanism with highly reduced computational complexity, up to a
linear rate. To better preserve edges while promoting similarity within
objects, we propose individualized processes over different frequency bands. In
particular, we study a case where the process is merely over low-frequency
components. By ablation study, we show that low frequency self-attention can
achieve very close or better performance relative to full frequency even
without retraining the network. Accordingly, we design and embed novel
plug-and-play modules to the head of a CNN network that we refer to as FsaNet.
The frequency self-attention 1) requires only a few low frequency coefficients
as input, 2) can be mathematically equivalent to spatial domain self-attention
with linear structures, 3) simplifies token mapping ($1\times1$ convolution)
stage and token mixing stage simultaneously. We show that frequency
self-attention requires $87.29\% \sim 90.04\%$ less memory, $96.13\% \sim
98.07\%$ less FLOPs, and $97.56\% \sim 98.18\%$ in run time than the regular
self-attention. Compared to other ResNet101-based self-attention networks,
\ourM achieves a new \sArt result ($83.0\%$ mIoU) on Cityscape test dataset and
competitive results on ADE20k and VOCaug. \ourM can also enhance MASK R-CNN for
instance segmentation on COCO. In addition, utilizing the proposed module,
Segformer can be boosted on a series of models with different scales, and
Segformer-B5 can be improved even without retraining. Code is accessible at
\url{https://github.com/zfy-csu/FsaNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panahi_A/0/1/0/all/0/1&quot;&gt;Ashkan Panahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Guangjun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00679">
<title>Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics. (arXiv:2212.00679v4 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated synthesis of provably correct controllers for cyber-physical
systems is crucial for deployment in safety-critical scenarios. However, hybrid
features and stochastic or unknown behaviours make this problem challenging. We
propose a method for synthesising controllers for Markov jump linear systems
(MJLSs), a class of discrete-time models for cyber-physical systems, so that
they certifiably satisfy probabilistic computation tree logic (PCTL) formulae.
An MJLS consists of a finite set of stochastic linear dynamics and discrete
jumps between these dynamics that are governed by a Markov decision process
(MDP). We consider the cases where the transition probabilities of this MDP are
either known up to an interval or completely unknown. Our approach is based on
a finite-state abstraction that captures both the discrete (mode-jumping) and
continuous (stochastic linear) behaviour of the MJLS. We formalise this
abstraction as an interval MDP (iMDP) for which we compute intervals of
transition probabilities using sampling techniques from the so-called &apos;scenario
approach&apos;, resulting in a probabilistically sound approximation. We apply our
method to multiple realistic benchmark problems, in particular, a temperature
control and an aerial vehicle delivery problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rickard_L/0/1/0/all/0/1&quot;&gt;Luke Rickard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Badings_T/0/1/0/all/0/1&quot;&gt;Thom Badings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Romao_L/0/1/0/all/0/1&quot;&gt;Licio Romao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02721">
<title>A Novel Deep Reinforcement Learning Based Automated Stock Trading System Using Cascaded LSTM Networks. (arXiv:2212.02721v2 [q-fin.CP] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02721</link>
<description rdf:parseType="Literal">&lt;p&gt;More and more stock trading strategies are constructed using deep
reinforcement learning (DRL) algorithms, but DRL methods originally widely used
in the gaming community are not directly adaptable to financial data with low
signal-to-noise ratios and unevenness, and thus suffer from performance
shortcomings. In this paper, to capture the hidden information, we propose a
DRL based stock trading system using cascaded LSTM, which first uses LSTM to
extract the time-series features from stock daily data, and then the features
extracted are fed to the agent for training, while the strategy functions in
reinforcement learning also use another LSTM for training. Experiments in DJI
in the US market and SSE50 in the Chinese stock market show that our model
outperforms previous baseline models in terms of cumulative returns and Sharp
ratio, and this advantage is more significant in the Chinese stock market, a
merging market. It indicates that our proposed method is a promising way to
build a automated stock trading system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;Jie Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jiashu Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baohua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sixue Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12247">
<title>Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1&quot;&gt;Chun Kai Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1&quot;&gt;Suzanne Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zihao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1&quot;&gt;Nicholas Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1&quot;&gt;Randy Auerbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13936">
<title>Generative AI Assistants in Software Development Education. (arXiv:2303.13936v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13936</link>
<description rdf:parseType="Literal">&lt;p&gt;The software development industry is amid another disruptive paradigm change
- adopting the use of generative AI (GAI) assistants for programming. Whilst AI
is already used in various areas of software engineering, GAI technologies,
such as GitHub Copilot and ChatGPT, have ignited peoples&apos; imaginations (and
fears). It is unclear how the industry will adapt, but the move to integrate
these technologies by large software companies, such as Microsoft (GitHub,
Bing) and Google (Bard), is a clear indication of intent and direction. We
performed exploratory interviews with industry professionals to understand
current practice and challenges, which we incorporate into our vision of a
future of software development education and make some pedagogical
recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_C/0/1/0/all/0/1&quot;&gt;Christopher Bull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kharrufa_A/0/1/0/all/0/1&quot;&gt;Ahmed Kharrufa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15435">
<title>The Stable Signature: Rooting Watermarks in Latent Diffusion Models. (arXiv:2303.15435v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15435</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative image modeling enables a wide range of applications but raises
ethical concerns about responsible deployment. This paper introduces an active
strategy combining image watermarking and Latent Diffusion Models. The goal is
for all generated images to conceal an invisible watermark allowing for future
detection and/or identification. The method quickly fine-tunes the latent
decoder of the image generator, conditioned on a binary signature. A
pre-trained watermark extractor recovers the hidden signature from any
generated image and a statistical test then determines whether it comes from
the generative model. We evaluate the invisibility and robustness of the
watermarks on a variety of generation tasks, showing that Stable Signature
works even after the images are modified. For instance, it detects the origin
of an image generated from a text prompt, then cropped to keep $10\%$ of the
content, with $90$+$\%$ accuracy at a false positive rate below 10$^{-6}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1&quot;&gt;Pierre Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1&quot;&gt;Guillaume Couairon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1&quot;&gt;Matthijs Douze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1&quot;&gt;Teddy Furon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04660">
<title>Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning. (arXiv:2304.04660v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04660</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipped with the trained environmental dynamics, model-based offline
reinforcement learning (RL) algorithms can often successfully learn good
policies from fixed-sized datasets, even some datasets with poor quality.
Unfortunately, however, it can not be guaranteed that the generated samples
from the trained dynamics model are reliable (e.g., some synthetic samples may
lie outside of the support region of the static dataset). To address this
issue, we propose Trajectory Truncation with Uncertainty (TATU), which
adaptively truncates the synthetic trajectory if the accumulated uncertainty
along the trajectory is too large. We theoretically show the performance bound
of TATU to justify its benefits. To empirically show the advantages of TATU, we
first combine it with two classical model-based offline RL algorithms, MOPO and
COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free
offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark
show that TATU significantly improves their performance, often by a large
margin. Code is available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiafei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoteng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1&quot;&gt;Le Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06710">
<title>Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06710</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifier-free guidance is an effective sampling technique in diffusion
models that has been widely adopted. The main idea is to extrapolate the model
in the direction of text guidance and away from null-text guidance. In this
paper, we demonstrate that null-text guidance in diffusion models is secretly a
cartoon-style creator, i.e., the generated images can be efficiently
transformed into cartoons by simply perturbing the null-text guidance.
Specifically, we proposed two disturbance methods, i.e., Rollback disturbance
(Back-D) and Image disturbance (Image-D), to construct misalignment between the
noisy images used for predicting null-text guidance and text guidance
(subsequently referred to as \textbf{null-text noisy image} and \textbf{text
noisy image} respectively) in the sampling process. Back-D achieves
cartoonization by altering the noise level of null-text noisy image via
replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces
high-fidelity, diverse cartoons by defining $x_t$ as a clean input image, which
further improves the incorporation of finer image details. Through
comprehensive experiments, we delved into the principle of noise disturbing for
null-text and uncovered that the efficacy of disturbance depends on the
correlation between the null-text noisy image and the source image. Moreover,
our proposed techniques, which can generate cartoon images and cartoonize
specific ones, are training-free and easily integrated as a plug-and-play
component in any classifier-free guided diffusion model. Project page is
available at \url{https://nulltextforcartoon.github.io/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Heliang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Long Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wanrong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenjing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00017">
<title>Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00017</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have achieved a milestone that undenia-bly
changed many held beliefs in artificial intelligence (AI). However, there
remains many limitations of these LLMs when it comes to true language
understanding, limitations that are a byproduct of the under-lying architecture
of deep neural networks. Moreover, and due to their subsymbolic nature,
whatever knowledge these models acquire about how language works will always be
buried in billions of microfeatures (weights), none of which is meaningful on
its own, making such models hopelessly unexplainable. To address these
limitations, we suggest com-bining the strength of symbolic representations
with what we believe to be the key to the success of LLMs, namely a successful
bottom-up re-verse engineering of language at scale. As such we argue for a
bottom-up reverse engineering of language in a symbolic setting. Hints on what
this project amounts to have been suggested by several authors, and we discuss
in some detail here how this project could be accomplished.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1&quot;&gt;Walid S. Saba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11650">
<title>FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11650</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning has gained popularity for distributed learning without
aggregating sensitive data from clients. But meanwhile, the distributed and
isolated nature of data isolation may be complicated by data quality, making it
more vulnerable to noisy labels. Many efforts exist to defend against the
negative impacts of noisy labels in centralized or federated settings. However,
there is a lack of a benchmark that comprehensively considers the impact of
noisy labels in a wide variety of typical FL settings. In this work, we serve
the first standardized benchmark that can help researchers fully explore
potential federated noisy settings. Also, we conduct comprehensive experiments
to explore the characteristics of these data settings and unravel challenging
scenarios on the federated noisy label learning, which may guide method
development in the future. We highlight the 20 basic settings for more than 5
datasets proposed in our benchmark and standardized simulation pipeline for
federated noisy label learning. We hope this benchmark can facilitate idea
verification in federated learning with noisy labels. \texttt{FedNoisy} is
available at \codeword{https://github.com/SMILELab-FL/FedNoisy}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jintao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Junyuan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00773">
<title>DifFSS: Diffusion Model for Few-Shot Semantic Segmentation. (arXiv:2307.00773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00773</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated excellent performance in image generation.
Although various few-shot semantic segmentation (FSS) models with different
network structures have been proposed, performance improvement has reached a
bottleneck. This paper presents the first work to leverage the diffusion model
for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve
the performance of the state-of-the-art FSS models by a large margin without
modifying their network structure. Specifically, we utilize the powerful
generation ability of diffusion models to generate diverse auxiliary support
images by using the semantic mask, scribble or soft HED boundary of the support
image as control conditions. This generation process simulates the variety
within the class of the query image, such as color, texture variation,
lighting, $etc$. As a result, FSS models can refer to more diverse support
images, yielding more robust representations, thereby achieving a consistent
improvement in segmentation performance. Extensive experiments on three
publicly available datasets based on existing advanced FSS models demonstrate
the effectiveness of the diffusion model for FSS task. Furthermore, we explore
in detail the impact of different input settings of the diffusion model on
segmentation performance. Hopefully, this completely new paradigm will bring
inspiration to the study of FSS task integrated with AI-generated content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03506">
<title>Derivative Free Weight-space Ensembling. (arXiv:2307.03506v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03506</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work suggests that interpolating between the weights of two
specialized language models can transfer knowledge between tasks in a way that
multi-task learning cannot. However, very few have explored interpolation
between more than two models, where each has a distinct knowledge base. In this
paper, we introduce Derivative Free Weight-space Ensembling (DFWE), a new
few-sample task transfer approach for open-domain dialogue. Our framework
creates a set of diverse expert language models trained using a predefined set
of source tasks. Next, we finetune each of the expert models on the target
task, approaching the target task from several distinct knowledge bases.
Finally, we linearly interpolate between the model weights using a
gradient-free-optimization algorithm, to efficiently find a good interpolation
weighting. We demonstrate the effectiveness of the method on FETA-Friends
outperforming the standard pretrain-finetune approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ninalga_D/0/1/0/all/0/1&quot;&gt;Dean Ninalga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04192">
<title>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04192</link>
<description rdf:parseType="Literal">&lt;p&gt;Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1&quot;&gt;Min-Yen Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1&quot;&gt;Soujanya Poria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06440">
<title>No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06440</link>
<description rdf:parseType="Literal">&lt;p&gt;The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1&quot;&gt;Jean Kaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1&quot;&gt;Oscar Key&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1&quot;&gt;Piotr Nawrot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1&quot;&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1&quot;&gt;Matt J. Kusner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08072">
<title>Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study. (arXiv:2307.08072v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08072</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the superior performance, Large Language Models~(LLMs) require
significant computational resources for deployment and use. To overcome this
issue, quantization methods have been widely applied to reduce the memory
footprint of LLMs as well as increasing the inference rate. However, a major
challenge is that low-bit quantization methods often lead to performance
degradation. It is important to understand how quantization impacts the
capacity of LLMs. Different from previous studies focused on overall
performance, this work aims to investigate the impact of quantization on
\emph{emergent abilities}, which are important characteristics that distinguish
LLMs from small language models. Specially, we examine the abilities of
in-context learning, chain-of-thought reasoning, and instruction-following in
quantized LLMs. Our empirical experiments show that these emergent abilities
still exist in 4-bit quantization models, while 2-bit models encounter severe
performance degradation on the test of these abilities. To improve the
performance of low-bit models, we conduct two special experiments: (1)
fine-gained impact analysis that studies which components (or substructures)
are more sensitive to quantization, and (2) performance compensation through
model fine-tuning. Our work derives a series of important findings to
understand the impact of quantization on emergent abilities, and sheds lights
on the possibilities of extremely low-bit quantization for LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peiyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zikang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Ze-Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Dawei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bolin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10569">
<title>Deceptive Alignment Monitoring. (arXiv:2307.10569v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10569</link>
<description rdf:parseType="Literal">&lt;p&gt;As the capabilities of large machine learning models continue to grow, and as
the autonomy afforded to such models continues to expand, the spectre of a new
adversary looms: the models themselves. The threat that a model might behave in
a seemingly reasonable manner, while secretly and subtly modifying its behavior
for ulterior reasons is often referred to as deceptive alignment in the AI
Safety &amp;amp; Alignment communities. Consequently, we call this new direction
Deceptive Alignment Monitoring. In this work, we identify emerging directions
in diverse machine learning subfields that we believe will become increasingly
important and intertwined in the near future for deceptive alignment
monitoring, and we argue that advances in these fields present both long-term
challenges and new research opportunities. We conclude by advocating for
greater involvement by the adversarial machine learning community in these
emerging directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carranza_A/0/1/0/all/0/1&quot;&gt;Andres Carranza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1&quot;&gt;Dhruv Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1&quot;&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tandon_A/0/1/0/all/0/1&quot;&gt;Arnuv Tandon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10930">
<title>MediaGPT : A Large Language Model For Chinese Media. (arXiv:2307.10930v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10930</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown remarkable capabilities in generating
high-quality text and making predictions based on large amounts of data,
including the media domain. However, in practical applications, the differences
between the media&apos;s use cases and the general-purpose applications of LLMs have
become increasingly apparent, especially Chinese. This paper examines the
unique characteristics of media-domain-specific LLMs compared to general LLMs,
designed a diverse set of task instruction types to cater the specific
requirements of the domain and constructed unique datasets that are tailored to
the media domain. Based on these, we proposed MediaGPT, a domain-specific LLM
for the Chinese media domain, training by domain-specific data and experts SFT
data. By performing human experts evaluation and strong model evaluation on a
validation set, this paper demonstrated that MediaGPT outperforms mainstream
models on various Chinese media domain tasks and verifies the importance of
domain data and domain-defined prompt types for building an effective
domain-specific LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhonghao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zijia Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Haiying Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12917">
<title>Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12917</link>
<description rdf:parseType="Literal">&lt;p&gt;With rapid advancements in depth sensors and deep learning, skeleton-based
person re-identification (re-ID) models have recently achieved remarkable
progress with many advantages. Most existing solutions learn single-level
skeleton features from body joints with the assumption of equal skeleton
importance, while they typically lack the ability to exploit more informative
skeleton features from various levels such as limb level with more global body
patterns. The label dependency of these methods also limits their flexibility
in learning more general skeleton representations. This paper proposes a
generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning
(Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with
unlabeled 3D skeletons. Firstly, we construct hierarchical representations of
skeletons to model coarse-to-fine body and motion features from the levels of
body joints, components, and limbs. Then a hierarchical meta-prototype
contrastive learning model is proposed to cluster and contrast the most typical
skeleton features (&quot;prototypes&quot;) from different-level skeletons. By converting
original prototypes into meta-prototypes with multiple homogeneous
transformations, we induce the model to learn the inherent consistency of
prototypes to capture more effective skeleton features for person re-ID.
Furthermore, we devise a hard skeleton mining mechanism to adaptively infer the
informative importance of each skeleton, so as to focus on harder skeletons to
learn more discriminative skeleton representations. Extensive evaluations on
five datasets demonstrate that our approach outperforms a wide variety of
state-of-the-art skeleton-based methods. We further show the general
applicability of our method to cross-view person re-ID and RGB-based scenarios
with estimated skeletons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1&quot;&gt;Haocong Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_C/0/1/0/all/0/1&quot;&gt;Cyril Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1&quot;&gt;Chunyan Miao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13421">
<title>On the Learning Dynamics of Attention Networks. (arXiv:2307.13421v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13421</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention models are typically learned by optimizing one of three standard
loss functions that are variously called -- soft attention, hard attention, and
latent variable marginal likelihood (LVML) attention. All three paradigms are
motivated by the same goal of finding two models -- a `focus&apos; model that
`selects&apos; the right \textit{segment} of the input and a `classification&apos; model
that processes the selected segment into the target label. However, they differ
significantly in the way the selected segments are aggregated, resulting in
distinct dynamics and final results. We observe a unique signature of models
learned using these paradigms and explain this as a consequence of the
evolution of the classification model under gradient descent when the focus
model is fixed. We also analyze these paradigms in a simple setting and derive
closed-form expressions for the parameter trajectory under gradient flow. With
the soft attention loss, the focus model improves quickly at initialization and
splutters later on. On the other hand, hard attention loss behaves in the
opposite fashion. Based on our observations, we propose a simple hybrid
approach that combines the advantages of the different loss functions and
demonstrates it on a collection of semi-synthetic and real-world datasets
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vashisht_R/0/1/0/all/0/1&quot;&gt;Rahul Vashisht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramaswamy_H/0/1/0/all/0/1&quot;&gt;Harish G. Ramaswamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13494">
<title>Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13494</link>
<description rdf:parseType="Literal">&lt;p&gt;Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches face the
data and workload drift problem for a long time. Although both query-driven and
hybrid methods are proposed to avoid this problem, even the state-of-art of
them suffer from high training and estimation costs, limited scalability,
instability, and long-tailed distribution problem on high cardinality and high
dimensional tables, which seriously affects the practical application of
learned cardinality estimators. In this paper, we prove that most of these
problems are directly caused by the widely used progressive sampling. We solve
this problem by introducing predicates into the autoregressive model and
propose Duet, a stable, efficient, and scalable hybrid method to estimate
cardinality directly without sampling or any non-differentiable process, which
can not only reduces the inference complexity from $O(n)$ to $O(1)$ compared to
Naru and UAE but also achieve higher accuracy on high cardinality and high
dimensional tables. Experimental results show that Duet can achieve all the
design goals above and be much more practical and even has a lower inference
cost on CPU than that of most learned methods on GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yabin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Chang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donghua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13528">
<title>FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13528</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method. We release the code of FacTool associated with
ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chern_I/0/1/0/all/0/1&quot;&gt;I-Chun Chern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chern_S/0/1/0/all/0/1&quot;&gt;Steffi Chern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kehua Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunting Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1&quot;&gt;Graham Neubig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengfei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13617">
<title>GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13617</link>
<description rdf:parseType="Literal">&lt;p&gt;Financial analysis is an important tool for evaluating company performance.
Practitioners work to answer financial questions to make profitable investment
decisions, and use advanced quantitative analyses to do so. As a result,
Financial Question Answering (QA) is a question answering task that requires
deep reasoning about numbers. Furthermore, it is unknown how well pre-trained
language models can reason in the financial domain. The current
state-of-the-art requires a retriever to collect relevant facts about the
financial question from the text and a generator to produce a valid financial
program and a final answer. However, recently large language models like GPT-3
have achieved state-of-the-art performance on wide variety of tasks with just a
few shot examples. We run several experiments with GPT-3 and find that a
separate retrieval model and logic engine continue to be essential components
to achieving SOTA performance in this task, particularly due to the precise
nature of financial questions and the complex information stored in financial
documents. With this understanding, our refined prompt-engineering approach on
GPT-3 achieves near SOTA accuracy without any fine-tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padua_R/0/1/0/all/0/1&quot;&gt;Raul Salles de Padua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_I/0/1/0/all/0/1&quot;&gt;Imran Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karakaplan_M/0/1/0/all/0/1&quot;&gt;Mustafa U. Karakaplan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10711">
<title>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing customization methods require access to multiple reference examples
to align pre-trained diffusion probabilistic models (DPMs) with user-provided
concepts. This paper aims to address the challenge of DPM customization when
the only available supervision is a differentiable metric defined on the
generated contents. Since the sampling procedure of DPMs involves recursive
calls to the denoising UNet, na\&quot;ive gradient backpropagation requires storing
the intermediate states of all iterations, resulting in extremely high memory
consumption. To overcome this issue, we propose a novel method AdjointDPM,
which first generates new samples from diffusion models by solving the
corresponding probability-flow ODEs. It then uses the adjoint sensitivity
method to backpropagate the gradients of the loss to the models&apos; parameters
(including conditioning signals, network weights, and initial noises) by
solving another augmented ODE. To reduce numerical errors in both the forward
generation and gradient backpropagation processes, we further reparameterize
the probability-flow ODE and augmented ODE as simple non-stiff ODEs using
exponential integration. Finally, we demonstrate the effectiveness of
AdjointDPM on three interesting tasks: converting visual effects into
identification text embeddings, finetuning DPMs for specific types of
stylization, and optimizing initial noise to generate adversarial samples for
security auditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>