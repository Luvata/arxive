<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.09053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.14956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.13392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2101.07458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2101.11183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.00717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.06847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.15430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.06809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.14092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.12900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.03341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.07268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.02894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.13648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.20030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01088" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.01220">
<title>ARHNet: Adaptive Region Harmonization for Lesion-aware Augmentation to Improve Segmentation Performance. (arXiv:2307.01220v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.01220</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately segmenting brain lesions in MRI scans is critical for providing
patients with prognoses and neurological monitoring. However, the performance
of CNN-based segmentation methods is constrained by the limited training set
size. Advanced data augmentation is an effective strategy to improve the
model&apos;s robustness. However, they often introduce intensity disparities between
foreground and background areas and boundary artifacts, which weakens the
effectiveness of such strategies. In this paper, we propose a foreground
harmonization framework (ARHNet) to tackle intensity disparities and make
synthetic images look more realistic. In particular, we propose an Adaptive
Region Harmonization (ARH) module to dynamically align foreground feature maps
to the background with an attention mechanism. We demonstrate the efficacy of
our method in improving the segmentation performance using real and synthetic
images. Experimental results on the ATLAS 2.0 dataset show that ARHNet
outperforms other methods for image harmonization tasks, and boosts the
down-stream segmentation performance. Our code is publicly available at
https://github.com/King-HAW/ARHNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huo_J/0/1/0/all/0/1&quot;&gt;Jiayu Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ouyang_X/0/1/0/all/0/1&quot;&gt;Xi Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Granados_A/0/1/0/all/0/1&quot;&gt;Alejandro Granados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1&quot;&gt;Rachel Sparks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01232">
<title>Robust Surgical Tools Detection in Endoscopic Videos with Noisy Data. (arXiv:2307.01232v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.01232</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, surgical data science has attracted substantial
interest from the machine learning (ML) community. Various studies have
demonstrated the efficacy of emerging ML techniques in analysing surgical data,
particularly recordings of procedures, for digitizing clinical and non-clinical
functions like preoperative planning, context-aware decision-making, and
operating skill assessment. However, this field is still in its infancy and
lacks representative, well-annotated datasets for training robust models in
intermediate ML tasks. Also, existing datasets suffer from inaccurate labels,
hindering the development of reliable models. In this paper, we propose a
systematic methodology for developing robust models for surgical tool detection
using noisy data. Our methodology introduces two key innovations: (1) an
intelligent active learning strategy for minimal dataset identification and
label correction by human experts; and (2) an assembling strategy for a
student-teacher model-based self-training framework to achieve the robust
classification of 14 surgical tools in a semi-supervised fashion. Furthermore,
we employ weighted data loaders to handle difficult class labels and address
class imbalance issues. The proposed methodology achieves an average F1-score
of 85.88\% for the ensemble model-based self-training with class weights, and
80.88\% without class weights for noisy labels. Also, our proposed method
significantly outperforms existing approaches, which effectively demonstrates
its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qayyum_A/0/1/0/all/0/1&quot;&gt;Adnan Qayyum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1&quot;&gt;Hassan Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Caputo_M/0/1/0/all/0/1&quot;&gt;Massimo Caputo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vohra_H/0/1/0/all/0/1&quot;&gt;Hunaid Vohra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akinosho_T/0/1/0/all/0/1&quot;&gt;Taofeek Akinosho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abioye_S/0/1/0/all/0/1&quot;&gt;Sofiat Abioye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berrou_I/0/1/0/all/0/1&quot;&gt;Ilhem Berrou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Capik_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Capik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qadir_J/0/1/0/all/0/1&quot;&gt;Junaid Qadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bilal_M/0/1/0/all/0/1&quot;&gt;Muhammad Bilal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01346">
<title>Patch-CNN: Training data-efficient deep learning for high-fidelity diffusion tensor estimation from minimal diffusion protocols. (arXiv:2307.01346v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01346</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method, Patch-CNN, for diffusion tensor (DT) estimation from
only six-direction diffusion weighted images (DWI). Deep learning-based methods
have been recently proposed for dMRI parameter estimation, using either
voxel-wise fully-connected neural networks (FCN) or image-wise convolutional
neural networks (CNN). In the acute clinical context -- where pressure of time
limits the number of imaged directions to a minimum -- existing approaches
either require an infeasible number of training images volumes (image-wise
CNNs), or do not estimate the fibre orientations (voxel-wise FCNs) required for
tractogram estimation. To overcome these limitations, we propose Patch-CNN, a
neural network with a minimal (non-voxel-wise) convolutional kernel
(3$\times$3$\times$3). Compared with voxel-wise FCNs, this has the advantage of
allowing the network to leverage local anatomical information. Compared with
image-wise CNNs, the minimal kernel vastly reduces training data demand.
Evaluated against both conventional model fitting and a voxel-wise FCN,
Patch-CNN, trained with a single subject is shown to improve the estimation of
both scalar dMRI parameters and fibre orientation from six-direction DWIs. The
improved fibre orientation estimation is shown to produce improved tractogram.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_Allcock_T/0/1/0/all/0/1&quot;&gt;Tobias Goodwin-Allcock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1&quot;&gt;Ting Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1&quot;&gt;Robert Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1&quot;&gt;Parashkev Nachev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01362">
<title>Direct Superpoints Matching for Fast and Robust Point Cloud Registration. (arXiv:2307.01362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01362</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep neural networks endow the downsampled superpoints with
discriminative feature representations, directly matching them is usually not
used alone in state-of-the-art methods, mainly for two reasons. First, the
correspondences are inevitably noisy, so RANSAC-like refinement is usually
adopted. Such ad hoc postprocessing, however, is slow and not differentiable,
which can not be jointly optimized with feature learning. Second, superpoints
are sparse and thus more RANSAC iterations are needed. Existing approaches use
the coarse-to-fine strategy to propagate the superpoints correspondences to the
point level, which are not discriminative enough and further necessitates the
postprocessing refinement. In this paper, we present a simple yet effective
approach to extract correspondences by directly matching superpoints using a
global softmax layer in an end-to-end manner, which are used to determine the
rigid transformation between the source and target point cloud. Compared with
methods that directly predict corresponding points, by leveraging the rich
information from the superpoints matchings, we can obtain more accurate
estimation of the transformation and effectively filter out outliers without
any postprocessing refinement. As a result, our approach is not only fast, but
also achieves state-of-the-art results on the challenging ModelNet and 3DMatch
benchmarks. Our code and model weights will be publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Aniket Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yiming Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1&quot;&gt;Hanumant Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huaizu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01378">
<title>A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series. (arXiv:2307.01378v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01378</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate estimation of building heights is essential for urban planning,
infrastructure management, and environmental analysis. In this study, we
propose a supervised Multimodal Building Height Regression Network (MBHR-Net)
for estimating building heights at 10m spatial resolution using Sentinel-1 (S1)
and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar
(SAR) data that offers valuable information on building structures, while S2
provides multispectral data that is sensitive to different land cover types,
vegetation phenology, and building shadows. Our MBHR-Net aims to extract
meaningful features from the S1 and S2 images to learn complex spatio-temporal
relationships between image patterns and building heights. The model is trained
and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE),
Intersection over Union (IOU), and R-squared (R2) score metrics are used to
evaluate the performance of the model. The preliminary results (3.73m RMSE,
0.95 IoU, 0.61 R2) demonstrate the effectiveness of our deep learning model in
accurately estimating building heights, showcasing its potential for urban
planning, environmental impact analysis, and other related applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1&quot;&gt;Ritu Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascetti_A/0/1/0/all/0/1&quot;&gt;Andrea Nascetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1&quot;&gt;Yifang Ban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01383">
<title>Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms. (arXiv:2307.01383v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;Monitoring cow body weight is crucial to support farm management decisions
due to its direct relationship with the growth, nutritional status, and health
of dairy cows. Cow body weight is a repeated trait, however, the majority of
previous body weight prediction research only used data collected at a single
point in time. Furthermore, the utility of deep learning-based segmentation for
body weight prediction using videos remains unanswered. Therefore, the
objectives of this study were to predict cow body weight from repeatedly
measured video data, to compare the performance of the thresholding and Mask
R-CNN deep learning approaches, to evaluate the predictive ability of body
weight regression models, and to promote open science in the animal science
community by releasing the source code for video-based body weight prediction.
A total of 40,405 depth images and depth map files were obtained from 10
lactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were
investigated to segment the cow&apos;s body from the background, including single
thresholding, adaptive thresholding, and Mask R-CNN. Four image-derived
biometric features, such as dorsal length, abdominal width, height, and volume,
were estimated from the segmented images. On average, the Mask-RCNN approach
combined with a linear mixed model resulted in the best prediction coefficient
of determination and mean absolute percentage error of 0.98 and 2.03%,
respectively, in the forecasting cross-validation. The Mask-RCNN approach was
also the best in the leave-three-cows-out cross-validation. The prediction
coefficients of determination and mean absolute percentage error of the
Mask-RCNN coupled with the linear mixed model were 0.90 and 4.70%,
respectively. Our results suggest that deep learning-based segmentation
improves the prediction performance of cow body weight from longitudinal depth
video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Y/0/1/0/all/0/1&quot;&gt;Ye Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campos_L/0/1/0/all/0/1&quot;&gt;Leticia M.Campos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haipeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanigan_M/0/1/0/all/0/1&quot;&gt;Mark D.Hanigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morota_G/0/1/0/all/0/1&quot;&gt;Gota Morota&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01421">
<title>Unsupervised Feature Learning with Emergent Data-Driven Prototypicality. (arXiv:2307.01421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01421</link>
<description rdf:parseType="Literal">&lt;p&gt;Given an image set without any labels, our goal is to train a model that maps
each image to a point in a feature space such that, not only proximity
indicates visual similarity, but where it is located directly encodes how
prototypical the image is according to the dataset.
&lt;/p&gt;
&lt;p&gt;Our key insight is to perform unsupervised feature learning in hyperbolic
instead of Euclidean space, where the distance between points still reflect
image similarity, and yet we gain additional capacity for representing
prototypicality with the location of the point: The closer it is to the origin,
the more prototypical it is. The latter property is simply emergent from
optimizing the usual metric learning objective: The image similar to many
training instances is best placed at the center of corresponding points in
Euclidean space, but closer to the origin in hyperbolic space.
&lt;/p&gt;
&lt;p&gt;We propose an unsupervised feature learning algorithm in Hyperbolic space
with sphere pACKing. HACK first generates uniformly packed particles in the
Poincar\&apos;e ball of hyperbolic space and then assigns each image uniquely to
each particle. Images after congealing are regarded more typical of the dataset
it belongs to. With our feature mapper simply trained to spread out training
instances in hyperbolic space, we observe that images move closer to the origin
with congealing, validating our idea of unsupervised prototypicality discovery.
We demonstrate that our data-driven prototypicality provides an easy and
superior unsupervised instance selection to reduce sample complexity, increase
model generalization with atypical instances and robustness with typical ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yunhui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youren Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yubei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01425">
<title>Consistent Multimodal Generation via A Unified GAN Framework. (arXiv:2307.01425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01425</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate how to generate multimodal image outputs, such as RGB, depth,
and surface normals, with a single generative model. The challenge is to
produce outputs that are realistic, and also consistent with each other. Our
solution builds on the StyleGAN3 architecture, with a shared backbone and
modality-specific branches in the last layers of the synthesis network, and we
propose per-modality fidelity discriminators and a cross-modality consistency
discriminator. In experiments on the Stanford2D3D dataset, we demonstrate
realistic and consistent generation of RGB, depth, and normal images. We also
show a training recipe to easily extend our pretrained model on a new domain,
even with a few pairwise data. We further evaluate the use of synthetically
generated RGB and depth pairs for training or fine-tuning depth estimators.
Code will be available at https://github.com/jessemelpolio/MultimodalGAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1&quot;&gt;Weijie Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1&quot;&gt;Zhixin Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirk_S/0/1/0/all/0/1&quot;&gt;Soeren Pirk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1&quot;&gt;Derek Hoiem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01426">
<title>DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection. (arXiv:2307.01426v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01426</link>
<description rdf:parseType="Literal">&lt;p&gt;A critical yet frequently overlooked challenge in the field of deepfake
detection is the lack of a standardized, unified, comprehensive benchmark. This
issue leads to unfair performance comparisons and potentially misleading
results. Specifically, there is a lack of uniformity in data processing
pipelines, resulting in inconsistent data inputs for detection models.
Additionally, there are noticeable differences in experimental settings, and
evaluation strategies and metrics lack standardization. To fill this gap, we
present the first comprehensive benchmark for deepfake detection, called
DeepfakeBench, which offers three key contributions: 1) a unified data
management system to ensure consistent input across all detectors, 2) an
integrated framework for state-of-the-art methods implementation, and 3)
standardized evaluation metrics and protocols to promote transparency and
reproducibility. Featuring an extensible, modular-based codebase, DeepfakeBench
contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series
of deepfake detection evaluation protocols and analysis tools, as well as
comprehensive evaluations. Moreover, we provide new insights based on extensive
analysis of these evaluations from various perspectives (e.g., data
augmentations, backbones). We hope that our efforts could facilitate future
research and foster innovation in this increasingly critical domain. All codes,
evaluations, and analyses of our benchmark are publicly available at
https://github.com/SCLBD/DeepfakeBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xinhang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01430">
<title>Continual Learning in Open-vocabulary Classification with Complementary Memory Systems. (arXiv:2307.01430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01430</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method for flexible continual learning in open-vocabulary
image classification, drawing inspiration from the complementary learning
systems observed in human cognition. We propose a &quot;tree probe&quot; method, an
adaption of lazy learning principles, which enables fast learning from new
examples with competitive accuracy to batch-trained linear models. Further, we
propose a method to combine predictions from a CLIP zero-shot model and the
exemplar-based model, using the zero-shot estimated probability that a sample&apos;s
class is within any of the exemplar classes. We test in data incremental, class
incremental, and task incremental settings, as well as ability to perform
flexible inference on varying subsets of zero-shot and learned categories. Our
proposed method achieves a good balance of learning speed, target task
effectiveness, and zero-shot effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_W/0/1/0/all/0/1&quot;&gt;Weijie Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1&quot;&gt;Derek Hoiem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01447">
<title>Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural Network. (arXiv:2307.01447v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01447</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately matching local features between a pair of images is a challenging
computer vision task. Previous studies typically use attention based graph
neural networks (GNNs) with fully-connected graphs over keypoints within/across
images for visual and geometric information reasoning. However, in the context
of feature matching, considerable keypoints are non-repeatable due to occlusion
and failure of the detector, and thus irrelevant for message passing. The
connectivity with non-repeatable keypoints not only introduces redundancy,
resulting in limited efficiency, but also interferes with the representation
aggregation process, leading to limited accuracy. Targeting towards high
accuracy and efficiency, we propose MaKeGNN, a sparse attention-based GNN
architecture which bypasses non-repeatable keypoints and leverages matchable
ones to guide compact and meaningful message passing. More specifically, our
Bilateral Context-Aware Sampling Module first dynamically samples two small
sets of well-distributed keypoints with high matchability scores from the image
pair. Then, our Matchable Keypoint-Assisted Context Aggregation Module regards
sampled informative keypoints as message bottlenecks and thus constrains each
keypoint only to retrieve favorable contextual information from intra- and
inter- matchable keypoints, evading the interference of irrelevant and
redundant connectivity with non-repeatable ones. Furthermore, considering the
potential noise in initial keypoints and sampled matchable ones, the MKACA
module adopts a matchability-guided attentional aggregation operation for purer
data-dependent context propagation. By these means, we achieve the
state-of-the-art performance on relative camera estimation, fundamental matrix
estimation, and visual localization, while significantly reducing computational
and memory complexity compared to typical attentional GNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zizhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiayi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01462">
<title>Practical Collaborative Perception: A Framework for Asynchronous and Multi-Agent 3D Object Detection. (arXiv:2307.01462v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.01462</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we improve the single-vehicle 3D object detection models using
LiDAR by extending their capacity to process point cloud sequences instead of
individual point clouds. In this step, we extend our previous work on
rectification of the shadow effect in the concatenation of point clouds to
boost the detection accuracy of multi-frame detection models. Our extension
includes incorporating HD Map and distilling an Oracle model. Next, we further
increase the performance of single-vehicle perception using multi-agent
collaboration via Vehicle-to-everything (V2X) communication. We devise a simple
yet effective collaboration method that achieves better bandwidth-performance
tradeoffs than prior arts while minimizing changes made to single-vehicle
detection models and assumptions on inter-agent synchronization. Experiments on
the V2X-Sim dataset show that our collaboration method achieves 98% performance
of the early collaboration while consuming the equivalent amount of bandwidth
usage of late collaboration which is 0.03% of early collaboration. The code
will be released at https://github.com/quan-dao/practical-collab-perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1&quot;&gt;Minh-Quan Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrio_J/0/1/0/all/0/1&quot;&gt;Julie Stephany Berrio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fremont_V/0/1/0/all/0/1&quot;&gt;Vincent Fr&amp;#xe9;mont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1&quot;&gt;Mao Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hery_E/0/1/0/all/0/1&quot;&gt;Elwan H&amp;#xe9;ry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worrall_S/0/1/0/all/0/1&quot;&gt;Stewart Worrall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01464">
<title>Unsupervised Quality Prediction for Improved Single-Frame and Weighted Sequential Visual Place Recognition. (arXiv:2307.01464v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01464</link>
<description rdf:parseType="Literal">&lt;p&gt;While substantial progress has been made in the absolute performance of
localization and Visual Place Recognition (VPR) techniques, it is becoming
increasingly clear from translating these systems into applications that other
capabilities like integrity and predictability are just as important,
especially for safety- or operationally-critical autonomous systems. In this
research we present a new, training-free approach to predicting the likely
quality of localization estimates, and a novel method for using these
predictions to bias a sequence-matching process to produce additional
performance gains beyond that of a naive sequence matching approach. Our
combined system is lightweight, runs in real-time and is agnostic to the
underlying VPR technique. On extensive experiments across four datasets and
three VPR techniques, we demonstrate our system improves precision performance,
especially at the high-precision/low-recall operating point. We also present
ablation and analysis identifying the performance contributions of the
prediction and weighted sequence matching components in isolation, and the
relationship between the quality of the prediction system and the benefits of
the weighted sequential matcher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carson_H/0/1/0/all/0/1&quot;&gt;Helen Carson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ford_J/0/1/0/all/0/1&quot;&gt;Jason J. Ford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01465">
<title>AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation. (arXiv:2307.01465v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01465</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image generation (FSIG) aims to learn to generate new and diverse
images given few (e.g., 10) training samples. Recent work has addressed FSIG by
leveraging a GAN pre-trained on a large-scale source domain and adapting it to
the target domain with few target samples. Central to recent FSIG methods are
knowledge preservation criteria, which select and preserve a subset of source
knowledge to the adapted model. However, a major limitation of existing methods
is that their knowledge preserving criteria consider only source domain/task
and fail to consider target domain/adaptation in selecting source knowledge,
casting doubt on their suitability for setups of different proximity between
source and target domain. Our work makes two contributions. Firstly, we revisit
recent FSIG works and their experiments. We reveal that under setups which
assumption of close proximity between source and target domains is relaxed,
many existing state-of-the-art (SOTA) methods which consider only source domain
in knowledge preserving perform no better than a baseline method. As our second
contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general
FSIG of different source-target domain proximity. Extensive experiments show
that AdAM consistently achieves SOTA performance in FSIG, including challenging
setups where source and target domains are more apart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasegaran_K/0/1/0/all/0/1&quot;&gt;Keshigeyan Chandrasegaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milad_A/0/1/0/all/0/1&quot;&gt;Abdollahzadeh Milad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruoteng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01467">
<title>Technical Report for Ego4D Long Term Action Anticipation Challenge 2023. (arXiv:2307.01467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01467</link>
<description rdf:parseType="Literal">&lt;p&gt;In this report, we describe the technical details of our approach for the
Ego4D Long-Term Action Anticipation Challenge 2023. The aim of this task is to
predict a sequence of future actions that will take place at an arbitrary time
or later, given an input video. To accomplish this task, we introduce three
improvements to the baseline model, which consists of an encoder that generates
clip-level features from the video, an aggregator that integrates multiple
clip-level features, and a decoder that outputs Z future actions. 1) Model
ensemble of SlowFast and SlowFast-CLIP; 2) Label smoothing to relax order
constraints for future actions; 3) Constraining the prediction of the action
class (verb, noun) based on word co-occurrence. Our method outperformed the
baseline performance and recorded as second place solution on the public
leaderboard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishibashi_T/0/1/0/all/0/1&quot;&gt;Tatsuya Ishibashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ono_K/0/1/0/all/0/1&quot;&gt;Kosuke Ono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugo_N/0/1/0/all/0/1&quot;&gt;Noriyuki Kugo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yuji Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01468">
<title>Generating Animatable 3D Cartoon Faces from Single Portraits. (arXiv:2307.01468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01468</link>
<description rdf:parseType="Literal">&lt;p&gt;With the booming of virtual reality (VR) technology, there is a growing need
for customized 3D avatars. However, traditional methods for 3D avatar modeling
are either time-consuming or fail to retain similarity to the person being
modeled. We present a novel framework to generate animatable 3D cartoon faces
from a single portrait image. We first transfer an input real-world portrait to
a stylized cartoon image with a StyleGAN. Then we propose a two-stage
reconstruction method to recover the 3D cartoon face with detailed texture,
which first makes a coarse estimation based on template models, and then
refines the model by non-rigid deformation under landmark supervision. Finally,
we propose a semantic preserving face rigging method based on manually created
templates and deformation transfer. Compared with prior arts, qualitative and
quantitative results show that our method achieves better accuracy, aesthetics,
and similarity criteria. Furthermore, we demonstrate the capability of
real-time facial animation of our 3D model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chuanyu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guowei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Taijiang Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yu-Kun Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01470">
<title>A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding. (arXiv:2307.01470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;Driver gaze plays an important role in different gaze-based applications such
as driver attentiveness detection, visual distraction detection, gaze behavior
understanding, and building driver assistance system. The main objective of
this study is to perform a comprehensive summary of driver gaze fundamentals,
methods to estimate driver gaze, and it&apos;s applications in real world driving
scenarios. We first discuss the fundamentals related to driver gaze, involving
head-mounted and remote setup based gaze estimation and the terminologies used
for each of these data collection methods. Next, we list out the existing
benchmark driver gaze datasets, highlighting the collection methodology and the
equipment used for such data collection. This is followed by a discussion of
the algorithms used for driver gaze estimation, which primarily involves
traditional machine learning and deep learning based techniques. The estimated
driver gaze is then used for understanding gaze behavior while maneuvering
through intersections, on-ramps, off-ramps, lane changing, and determining the
effect of roadside advertising structures. Finally, we have discussed the
limitations in the existing literature, challenges, and the future scope in
driver gaze estimation and gaze-based applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pavan Kumar Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1&quot;&gt;Pranamesh Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01473">
<title>Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have demonstrated remarkable capabilities in learning
complex patterns and concepts from training data. However, recent findings
indicate that these models tend to rely heavily on simple and easily
discernible features present in the background of images rather than the main
concepts or objects they are intended to classify. This phenomenon poses a
challenge to image classifiers as the crucial elements of interest in images
may be overshadowed. In this paper, we propose a novel approach to address this
issue and improve the learning of main concepts by image classifiers. Our
central idea revolves around concurrently guiding the model&apos;s attention toward
the foreground during the classification task. By emphasizing the foreground,
which encapsulates the primary objects of interest, we aim to shift the focus
of the model away from the dominant influence of the background. To accomplish
this, we introduce a mechanism that encourages the model to allocate sufficient
attention to the foreground. We investigate various strategies, including
modifying the loss function or incorporating additional architectural
components, to enable the classifier to effectively capture the primary concept
within an image. Additionally, we explore the impact of different foreground
attention mechanisms on model performance and provide insights into their
effectiveness. Through extensive experimentation on benchmark datasets, we
demonstrate the efficacy of our proposed approach in improving the
classification accuracy of image classifiers. Our findings highlight the
importance of foreground attention in enhancing model understanding and
representation of the main concepts within images. The results of this study
contribute to advancing the field of image classification and provide valuable
insights for developing more robust and accurate deep-learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_R/0/1/0/all/0/1&quot;&gt;Raha Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Rajabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1&quot;&gt;Mohamamd Khalooiem Mohammad Sabokrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01486">
<title>H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation. (arXiv:2307.01486v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.01486</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep learning methods have been widely used for tumor segmentation
of multimodal medical images with promising results. However, most existing
methods are limited by insufficient representational ability, specific modality
number and high computational complexity. In this paper, we propose a hybrid
densely connected network for tumor segmentation, named H-DenseFormer, which
combines the representational power of the Convolutional Neural Network (CNN)
and the Transformer structures. Specifically, H-DenseFormer integrates a
Transformer-based Multi-path Parallel Embedding (MPE) module that can take an
arbitrary number of modalities as input to extract the fusion features from
different modalities. Then, the multimodal fusion features are delivered to
different levels of the encoder to enhance multimodal learning representation.
Besides, we design a lightweight Densely Connected Transformer (DCT) block to
replace the standard Transformer block, thus significantly reducing
computational complexity. We conduct extensive experiments on two public
multimodal datasets, HECKTOR21 and PI-CAI22. The experimental results show that
our proposed method outperforms the existing state-of-the-art methods while
having lower computational complexity. The source code is available at
https://github.com/shijun18/H-DenseFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kan_H/0/1/0/all/0/1&quot;&gt;Hongyu Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1&quot;&gt;Shulan Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Minfan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Liang Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+An_H/0/1/0/all/0/1&quot;&gt;Hong An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xudong Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01489">
<title>Semantic Segmentation on 3D Point Clouds with High Density Variations. (arXiv:2307.01489v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01489</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR scanning for surveying applications acquire measurements over wide
areas and long distances, which produces large-scale 3D point clouds with
significant local density variations. While existing 3D semantic segmentation
models conduct downsampling and upsampling to build robustness against varying
point densities, they are less effective under the large local density
variations characteristic of point clouds from surveying applications. To
alleviate this weakness, we propose a novel architecture called HDVNet that
contains a nested set of encoder-decoder pathways, each handling a specific
point density range. Limiting the interconnections between the feature maps
enables HDVNet to gauge the reliability of each feature based on the density of
a point, e.g., downweighting high density features not existing in low density
objects. By effectively handling input density variations, HDVNet outperforms
state-of-the-art models in segmentation accuracy on real point clouds with
inconsistent density, using just over half the weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faulkner_R/0/1/0/all/0/1&quot;&gt;Ryan Faulkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haub_L/0/1/0/all/0/1&quot;&gt;Luke Haub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratcliffe_S/0/1/0/all/0/1&quot;&gt;Simon Ratcliffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1&quot;&gt;Tat-Jun Chin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01492">
<title>FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation. (arXiv:2307.01492v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01492</link>
<description rdf:parseType="Literal">&lt;p&gt;This technical report summarizes the winning solution for the 3D Occupancy
Prediction Challenge, which is held in conjunction with the CVPR 2023 Workshop
on End-to-End Autonomous Driving and CVPR 23 Workshop on Vision-Centric
Autonomous Driving Workshop. Our proposed solution FB-OCC builds upon FB-BEV, a
cutting-edge camera-based bird&apos;s-eye view perception design using
forward-backward projection. On top of FB-BEV, we further study novel designs
and optimization tailored to the 3D occupancy prediction task, including joint
depth-semantic pre-training, joint voxel-BEV representation, model scaling up,
and effective post-processing strategies. These designs and optimization result
in a state-of-the-art mIoU score of 54.19% on the nuScenes dataset, ranking the
1st place in the challenge track. Code and models will be released at:
https://github.com/NVlabs/FB-BEV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Austin_D/0/1/0/all/0/1&quot;&gt;David Austin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Mingsheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Shiyi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01502">
<title>HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair. (arXiv:2307.01502v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01502</link>
<description rdf:parseType="Literal">&lt;p&gt;Abdominal wall defects often lead to pain, discomfort, and recurrence of
incisional hernias, resulting in significant morbidity and repeated surgical
repairs worldwide. Mesh repair for large hernias is usually based on the defect
area with a fixed overlap, without considering biomechanical aspects such as
muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal
wall distention. To address this issue, we present a biomechanical approach to
incisional hernia repair that takes into account the unstable abdominal wall.
Additionally, we introduce HEDI, a tool that uses dynamic computed tomography
with Valsalva maneuver to automatically detect and assess hernia size, volume,
and abdominal wall instability. Our first clinical application of HEDI in the
preoperative evaluation of 31 patients shows significantly improved success
rates compared to reported rates, with all patients remaining pain-free and
showing no hernia recurrence after three years of follow-up.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Relle_J/0/1/0/all/0/1&quot;&gt;Jacob J. Relle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voss_S/0/1/0/all/0/1&quot;&gt;Samuel Vo&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raschidi_R/0/1/0/all/0/1&quot;&gt;Ramesch Raschidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nessel_R/0/1/0/all/0/1&quot;&gt;Regine Nessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorich_J/0/1/0/all/0/1&quot;&gt;Johannes G&amp;#xf6;rich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielputz_M/0/1/0/all/0/1&quot;&gt;Mark O. Wielp&amp;#xfc;tz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loffler_T/0/1/0/all/0/1&quot;&gt;Thorsten L&amp;#xf6;ffler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heuveline_V/0/1/0/all/0/1&quot;&gt;Vincent Heuveline&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallinowski_F/0/1/0/all/0/1&quot;&gt;Friedrich Kallinowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Losel_P/0/1/0/all/0/1&quot;&gt;Philipp D. L&amp;#xf6;sel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01514">
<title>SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01514</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning in federated learning paradigm has been gaining a
lot of interest both in industry and research due to the collaborative learning
capability on unlabeled yet isolated data. However, self-supervised based
federated learning strategies suffer from performance degradation due to label
scarcity and diverse data distributions, i.e., data heterogeneity. In this
paper, we propose the SelfFed framework for Internet of Medical Things (IoMT).
Our proposed SelfFed framework works in two phases. The first phase is the
pre-training paradigm that performs augmentive modeling using Swin Transformer
based encoder in a decentralized manner. The first phase of SelfFed framework
helps to overcome the data heterogeneity issue. The second phase is the
fine-tuning paradigm that introduces contrastive network and a novel
aggregation strategy that is trained on limited labeled data for a target task
in a decentralized manner. This fine-tuning stage overcomes the label scarcity
problem. We perform our experimental analysis on publicly available medical
imaging datasets and show that our proposed SelfFed framework performs better
when compared to existing baselines concerning non-independent and identically
distributed (IID) data and label scarcity. Our method achieves a maximum
improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID
dataset. Further, our proposed method outperforms existing baselines even when
trained on a few (10%) labeled instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1&quot;&gt;Sunder Ali Khowaja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dev_K/0/1/0/all/0/1&quot;&gt;Kapal Dev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Syed Muhammad Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linguraru_M/0/1/0/all/0/1&quot;&gt;Marius George Linguraru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01515">
<title>LPN: Language-guided Prototypical Network for few-shot classification. (arXiv:2307.01515v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01515</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot classification aims to adapt to new tasks with limited labeled
examples. To fully use the accessible data, recent methods explore suitable
measures for the similarity between the query and support images and better
high-dimensional features with meta-training and pre-training strategies.
However, the potential of multi-modality information has barely been explored,
which may bring promising improvement for few-shot classification. In this
paper, we propose a Language-guided Prototypical Network (LPN) for few-shot
classification, which leverages the complementarity of vision and language
modalities via two parallel branches. Concretely, to introduce language
modality with limited samples in the visual task, we leverage a pre-trained
text encoder to extract class-level text features directly from class names
while processing images with a conventional image encoder. Then, a
language-guided decoder is introduced to obtain text features corresponding to
each image by aligning class-level features with visual features. In addition,
to take advantage of class-level features and prototypes, we build a refined
prototypical head that generates robust prototypes in the text branch for
follow-up measurement. Finally, we aggregate the visual and text logits to
calibrate the deviation of a single modality. Extensive experiments demonstrate
the competitiveness of LPN against state-of-the-art methods on benchmark
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kaihui Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chule Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01520">
<title>LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack. (arXiv:2307.01520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfakes, malicious visual contents created by generative models, pose an
increasingly harmful threat to society. To proactively mitigate deepfake
damages, recent studies have employed adversarial perturbation to disrupt
deepfake model outputs. However, previous approaches primarily focus on
generating distorted outputs based on only predetermined target attributes,
leading to a lack of robustness in real-world scenarios where target attributes
are unknown. Additionally, the transferability of perturbations between two
prominent generative models, Generative Adversarial Networks (GANs) and
Diffusion Models, remains unexplored. In this paper, we emphasize the
importance of target attribute-transferability and model-transferability for
achieving robust deepfake disruption. To address this challenge, we propose a
simple yet effective disruption method called Latent Ensemble ATtack (LEAT),
which attacks the independent latent encoding process. By disrupting the latent
encoding process, it generates distorted output images in subsequent generation
processes, regardless of the given target attributes. This target
attribute-agnostic attack ensures robust disruption even when the target
attributes are unknown. Additionally, we introduce a Normalized Gradient
Ensemble strategy that effectively aggregates gradients for iterative gradient
attacks, enabling simultaneous attacks on various types of deepfake models,
involving both GAN-based and Diffusion-based models. Moreover, we demonstrate
the insufficiency of evaluating disruption quality solely based on pixel-level
differences. As a result, we propose an alternative protocol for
comprehensively evaluating the success of defense. Extensive experiments
confirm the efficacy of our method in disrupting deepfakes in real-world
scenarios, reporting a higher defense success rate compared to previous
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_J/0/1/0/all/0/1&quot;&gt;Joonkyo Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hyunsoo Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01524">
<title>Exploiting Richness of Learned Compressed Representation of Images for Semantic Segmentation. (arXiv:2307.01524v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01524</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles and Advanced Driving Assistance Systems (ADAS) have the
potential to radically change the way we travel. Many such vehicles currently
rely on segmentation and object detection algorithms to detect and track
objects around its surrounding. The data collected from the vehicles are often
sent to cloud servers to facilitate continual/life-long learning of these
algorithms. Considering the bandwidth constraints, the data is compressed
before sending it to servers, where it is typically decompressed for training
and analysis. In this work, we propose the use of a learning-based compression
Codec to reduce the overhead in latency incurred for the decompression
operation in the standard pipeline. We demonstrate that the learned compressed
representation can also be used to perform tasks like semantic segmentation in
addition to decompression to obtain the images. We experimentally validate the
proposed pipeline on the Cityscapes dataset, where we achieve a compression
factor up to $66 \times$ while preserving the information required to perform
segmentation with a dice coefficient of $0.84$ as compared to $0.88$ achieved
using decompressed images while reducing the overall compute by $11\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakaiya_R/0/1/0/all/0/1&quot;&gt;Ravi Kakaiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathish_R/0/1/0/all/0/1&quot;&gt;Rakshith Sathish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethuraman_R/0/1/0/all/0/1&quot;&gt;Ramanathan Sethuraman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01530">
<title>Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions. (arXiv:2307.01530v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01530</link>
<description rdf:parseType="Literal">&lt;p&gt;Harvesting fully ripe tomatoes with mobile robots presents significant
challenges in real-world scenarios. These challenges arise from factors such as
occlusion caused by leaves and branches, as well as the color similarity
between tomatoes and the surrounding foliage during the fruit development
stage. The natural environment further compounds these issues with varying
light conditions, viewing angles, occlusion factors, and different maturity
levels. To overcome these obstacles, this research introduces a novel framework
that leverages a convolutional transformer architecture to autonomously
recognize and grade tomatoes, irrespective of their occlusion level, lighting
conditions, and ripeness. The proposed model is trained and tested using
carefully annotated images curated specifically for this purpose. The dataset
is prepared under various lighting conditions, viewing perspectives, and
employs different mobile camera sensors, distinguishing it from existing
datasets such as Laboro Tomato and Rob2Pheno Annotated Tomato. The
effectiveness of the proposed framework in handling cluttered and occluded
tomato instances was evaluated using two additional public datasets, Laboro
Tomato and Rob2Pheno Annotated Tomato, as benchmarks. The evaluation results
across these three datasets demonstrate the exceptional performance of our
proposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and
66.39% in terms of mean average precision scores for KUTomaData, Laboro Tomato,
and Rob2Pheno Annotated Tomato, respectively. The results underscore the
superiority of the proposed model in accurately detecting and delineating
tomatoes compared to baseline methods and previous approaches. Specifically,
the model achieves an F1-score of 80.14%, a Dice coefficient of 73.26%, and a
mean IoU of 66.41% on the KUTomaData image dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asim Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1&quot;&gt;Taimur Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafay_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1&quot;&gt;Israa Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1&quot;&gt;Naoufel Werghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1&quot;&gt;Lakmal Seneviratne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_I/0/1/0/all/0/1&quot;&gt;Irfan Hussain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01533">
<title>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations. (arXiv:2307.01533v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01533</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to address the unsupervised video anomaly detection (VAD)
problem, which involves classifying each frame in a video as normal or
abnormal, without any access to labels. To accomplish this, the proposed method
employs conditional diffusion models, where the input data is the
spatiotemporal features extracted from a pre-trained network, and the condition
is the features extracted from compact motion representations that summarize a
given video segment in terms of its motion and appearance. Our method utilizes
a data-driven threshold and considers a high reconstruction error as an
indicator of anomalous events. This study is the first to utilize compact
motion representations for VAD and the experiments conducted on two large-scale
VAD benchmarks demonstrate that they supply relevant information to the
diffusion model, and consequently improve VAD performances w.r.t the prior art.
Importantly, our method exhibits better generalization performance across
different datasets, notably outperforming both the state-of-the-art and
baseline methods. The code of our method is available at
https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tur_A/0/1/0/all/0/1&quot;&gt;Anil Osman Tur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1&quot;&gt;Nicola Dall&amp;#x27;Asen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1&quot;&gt;Cigdem Beyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01545">
<title>EffSeg: Efficient Fine-Grained Instance Segmentation using Structure-Preserving Sparsity. (arXiv:2307.01545v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01545</link>
<description rdf:parseType="Literal">&lt;p&gt;Many two-stage instance segmentation heads predict a coarse 28x28 mask per
instance, which is insufficient to capture the fine-grained details of many
objects. To address this issue, PointRend and RefineMask predict a 112x112
segmentation mask resulting in higher quality segmentations. Both methods
however have limitations by either not having access to neighboring features
(PointRend) or by performing computation at all spatial locations instead of
sparsely (RefineMask). In this work, we propose EffSeg performing fine-grained
instance segmentation in an efficient way by using our Structure-Preserving
Sparsity (SPS) method based on separately storing the active features, the
passive features and a dense 2D index map containing the feature indices. The
goal of the index map is to preserve the 2D spatial configuration or structure
between the features such that any 2D operation can still be performed. EffSeg
achieves similar performance on COCO compared to RefineMask, while reducing the
number of FLOPs by 71% and increasing the FPS by 29%. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picron_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Picron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01557">
<title>Separated RoadTopoFormer. (arXiv:2307.01557v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01557</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding driving scenarios is crucial to realizing autonomous driving.
Previous works such as map learning and BEV lane detection neglect the
connection relationship between lane instances, and traffic elements detection
tasks usually neglect the relationship with lane lines. To address these
issues, the task is presented which includes 4 sub-tasks, the detection of
traffic elements, the detection of lane centerlines, reasoning connection
relationships among lanes, and reasoning assignment relationships between lanes
and traffic elements. We present Separated RoadTopoFormer to tackle the issues,
which is an end-to-end framework that detects lane centerline and traffic
elements with reasoning relationships among them. We optimize each module
separately to prevent interaction with each other and aggregate them together
with few finetunes. For two detection heads, we adopted a DETR-like
architecture to detect objects, and for the relationship head, we concat two
instance features from front detectors and feed them to the classifier to
obtain relationship probability. Our final submission achieves 0.445 OLS, which
is competitive in both sub-task and combined scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mingjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanxian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinzhang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirasao_A/0/1/0/all/0/1&quot;&gt;Ashish Sirasao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01578">
<title>Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01578</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though data annotation is extremely important for interpretability,
research and development of artificial intelligence solutions, most research
efforts such as active learning or few-shot learning focus on the sample
efficiency problem. This paper studies the neglected complementary problem of
getting annotated data given a predictor. For the simple binary classification
setting, we present the spectrum ranging from optimal general solutions to
practical efficient methods. The problem is framed as the full annotation of a
binary classification dataset with the minimal number of yes/no questions when
a predictor is available. For the case of general binary questions the solution
is found in coding theory, where the optimal questioning strategy is given by
the Huffman encoding of the possible labelings. However, this approach is
computationally intractable even for small dataset sizes. We propose an
alternative practical solution based on several heuristics and lookahead
minimization of proxy cost functions. The proposed solution is analysed,
compared with optimal solutions and evaluated on several synthetic and
real-world datasets. On these datasets, the method allows a significant
improvement ($23-86\%$) in annotation efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchesoni_Acland_F/0/1/0/all/0/1&quot;&gt;Franco Marchesoni-Acland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morel_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Morel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kherroubi_J/0/1/0/all/0/1&quot;&gt;Josselin Kherroubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01582">
<title>IAdet: Simplest human-in-the-loop object detection. (arXiv:2307.01582v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01582</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a strategy for training models while annotating data named
Intelligent Annotation (IA). IA involves three modules: (1) assisted data
annotation, (2) background model training, and (3) active selection of the next
datapoints. Under this framework, we open-source the IAdet tool, which is
specific for single-class object detection. Additionally, we devise a method
for automatically evaluating such a human-in-the-loop system. For the PASCAL
VOC dataset, the IAdet tool reduces the database annotation time by $25\%$
while providing a trained model for free. These results are obtained for a
deliberately very simple IAdet design. As a consequence, IAdet is susceptible
to multiple easy improvements, paving the way for powerful human-in-the-loop
object detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchesoni_Acland_F/0/1/0/all/0/1&quot;&gt;Franco Marchesoni-Acland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01583">
<title>Learning Lie Group Symmetry Transformations with Neural Networks. (arXiv:2307.01583v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01583</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of detecting and quantifying the presence of symmetries in
datasets is useful for model selection, generative modeling, and data analysis,
amongst others. While existing methods for hard-coding transformations in
neural networks require prior knowledge of the symmetries of the task at hand,
this work focuses on discovering and characterizing unknown symmetries present
in the dataset, namely, Lie group symmetry transformations beyond the
traditional ones usually considered in the field (rotation, scaling, and
translation). Specifically, we consider a scenario in which a dataset has been
transformed by a one-parameter subgroup of transformations with different
parameter values for each data point. Our goal is to characterize the
transformation group and the distribution of the parameter values. The results
showcase the effectiveness of the approach in both these settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabel_A/0/1/0/all/0/1&quot;&gt;Alex Gabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_V/0/1/0/all/0/1&quot;&gt;Victoria Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valperga_R/0/1/0/all/0/1&quot;&gt;Riccardo Valperga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_J/0/1/0/all/0/1&quot;&gt;Jeroen S. W. Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1&quot;&gt;Kevin Webster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quax_R/0/1/0/all/0/1&quot;&gt;Rick Quax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1&quot;&gt;Efstratios Gavves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01630">
<title>ChildPlay: A New Benchmark for Understanding Children&apos;s Gaze Behaviour. (arXiv:2307.01630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01630</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaze behaviors such as eye-contact or shared attention are important markers
for diagnosing developmental disorders in children. While previous studies have
looked at some of these elements, the analysis is usually performed on private
datasets and is restricted to lab settings. Furthermore, all publicly available
gaze target prediction benchmarks mostly contain instances of adults, which
makes models trained on them less applicable to scenarios with young children.
In this paper, we propose the first study for predicting the gaze target of
children and interacting adults. To this end, we introduce the ChildPlay
dataset: a curated collection of short video clips featuring children playing
and interacting with adults in uncontrolled environments (e.g. kindergarten,
therapy centers, preschools etc.), which we annotate with rich gaze
information. We further propose a new model for gaze target prediction that is
geometrically grounded by explicitly identifying the scene parts in the 3D
field of view (3DFoV) of the person, leveraging recent geometry preserving
depth inference methods. Our model achieves state of the art results on
benchmark datasets and ChildPlay. Furthermore, results show that looking at
faces prediction performance on children is much worse than on adults, and can
be significantly improved by fine-tuning models using child gaze annotations.
Our dataset and models will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tafasca_S/0/1/0/all/0/1&quot;&gt;Samy Tafasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Anshul Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odobez_J/0/1/0/all/0/1&quot;&gt;Jean-Marc Odobez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01645">
<title>In-Domain Self-Supervised Learning Can Lead to Improvements in Remote Sensing Image Classification. (arXiv:2307.01645v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01645</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has emerged as a promising approach for remote
sensing image classification due to its ability to leverage large amounts of
unlabeled data. In contrast to traditional supervised learning, SSL aims to
learn representations of data without the need for explicit labels. This is
achieved by formulating auxiliary tasks that can be used to create
pseudo-labels for the unlabeled data and learn pre-trained models. The
pre-trained models can then be fine-tuned on downstream tasks such as remote
sensing image scene classification. The paper analyzes the effectiveness of SSL
pre-training using Million AID - a large unlabeled remote sensing dataset on
various remote sensing image scene classification datasets as downstream tasks.
More specifically, we evaluate the effectiveness of SSL pre-training using the
iBOT framework coupled with Vision transformers (ViT) in contrast to supervised
pre-training of ViT using the ImageNet dataset. The comprehensive experimental
work across 14 datasets with diverse properties reveals that in-domain SSL
leads to improved predictive performance of models compared to the supervised
counterparts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrovski_I/0/1/0/all/0/1&quot;&gt;Ivica Dimitrovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitanovski_I/0/1/0/all/0/1&quot;&gt;Ivan Kitanovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1&quot;&gt;Nikola Simidjievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1&quot;&gt;Dragi Kocev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01651">
<title>Task Planning Support for Arborists and Foresters: Comparing Deep Learning Approaches for Tree Inventory and Tree Vitality Assessment Based on UAV-Data. (arXiv:2307.01651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate crisis and correlating prolonged, more intense periods of drought
threaten tree health in cities and forests. In consequence, arborists and
foresters suffer from increasing workloads and, in the best case, a consistent
but often declining workforce. To optimise workflows and increase productivity,
we propose a novel open-source end-to-end approach that generates helpful
information and improves task planning of those who care for trees in and
around cities. Our approach is based on RGB and multispectral UAV data, which
is used to create tree inventories of city parks and forests and to deduce tree
vitality assessments through statistical indices and Deep Learning. Due to EU
restrictions regarding flying drones in urban areas, we will also use
multispectral satellite data and fifteen soil moisture sensors to extend our
tree vitality-related basis of data. Furthermore, Bamberg already has a
georeferenced tree cadastre of around 15,000 solitary trees in the city area,
which is also used to generate helpful information. All mentioned data is then
joined and visualised in an interactive web application allowing arborists and
foresters to generate individual and flexible evaluations, thereby improving
daily task planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troles_J/0/1/0/all/0/1&quot;&gt;Jonas-Dario Troles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nieding_R/0/1/0/all/0/1&quot;&gt;Richard Nieding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simons_S/0/1/0/all/0/1&quot;&gt;Sonia Simons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1&quot;&gt;Ute Schmid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01663">
<title>Exploring Transformers for On-Line Handwritten Signature Verification. (arXiv:2307.01663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01663</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of mobile biometrics as a user-friendly authentication method
has increased in the last years. Recent studies have proposed novel behavioral
biometric recognition systems based on Transformers, which currently outperform
the state of the art in several application scenarios. On-line handwritten
signature verification aims to verify the identity of subjects, based on their
biometric signatures acquired using electronic devices such as tablets or
smartphones. This paper investigates the suitability of architectures based on
recent Transformers for on-line signature verification. In particular, four
different configurations are studied, two of them rely on the Vanilla
Transformer encoder, and the two others have been successfully applied to the
tasks of gait and activity recognition. We evaluate the four proposed
configurations according to the experimental protocol proposed in the
SVC-onGoing competition. The results obtained in our experiments are promising,
and promote the use of Transformers for on-line signature verification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melzi_P/0/1/0/all/0/1&quot;&gt;Pietro Melzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delgado_Santos_P/0/1/0/all/0/1&quot;&gt;Paula Delgado-Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stragapede_G/0/1/0/all/0/1&quot;&gt;Giuseppe Stragapede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1&quot;&gt;Javier Ortega-Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01666">
<title>Sensors and Systems for Monitoring Mental Fatigue: A systematic review. (arXiv:2307.01666v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.01666</link>
<description rdf:parseType="Literal">&lt;p&gt;Mental fatigue is a leading cause of motor vehicle accidents, medical errors,
loss of workplace productivity, and student disengagements in e-learning
environment. Development of sensors and systems that can reliably track mental
fatigue can prevent accidents, reduce errors, and help increase workplace
productivity. This review provides a critical summary of theoretical models of
mental fatigue, a description of key enabling sensor technologies, and a
systematic review of recent studies using biosensor-based systems for tracking
mental fatigue in humans. We conducted a systematic search and review of recent
literature which focused on detection and tracking of mental fatigue in humans.
The search yielded 57 studies (N=1082), majority of which used
electroencephalography (EEG) based sensors for tracking mental fatigue. We
found that EEG-based sensors can provide a moderate to good sensitivity for
fatigue detection. Notably, we found no incremental benefit of using
high-density EEG sensors for application in mental fatigue detection. Given the
findings, we provide a critical discussion on the integration of wearable EEG
and ambient sensors in the context of achieving real-world monitoring. Future
work required to advance and adapt the technologies toward widespread
deployment of wearable sensors and systems for fatigue monitoring in
semi-autonomous and autonomous industries is examined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Prabin Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Justus_J/0/1/0/all/0/1&quot;&gt;Joanna C. Justus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poudel_G/0/1/0/all/0/1&quot;&gt;Govinda R. Poudel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01668">
<title>Training Energy-Based Models with Diffusion Contrastive Divergences. (arXiv:2307.01668v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01668</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy-Based Models (EBMs) have been widely used for generative modeling.
Contrastive Divergence (CD), a prevailing training objective for EBMs, requires
sampling from the EBM with Markov Chain Monte Carlo methods (MCMCs), which
leads to an irreconcilable trade-off between the computational burden and the
validity of the CD. Running MCMCs till convergence is computationally
intensive. On the other hand, short-run MCMC brings in an extra non-negligible
parameter gradient term that is difficult to handle. In this paper, we provide
a general interpretation of CD, viewing it as a special instance of our
proposed Diffusion Contrastive Divergence (DCD) family. By replacing the
Langevin dynamic used in CD with other EBM-parameter-free diffusion processes,
we propose a more efficient divergence. We show that the proposed DCDs are both
more computationally efficient than the CD and are not limited to a
non-negligible gradient term. We conduct intensive experiments, including both
synthesis data modeling and high-dimensional image denoising and generation, to
show the advantages of the proposed DCDs. On the synthetic data learning and
image denoising experiments, our proposed DCD outperforms CD by a large margin.
In image generation experiments, the proposed DCD is capable of training an
energy-based model for generating the Celab-A $32\times 32$ dataset, which is
comparable to existing EBMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weijian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tianyang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01683">
<title>Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01683</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision and machine learning, a crucial challenge is to lower the
computation and memory demands for neural network inference. A commonplace
solution to address this challenge is through the use of binarization. By
binarizing the network weights and activations, one can significantly reduce
computational complexity by substituting the computationally expensive floating
operations with faster bitwise operations. This leads to a more efficient
neural network inference that can be deployed on low-resource devices. In this
work, we extend previous approaches that trained networks with discrete weights
using the local reparameterization trick to also allow for discrete
activations. The original approach optimized a distribution over the discrete
weights and uses the central limit theorem to approximate the pre-activation
with a continuous Gaussian distribution. Here we show that the probabilistic
modeling can also allow effective training of networks with discrete activation
as well. This further reduces runtime and memory footprint at inference time
with state-of-the-art results for networks with binary activations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_G/0/1/0/all/0/1&quot;&gt;Guy Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1&quot;&gt;Aviv Navon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01694">
<title>Spike-driven Transformer. (arXiv:2307.01694v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.01694</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs) provide an energy-efficient deep learning
option due to their unique spike-based event-driven (i.e., spike-driven)
paradigm. In this paper, we incorporate the spike-driven paradigm into
Transformer by the proposed Spike-driven Transformer with four unique
properties: 1) Event-driven, no calculation is triggered when the input of
Transformer is zero; 2) Binary spike communication, all matrix multiplications
associated with the spike matrix can be transformed into sparse additions; 3)
Self-attention with linear complexity at both token and channel dimensions; 4)
The operations between spike-form Query, Key, and Value are mask and addition.
Together, there are only sparse addition operations in the Spike-driven
Transformer. To this end, we design a novel Spike-Driven Self-Attention (SDSA),
which exploits only mask and addition operations without any multiplication,
and thus having up to $87.2\times$ lower computation energy than vanilla
self-attention. Especially in SDSA, the matrix multiplication between Query,
Key, and Value is designed as the mask operation. In addition, we rearrange all
residual connections in the vanilla Transformer before the activation functions
to ensure that all neurons transmit binary spike signals. It is shown that the
Spike-driven Transformer can achieve 77.1\% top-1 accuracy on ImageNet-1K,
which is the state-of-the-art result in the SNN field. The source code is
available at https://github.com/BICLab/Spike-Driven-Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1&quot;&gt;Man Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiakui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhaokun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01703">
<title>Augment Features Beyond Color for Domain Generalized Segmentation. (arXiv:2307.01703v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01703</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalized semantic segmentation (DGSS) is an essential but highly
challenging task, in which the model is trained only on source data and any
target data is not available. Previous DGSS methods can be partitioned into
augmentation-based and normalization-based ones. The former either introduces
extra biased data or only conducts channel-wise adjustments for data
augmentation, and the latter may discard beneficial visual information, both of
which lead to limited performance in DGSS. Contrarily, our method performs
inter-channel transformation and meanwhile evades domain-specific biases, thus
diversifying data and enhancing model generalization performance. Specifically,
our method consists of two modules: random image color augmentation (RICA) and
random feature distribution augmentation (RFDA). RICA converts images from RGB
to the CIELAB color model and randomizes color maps in a perception-based way
for image enhancement purposes. We further this augmentation by extending it
beyond color to feature space using a CycleGAN-based generative network, which
complements RICA and further boosts generalization capability. We conduct
extensive experiments, and the generalization results from the synthetic GTAV
and SYNTHIA to the real Cityscapes, BDDS, and Mapillary datasets show that our
method achieves state-of-the-art performance in DGSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1&quot;&gt;Pavlo Melnyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1&quot;&gt;Michael Felsberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01704">
<title>Graph-Ensemble Learning Model for Multi-label Skin Lesion Classification using Dermoscopy and Clinical Images. (arXiv:2307.01704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01704</link>
<description rdf:parseType="Literal">&lt;p&gt;Many skin lesion analysis (SLA) methods recently focused on developing a
multi-modal-based multi-label classification method due to two factors. The
first is multi-modal data, i.e., clinical and dermoscopy images, which can
provide complementary information to obtain more accurate results than
single-modal data. The second one is that multi-label classification, i.e.,
seven-point checklist (SPC) criteria as an auxiliary classification task can
not only boost the diagnostic accuracy of melanoma in the deep learning (DL)
pipeline but also provide more useful functions to the clinical doctor as it is
commonly used in clinical dermatologist&apos;s diagnosis. However, most methods only
focus on designing a better module for multi-modal data fusion; few methods
explore utilizing the label correlation between SPC and skin disease for
performance improvement. This study fills the gap that introduces a Graph
Convolution Network (GCN) to exploit prior co-occurrence between each category
as a correlation matrix into the DL model for the multi-label classification.
However, directly applying GCN degraded the performances in our experiments; we
attribute this to the weak generalization ability of GCN in the scenario of
insufficient statistical samples of medical data. We tackle this issue by
proposing a Graph-Ensemble Learning Model (GELN) that views the prediction from
GCN as complementary information of the predictions from the fusion model and
adaptively fuses them by a weighted averaging scheme, which can utilize the
valuable information from GCN while avoiding its negative influences as much as
possible. To evaluate our method, we conduct experiments on public datasets.
The results illustrate that our GELN can consistently improve the
classification performance on different datasets and that the proposed method
can achieve state-of-the-art performance in SPC and diagnosis classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Peng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_Y/0/1/0/all/0/1&quot;&gt;Yang Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01738">
<title>Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis. (arXiv:2307.01738v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.01738</link>
<description rdf:parseType="Literal">&lt;p&gt;Trustworthy deployment of deep learning medical imaging models into
real-world clinical practice requires that they be calibrated. However, models
that are well calibrated overall can still be poorly calibrated for a
sub-population, potentially resulting in a clinician unwittingly making poor
decisions for this group based on the recommendations of the model. Although
methods have been shown to successfully mitigate biases across subgroups in
terms of model accuracy, this work focuses on the open problem of mitigating
calibration biases in the context of medical image analysis. Our method does
not require subgroup attributes during training, permitting the flexibility to
mitigate biases for different choices of sensitive attributes without
re-training. To this end, we propose a novel two-stage method: Cluster-Focal to
first identify poorly calibrated samples, cluster them into groups, and then
introduce group-wise focal loss to improve calibration bias. We evaluate our
method on skin lesion classification with the public HAM10000 dataset, and on
predicting future lesional activity for multiple sclerosis (MS) patients. In
addition to considering traditional sensitive attributes (e.g. age, sex) with
demographic subgroups, we also consider biases among groups with different
image-derived attributes, such as lesion load, which are required in medical
image analysis. Our results demonstrate that our method effectively controls
calibration error in the worst-performing subgroups while preserving prediction
performance, and outperforming recent baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shui_C/0/1/0/all/0/1&quot;&gt;Changjian Shui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1&quot;&gt;Justin Szeto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1&quot;&gt;Raghav Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1&quot;&gt;Douglas Arnold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1&quot;&gt;Tal Arbel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01740">
<title>Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT. (arXiv:2307.01740v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01740</link>
<description rdf:parseType="Literal">&lt;p&gt;Stroke lesion volume is a key radiologic measurement for assessing the
prognosis of Acute Ischemic Stroke (AIS) patients, which is challenging to be
automatically measured on Non-Contrast CT (NCCT) scans. Recent diffusion
probabilistic models have shown potentials of being used for image
segmentation. In this paper, a novel Synchronous image-label Diffusion
Probability Model (SDPM) is proposed for stroke lesion segmentation on NCCT
using Markov diffusion process. The proposed SDPM is fully based on a Latent
Variable Model (LVM), offering a complete probabilistic elaboration. An
additional net-stream, parallel with a noise prediction stream, is introduced
to obtain initial noisy label estimates for efficiently inferring the final
labels. By optimizing the specified variational boundaries, the trained model
can infer multiple label estimates for reference given the input images with
noises. The proposed model was assessed on three stroke lesion datasets
including one public and two private datasets. Compared to several U-net and
transformer-based segmentation methods, our proposed SDPM model is able to
achieve state-of-the-art performance. The code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianhai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_T/0/1/0/all/0/1&quot;&gt;Tonghua Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDonald_E/0/1/0/all/0/1&quot;&gt;Ethan MacDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_A/0/1/0/all/0/1&quot;&gt;Aravind Ganesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01741">
<title>Ben-ge: Extending BigEarthNet with Geographical and Environmental Data. (arXiv:2307.01741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01741</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning methods have proven to be a powerful tool in the analysis of
large amounts of complex Earth observation data. However, while Earth
observation data are multi-modal in most cases, only single or few modalities
are typically considered. In this work, we present the ben-ge dataset, which
supplements the BigEarthNet-MM dataset by compiling freely and globally
available geographical and environmental data. Based on this dataset, we
showcase the value of combining different data modalities for the downstream
tasks of patch-based land-use/land-cover classification and land-use/land-cover
segmentation. ben-ge is freely available and expected to serve as a test bed
for fully supervised and self-supervised Earth observation applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mommert_M/0/1/0/all/0/1&quot;&gt;Michael Mommert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesseli_N/0/1/0/all/0/1&quot;&gt;Nicolas Kesseli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;lle Hanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheibenreif_L/0/1/0/all/0/1&quot;&gt;Linus Scheibenreif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borth_D/0/1/0/all/0/1&quot;&gt;Damian Borth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1&quot;&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01750">
<title>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01750</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a novel framework for single-domain generalized object
detection (i.e., Single-DGOD), where we are interested in learning and
maintaining the semantic structures of self-augmented compound cross-domain
samples to enhance the model&apos;s generalization ability. Different from DGOD
trained on multiple source domains, Single-DGOD is far more challenging to
generalize well to multiple target domains with only one single source domain.
Existing methods mostly adopt a similar treatment from DGOD to learn
domain-invariant features by decoupling or compressing the semantic space.
However, there may have two potential limitations: 1) pseudo attribute-label
correlation, due to extremely scarce single-domain data; and 2) the semantic
structural information is usually ignored, i.e., we found the affinities of
instance-level semantic relations in samples are crucial to model
generalization. In this paper, we introduce Semantic Reasoning with Compound
Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main
components, namely, the texture-based self-augmentation (TBSA) module, and the
local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the
effects of irrelevant attributes associated with labels, such as light, shadow,
color, etc., at the image level by a light-yet-efficient self-augmentation.
Moreover, LGSR is used to further model the semantic relationships on instance
features to uncover and maintain the intrinsic semantic structures. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of the
proposed SRCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhijie Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Luyao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xinghao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01754">
<title>K-complex Detection Using Fourier Spectrum Analysis In EEG. (arXiv:2307.01754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01754</link>
<description rdf:parseType="Literal">&lt;p&gt;K-complexes are an important marker of brain activity and are used both in
clinical practice to perform sleep scoring, and in research. However, due to
the size of electroencephalography (EEG) records, as well as the subjective
nature of K-complex detection performed by somnologists, it is reasonable to
automate K-complex detection. Previous works in this field of research have
relied on the values of true positive rate and false positive rate to quantify
the effectiveness of proposed methods, however this set of metrics may be
misleading. The objective of the present research is to find a more accurate
set of metrics and use them to develop a new method of K-complex detection,
which would not rely on neural networks. Thus, the present article proposes two
new methods for K-complex detection based on the fast Fourier transform. The
results achieved demonstrated that the proposed methods offered a quality of
K-complex detection that is either similar or superior to the quality of the
methods demonstrated in previous works, including the methods employing neural
networks, while requiring less computational power, meaning that K-complex
detection does not require the use of neural networks. The proposed methods
were evaluated using a new set of metrics, which is more representative of the
quality of K-complex detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Protopopov_A/0/1/0/all/0/1&quot;&gt;Alexey Protopopov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01759">
<title>Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification. (arXiv:2307.01759v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01759</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism spectrum disorder (ASD) is a prevalent psychiatric condition
characterized by atypical cognitive, emotional, and social patterns. Timely and
accurate diagnosis is crucial for effective interventions and improved outcomes
in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced
Transformer framework, METAFormer, ASD classification. Our framework utilizes
resting-state functional magnetic resonance imaging data from the ABIDE I
dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer
employs a multi-atlas approach, where flattened connectivity matrices from the
AAL, CC200, and DOS160 atlases serve as input to the transformer encoder.
Notably, we demonstrate that self-supervised pretraining, involving the
reconstruction of masked values from the input, significantly enhances
classification performance without the need for additional or separate training
data. Through stratified cross-validation, we evaluate the proposed framework
and show that it surpasses state-of-the-art performance on the ABIDE I dataset,
with an average accuracy of 83.7% and an AUC-score of 0.832. The code for our
framework is available at https://github.com/Lugges991/METAFormer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahler_L/0/1/0/all/0/1&quot;&gt;Lucas Mahler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiglechner_J/0/1/0/all/0/1&quot;&gt;Julius Steiglechner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birk_F/0/1/0/all/0/1&quot;&gt;Florian Birk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heczko_S/0/1/0/all/0/1&quot;&gt;Samuel Heczko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheffler_K/0/1/0/all/0/1&quot;&gt;Klaus Scheffler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohmann_G/0/1/0/all/0/1&quot;&gt;Gabriele Lohmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01767">
<title>Localized Data Work as a Precondition for Data-Centric ML: A Case Study of Full Lifecycle Crop Disease Identification in Ghana. (arXiv:2307.01767v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01767</link>
<description rdf:parseType="Literal">&lt;p&gt;The Ghana Cashew Disease Identification with Artificial Intelligence (CADI
AI) project demonstrates the importance of sound data work as a precondition
for the delivery of useful, localized datacentric solutions for public good
tasks such as agricultural productivity and food security. Drone collected data
and machine learning are utilized to determine crop stressors. Data, model and
the final app are developed jointly and made available to local farmers via a
desktop application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akogo_D/0/1/0/all/0/1&quot;&gt;Darlington Akogo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samori_I/0/1/0/all/0/1&quot;&gt;Issah Samori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akafia_C/0/1/0/all/0/1&quot;&gt;Cyril Akafia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiagbor_H/0/1/0/all/0/1&quot;&gt;Harriet Fiagbor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kangah_A/0/1/0/all/0/1&quot;&gt;Andrews Kangah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiedu_D/0/1/0/all/0/1&quot;&gt;Donald Kwame Asiedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuachie_K/0/1/0/all/0/1&quot;&gt;Kwabena Fuachie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oala_L/0/1/0/all/0/1&quot;&gt;Luis Oala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01778">
<title>Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01778</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have proposed to craft adversarial clothes for evading person
detectors, while they are either only effective at limited viewing angles or
very conspicuous to humans. We aim to craft adversarial texture for clothes
based on 3D modeling, an idea that has been used to craft rigid adversarial
objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes
are non-rigid, leading to difficulties in physical realization. In order to
craft natural-looking adversarial clothes that can evade person detectors at
multiple viewing angles, we propose adversarial camouflage textures (AdvCaT)
that resemble one kind of the typical textures of daily clothes, camouflage
textures. We leverage the Voronoi diagram and Gumbel-softmax trick to
parameterize the camouflage textures and optimize the parameters via 3D
modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes
combining topologically plausible projection (TopoProj) and Thin Plate Spline
(TPS) to narrow the gap between digital and real-world objects. We printed the
developed 3D texture pieces on fabric materials and tailored them into T-shirts
and trousers. Experiments show high attack success rates of these clothes
against multiple detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhanhao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wenda Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaopei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01798">
<title>Edge-aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI. (arXiv:2307.01798v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.01798</link>
<description rdf:parseType="Literal">&lt;p&gt;Simultaneous multi-index quantification, segmentation, and uncertainty
estimation of liver tumors on multi-modality non-contrast magnetic resonance
imaging (NCMRI) are crucial for accurate diagnosis. However, existing methods
lack an effective mechanism for multi-modality NCMRI fusion and accurate
boundary information capture, making these tasks challenging. To address these
issues, this paper proposes a unified framework, namely edge-aware multi-task
network (EaMtNet), to associate multi-index quantification, segmentation, and
uncertainty of liver tumors on the multi-modality NCMRI. The EaMtNet employs
two parallel CNN encoders and the Sobel filters to extract local features and
edge maps, respectively. The newly designed edge-aware feature aggregation
module (EaFA) is used for feature fusion and selection, making the network
edge-aware by capturing long-range dependency between feature and edge maps.
Multi-tasking leverages prediction discrepancy to estimate uncertainty and
improve segmentation and quantification performance. Extensive experiments are
performed on multi-modality NCMRI with 250 clinical subjects. The proposed
model outperforms the state-of-the-art by a large margin, achieving a dice
similarity coefficient of 90.01$\pm$1.23 and a mean absolute error of
2.72$\pm$0.58 mm for MD. The results demonstrate the potential of EaMtNet as a
reliable clinical-aided tool for medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiaojiao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qinmin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanghui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01806">
<title>DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification. (arXiv:2307.01806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01806</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel learning paradigm called &quot;DeepFlorist&quot; for
flower classification using ensemble learning as a meta-classifier. DeepFlorist
combines the power of deep learning with the robustness of ensemble methods to
achieve accurate and reliable flower classification results. The proposed
network architecture leverages a combination of dense convolutional and
convolutional neural networks (DCNNs and CNNs) to extract high-level features
from flower images, followed by a fully connected layer for classification. To
enhance the performance and generalization of DeepFlorist, an ensemble learning
approach is employed, incorporating multiple diverse models to improve the
classification accuracy. Experimental results on benchmark flower datasets
demonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art
methods in terms of accuracy and robustness. The proposed framework holds
significant potential for automated flower recognition systems in real-world
applications, enabling advancements in plant taxonomy, conservation efforts,
and ecological studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadangi_A/0/1/0/all/0/1&quot;&gt;Afshin Khadangi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01807">
<title>SUIT: Learning Significance-guided Information for 3D Temporal Detection. (arXiv:2307.01807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01807</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection from LiDAR point cloud is of critical importance for
autonomous driving and robotics. While sequential point cloud has the potential
to enhance 3D perception through temporal information, utilizing these temporal
features effectively and efficiently remains a challenging problem. Based on
the observation that the foreground information is sparsely distributed in
LiDAR scenes, we believe sufficient knowledge can be provided by sparse format
rather than dense maps. To this end, we propose to learn Significance-gUided
Information for 3D Temporal detection (SUIT), which simplifies temporal
information as sparse features for information fusion across frames.
Specifically, we first introduce a significant sampling mechanism that extracts
information-rich yet sparse features based on predicted object centroids. On
top of that, we present an explicit geometric transformation learning
technique, which learns the object-centric transformations among sparse
features across frames. We evaluate our method on large-scale nuScenes and
Waymo dataset, where our SUIT not only significantly reduces the memory and
computation cost of temporal fusion, but also performs well over the
state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiachen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yihan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01817">
<title>Human Trajectory Forecasting with Explainable Behavioral Uncertainty. (arXiv:2307.01817v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01817</link>
<description rdf:parseType="Literal">&lt;p&gt;Human trajectory forecasting helps to understand and predict human behaviors,
enabling applications from social robots to self-driving cars, and therefore
has been heavily investigated. Most existing methods can be divided into
model-free and model-based methods. Model-free methods offer superior
prediction accuracy but lack explainability, while model-based methods provide
explainability but cannot predict well. Combining both methodologies, we
propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM,
where a behavior SDE model is combined with Bayesian neural networks (BNNs).
While the NNs provide superior predictive power, the SDE offers strong
explainability with quantifiable uncertainty in behavior and observation. We
show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy,
compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to
drastically different scenes with different environments and crowd densities (~
20 times higher than the testing data). Finally, BNSP-SFM can provide
predictions with confidence to better explain potential causes of behaviors.
The code will be released upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1&quot;&gt;Jiangbei Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01831">
<title>DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01831</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful
effectiveness in generating high-quality 2D images. However, it is still being
determined whether the Transformer architecture performs equally well in 3D
shape generation, as previous 3D diffusion methods mostly adopted the U-Net
architecture. To bridge this gap, we propose a novel Diffusion Transformer for
3D shape generation, namely DiT-3D, which can directly operate the denoising
process on voxelized point clouds using plain Transformers. Compared to
existing U-Net approaches, our DiT-3D is more scalable in model size and
produces much higher quality generations. Specifically, the DiT-3D adopts the
design philosophy of DiT but modifies it by incorporating 3D positional and
patch embeddings to adaptively aggregate input from voxelized point clouds. To
reduce the computational cost of self-attention in 3D shape generation, we
incorporate 3D window attention into Transformer blocks, as the increased 3D
token length resulting from the additional dimension of voxels can lead to high
computation. Finally, linear and devoxelization layers are used to predict the
denoised point clouds. In addition, our transformer architecture supports
efficient fine-tuning from 2D to 3D, where the pre-trained DiT-2D checkpoint on
ImageNet can significantly improve DiT-3D on ShapeNet. Experimental results on
the ShapeNet dataset demonstrate that the proposed DiT-3D achieves
state-of-the-art performance in high-fidelity and diverse 3D point cloud
generation. In particular, our DiT-3D decreases the 1-Nearest Neighbor Accuracy
of the state-of-the-art method by 4.59 and increases the Coverage metric by
3.51 when evaluated on Chamfer Distance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01836">
<title>On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution. (arXiv:2307.01836v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01836</link>
<description rdf:parseType="Literal">&lt;p&gt;We study matrix forms of quaternionic versions of the Fourier Transform and
Convolution operations. Quaternions offer a powerful representation unit,
however they are related to difficulties in their use that stem foremost from
non-commutativity of quaternion multiplication, and due to that $\mu^2 = -1$
posseses infinite solutions in the quaternion domain. Handling of quaternionic
matrices is consequently complicated in several aspects (definition of
eigenstructure, determinant, etc.). Our research findings clarify the relation
of the Quaternion Fourier Transform matrix to the standard (complex) Discrete
Fourier Transform matrix, and the extend on which well-known complex-domain
theorems extend to quaternions. We focus especially on the relation of
Quaternion Fourier Transform matrices to Quaternion Circulant matrices
(representing quaternionic convolution), and the eigenstructure of the latter.
A proof-of-concept application that makes direct use of our theoretical results
is presented, where we produce a method to bound the spectral norm of a
Quaternionic Convolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sfikas_G/0/1/0/all/0/1&quot;&gt;Giorgos Sfikas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Retsinas_G/0/1/0/all/0/1&quot;&gt;George Retsinas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01838">
<title>EdgeFace: Efficient Face Recognition Model for Edge Devices. (arXiv:2307.01838v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01838</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present EdgeFace, a lightweight and efficient face
recognition network inspired by the hybrid architecture of EdgeNeXt. By
effectively combining the strengths of both CNN and Transformer models, and a
low rank linear layer, EdgeFace achieves excellent face recognition performance
optimized for edge devices. The proposed EdgeFace network not only maintains
low computational costs and compact storage, but also achieves high face
recognition accuracy, making it suitable for deployment on edge devices.
Extensive experiments on challenging benchmark face datasets demonstrate the
effectiveness and efficiency of EdgeFace in comparison to state-of-the-art
lightweight models and deep face recognition models. Our EdgeFace model with
1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B
(92.67%), and IJB-C (94.85%), outperforming other efficient models with larger
computational complexities. The code to replicate the experiments will be made
available publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1&quot;&gt;Anjith George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecabert_C/0/1/0/all/0/1&quot;&gt;Christophe Ecabert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahreza_H/0/1/0/all/0/1&quot;&gt;Hatef Otroshi Shahreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotwal_K/0/1/0/all/0/1&quot;&gt;Ketan Kotwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1&quot;&gt;Sebastien Marcel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01844">
<title>Advancing Wound Filling Extraction on 3D Faces: A Auto-Segmentation and Wound Face Regeneration Approach. (arXiv:2307.01844v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01844</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial wound segmentation plays a crucial role in preoperative planning and
optimizing patient outcomes in various medical applications. In this paper, we
propose an efficient approach for automating 3D facial wound segmentation using
a two-stream graph convolutional network. Our method leverages the Cir3D-FaIR
dataset and addresses the challenge of data imbalance through extensive
experimentation with different loss functions. To achieve accurate
segmentation, we conducted thorough experiments and selected a high-performing
model from the trained models. The selected model demonstrates exceptional
segmentation performance for complex 3D facial wounds. Furthermore, based on
the segmentation model, we propose an improved approach for extracting 3D
facial wound fillers and compare it to the results of the previous study. Our
method achieved a remarkable accuracy of 0.9999986\% on the test suite,
surpassing the performance of the previous method. From this result, we use 3D
printing technology to illustrate the shape of the wound filling. The outcomes
of this study have significant implications for physicians involved in
preoperative planning and intervention design. By automating facial wound
segmentation and improving the accuracy of wound-filling extraction, our
approach can assist in carefully assessing and optimizing interventions,
leading to enhanced patient outcomes. Additionally, it contributes to advancing
facial reconstruction techniques by utilizing machine learning and 3D
bioprinting for printing skin tissue implants. Our source code is available at
\url{https://github.com/SIMOGroup/WoundFilling3D}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Q. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thinh D. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong D. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Xuan_H/0/1/0/all/0/1&quot;&gt;H. Nguyen-Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01845">
<title>Deep Features for Contactless Fingerprint Presentation Attack Detection: Can They Be Generalized?. (arXiv:2307.01845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01845</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid evolution of high-end smartphones with advanced high-resolution
cameras has resulted in contactless capture of fingerprint biometrics that are
more reliable and suitable for verification. Similar to other biometric
systems, contactless fingerprint-verification systems are vulnerable to
presentation attacks. In this paper, we present a comparative study on the
generalizability of seven different pre-trained Convolutional Neural Networks
(CNN) and a Vision Transformer (ViT) to reliably detect presentation attacks.
Extensive experiments were carried out on publicly available smartphone-based
presentation attack datasets using four different Presentation Attack
Instruments (PAI). The detection performance of the eighth deep feature
technique was evaluated using the leave-one-out protocol to benchmark the
generalization performance for unseen PAI. The obtained results indicated the
best generalization performance with the ResNet50 CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hailin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1&quot;&gt;Raghavendra Ramachandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01846">
<title>Grad-FEC: Unequal Loss Protection of Deep Features in Collaborative Intelligence. (arXiv:2307.01846v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.01846</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative intelligence (CI) involves dividing an artificial intelligence
(AI) model into two parts: front-end, to be deployed on an edge device, and
back-end, to be deployed in the cloud. The deep feature tensors produced by the
front-end are transmitted to the cloud through a communication channel, which
may be subject to packet loss. To address this issue, in this paper, we propose
a novel approach to enhance the resilience of the CI system in the presence of
packet loss through Unequal Loss Protection (ULP). The proposed ULP approach
involves a feature importance estimator, which estimates the importance of
feature packets produced by the front-end, and then selectively applies Forward
Error Correction (FEC) codes to protect important packets. Experimental results
demonstrate that the proposed approach can significantly improve the
reliability and robustness of the CI system in the presence of packet loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uyanik_K/0/1/0/all/0/1&quot;&gt;Korcan Uyanik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yeganli_S/0/1/0/all/0/1&quot;&gt;S. Faegheh Yeganli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bajic_I/0/1/0/all/0/1&quot;&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01848">
<title>Embodied Task Planning with Large Language Models. (arXiv:2307.01848v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.01848</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipping embodied agents with commonsense is important for robots to
successfully complete complex human instructions in general environments.
Recent large language models (LLM) can embed rich semantic knowledge for agents
in plan generation of complex tasks, while they lack the information about the
realistic world and usually yield infeasible action sequences. In this paper,
we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning
with physical scene constraint, where the agent generates executable plans
according to the existed objects in the scene by aligning LLMs with the visual
perception models. Specifically, we first construct a multimodal dataset
containing triplets of indoor scenes, instructions and action plans, where we
provide the designed prompts and the list of existing objects in the scene for
GPT-3.5 to generate a large number of instructions and corresponding planned
actions. The generated data is leveraged for grounded plan tuning of
pre-trained LLMs. During inference, we discover the objects in the scene by
extending open-vocabulary object detectors to multi-view RGB images collected
in different achievable locations. Experimental results show that the generated
plan from our TaPA framework can achieve higher success rate than LLaVA and
GPT-3.5 by a sizable margin, which indicates the practicality of embodied task
planning in general and complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiuwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Haibin Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01849">
<title>Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.01849</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequence modeling approaches have shown promising results in robot imitation
learning. Recently, diffusion models have been adopted for behavioral cloning,
benefiting from their exceptional capabilities in modeling complex data
distribution. In this work, we propose Crossway Diffusion, a method to enhance
diffusion-based visuomotor policy learning by using an extra self-supervised
learning (SSL) objective. The standard diffusion-based policy generates action
sequences from random noise conditioned on visual observations and other
low-dimensional states. We further extend this by introducing a new decoder
that reconstructs raw image pixels (and other state information) from the
intermediate representations of the reverse diffusion process, and train the
model jointly using the SSL loss. Our experiments demonstrate the effectiveness
of Crossway Diffusion in various simulated and real-world robot tasks,
confirming its advantages over the standard diffusion-based policy. We
demonstrate that such self-supervised reconstruction enables better
representation for policy learning, especially when the demonstrations have
different proficiencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belagali_V/0/1/0/all/0/1&quot;&gt;Varun Belagali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jinghuan Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.09053">
<title>Cross-Shape Attention for Part Segmentation of 3D Point Clouds. (arXiv:2003.09053v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.09053</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a deep learning method that propagates point-wise feature
representations across shapes within a collection for the purpose of 3D shape
segmentation. We propose a cross-shape attention mechanism to enable
interactions between a shape&apos;s point-wise features and those of other shapes.
The mechanism assesses both the degree of interaction between points and also
mediates feature propagation across shapes, improving the accuracy and
consistency of the resulting point-wise feature representations for shape
segmentation. Our method also proposes a shape retrieval measure to select
suitable shapes for cross-shape attention operations for each test shape. Our
experiments demonstrate that our approach yields state-of-the-art results in
the popular PartNet dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1&quot;&gt;Marios Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddhant Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Petrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1&quot;&gt;Melinos Averkiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1&quot;&gt;Evangelos Kalogerakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.14956">
<title>Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.14956</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from noisy labels is an important concern because of the lack of
accurate ground-truth labels in plenty of real-world scenarios. In practice,
various approaches for this concern first make some corrections corresponding
to potentially noisy-labeled instances, and then update predictive model with
information of the made corrections. However, in specific areas, such as
medical histopathology whole slide image analysis (MHWSIA), it is often
difficult or even impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. In this paper, we
focus on alleviating these two problems. For the problem 1), we present
one-step abductive multi-target learning (OSAMTL) that imposes a one-step
logical reasoning upon machine learning via a multi-target learning procedure
to constrain the predictions of the learning model to be subject to our prior
knowledge about the true target. For the problem 2), we propose a logical
assessment formula (LAF) that evaluates the logical rationality of the outputs
of an approach by estimating the consistencies between the predictions of the
learning model and the logical facts narrated from the results of the one-step
logical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori
(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable
the machine learning model achieving logically more rational predictions, which
is beyond various state-of-the-art approaches in handling complex noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiayi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhongxi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.13392">
<title>Deep Learning-Based Human Pose Estimation: A Survey. (arXiv:2012.13392v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.13392</link>
<description rdf:parseType="Literal">&lt;p&gt;Human pose estimation aims to locate the human body parts and build human
body representation (e.g., body skeleton) from input data such as images and
videos. It has drawn increasing attention during the past decade and has been
utilized in a wide range of applications including human-computer interaction,
motion analysis, augmented reality, and virtual reality. Although the recently
developed deep learning-based solutions have achieved high performance in human
pose estimation, there still remain challenges due to insufficient training
data, depth ambiguities, and occlusion. The goal of this survey paper is to
provide a comprehensive review of recent deep learning-based solutions for both
2D and 3D pose estimation via a systematic analysis and comparison of these
solutions based on their input data and inference procedures. More than 250
research papers since 2014 are covered in this survey. Furthermore, 2D and 3D
human pose estimation datasets and evaluation metrics are included.
Quantitative performance comparisons of the reviewed methods on popular
datasets are summarized and discussed. Finally, the challenges involved,
applications, and future research directions are concluded. A regularly updated
project page is provided: \url{https://github.com/zczcwh/DL-HPE}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Ce Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenhan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Taojiannan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Ju Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kehtarnavaz_N/0/1/0/all/0/1&quot;&gt;Nasser Kehtarnavaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2101.07458">
<title>Hybrid Trilinear and Bilinear Programming for Aligning Partially Overlapping Point Sets. (arXiv:2101.07458v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2101.07458</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications, we need algorithms which can align partially
overlapping point sets and are invariant to the corresponding transformations.
In this work, a method possessing such properties is realized by minimizing the
objective of the robust point matching (RPM) algorithm. We first show that the
RPM objective is a cubic polynomial. We then utilize the convex envelopes of
trilinear and bilinear monomials to derive its lower bound function. The
resulting lower bound problem has the merit that it can be efficiently solved
via linear assignment and low dimensional convex quadratic programming. We next
develop a branch-and-bound (BnB) algorithm which only branches over the
transformation variables and runs efficiently. Experimental results
demonstrated better robustness of the proposed method against non-rigid
deformation, positional noise and outliers in case that outliers are not mixed
with inliers when compared with the state-of-the-art approaches. They also
showed that it has competitive efficiency and scales well with problem size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_W/0/1/0/all/0/1&quot;&gt;Wei Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2101.11183">
<title>DeepOIS: Gyroscope-Guided Deep Optical Image Stabilizer Compensation. (arXiv:2101.11183v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2101.11183</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile captured images can be aligned using their gyroscope sensors. Optical
image stabilizer (OIS) terminates this possibility by adjusting the images
during the capturing. In this work, we propose a deep network that compensates
the motions caused by the OIS, such that the gyroscopes can be used for image
alignment on the OIS cameras. To achieve this, first, we record both videos and
gyroscopes with an OIS camera as training data. Then, we convert gyroscope
readings into motion fields. Second, we propose a Fundamental Mixtures motion
model for rolling shutter cameras, where an array of rotations within a frame
are extracted as the ground-truth guidance. Third, we train a convolutional
neural network with gyroscope motions as input to compensate for the OIS
motion. Once finished, the compensation network can be applied for other
scenes, where the image alignment is purely based on gyroscopes with no need
for images contents, delivering strong robustness. Experiments show that our
results are comparable with that of non-OIS cameras, and outperform image-based
alignment results with a relatively large margin. Code and dataset are
available at https://github.com/lhaippp/DeepOIS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haipeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.00717">
<title>Synthetic Data for Model Selection. (arXiv:2105.00717v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2105.00717</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in synthetic data generation approaches made it possible
to produce highly photorealistic images which are hardly distinguishable from
real ones. Furthermore, synthetic generation pipelines have the potential to
generate an unlimited number of images. The combination of high photorealism
and scale turn synthetic data into a promising candidate for improving various
machine learning (ML) pipelines. Thus far, a large body of research in this
field has focused on using synthetic images for training, by augmenting and
enlarging training data. In contrast to using synthetic data for training, in
this work we explore whether synthetic data can be beneficial for model
selection. Considering the task of image classification, we demonstrate that
when data is scarce, synthetic data can be used to replace the held out
validation set, thus allowing to train on a larger dataset. We also introduce a
novel method to calibrate the synthetic error estimation to fit that of the
real domain. We show that such calibration significantly improves the
usefulness of synthetic data for model selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoshan_A/0/1/0/all/0/1&quot;&gt;Alon Shoshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhonker_N/0/1/0/all/0/1&quot;&gt;Nadav Bhonker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kviatkovsky_I/0/1/0/all/0/1&quot;&gt;Igor Kviatkovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fintz_M/0/1/0/all/0/1&quot;&gt;Matan Fintz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1&quot;&gt;Gerard Medioni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.06847">
<title>Video Super-Resolution Transformer. (arXiv:2106.06847v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.06847</link>
<description rdf:parseType="Literal">&lt;p&gt;Video super-resolution (VSR), with the aim to restore a high-resolution video
from its corresponding low-resolution version, is a spatial-temporal sequence
prediction problem. Recently, Transformer has been gaining popularity due to
its parallel computing ability for sequence-to-sequence modeling. Thus, it
seems to be straightforward to apply the vision Transformer to solve VSR.
However, the typical block design of Transformer with a fully connected
self-attention layer and a token-wise feed-forward layer does not fit well for
VSR due to the following two reasons. First, the fully connected self-attention
layer neglects to exploit the data locality because this layer relies on linear
layers to compute attention maps. Second, the token-wise feed-forward layer
lacks the feature alignment which is important for VSR since this layer
independently processes each of the input token embeddings without any
interaction among them. In this paper, we make the first attempt to adapt
Transformer for VSR. Specifically, to tackle the first issue, we present a
spatial-temporal convolutional self-attention layer with a theoretical
understanding to exploit the locality information. For the second issue, we
design a bidirectional optical flow-based feed-forward layer to discover the
correlations across different video frames and also align features. Extensive
experiments on several benchmark datasets demonstrate the effectiveness of our
proposed method. The code will be available at
https://github.com/caojiezhang/VSR-Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiezhang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.15430">
<title>The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.15430</link>
<description rdf:parseType="Literal">&lt;p&gt;In spite of the dominant performances of deep neural networks, recent works
have shown that they are poorly calibrated, resulting in over-confident
predictions. Miscalibration can be exacerbated by overfitting due to the
minimization of the cross-entropy during training, as it promotes the predicted
softmax probabilities to match the one-hot label assignments. This yields a
pre-softmax activation of the correct class that is significantly larger than
the remaining activations. Recent evidence from the literature suggests that
loss functions that embed implicit or explicit maximization of the entropy of
predictions yield state-of-the-art calibration performances. We provide a
unifying constrained-optimization perspective of current state-of-the-art
calibration losses. Specifically, these losses could be viewed as
approximations of a linear penalty (or a Lagrangian) imposing equality
constraints on logit distances. This points to an important limitation of such
underlying equality constraints, whose ensuing gradients constantly push
towards a non-informative solution, which might prevent from reaching the best
compromise between the discriminative performance and calibration of the model
during gradient-based optimization. Following our observations, we propose a
simple and flexible generalization based on inequality constraints, which
imposes a controllable margin on logit distances. Comprehensive experiments on
a variety of image classification, semantic segmentation and NLP benchmarks
demonstrate that our method sets novel state-of-the-art results on these tasks
in terms of network calibration, without affecting the discriminative
performance. The code is available at https://github.com/by-liu/MbLS .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1&quot;&gt;Adrian Galdran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.06809">
<title>Persistent Animal Identification Leveraging Non-Visual Markers. (arXiv:2112.06809v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.06809</link>
<description rdf:parseType="Literal">&lt;p&gt;Our objective is to locate and provide a unique identifier for each mouse in
a cluttered home-cage environment through time, as a precursor to automated
behaviour recognition for biological research. This is a very challenging
problem due to (i) the lack of distinguishing visual features for each mouse,
and (ii) the close confines of the scene with constant occlusion, making
standard visual tracking approaches unusable. However, a coarse estimate of
each mouse&apos;s location is available from a unique RFID implant, so there is the
potential to optimally combine information from (weak) tracking with coarse
information on identity. To achieve our objective, we make the following key
contributions: (a) the formulation of the object identification problem as an
assignment problem (solved using Integer Linear Programming), and (b) a novel
probabilistic model of the affinity between tracklets and RFID data. The latter
is a crucial part of the model, as it provides a principled probabilistic
treatment of object detections given coarse localisation. Our approach achieves
77% accuracy on this animal identification problem, and is able to reject
spurious detections when the animals are hidden.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_M/0/1/0/all/0/1&quot;&gt;Michael P. J. Camilleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1&quot;&gt;Rasneer S. Bains&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher K. I. Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05121">
<title>STEdge: Self-training Edge Detection with Multi-layer Teaching and Regularization. (arXiv:2201.05121v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05121</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based edge detection has hereunto been strongly supervised with
pixel-wise annotations which are tedious to obtain manually. We study the
problem of self-training edge detection, leveraging the untapped wealth of
large-scale unlabeled image datasets. We design a self-supervised framework
with multi-layer regularization and self-teaching. In particular, we impose a
consistency regularization which enforces the outputs from each of the multiple
layers to be consistent for the input image and its perturbed counterpart. We
adopt L0-smoothing as the &apos;perturbation&apos; to encourage edge prediction lying on
salient boundaries following the cluster assumption in self-supervised
learning. Meanwhile, the network is trained with multi-layer supervision by
pseudo labels which are initialized with Canny edges and then iteratively
refined by the network as the training proceeds. The regularization and
self-teaching together attain a good balance of precision and recall, leading
to a significant performance boost over supervised methods, with lightweight
refinement on the target dataset. Furthermore, our method demonstrates strong
cross-dataset generality. For example, it attains 4.8% improvement for ODS and
5.8% for OIS when tested on the unseen BIPED dataset, compared to the
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunfan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Renjiao Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhiping Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12926">
<title>Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12926</link>
<description rdf:parseType="Literal">&lt;p&gt;In tasks like semantic parsing, instruction following, and question
answering, standard deep networks fail to generalize compositionally from small
datasets. Many existing approaches overcome this limitation with model
architectures that enforce a compositional process of sentence interpretation.
In this paper, we present a domain-general and model-agnostic formulation of
compositionality as a constraint on symmetries of data distributions rather
than models. Informally, we prove that whenever a task can be solved by a
compositional model, there is a corresponding data augmentation scheme -- a
procedure for transforming examples into other well formed examples -- that
imparts compositional inductive bias on any model trained to solve the same
task. We describe a procedure called LEXSYM that discovers these
transformations automatically, then applies them to training data for ordinary
neural sequence models. Unlike existing compositional data augmentation
procedures, LEXSYM can be deployed agnostically across text, structured data,
and even images. It matches or surpasses state-of-the-art, task-specific models
on COGS semantic parsing, SCAN and ALCHEMY instruction following, and
CLEVR-COGENT visual question answering datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1&quot;&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.14092">
<title>Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.14092</link>
<description rdf:parseType="Literal">&lt;p&gt;The physical and textural attributes of objects have been widely studied for
recognition, detection and segmentation tasks in computer vision.~A number of
datasets, such as large scale ImageNet, have been proposed for feature learning
using data hungry deep neural networks and for hand-crafted feature extraction.
To intelligently interact with objects, robots and intelligent machines need
the ability to infer beyond the traditional physical/textural attributes, and
understand/learn visual cues, called visual affordances, for affordance
recognition, detection and segmentation. To date there is no publicly available
large dataset for visual affordance understanding and learning. In this paper,
we introduce a large scale multi-view RGBD visual affordance learning dataset,
a benchmark of 47210 RGBD images from 37 object categories, annotated with 15
visual affordance categories. To the best of our knowledge, this is the first
ever and the largest multi-view RGBD visual affordance learning dataset. We
benchmark the proposed dataset for affordance segmentation and recognition
tasks using popular Vision Transformer and Convolutional Neural Networks.
Several state-of-the-art deep learning networks are evaluated each for
affordance recognition and segmentation tasks. Our experimental results
showcase the challenging nature of the dataset and present definite prospects
for new and robust affordance learning algorithms. The dataset is publicly
available at https://sites.google.com/view/afaqshah/dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_Z/0/1/0/all/0/1&quot;&gt;Zeyad Khalifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Syed Afaq Ali Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.12900">
<title>Cross-Camera Trajectories Help Person Retrieval in a Camera Network. (arXiv:2204.12900v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.12900</link>
<description rdf:parseType="Literal">&lt;p&gt;We are concerned with retrieving a query person from multiple videos captured
by a non-overlapping camera network. Existing methods often rely on purely
visual matching or consider temporal constraints but ignore the spatial
information of the camera network. To address this issue, we propose a
pedestrian retrieval framework based on cross-camera trajectory generation,
which integrates both temporal and spatial information. To obtain pedestrian
trajectories, we propose a novel cross-camera spatio-temporal model that
integrates pedestrians&apos; walking habits and the path layout between cameras to
form a joint probability distribution. Such a spatio-temporal model among a
camera network can be specified using sparsely sampled pedestrian data. Based
on the spatio-temporal model, cross-camera trajectories can be extracted by the
conditional random field model and further optimized by restricted non-negative
matrix factorization. Finally, a trajectory re-ranking technique is proposed to
improve the pedestrian retrieval results. To verify the effectiveness of our
method, we construct the first cross-camera pedestrian trajectory dataset, the
Person Trajectory Dataset, in real surveillance scenarios. Extensive
experiments verify the effectiveness and robustness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohua Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1&quot;&gt;Jianhuang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11018">
<title>A Comprehensive Handwritten Paragraph Text Recognition System: LexiconNet. (arXiv:2205.11018v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11018</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we have presented an efficient procedure using two
state-of-the-art approaches from the literature of handwritten text recognition
as Vertical Attention Network and Word Beam Search. The attention module is
responsible for internal line segmentation that consequently processes a page
in a line-by-line manner. At the decoding step, we have added a connectionist
temporal classification-based word beam search decoder as a post-processing
step. In this study, an end-to-end paragraph recognition system is presented
with a lexicon decoder as a post-processing step. Our procedure reports
state-of-the-art results on standard datasets. The reported character error
rate is 3.24% on the IAM dataset with 27.19% improvement, 1.13% on RIMES with
40.83% improvement and 2.43% on the READ-16 dataset with 32.31% improvement
from existing literature and the word error rate is 8.29% on IAM dataset with
43.02% improvement, 2.94% on RIMES dataset with 56.25% improvement and 7.35% on
READ-2016 dataset with 47.27% improvement from the existing results. The
character error rate and word error rate reported in this work surpass the
results reported in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumari_L/0/1/0/all/0/1&quot;&gt;Lalita Kumari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sukhdeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathore_V/0/1/0/all/0/1&quot;&gt;Vaibhav Varish Singh Rathore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anuj Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14330">
<title>Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14330</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seung-Hwan Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1&quot;&gt;Szymon Rusinkiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1&quot;&gt;Felix Heide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.03341">
<title>Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.03341</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have pushed the state-of-the-art for visual
perception tasks. The self-attention mechanism underpinning the strength of
ViTs has a quadratic complexity in both computation and memory usage. This
motivates the development of approximating the self-attention at linear
complexity. However, an in-depth analysis in this work reveals that existing
methods are either theoretically flawed or empirically ineffective for visual
recognition. We identify that their limitations are rooted in the inheritance
of softmax-based self-attention during approximations, that is, normalizing the
scaled dot-product between token feature vectors using the softmax function. As
preserving the softmax operation challenges any subsequent linearization
efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are
proposed. Specifically, a Gaussian kernel function is adopted to replace the
dot-product similarity, enabling a full self-attention matrix to be
approximated under low-rank matrix decomposition. For computational robustness,
we estimate the Moore-Penrose inverse using an iterative Newton-Raphson method
in the forward process only, while calculating its theoretical gradients only
once in the backward process. To further expand applicability (e.g., dense
prediction tasks), an efficient symmetric normalization technique is
introduced. Extensive experiments on ImageNet, COCO, and ADE20K show that our
SOFT significantly improves the computational efficiency of existing ViT
variants. With linear complexity, much longer token sequences are permitted by
SOFT, resulting in superior trade-off between accuracy and complexity. Code and
models are available at https://github.com/fudan-zvg/SOFT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiachen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06079">
<title>Teachers in concordance for pseudo-labeling of 3D sequential data. (arXiv:2207.06079v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06079</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic pseudo-labeling is a powerful tool to tap into large amounts of
sequential unlabeled data. It is specially appealing in safety-critical
applications of autonomous driving, where performance requirements are extreme,
datasets are large, and manual labeling is very challenging. We propose to
leverage sequences of point clouds to boost the pseudolabeling technique in a
teacher-student setup via training multiple teachers, each with access to
different temporal information. This set of teachers, dubbed Concordance,
provides higher quality pseudo-labels for student training than standard
methods. The output of multiple teachers is combined via a novel pseudo label
confidence-guided criterion. Our experimental evaluation focuses on the 3D
point cloud domain and urban driving scenarios. We show the performance of our
method applied to 3D semantic segmentation and 3D object detection on three
benchmark datasets. Our approach, which uses only 20% manual labels,
outperforms some fully supervised methods. A notable performance boost is
achieved for classes rarely appearing in training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebrehiwot_A/0/1/0/all/0/1&quot;&gt;Awet Haileslassie Gebrehiwot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vacek_P/0/1/0/all/0/1&quot;&gt;Patrik Vacek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hurych_D/0/1/0/all/0/1&quot;&gt;David Hurych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_K/0/1/0/all/0/1&quot;&gt;Karel Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svoboda_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Svoboda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06825">
<title>Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions. (arXiv:2207.06825v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06825</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the scarcity of dense pixel-level semantic annotations for images
recorded in adverse visual conditions, there has been a keen interest in
unsupervised domain adaptation (UDA) for the semantic segmentation of such
images. UDA adapts models trained on normal conditions to the target
adverse-condition domains. Meanwhile, multiple datasets with driving scenes
provide corresponding images of the same scenes across multiple conditions,
which can serve as a form of weak supervision for domain adaptation. We propose
Refign, a generic extension to self-training-based UDA methods which leverages
these cross-domain correspondences. Refign consists of two steps: (1) aligning
the normal-condition image to the corresponding adverse-condition image using
an uncertainty-aware dense matching network, and (2) refining the adverse
prediction with the normal prediction using an adaptive label correction
mechanism. We design custom modules to streamline both steps and set the new
state of the art for domain-adaptive semantic segmentation on several
adverse-condition benchmarks, including ACDC and Dark Zurich. The approach
introduces no extra training parameters, minimal computational overhead --
during training only -- and can be used as a drop-in extension to improve any
given self-training-based UDA method. Code is available at
https://github.com/brdav/refign.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1&quot;&gt;David Bruggemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1&quot;&gt;Christos Sakaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1&quot;&gt;Prune Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.07268">
<title>Lightweight Vision Transformer with Cross Feature Attention. (arXiv:2207.07268v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.07268</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision transformers (ViTs) have achieved great performance
in visual recognition tasks. Convolutional neural networks (CNNs) exploit
spatial inductive bias to learn visual representations, but these networks are
spatially local. ViTs can learn global representations with their
self-attention mechanism, but they are usually heavy-weight and unsuitable for
mobile devices. In this paper, we propose cross feature attention (XFA) to
bring down computation cost for transformers, and combine efficient mobile CNNs
to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can
serve as a general-purpose backbone to learn both global and local
representation. Experimental results show that XFormer outperforms numerous CNN
and ViT-based models across different tasks and datasets. On ImageNet1K
dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,
which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT
(ViT-based) for similar number of parameters. Our model also performs well when
transferring to object detection and semantic segmentation tasks. On MS COCO
dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -&amp;gt; 33.2 AP) in YOLOv3
framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with
only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,
surpassing state-of-the-art lightweight segmentation networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Youpeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huadong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yingying Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+A_Y/0/1/0/all/0/1&quot;&gt;Yong A&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.02894">
<title>Redesigning Multi-Scale Neural Network for Crowd Counting. (arXiv:2208.02894v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.02894</link>
<description rdf:parseType="Literal">&lt;p&gt;Perspective distortions and crowd variations make crowd counting a
challenging task in computer vision. To tackle it, many previous works have
used multi-scale architecture in deep neural networks (DNNs). Multi-scale
branches can be either directly merged (e.g. by concatenation) or merged
through the guidance of proxies (e.g. attentions) in the DNNs. Despite their
prevalence, these combination methods are not sophisticated enough to deal with
the per-pixel performance discrepancy over multi-scale density maps. In this
work, we redesign the multi-scale neural network by introducing a hierarchical
mixture of density experts, which hierarchically merges multi-scale density
maps for crowd counting. Within the hierarchical structure, an expert
competition and collaboration scheme is presented to encourage contributions
from all scales; pixel-wise soft gating nets are introduced to provide
pixel-wise soft weights for scale combinations in different hierarchies. The
network is optimized using both the crowd density map and the local counting
map, where the latter is obtained by local integration on the former.
Optimizing both can be problematic because of their potential conflicts. We
introduce a new relative local counting loss based on relative count
differences among hard-predicted local regions in an image, which proves to be
complementary to the conventional absolute error loss on the density map.
Experiments show that our method achieves the state-of-the-art performance on
five public datasets, i.e. ShanghaiTech, UCF_CC_50, JHU-CROWD++, NWPU-Crowd and
Trancos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1&quot;&gt;Stefanos Zafeiriou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12776">
<title>SFusion: Self-attention based N-to-One Multimodal Fusion Block. (arXiv:2208.12776v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12776</link>
<description rdf:parseType="Literal">&lt;p&gt;People perceive the world with different senses, such as sight, hearing,
smell, and touch. Processing and fusing information from multiple modalities
enables Artificial Intelligence to understand the world around us more easily.
However, when there are missing modalities, the number of available modalities
is different in diverse situations, which leads to an N-to-One fusion problem.
To solve this problem, we propose a self-attention based fusion block called
SFusion. Different from preset formulations or convolution based methods, the
proposed block automatically learns to fuse available modalities without
synthesizing or zero-padding missing ones. Specifically, the feature
representations extracted from upstream processing model are projected as
tokens and fed into self-attention module to generate latent multimodal
correlations. Then, a modal attention mechanism is introduced to build a shared
representation, which can be applied by the downstream decision model. The
proposed SFusion can be easily integrated into existing multimodal analysis
networks. In this work, we apply SFusion to different backbone networks for
human activity recognition and brain tumor segmentation tasks. Extensive
experimental results show that the SFusion block achieves better performance
than the competing fusion strategies. Our code is available at
https://github.com/scut-cszcl/SFusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zecheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jia Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianlong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.13648">
<title>Dynamic Data-Free Knowledge Distillation by Easy-to-Hard Learning Strategy. (arXiv:2208.13648v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.13648</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-free knowledge distillation (DFKD) is a widely-used strategy for
Knowledge Distillation (KD) whose training data is not available. It trains a
lightweight student model with the aid of a large pretrained teacher model
without any access to training data. However, existing DFKD methods suffer from
inadequate and unstable training process, as they do not adjust the generation
target dynamically based on the status of the student model during learning. To
address this limitation, we propose a novel DFKD method called CuDFKD. It
teaches students by a dynamic strategy that gradually generates easy-to-hard
pseudo samples, mirroring how humans learn. Besides, CuDFKD adapts the
generation target dynamically according to the status of student model.
Moreover, We provide a theoretical analysis of the majorization minimization
(MM) algorithm and explain the convergence of CuDFKD. To measure the robustness
and fidelity of DFKD methods, we propose two more metrics, and experiments
shows CuDFKD has comparable performance to state-of-the-art (SOTA) DFKD methods
on all datasets. Experiments also present that our CuDFKD has the fastest
convergence and best robustness over other SOTA DFKD methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingru Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haishuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06606">
<title>PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning. (arXiv:2209.06606v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06606</link>
<description rdf:parseType="Literal">&lt;p&gt;Plasticity and stability are needed in class-incremental learning in order to
learn from new data while preserving past knowledge. Due to catastrophic
forgetting, finding a compromise between these two properties is particularly
challenging when no memory buffer is available. Mainstream methods need to
store two deep models since they integrate new classes using fine-tuning with
knowledge distillation from the previous incremental state. We propose a method
which has similar number of parameters but distributes them differently in
order to find a better balance between plasticity and stability. Following an
approach already deployed by transfer-based incremental methods, we freeze the
feature extractor after the initial state. Classes in the oldest incremental
states are trained with this frozen extractor to ensure stability. Recent
classes are predicted using partially fine-tuned models in order to introduce
plasticity. Our proposed plasticity layer can be incorporated to any
transfer-based method designed for exemplar-free incremental learning, and we
apply it to two such methods. Evaluation is done with three large-scale
datasets. Results show that performance gains are obtained in all tested
configurations compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petit_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;goire Petit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1&quot;&gt;Adrian Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belouadah_E/0/1/0/all/0/1&quot;&gt;Eden Belouadah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1&quot;&gt;David Picard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delezoide_B/0/1/0/all/0/1&quot;&gt;Bertrand Delezoide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00708">
<title>EraseNet: A Recurrent Residual Network for Supervised Document Cleaning. (arXiv:2210.00708v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00708</link>
<description rdf:parseType="Literal">&lt;p&gt;Document denoising is considered one of the most challenging tasks in
computer vision. There exist millions of documents that are still to be
digitized, but problems like document degradation due to natural and man-made
factors make this task very difficult. This paper introduces a supervised
approach for cleaning dirty documents using a new fully convolutional
auto-encoder architecture. This paper focuses on restoring documents with
discrepancies like deformities caused due to aging of a document, creases left
on the pages that were xeroxed, random black patches, lightly visible text,
etc., and also improving the quality of the image for better optical character
recognition system (OCR) performance. Removing noise from scanned documents is
a very important step before the documents as this noise can severely affect
the performance of an OCR system. The experiments in this paper have shown
promising results as the model is able to learn a variety of ordinary as well
as unusual noises and rectify them efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinde_Y/0/1/0/all/0/1&quot;&gt;Yashowardhan Shinde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1&quot;&gt;Kishore Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuberkar_S/0/1/0/all/0/1&quot;&gt;Sachin Kuberkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01361">
<title>Uncertainty-Aware Lidar Place Recognition in Novel Environments. (arXiv:2210.01361v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01361</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art lidar place recognition models exhibit unreliable
performance when tested on environments different from their training dataset,
which limits their use in complex and evolving environments. To address this
issue, we investigate the task of uncertainty-aware lidar place recognition,
where each predicted place must have an associated uncertainty that can be used
to identify and reject incorrect predictions. We introduce a novel evaluation
protocol and present the first comprehensive benchmark for this task, testing
across five uncertainty estimation techniques and three large-scale datasets.
Our results show that an Ensembles approach is the highest performing
technique, consistently improving the performance of lidar place recognition
and uncertainty estimation in novel environments, though it incurs a
computational cost. Code is publicly available at
https://github.com/csiro-robotics/Uncertainty-LPR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mason_K/0/1/0/all/0/1&quot;&gt;Keita Mason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1&quot;&gt;Joshua Knights&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1&quot;&gt;Milad Ramezani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;Dimity Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09477">
<title>UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image. (arXiv:2210.09477v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09477</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-driven image generation methods have shown impressive results recently,
allowing casual users to generate high quality images by providing textual
descriptions. However, similar capabilities for editing existing images are
still out of reach. Text-driven image editing methods usually need edit masks,
struggle with edits that require significant visual changes and cannot easily
keep specific details of the edited portion. In this paper we make the
observation that image-generation models can be converted to image-editing
models simply by fine-tuning them on a single image. We also show that
initializing the stochastic sampler with a noised version of the base image
before the sampling and interpolating relevant details from the base image
after sampling further increase the quality of the edit operation. Combining
these observations, we propose UniTune, a novel image editing method. UniTune
gets as input an arbitrary image and a textual edit description, and carries
out the edit while maintaining high fidelity to the input image. UniTune does
not require additional inputs, like masks or sketches, and can perform multiple
edits on the same image without retraining. We test our method using the Imagen
model in a range of different use cases. We demonstrate that it is broadly
applicable and can perform a surprisingly wide range of expressive editing
operations, including those requiring significant visual changes that were
previously impossible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valevski_D/0/1/0/all/0/1&quot;&gt;Dani Valevski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalman_M/0/1/0/all/0/1&quot;&gt;Matan Kalman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molad_E/0/1/0/all/0/1&quot;&gt;Eyal Molad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segalis_E/0/1/0/all/0/1&quot;&gt;Eyal Segalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yossi Matias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leviathan_Y/0/1/0/all/0/1&quot;&gt;Yaniv Leviathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09643">
<title>Improving Adversarial Robustness by Contrastive Guided Diffusion Process. (arXiv:2210.09643v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09643</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data generation has become an emerging tool to help improve the
adversarial robustness in classification tasks since robust learning requires a
significantly larger amount of training samples compared with standard
classification tasks. Among various deep generative models, the diffusion model
has been shown to produce high-quality synthetic images and has achieved good
performance in improving the adversarial robustness. However, diffusion-type
methods are typically slow in data generation as compared with other generative
models. Although different acceleration techniques have been proposed recently,
it is also of great importance to study how to improve the sample efficiency of
generated data for the downstream task. In this paper, we first analyze the
optimality condition of synthetic distribution for achieving non-trivial robust
accuracy. We show that enhancing the distinguishability among the generated
data is critical for improving adversarial robustness. Thus, we propose the
Contrastive-Guided Diffusion Process (Contrastive-DP), which adopts the
contrastive loss to guide the diffusion model in data generation. We verify our
theoretical results using simulations and demonstrate the good performance of
Contrastive-DP on image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1&quot;&gt;Yidong Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Liyan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Guang Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10209">
<title>Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10209</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) methods focus on accumulating knowledge over time
while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)
proposed a CL method, SupSup, which uses a randomly initialized, fixed base
network (model) and finds a supermask for each new task that selectively keeps
or removes each weight to produce a subnetwork. They prevent forgetting as the
network weights are not being updated. Although there is no forgetting, the
performance of SupSup is sub-optimal because fixed weights restrict its
representational power. Furthermore, there is no accumulation or transfer of
knowledge inside the model when new tasks are learned. Hence, we propose
ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and
non-overlapping subnetwork weight training. This avoids conflicting updates to
the shared weights by subsequent tasks to improve performance while still
preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge
Transfer (KKT) module that utilizes previously acquired knowledge to learn new
tasks better and faster. We demonstrate that ExSSNeT outperforms strong
previous methods on both NLP and Vision domains while preventing forgetting.
Moreover, ExSSNeT is particularly advantageous for sparse masks that activate
2-10% of the model parameters, resulting in an average improvement of 8.3% over
SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code
is available at https://github.com/prateeky2806/exessnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Prateek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17106">
<title>Intelligent Painter: Picture Composition With Resampling Diffusion Model. (arXiv:2210.17106v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17106</link>
<description rdf:parseType="Literal">&lt;p&gt;Have you ever thought that you can be an intelligent painter? This means that
you can paint a picture with a few expected objects in mind, or with a
desirable scene. This is different from normal inpainting approaches for which
the location of specific objects cannot be determined. In this paper, we
present an intelligent painter that generate a person&apos;s imaginary scene in one
go, given explicit hints. We propose a resampling strategy for Denoising
Diffusion Probabilistic Model (DDPM) to intelligently compose unconditional
harmonized pictures according to the input subjects at specific locations. By
exploiting the diffusion property, we resample efficiently to produce realistic
pictures. Experimental results show that our resampling method favors the
semantic meaning of the generated output efficiently and generates less blurry
output. Quantitative analysis of image quality assessment shows that our method
produces higher perceptual quality images compared with the state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_W/0/1/0/all/0/1&quot;&gt;Wing-Fung Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siu_W/0/1/0/all/0/1&quot;&gt;Wan-Chi Siu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;H. Anthony Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06757">
<title>DriftRec: Adapting diffusion models to blind JPEG restoration. (arXiv:2211.06757v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06757</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we utilize the high-fidelity generation abilities of diffusion
models to solve blind JPEG restoration at high compression levels. We propose
an elegant modification of the forward stochastic differential equation of
diffusion models to adapt them to this restoration task and name our method
DriftRec. Comparing DriftRec against an $L_2$ regression baseline with the same
network architecture and two state-of-the-art techniques for JPEG restoration,
we show that our approach can escape the tendency of other methods to generate
blurry images, and recovers the distribution of clean images significantly more
faithfully. For this, only a dataset of clean/corrupted image pairs and no
knowledge about the corruption operation is required, enabling wider
applicability to other restoration tasks. In contrast to other conditional and
unconditional diffusion models, we utilize the idea that the distributions of
clean and corrupted images are much closer to each other than each is to the
usual Gaussian prior of the reverse process in diffusion models. Our approach
therefore requires only low levels of added noise, and needs comparatively few
sampling steps even without further optimizations. We show that DriftRec
naturally generalizes to realistic and difficult scenarios such as unaligned
double JPEG compression and blind restoration of JPEGs found online, without
having encountered such examples during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Welker_S/0/1/0/all/0/1&quot;&gt;Simon Welker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chapman_H/0/1/0/all/0/1&quot;&gt;Henry N. Chapman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1&quot;&gt;Timo Gerkmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12193">
<title>Anatomy-guided domain adaptation for 3D in-bed human pose estimation. (arXiv:2211.12193v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12193</link>
<description rdf:parseType="Literal">&lt;p&gt;3D human pose estimation is a key component of clinical monitoring systems.
The clinical applicability of deep pose estimation models, however, is limited
by their poor generalization under domain shifts along with their need for
sufficient labeled training data. As a remedy, we present a novel domain
adaptation method, adapting a model from a labeled source to a shifted
unlabeled target domain. Our method comprises two complementary adaptation
strategies based on prior knowledge about human anatomy. First, we guide the
learning process in the target domain by constraining predictions to the space
of anatomically plausible poses. To this end, we embed the prior knowledge into
an anatomical loss function that penalizes asymmetric limb lengths, implausible
bone lengths, and implausible joint angles. Second, we propose to filter pseudo
labels for self-training according to their anatomical plausibility and
incorporate the concept into the Mean Teacher paradigm. We unify both
strategies in a point cloud-based framework applicable to unsupervised and
source-free domain adaptation. Evaluation is performed for in-bed pose
estimation under two adaptation scenarios, using the public SLP dataset and a
newly created dataset. Our method consistently outperforms various
state-of-the-art domain adaptation methods, surpasses the baseline model by
31%/66%, and reduces the domain gap by 65%/82%. Source code is available at
https://github.com/multimodallearning/da-3dhpe-anatomy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigalke_A/0/1/0/all/0/1&quot;&gt;Alexander Bigalke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1&quot;&gt;Lasse Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diesel_J/0/1/0/all/0/1&quot;&gt;Jasper Diesel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennigs_C/0/1/0/all/0/1&quot;&gt;Carlotta Hennigs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostalski_P/0/1/0/all/0/1&quot;&gt;Philipp Rostalski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1&quot;&gt;Mattias P. Heinrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02090">
<title>Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling. (arXiv:2212.02090v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02090</link>
<description rdf:parseType="Literal">&lt;p&gt;To capture the relationship between samples and labels, conditional
generative models often inherit spurious correlations from the training
dataset. This can result in label-conditional distributions that are imbalanced
with respect to another latent attribute. To mitigate this issue, which we call
spurious causality of conditional generation, we propose a general two-step
strategy. (a) Fairness Intervention (FI): emphasize the minority samples that
are hard to generate due to the spurious correlation in the training dataset.
(b) Corrective Sampling (CS): explicitly filter the generated samples and
ensure that they follow the desired latent attribute distribution. We have
designed the fairness intervention to work for various degrees of supervision
on the spurious attribute, including unsupervised, weakly-supervised, and
semi-supervised scenarios. Our experimental results demonstrate that FICS can
effectively resolve spurious causality of conditional generation across various
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1&quot;&gt;Junhyun Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05136">
<title>CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection. (arXiv:2212.05136v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05136</link>
<description rdf:parseType="Literal">&lt;p&gt;Video anomaly detection (VAD) -- commonly formulated as a multiple-instance
learning problem in a weakly-supervised manner due to its labor-intensive
nature -- is a challenging problem in video surveillance where the frames of
anomaly need to be localized in an untrimmed video. In this paper, we first
propose to utilize the ViT-encoded visual features from CLIP, in contrast with
the conventional C3D or I3D features in the domain, to efficiently extract
discriminative representations in the novel technique. We then model temporal
dependencies and nominate the snippets of interest by leveraging our proposed
Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of
TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA
outperforms the existing state-of-the-art (SOTA) methods by a large margin on
three commonly-used benchmark datasets in the VAD problem (UCF-Crime,
ShanghaiTech Campus, and XD-Violence). Our source code is available at
https://github.com/joos2010kj/CLIP-TSA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1&quot;&gt;Hyekang Kevin Joo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1&quot;&gt;Khoa Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1&quot;&gt;Kashu Yamazaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08846">
<title>Painterly Image Harmonization in Dual Domains. (arXiv:2212.08846v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08846</link>
<description rdf:parseType="Literal">&lt;p&gt;Image harmonization aims to produce visually harmonious composite images by
adjusting the foreground appearance to be compatible with the background. When
the composite image has photographic foreground and painterly background, the
task is called painterly image harmonization. There are only few works on this
task, which are either time-consuming or weak in generating well-harmonized
results. In this work, we propose a novel painterly harmonization network
consisting of a dual-domain generator and a dual-domain discriminator, which
harmonizes the composite image in both spatial domain and frequency domain. The
dual-domain generator performs harmonization by using AdaIN modules in the
spatial domain and our proposed ResFFT modules in the frequency domain. The
dual-domain discriminator attempts to distinguish the inharmonious patches
based on the spatial feature and frequency feature of each patch, which can
enhance the ability of generator in an adversarial manner. Extensive
experiments on the benchmark dataset show the effectiveness of our method. Our
code and model are available at
https://github.com/bcmi/PHDNet-Painterly-Image-Harmonization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Junyan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Li Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09530">
<title>HARP: Personalized Hand Reconstruction from a Monocular RGB Video. (arXiv:2212.09530v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09530</link>
<description rdf:parseType="Literal">&lt;p&gt;We present HARP (HAnd Reconstruction and Personalization), a personalized
hand avatar creation approach that takes a short monocular RGB video of a human
hand as input and reconstructs a faithful hand avatar exhibiting a
high-fidelity appearance and geometry. In contrast to the major trend of neural
implicit representations, HARP models a hand with a mesh-based parametric hand
model, a vertex displacement map, a normal map, and an albedo without any
neural components. As validated by our experiments, the explicit nature of our
representation enables a truly scalable, robust, and efficient approach to hand
avatar creation. HARP is optimized via gradient descent from a short sequence
captured by a hand-held mobile phone and can be directly used in AR/VR
applications with real-time rendering capability. To enable this, we carefully
design and implement a shadow-aware differentiable rendering scheme that is
robust to high degree articulations and self-shadowing regularly present in
hand motion sequences, as well as challenging lighting conditions. It also
generalizes to unseen poses and novel viewpoints, producing photo-realistic
renderings of hand animations performing highly-articulated motions.
Furthermore, the learned HARP representation can be used for improving 3D hand
pose estimation quality in challenging viewpoints. The key advantages of HARP
are validated by the in-depth analyses on appearance reconstruction, novel-view
and novel pose synthesis, and 3D hand pose refinement. It is an AR/VR-ready
personalized hand representation that shows superior fidelity and scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karunratanakul_K/0/1/0/all/0/1&quot;&gt;Korrawe Karunratanakul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prokudin_S/0/1/0/all/0/1&quot;&gt;Sergey Prokudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10549">
<title>Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10549</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., &quot;mug in grass&quot;) with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from &apos;mug&apos; to &apos;grass&apos;
(capturing the semantic relation &apos;in&apos;) to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a &apos;change of basis&apos; provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Rohan Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11325">
<title>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation. (arXiv:2302.11325v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11325</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a deep learning framework for medical video segmentation.
Convolution neural network (CNN) and transformer-based methods have achieved
great milestones in medical image segmentation tasks due to their incredible
semantic feature encoding and global information comprehension abilities.
However, most existing approaches ignore a salient aspect of medical video data
- the temporal dimension. Our proposed framework explicitly extracts features
from neighbouring frames across the temporal dimension and incorporates them
with a temporal feature blender, which then tokenises the high-level
spatio-temporal feature to form a strong global feature encoded via a Swin
Transformer. The final segmentation results are produced via a UNet-like
encoder-decoder architecture. Our model outperforms other approaches by a
significant margin and improves the segmentation benchmarks on the VFSS2022
dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasets
tested. Our studies also show the efficacy of the temporal feature blending
scheme and cross-dataset transferability of learned capabilities. Code and
models are fully available at https://github.com/SimonZeng7108/Video-SwinUNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1&quot;&gt;Chengxi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smithard_D/0/1/0/all/0/1&quot;&gt;David Smithard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1&quot;&gt;Majid Mirmehdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambaruto_A/0/1/0/all/0/1&quot;&gt;Alberto M Gambaruto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1&quot;&gt;Tilo Burghardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00865">
<title>AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context Processing for Representation Learning of Giga-pixel Images. (arXiv:2303.00865v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00865</link>
<description rdf:parseType="Literal">&lt;p&gt;Processing giga-pixel whole slide histopathology images (WSI) is a
computationally expensive task. Multiple instance learning (MIL) has become the
conventional approach to process WSIs, in which these images are split into
smaller patches for further processing. However, MIL-based techniques ignore
explicit information about the individual cells within a patch. In this paper,
by defining the novel concept of shared-context processing, we designed a
multi-modal Graph Transformer (AMIGO) that uses the celluar graph within the
tissue to provide a single representation for a patient while taking advantage
of the hierarchical structure of the tissue, enabling a dynamic focus between
cell-level and tissue-level information. We benchmarked the performance of our
model against multiple state-of-the-art methods in survival prediction and
showed that ours can significantly outperform all of them including
hierarchical Vision Transformer (ViT). More importantly, we show that our model
is strongly robust to missing information to an extent that it can achieve the
same performance with as low as 20% of the data. Finally, in two different
cancer datasets, we demonstrated that our model was able to stratify the
patients into low-risk and high-risk groups while other state-of-the-art
methods failed to achieve this goal. We also publish a large dataset of
immunohistochemistry images (InUIT) containing 1,600 tissue microarray (TMA)
cores from 188 patients along with their survival information, making it one of
the largest publicly available datasets in this context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakhli_R/0/1/0/all/0/1&quot;&gt;Ramin Nakhli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Puria Azadi Moghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1&quot;&gt;Haoyang Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahani_H/0/1/0/all/0/1&quot;&gt;Hossein Farahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baras_A/0/1/0/all/0/1&quot;&gt;Alexander Baras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilks_B/0/1/0/all/0/1&quot;&gt;Blake Gilks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bashashati_A/0/1/0/all/0/1&quot;&gt;Ali Bashashati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08325">
<title>FairAdaBN: Mitigating unfairness with adaptive batch normalization and its application to dermatological disease classification. (arXiv:2303.08325v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08325</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is becoming increasingly ubiquitous in medical research and
applications while involving sensitive information and even critical diagnosis
decisions. Researchers observe a significant performance disparity among
subgroups with different demographic attributes, which is called model
unfairness, and put lots of effort into carefully designing elegant
architectures to address unfairness, which poses heavy training burden, brings
poor generalization, and reveals the trade-off between model performance and
fairness. To tackle these issues, we propose FairAdaBN by making batch
normalization adaptive to sensitive attribute. This simple but effective design
can be adopted to several classification backbones that are originally unaware
of fairness. Additionally, we derive a novel loss function that restrains
statistical parity between subgroups on mini-batches, encouraging the model to
converge with considerable fairness. In order to evaluate the trade-off between
model performance and fairness, we propose a new metric, named
Fairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness
improvement over accuracy drop. Experiments on two dermatological datasets show
that our proposed method outperforms other methods on fairness criteria and
FATE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zikang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Quan Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12776">
<title>Dense Distinct Query for End-to-End Object Detection. (arXiv:2303.12776v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12776</link>
<description rdf:parseType="Literal">&lt;p&gt;One-to-one label assignment in object detection has successfully obviated the
need for non-maximum suppression (NMS) as postprocessing and makes the pipeline
end-to-end. However, it triggers a new dilemma as the widely used sparse
queries cannot guarantee a high recall, while dense queries inevitably bring
more similar queries and encounter optimization difficulties. As both sparse
and dense queries are problematic, then what are the expected queries in
end-to-end object detection? This paper shows that the solution should be Dense
Distinct Queries (DDQ). Concretely, we first lay dense queries like traditional
detectors and then select distinct ones for one-to-one assignments. DDQ blends
the advantages of traditional and recent end-to-end detectors and significantly
improves the performance of various detectors including FCN, R-CNN, and DETRs.
Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12
epochs using a ResNet-50 backbone, outperforming all existing detectors in the
same setting. DDQ also shares the benefit of end-to-end detectors in crowded
scenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers
to consider the complementarity between traditional methods and end-to-end
detectors. The source code can be found at
\url{https://github.com/jshilong/DDQ}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shilong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1&quot;&gt;Chengqi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01992">
<title>Cross-modulated Few-shot Image Generation for Colorectal Tissue Classification. (arXiv:2304.01992v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01992</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a few-shot colorectal tissue image generation method
for addressing the scarcity of histopathological training data for rare cancer
tissues. Our few-shot generation method, named XM-GAN, takes one base and a
pair of reference tissue images as input and generates high-quality yet diverse
images. Within our XM-GAN, a novel controllable fusion block densely aggregates
local regions of reference images based on their similarity to those in the
base image, resulting in locally consistent features. To the best of our
knowledge, we are the first to investigate few-shot generation in colorectal
tissue images. We evaluate our few-shot colorectral tissue image generation by
performing extensive qualitative, quantitative and subject specialist
(pathologist) based evaluations. Specifically, in specialist-based evaluation,
pathologists could differentiate between our XM-GAN generated tissue images and
real images only 55% time. Moreover, we utilize these generated images as data
augmentation to address the few-shot tissue image classification task,
achieving a gain of 4.4% in terms of mean accuracy over the vanilla few-shot
classifier. Code: \url{https://github.com/VIROBO-15/XM-GAN}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Amandeep Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhunia_A/0/1/0/all/0/1&quot;&gt;Ankan kumar Bhunia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Narayan_S/0/1/0/all/0/1&quot;&gt;Sanath Narayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Laaksonen_J/0/1/0/all/0/1&quot;&gt;Jorma Laaksonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04106">
<title>MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation. (arXiv:2304.04106v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04106</link>
<description rdf:parseType="Literal">&lt;p&gt;Acquiring and annotating sufficient labeled data is crucial in developing
accurate and robust learning-based models, but obtaining such data can be
challenging in many medical image segmentation tasks. One promising solution is
to synthesize realistic data with ground-truth mask annotations. However, no
prior studies have explored generating complete 3D volumetric images with
masks. In this paper, we present MedGen3D, a deep generative framework that can
generate paired 3D medical images and masks. First, we represent the 3D medical
data as 2D sequences and propose the Multi-Condition Diffusion Probabilistic
Model (MC-DPM) to generate multi-label mask sequences adhering to anatomical
geometry. Then, we use an image sequence generator and semantic diffusion
refiner conditioned on the generated mask sequences to produce realistic 3D
medical images that align with the generated masks. Our proposed framework
guarantees accurate alignment between synthetic images and segmentation maps.
Experiments on 3D thoracic CT and brain MRI datasets show that our synthetic
data is both diverse and faithful to the original data, and demonstrate the
benefits for downstream segmentation tasks. We anticipate that MedGen3D&apos;s
ability to synthesize paired 3D medical images and masks will prove valuable in
training deep learning models for medical imaging tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yifeng Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khosravi_P/0/1/0/all/0/1&quot;&gt;Pooya Khosravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shanlin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiangyi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04963">
<title>PlantDet: A benchmark for Plant Detection in the Three-Rivers-Source Region. (arXiv:2304.04963v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04963</link>
<description rdf:parseType="Literal">&lt;p&gt;The Three-River-Source region is a highly significant natural reserve in
China that harbors a plethora of untamed botanical resources. To meet the
practical requirements of botanical research and intelligent plant management,
we construct a large-scale dataset for Plant detection in the
Three-River-Source region (PTRS). This dataset comprises 6965 high-resolution
images of 2160*3840 pixels, captured by diverse sensors and platforms, and
featuring objects of varying shapes and sizes. Subsequently, a team of
botanical image interpretation experts annotated these images with 21 commonly
occurring object categories. The fully annotated PTRS images contain 122, 300
instances of plant leaves, each labeled by a horizontal rectangle. The PTRS
presents us with challenges such as dense occlusion, varying leaf resolutions,
and high feature similarity among plants, prompting us to develop a novel
object detection network named PlantDet. This network employs a window-based
efficient self-attention module (ST block) to generate robust feature
representation at multiple scales, improving the detection efficiency for small
and densely-occluded objects. Our experimental results validate the efficacy of
our proposed plant detection benchmark, with a precision of 88.1%, a mean
average precision (mAP) of 77.6%, and a higher recall compared to the baseline.
Additionally, our method effectively overcomes the issue of missing small
objects. We intend to share our data and code with interested parties to
advance further research in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huanhuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu-an Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhaba_J/0/1/0/all/0/1&quot;&gt;Jiangcai Zhaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guomei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yongga_L/0/1/0/all/0/1&quot;&gt;Lamao Yongga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12235">
<title>Multi-cropping Contrastive Learning and Domain Consistency for Unsupervised Image-to-Image Translation. (arXiv:2304.12235v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12235</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, unsupervised image-to-image translation methods based on
contrastive learning have achieved state-of-the-art results in many tasks.
However, in the previous works, the negatives are sampled from the input image
itself, which inspires us to design a data augmentation method to improve the
quality of the selected negatives. Moreover, the previous methods only preserve
the content consistency via patch-wise contrastive learning in the embedding
space, which ignores the domain consistency between the generated images and
the real images of the target domain. In this paper, we propose a novel
unsupervised image-to-image translation framework based on multi-cropping
contrastive learning and domain consistency, called MCDUT. Specifically, we
obtain the multi-cropping views via the center-cropping and the random-cropping
with the aim of further generating the high-quality negative examples. To
constrain the embeddings in the deep feature space, we formulate a new domain
consistency loss, which encourages the generated images to be close to the real
images in the embedding space of the same domain. Furthermore, we present a
dual coordinate attention network by embedding positional information into the
channel, which called DCA. We employ the DCA network in the design of
generator, which makes the generator capture the horizontal and vertical global
information of dependency. In many image-to-image translation tasks, our method
achieves state-of-the-art results, and the advantages of our method have been
proven through extensive comparison experiments and ablation research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wei-Ling Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Cheng-Wei Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13009">
<title>The Potential of Visual ChatGPT For Remote Sensing. (arXiv:2304.13009v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13009</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Natural Language Processing (NLP), particularly in
Large Language Models (LLMs), associated with deep learning-based computer
vision techniques, have shown substantial potential for automating a variety of
tasks. One notable model is Visual ChatGPT, which combines ChatGPT&apos;s LLM
capabilities with visual computation to enable effective image analysis. The
model&apos;s ability to process images based on textual inputs can revolutionize
diverse fields. However, its application in the remote sensing domain remains
unexplored. This is the first paper to examine the potential of Visual ChatGPT,
a cutting-edge LLM founded on the GPT architecture, to tackle the aspects of
image processing related to the remote sensing domain. Among its current
capabilities, Visual ChatGPT can generate textual descriptions of images,
perform canny edge and straight line detection, and conduct image segmentation.
These offer valuable insights into image content and facilitate the
interpretation and extraction of information. By exploring the applicability of
these techniques within publicly available datasets of satellite images, we
demonstrate the current model&apos;s limitations in dealing with remote sensing
images, highlighting its challenges and future prospects. Although still in
early development, we believe that the combination of LLMs and visual models
holds a significant potential to transform remote sensing image processing,
creating accessible and practical application opportunities in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osco_L/0/1/0/all/0/1&quot;&gt;Lucas Prado Osco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemos_E/0/1/0/all/0/1&quot;&gt;Eduardo Lopes de Lemos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncalves_W/0/1/0/all/0/1&quot;&gt;Wesley Nunes Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_A/0/1/0/all/0/1&quot;&gt;Ana Paula Marques Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Marcato Junior&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00417">
<title>Transformer-based Sequence Labeling for Audio Classification based on MFCCs. (arXiv:2305.00417v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00417</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio classification is vital in areas such as speech and music recognition.
Feature extraction from the audio signal, such as Mel-Spectrograms and MFCCs,
is a critical step in audio classification. These features are transformed into
spectrograms for classification. Researchers have explored various techniques,
including traditional machine and deep learning methods to classify
spectrograms, but these can be computationally expensive. To simplify this
process, a more straightforward approach inspired by sequence classification in
NLP can be used. This paper proposes a Transformer-encoder-based model for
audio classification using MFCCs. The model was benchmarked against the ESC-50,
Speech Commands v0.02 and UrbanSound8k datasets and has shown strong
performance, with the highest accuracy of 95.2% obtained upon training the
model on the UrbanSound8k dataset. The model consisted of a mere 127,544 total
parameters, making it light-weight yet highly efficient at the audio
classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonali_C/0/1/0/all/0/1&quot;&gt;C. S. Sonali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_C/0/1/0/all/0/1&quot;&gt;Chinmayi B S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_A/0/1/0/all/0/1&quot;&gt;Ahana Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00432">
<title>Synthetic Data-based Detection of Zebras in Drone Imagery. (arXiv:2305.00432v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00432</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, there is a wide availability of datasets that enable the training
of common object detectors or human detectors. These come in the form of
labelled real-world images and require either a significant amount of human
effort, with a high probability of errors such as missing labels, or very
constrained scenarios, e.g. VICON systems. On the other hand, uncommon
scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain
information, such as human shapes, are hardly available. To overcome this,
synthetic data generation with realistic rendering technologies has recently
gained traction and advanced research areas such as target tracking and human
pose estimation. However, subjects such as wild animals are still usually not
well represented in such datasets. In this work, we first show that a
pre-trained YOLO detector can not identify zebras in real images recorded from
aerial viewpoints. To solve this, we present an approach for training an animal
detector using only synthetic data. We start by generating a novel synthetic
zebra dataset using GRADE, a state-of-the-art framework for data generation.
The dataset includes RGB, depth, skeletal joint locations, pose, shape and
instance segmentations for each subject. We use this to train a YOLO detector
from scratch. Through extensive evaluations of our model with real-world data
from i) limited datasets available on the internet and ii) a new one collected
and manually labelled by us, we show that we can detect zebras by using only
synthetic data during training. The code, results, trained models, and both the
generated and training data are provided as open-source at
https://eliabntt.github.io/grade-rr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonetto_E/0/1/0/all/0/1&quot;&gt;Elia Bonetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1&quot;&gt;Aamir Ahmad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01361">
<title>Boosting Adversarial Transferability via Fusing Logits of Top-1 Decomposed Feature. (arXiv:2305.01361v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01361</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has shown that Deep Neural Networks (DNNs) are highly
vulnerable to adversarial samples, which are highly transferable and can be
used to attack other unknown black-box models. To improve the transferability
of adversarial samples, several feature-based adversarial attack methods have
been proposed to disrupt neuron activation in the middle layers. However,
current state-of-the-art feature-based attack methods typically require
additional computation costs for estimating the importance of neurons. To
address this challenge, we propose a Singular Value Decomposition (SVD)-based
feature-level attack method. Our approach is inspired by the discovery that
eigenvectors associated with the larger singular values decomposed from the
middle layer features exhibit superior generalization and attention properties.
Specifically, we conduct the attack by retaining the decomposed Top-1 singular
value-associated feature for computing the output logits, which are then
combined with the original logits to optimize adversarial examples. Our
extensive experimental results verify the effectiveness of our proposed method,
which can be easily integrated into various baselines to significantly enhance
the transferability of adversarial samples for disturbing normally trained CNNs
and advanced defense strategies. The source code of this study is available at
https://github.com/WJJLL/SVD-SSA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1&quot;&gt;Juanjuan Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhiming Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dazhen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shaozi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhun Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07611">
<title>Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07611</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal sentiment analysis has become an important research area in the
field of artificial intelligence. With the latest advances in deep learning,
this technology has reached new heights. It has great potential for both
application and research, making it a popular research topic. This review
provides an overview of the definition, background, and development of
multimodal sentiment analysis. It also covers recent datasets and advanced
models, emphasizing the challenges and future prospects of this technology.
Finally, it looks ahead to future research directions. It should be noted that
this review provides constructive suggestions for promising research directions
and building better performing multimodal sentiment analysis models, which can
help researchers in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1&quot;&gt;Songning Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xifeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoxuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhaoxia Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11019">
<title>Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11019</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of Audio-Visual Segmentation (AVS) is to localise the sounding
objects within visual scenes by accurately predicting pixel-wise segmentation
masks. To tackle the task, it involves a comprehensive consideration of both
the data and model aspects. In this paper, first, we initiate a novel pipeline
for generating artificial data for the AVS task without human annotating. We
leverage existing image segmentation and audio datasets to match the image-mask
pairs with its corresponding audio samples with the linkage of category labels,
that allows us to effortlessly compose (image, audio, mask) triplets for
training AVS models. The pipeline is annotation-free and scalable to cover a
large number of categories. Additionally, we introduce a lightweight approach
SAMA-AVS to adapt the pre-trained segment anything model~(SAM) to the AVS task.
By introducing only a small number of trainable parameters with adapters, the
proposed model can effectively achieve adequate audio-visual fusion and
interaction in the encoding stage with vast majority of parameters fixed. We
conduct extensive experiments, and the results show our proposed model
remarkably surpasses other competing methods. Moreover, by using the proposed
model pretrained with our synthetic data, the performance on real AVSBench data
is further improved, achieving 83.17 mIoU on S4 subset and 66.95 mIoU on MS3
set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinxiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Chen Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chaofan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16656">
<title>Clustering Method for Time-Series Images Using Quantum-Inspired Computing Technology. (arXiv:2305.16656v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16656</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-series clustering serves as a powerful data mining technique for
time-series data in the absence of prior knowledge about clusters. A large
amount of time-series data with large size has been acquired and used in
various research fields. Hence, clustering method with low computational cost
is required. Given that a quantum-inspired computing technology, such as a
simulated annealing machine, surpasses conventional computers in terms of fast
and accurately solving combinatorial optimization problems, it holds promise
for accomplishing clustering tasks that are challenging to achieve using
existing methods. This study proposes a novel time-series clustering method
that leverages an annealing machine. The proposed method facilitates an even
classification of time-series data into clusters close to each other while
maintaining robustness against outliers. Moreover, its applicability extends to
time-series images. We compared the proposed method with a standard existing
method for clustering an online distributed dataset. In the existing method,
the distances between each data are calculated based on the Euclidean distance
metric, and the clustering is performed using the k-means++ method. We found
that both methods yielded comparable results. Furthermore, the proposed method
was applied to a flow measurement image dataset containing noticeable noise
with a signal-to-noise ratio of approximately 1. Despite a small signal
variation of approximately 2%, the proposed method effectively classified the
data without any overlap among the clusters. In contrast, the clustering
results by the standard existing method and the conditional image sampling
(CIS) method, a specialized technique for flow measurement data, displayed
overlapping clusters. Consequently, the proposed method provides better results
than the other two methods, demonstrating its potential as a superior
clustering method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Inoue_T/0/1/0/all/0/1&quot;&gt;Tomoki Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kubota_K/0/1/0/all/0/1&quot;&gt;Koyo Kubota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ikami_T/0/1/0/all/0/1&quot;&gt;Tsubasa Ikami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egami_Y/0/1/0/all/0/1&quot;&gt;Yasuhiro Egami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nagai_H/0/1/0/all/0/1&quot;&gt;Hiroki Nagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kashikawa_T/0/1/0/all/0/1&quot;&gt;Takahiro Kashikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kimura_K/0/1/0/all/0/1&quot;&gt;Koichi Kimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Matsuda_Y/0/1/0/all/0/1&quot;&gt;Yu Matsuda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17376">
<title>DePF: A Novel Fusion Approach based on Decomposition Pooling for Infrared and Visible Images. (arXiv:2305.17376v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17376</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared and visible image fusion aims to generate synthetic images
simultaneously containing salient features and rich texture details, which can
be used to boost downstream tasks. However, existing fusion methods are
suffering from the issues of texture loss and edge information deficiency,
which result in suboptimal fusion results. Meanwhile, the straight-forward
up-sampling operator can not well preserve the source information from
multi-scale features. To address these issues, a novel fusion network based on
the decomposition pooling (de-pooling) manner is proposed, termed as DePF.
Specifically, a de-pooling based encoder is designed to extract multi-scale
image and detail features of source images at the same time. In addition, the
spatial attention model is used to aggregate these salient features. After
that, the fused features will be reconstructed by the decoder, in which the
up-sampling operator is replaced by the de-pooling reversed operation.
Different from the common max-pooling technique, image features after the
de-pooling layer can retain abundant details information, which is benefit to
the fusion process. In this case, rich texture information and multi-scale
information are maintained during the reconstruction phase. The experimental
results demonstrate that the proposed method exhibits superior fusion
performance over the state-of-the-arts on multiple image fusion benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yongbiao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chunyang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoning Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.20030">
<title>Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust. (arXiv:2305.20030v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.20030</link>
<description rdf:parseType="Literal">&lt;p&gt;Watermarking the outputs of generative models is a crucial technique for
tracing copyright and preventing potential harm from AI-generated content. In
this paper, we introduce a novel technique called Tree-Ring Watermarking that
robustly fingerprints diffusion model outputs. Unlike existing methods that
perform post-hoc modifications to images after sampling, Tree-Ring Watermarking
subtly influences the entire sampling process, resulting in a model fingerprint
that is invisible to humans. The watermark embeds a pattern into the initial
noise vector used for sampling. These patterns are structured in Fourier space
so that they are invariant to convolutions, crops, dilations, flips, and
rotations. After image generation, the watermark signal is detected by
inverting the diffusion process to retrieve the noise vector, which is then
checked for the embedded signal. We demonstrate that this technique can be
easily applied to arbitrary diffusion models, including text-conditioned Stable
Diffusion, as a plug-in with negligible loss in FID. Our watermark is
semantically hidden in the image space and is far more robust than watermarking
alternatives that are currently deployed. Code is available at
https://github.com/YuxinWenRick/tree-ring-watermark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchenbauer_J/0/1/0/all/0/1&quot;&gt;John Kirchenbauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1&quot;&gt;Jonas Geiping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02176">
<title>TransRUPNet for Improved Out-of-Distribution Generalization in Polyp Segmentation. (arXiv:2306.02176v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02176</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) generalization is a critical challenge in deep
learning. It is specifically important when the test samples are drawn from a
different distribution than the training data. We develop a novel real-time
deep learning based architecture, TransRUPNet that is based on a Transformer
and residual upsampling network for colorectal polyp segmentation to improve
OOD generalization. The proposed architecture, TransRUPNet, is an
encoder-decoder network that consists of three encoder blocks, three decoder
blocks, and some additional upsampling blocks at the end of the network. With
the image size of $256\times256$, the proposed method achieves an excellent
real-time operation speed of \textbf{47.07} frames per second with an average
mean dice coefficient score of 0.7786 and mean Intersection over Union of
0.7210 on the out-of-distribution polyp datasets. The results on the publicly
available PolypGen dataset (OOD dataset in our case) suggest that TransRUPNet
can give real-time feedback while retaining high accuracy for in-distribution
dataset. Furthermore, we demonstrate the generalizability of the proposed
method by showing that it significantly improves performance on OOD datasets
compared to the existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1&quot;&gt;Nikhil Kumar Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Debayan Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04719">
<title>Don&apos;t trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04719</link>
<description rdf:parseType="Literal">&lt;p&gt;How do neural networks extract patterns from pixels? Feature visualizations
attempt to answer this important question by visualizing highly activating
patterns through optimization. Today, visualization methods form the foundation
of our knowledge about the internal workings of neural networks, as a type of
mechanistic interpretability. Here we ask: How reliable are feature
visualizations? We start our investigation by developing network circuits that
trick feature visualizations into showing arbitrary patterns that are
completely disconnected from normal network behavior on natural input. We then
provide evidence for a similar phenomenon occurring in standard, unmanipulated
networks: feature visualizations are processed very differently from standard
input, casting doubt on their ability to &quot;explain&quot; how neural networks process
natural images. We underpin this empirical finding by theory proving that the
set of functions that can be reliably understood by feature visualization is
extremely small and does not include general black-box neural networks.
Therefore, a promising way forward could be the development of networks that
enforce certain structures in order to ensure more reliable feature
visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06722">
<title>$E(2)$-Equivariant Vision Transformer. (arXiv:2306.06722v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06722</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformer (ViT) has achieved remarkable performance in computer
vision. However, positional encoding in ViT makes it substantially difficult to
learn the intrinsic equivariance in data. Initial attempts have been made on
designing equivariant ViT but are proved defective in some cases in this paper.
To address this issue, we design a Group Equivariant Vision Transformer
(GE-ViT) via a novel, effective positional encoding operator. We prove that
GE-ViT meets all the theoretical requirements of an equivariant neural network.
Comprehensive experiments are conducted on standard benchmark datasets,
demonstrating that GE-ViT significantly outperforms non-equivariant
self-attention networks. The code is available at
https://github.com/ZJUCDSYangKaifan/GEVit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Fengxiang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07520">
<title>Retrieve Anyone: A General-purpose Person Re-identification Task with Instructions. (arXiv:2306.07520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07520</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence can retrieve any person according to both visual and
language descriptions. However, the current computer vision community studies
specific person re-identification (ReID) tasks in different scenarios
separately, which limits the applications in the real world. This paper strives
to resolve this problem by proposing a new instruct-ReID task that requires the
model to retrieve images according to the given image or language
instructions.Our instruct-ReID is a more general ReID setting, where existing
ReID tasks can be viewed as special cases by designing different instructions.
We propose a large-scale OmniReID benchmark and an adaptive triplet loss as a
baseline method to facilitate research in this new setting. Experimental
results show that the baseline model trained on our OmniReID benchmark can
improve +0.6%, +1.4%, 0.2% mAP on Market1501, CUHK03, MSMT17 for traditional
ReID, +0.8%, +2.0%, +13.4% mAP on PRCC, VC-Clothes, LTCC for clothes-changing
ReID, +11.7% mAP on COCAS+ real2 for clothestemplate based clothes-changing
ReID when using only RGB images, +25.4% mAP on COCAS+ real2 for our newly
defined language-instructed ReID. The dataset, model, and code will be
available at https://github.com/hwz-zju/Instruct-ReID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Weizhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yiheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qingsong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1&quot;&gt;Donglian Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11730">
<title>Segment Anything Model (SAM) for Radiation Oncology. (arXiv:2306.11730v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we evaluate the performance of the Segment Anything Model
(SAM) in clinical radiotherapy. Our results indicate that SAM&apos;s &apos;segment
anything&apos; mode can achieve clinically acceptable segmentation results in most
organs-at-risk (OARs) with Dice scores higher than 0.7. SAM&apos;s &apos;box prompt&apos; mode
further improves the Dice scores by 0.1 to 0.5. Considering the size of the
organ and the clarity of its boundary, SAM displays better performance for
large organs with clear boundaries but performs worse for smaller organs with
unclear boundaries. Given that SAM, a model pre-trained purely on natural
images, can handle the delineation of OARs from medical images with clinically
acceptable accuracy, these results highlight SAM&apos;s robust generalization
capabilities with consistent accuracy in automatic segmentation for
radiotherapy. In other words, SAM can achieve delineation of different OARs at
different sites using a generic automatic segmentation model. SAM&apos;s
generalization capabilities across different disease sites suggest that it is
technically feasible to develop a generic model for automatic segmentation in
radiotherapy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Holmes_J/0/1/0/all/0/1&quot;&gt;Jason Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hongying Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Haixing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dajiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12070">
<title>Task-Robust Pre-Training for Worst-Case Downstream Adaptation. (arXiv:2306.12070v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12070</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training has achieved remarkable success when transferred to downstream
tasks. In machine learning, we care about not only the good performance of a
model but also its behavior under reasonable shifts of condition. The same
philosophy holds when pre-training a foundation model. However, the foundation
model may not uniformly behave well for a series of related downstream tasks.
This happens, for example, when conducting mask recovery regression where the
recovery ability or the training instances diverge like pattern features are
extracted dominantly on pre-training, but semantic features are also required
on a downstream task. This paper considers pre-training a model that guarantees
a uniformly good performance over the downstream tasks. We call this goal as
$\textit{downstream-task robustness}$. Our method first separates the upstream
task into several representative ones and applies a simple minimax loss for
pre-training. We then design an efficient algorithm to solve the minimax loss
and prove its convergence in the convex setting. In the experiments, we show
both on large-scale natural language processing and computer vision datasets
our method increases the metrics on worse-case downstream tasks. Additionally,
some theoretical explanations for why our loss is beneficial are provided.
Specifically, we show fewer samples are inherently required for the most
challenging downstream task in some cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xingyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Cong Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouchen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13695">
<title>Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13695</link>
<description rdf:parseType="Literal">&lt;p&gt;Color Doppler echocardiography is a widely used non-invasive imaging modality
that provides real-time information about the intracardiac blood flow. In an
apical long-axis view of the left ventricle, color Doppler is subject to phase
wrapping, or aliasing, especially during cardiac filling and ejection. When
setting up quantitative methods based on color Doppler, it is necessary to
correct this wrapping artifact. We developed an unfolded primal-dual network to
unwrap (dealias) color Doppler echocardiographic images and compared its
effectiveness against two state-of-the-art segmentation approaches based on
nnU-Net and transformer models. We trained and evaluated the performance of
each method on an in-house dataset and found that the nnU-Net-based method
provided the best dealiased results, followed by the primal-dual approach and
the transformer-based technique. Noteworthy, the primal-dual network, which had
significantly fewer trainable parameters, performed competitively with respect
to the other two methods, demonstrating the high potential of deep unfolding
methods. Our results suggest that deep learning-based methods can effectively
remove aliasing artifacts in color Doppler echocardiographic images,
outperforming DeAN, a state-of-the-art semi-automatic technique. Overall, our
results show that deep learning-based methods have the potential to effectively
preprocess color Doppler images for downstream quantitative analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Hang Jung Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bernard_O/0/1/0/all/0/1&quot;&gt;Olivier Bernard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ducros_N/0/1/0/all/0/1&quot;&gt;Nicolas Ducros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_D/0/1/0/all/0/1&quot;&gt;Damien Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14749">
<title>A denoised Mean Teacher for domain adaptive point cloud registration. (arXiv:2306.14749v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14749</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud-based medical registration promises increased computational
efficiency, robustness to intensity shifts, and anonymity preservation but is
limited by the inefficacy of unsupervised learning with similarity metrics.
Supervised training on synthetic deformations is an alternative but, in turn,
suffers from the domain gap to the real domain. In this work, we aim to tackle
this gap through domain adaptation. Self-training with the Mean Teacher is an
established approach to this problem but is impaired by the inherent noise of
the pseudo labels from the teacher. As a remedy, we present a denoised
teacher-student paradigm for point cloud registration, comprising two
complementary denoising strategies. First, we propose to filter pseudo labels
based on the Chamfer distances of teacher and student registrations, thus
preventing detrimental supervision by the teacher. Second, we make the teacher
dynamically synthesize novel training pairs with noise-free labels by warping
its moving inputs with the predicted deformations. Evaluation is performed for
inhale-to-exhale registration of lung vessel trees on the public PVT dataset
under two domain shifts. Our method surpasses the baseline Mean Teacher by
13.5/62.8%, consistently outperforms diverse competitors, and sets a new
state-of-the-art accuracy (TRE=2.31mm). Code is available at
https://github.com/multimodallearning/denoised_mt_pcd_reg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigalke_A/0/1/0/all/0/1&quot;&gt;Alexander Bigalke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1&quot;&gt;Mattias P. Heinrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14937">
<title>Minimum Description Length Clustering to Measure Meaningful Image Complexity. (arXiv:2306.14937v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14937</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing image complexity metrics cannot distinguish meaningful content from
noise. This means that white noise images, which contain no meaningful
information, are judged as highly complex. We present a new image complexity
metric through hierarchical clustering of patches. We use the minimum
description length principle to determine the number of clusters and designate
certain points as outliers and, hence, correctly assign white noise a low
score. The presented method has similarities to theoretical ideas for measuring
meaningful complexity. We conduct experiments on seven different sets of
images, which show that our method assigns the most accurate scores to all
images considered. Additionally, comparing the different levels of the
hierarchy of clusters can reveal how complexity manifests at different scales,
from local detail to global structure. We then present ablation studies showing
the contribution of the components of our method, and that it continues to
assign reasonable scores when the inputs are modified in certain ways,
including the addition of Gaussian noise and the lowering of the resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahon_L/0/1/0/all/0/1&quot;&gt;Louis Mahon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1&quot;&gt;Thomas Lukasiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15416">
<title>Irregular Change Detection in Sparse Bi-Temporal Point Clouds using Learned Place Recognition Descriptors and Point-to-Voxel Comparison. (arXiv:2306.15416v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15416</link>
<description rdf:parseType="Literal">&lt;p&gt;Change detection and irregular object extraction in 3D point clouds is a
challenging task that is of high importance not only for autonomous navigation
but also for updating existing digital twin models of various industrial
environments. This article proposes an innovative approach for change detection
in 3D point clouds using deep learned place recognition descriptors and
irregular object extraction based on voxel-to-point comparison. The proposed
method first aligns the bi-temporal point clouds using a map-merging algorithm
in order to establish a common coordinate frame. Then, it utilizes deep
learning techniques to extract robust and discriminative features from the 3D
point cloud scans, which are used to detect changes between consecutive point
cloud frames and therefore find the changed areas. Finally, the altered areas
are sampled and compared between the two time instances to extract any
obstructions that caused the area to change. The proposed method was
successfully evaluated in real-world field experiments, where it was able to
detect different types of changes in 3D point clouds, such as object or
muck-pile addition and displacement, showcasing the effectiveness of the
approach. The results of this study demonstrate important implications for
various applications, including safety and security monitoring in construction
sites, mapping and exploration and suggests potential future research
directions in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stathoulopoulos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Stathoulopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koval_A/0/1/0/all/0/1&quot;&gt;Anton Koval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolakopoulos_G/0/1/0/all/0/1&quot;&gt;George Nikolakopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15767">
<title>Evidential Detection and Tracking Collaboration: New Problem, Benchmark and Algorithm for Robust Anti-UAV System. (arXiv:2306.15767v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15767</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Vehicles (UAVs) have been widely used in many areas,
including transportation, surveillance, and military. However, their potential
for safety and privacy violations is an increasing issue and highly limits
their broader applications, underscoring the critical importance of UAV
perception and defense (anti-UAV). Still, previous works have simplified such
an anti-UAV task as a tracking problem, where the prior information of UAVs is
always provided; such a scheme fails in real-world anti-UAV tasks (i.e. complex
scenes, indeterminate-appear and -reappear UAVs, and real-time UAV
surveillance). In this paper, we first formulate a new and practical anti-UAV
problem featuring the UAVs perception in complex scenes without prior UAVs
information. To benchmark such a challenging task, we propose the largest UAV
dataset dubbed AntiUAV600 and a new evaluation metric. The AntiUAV600 comprises
600 video sequences of challenging scenes with random, fast, and small-scale
UAVs, with over 723K thermal infrared frames densely annotated with bounding
boxes. Finally, we develop a novel anti-UAV approach via an evidential
collaboration of global UAVs detection and local UAVs tracking, which
effectively tackles the proposed problem and can serve as a strong baseline for
future research. Extensive experiments show our method outperforms SOTA
approaches and validate the ability of AntiUAV600 to enhance UAV perception
performance due to its large scale and complexity. Our dataset, pretrained
models, and source codes will be released publically.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xue-Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia-Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Gang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiao-Jun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16759">
<title>SaaFormer: Spectral-spatial Axial Aggregation Transformer for Hyperspectral Image Classification. (arXiv:2306.16759v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16759</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSI) captured from earth observing satellites and
aircraft is becoming increasingly important for applications in agriculture,
environmental monitoring, mining, etc. Due to the limited available
hyperspectral datasets, the pixel-wise random sampling is the most commonly
used training-test dataset partition approach, which has significant overlap
between samples in training and test datasets. Furthermore, our experimental
observations indicates that regions with larger overlap often exhibit higher
classification accuracy. Consequently, the pixel-wise random sampling approach
poses a risk of data leakage. Thus, we propose a block-wise sampling method to
minimize the potential for data leakage. Our experimental findings also confirm
the presence of data leakage in models such as 2DCNN. Further, We propose a
spectral-spatial axial aggregation transformer model, namely SaaFormer, to
address the challenges associated with hyperspectral image classifier that
considers HSI as long sequential three-dimensional images. The model comprises
two primary components: axial aggregation attention and multi-level
spectral-spatial extraction. The axial aggregation attention mechanism
effectively exploits the continuity and correlation among spectral bands at
each pixel position in hyperspectral images, while aggregating spatial
dimension features. This enables SaaFormer to maintain high precision even
under block-wise sampling. The multi-level spectral-spatial extraction
structure is designed to capture the sensitivity of different material
components to specific spectral bands, allowing the model to focus on a broader
range of spectral details. The results on six publicly available datasets
demonstrate that our model exhibits comparable performance when using random
sampling, while significantly outperforming other methods when employing
block-wise sampling partition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1&quot;&gt;Enzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhichang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dazhi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17010">
<title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose \textit{milliFlow}, a novel deep learning method for
scene flow estimation as a complementary motion information for mmWave point
cloud, serving as an intermediate level of features and directly benefiting
downstream human motion sensing tasks. Experimental results demonstrate the
superior performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we provide our codebase and dataset for
open access.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peijun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17059">
<title>The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17059</link>
<description rdf:parseType="Literal">&lt;p&gt;Scanned historical maps in libraries and archives are valuable repositories
of geographic data that often do not exist elsewhere. Despite the potential of
machine learning tools like the Google Vision APIs for automatically
transcribing text from these maps into machine-readable formats, they do not
work well with large-sized images (e.g., high-resolution scanned documents),
cannot infer the relation between the recognized text and other datasets, and
are challenging to integrate with post-processing tools. This paper introduces
the mapKurator system, an end-to-end system integrating machine learning models
with a comprehensive data processing pipeline. mapKurator empowers automated
extraction, post-processing, and linkage of text labels from large numbers of
large-dimension historical map scans. The output data, comprising bounding
polygons and recognized text, is in the standard GeoJSON format, making it
easily modifiable within Geographic Information Systems (GIS). The proposed
system allows users to quickly generate valuable data from large numbers of
historical maps for in-depth analysis of the map content and, in turn,
encourages map findability, accessibility, interoperability, and reusability
(FAIR principles). We deployed the mapKurator system and enabled the processing
of over 60,000 maps and over 100 million text/place names in the David Rumsey
Historical Map collection. We also demonstrated a seamless integration of
mapKurator with a collaborative web platform to enable accessing automated
approaches for extracting and linking text labels from historical map scans and
collective work to improve the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jina Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zekun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yijun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namgung_M/0/1/0/all/0/1&quot;&gt;Min Namgung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1&quot;&gt;Leeje Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1&quot;&gt;Yao-Yi Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17431">
<title>Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection. (arXiv:2306.17431v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting the salient objects in a remote sensing image has wide applications
for the interdisciplinary research. Many existing deep learning methods have
been proposed for Salient Object Detection (SOD) in remote sensing images and
get remarkable results. However, the recent adversarial attack examples,
generated by changing a few pixel values on the original remote sensing image,
could result in a collapse for the well-trained deep learning based SOD model.
Different with existing methods adding perturbation to original images, we
propose to jointly tune adversarial exposure and additive perturbation for
attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is
natural and common in remote sensing images, however, camouflaging cloud based
adversarial attack and defense for remote sensing images are not well studied
before. Furthermore, we design DefenseNet as a learn-able pre-processing to the
adversarial cloudy images so as to preserve the performance of the deep
learning based remote sensing SOD model, without tuning the already deployed
deep SOD model. By considering both regular and generalized adversarial
examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in
white-box setting and other attack methods in black-box setting. Experimental
results on a synthesized benchmark from the public remote sensing SOD dataset
(EORSSD) show the promising defense against adversarial cloud attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huiming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Lan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zibo Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuewei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17643">
<title>Neural 3D Scene Reconstruction from Multiple 2D Images without 3D Supervision. (arXiv:2306.17643v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17643</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural 3D scene reconstruction methods have achieved impressive performance
when reconstructing complex geometry and low-textured regions in indoor scenes.
However, these methods heavily rely on 3D data which is costly and
time-consuming to obtain in real world. In this paper, we propose a novel
neural reconstruction method that reconstructs scenes using sparse depth under
the plane constraints without 3D supervision. We introduce a signed distance
function field, a color field, and a probability field to represent a scene. We
optimize these fields to reconstruct the scene by using differentiable ray
marching with accessible 2D images as supervision. We improve the
reconstruction quality of complex geometry scene regions with sparse depth
obtained by using the geometric constraints. The geometric constraints project
3D points on the surface to similar-looking regions with similar features in
different 2D images. We impose the plane constraints to make large planes
parallel or vertical to the indoor floor. Both two constraints help reconstruct
accurate and smooth geometry structures of the scene. Without 3D supervision,
our method achieves competitive performance compared with existing methods that
use 3D supervision on the ScanNet dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Che Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yunde Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00397">
<title>Improving CNN-based Person Re-identification using score Normalization. (arXiv:2307.00397v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00397</link>
<description rdf:parseType="Literal">&lt;p&gt;Person re-identification (PRe-ID) is a crucial task in security,
surveillance, and retail analysis, which involves identifying an individual
across multiple cameras and views. However, it is a challenging task due to
changes in illumination, background, and viewpoint. Efficient feature
extraction and metric learning algorithms are essential for a successful PRe-ID
system. This paper proposes a novel approach for PRe-ID, which combines a
Convolutional Neural Network (CNN) based feature extraction method with
Cross-view Quadratic Discriminant Analysis (XQDA) for metric learning.
Additionally, a matching algorithm that employs Mahalanobis distance and a
score normalization process to address inconsistencies between camera scores is
implemented. The proposed approach is tested on four challenging datasets,
including VIPeR, GRID, CUHK01, and PRID450S, and promising results are
obtained. For example, without normalization, the rank-20 rate accuracies of
the GRID, CUHK01, VIPeR and PRID450S datasets were 61.92%, 83.90%, 92.03%,
96.22%; however, after score normalization, they have increased to 64.64%,
89.30%, 92.78%, and 98.76%, respectively. Accordingly, the promising results on
four challenging datasets indicate the effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chouchane_A/0/1/0/all/0/1&quot;&gt;Ammar Chouchane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouamane_A/0/1/0/all/0/1&quot;&gt;Abdelmalik Ouamane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1&quot;&gt;Wathiq Mansoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atalla_S/0/1/0/all/0/1&quot;&gt;Shadi Atalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzaibak_A/0/1/0/all/0/1&quot;&gt;Afaf Benzaibak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boudellal_C/0/1/0/all/0/1&quot;&gt;Chahrazed Boudellal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00804">
<title>SketchMetaFace: A Learning-based Sketching Interface for High-fidelity 3D Character Face Modeling. (arXiv:2307.00804v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00804</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling 3D avatars benefits various application scenarios such as AR/VR,
gaming, and filming. Character faces contribute significant diversity and
vividity as a vital component of avatars. However, building 3D character face
models usually requires a heavy workload with commercial tools, even for
experienced artists. Various existing sketch-based tools fail to support
amateurs in modeling diverse facial shapes and rich geometric details. In this
paper, we present SketchMetaFace - a sketching system targeting amateur users
to model high-fidelity 3D faces in minutes. We carefully design both the user
interface and the underlying algorithm. First, curvature-aware strokes are
adopted to better support the controllability of carving facial details.
Second, considering the key problem of mapping a 2D sketch map to a 3D model,
we develop a novel learning-based method termed &quot;Implicit and Depth Guided Mesh
Modeling&quot; (IDGMM). It fuses the advantages of mesh, implicit, and depth
representations to achieve high-quality results with high efficiency. In
addition, to further support usability, we present a coarse-to-fine 2D
sketching interface design and a data-driven stroke suggestion tool. User
studies demonstrate the superiority of our system over existing modeling tools
in terms of the ease to use and visual quality of results. Experimental
analyses also show that IDGMM reaches a better trade-off between accuracy and
efficiency. SketchMetaFace is available at
https://zhongjinluo.github.io/SketchMetaFace/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhongjin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1&quot;&gt;Dong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Heming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongbo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01146">
<title>AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01146</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shengyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09325">
<title>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2211.09325</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we imbue robots with the ability to efficiently manipulate unseen
objects and transfer relevant skills based on demonstrations? End-to-end
learning methods often fail to generalize to novel objects or unseen
configurations. Instead, we focus on the task-specific pose relationship
between relevant parts of interacting objects. We conjecture that this
relationship is a generalizable notion of a manipulation task that can transfer
to new objects in the same category; examples include the relationship between
the pose of a pan relative to an oven or the pose of a mug relative to a mug
rack. We call this task-specific pose relationship &quot;cross-pose&quot; and provide a
mathematical definition of this concept. We propose a vision-based system that
learns to estimate the cross-pose between two objects for a given manipulation
task using learned cross-object correspondences. The estimated cross-pose is
then used to guide a downstream motion planner to manipulate the objects into
the desired pose relationship (placing a pan into the oven or the mug onto the
mug rack). We demonstrate our method&apos;s capability to generalize to unseen
objects, in some cases after training on only 10 demonstrations in the real
world. Results show that our system achieves state-of-the-art performance in
both simulated and real-world experiments across a number of tasks.
Supplementary information and videos can be found at
https://sites.google.com/view/tax-pose/home.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chuer Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1&quot;&gt;Brian Okorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Harry Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1&quot;&gt;Ben Eisner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01088">
<title>Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data. (arXiv:2307.01088v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.01088</link>
<description rdf:parseType="Literal">&lt;p&gt;Conformal prediction has emerged as a rigorous means of providing deep
learning models with reliable uncertainty estimates and safety guarantees. Yet,
its performance is known to degrade under distribution shift and long-tailed
class distributions, which are often present in real world applications. Here,
we characterize the performance of several post-hoc and training-based
conformal prediction methods under these settings, providing the first
empirical evaluation on large-scale datasets and models. We show that across
numerous conformal methods and neural network families, performance greatly
degrades under distribution shifts violating safety guarantees. Similarly, we
show that in long-tailed settings the guarantees are frequently violated on
many classes. Understanding the limitations of these methods is necessary for
deployment in real world and safety-critical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasa_K/0/1/0/all/0/1&quot;&gt;Kevin Kasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>