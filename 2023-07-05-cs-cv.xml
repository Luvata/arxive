<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.06534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.04597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.09927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.05768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.00259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.03624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.11442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.02937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.08446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.14451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.10553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.03475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00068" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.00028">
<title>Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00028</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks for computer vision extract uninterpretable features despite
achieving high accuracy on benchmarks. In contrast, humans can explain their
predictions using succinct and intuitive descriptions. To incorporate
explainability into neural networks, we train a vision model whose feature
representations are text. We show that such a model can effectively classify
ImageNet images, and we discuss the challenges we encountered when training it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifullah_K/0/1/0/all/0/1&quot;&gt;Khalid Saifullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1&quot;&gt;Jonas Geiping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00037">
<title>MARF: The Medial Atom Ray Field Object Representation. (arXiv:2307.00037v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2307.00037</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Medial Atom Ray Fields (MARFs), a novel neural object
representation that enables accurate differentiable surface rendering with a
single network evaluation per camera ray. Existing neural ray fields struggle
with multi-view consistency and representing surface discontinuities. MARFs
address both using a medial shape representation, a dual representation of
solid geometry that yields cheap geometrically grounded surface normals, in
turn enabling computing analytical curvature despite the network having no
second derivative. MARFs map a camera ray to multiple medial intersection
candidates, subject to ray-sphere intersection testing. We illustrate how the
learned medial shape quantities applies to sub-surface scattering, part
segmentation, and aid representing a space of articulated shapes. Able to learn
a space of shape priors, MARFs may prove useful for tasks like shape retrieval
and shape completion, among others. Code and data can be found at
https://github.com/pbsds/MARF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundt_P/0/1/0/all/0/1&quot;&gt;Peder Bergebakken Sundt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theoharis_T/0/1/0/all/0/1&quot;&gt;Theoharis Theoharis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00038">
<title>Training-free Object Counting with Prompts. (arXiv:2307.00038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00038</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the problem of object counting in images. Existing
approaches rely on extensive training data with point annotations for each
object, making data collection labor-intensive and time-consuming. To overcome
this, we propose a training-free object counter that treats the counting task
as a segmentation problem. Our approach leverages the Segment Anything Model
(SAM), known for its high-quality masks and zero-shot segmentation capability.
However, the vanilla mask generation method of SAM lacks class-specific
information in the masks, resulting in inferior counting accuracy. To overcome
this limitation, we introduce a prior-guided mask generation method that
incorporates three types of priors into the segmentation process, enhancing
efficiency and accuracy. Additionally, we tackle the issue of counting objects
specified through free-form text by proposing a two-stage approach that
combines reference object selection and prior-guided mask generation. Extensive
experiments on standard datasets demonstrate the competitive performance of our
training-free counter compared to learning-based approaches. This paper
presents a promising solution for counting objects in various scenarios without
the need for extensive data collection and model training. Code is available at
https://github.com/shizenglin/training-free-object-counter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zenglin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Ying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00039">
<title>Towards Brain Inspired Design for Addressing the Shortcomings of ANNs. (arXiv:2307.00039v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.00039</link>
<description rdf:parseType="Literal">&lt;p&gt;As our understanding of the mechanisms of brain function is enhanced, the
value of insights gained from neuroscience to the development of AI algorithms
deserves further consideration. Here, we draw parallels with an existing
tree-based ANN architecture and a recent neuroscience study[27] arguing that
the error-based organization of neurons in the cerebellum that share a
preference for a personalized view of the entire error space, may account for
several desirable features of behavior and learning. We then analyze the
learning behavior and characteristics of the model under varying scenarios to
gauge the potential benefits of a similar mechanism in ANN. Our empirical
results suggest that having separate populations of neurons with personalized
error views can enable efficient learning under class imbalance and limited
data, and reduce the susceptibility to unintended shortcut strategies, leading
to improved generalization. This work highlights the potential of translating
the learning machinery of the brain into the design of a new generation of ANNs
and provides further credence to the argument that biologically inspired AI may
hold the key to overcoming the shortcomings of ANNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarfraz_F/0/1/0/all/0/1&quot;&gt;Fahad Sarfraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1&quot;&gt;Elahe Arani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1&quot;&gt;Bahram Zonooz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00040">
<title>DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00040</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI has made significant strides in computer vision, particularly
in image/video synthesis conditioned on text descriptions. Despite the
advancements, it remains challenging especially in the generation of
human-centric content such as dance synthesis. Existing dance synthesis methods
struggle with the gap between synthesized content and real-world dance
scenarios. In this paper, we define a new problem setting: Referring Human
Dance Generation, which focuses on real-world dance scenarios with three
important properties: (i) Faithfulness: the synthesis should retain the
appearance of both human subject foreground and background from the reference
image, and precisely follow the target pose; (ii) Generalizability: the model
should generalize to unseen human subjects, backgrounds, and poses; (iii)
Compositionality: it should allow for composition of seen/unseen subjects,
backgrounds, and poses from different sources. To address these challenges, we
introduce a novel approach, DISCO, which includes a novel model architecture
with disentangled control to improve the faithfulness and compositionality of
dance synthesis, and an effective human attribute pre-training for better
generalizability to unseen humans. Extensive qualitative and quantitative
results demonstrate that DISCO can generate high-quality human dance images and
videos with diverse appearances and flexible motions. Code, demo, video and
visualization are available at: https://disco-dance.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chung-Ching Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00064">
<title>Situated Cameras, Situated Knowledges: Towards an Egocentric Epistemology for Computer Vision. (arXiv:2307.00064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00064</link>
<description rdf:parseType="Literal">&lt;p&gt;In her influential 1988 paper, Situated Knowledges, Donna Haraway uses vision
and perspective as a metaphor to discuss scientific knowledge. Today,
egocentric computer vision discusses many of the same issues, except in a
literal vision context. In this short position paper, we collapse that
metaphor, and explore the interactions between feminist epistemology and
egocentric CV as &quot;Egocentric Epistemology.&quot; Using this framework, we argue for
the use of qualitative, human-centric methods as a complement to performance
benchmarks, to center both the literal and metaphorical perspective of human
crowd workers in CV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goree_S/0/1/0/all/0/1&quot;&gt;Samuel Goree&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1&quot;&gt;David Crandall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00083">
<title>A Parts Based Registration Loss for Detecting Knee Joint Areas. (arXiv:2307.00083v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00083</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a parts based loss is considered for finetune registering knee
joint areas. Here the parts are defined as abstract feature vectors with
location and they are automatically selected from a reference image. For a test
image the detected parts are encouraged to have a similar spatial configuration
than the corresponding parts in the reference image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiirola_J/0/1/0/all/0/1&quot;&gt;Juha Tiirola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00097">
<title>Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation. (arXiv:2307.00097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00097</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, CLIP-based approaches have exhibited remarkable performance on
generalization and few-shot learning tasks, fueled by the power of contrastive
language-vision pre-training. In particular, prompt tuning has emerged as an
effective strategy to adapt the pre-trained language-vision models to
downstream tasks by employing task-related textual tokens. Motivated by this
progress, in this work we question whether other fundamental problems, such as
weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning.
Our findings reveal two interesting observations that shed light on the impact
of prompt tuning on WSSS. First, modifying only the class token of the text
prompt results in a greater impact on the Class Activation Map (CAM), compared
to arguably more complex strategies that optimize the context. And second, the
class token associated with the image ground truth does not necessarily
correspond to the category that yields the best CAM. Motivated by these
observations, we introduce a novel approach based on a PrOmpt cLass lEarning
(POLE) strategy. Through extensive experiments we demonstrate that our simple,
yet efficient approach achieves SOTA performance in a well-known WSSS
benchmark. These results highlight not only the benefits of language-vision
models in WSSS but also the potential of prompt learning for this problem. The
code is available at https://github.com/rB080/WSS_POLE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_B/0/1/0/all/0/1&quot;&gt;Balamurali Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1&quot;&gt;Rukhshanda Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1&quot;&gt;Rajarshi Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00104">
<title>Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems. (arXiv:2307.00104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00104</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper addresses the challenge of detecting obscured wildfires
(when the fire flames are covered by trees, smoke, clouds, and other natural
barriers) in real-time using drones equipped only with RGB cameras. We propose
a novel methodology that employs semantic segmentation based on the temporal
analysis of smoke patterns in video sequences. Our approach utilizes an
encoder-decoder architecture based on deep convolutional neural network
architecture with a pre-trained CNN encoder and 3D convolutions for decoding
while using sequential stacking of features to exploit temporal variations. The
predicted fire locations can assist drones in effectively combating forest
fires and pinpoint fire retardant chemical drop on exact flame locations. We
applied our method to a curated dataset derived from the FLAME2 dataset that
includes RGB video along with IR video to determine the ground truth. Our
proposed method has a unique property of detecting obscured fire and achieves a
Dice score of 85.88%, while achieving a high precision of 92.47% and
classification accuracy of 90.67% on test data showing promising results when
inspected visually. Indeed, our method outperforms other methods by a
significant margin in terms of video-level fire classification as we obtained
about 100% accuracy using MobileNet+CBAM as the encoder backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meleti_U/0/1/0/all/0/1&quot;&gt;Uma Meleti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Razi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00122">
<title>An End-to-End Review of Gaze Estimation and its Interactive Applications on Handheld Mobile Devices. (arXiv:2307.00122v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.00122</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years we have witnessed an increasing number of interactive systems
on handheld mobile devices which utilise gaze as a single or complementary
interaction modality. This trend is driven by the enhanced computational power
of these devices, higher resolution and capacity of their cameras, and improved
gaze estimation accuracy obtained from advanced machine learning techniques,
especially in deep learning. As the literature is fast progressing, there is a
pressing need to review the state of the art, delineate the boundary, and
identify the key research challenges and opportunities in gaze estimation and
interaction. This paper aims to serve this purpose by presenting an end-to-end
holistic view in this area, from gaze capturing sensors, to gaze estimation
workflows, to deep learning techniques, and to gaze interactive applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yaxiong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shijing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khamis_M/0/1/0/all/0/1&quot;&gt;Mohamed Khamis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Juan Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00149">
<title>Hierarchical Neural Coding for Controllable CAD Model Generation. (arXiv:2307.00149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00149</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel generative model for Computer Aided Design (CAD)
that 1) represents high-level design concepts of a CAD model as a three-level
hierarchical tree of neural codes, from global part arrangement down to local
curve geometry; and 2) controls the generation or completion of CAD models by
specifying the target design using a code tree. Concretely, a novel variant of
a vector quantized VAE with &quot;masked skip connection&quot; extracts design variations
as neural codebooks at three levels. Two-stage cascaded auto-regressive
transformers learn to generate code trees from incomplete CAD models and then
complete CAD models following the intended design. Extensive experiments
demonstrate superior performance on conventional tasks such as random
generation while enabling novel interaction capabilities on conditional
generation tasks. The code is available at
https://github.com/samxuxiang/hnc-cad.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1&quot;&gt;Pradeep Kumar Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1&quot;&gt;Joseph G. Lambourne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1&quot;&gt;Karl D.D. Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1&quot;&gt;Yasutaka Furukawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00154">
<title>Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00154</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pretrained plain vision Transformers (ViTs) have been the workhorse for
many downstream tasks. However, existing works utilizing off-the-shelf ViTs are
inefficient in terms of training and deployment, because adopting ViTs with
individual sizes requires separate training and is restricted by fixed
performance-efficiency trade-offs. In this paper, we are inspired by stitchable
neural networks, which is a new framework that cheaply produces a single model
that covers rich subnetworks by stitching pretrained model families, supporting
diverse performance-efficiency trade-offs at runtime. Building upon this
foundation, we introduce SN-Netv2, a systematically improved model stitching
framework to facilitate downstream task adaptation. Specifically, we first
propose a Two-way stitching scheme to enlarge the stitching space. We then
design a resource-constrained sampling strategy that takes into account the
underlying FLOPs distributions in the space for improved sampling. Finally, we
observe that learning stitching layers is a low-rank update, which plays an
essential role on downstream tasks to stabilize training and ensure a good
Pareto frontier. With extensive experiments on ImageNet-1K, ADE20K,
COCO-Stuff-10K, NYUv2 and COCO-2017, SN-Netv2 demonstrates strong ability to
serve as a flexible vision backbone, achieving great advantages in both
training efficiency and adaptation. Code will be released at
https://github.com/ziplab/SN-Netv2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00174">
<title>Multiscale Progressive Text Prompt Network for Medical Image Segmentation. (arXiv:2307.00174v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.00174</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate segmentation of medical images is a crucial step in obtaining
reliable morphological statistics. However, training a deep neural network for
this task requires a large amount of labeled data to ensure high-accuracy
results. To address this issue, we propose using progressive text prompts as
prior knowledge to guide the segmentation process. Our model consists of two
stages. In the first stage, we perform contrastive learning on natural images
to pretrain a powerful prior prompt encoder (PPE). This PPE leverages text
prior prompts to generate multimodality features. In the second stage, medical
image and text prior prompts are sent into the PPE inherited from the first
stage to achieve the downstream medical image segmentation task. A multiscale
feature fusion block (MSFF) combines the features from the PPE to produce
multiscale multimodality features. These two progressive features not only
bridge the semantic gap but also improve prediction accuracy. Finally, an
UpAttention block refines the predicted results by merging the image and text
features. This design provides a simple and accurate way to leverage multiscale
progressive text prior prompts for medical image segmentation. Compared with
using only images, our model achieves high-quality results with low data
annotation costs. Moreover, our model not only has excellent reliability and
validity on medical images but also performs well on natural images. The
experimental results on different image datasets demonstrate that our model is
effective and robust for image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xianjun Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qianqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuejun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00179">
<title>Unsupervised Coordinate-Based Video Denoising. (arXiv:2307.00179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00179</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel unsupervised video denoising deep
learning approach that can help to mitigate data scarcity issues and shows
robustness against different noise patterns, enhancing its broad applicability.
Our method comprises three modules: a Feature generator creating features maps,
a Denoise-Net generating denoised but slightly blurry reference frames, and a
Refine-Net re-introducing high-frequency details. By leveraging the
coordinate-based network, we can greatly simplify the network structure while
preserving high-frequency details in the denoised video frames. Extensive
experiments on both simulated and real-captured demonstrate that our method can
effectively denoise real-world calcium imaging video sequences without prior
knowledge of noise models and data augmentation during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aiyetigbo_M/0/1/0/all/0/1&quot;&gt;Mary Damilola Aiyetigbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravichandran_D/0/1/0/all/0/1&quot;&gt;Dineshchandar Ravichandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalhoub_R/0/1/0/all/0/1&quot;&gt;Reda Chalhoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalivas_P/0/1/0/all/0/1&quot;&gt;Peter Kalivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Nianyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00182">
<title>Single-Stage Heavy-Tailed Food Classification. (arXiv:2307.00182v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00182</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based food image classification has enabled more accurate
nutrition content analysis for image-based dietary assessment by predicting the
types of food in eating occasion images. However, there are two major obstacles
to apply food classification in real life applications. First, real life food
images are usually heavy-tailed distributed, resulting in severe
class-imbalance issue. Second, it is challenging to train a single-stage (i.e.
end-to-end) framework under heavy-tailed data distribution, which cause the
over-predictions towards head classes with rich instances and under-predictions
towards tail classes with rare instance. In this work, we address both issues
by introducing a novel single-stage heavy-tailed food classification framework.
Our method is evaluated on two heavy-tailed food benchmark datasets, Food101-LT
and VFN-LT, and achieves the best performance compared to existing work with
over 5% improvements for top-1 accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiangpeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengqing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00183">
<title>Long-Tailed Continual Learning For Visual Food Recognition. (arXiv:2307.00183v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00183</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based food recognition has achieved remarkable progress in
predicting food types given an eating occasion image. However, there are two
major obstacles that hinder deployment in real world scenario. First, as new
foods appear sequentially overtime, a trained model needs to learn the new
classes continuously without causing catastrophic forgetting for already
learned knowledge of existing food types. Second, the distribution of food
images in real life is usually long-tailed as a small number of popular food
types are consumed more frequently than others, which can vary in different
populations. This requires the food recognition method to learn from
class-imbalanced data by improving the generalization ability on instance-rare
food classes. In this work, we focus on long-tailed continual learning and aim
to address both aforementioned challenges. As existing long-tailed food image
datasets only consider healthy people population, we introduce two new
benchmark food image datasets, VFN-INSULIN and VFN-T2D, which exhibits on the
real world food consumption for insulin takers and individuals with type 2
diabetes without taking insulin, respectively. We propose a novel end-to-end
framework for long-tailed continual learning, which effectively addresses the
catastrophic forgetting by applying an additional predictor for knowledge
distillation to avoid misalignment of representation during continual learning.
We also introduce a novel data augmentation technique by integrating
class-activation-map (CAM) and CutMix, which significantly improves the
generalization ability for instance-rare food classes to address the
class-imbalance issue. The proposed method show promising performance with
large margin improvements compared with existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiangpeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Luotao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jack Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eicher_Miller_H/0/1/0/all/0/1&quot;&gt;Heather A. Eicher-Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengqing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00198">
<title>Filter Pruning for Efficient CNNs via Knowledge-driven Differential Filter Sampler. (arXiv:2307.00198v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00198</link>
<description rdf:parseType="Literal">&lt;p&gt;Filter pruning simultaneously accelerates the computation and reduces the
memory overhead of CNNs, which can be effectively applied to edge devices and
cloud services. In this paper, we propose a novel Knowledge-driven Differential
Filter Sampler~(KDFS) with Masked Filter Modeling~(MFM) framework for filter
pruning, which globally prunes the redundant filters based on the prior
knowledge of a pre-trained model in a differential and non-alternative
optimization. Specifically, we design a differential sampler with learnable
sampling parameters to build a binary mask vector for each layer, determining
whether the corresponding filters are redundant. To learn the mask, we
introduce masked filter modeling to construct PCA-like knowledge by aligning
the intermediate features from the pre-trained teacher model and the outputs of
the student decoder taking sampling features as the input. The mask and sampler
are directly optimized by the Gumbel-Softmax Straight-Through Gradient
Estimator in an end-to-end manner in combination with global pruning
constraint, MFM reconstruction error, and dark knowledge. Extensive experiments
demonstrate the proposed KDFS&apos;s effectiveness in compressing the base models on
various datasets. For instance, the pruned ResNet-50 on ImageNet achieves
$55.36\%$ computation reduction, and $42.86\%$ parameter reduction, while only
dropping $0.35\%$ Top-1 accuracy, significantly outperforming the
state-of-the-art methods. The code is available at
\url{https://github.com/Osilly/KDFS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shaohui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenxuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jungong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00209">
<title>Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection
of hyperbole is an important part of understanding human expression. There have
been several studies on hyperbole detection, but most of which focus on text
modality only. However, with the development of social media, people can create
hyperbolic expressions with various modalities, including text, images, videos,
etc. In this paper, we focus on multimodal hyperbole detection. We create a
multimodal detection dataset\footnote{The dataset will be released to the
community.} from Weibo (a Chinese social media) and carry out some studies on
it. We treat the text and image from a piece of weibo as two modalities and
explore the role of text and image for hyperbole detection. Different
pre-trained multimodal encoders are also evaluated on this downstream task to
show their performance. Besides, since this dataset is constructed from five
different topics, we also evaluate the cross-domain performance of different
models. These studies can serve as a benchmark and point out the direction of
further study on multimodal hyperbole detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00211">
<title>AIGCIQA2023: A Large-scale Image Quality Assessment Database for AI Generated Images: from the Perspectives of Quality, Authenticity and Correspondence. (arXiv:2307.00211v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00211</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, in order to get a better understanding of the human visual
preferences for AIGIs, a large-scale IQA database for AIGC is established,
which is named as AIGCIQA2023. We first generate over 2000 images based on 6
state-of-the-art text-to-image generation models using 100 prompts.
&lt;/p&gt;
&lt;p&gt;Based on these images, a well-organized subjective experiment is conducted to
assess the human visual preferences for each image from three perspectives
including quality, authenticity and correspondence.
&lt;/p&gt;
&lt;p&gt;Finally, based on this large-scale database, we conduct a benchmark
experiment to evaluate the performance of several state-of-the-art IQA metrics
on our constructed database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiarui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Huiyu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00212">
<title>Internal-External Boundary Attention Fusion for Glass Surface Segmentation. (arXiv:2307.00212v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00212</link>
<description rdf:parseType="Literal">&lt;p&gt;Glass surfaces of transparent objects and mirrors are not able to be uniquely
and explicitly characterized by their visual appearances because they contain
the visual appearance of other reflected or transmitted surfaces as well.
Detecting glass regions from a single-color image is a challenging task. Recent
deep-learning approaches have paid attention to the description of glass
surface boundary where the transition of visual appearances between glass and
non-glass surfaces are observed. In this work, we analytically investigate how
glass surface boundary helps to characterize glass objects. Inspired by prior
semantic segmentation approaches with challenging image types such as X-ray or
CT scans, we propose separated internal-external boundary attention modules
that individually learn and selectively integrate visual characteristics of the
inside and outside region of glass surface from a single color image. Our
proposed method is evaluated on six public benchmarks comparing with
state-of-the-art methods showing promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongshen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungkyu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00213">
<title>More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data. (arXiv:2307.00213v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00213</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers are very powerful tools for a variety of tasks across domains,
from text generation to image captioning. However, transformers require
substantial amounts of training data, which is often a challenge in biomedical
settings, where high quality labeled data can be challenging or expensive to
obtain. This study investigates the efficacy of Compact Convolutional
Transformers (CCT) for robust medical image classification with limited data,
addressing a key issue faced by conventional Vision Transformers - their
requirement for large datasets. A hybrid of transformers and convolutional
layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed
a benchmark dataset of peripheral blood cell images of eight distinct cell
types, each represented by approximately 2,000 low-resolution (28x28x3 pixel)
samples. Despite the dataset size being smaller than those typically used with
Vision Transformers, we achieved a commendable classification accuracy of
92.49% and a micro-average ROC AUC of 0.9935. The CCT also learned quickly,
exceeding 80% validation accuracy after five epochs. Analysis of per-class
precision, recall, F1, and ROC showed that performance was strong across cell
types. Our findings underscore the robustness of CCTs, indicating their
potential as a solution to data scarcity issues prevalent in biomedical
imaging. We substantiate the applicability of CCTs in data-constrained areas
and encourage further work on CCTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1&quot;&gt;Andrew Kean Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00225">
<title>StyleStegan: Leak-free Style Transfer Based on Feature Steganography. (arXiv:2307.00225v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00225</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern social networks, existing style transfer methods suffer from a
serious content leakage issue, which hampers the ability to achieve serial and
reversible stylization, thereby hindering the further propagation of stylized
images in social networks. To address this problem, we propose a leak-free
style transfer method based on feature steganography. Our method consists of
two main components: a style transfer method that accomplishes artistic
stylization on the original image and an image steganography method that embeds
content feature secrets on the stylized image. The main contributions of our
work are as follows: 1) We identify and explain the phenomenon of content
leakage and its underlying causes, which arise from content inconsistencies
between the original image and its subsequent stylized image. 2) We design a
neural flow model for achieving loss-free and biased-free style transfer. 3) We
introduce steganography to hide content feature information on the stylized
image and control the subsequent usage rights. 4) We conduct comprehensive
experimental validation using publicly available datasets MS-COCO and Wikiart.
The results demonstrate that StyleStegan successfully mitigates the content
leakage issue in serial and reversible style transfer tasks. The SSIM
performance metrics for these tasks are 14.98% and 7.28% higher, respectively,
compared to a suboptimal baseline model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiujian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingshan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1&quot;&gt;Qichao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00226">
<title>S-Omninet: Structured Data Enhanced Universal Multimodal Learning Architecture. (arXiv:2307.00226v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00226</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal multitask learning has attracted an increasing interest in recent
years. Singlemodal models have been advancing rapidly and have achieved
astonishing results on various tasks across multiple domains. Multimodal
learning offers opportunities for further improvements by integrating data from
multiple modalities. Many methods are proposed to learn on a specific type of
multimodal data, such as vision and language data. A few of them are designed
to handle several modalities and tasks at a time. In this work, we extend and
improve Omninet, an architecture that is capable of handling multiple
modalities and tasks at a time, by introducing cross-cache attention,
integrating patch embeddings for vision inputs, and supporting structured data.
The proposed Structured-data-enhanced Omninet (S-Omninet) is a universal model
that is capable of learning from structured data of various dimensions
effectively with unstructured data through cross-cache attention, which enables
interactions among spatial, temporal, and structured features. We also enhance
spatial representations in a spatial cache with patch embeddings. We evaluate
the proposed model on several multimodal datasets and demonstrate a significant
improvement over the baseline, Omninet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Ye Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utke_J/0/1/0/all/0/1&quot;&gt;Jean Utke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00231">
<title>Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study. (arXiv:2307.00231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00231</link>
<description rdf:parseType="Literal">&lt;p&gt;The back-propagation algorithm has long been the de-facto standard in
optimizing weights and biases in neural networks, particularly in cutting-edge
deep learning models. Its widespread adoption in fields like natural language
processing, computer vision, and remote sensing has revolutionized automation
in various tasks. The popularity of back-propagation stems from its ability to
achieve outstanding performance in tasks such as classification, detection, and
segmentation. Nevertheless, back-propagation is not without its limitations,
encompassing sensitivity to initial conditions, vanishing gradients,
overfitting, and computational complexity. The recent introduction of a
forward-forward algorithm (FFA), which computes local goodness functions to
optimize network parameters, alleviates the dependence on substantial
computational resources and the constant need for architectural scaling. This
study investigates the application of FFA for hyperspectral image
classification. Experimental results and comparative analysis are provided with
the use of the traditional back-propagation algorithm. Preliminary results show
the potential behind FFA and its promises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paheding_S/0/1/0/all/0/1&quot;&gt;Sidike Paheding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyes_Angulo_A/0/1/0/all/0/1&quot;&gt;Abel A. Reyes-Angulo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00240">
<title>VesselMorph: Domain-Generalized Retinal Vessel Segmentation via Shape-Aware Representation. (arXiv:2307.00240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00240</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the absence of a single standardized imaging protocol, domain shift
between data acquired from different sites is an inherent property of medical
images and has become a major obstacle for large-scale deployment of
learning-based algorithms. For retinal vessel images, domain shift usually
presents as the variation of intensity, contrast and resolution, while the
basic tubular shape of vessels remains unaffected. Thus, taking advantage of
such domain-invariant morphological features can greatly improve the
generalizability of deep models. In this study, we propose a method named
VesselMorph which generalizes the 2D retinal vessel segmentation task by
synthesizing a shape-aware representation. Inspired by the traditional Frangi
filter and the diffusion tensor imaging literature, we introduce a
Hessian-based bipolar tensor field to depict the morphology of the vessels so
that the shape information is taken into account. We map the intensity image
and the tensor field to a latent space for feature extraction. Then we fuse the
two latent representations via a weight-balancing trick and feed the result to
a segmentation network. We evaluate on six public datasets of fundus and OCT
angiography images from diverse patient populations. VesselMorph achieves
superior generalization performance compared with competing methods in
different domain shift scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00245">
<title>Deep Angiogram: Trivializing Retinal Vessel Segmentation. (arXiv:2307.00245v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.00245</link>
<description rdf:parseType="Literal">&lt;p&gt;Among the research efforts to segment the retinal vasculature from fundus
images, deep learning models consistently achieve superior performance.
However, this data-driven approach is very sensitive to domain shifts. For
fundus images, such data distribution changes can easily be caused by
variations in illumination conditions as well as the presence of
disease-related features such as hemorrhages and drusen. Since the source
domain may not include all possible types of pathological cases, a model that
can robustly recognize vessels on unseen domains is desirable but remains
elusive, despite many proposed segmentation networks of ever-increasing
complexity. In this work, we propose a contrastive variational auto-encoder
that can filter out irrelevant features and synthesize a latent image, named
deep angiogram, representing only the retinal vessels. Then segmentation can be
readily accomplished by thresholding the deep angiogram. The generalizability
of the synthetic network is improved by the contrastive loss that makes the
model less sensitive to variations of image contrast and noisy features.
Compared to baseline deep segmentation networks, our model achieves higher
segmentation performance via simple thresholding. Our experiments show that the
model can generate stable angiograms on different target domains, providing
excellent visualization of vessels and a non-invasive, safe alternative to
fluorescein angiography.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yuankai K. Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00257">
<title>Efficient Subclass Segmentation in Medical Images. (arXiv:2307.00257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00257</link>
<description rdf:parseType="Literal">&lt;p&gt;As research interests in medical image analysis become increasingly
fine-grained, the cost for extensive annotation also rises. One feasible way to
reduce the cost is to annotate with coarse-grained superclass labels while
using limited fine-grained annotations as a complement. In this way,
fine-grained data learning is assisted by ample coarse annotations. Recent
studies in classification tasks have adopted this method to achieve
satisfactory results. However, there is a lack of research on efficient
learning of fine-grained subclasses in semantic segmentation tasks. In this
paper, we propose a novel approach that leverages the hierarchical structure of
categories to design network architecture. Meanwhile, a task-driven data
generation method is presented to make it easier for the network to recognize
different subclass categories. Specifically, we introduce a Prior Concatenation
module that enhances confidence in subclass segmentation by concatenating
predicted logits from the superclass classifier, a Separate Normalization
module that stretches the intra-class distance within the same superclass to
facilitate subclass segmentation, and a HierarchicalMix model that generates
high-quality pseudo labels for unlabeled samples by fusing only similar
superclass regions from labeled and unlabeled images. Our experiments on the
BraTS2021 and ACDC datasets demonstrate that our approach achieves comparable
accuracy to a model trained with full subclass annotations, with limited
subclass annotations and sufficient superclass annotations. Our approach offers
a promising solution for efficient fine-grained subclass segmentation in
medical images. Our code is publicly available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Linrui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1&quot;&gt;Wenhui Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00269">
<title>AE-RED: A Hyperspectral Unmixing Framework Powered by Deep Autoencoder and Regularization by Denoising. (arXiv:2307.00269v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00269</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral unmixing has been extensively studied with a variety of methods and
used in many applications. Recently, data-driven techniques with deep learning
methods have obtained great attention to spectral unmixing for its superior
learning ability to automatically learn the structure information. In
particular, autoencoder based architectures are elaborately designed to solve
blind unmixing and model complex nonlinear mixtures. Nevertheless, these
methods perform unmixing task as blackboxes and lack of interpretability. On
the other hand, conventional unmixing methods carefully design the regularizer
to add explicit information, in which algorithms such as plug-and-play (PnP)
strategies utilize off-the-shelf denoisers to plug powerful priors. In this
paper, we propose a generic unmixing framework to integrate the autoencoder
network with regularization by denoising (RED), named AE-RED. More specially,
we decompose the unmixing optimized problem into two subproblems. The first one
is solved using deep autoencoders to implicitly regularize the estimates and
model the mixture mechanism. The second one leverages the denoiser to bring in
the explicit information. In this way, both the characteristics of the deep
autoencoder based unmixing methods and priors provided by denoisers are merged
into our well-designed framework to enhance the unmixing performance.
Experiment results on both synthetic and real data sets show the superiority of
our proposed framework compared with state-of-the-art unmixing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Min Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobigeon_N/0/1/0/all/0/1&quot;&gt;Nicolas Dobigeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00270">
<title>HrSegNet : Real-time High-Resolution Neural Network with Semantic Guidance for Crack Segmentation. (arXiv:2307.00270v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00270</link>
<description rdf:parseType="Literal">&lt;p&gt;Through extensive research on deep learning in recent years and its
application in construction, crack detection has evolved rapidly from rough
detection at the image-level and patch-level to fine-grained detection at the
pixel-level, which better suits the nature of this field. Despite numerous
existing studies utilizing off-the-shelf deep learning models or enhancing
them, these models are not always effective or efficient in real-world
applications. In order to bridge this gap, we propose a High-resolution model
with Semantic guidance, specifically designed for real-time crack segmentation,
referred to as HrSegNet. Our model maintains high resolution throughout the
entire process, as opposed to recovering from low-resolution features to
high-resolution ones, thereby maximizing the preservation of crack details.
Moreover, to enhance the context information, we use low-resolution semantic
features to guide the reconstruction of high-resolution features. To ensure the
efficiency of the algorithm, we design a simple yet effective method to control
the computation cost of the entire model by controlling the capacity of
high-resolution channels, while providing the model with extremely strong
scalability. Extensive quantitative and qualitative evaluations demonstrate
that our proposed HrSegNet has exceptional crack segmentation capabilities, and
that maintaining high resolution and semantic guidance are crucial to the final
prediction. Compared to state-of-the-art segmentation models, HrSegNet achieves
the best trade-off between efficiency and effectiveness. Specifically, on the
crack dataset CrackSeg9k, our fastest model HrSegNet-B16 achieves a speed of
182 FPS with 78.43% mIoU, while our most accurate model HrSegNet-B48 achieves
80.32% mIoU with an inference speed of 140.3 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongshang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ronggui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1&quot;&gt;Gaoli Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00274">
<title>Common Knowledge Learning for Generating Transferable Adversarial Examples. (arXiv:2307.00274v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00274</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on an important type of black-box attacks, i.e.,
transfer-based adversarial attacks, where the adversary generates adversarial
examples by a substitute (source) model and utilize them to attack an unseen
target model, without knowing its information. Existing methods tend to give
unsatisfactory adversarial transferability when the source and target models
are from different types of DNN architectures (e.g. ResNet-18 and Swin
Transformer). In this paper, we observe that the above phenomenon is induced by
the output inconsistency problem. To alleviate this problem while effectively
utilizing the existing DNN models, we propose a common knowledge learning (CKL)
framework to learn better network weights to generate adversarial examples with
better transferability, under fixed network architectures. Specifically, to
reduce the model-specific features and obtain better output distributions, we
construct a multi-teacher framework, where the knowledge is distilled from
different teacher architectures into one student network. By considering that
the gradient of input is usually utilized to generated adversarial examples, we
impose constraints on the gradients between the student and teacher models, to
further alleviate the output inconsistency problem and enhance the adversarial
transferability. Extensive experiments demonstrate that our proposed work can
significantly improve the adversarial transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruijie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanfang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junfu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00280">
<title>SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (arXiv:2307.00280v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00280</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive studies have shown that deep learning models are vulnerable to
adversarial and natural noises, yet little is known about model robustness on
noises caused by different system implementations. In this paper, we for the
first time introduce SysNoise, a frequently occurred but often overlooked noise
in the deep learning training-deployment cycle. In particular, SysNoise happens
when the source training system switches to a disparate target system in
deployments, where various tiny system mismatch adds up to a non-negligible
difference. We first identify and classify SysNoise into three categories based
on the inference stage; we then build a holistic benchmark to quantitatively
measure the impact of SysNoise on 20+ models, comprehending image
classification, object detection, instance segmentation and natural language
processing tasks. Our extensive experiments revealed that SysNoise could bring
certain impacts on model robustness across different tasks and common
mitigations like data augmentation and adversarial training show limited
effects on it. Together, our findings open a new research topic and we hope
this work will raise research attention to deep learning deployment systems
accounting for model performance. We have open-sourced the benchmark and
framework at https://modeltc.github.io/systemnoise_web.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Ruihao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tianzi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fengwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00290">
<title>All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00290</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) is a recently proposed prompt-based
segmentation model in a generic zero-shot segmentation approach. With the
zero-shot segmentation capacity, SAM achieved impressive flexibility and
precision on various segmentation tasks. However, the current pipeline requires
manual prompts during the inference stage, which is still resource intensive
for biomedical image segmentation. In this paper, instead of using prompts
during the inference stage, we introduce a pipeline that utilizes the SAM,
called all-in-SAM, through the entire AI development workflow (from annotation
generation to model finetuning) without requiring manual prompts during the
inference stage. Specifically, SAM is first employed to generate pixel-level
annotations from weak prompts (e.g., points, bounding box). Then, the
pixel-level annotations are used to finetune the SAM segmentation model rather
than training from scratch. Our experimental results reveal two key findings:
1) the proposed pipeline surpasses the state-of-the-art (SOTA) methods in a
nuclei segmentation task on the public Monuseg dataset, and 2) the utilization
of weak and few annotations for SAM finetuning achieves competitive performance
compared to using strong pixel-wise annotated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1&quot;&gt;Ruining Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Quan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1&quot;&gt;Tianyuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1&quot;&gt;Shunxing Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remedios_L/0/1/0/all/0/1&quot;&gt;Lucas W. Remedios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yucheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yuankai Huo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00293">
<title>AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.00293</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Transformers have gained considerable attention because they achieve
both the energy efficiency of Spiking Neural Networks (SNNs) and the high
capacity of Transformers. However, the existing Spiking Transformer
architectures, derived from ANNs, exhibit a notable architectural gap,
resulting in suboptimal performance compared to their ANN counterparts.
Traditional approaches to discovering optimal architectures primarily rely on
either manual procedures, which are time-consuming, or Neural Architecture
Search (NAS) methods, which are usually expensive in terms of memory footprints
and computation time. To address these limitations, we introduce AutoST, a
training-free NAS method for Spiking Transformers, to rapidly identify
high-performance and energy-efficient Spiking Transformer architectures. Unlike
existing training-free NAS methods, which struggle with the
non-differentiability and high sparsity inherent in SNNs, we propose to utilize
Floating-Point Operations (FLOPs) as a performance metric, which is independent
of model computations and training dynamics, leading to a stronger correlation
with performance. Moreover, to enable the search for energy-efficient
architectures, we leverage activation patterns during initialization to
estimate the energy consumption of Spiking Transformers. Our extensive
experiments show that AutoST models outperform state-of-the-art manually or
automatically designed SNN architectures on static and neuromorphic datasets,
while significantly reducing energy consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qidong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jinku Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongkuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00300">
<title>DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation. (arXiv:2307.00300v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00300</link>
<description rdf:parseType="Literal">&lt;p&gt;While large-scale pre-trained text-to-image models can synthesize diverse and
high-quality human-centric images, an intractable problem is how to preserve
the face identity for conditioned face images. Existing methods either require
time-consuming optimization for each face-identity or learning an efficient
encoder at the cost of harming the editability of models. In this work, we
present an optimization-free method for each face identity, meanwhile keeping
the editability for text-to-image models. Specifically, we propose a novel
face-identity encoder to learn an accurate representation of human faces, which
applies multi-scale face features followed by a multi-embedding projector to
directly generate the pseudo words in the text embedding space. Besides, we
propose self-augmented editability learning to enhance the editability of
models, which is achieved by constructing paired generated face and edited face
images using celebrity names, aiming at transferring mature ability of
off-the-shelf text-to-image models in celebrity faces to unseen faces.
Extensive experiments show that our methods can generate identity-preserved
images under different scenes at a much faster speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuowei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shancheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mengqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhendong Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00306">
<title>SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation. (arXiv:2307.00306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00306</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting objects and estimating their 6D poses is essential for automated
systems to interact safely with the environment. Most 6D pose estimators,
however, rely on a single camera frame and suffer from occlusions and
ambiguities due to object symmetries. We overcome this issue by presenting a
novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach
efficiently fuses the RGB-D frames from multiple perspectives in a deep
multi-directional fusion network and predicts predefined keypoints for all
objects in the scene simultaneously. Based on the keypoints and an instance
semantic segmentation, we efficiently compute the 6D poses by least-squares
fitting. To address the ambiguity issues for symmetric objects, we propose a
novel training procedure for symmetry-aware keypoint detection including a new
objective function. Our SyMFM6D network significantly outperforms the
state-of-the-art in both single-view and multi-view 6D pose estimation. We
furthermore show the effectiveness of our symmetry-aware training procedure and
demonstrate that our approach is robust towards inaccurate camera calibration
and dynamic camera setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duffhauss_F/0/1/0/all/0/1&quot;&gt;Fabian Duffhauss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1&quot;&gt;Sebastian Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00309">
<title>Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (arXiv:2307.00309v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00309</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has successfully solved a wide range of tasks in 2D vision as a
dominant AI technique. Recently, deep learning on 3D point clouds is becoming
increasingly popular for addressing various tasks in this field. Despite
remarkable achievements, deep learning algorithms are vulnerable to adversarial
attacks. These attacks are imperceptible to the human eye but can easily fool
deep neural networks in the testing and deployment stage. To encourage future
research, this survey summarizes the current progress on adversarial attack and
defense techniques on point cloud classification. This paper first introduces
the principles and characteristics of adversarial attacks and summarizes and
analyzes the adversarial example generation methods in recent years. Besides,
it classifies defense strategies as input transformation, data optimization,
and deep model modification. Finally, it presents several challenging issues
and future research directions in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1&quot;&gt;Hanieh Naderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1&quot;&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00313">
<title>PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers. (arXiv:2307.00313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00313</link>
<description rdf:parseType="Literal">&lt;p&gt;The Transformer-based detectors (i.e., DETR) have demonstrated impressive
performance on end-to-end object detection. However, transferring DETR to
different data distributions may lead to a significant performance degradation.
Existing adaptation techniques focus on model-based approaches, which aim to
leverage feature alignment to narrow the distribution shift between different
domains. In this study, we propose a hierarchical Prompt Domain Memory (PDM)
for adapting detection transformers to different distributions. PDM
comprehensively leverages the prompt memory to extract domain-specific
knowledge and explicitly constructs a long-term memory space for the data
distribution, which represents better domain diversity compared to existing
methods. Specifically, each prompt and its corresponding distribution value are
paired in the memory space, and we inject top M distribution-similar prompts
into the input and multi-level embeddings of DETR. Additionally, we introduce
the Prompt Memory Alignment (PMA) to reduce the discrepancy between the source
and target domains by fully leveraging the domain-specific knowledge extracted
from the prompt domain memory. Extensive experiments demonstrate that our
method outperforms state-of-the-art domain adaptive object detection methods on
three benchmarks, including scene, synthetic to real, and weather adaptation.
Codes will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1&quot;&gt;Peidong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Senqiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiarui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaodong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00314">
<title>Detection of River Sandbank for Sand Mining with the Presence of Other High Mineral Content Regions Using Multi-spectral Images. (arXiv:2307.00314v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00314</link>
<description rdf:parseType="Literal">&lt;p&gt;Sand mining is a booming industry. The river sandbank is one of the primary
sources of sand mining. Detection of potential river sandbank regions for sand
mining directly impacts the economy, society, and environment. In the past,
semi-supervised and supervised techniques have been used to detect mining
regions including sand mining. A few techniques employ multi-modal analysis
combining different modalities such as multi-spectral imaging, synthetic
aperture radar (\emph{SAR}) imaging, aerial images, and point cloud data.
However, the distinguishing spectral characteristics of river sandbank regions
are yet to be fully explored. This paper provides a novel method to detect
river sandbank regions for sand mining using multi-spectral images without any
labeled data over the seasons. Association with a river stream and the
abundance of minerals are the most prominent features of such a region. The
proposed work uses these distinguishing features to determine the spectral
signature of a river sandbank region, which is robust to other high mineral
abundance regions. It follows a two-step approach, where first, potential high
mineral regions are detected and next, they are segregated using the presence
of a river stream. The proposed technique provides average accuracy, precision,
and recall of 90.75%, 85.47%, and 73.5%, respectively over the seasons from
Landsat 8 images without using any labeled dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_J/0/1/0/all/0/1&quot;&gt;Jit Mukherjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00320">
<title>Automatic Solver Generator for Systems of Laurent Polynomial Equations. (arXiv:2307.00320v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00320</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision applications, the following problem often arises: Given a
family of (Laurent) polynomial systems with the same monomial structure but
varying coefficients, find a solver that computes solutions for any family
member as fast as possible. Under appropriate genericity assumptions, the
dimension and degree of the respective polynomial ideal remain unchanged for
each particular system in the same family. The state-of-the-art approach to
solving such problems is based on elimination templates, which are the
coefficient (Macaulay) matrices that encode the transformation from the initial
polynomials to the polynomials needed to construct the action matrix. Knowing
an action matrix, the solutions of the system are computed from its
eigenvectors. The important property of an elimination template is that it
applies to all polynomial systems in the family. In this paper, we propose a
new practical algorithm that checks whether a given set of Laurent polynomials
is sufficient to construct an elimination template. Based on this algorithm, we
propose an automatic solver generator for systems of Laurent polynomial
equations. The new generator is simple and fast; it applies to ideals with
positive-dimensional components; it allows one to uncover partial $p$-fold
symmetries automatically. We test our generator on various minimal problems,
mostly in geometric computer vision. The speed of the generated solvers exceeds
the state-of-the-art in most cases. In particular, we propose the solvers for
the following problems: optimal 3-view triangulation, semi-generalized hybrid
pose estimation and minimal time-of-arrival self-calibration. The experiments
on synthetic scenes show that our solvers are numerically accurate and either
comparable to or significantly faster than the state-of-the-art solvers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martyushev_E/0/1/0/all/0/1&quot;&gt;Evgeniy Martyushev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhayani_S/0/1/0/all/0/1&quot;&gt;Snehal Bhayani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajdla_T/0/1/0/all/0/1&quot;&gt;Tomas Pajdla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.06534">
<title>Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Diagnosis. (arXiv:2003.06534v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.06534</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical diagnosis assistant (MDA) aims to build an interactive diagnostic
agent to sequentially inquire about symptoms for discriminating diseases.
However, since the dialogue records used to build a patient simulator are
collected passively, the data might be deteriorated by some task-unrelated
biases, such as the preference of the collectors. These biases might hinder the
diagnostic agent to capture transportable knowledge from the simulator. This
work attempts to address these critical issues in MDA by taking advantage of
the causal diagram to identify and resolve two representative non-causal
biases, i.e., (i) default-answer bias and (ii) distributional inquiry bias.
Specifically, Bias (i) originates from the patient simulator which tries to
answer the unrecorded inquiries with some biased default answers. Consequently,
the diagnostic agents cannot fully demonstrate their advantages due to the
biased answers. To eliminate this bias and inspired by the propensity score
matching technique with causal diagram, we propose a propensity-based patient
simulator to effectively answer unrecorded inquiry by drawing knowledge from
the other records; Bias (ii) inherently comes along with the passively
collected data, and is one of the key obstacles for training the agent towards
&quot;learning how&quot; rather than &quot;remembering what&quot;. For example, within the
distribution of training data, if a symptom is highly coupled with a certain
disease, the agent might learn to only inquire about that symptom to
discriminate that disease, thus might not generalize to the out-of-distribution
cases. To this end, we propose a progressive assurance agent, which includes
the dual processes accounting for symptom inquiry and disease diagnosis
respectively. The inquiry process is driven by the diagnosis process in a
top-down manner to inquire about symptoms for enhancing diagnostic confidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junfan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Keze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.04597">
<title>Generalized iterated-sums signatures. (arXiv:2012.04597v3 [math.RA] UPDATED)</title>
<link>http://arxiv.org/abs/2012.04597</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the algebraic properties of a generalized version of the
iterated-sums signature, inspired by previous work of F.~Kir\&apos;aly and
H.~Oberhauser. In particular, we show how to recover the character property of
the associated linear map over the tensor algebra by considering a deformed
quasi-shuffle product of words on the latter. We introduce three non-linear
transformations on iterated-sums signatures, close in spirit to Machine
Learning applications, and show some of their properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Diehl_J/0/1/0/all/0/1&quot;&gt;Joscha Diehl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ebrahimi_Fard_K/0/1/0/all/0/1&quot;&gt;Kurusch Ebrahimi-Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tapia_N/0/1/0/all/0/1&quot;&gt;Nikolas Tapia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.09927">
<title>Robust Extrinsic Symmetry Estimation in 3D Point Clouds. (arXiv:2109.09927v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.09927</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting the reflection symmetry plane of an object represented by a 3D
point cloud is a fundamental problem in 3D computer vision and geometry
processing due to its various applications, such as compression, object
detection, robotic grasping, 3D surface reconstruction, etc. There exist
several efficient approaches for solving this problem for clean 3D point
clouds. However, it is a challenging problem to solve in the presence of
outliers and missing parts. The existing methods try to overcome this challenge
mostly by voting-based techniques but do not work efficiently. In this work, we
proposed a statistical estimator-based approach for the plane of reflection
symmetry that is robust to outliers and missing parts. We pose the problem of
finding the optimal estimator for the reflection symmetry as an optimization
problem on a 2-Sphere that quickly converges to the global solution for an
approximate initialization. We further adapt the heat kernel signature for
symmetry invariant matching of mirror symmetric points. This approach helps us
to decouple the chicken-and-egg problem of finding the optimal symmetry plane
and correspondences between the reflective symmetric points. The proposed
approach achieves comparable mean ground-truth error and 4.5\% increment in the
F-score as compared to the state-of-the-art approaches on the benchmark
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagar_R/0/1/0/all/0/1&quot;&gt;Rajendra Nagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03549">
<title>GaTector: A Unified Framework for Gaze Object Prediction. (arXiv:2112.03549v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03549</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaze object prediction is a newly proposed task that aims to discover the
objects being stared at by humans. It is of great application significance but
still lacks a unified solution framework. An intuitive solution is to
incorporate an object detection branch into an existing gaze prediction method.
However, previous gaze prediction methods usually use two different networks to
extract features from scene image and head image, which would lead to heavy
network architecture and prevent each branch from joint optimization. In this
paper, we build a novel framework named GaTector to tackle the gaze object
prediction problem in a unified way. Particularly, a specific-general-specific
(SGS) feature extractor is firstly proposed to utilize a shared backbone to
extract general features for both scene and head images. To better consider the
specificity of inputs and tasks, SGS introduces two input-specific blocks
before the shared backbone and three task-specific blocks after the shared
backbone. Specifically, a novel Defocus layer is designed to generate
object-specific features for the object detection task without losing
information or requiring extra computations. Moreover, the energy aggregation
loss is introduced to guide the gaze heatmap to concentrate on the stared box.
In the end, we propose a novel wUoC metric that can reveal the difference
between boxes even when they share no overlapping area. Extensive experiments
on the GOO dataset verify the superiority of our method in all three tracks,
i.e. object detection, gaze estimation, and gaze object prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baoshan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhijie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.05768">
<title>Spectral Compressive Imaging Reconstruction Using Convolution and Contextual Transformer. (arXiv:2201.05768v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.05768</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral compressive imaging (SCI) is able to encode the high-dimensional
hyperspectral image to a 2D measurement, and then uses algorithms to
reconstruct the spatio-spectral data-cube. At present, the main bottleneck of
SCI is the reconstruction algorithm, and the state-of-the-art (SOTA)
reconstruction methods generally face the problem of long reconstruction time
and/or poor detail recovery. In this paper, we propose a novel hybrid network
module, namely CCoT (Convolution and Contextual Transformer) block, which can
acquire the inductive bias ability of convolution and the powerful modeling
ability of transformer simultaneously,and is conducive to improving the quality
of reconstruction to restore fine details. We integrate the proposed CCoT block
into deep unfolding framework based on the generalized alternating projection
algorithm, and further propose the GAP-CCoT network. Through the experiments of
extensive synthetic and real data, our proposed model achieves higher
reconstruction quality ($&amp;gt;$2dB in PSNR on simulated benchmark datasets) and
shorter running time than existing SOTA algorithms by a large margin. The code
and models are publicly available at https://github.com/ucaswangls/GAP-CCoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lishun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zongliang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yong Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.00259">
<title>Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection. (arXiv:2203.00259v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.00259</link>
<description rdf:parseType="Literal">&lt;p&gt;Density-based and classification-based methods have ruled unsupervised
anomaly detection in recent years, while reconstruction-based methods are
rarely mentioned for the poor reconstruction ability and low performance.
However, the latter requires no costly extra training samples for the
unsupervised training that is more practical, so this paper focuses on
improving this kind of method and proposes a novel Omni-frequency
Channel-selection Reconstruction (OCR-GAN) network to handle anomaly detection
task in a perspective of frequency. Concretely, we propose a Frequency
Decoupling (FD) module to decouple the input image into different frequency
components and model the reconstruction process as a combination of parallel
omni-frequency image restorations, as we observe a significant difference in
the frequency distribution of normal and abnormal images. Given the correlation
among multiple frequencies, we further propose a Channel Selection (CS) module
that performs frequency interaction among different encoders by adaptively
selecting different channels. Abundant experiments demonstrate the
effectiveness and superiority of our approach over different kinds of methods,
e.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec AD
dataset without extra training data that markedly surpasses the
reconstruction-based baseline by +38.1 and the current SOTA method by +0.3.
Source code is available at https://github.com/zhangzjn/OCR-GAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yufei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiangning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiwei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Runze Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shuwen Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.03624">
<title>FCNet: A Convolutional Neural Network for Arbitrary-Length Exposure Estimation. (arXiv:2203.03624v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.03624</link>
<description rdf:parseType="Literal">&lt;p&gt;The photographs captured by digital cameras usually suffer from over or under
exposure problems. For image exposure enhancement, the tasks of Single-Exposure
Correction (SEC) and Multi-Exposure Fusion (MEF) are widely studied in the
image processing community. However, current SEC or MEF methods are developed
under different motivations and thus ignore the internal correlation between
SEC and MEF, making it difficult to process arbitrary-length sequences with
improper exposures. Besides, the MEF methods usually fail at estimating the
exposure of a sequence containing only under-exposed or over-exposed images. To
alleviate these problems, in this paper, we develop a novel Fusion-Correction
Network (FCNet) to tackle an arbitrary-length (including one) image sequence
with improper exposures. This is achieved by fusing and correcting an image
sequence by Laplacian Pyramid (LP) image decomposition. In each LP level, the
low-frequency base component of the input image sequence is fed into a Fusion
block and a Correction block sequentially for consecutive exposure estimation,
implemented by alternative exposure fusion and correction. The
exposure-corrected image in current LP level is upsampled and fused with the
high-frequency detail components of the input image sequence in the next LP
level, to output the base component for the Fusion and Correction blocks in
next LP level. Experiments on the benchmark dataset demonstrate that our FCNet
is effective on arbitrary-length exposure estimation, including both SEC and
MEF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.11442">
<title>Scalable Video Object Segmentation with Identification Mechanism. (arXiv:2203.11442v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.11442</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into the challenges of achieving scalable and effective
multi-object modeling for semi-supervised Video Object Segmentation (VOS).
Previous VOS methods decode features with a single positive object, limiting
the learning of multi-object representation as they must match and segment each
target separately under multi-object scenarios. Additionally, earlier
techniques catered to specific application objectives and lacked the
flexibility to fulfill different speed-accuracy requirements. To address these
problems, we present two innovative approaches, Associating Objects with
Transformers (AOT) and Associating Objects with Scalable Transformers (AOST).
In pursuing effective multi-object modeling, AOT introduces the IDentification
(ID) mechanism to allocate each object a unique identity. This approach enables
the network to model the associations among all objects simultaneously, thus
facilitating the tracking and segmentation of objects in a single network pass.
To address the challenge of inflexible deployment, AOST further integrates
scalable long short-term transformers that incorporate layer-wise ID-based
attention and scalable supervision. This overcomes ID embeddings&apos;
representation limitations and enables online architecture scalability in VOS
for the first time. Given the absence of a benchmark for VOS involving densely
multi-object annotations, we propose a challenging Video Object Segmentation in
the Wild (VOSW) benchmark to validate our approaches. We evaluated various AOT
and AOST variants using extensive experiments across VOSW and five
commonly-used VOS benchmarks. Our approaches surpass the state-of-the-art
competitors and display exceptional efficiency and scalability consistently
across all six benchmarks. Moreover, we notably achieved the 1st position in
the 3rd Large-scale Video Object Segmentation Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1&quot;&gt;Jiaxu Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.02937">
<title>Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.02937</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network classifiers can largely rely on simple spurious features, such
as backgrounds, to make predictions. However, even in these cases, we show that
they still often learn core features associated with the desired attributes of
the data, contrary to recent findings. Inspired by this insight, we demonstrate
that simple last layer retraining can match or outperform state-of-the-art
approaches on spurious correlation benchmarks, but with profoundly lower
complexity and computational expenses. Moreover, we show that last layer
retraining on large ImageNet-trained models can also significantly reduce
reliance on background and texture information, improving robustness to
covariate shift, after only minutes of training on a single GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirichenko_P/0/1/0/all/0/1&quot;&gt;Polina Kirichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1&quot;&gt;Pavel Izmailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.08446">
<title>VSA: Learning Varied-Size Window Attention in Vision Transformers. (arXiv:2204.08446v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.08446</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention within windows has been widely explored in vision transformers to
balance the performance, computation complexity, and memory footprint. However,
current models adopt a hand-crafted fixed-size window design, which restricts
their capacity of modeling long-term dependencies and adapting to objects of
different sizes. To address this drawback, we propose
\textbf{V}aried-\textbf{S}ize Window \textbf{A}ttention (VSA) to learn adaptive
window configurations from data. Specifically, based on the tokens within each
default window, VSA employs a window regression module to predict the size and
location of the target window, i.e., the attention area where the key and value
tokens are sampled. By adopting VSA independently for each attention head, it
can model long-term dependencies, capture rich context from diverse windows,
and promote information exchange among overlapped windows. VSA is an
easy-to-implement module that can replace the window attention in
state-of-the-art representative models with minor modifications and negligible
extra computational cost while improving their performance by a large margin,
e.g., 1.1\% for Swin-T on ImageNet classification. In addition, the performance
gain increases when using larger images for training and test. Experimental
results on more downstream tasks, including object detection, instance
segmentation, and semantic segmentation, further demonstrate the superiority of
VSA over the vanilla window attention in dealing with objects of different
sizes. The code will be released
https://github.com/ViTAE-Transformer/ViTAE-VSA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yufei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10824">
<title>ReLU Fields: The Little Non-linearity That Could. (arXiv:2205.10824v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10824</link>
<description rdf:parseType="Literal">&lt;p&gt;In many recent works, multi-layer perceptions (MLPs) have been shown to be
suitable for modeling complex spatially-varying functions including images and
3D scenes. Although the MLPs are able to represent complex scenes with
unprecedented quality and memory footprint, this expressive power of the MLPs,
however, comes at the cost of long training and inference times. On the other
hand, bilinear/trilinear interpolation on regular grid based representations
can give fast training and inference times, but cannot match the quality of
MLPs without requiring significant additional memory. Hence, in this work, we
investigate what is the smallest change to grid-based representations that
allows for retaining the high fidelity result of MLPs while enabling fast
reconstruction and rendering times. We introduce a surprisingly simple change
that achieves this task -- simply allowing a fixed non-linearity (ReLU) on
interpolated grid values. When combined with coarse to-fine optimization, we
show that such an approach becomes competitive with the state-of-the-art. We
report results on radiance fields, and occupancy fields, and compare against
multiple existing alternatives. Code and data for the paper are available at
https://geometry.cs.ucl.ac.uk/projects/2022/relu_fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnewar_A/0/1/0/all/0/1&quot;&gt;Animesh Karnewar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1&quot;&gt;Tobias Ritschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1&quot;&gt;Oliver Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1&quot;&gt;Niloy J. Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08751">
<title>Perceptual Quality Assessment of Virtual Reality Videos in the Wild. (arXiv:2206.08751v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08751</link>
<description rdf:parseType="Literal">&lt;p&gt;Investigating how people perceive virtual reality videos in the wild (\ie,
those captured by everyday users) is a crucial and challenging task in
VR-related applications due to complex \textit{authentic} distortions localized
in space and time. Existing panoramic video databases only consider synthetic
distortions, assume fixed viewing conditions, and are limited in size. To
overcome these shortcomings, we construct the VR Video Quality in the Wild
(VRVQW) database, which is one of the first of its kind, and contains $502$
user-generated videos with diverse content and distortion characteristics.
Based on VRVQW, we conduct a formal psychophysical experiment to record the
scanpaths and perceived quality scores from $139$ participants under two
different viewing conditions. We provide a thorough statistical analysis of the
recorded data, observing significant impact of viewing conditions on both human
scanpaths and perceived quality. Moreover, we develop an objective quality
assessment model for VR videos based on pseudocylindrical representation and
convolution. Results on the proposed VRVQW show that our method is superior to
existing video quality assessment models, only underperforming viewport-based
models that otherwise rely on human scanpaths for projection. Last, we explore
the additional use of the VRVQW dataset to benchmark saliency detection
techniques, highlighting the need for further research. We have made the
database and code available at
\url{https://github.com/limuhit/VR-Video-Quality-in-the-Wild}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wen Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yiru Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1&quot;&gt;Xiangjie Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yabin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Long Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kede Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.14451">
<title>SRCN3D: Sparse R-CNN 3D for Compact Convolutional Multi-View 3D Object Detection and Tracking. (arXiv:2206.14451v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.14451</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection and tracking of moving objects is an essential component in
environmental perception for autonomous driving. In the flourishing field of
multi-view 3D camera-based detectors, different transformer-based pipelines are
designed to learn queries in 3D space from 2D feature maps of perspective
views, but the dominant dense BEV query mechanism is computationally
inefficient. This paper proposes Sparse R-CNN 3D (SRCN3D), a novel two-stage
fully-sparse detector that incorporates sparse queries, sparse attention with
box-wise sampling, and sparse prediction. SRCN3D adopts a cascade structure
with the twin-track update of both a fixed number of query boxes and latent
query features. Our novel sparse feature sampling module only utilizes local 2D
region of interest (RoI) features calculated by the projection of 3D query
boxes for further box refinement, leading to a fully-convolutional and
deployment-friendly pipeline. For multi-object tracking, motion features, query
features and RoI features are comprehensively utilized in multi-hypotheses data
association. Extensive experiments on nuScenes dataset demonstrate that SRCN3D
achieves competitive performance in both 3D object detection and multi-object
tracking tasks, while also exhibiting superior efficiency compared to
transformer-based methods. Code and models are available at
https://github.com/synsin0/SRCN3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yining Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jingyan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shiqi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diange Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.10553">
<title>MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.10553</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MABe22, a large-scale, multi-agent video and trajectory
benchmark to assess the quality of learned behavior representations. This
dataset is collected from a variety of biology experiments, and includes
triplets of interacting mice (4.7 million frames video+pose tracking data, 10
million frames pose only), symbiotic beetle-ant interactions (10 million frames
video data), and groups of interacting flies (4.4 million frames of pose
tracking data). Accompanying these data, we introduce a panel of real-life
downstream analysis tasks to assess the quality of learned representations by
evaluating how well they preserve information about the experimental conditions
(e.g. strain, time of day, optogenetic stimulation) and animal behavior. We
test multiple state-of-the-art self-supervised video and trajectory
representation learning methods to demonstrate the use of our benchmark,
revealing that methods developed using human action datasets do not fully
translate to animal datasets. We hope that our benchmark and dataset encourage
a broader exploration of behavior representation learning methods across
species and settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_M/0/1/0/all/0/1&quot;&gt;Markus Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_A/0/1/0/all/0/1&quot;&gt;Andrew Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Dipam Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geuther_B/0/1/0/all/0/1&quot;&gt;Brian Geuther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_E/0/1/0/all/0/1&quot;&gt;Edward Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1&quot;&gt;Heng Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vivek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oleszko_S/0/1/0/all/0/1&quot;&gt;Sebastian Oleszko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Partridge_Z/0/1/0/all/0/1&quot;&gt;Zachary Partridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peelman_M/0/1/0/all/0/1&quot;&gt;Milan Peelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robie_A/0/1/0/all/0/1&quot;&gt;Alice Robie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schretter_C/0/1/0/all/0/1&quot;&gt;Catherine E. Schretter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheppard_K/0/1/0/all/0/1&quot;&gt;Keith Sheppard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uttarwar_P/0/1/0/all/0/1&quot;&gt;Param Uttarwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1&quot;&gt;Julian M. Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_E/0/1/0/all/0/1&quot;&gt;Eric Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_J/0/1/0/all/0/1&quot;&gt;Joseph Parker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1&quot;&gt;Pietro Perona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branson_K/0/1/0/all/0/1&quot;&gt;Kristin Branson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1&quot;&gt;Ann Kennedy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12752">
<title>T-Person-GAN: Text-to-Person Image Generation with Identity-Consistency and Manifold Mix-Up. (arXiv:2208.12752v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12752</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an end-to-end approach to generate high-resolution
person images conditioned on texts only. State-of-the-art text-to-image
generation models are mainly designed for center-object generation, e.g.,
flowers and birds. Unlike center-placed objects with similar shapes and
orientation, person image generation is a more challenging task, for which we
observe the followings: 1) the generated images for the same person exhibit
visual details with identity-consistency, e.g., identity-related
textures/clothes/shoes across the images, and 2) those images should be
discriminant for being robust against the inter-person variations caused by
visual ambiguities. To address the above challenges, we develop an effective
generative model to produce person images with two novel mechanisms. In
particular, our first mechanism (called T-Person-GAN-ID) is to integrate the
one-stream generator with an identity-preserving network such that the
representations of generated data are regularized in their feature space to
ensure the identity-consistency. The second mechanism (called
T-Person-GAN-ID-MM) is based on the manifold mix-up to produce mixed images via
the linear interpolation across generated images from different manifold
identities, and we further enforce such interpolated images to be linearly
classified in the feature space. This amounts to learning a linear
classification boundary that can perfectly separate images from two identities.
Our proposed method is empirically validated to achieve a remarkable
improvement in text-to-person image generation. Our architecture is orthogonal
to StackGAN++ , and focuses on person image generation, with all of them
together to enrich the spectrum of GANs for the image generation task. Codes
are available on
\url{https://github.com/linwu-github/Person-Image-Generation.git}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Deyin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lin Yuanbo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00798">
<title>PCDNF: Revisiting Learning-based Point Cloud Denoising via Joint Normal Filtering. (arXiv:2209.00798v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00798</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering high quality surfaces from noisy point clouds, known as point
cloud denoising, is a fundamental yet challenging problem in geometry
processing. Most of the existing methods either directly denoise the noisy
input or filter raw normals followed by updating point positions. Motivated by
the essential interplay between point cloud denoising and normal filtering, we
revisit point cloud denoising from a multitask perspective, and propose an
end-to-end network, named PCDNF, to denoise point clouds via joint normal
filtering. In particular, we introduce an auxiliary normal filtering task to
help the overall network remove noise more effectively while preserving
geometric features more accurately. In addition to the overall architecture,
our network has two novel modules. On one hand, to improve noise removal
performance, we design a shape-aware selector to construct the latent tangent
space representation of the specific point by comprehensively considering the
learned point and normal features and geometry priors. On the other hand, point
features are more suitable for describing geometric details, and normal
features are more conducive for representing geometric structures (e.g., sharp
edges and corners). Combining point and normal features allows us to overcome
their weaknesses. Thus, we design a feature refinement module to fuse point and
normal features for better recovering geometric information. Extensive
evaluations, comparisons, and ablation studies demonstrate that the proposed
method outperforms state-of-the-arts for both point cloud denoising and normal
filtering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yaowu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_S/0/1/0/all/0/1&quot;&gt;Sijing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Renjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Ying He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.03475">
<title>Convolutional Neural Network (CNN) to reduce construction loss in JPEG compression caused by Discrete Fourier Transform (DFT). (arXiv:2209.03475v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.03475</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent decades, digital image processing has gained enormous popularity.
Consequently, a number of data compression strategies have been put forth, with
the goal of minimizing the amount of information required to represent images.
Among them, JPEG compression is one of the most popular methods that has been
widely applied in multimedia and digital applications. The periodic nature of
DFT makes it impossible to meet the periodic condition of an image&apos;s opposing
edges without producing severe artifacts, which lowers the image&apos;s perceptual
visual quality. On the other hand, deep learning has recently achieved
outstanding results for applications like speech recognition, image reduction,
and natural language processing. Convolutional Neural Networks (CNN) have
received more attention than most other types of deep neural networks. The use
of convolution in feature extraction results in a less redundant feature map
and a smaller dataset, both of which are crucial for image compression. In this
work, an effective image compression method is purposed using autoencoders. The
study&apos;s findings revealed a number of important trends that suggested better
reconstruction along with good compression can be achieved using autoencoders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kunwar_S/0/1/0/all/0/1&quot;&gt;Suman Kunwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08101">
<title>Budget-Aware Pruning for Multi-Domain Learning. (arXiv:2210.08101v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08101</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has achieved state-of-the-art performance on several computer
vision tasks and domains. Nevertheless, it still has a high computational cost
and demands a significant amount of parameters. Such requirements hinder the
use in resource-limited environments and demand both software and hardware
optimization. Another limitation is that deep models are usually specialized
into a single domain or task, requiring them to learn and store new parameters
for each new one. Multi-Domain Learning (MDL) attempts to solve this problem by
learning a single model that is capable of performing well in multiple domains.
Nevertheless, the models are usually larger than the baseline for a single
domain. This work tackles both of these problems: our objective is to prune
models capable of handling multiple domains according to a user defined budget,
making them more computationally affordable while keeping a similar
classification performance. We achieve this by encouraging all domains to use a
similar subset of filters from the baseline model, up to the amount defined by
the user&apos;s budget. Then, filters that are not used by any domain are pruned
from the network. The proposed approach innovates by better adapting to
resource-limited devices while, to our knowledge, being the only work that is
capable of handling multiple domains at test time with fewer parameters and
lower computational complexity than the baseline model for a single domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_S/0/1/0/all/0/1&quot;&gt;Samuel Felipe dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berriel_R/0/1/0/all/0/1&quot;&gt;Rodrigo Berriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_Santos_T/0/1/0/all/0/1&quot;&gt;Thiago Oliveira-Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1&quot;&gt;Jurandy Almeida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08710">
<title>Handling Label Uncertainty for Camera Incremental Person Re-Identification. (arXiv:2210.08710v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08710</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental learning for person re-identification (ReID) aims to develop
models that can be trained with a continuous data stream, which is a more
practical setting for real-world applications. However, the existing
incremental ReID methods make two strong assumptions that the cameras are fixed
and the new-emerging data is class-disjoint from previous classes. This is
unrealistic as previously observed pedestrians may re-appear and be captured
again by new cameras. In this paper, we investigate person ReID in an
unexplored scenario named Camera Incremental Person ReID (CIPR), which advances
existing lifelong person ReID by taking into account the class overlap issue.
Specifically, new data collected from new cameras may probably contain an
unknown proportion of identities seen before. This subsequently leads to the
lack of cross-camera annotations for new data due to privacy concerns. To
address these challenges, we propose a novel framework ExtendOVA. First, to
handle the class overlap issue, we introduce an instance-wise seen-class
identification module to discover previously seen identities at the instance
level. Then, we propose a criterion for selecting confident ID-wise candidates
and also devise an early learning regularization term to correct noise issues
in pseudo labels. Furthermore, to compensate for the lack of previous data, we
resort prototypical memory bank to create surrogate features, along with a
cross-camera distillation loss to further retain the inter-camera relationship.
The comprehensive experimental results on multiple benchmarks show that
ExtendOVA significantly outperforms the state-of-the-arts with remarkable
advantages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zexian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dayan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wanqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10969">
<title>SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading. (arXiv:2210.10969v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10969</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has been widely applied to learn image
representations through exploiting unlabeled images. However, it has not been
fully explored in the medical image analysis field. In this work, we propose
Saliency-guided Self-Supervised image Transformer (SSiT) for diabetic
retinopathy (DR) grading from fundus images. We novelly introduce saliency maps
into SSL, with a goal of guiding self-supervised pre-training with
domain-specific prior knowledge. Specifically, two saliency-guided learning
tasks are employed in SSiT: (1) We conduct saliency-guided contrastive learning
based on the momentum contrast, wherein we utilize fundus images&apos; saliency maps
to remove trivial patches from the input sequences of the momentum-updated key
encoder. And thus, the key encoder is constrained to provide target
representations focusing on salient regions, guiding the query encoder to
capture salient features. (2) We train the query encoder to predict the
saliency segmentation, encouraging preservation of fine-grained information in
the learned representations. Extensive experiments are conducted on four
publicly-accessible fundus image datasets. The proposed SSiT significantly
outperforms other representative state-of-the-art SSL methods on all datasets
and under various evaluation settings, establishing the effectiveness of the
learned representations from SSiT. The source code is available at
https://github.com/YijinHuang/SSiT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yijin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Junyan Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pujin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tam_R/0/1/0/all/0/1&quot;&gt;Roger Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaoying Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02133">
<title>Streaming Audio-Visual Speech Recognition with Alignment Regularization. (arXiv:2211.02133v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02133</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a streaming AV-ASR system based on a hybrid
connectionist temporal classification (CTC)/attention neural network
architecture. The audio and the visual encoder neural networks are both based
on the conformer architecture, which is made streamable using chunk-wise
self-attention (CSA) and causal convolution. Streaming recognition with a
decoder neural network is realized by using the triggered attention technique,
which performs time-synchronous decoding with joint CTC/attention scoring.
Additionally, we propose a novel alignment regularization technique that
promotes synchronization of the audio and visual encoder, which in turn results
in better word error rates (WERs) at all SNR levels for streaming and offline
AV-ASR models. The proposed AV-ASR model achieves WERs of 2.0% and 2.6% on the
Lip Reading Sentences 3 (LRS3) dataset in an offline and online setup,
respectively, which both present state-of-the-art results when no external
training data are used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moritz_N/0/1/0/all/0/1&quot;&gt;Niko Moritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Petridis_S/0/1/0/all/0/1&quot;&gt;Stavros Petridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuegen_C/0/1/0/all/0/1&quot;&gt;Christian Fuegen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pantic_M/0/1/0/all/0/1&quot;&gt;Maja Pantic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05039">
<title>Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task. (arXiv:2211.05039v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05039</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a challenging decision-making task that we call active
acquisition for multimodal temporal data (A2MT). In many real-world scenarios,
input features are not readily available at test time and must instead be
acquired at significant cost. With A2MT, we aim to learn agents that actively
select which modalities of an input to acquire, trading off acquisition cost
and predictive performance. A2MT extends a previous task called active feature
acquisition to temporal decision making about high-dimensional inputs. We
propose a method based on the Perceiver IO architecture to address A2MT in
practice. Our agents are able to solve a novel synthetic scenario requiring
practically relevant cross-modal reasoning skills. On two large-scale,
real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn
cost-reactive acquisition behavior. However, an ablation reveals they are
unable to learn adaptive acquisition strategies, emphasizing the difficulty of
the task even for state-of-the-art models. Applications of A2MT may be
impactful in domains like medicine, robotics, or finance, where modalities
differ in acquisition cost and informativeness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kossen_J/0/1/0/all/0/1&quot;&gt;Jannik Kossen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1&quot;&gt;C&amp;#x103;t&amp;#x103;lina Cangea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vertes_E/0/1/0/all/0/1&quot;&gt;Eszter V&amp;#xe9;rtes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1&quot;&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1&quot;&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ktena_I/0/1/0/all/0/1&quot;&gt;Ira Ktena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1&quot;&gt;Nenad Tomasev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belgrave_D/0/1/0/all/0/1&quot;&gt;Danielle Belgrave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05862">
<title>MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis. (arXiv:2211.05862v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05862</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple instance learning exhibits a powerful approach for whole slide
image-based diagnosis in the absence of pixel- or patch-level annotations. In
spite of the huge size of hole slide images, the number of individual slides is
often rather small, leading to a small number of labeled samples. To improve
training, we propose and investigate different data augmentation strategies for
multiple instance learning based on the idea of linear interpolations of
feature vectors (known as MixUp). Based on state-of-the-art multiple instance
learning architectures and two thyroid cancer data sets, an exhaustive study is
conducted considering a range of common data augmentation strategies. Whereas a
strategy based on to the original MixUp approach showed decreases in accuracy,
the use of a novel intra-slide interpolation method led to consistent increases
in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadermayr_M/0/1/0/all/0/1&quot;&gt;Michael Gadermayr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koller_L/0/1/0/all/0/1&quot;&gt;Lukas Koller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschuchnig_M/0/1/0/all/0/1&quot;&gt;Maximilian Tschuchnig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stangassinger_L/0/1/0/all/0/1&quot;&gt;Lea Maria Stangassinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreutzer_C/0/1/0/all/0/1&quot;&gt;Christina Kreutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couillard_Despres_S/0/1/0/all/0/1&quot;&gt;Sebastien Couillard-Despres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostingh_G/0/1/0/all/0/1&quot;&gt;Gertie Janneke Oostingh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hittmair_A/0/1/0/all/0/1&quot;&gt;Anton Hittmair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12860">
<title>DETRs with Collaborative Hybrid Assignments Training. (arXiv:2211.12860v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12860</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide the observation that too few queries assigned as
positive samples in DETR with one-to-one set matching leads to sparse
supervisions on the encoder&apos;s output which considerably hurt the discriminative
feature learning of the encoder and vice visa for attention learning in the
decoder. To alleviate this, we present a novel collaborative hybrid assignments
training scheme, namely Co-DETR, to learn more efficient and effective
DETR-based detectors from versatile label assignment manners. This new training
scheme can easily enhance the encoder&apos;s learning ability in end-to-end
detectors by training the multiple parallel auxiliary heads supervised by
one-to-many label assignments such as ATSS and Faster RCNN. In addition, we
conduct extra customized positive queries by extracting the positive
coordinates from these auxiliary heads to improve the training efficiency of
positive samples in the decoder. In inference, these auxiliary heads are
discarded and thus our method introduces no additional parameters and
computational cost to the original detector while requiring no hand-crafted
non-maximum suppression (NMS). We conduct extensive experiments to evaluate the
effectiveness of the proposed approach on DETR variants, including DAB-DETR,
Deformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic
Deformable-DETR by 5.8% AP in 12-epoch training and 3.2% AP in 36-epoch
training. The state-of-the-art DINO-Deformable-DETR with Swin-L can still be
improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with
ViT-L backbone, we achieve 65.6% AP on COCO test-dev, outperforming previous
methods with much fewer model sizes. Codes will be available at
https://github.com/Sense-X/Co-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guanglu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15875">
<title>Data Poisoning Attack Aiming the Vulnerability of Continual Learning. (arXiv:2211.15875v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15875</link>
<description rdf:parseType="Literal">&lt;p&gt;Generally, regularization-based continual learning models limit access to the
previous task data to imitate the real-world constraints related to memory and
privacy. However, this introduces a problem in these models by not being able
to track the performance on each task. In essence, current continual learning
methods are susceptible to attacks on previous tasks. We demonstrate the
vulnerability of regularization-based continual learning methods by presenting
a simple task-specific data poisoning attack that can be used in the learning
process of a new task. Training data generated by the proposed attack causes
performance degradation on a specific task targeted by the attacker. We
experiment with the attack on the two representative regularization-based
continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI), trained with variants of MNIST dataset. The experiment
results justify the vulnerability proposed in this paper and demonstrate the
importance of developing continual learning models that are robust to
adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1&quot;&gt;Gyojin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaehyun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1&quot;&gt;Hyeong Gwon Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16779">
<title>Attention-Based Depth Distillation with 3D-Aware Positional Encoding for Monocular 3D Object Detection. (arXiv:2211.16779v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16779</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular 3D object detection is a low-cost but challenging task, as it
requires generating accurate 3D localization solely from a single image input.
Recent developed depth-assisted methods show promising results by using
explicit depth maps as intermediate features, which are either precomputed by
monocular depth estimation networks or jointly evaluated with 3D object
detection. However, inevitable errors from estimated depth priors may lead to
misaligned semantic information and 3D localization, hence resulting in feature
smearing and suboptimal predictions. To mitigate this issue, we propose ADD, an
Attention-based Depth knowledge Distillation framework with 3D-aware positional
encoding. Unlike previous knowledge distillation frameworks that adopt stereo-
or LiDAR-based teachers, we build up our teacher with identical architecture as
the student but with extra ground-truth depth as input. Credit to our teacher
design, our framework is seamless, domain-gap free, easily implementable, and
is compatible with object-wise ground-truth depth. Specifically, we leverage
intermediate features and responses for knowledge distillation. Considering
long-range 3D dependencies, we propose \emph{3D-aware self-attention} and
\emph{target-aware cross-attention} modules for student adaptation. Extensive
experiments are performed to verify the effectiveness of our framework on the
challenging KITTI 3D object detection benchmark. We implement our framework on
three representative monocular detectors, and we achieve state-of-the-art
performance with no additional inference computational cost relative to
baseline models. Our code is available at https://github.com/rockywind/ADD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zizhang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1&quot;&gt;Jian Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoquan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01165">
<title>Deep Active Learning for Multi-Label Classification of Remote Sensing Images. (arXiv:2212.01165v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01165</link>
<description rdf:parseType="Literal">&lt;p&gt;In this letter, we introduce deep active learning (AL) for multi-label
classification (MLC) problems in remote sensing (RS). In particular, we
investigate the effectiveness of several AL query functions for MLC of RS
images. Unlike the existing AL query functions (which are defined for
single-label classification or semantic segmentation problems), each query
function in this paper is based on the evaluation of two criteria: i)
multi-label uncertainty; and ii) multi-label diversity. The multi-label
uncertainty criterion is associated to the confidence of the deep neural
networks (DNNs) in correctly assigning multi-labels to each image. To assess
this criterion, we investigate three strategies: i) learning multi-label loss
ordering; ii) measuring temporal discrepancy of multi-label predictions; and
iii) measuring magnitude of approximated gradient embeddings. The multi-label
diversity criterion is associated to the selection of a set of images that are
as diverse as possible to each other that prevents redundancy among them. To
assess this criterion, we exploit a clustering based strategy. We combine each
of the above-mentioned uncertainty strategies with the clustering based
diversity strategy, resulting in three different query functions. All the
considered query functions are introduced for the first time in the framework
of MLC problems in RS. Experimental results obtained on two benchmark archives
show that these query functions result in the selection of a highly informative
set of samples at each iteration of the AL process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollenbrok_L/0/1/0/all/0/1&quot;&gt;Lars M&amp;#xf6;llenbrok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1&quot;&gt;Gencer Sumbul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1&quot;&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10908">
<title>Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection. (arXiv:2301.10908v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10908</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a simple method to distill and detect backdoor patterns
within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the
&quot;minimal essence&quot; from an input image responsible for the model&apos;s prediction.
CD optimizes an input mask to extract a small pattern from the input image that
can lead to the same model output (i.e., logits or deep features). The
extracted pattern can help understand the cognitive mechanism of a model on
clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP).
Using CD and the distilled CPs, we uncover an interesting phenomenon of
backdoor attacks: despite the various forms and sizes of trigger patterns used
by different attacks, the CPs of backdoor samples are all surprisingly and
suspiciously small. One thus can leverage the learned mask to detect and remove
backdoor examples from poisoned training datasets. We conduct extensive
experiments to show that CD can robustly detect a wide range of advanced
backdoor attacks. We also show that CD can potentially be applied to help
detect potential biases from face datasets. Code is available at
\url{https://github.com/HanxunH/CognitiveDistillation}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hanxun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01034">
<title>An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D LiDAR. (arXiv:2302.01034v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01034</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle pose estimation with LiDAR is essential in the perception technology
of autonomous driving. However, due to incomplete observation measurements and
sparsity of the LiDAR point cloud, it is challenging to achieve satisfactory
pose extraction based on 3D LiDAR by using the existing pose estimation
methods. In addition, the requirement for real-time performance further
increases the difficulty of the pose estimation task. In this paper, we
proposed a novel convex hull-based vehicle pose estimation method. The
extracted 3D cluster is reduced to the convex hull, reducing the computation
burden and retaining contour information. Then a novel criterion based on the
minimum occlusion area is developed for the search-based algorithm, which can
achieve accurate pose estimation. This criterion also makes the proposed
algorithm especially suitable for obstacle avoidance. The proposed algorithm is
validated on the KITTI dataset and a manually labeled dataset acquired at an
industrial park. The results show that our proposed method can achieve better
accuracy than the state-of-the-art pose estimation method while maintaining
real-time speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ningning Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06279">
<title>Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. (arXiv:2302.06279v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06279</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have demonstrated remarkable performance across
various tasks, including image and speech recognition. However, maximizing the
effectiveness of DNNs requires meticulous optimization of numerous
hyperparameters and network parameters through training. Moreover,
high-performance DNNs entail many parameters, which consume significant energy
during training. In order to overcome these challenges, researchers have turned
to spiking neural networks (SNNs), which offer enhanced energy efficiency and
biologically plausible data processing capabilities, rendering them highly
suitable for sensory data tasks, particularly in neuromorphic data. Despite
their advantages, SNNs, like DNNs, are susceptible to various threats,
including adversarial examples and backdoor attacks. Yet, the field of SNNs
still needs to be explored in terms of understanding and countering these
attacks.
&lt;/p&gt;
&lt;p&gt;This paper delves into backdoor attacks in SNNs using neuromorphic datasets
and diverse triggers. Specifically, we explore backdoor triggers within
neuromorphic data that can manipulate their position and color, providing a
broader scope of possibilities than conventional triggers in domains like
images. We present various attack strategies, achieving an attack success rate
of up to 100\% while maintaining a negligible impact on clean accuracy.
Furthermore, we assess these attacks&apos; stealthiness, revealing that our most
potent attacks possess significant stealth capabilities. Lastly, we adapt
several state-of-the-art defenses from the image domain, evaluating their
efficacy on neuromorphic data and uncovering instances where they fall short,
leading to compromised performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abad_G/0/1/0/all/0/1&quot;&gt;Gorka Abad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ersoy_O/0/1/0/all/0/1&quot;&gt;Oguzhan Ersoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbieta_A/0/1/0/all/0/1&quot;&gt;Aitor Urbieta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07570">
<title>Super-Resolution of BVOC Maps by Adapting Deep Learning Methods. (arXiv:2302.07570v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07570</link>
<description rdf:parseType="Literal">&lt;p&gt;Biogenic Volatile Organic Compounds (BVOCs) play a critical role in
biosphere-atmosphere interactions, being a key factor in the physical and
chemical properties of the atmosphere and climate. Acquiring large and
fine-grained BVOC emission maps is expensive and time-consuming, so most
available BVOC data are obtained on a loose and sparse sampling grid or on
small regions. However, high-resolution BVOC data are desirable in many
applications, such as air quality, atmospheric chemistry, and climate
monitoring. In this work, we investigate the possibility of enhancing BVOC
acquisitions, further explaining the relationships between the environment and
these compounds. We do so by comparing the performances of several
state-of-the-art neural networks proposed for image Super-Resolution (SR),
adapting them to overcome the challenges posed by the large dynamic range of
the emission and reduce the impact of outliers in the prediction. Moreover, we
also consider realistic scenarios, considering both temporal and geographical
constraints. Finally, we present possible future developments regarding SR
generalization, considering the scale-invariance property and super-resolving
emissions from unseen compounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Giganti_A/0/1/0/all/0/1&quot;&gt;Antonio Giganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandelli_S/0/1/0/all/0/1&quot;&gt;Sara Mandelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bestagini_P/0/1/0/all/0/1&quot;&gt;Paolo Bestagini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marcon_M/0/1/0/all/0/1&quot;&gt;Marco Marcon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tubaro_S/0/1/0/all/0/1&quot;&gt;Stefano Tubaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10289">
<title>Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models. (arXiv:2302.10289v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10289</link>
<description rdf:parseType="Literal">&lt;p&gt;We use concept-based interpretable models to mitigate shortcut learning.
Existing methods lack interpretability. Beginning with a Blackbox, we
iteratively carve out a mixture of interpretable experts (MoIE) and a residual
network. Each expert explains a subset of data using First Order Logic (FOL).
While explaining a sample, the FOL from biased BB-derived MoIE detects the
shortcut effectively. Finetuning the BB with Metadata Normalization (MDN)
eliminates the shortcut. The FOLs from the finetuned-BB-derived MoIE verify the
elimination of the shortcut. Our experiments show that MoIE does not hurt the
accuracy of the original BB and eliminates shortcuts effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10894">
<title>Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10894</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable AI tools are often motivated by the goal of understanding model
behavior in out-of-distribution (OOD) contexts. Despite the attention this area
of study receives, there are comparatively few cases where these tools have
identified novel, previously unknown, bugs in models. We argue that this is
due, in part, to a common feature of many interpretability methods: they
analyze and explain the behavior of a model using a particular dataset. While
this is useful, such tools can only analyze behaviors induced by features that
the user can sample or identify in advance. To address this, a growing body of
research involves interpreting models using feature synthesis methods which do
not depend on a dataset.
&lt;/p&gt;
&lt;p&gt;In this paper, our primary contribution is a benchmark to evaluate
interpretability tools. Our key insight is that we can train models that
respond to specific triggers (e.g., a specific patch inserted into an image)
with specific outputs (i.e. a label) and then evaluate interpretability tools
based on whether they help humans identify these triggers. We make four
contributions. (1) We propose trojan discovery as an evaluation task for
interpretability tools and introduce a trojan-discovery benchmark with 12
trojans of 3 different types. (2) We demonstrate the difficulty of this
benchmark with a preliminary evaluation of 16 feature attribution/saliency
tools. Even with access to data with a trojan&apos;s trigger, these methods
regularly fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on
our benchmark. (4) We introduce and evaluate 2 variants of the best-performing
method from the previous evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_T/0/1/0/all/0/1&quot;&gt;Tong Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kevin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1&quot;&gt;Kaivalya Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11012">
<title>Likelihood Annealing: Fast Calibrated Uncertainty for Regression. (arXiv:2302.11012v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11012</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning have shown that uncertainty estimation is
becoming increasingly important in applications such as medical imaging,
natural language processing, and autonomous systems. However, accurately
quantifying uncertainty remains a challenging problem, especially in regression
tasks where the output space is continuous. Deep learning approaches that allow
uncertainty estimation for regression problems often converge slowly and yield
poorly calibrated uncertainty estimates that can not be effectively used for
quantification. Recently proposed post hoc calibration techniques are seldom
applicable to regression problems and often add overhead to an already slow
model training phase. This work presents a fast calibrated uncertainty
estimation method for regression tasks called Likelihood Annealing, that
consistently improves the convergence of deep regression models and yields
calibrated uncertainty without any post hoc calibration phase. Unlike previous
methods for calibrated uncertainty in regression that focus only on
low-dimensional regression problems, our method works well on a broad spectrum
of regression problems, including high-dimensional regression.Our empirical
analysis shows that our approach is generalizable to various network
architectures, including multilayer perceptrons, 1D/2D convolutional networks,
and graph neural networks, on five vastly diverse tasks, i.e., chaotic particle
trajectory denoising, physical property prediction of molecules using 3D
atomistic representation, natural image super-resolution, and medical image
translation using MRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1&quot;&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jae Myung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04253">
<title>TMHOI: Translational Model for Human-Object Interaction Detection. (arXiv:2303.04253v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04253</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting human-object interactions (HOIs) is an intricate challenge in the
field of computer vision. Existing methods for HOI detection heavily rely on
appearance-based features, but these may not fully capture all the essential
characteristics necessary for accurate detection. To overcome these challenges,
we propose an innovative graph-based approach called TMGHOI (Translational
Model for Human-Object Interaction Detection). Our method effectively captures
the sentiment representation of HOIs by integrating both spatial and semantic
knowledge. By representing HOIs as a graph, where the interaction components
serve as nodes and their spatial relationships as edges. To extract crucial
spatial and semantic information, TMGHOI employs separate spatial and semantic
encoders. Subsequently, these encodings are combined to construct a knowledge
graph that effectively captures the sentiment representation of HOIs.
Additionally, the ability to incorporate prior knowledge enhances the
understanding of interactions, further boosting detection accuracy. We
conducted extensive evaluations on the widely-used HICO-DET datasets to
demonstrate the effectiveness of TMGHOI. Our approach outperformed existing
state-of-the-art graph-based methods by a significant margin, showcasing its
potential as a superior solution for HOI detection. We are confident that
TMGHOI has the potential to significantly improve the accuracy and efficiency
of HOI detection. Its integration of spatial and semantic knowledge, along with
its computational efficiency and practicality, makes it a valuable tool for
researchers and practitioners in the computer vision community. As with any
research, we acknowledge the importance of further exploration and evaluation
on various datasets to establish the generalizability and robustness of our
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lijing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Q/0/1/0/all/0/1&quot;&gt;Qizhen Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1&quot;&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Houbing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamal_A/0/1/0/all/0/1&quot;&gt;Acharya Kamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qing Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1&quot;&gt;Shuteng Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07399">
<title>RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose. (arXiv:2303.07399v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07399</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on 2D pose estimation have achieved excellent performance on
public benchmarks, yet its application in the industrial community still
suffers from heavy model parameters and high latency. In order to bridge this
gap, we empirically explore key factors in pose estimation including paradigm,
model architecture, training strategy, and deployment, and present a
high-performance real-time multi-person pose estimation framework, RTMPose,
based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on an
Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-l
achieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluate
RTMPose&apos;s capability in critical real-time applications, we also report the
performance after deploying on the mobile device. Our RTMPose-s achieves 72.2%
AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing
open-source libraries. Code and models are released at
https://github.com/open-mmlab/mmpose/tree/1.x/projects/rtmpose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Peng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Ningsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1&quot;&gt;Rui Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1&quot;&gt;Chengqi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yining Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08611">
<title>Improving Fast Auto-Focus with Event Polarity. (arXiv:2303.08611v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08611</link>
<description rdf:parseType="Literal">&lt;p&gt;Fast and accurate auto-focus in adverse conditions remains an arduous task.
The emergence of event cameras has opened up new possibilities for addressing
the challenge. This paper presents a new high-speed and accurate event-based
focusing algorithm. Specifically, the symmetrical relationship between the
event polarities in focusing is investigated, and the event-based focus
evaluation function is proposed based on the principles of the event cameras
and the imaging model in the focusing process. Comprehensive experiments on the
public event-based autofocus dataset (EAD) show the robustness of the model.
Furthermore, precise focus with less than one depth of focus is achieved within
0.004 seconds on our self-built high-speed focusing platform. The dataset and
code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yuhan Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuqin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_D/0/1/0/all/0/1&quot;&gt;Diyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09064">
<title>Rethinking the U-Net, ResUnet, and U-Net3+ architectures with dual skip connections for building footprint extraction. (arXiv:2303.09064v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09064</link>
<description rdf:parseType="Literal">&lt;p&gt;The importance of building footprints and their inventory has been recognised
as foundational spatial information for multiple societal problems. Extracting
complex urban buildings involves the segmentation of very high-resolution (VHR)
earth observation (EO) images. U-Net is a common deep learning network and
foundation for its new incarnations like ResUnet, U-Net++ and U-Net3+ for such
segmentation. The re-incarnations look for efficiency gain by re-designing the
skip connection component and exploiting the multi-scale features in U-Net.
However, skip connections do not always improve these networks and removing
some of them provides efficiency gains and reduced network parameters. In this
paper, we propose three dual skip connection mechanisms for U-Net, ResUnet, and
U-Net3+. These mechanisms deepen the feature maps forwarded by the skip
connections and allow us to study which skip connections need to be denser to
yield the highest efficiency gain. The mechanisms are evaluated on feature maps
of different scales in the three networks, producing nine new network
configurations. The networks are evaluated against their original vanilla
versions using four building footprint datasets (three existing and one new) of
different spatial resolutions: VHR (0.3m), high-resolution (1m and 1.2m), and
multi-resolution (0.3+0.6+1.2m). The proposed mechanisms report efficiency gain
on four evaluation measures for U-Net and ResUnet, and up to 17.7% and 18.4%
gain in F1 score and Intersection over Union (IoU) for U-Net3+. The codes will
be available in a GitHub link after peer review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_B/0/1/0/all/0/1&quot;&gt;Bipul Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aryal_J/0/1/0/all/0/1&quot;&gt;Jagannath Aryal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabifard_A/0/1/0/all/0/1&quot;&gt;Abbas Rajabifard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11305">
<title>SVDiff: Compact Parameter Space for Diffusion Fine-Tuning. (arXiv:2303.11305v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11305</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved remarkable success in text-to-image
generation, enabling the creation of high-quality images from text prompts or
other modalities. However, existing methods for customizing these models are
limited by handling multiple personalized subjects and the risk of overfitting.
Moreover, their large number of parameters is inefficient for model storage. In
this paper, we propose a novel approach to address these limitations in
existing text-to-image diffusion models for personalization. Our method
involves fine-tuning the singular values of the weight matrices, leading to a
compact and efficient parameter space that reduces the risk of overfitting and
language drifting. We also propose a Cut-Mix-Unmix data-augmentation technique
to enhance the quality of multi-subject image generation and a simple
text-based image editing framework. Our proposed SVDiff method has a
significantly smaller model size compared to existing methods (approximately
2,200 times fewer parameters compared with vanilla DreamBooth), making it more
practical for real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Ligong Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Han Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1&quot;&gt;Peyman Milanfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15274">
<title>Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention. (arXiv:2303.15274v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15274</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting human gaze is important in Human-Computer Interaction (HCI).
However, to practically serve HCI applications, gaze prediction models must be
scalable, fast, and accurate in their spatial and temporal gaze predictions.
Recent scanpath prediction models focus on goal-directed attention (search).
Such models are limited in their application due to a common approach relying
on trained target detectors for all possible objects, and the availability of
human gaze data for their training (both not scalable). In response, we pose a
new task called ZeroGaze, a new variant of zero-shot learning where gaze is
predicted for never-before-searched objects, and we develop a novel model,
Gazeformer, to solve the ZeroGaze problem. In contrast to existing methods
using object detector modules, Gazeformer encodes the target using a natural
language model, thus leveraging semantic similarities in scanpath prediction.
We use a transformer-based encoder-decoder architecture because transformers
are particularly useful for generating contextual representations. Gazeformer
surpasses other models by a large margin on the ZeroGaze setting. It also
outperforms existing target-detection models on standard gaze prediction for
both target-present and target-absent search tasks. In addition to its improved
performance, Gazeformer is more than five times faster than the
state-of-the-art target-present visual search model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1&quot;&gt;Sounak Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Seoyoung Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1&quot;&gt;Dimitris Samaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zelinsky_G/0/1/0/all/0/1&quot;&gt;Gregory Zelinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1&quot;&gt;Minh Hoai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15485">
<title>Transfer-Once-For-All: AI Model Optimization for Edge. (arXiv:2303.15485v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15485</link>
<description rdf:parseType="Literal">&lt;p&gt;Weight-sharing neural architecture search aims to optimize a configurable
neural network model (supernet) for a variety of deployment scenarios across
many devices with different resource constraints. Existing approaches use
evolutionary search to extract models of different sizes from a supernet
trained on a very large data set, and then fine-tune the extracted models on
the typically small, real-world data set of interest. The computational cost of
training thus grows linearly with the number of different model deployment
scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style
training on small data sets with constant computational training cost over any
number of edge deployment scenarios. Given a task, TOFA obtains custom neural
networks, both the topology and the weights, optimized for any number of edge
deployment scenarios. To overcome the challenges arising from small data, TOFA
utilizes a unified semi-supervised training loss to simultaneously train all
subnets within the supernet, coupled with on-the-fly architecture selection at
deployment time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1&quot;&gt;Achintya Kundu&lt;/a&gt; (IBM Research), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wynter_L/0/1/0/all/0/1&quot;&gt;Laura Wynter&lt;/a&gt; (IBM Research), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Rhui Dih Lee&lt;/a&gt; (IBM Research), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bathen_L/0/1/0/all/0/1&quot;&gt;Luis Angel Bathen&lt;/a&gt; (IBM Research)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00429">
<title>Information Recovery-Driven Deep Incomplete Multiview Clustering Network. (arXiv:2304.00429v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00429</link>
<description rdf:parseType="Literal">&lt;p&gt;Incomplete multi-view clustering is a hot and emerging topic. It is well
known that unavoidable data incompleteness greatly weakens the effective
information of multi-view data. To date, existing incomplete multi-view
clustering methods usually bypass unavailable views according to prior missing
information, which is considered as a second-best scheme based on evasion.
Other methods that attempt to recover missing information are mostly applicable
to specific two-view datasets. To handle these problems, in this paper, we
propose an information recovery-driven deep incomplete multi-view clustering
network, termed as RecFormer. Concretely, a two-stage autoencoder network with
the self-attention structure is built to synchronously extract high-level
semantic representations of multiple views and recover the missing data.
Besides, we develop a recurrent graph reconstruction mechanism that cleverly
leverages the restored views to promote the representation learning and the
further data reconstruction. Visualization of recovery results are given and
sufficient experimental results confirm that our RecFormer has obvious
advantages over other top methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaoling Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00962">
<title>RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00962</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing 3D scene understanding tasks have achieved high performance on
close-set benchmarks but fail to handle novel categories in real-world
applications. To this end, we propose a Regional Point-Language Contrastive
learning framework, namely RegionPLC, for open-world 3D scene understanding,
which equips models trained on closed-set datasets with open-vocabulary
recognition capabilities. We propose dense visual prompts to elicit
region-level visual-language knowledge from 2D foundation models via
captioning, which further allows us to build dense regional point-language
associations. Then, we design a point-discriminative contrastive learning
objective to enable point-independent learning from captions for dense scene
understanding. We conduct extensive experiments on ScanNet, ScanNet200, and
nuScenes datasets. Our RegionPLC significantly outperforms previous
base-annotated 3D open-world scene understanding approaches by an average of
11.6\% and 6.6\% for semantic and instance segmentation, respectively. It also
shows promising open-world results in absence of any human annotation with low
training and inference costs. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Runyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05538">
<title>ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification. (arXiv:2304.05538v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05538</link>
<description rdf:parseType="Literal">&lt;p&gt;Image classifiers are information-discarding machines, by design. Yet, how
these models discard information remains mysterious. We hypothesize that one
way for image classifiers to reach high accuracy is to first zoom to the most
discriminative region in the image and then extract features from there to
predict image labels, discarding the rest of the image. Studying six popular
networks ranging from AlexNet to CLIP, we find that proper framing of the input
image can lead to the correct classification of 98.91% of ImageNet images.
Furthermore, we uncover positional biases in various datasets, especially a
strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally,
leveraging our insights into the potential of zooming, we propose a test-time
augmentation (TTA) technique that improves classification accuracy by forcing
models to explicitly perform zoom-in operations before making predictions. Our
method is more interpretable, accurate, and faster than MEMO, a
state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark
that challenges SOTA classifiers including large vision-language models even
when optimal zooming is allowed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Taesiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1&quot;&gt;Giang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habchi_S/0/1/0/all/0/1&quot;&gt;Sarra Habchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezemer_C/0/1/0/all/0/1&quot;&gt;Cor-Paul Bezemer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05841">
<title>Exploring Diffusion Models for Unsupervised Video Anomaly Detection. (arXiv:2304.05841v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05841</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the performance of diffusion models for video anomaly
detection (VAD) within the most challenging but also the most operational
scenario in which the data annotations are not used. As being sparse, diverse,
contextual, and often ambiguous, detecting abnormal events precisely is a very
ambitious task. To this end, we rely only on the information-rich
spatio-temporal data, and the reconstruction power of the diffusion models such
that a high reconstruction error is utilized to decide the abnormality.
Experiments performed on two large-scale video anomaly detection datasets
demonstrate the consistent improvement of the proposed method over the
state-of-the-art generative models while in some cases our method achieves
better scores than the more complex models. This is the first study using a
diffusion model and examining its parameters&apos; influence to present guidance for
VAD in surveillance scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tur_A/0/1/0/all/0/1&quot;&gt;Anil Osman Tur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1&quot;&gt;Nicola Dall&amp;#x27;Asen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1&quot;&gt;Cigdem Beyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06502">
<title>Variations of Squeeze and Excitation networks. (arXiv:2304.06502v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06502</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks learns spatial features and are heavily
interlinked within kernels. The SE module have broken the traditional route of
neural networks passing the entire result to next layer. Instead SE only passes
important features to be learned with its squeeze and excitation (SE) module.
We propose variations of the SE module which improvises the process of squeeze
and excitation and enhances the performance. The proposed squeezing or exciting
the layer makes it possible for having a smooth transition of layer weights.
These proposed variations also retain the characteristics of SE module. The
experimented results are carried out on residual networks and the results are
tabulated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+NV_M/0/1/0/all/0/1&quot;&gt;Mahendran NV&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06968">
<title>Domain shifts in dermoscopic skin cancer datasets: Evaluation of essential limitations for clinical translation. (arXiv:2304.06968v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06968</link>
<description rdf:parseType="Literal">&lt;p&gt;The limited ability of Convolutional Neural Networks to generalize to images
from previously unseen domains is a major limitation, in particular, for
safety-critical clinical tasks such as dermoscopic skin cancer classification.
In order to translate CNN-based applications into the clinic, it is essential
that they are able to adapt to domain shifts. Such new conditions can arise
through the use of different image acquisition systems or varying lighting
conditions. In dermoscopy, shifts can also occur as a change in patient age or
occurence of rare lesion localizations (e.g. palms). These are not prominently
represented in most training datasets and can therefore lead to a decrease in
performance. In order to verify the generalizability of classification models
in real world clinical settings it is crucial to have access to data which
mimics such domain shifts. To our knowledge no dermoscopic image dataset exists
where such domain shifts are properly described and quantified. We therefore
grouped publicly available images from ISIC archive based on their metadata
(e.g. acquisition location, lesion localization, patient age) to generate
meaningful domains. To verify that these domains are in fact distinct, we used
multiple quantification measures to estimate the presence and intensity of
domain shifts. Additionally, we analyzed the performance on these domains with
and without an unsupervised domain adaptation technique. We observed that in
most of our grouped domains, domain shifts in fact exist. Based on our results,
we believe these datasets to be helpful for testing the generalization
capabilities of dermoscopic skin cancer classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fogelberg_K/0/1/0/all/0/1&quot;&gt;Katharina Fogelberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamarthi_S/0/1/0/all/0/1&quot;&gt;Sireesha Chamarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maron_R/0/1/0/all/0/1&quot;&gt;Roman C. Maron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niebling_J/0/1/0/all/0/1&quot;&gt;Julia Niebling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinker_T/0/1/0/all/0/1&quot;&gt;Titus J. Brinker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08364">
<title>Transformer with Selective Shuffled Position Embedding and Key-Patch Exchange Strategy for Early Detection of Knee Osteoarthritis. (arXiv:2304.08364v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08364</link>
<description rdf:parseType="Literal">&lt;p&gt;Knee OsteoArthritis (KOA) is a widespread musculoskeletal disorder that can
severely impact the mobility of older individuals. Insufficient medical data
presents a significant obstacle for effectively training models due to the high
cost associated with data labelling. Currently, deep learning-based models
extensively utilize data augmentation techniques to improve their
generalization ability and alleviate overfitting. However, conventional data
augmentation techniques are primarily based on the original data and fail to
introduce substantial diversity to the dataset. In this paper, we propose a
novel approach based on the Vision Transformer (ViT) model with original
Selective Shuffled Position Embedding (SSPE) and key-patch exchange strategies
to obtain different input sequences as a method of data augmentation for early
detection of KOA (KL-0 vs KL-2). More specifically, we fix and shuffle the
position embedding of key and non-key patches, respectively. Then, for the
target image, we randomly select other candidate images from the training set
to exchange their key patches and thus obtain different input sequences.
Finally, a hybrid loss function is developed by incorporating multiple loss
functions for different types of the sequences. According to the experimental
results, the generated data are considered valid as they lead to a notable
improvement in the model&apos;s classification performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chetouani_A/0/1/0/all/0/1&quot;&gt;Aladine Chetouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jarraya_M/0/1/0/all/0/1&quot;&gt;Mohamed Jarraya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hans_D/0/1/0/all/0/1&quot;&gt;Didier Hans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jennane_R/0/1/0/all/0/1&quot;&gt;Rachid Jennane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09914">
<title>The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09914</link>
<description rdf:parseType="Literal">&lt;p&gt;Online media has revolutionized the way political information is disseminated
and consumed on a global scale, and this shift has compelled political figures
to adopt new strategies of capturing and retaining voter attention. These
strategies often rely on emotional persuasion and appeal, and as visual content
becomes increasingly prevalent in virtual space, much of political
communication too has come to be marked by evocative video content and imagery.
The present paper offers a novel approach to analyzing material of this kind.
We apply a deep-learning-based computer-vision algorithm to a sample of 220
YouTube videos depicting political leaders from 15 different countries, which
is based on an existing trained convolutional neural network architecture
provided by the Python library fer. The algorithm returns emotion scores
representing the relative presence of 6 emotional states (anger, disgust, fear,
happiness, sadness, and surprise) and a neutral expression for each frame of
the processed YouTube video. We observe statistically significant differences
in the average score of expressed negative emotions between groups of leaders
with varying degrees of populist rhetoric as defined by the Global Party Survey
(GPS), indicating that populist leaders tend to express negative emotions to a
greater extent during their public performance than their non-populist
counterparts. Overall, our contribution provides insight into the
characteristics of visual self-representation among political leaders, as well
as an open-source workflow for further computational studies of their
non-verbal communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Major_S/0/1/0/all/0/1&quot;&gt;Sara Major&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasevic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Toma&amp;#x161;evi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11234">
<title>Advances in Deep Concealed Scene Understanding. (arXiv:2304.11234v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11234</link>
<description rdf:parseType="Literal">&lt;p&gt;Concealed scene understanding (CSU) is a hot computer vision topic aiming to
perceive objects exhibiting camouflage. The current boom in terms of techniques
and applications warrants an up-to-date survey. This can help researchers to
better understand the global CSU field, including both current achievements and
remaining challenges. This paper makes four contributions: (1) For the first
time, we present a comprehensive survey of deep learning techniques aimed at
CSU, including a taxonomy, task-specific challenges, and ongoing developments.
(2) To allow for an authoritative quantification of the state-of-the-art, we
offer the largest and latest benchmark for concealed object segmentation (COS).
(3) To evaluate the generalizability of deep CSU in practical scenarios, we
collect the largest concealed defect segmentation dataset termed CDS2K with the
hard cases from diversified industrial scenarios, on which we construct a
comprehensive benchmark. (4) We discuss open problems and potential research
directions for CSU. Our code and datasets are available at
https://github.com/DengPingFan/CSU, which will be updated continuously to watch
and summarize the advancements in this rapidly evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1&quot;&gt;Christos Sakaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12235">
<title>Multi-crop Contrastive Learning and Domain Consistency for Unsupervised Image-to-Image Translation. (arXiv:2304.12235v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12235</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, unsupervised image-to-image translation methods based on
contrastive learning have achieved state-of-the-art results in many tasks.
However, in the previous work, the negatives are sampled from the input image
itself, which inspires us to design a data augmentation method to improve the
quality of the selected negatives. Moreover, retaining the content similarity
via patch-wise contrastive learning in the embedding space, the previous
methods ignore the domain consistency between the generated image and the real
images of target domain. In this paper, we propose a novel unsupervised
image-to-image translation framework based on multi-crop contrastive learning
and domain consistency, called MCDUT. Specifically, we obtain the multi-crop
views via the center-crop and the random-crop to generate the negatives, which
can increase the quality of the negatives. To constrain the embeddings in the
deep feature space, we formulate a new domain consistency loss, which
encourages the generated images to be close to the real images in the embedding
space of same domain. Furthermore, we present a dual coordinate attention
network by embedding positional information into channel attention, which
called DCA. We employ the DCA network in the design of generator, which makes
the generator capture the horizontal and vertical global information of
dependency. In many image-to-image translation tasks, our method achieves
state-of-the-art results, and the advantages of our method have been proven
through extensive comparison experiments and ablation research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wei-Ling Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Cheng-Wei Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12944">
<title>Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12944</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the significant recent progress in deep generative models, the
underlying structure of their latent spaces is still poorly understood, thereby
making the task of performing semantically meaningful latent traversals an open
research challenge. Most prior work has aimed to solve this challenge by
modeling latent structures linearly, and finding corresponding linear
directions which result in `disentangled&apos; generations. In this work, we instead
propose to model latent structures with a learned dynamic potential landscape,
thereby performing latent traversals as the flow of samples down the
landscape&apos;s gradient. Inspired by physics, optimal transport, and neuroscience,
these potential landscapes are learned as physically realistic partial
differential equations, thereby allowing them to flexibly vary over both space
and time. To achieve disentanglement, multiple potentials are learned
simultaneously, and are constrained by a classifier to be distinct and
semantically self-consistent. Experimentally, we demonstrate that our method
achieves both more qualitatively and quantitatively disentangled trajectories
than state-of-the-art baselines. Further, we demonstrate that our method can be
integrated as a regularization term during training, thereby acting as an
inductive bias towards the learning of structured representations, ultimately
improving model likelihood on similarly structured data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yue Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1&quot;&gt;T. Anderson Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14005">
<title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding. (arXiv:2304.14005v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14005</link>
<description rdf:parseType="Literal">&lt;p&gt;Although 3D-aware GANs based on neural radiance fields have achieved
competitive performance, their applicability is still limited to objects or
scenes with the ground-truths or prediction models for clearly defined
canonical camera poses. To extend the scope of applicable datasets, we propose
a novel 3D-aware GAN optimization technique through contrastive learning with
implicit pose embeddings. To this end, we first revise the discriminator design
and remove dependency on ground-truth camera poses. Then, to capture complex
and challenging 3D scene structures more effectively, we make the discriminator
estimate a high-dimensional implicit pose embedding from a given image and
perform contrastive learning on the pose embedding. The proposed approach can
be employed for the dataset, where the canonical camera pose is ill-defined
because it does not look up or estimate camera poses. Experimental results show
that our algorithm outperforms existing methods by large margins on the
datasets with multiple object categories and inconsistent canonical camera
poses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mijeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunjoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00194">
<title>Searching from Area to Point: A Hierarchical Framework for Semantic-Geometric Combined Feature Matching. (arXiv:2305.00194v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00194</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature matching is a crucial technique in computer vision. A unified
perspective for this task is to treat it as a searching problem, aiming at an
efficient search strategy to narrow the search space to point matches between
images. One of the key aspects of search strategy is the search space, which in
current approaches is not carefully defined, resulting in limited matching
accuracy. This paper, thus, pays attention to the search space and proposes to
set the initial search space for point matching as the matched image areas
containing prominent semantic, named semantic area matches. This search space
favors point matching by salient features and alleviates the accuracy
limitation in recent Transformer-based matching methods. To achieve this search
space, we introduce a hierarchical feature matching framework: Area to Point
Matching (A2PM), to first find semantic area matches between images and later
perform point matching on area matches. We further propose Semantic and
Geometry Area Matching (SGAM) method to realize this framework, which utilizes
semantic prior and geometry consistency to establish accurate area matches
between images. By integrating SGAM with off-the-shelf state-of-the-art
matchers, our method, adopting the A2PM framework, achieves encouraging
precision improvements in massive point matching and pose estimation
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yesheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1&quot;&gt;Dahong Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07015">
<title>Exploiting Diffusion Prior for Real-World Image Super-Resolution. (arXiv:2305.07015v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07015</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
introduce a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1&quot;&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shangchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1&quot;&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07270">
<title>SSD-MonoDETR: Supervised Scale-aware Deformable Transformer for Monocular 3D Object Detection. (arXiv:2305.07270v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07270</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based methods have demonstrated superior performance for
monocular 3D object detection recently, which aims at predicting 3D attributes
from a single 2D image. Most existing transformer-based methods leverage both
visual and depth representations to explore valuable query points on objects,
and the quality of the learned query points has a great impact on detection
accuracy. Unfortunately, existing unsupervised attention mechanisms in
transformers are prone to generate low-quality query features due to inaccurate
receptive fields, especially on hard objects. To tackle this problem, this
paper proposes a novel Supervised Scale-aware Deformable Attention (SSDA) for
monocular 3D object detection. Specifically, SSDA presets several masks with
different scales and utilizes depth and visual features to adaptively learn a
scale-aware filter for object query augmentation. Imposing the scale awareness,
SSDA could well predict the accurate receptive field of an object query to
support robust query feature generation. Aside from this, SSDA is assigned with
a Weighted Scale Matching (WSM) loss to supervise scale prediction, which
presents more confident results as compared to the unsupervised attention
mechanisms. Extensive experiments on the KITTI benchmark demonstrate that SSDA
significantly improves the detection accuracy, especially on moderate and hard
objects, yielding state-of-the-art performance as compared to the existing
approaches. Our code will be made publicly available at
https://github.com/mikasa3lili/SSD-MonoDETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiacheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haolong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10029">
<title>TextSLAM: Visual SLAM with Semantic Planar Text Features. (arXiv:2305.10029v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10029</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel visual SLAM method that integrates text objects tightly by
treating them as semantic features via fully exploring their geometric and
semantic prior. The text object is modeled as a texture-rich planar patch whose
semantic meaning is extracted and updated on the fly for better data
association. With the full exploration of locally planar characteristics and
semantic meaning of text objects, the SLAM system becomes more accurate and
robust even under challenging conditions such as image blurring, large
viewpoint changes, and significant illumination variations (day and night). We
tested our method in various scenes with the ground truth data. The results
show that integrating texture features leads to a more superior SLAM system
that can match images across day and night. The reconstructed semantic 3D text
map could be useful for navigation and scene understanding in robotic and mixed
reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1&quot;&gt;Danping Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1&quot;&gt;Xinghan Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_L/0/1/0/all/0/1&quot;&gt;Ling Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenxian Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10210">
<title>Object Re-Identification from Point Clouds. (arXiv:2305.10210v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10210</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the problem of object re-identification (ReID) in a 3D
multi-object tracking (MOT) context, by learning to match pairs of objects from
cropped (e.g., using their predicted 3D bounding boxes) point cloud
observations. We are not concerned with SOTA performance for 3D MOT, however.
Instead, we seek to answer the following question: In a realistic tracking
by-detection context, how does object ReID from point clouds perform relative
to ReID from images? To enable such a study, we propose a lightweight matching
head that can be concatenated to any set or sequence processing backbone (e.g.,
PointNet or ViT), creating a family of comparable object ReID networks for both
modalities. Run in siamese style, our proposed point-cloud ReID networks can
make thousands of pairwise comparisons in real-time (10 hz). Our findings
demonstrate that their performance increases with higher sensor resolution and
approaches that of image ReID when observations are sufficiently dense.
Additionally, we investigate our network&apos;s ability to enhance 3D multi-object
tracking (MOT), showing that our point-cloud ReID networks can successfully
re-identify objects which led a strong motion-based tracker into error. To our
knowledge, we are the first to study real-time object re-identification from
point clouds in a 3D multi-object tracking context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Therien_B/0/1/0/all/0/1&quot;&gt;Benjamin Th&amp;#xe9;rien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengjie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_A/0/1/0/all/0/1&quot;&gt;Adrian Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1&quot;&gt;Krzysztof Czarnecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13093">
<title>Restore Anything Pipeline: Segment Anything Meets Image Restoration. (arXiv:2305.13093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13093</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent image restoration methods have produced significant advancements using
deep learning. However, existing methods tend to treat the whole image as a
single entity, failing to account for the distinct objects in the image that
exhibit individual texture properties. Existing methods also typically generate
a single result, which may not suit the preferences of different users. In this
paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive
and per-object level image restoration approach that incorporates a
controllable model to generate different results that users may choose from.
RAP incorporates image segmentation through the recent Segment Anything Model
(SAM) into a controllable image restoration model to create a user-friendly
pipeline for several image restoration tasks. We demonstrate the versatility of
RAP by applying it to three common image restoration tasks: image deblurring,
image denoising, and JPEG artifact removal. Our experiments show that RAP
produces superior visual results compared to state-of-the-art methods. RAP
represents a promising direction for image restoration, providing users with
greater control, and enabling image restoration at an object level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiaxi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holz_C/0/1/0/all/0/1&quot;&gt;Christian Holz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15957">
<title>DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification. (arXiv:2305.15957v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15957</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained models have had a significant impact on computer vision by
enabling multi-modal learning, where the CLIP model has achieved impressive
results in image classification, object detection, and semantic segmentation.
However, the model&apos;s performance on 3D point cloud processing tasks is limited
due to the domain gap between depth maps from 3D projection and training images
of CLIP. This paper proposes DiffCLIP, a new pre-training framework that
incorporates stable diffusion with ControlNet to minimize the domain gap in the
visual branch. Additionally, a style-prompt generation module is introduced for
few-shot tasks in the textual branch. Extensive experiments on the ModelNet10,
ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilities
for 3D understanding. By using stable diffusion and style-prompt generation,
DiffCLIP achieves an accuracy of 43.2\% for zero-shot classification on OBJ\_BG
of ScanObjectNN, which is state-of-the-art performance, and an accuracy of
80.6\% for zero-shot classification on ModelNet10, which is comparable to
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sitian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zilin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Linqian Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Harry Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinxiao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16555">
<title>CVB: A Video Dataset of Cattle Visual Behaviors. (arXiv:2305.16555v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16555</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing image/video datasets for cattle behavior recognition are mostly
small, lack well-defined labels, or are collected in unrealistic controlled
environments. This limits the utility of machine learning (ML) models learned
from them. Therefore, we introduce a new dataset, called Cattle Visual
Behaviors (CVB), that consists of 502 video clips, each fifteen seconds long,
captured in natural lighting conditions, and annotated with eleven visually
perceptible behaviors of grazing cattle. We use the Computer Vision Annotation
Tool (CVAT) to collect our annotations. To make the procedure more efficient,
we perform an initial detection and tracking of cattle in the videos using
appropriate pre-trained models. The results are corrected by domain experts
along with cattle behavior labeling in CVAT. The pre-hoc detection and tracking
step significantly reduces the manual annotation time and effort. Moreover, we
convert CVB to the atomic visual action (AVA) format and train and evaluate the
popular SlowFast action recognition model on it. The associated preliminary
results confirm that we can localize the cattle and recognize their frequently
occurring behaviors with confidence. By creating and sharing CVB, our aim is to
develop improved models capable of recognizing all important behaviors
accurately and to assist other researchers and practitioners in developing and
evaluating new ML models for cattle behavior classification using video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zia_A/0/1/0/all/0/1&quot;&gt;Ali Zia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Renuka Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arablouei_R/0/1/0/all/0/1&quot;&gt;Reza Arablouei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bishop_Hurley_G/0/1/0/all/0/1&quot;&gt;Greg Bishop-Hurley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McNally_J/0/1/0/all/0/1&quot;&gt;Jody McNally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagnall_N/0/1/0/all/0/1&quot;&gt;Neil Bagnall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolland_V/0/1/0/all/0/1&quot;&gt;Vivien Rolland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusy_B/0/1/0/all/0/1&quot;&gt;Brano Kusy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingham_A/0/1/0/all/0/1&quot;&gt;Aaron Ingham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18326">
<title>BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18326</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a large-scale video subtitle translation dataset, BigVideo, to
facilitate the study of multi-modality machine translation. Compared with the
widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger,
consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also
introduce two deliberately designed test sets to verify the necessity of visual
information: Ambiguous with the presence of ambiguous words, and Unambiguous in
which the text context is self-contained for translation. To better model the
common semantics shared across texts and videos, we introduce a contrastive
learning method in the cross-modal encoder. Extensive experiments on the
BigVideo show that: a) Visual information consistently improves the NMT model
in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test
sets. b) Visual information helps disambiguation, compared to the strong text
baseline on terminology-targeted scores and human evaluation. Dataset and our
implementations are available at https://github.com/DeepLearnXMU/BigVideo-VMT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1&quot;&gt;Liyan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Luyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Ningxin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zewei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shanbo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Degen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinsong Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18878">
<title>BPF Algorithms for Multiple Source-Translation Computed Tomography Reconstruction. (arXiv:2305.18878v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18878</link>
<description rdf:parseType="Literal">&lt;p&gt;Micro-computed tomography (micro-CT) is a widely used state-of-the-art
instrument employed to study the morphological structures of objects in various
fields. However, its small field-of-view (FOV) cannot meet the pressing demand
for imaging relatively large objects at high spatial resolutions. Recently, we
devised a novel scanning mode called multiple source translation CT (mSTCT)
that effectively enlarges the FOV of the micro-CT and correspondingly developed
a virtual projection-based filtered backprojection (V-FBP) algorithm for
reconstruction. Although V-FBP skillfully solves the truncation problem in
mSTCT, it requires densely sampled projections to arrive at high-resolution
reconstruction, which reduces imaging efficiency. In this paper, we developed
two backprojection-filtration (BPF)-based algorithms for mSTCT: S-BPF
(derivatives along source) and D-BPF (derivatives along detector). D-BPF can
achieve high-resolution reconstruction with fewer projections than V-FBP and
S-BPF. Through simulated and real experiments conducted in this paper, we
demonstrate that D-BPF can reduce source sampling by 75% compared with V-FBP at
the same spatial resolution, which makes mSTCT more feasible in practice.
Meanwhile, S-BPF can yield more stable results than D-BPF, which is similar to
V-FBP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Wang&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haijun Yu&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yixing Huang&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shunli Wang&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1&quot;&gt;Song Ni&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongfeng Li&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fenglin Liu&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Junning Cui&lt;/a&gt; (1 and 2) ((1) Center of Ultra-Precision Optoelectronic Instrument Engineering, Harbin Institute of Technology, Harbin 150080, China, (2) Key Lab of Ultra-Precision Intelligent Instrumentation (Harbin Institute of Technology), Ministry of Industry and Information Technology, Harbin 150080, China, (3) Key Laboratory of Optoelectronic Technology and Systems, Ministry of Education, Chongqing University, Chongqing 400044, China, (4) Oncology, University Hospital Erlangen, Friedrich-Alexander-University Erlangen-Nuremberg, 91054 Erlangen, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00854">
<title>Spatio-Angular Convolutions for Super-resolution in Diffusion MRI. (arXiv:2306.00854v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00854</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion MRI (dMRI) is a widely used imaging modality, but requires long
scanning times to acquire high resolution datasets. By leveraging the unique
geometry present within this domain, we present a novel approach to dMRI
angular super-resolution that extends upon the parametric continuous
convolution (PCConv) framework. We introduce several additions to the operation
including a Fourier feature mapping, global coordinates, and domain specific
context. Using this framework, we build a fully parametric continuous
convolution network (PCCNN) and compare against existing models. We demonstrate
the PCCNN performs competitively while using significantly less parameters.
Moreover, we show that this formulation generalises well to clinically relevant
downstream analyses such as fixel-based analysis, and neurite orientation
dispersion and density imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyon_M/0/1/0/all/0/1&quot;&gt;Matthew Lyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Armitage_P/0/1/0/all/0/1&quot;&gt;Paul Armitage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alvarez_M/0/1/0/all/0/1&quot;&gt;Mauricio A &amp;#xc1;lvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00942">
<title>Train Offline, Test Online: A Real Robot Learning Benchmark. (arXiv:2306.00942v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00942</link>
<description rdf:parseType="Literal">&lt;p&gt;Three challenges limit the progress of robot learning research: robots are
expensive (few labs can participate), everyone uses different robots (findings
do not generalize across labs), and we lack internet-scale robotics data. We
take on these challenges via a new benchmark: Train Offline, Test Online
(TOTO). TOTO provides remote users with access to shared robotic hardware for
evaluating methods on common tasks and an open-source dataset of these tasks
for offline training. Its manipulation task suite requires challenging
generalization to unseen objects, positions, and lighting. We present initial
results on TOTO comparing five pretrained visual representations and four
offline policy learning baselines, remotely contributed by five institutions.
The real promise of TOTO, however, lies in the future: we release the benchmark
for additional submissions from any user, enabling easy, direct comparison to
several methods without the need to obtain hardware or collect data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Gaoyue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_V/0/1/0/all/0/1&quot;&gt;Victoria Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srirama_M/0/1/0/all/0/1&quot;&gt;Mohan Kumar Srirama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1&quot;&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pari_J/0/1/0/all/0/1&quot;&gt;Jyothish Pari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1&quot;&gt;Kyle Hatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aryan Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1&quot;&gt;Lerrel Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01598">
<title>Towards Source-free Domain Adaptive Semantic Segmentation via Importance-aware and Prototype-contrast Learning. (arXiv:2306.01598v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01598</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. Code will be made publicly available at
https://github.com/yihong-97/Source-free_IAPC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yihong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04527">
<title>ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04527</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization is critical for real-world applications of machine
learning models to microscopy images, including histopathology and fluorescence
imaging. Artifacts in histopathology arise through a complex combination of
factors relating to tissue collection and laboratory processing, as well as
factors intrinsic to patient samples. In fluorescence imaging, these artifacts
stem from variations across experimental batches. The complexity and subtlety
of these artifacts make the enumeration of data domains intractable. Therefore,
augmentation-based methods of domain generalization that require domain
identifiers and manual fine-tuning are inadequate in this setting. To overcome
this challenge, we introduce ContriMix, a domain generalization technique that
learns to generate synthetic images by disentangling and permuting the
biological content (&quot;content&quot;) and technical variations (&quot;attributes&quot;) in
microscopy images. ContriMix does not rely on domain identifiers or handcrafted
augmentations and makes no assumptions about the input characteristics of
images. We assess the performance of ContriMix on two pathology datasets
(Camelyon17-WILDS and a prostate cell classification dataset) and one
fluorescence microscopy dataset (RxRx1-WILDS). ContriMix outperforms current
state-of-the-art methods in all datasets, motivating its usage for microscopy
image analysis in real-world settings where domain information is hard to come
by.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juyal_D/0/1/0/all/0/1&quot;&gt;Dinkar Juyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1&quot;&gt;Aaditya Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nofallah_S/0/1/0/all/0/1&quot;&gt;Shima Nofallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chintan Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gullapally_S/0/1/0/all/0/1&quot;&gt;Sai Chowdary Gullapally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Griffin_M/0/1/0/all/0/1&quot;&gt;Michael Griffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sampat_A/0/1/0/all/0/1&quot;&gt;Anand Sampat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abel_J/0/1/0/all/0/1&quot;&gt;John Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Justin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taylor_Weiner_A/0/1/0/all/0/1&quot;&gt;Amaro Taylor-Weiner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04940">
<title>Layer-level activation mechanism. (arXiv:2306.04940v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04940</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel activation mechanism aimed at establishing
layer-level activation (LayerAct) functions. These functions are designed to be
more noise-robust compared to traditional element-level activation functions by
reducing the layer-level fluctuation of the activation outputs due to shift in
inputs. Moreover, the LayerAct functions achieve a zero-like mean activation
output without restricting the activation output space. We present an analysis
and experiments demonstrating that LayerAct functions exhibit superior
noise-robustness compared to element-level activation functions, and
empirically show that these functions have a zero-like mean activation.
Experimental results on three benchmark image classification tasks show that
LayerAct functions excel in handling noisy image datasets, outperforming
element-level activation functions, while the performance on clean datasets is
also superior in most cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kihyuk Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1&quot;&gt;Chiehyeon Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06211">
<title>A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering. (arXiv:2306.06211v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06211</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment anything model (SAM) developed by Meta AI Research has recently
attracted significant attention. Trained on a large segmentation dataset of
over 1 billion masks, SAM is capable of segmenting any object on a certain
image. In the original SAM work, the authors turned to zero-short transfer
tasks (like edge detection) for evaluating the performance of SAM. Recently,
numerous works have attempted to investigate the performance of SAM in various
scenarios to recognize and segment objects. Moreover, numerous projects have
emerged to show the versatility of SAM as a foundation model by combining it
with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With
the relevant papers and projects increasing exponentially, it is challenging
for the readers to catch up with the development of SAM. To this end, this work
conducts the first yet comprehensive survey on SAM. This is an ongoing project
and we intend to update the manuscript on a regular basis. Therefore, readers
are welcome to contact us if they complete new works related to SAM so that we
can include them in our next version.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puspitasari_F/0/1/0/all/0/1&quot;&gt;Fachrina Dewi Puspitasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1&quot;&gt;Taegoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1&quot;&gt;Xinru Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenshuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Caiyan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1&quot;&gt;Francois Rameau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1&quot;&gt;Lik-Hang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Sung-Ho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07743">
<title>V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07743</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the successes of recent developments in visual AI, different
shortcomings still exist; from missing exact logical reasoning, to abstract
generalization abilities, to understanding complex and noisy scenes.
Unfortunately, existing benchmarks, were not designed to capture more than a
few of these aspects. Whereas deep learning datasets focus on visually complex
data but simple visual reasoning tasks, inductive logic datasets involve
complex logical learning tasks, however, lack the visual component. To address
this, we propose the visual logical learning dataset, V-LoL, that seamlessly
combines visual and logical challenges. Notably, we introduce the first
instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic
benchmark in symbolic AI, the Michalski train problem. By incorporating
intricate visual scenes and flexible logical reasoning tasks within a versatile
framework, V-LoL-Trains provides a platform for investigating a wide range of
visual logical learning challenges. We evaluate a variety of AI systems
including traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our
evaluations demonstrate that even state-of-the-art AI faces difficulties in
dealing with visual logical learning challenges, highlighting unique advantages
and limitations specific to each methodology. Overall, V-LoL opens up new
avenues for understanding and enhancing current abilities in visual logical
learning for AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helff_L/0/1/0/all/0/1&quot;&gt;Lukas Helff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1&quot;&gt;Wolfgang Stammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1&quot;&gt;Hikaru Shindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Dhami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07874">
<title>Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07874</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation aims to mitigate distribution shifts among different
domains. However, traditional formulations are mostly limited to categorical
domains, greatly simplifying nuanced domain relationships in the real world. In
this work, we tackle a generalization with taxonomy-structured domains, which
formalizes domains with nested, hierarchical similarity structures such as
animal species and product catalogs. We build on the classic adversarial
framework and introduce a novel taxonomist, which competes with the adversarial
discriminator to preserve the taxonomy information. The equilibrium recovers
the classic adversarial domain adaptation&apos;s solution if given a non-informative
domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root
node) while yielding non-trivial results with other taxonomies. Empirically,
our method achieves state-of-the-art performance on both synthetic and
real-world datasets with successful adaptation. Code is available at
https://github.com/Wang-ML-Lab/TSDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zihao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1&quot;&gt;Guang-Yuan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Guang-He Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07980">
<title>Dark Web Activity Classification Using Deep Learning. (arXiv:2306.07980v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07980</link>
<description rdf:parseType="Literal">&lt;p&gt;In contemporary times, people rely heavily on the internet and search engines
to obtain information, either directly or indirectly. However, the information
accessible to users constitutes merely 4% of the overall information present on
the internet, which is commonly known as the surface web. The remaining
information that eludes search engines is called the deep web. The deep web
encompasses deliberately hidden information, such as personal email accounts,
social media accounts, online banking accounts, and other confidential data.
The deep web contains several critical applications, including databases of
universities, banks, and civil records, which are off-limits and illegal to
access. The dark web is a subset of the deep web that provides an ideal
platform for criminals and smugglers to engage in illicit activities, such as
drug trafficking, weapon smuggling, selling stolen bank cards, and money
laundering. In this article, we propose a search engine that employs deep
learning to detect the titles of activities on the dark web. We focus on five
categories of activities, including drug trading, weapon trading, selling
stolen bank cards, selling fake IDs, and selling illegal currencies. Our aim is
to extract relevant images from websites with a &quot;.onion&quot; extension and identify
the titles of websites without images by extracting keywords from the text of
the pages. Furthermore, we introduce a dataset of images called Darkoob, which
we have gathered and used to evaluate our proposed method. Our experimental
results demonstrate that the proposed method achieves an accuracy rate of 94%
on the test dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayzi_A/0/1/0/all/0/1&quot;&gt;Ali Fayzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayzi_M/0/1/0/all/0/1&quot;&gt;Mohammad Fayzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_K/0/1/0/all/0/1&quot;&gt;Kourosh Dadashtabar Ahmadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08422">
<title>X-Detect: Explainable Adversarial Patch Detection for Object Detectors in Retail. (arXiv:2306.08422v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08422</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection models, which are widely used in various domains (such as
retail), have been shown to be vulnerable to adversarial attacks. Existing
methods for detecting adversarial attacks on object detectors have had
difficulty detecting new real-life attacks. We present X-Detect, a novel
adversarial patch detector that can: i) detect adversarial samples in real
time, allowing the defender to take preventive action; ii) provide explanations
for the alerts raised to support the defender&apos;s decision-making process, and
iii) handle unfamiliar threats in the form of new attacks. Given a new scene,
X-Detect uses an ensemble of explainable-by-design detectors that utilize
object extraction, scene manipulation, and feature transformation techniques to
determine whether an alert needs to be raised. X-Detect was evaluated in both
the physical and digital space using five different attack scenarios (including
adaptive attacks) and the COCO dataset and our new Superstore dataset. The
physical evaluation was performed using a smart shopping cart setup in
real-world settings and included 17 adversarial patch attacks recorded in 1,700
adversarial videos. The results showed that X-Detect outperforms the
state-of-the-art methods in distinguishing between benign and adversarial
scenes for all attack scenarios while maintaining a 0% FPR (no false alarms)
and providing actionable explanations for the alerts raised. A demo is
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofman_O/0/1/0/all/0/1&quot;&gt;Omer Hofman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giloni_A/0/1/0/all/0/1&quot;&gt;Amit Giloni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayun_Y/0/1/0/all/0/1&quot;&gt;Yarin Hayun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morikawa_I/0/1/0/all/0/1&quot;&gt;Ikuya Morikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimizu_T/0/1/0/all/0/1&quot;&gt;Toshiya Shimizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1&quot;&gt;Asaf Shabtai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08832">
<title>Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding. (arXiv:2306.08832v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08832</link>
<description rdf:parseType="Literal">&lt;p&gt;Current Vision and Language Models (VLMs) demonstrate strong performance
across various vision-language tasks, yet they struggle with fine-grained
understanding. This issue stems from weak image-caption alignment in
pretraining datasets and a simplified contrastive objective that fails to
distinguish nuanced grounding elements such as relations, actions, and
attributes. As a result, the models tend to learn bag-of-words representations.
To mitigate these challenges, we introduce an intra-modal contrastive loss and
a unique cross-modal rank loss with an adaptive threshold that serves as
curriculum learning, utilizing our automatically generated hard negatives to
augment the model&apos;s capacity. Our strategy, which does not necessitate
additional annotations or parameters, can be incorporated into any VLM trained
with an image-text contrastive loss. Upon application to CLIP, our method leads
to significant improvements on four fine-grained benchmarks, and it also
enhances the performance of X-VLM, which is the state-of-art moodel on
fine-grained reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awal_R/0/1/0/all/0/1&quot;&gt;Rabiul Awal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Aishwarya Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08984">
<title>Tree Variational Autoencoders. (arXiv:2306.08984v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08984</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new generative hierarchical clustering model that learns a
flexible tree-based posterior distribution over latent variables. The proposed
Tree Variational Autoencoder (TreeVAE) hierarchically divides samples according
to their intrinsic characteristics, shedding light on hidden structure in the
data. It adapts its architecture to discover the optimal tree for encoding
dependencies between latent variables. The proposed tree-based generative
architecture permits lightweight conditional inference and improves generative
performance by utilizing specialized leaf decoders. We show that TreeVAE
uncovers underlying clusters in the data and finds meaningful hierarchical
relations between the different groups on a variety of datasets, including
real-world imaging data. We present empirically that TreeVAE provides a more
competitive log-likelihood lower bound than the sequential counterparts.
Finally, due to its generative nature, TreeVAE is able to generate new samples
from the discovered clusters via conditional sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1&quot;&gt;Laura Manduchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandenhirtz_M/0/1/0/all/0/1&quot;&gt;Moritz Vandenhirtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryser_A/0/1/0/all/0/1&quot;&gt;Alain Ryser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09165">
<title>DEYOv2: Rank Feature with Greedy Matching for End-to-End Object Detection. (arXiv:2306.09165v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09165</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel object detector called DEYOv2, an improved
version of the first-generation DEYO (DETR with YOLO) model. DEYOv2, similar to
its predecessor, DEYOv2 employs a progressive reasoning approach to accelerate
model training and enhance performance. The study delves into the limitations
of one-to-one matching in optimization and proposes solutions to effectively
address the issue, such as Rank Feature and Greedy Matching. This approach
enables the third stage of DEYOv2 to maximize information acquisition from the
first and second stages without needing NMS, achieving end-to-end optimization.
By combining dense queries, sparse queries, one-to-many matching, and
one-to-one matching, DEYOv2 leverages the advantages of each method. It
outperforms all existing query-based end-to-end detectors under the same
settings. When using ResNet-50 as the backbone and multi-scale features on the
COCO dataset, DEYOv2 achieves 51.1 AP and 51.8 AP in 12 and 24 epochs,
respectively. Compared to the end-to-end model DINO, DEYOv2 provides
significant performance gains of 2.1 AP and 1.4 AP in the two epoch settings.
To the best of our knowledge, DEYOv2 is the first fully end-to-end object
detector that combines the respective strengths of classical detectors and
query-based detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1&quot;&gt;Haodong Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10756">
<title>A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10756</link>
<description rdf:parseType="Literal">&lt;p&gt;The rehabilitation treatment helps to heal minor sports and occupational
injuries. In a traditional rehabilitation process, a therapist will assign
certain actions to a patient to perform in between hospital visits, and it will
rely on the patient to remember actions correctly and the schedule to perform
them. Unfortunately, many patients forget to perform actions or fail to recall
actions in detail. As a consequence, the rehabilitation treatment is hampered
or, in the worst case, the patient may suffer from additional injury caused by
performing incorrect actions. To resolve these issues, we propose a HRNet-based
rehabilitation monitoring system, which can remind a patient when to perform
the actions and display the actions for the patient to follow via the patient&apos;s
smartphone. In addition, it helps the therapist to monitor the progress of the
rehabilitation for the patient. Our system consists of an iOS app and several
components at the server side. The app is in charge of displaying and
collecting action videos. The server computes the similarity score between the
therapist&apos;s actions and the patient&apos;s in the videos to keep track of the number
of repetitions of each action. Theses stats will be shown to both of the
patient and therapist. The extensive experiments show that the F1-Score of the
similarity calculation is as high as 0.9 and the soft accuracy of the number of
repetitions is higher than 90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_Y/0/1/0/all/0/1&quot;&gt;Yi-Ching Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liou_F/0/1/0/all/0/1&quot;&gt;Fong-Syuan Liou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsuan Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Z/0/1/0/all/0/1&quot;&gt;Zi-Cing Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;MIn-Te Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11990">
<title>Evaluating the Adversarial Robustness of Convolution-based Human Motion Prediction. (arXiv:2306.11990v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11990</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion prediction has achieved a brilliant performance with the help of
CNNs, which facilitates human-machine cooperation. However, currently, there is
no work evaluating the potential risk in human motion prediction when facing
adversarial attacks, which may cause danger in real applications. The
adversarial attack will face two problems against human motion prediction: 1.
For naturalness, pose data is highly related to the physical dynamics of human
skeletons where Lp norm constraints cannot constrain the adversarial example
well; 2. Unlike the pixel value in images, pose data is diverse at scale
because of the different acquisition equipment and the data processing, which
makes it hard to set fixed parameters to perform attacks. To solve the problems
above, we propose a new adversarial attack method that perturbs the input human
motion sequence by maximizing the prediction error with physical constraints.
Specifically, we introduce a novel adaptable scheme that facilitates the attack
to suit the scale of the target pose and two physical constraints to enhance
the imperceptibility of the adversarial example. The evaluating experiments on
three datasets show that the prediction errors of all target models are
enlarged significantly, which means current convolution-based human motion
prediction models can be easily disturbed under the proposed attack. The
quantitative analysis shows that prior knowledge and semantic information
modeling can be the key to the adversarial robustness of human motion
predictors. The qualitative results indicate that the adversarial sample is
hard to be noticed when compared frame by frame but is relatively easy to be
detected when the sample is animated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1&quot;&gt;Chengxu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1&quot;&gt;Yonghao Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13394">
<title>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. (arXiv:2306.13394v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13394</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform
multimodal tasks, showing amazing emergent abilities in recent studies, such as
writing poems based on an image. However, it is difficult for these case
studies to fully reflect the performance of MLLM, lacking a comprehensive
evaluation. In this paper, we fill in this blank, presenting the first MLLM
Evaluation benchmark MME. It measures both perception and cognition abilities
on a total of 14 subtasks. In order to avoid data leakage that may arise from
direct use of public datasets for evaluation, the annotations of
instruction-answer pairs are all manually designed. The concise instruction
design allows us to fairly compare MLLMs, instead of struggling in prompt
engineering. Besides, with such an instruction, we can also easily carry out
quantitative statistics. A total of 12 advanced MLLMs are comprehensively
evaluated on our MME, which not only suggests that existing MLLMs still have a
large room for improvement, but also reveals the potential directions for the
subsequent model optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chaoyou Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peixian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yulei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengdan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinrui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14289">
<title>Faster Segment Anything: Towards Lightweight SAM for Mobile Applications. (arXiv:2306.14289v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14289</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Model (SAM) has attracted significant attention due to its
impressive zero-shot transfer performance and high versatility for numerous
vision applications (like image editing with fine-grained control). Many of
such applications need to be run on resource-constraint edge devices, like
mobile phones. In this work, we aim to make SAM mobile-friendly by replacing
the heavyweight image encoder with a lightweight one. A naive way to train such
a new SAM as in the original SAM paper leads to unsatisfactory performance,
especially when limited training sources are available. We find that this is
mainly caused by the coupled optimization of the image encoder and mask
decoder, motivated by which we propose decoupled distillation. Concretely, we
distill the knowledge from the heavy image encoder (ViT-H in the original SAM)
to a lightweight image encoder, which can be automatically compatible with the
mask decoder in the original SAM. The training can be completed on a single GPU
within less than one day, and the resulting lightweight SAM is termed MobileSAM
which is more than 60 times smaller yet performs on par with the original SAM.
For inference speed, With a single GPU, MobileSAM runs around 10ms per image:
8ms on the image encoder and 4ms on the mask decoder. With superior
performance, our MobileSAM is around 5 times faster than the concurrent FastSAM
and 7 times smaller, making it more suitable for mobile applications. Moreover,
we show that MobileSAM can run relatively smoothly on CPU. The code for our
project is provided at
\href{https://github.com/ChaoningZhang/MobileSAM}{\textcolor{red}{MobileSAM}}),
with a demo showing that MobileSAM can run relatively smoothly on CPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongshen Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jung Uk Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Sung-Ho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungkyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1&quot;&gt;Choong Seon Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14518">
<title>Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis. (arXiv:2306.14518v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14518</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness has become increasingly pivotal in medical image recognition.
However, without mitigating bias, deploying unfair medical AI systems could
harm the interests of underprivileged populations. In this paper, we observe
that while features extracted from the deeper layers of neural networks
generally offer higher accuracy, fairness conditions deteriorate as we extract
features from deeper layers. This phenomenon motivates us to extend the concept
of multi-exit frameworks. Unlike existing works mainly focusing on accuracy,
our multi-exit framework is fairness-oriented; the internal classifiers are
trained to be more accurate and fairer, with high extensibility to apply to
most existing fairness-aware frameworks. During inference, any instance with
high confidence from an internal classifier is allowed to exit early.
Experimental results show that the proposed framework can improve the fairness
condition over the state-of-the-art in two dermatological disease datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Ching-Hao Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hao-Wei Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Jen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14891">
<title>Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction. (arXiv:2306.14891v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14891</link>
<description rdf:parseType="Literal">&lt;p&gt;Image diffusion has recently shown remarkable performance in image synthesis
and implicitly as an image prior. Such a prior has been used with conditioning
to solve the inpainting problem, but only supporting binary user-based
conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion
priors can be exploited with controllable strength. Our fuzzy conditioning can
be applied pixel-wise, enabling the modification of different image components
to varying degrees. Additionally, we propose an application to facial image
correction, where we combine our fuzzy-conditioned diffusion with
diffusion-derived attention maps. Our map estimates the degree of anomaly, and
we obtain it by projecting on the diffusion space. We show how our approach
also leads to interpretable and autonomous facial image correction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helou_M/0/1/0/all/0/1&quot;&gt;Majed El Helou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15195">
<title>Shikra: Unleashing Multimodal LLM&apos;s Referential Dialogue Magic. (arXiv:2306.15195v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15195</link>
<description rdf:parseType="Literal">&lt;p&gt;In human conversations, individuals can indicate relevant regions within a
scene while addressing others. In turn, the other person can then respond by
referring to specific regions if necessary. This natural referential ability in
dialogue remains absent in current Multimodal Large Language Models (MLLMs). To
fill this gap, this paper proposes an MLLM called Shikra, which can handle
spatial coordinate inputs and outputs in natural language. Its architecture
consists of a vision encoder, an alignment layer, and a LLM. It is designed to
be straightforward and simple, without the need for extra vocabularies,
position encoder, pre-/post-detection modules, or external plug-in models. All
inputs and outputs are in natural language form. Referential dialogue is a
superset of various vision-language (VL) tasks. Shikra can naturally handle
location-related tasks like REC and PointQA, as well as conventional VL tasks
such as Image Captioning and VQA. Experimental results showcase Shikra&apos;s
promising performance. Furthermore, it enables numerous exciting applications,
like providing mentioned objects&apos; coordinates in chains of thoughts and
comparing user-pointed regions similarities. Our code, model and dataset are
accessed at https://github.com/shikras/shikra.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Keqin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Weili Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Feng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15490">
<title>EVD Surgical Guidance with Retro-Reflective Tool Tracking and Spatial Reconstruction using Head-Mounted Augmented Reality Device. (arXiv:2306.15490v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15490</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented Reality (AR) has been used to facilitate surgical guidance during
External Ventricular Drain (EVD) surgery, reducing the risks of misplacement in
manual operations. During this procedure, the key challenge is accurately
estimating the spatial relationship between pre-operative images and actual
patient anatomy in AR environment. This research proposes a novel framework
utilizing Time of Flight (ToF) depth sensors integrated in commercially
available AR Head Mounted Devices (HMD) for precise EVD surgical guidance. As
previous studies have proven depth errors for ToF sensors, we first assessed
their properties on AR-HMDs. Subsequently, a depth error model and
patient-specific parameter identification method are introduced for accurate
surface information. A tracking pipeline combining retro-reflective markers and
point clouds is then proposed for accurate head tracking. The head surface is
reconstructed using depth data for spatial registration, avoiding fixing
tracking targets rigidly on the patient&apos;s skull. Firstly, $7.580\pm 1.488 mm$
depth value error was revealed on human skin, indicating the significance of
depth correction. Our results showed that the error was reduced by over $85\%$
using proposed depth correction method on head phantoms in different materials.
Meanwhile, the head surface reconstructed with corrected depth data achieved
sub-millimetre accuracy. An experiment on sheep head revealed $0.79 mm$
reconstruction error. Furthermore, a user study was conducted for the
performance in simulated EVD surgery, where five surgeons performed nine k-wire
injections on a head phantom with virtual guidance. Results of this study
revealed $2.09 \pm 0.16 mm$ translational accuracy and $2.97\pm 0.91$ degree
orientational accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Wenqing Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Du Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Long Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangzhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15868">
<title>GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15868</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised contrastive learning (SSCL) has achieved significant
milestones in remote sensing image (RSI) understanding. Its essence lies in
designing an unsupervised instance discrimination pretext task to extract image
features from a large number of unlabeled images that are beneficial for
downstream tasks. However, existing instance discrimination based SSCL suffer
from two limitations when applied to the RSI semantic segmentation task: 1)
Positive sample confounding issue; 2) Feature adaptation bias. It introduces a
feature adaptation bias when applied to semantic segmentation tasks that
require pixel-level or object-level features. In this study, We observed that
the discrimination information can be mapped to specific regions in RSI through
the gradient of unsupervised contrastive loss, these specific regions tend to
contain singular ground objects. Based on this, we propose contrastive learning
with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation.
GraSS consists of two stages: Instance Discrimination warm-up (ID warm-up) and
Gradient guided Sampling contrastive training (GS training). The ID warm-up
aims to provide initial discrimination information to the contrastive loss
gradients. The GS training stage aims to utilize the discrimination information
contained in the contrastive loss gradients and adaptively select regions in
RSI patches that contain more singular ground objects, in order to construct
new positive and negative samples. Experimental results on three open datasets
demonstrate that GraSS effectively enhances the performance of SSCL in
high-resolution RSI semantic segmentation. Compared to seven baseline methods
from five different types of SSCL, GraSS achieves an average improvement of
1.57\% and a maximum improvement of 3.58\% in terms of mean intersection over
the union. The source code is available at https://github.com/GeoX-Lab/GraSS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1&quot;&gt;Chao Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chengli Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haifeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16103">
<title>1M parameters are enough? A lightweight CNN-based model for medical image segmentation. (arXiv:2306.16103v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16103</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) and Transformer-based models are being
widely applied in medical image segmentation thanks to their ability to extract
high-level features and capture important aspects of the image. However, there
is often a trade-off between the need for high accuracy and the desire for low
computational cost. A model with higher parameters can theoretically achieve
better performance but also result in more computational complexity and higher
memory usage, and thus is not practical to implement. In this paper, we look
for a lightweight U-Net-based model which can remain the same or even achieve
better performance, namely U-Lite. We design U-Lite based on the principle of
Depthwise Separable Convolution so that the model can both leverage the
strength of CNNs and reduce a remarkable number of computing parameters.
Specifically, we propose Axial Depthwise Convolutions with kernels 7x7 in both
the encoder and decoder to enlarge the model receptive field. To further
improve the performance, we use several Axial Dilated Depthwise Convolutions
with filters 3x3 for the bottleneck as one of our branches. Overall, U-Lite
contains only 878K parameters, 35 times less than the traditional U-Net, and
much more times less than other modern Transformer-based models. The proposed
model cuts down a large amount of computational complexity while attaining an
impressive performance on medical segmentation tasks compared to other
state-of-the-art architectures. The code will be available at:
https://github.com/duong-db/U-Lite.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dinh_B/0/1/0/all/0/1&quot;&gt;Binh-Duong Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh-Thu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thi-Thao Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pham_V/0/1/0/all/0/1&quot;&gt;Van-Truong Pham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16180">
<title>Pseudo-Bag Mixup Augmentation for Multiple Instance Learning-Based Whole Slide Image Classification. (arXiv:2306.16180v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16180</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the special situation of modeling gigapixel images, multiple instance
learning (MIL) has become one of the most important frameworks for Whole Slide
Image (WSI) classification. In current practice, most MIL networks often face
two unavoidable problems in training: i) insufficient WSI data, and ii) the
sample memorization inclination inherent in neural networks. These problems may
hinder MIL models from adequate and efficient training, suppressing the
continuous performance promotion of classification models on WSIs. Inspired by
the basic idea of Mixup, this paper proposes a new Pseudo-bag Mixup (PseMix)
data augmentation scheme to improve the training of MIL models. This scheme
generalizes the Mixup strategy for general images to special WSIs via
pseudo-bags so as to be applied in MIL-based WSI classification. Cooperated by
pseudo-bags, our PseMix fulfills the critical size alignment and semantic
alignment in Mixup strategy. Moreover, it is designed as an efficient and
decoupled method, neither involving time-consuming operations nor relying on
MIL model predictions. Comparative experiments and ablation studies are
specially designed to evaluate the effectiveness and advantages of our PseMix.
Experimental results show that PseMix could often assist state-of-the-art MIL
networks to refresh the classification performance on WSIs. Besides, it could
also boost the generalization ability of MIL models, and promote their
robustness to patch occlusion and noisy labels. Our source code is available at
https://github.com/liupei101/PseMix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Luping Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1&quot;&gt;Feng Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16736">
<title>GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction. (arXiv:2306.16736v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16736</link>
<description rdf:parseType="Literal">&lt;p&gt;Demystifying complex human-ground interactions is essential for accurate and
realistic 3D human motion reconstruction from RGB videos, as it ensures
consistency between the humans and the ground plane. Prior methods have modeled
human-ground interactions either implicitly or in a sparse manner, often
resulting in unrealistic and incorrect motions when faced with noise and
uncertainty. In contrast, our approach explicitly represents these interactions
in a dense and continuous manner. To this end, we propose a novel Ground-aware
Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly
learns the distribution of transitions in both pose and interaction between
every joint and ground plane at each time step of a motion sequence. It is
trained to explicitly promote consistency between the motion and distance
change towards the ground. After training, we establish a joint optimization
strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization
towards the space of plausible ground-aware motions. This leads to realistic
and coherent motion reconstruction, irrespective of the assumed or learned
ground plane. Through extensive evaluation on the AMASS and AIST++ datasets,
our model demonstrates good generalization and discriminating abilities in
challenging cases including complex and ambiguous human-ground interactions.
The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Sihan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17115">
<title>Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation. (arXiv:2306.17115v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17115</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel alignment-before-generation approach to tackle the
challenging task of generating general 3D shapes based on 2D images or texts.
Directly learning a conditional generative model from images or texts to 3D
shapes is prone to producing inconsistent results with the conditions because
3D shapes have an additional dimension whose distribution significantly differs
from that of 2D images and texts. To bridge the domain gap among the three
modalities and facilitate multi-modal-conditioned 3D shape generation, we
explore representing 3D shapes in a shape-image-text-aligned space. Our
framework comprises two models: a Shape-Image-Text-Aligned Variational
Auto-Encoder (SITA-VAE) and a conditional Aligned Shape Latent Diffusion Model
(ASLDM). The former model encodes the 3D shapes into the shape latent space
aligned to the image and text and reconstructs the fine-grained 3D neural
fields corresponding to given shape embeddings via the transformer-based
decoder. The latter model learns a probabilistic mapping function from the
image or text space to the latent shape space. Our extensive experiments
demonstrate that our proposed approach can generate higher-quality and more
diverse 3D shapes that better semantically conform to the visual or textural
conditional inputs, validating the effectiveness of the
shape-image-text-aligned space for cross-modality 3D shape generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xianfang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shenghua Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17624">
<title>Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17624</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating learning-friendly representations for points in space is a
fundamental and long-standing problem in ML. Recently, multi-scale encoding
schemes (such as Space2Vec and NeRF) were proposed to directly encode any point
in 2D/3D Euclidean space as a high-dimensional vector, and has been
successfully applied to various geospatial prediction and generative tasks.
However, all current 2D and 3D location encoders are designed to model point
distances in Euclidean space. So when applied to large-scale real-world GPS
coordinate datasets, which require distance metric learning on the spherical
surface, both types of models can fail due to the map projection distortion
problem (2D) and the spherical-to-Euclidean distance approximation error (3D).
To solve these problems, we propose a multi-scale location encoder called
Sphere2Vec which can preserve spherical distances when encoding point
coordinates on a spherical surface. We developed a unified view of
distance-reserving encoding on spheres based on the DFS. We also provide
theoretical proof that the Sphere2Vec preserves the spherical surface distance
between any two points, while existing encoding schemes do not. Experiments on
20 synthetic datasets show that Sphere2Vec can outperform all baseline models
on all these datasets with up to 30.8% error rate reduction. We then apply
Sphere2Vec to three geo-aware image classification tasks - fine-grained species
recognition, Flickr image recognition, and remote sensing image classification.
Results on 7 real-world datasets show the superiority of Sphere2Vec over
multiple location encoders on all three tasks. Further analysis shows that
Sphere2Vec outperforms other location encoder models, especially in the polar
regions and data-sparse areas because of its nature for spherical surface
distance preservation. Code and data are available at
https://gengchenmai.github.io/sphere2vec-website/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1&quot;&gt;Gengchen Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1&quot;&gt;Yao Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wenyun Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yutong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1&quot;&gt;Krzysztof Janowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1&quot;&gt;Ni Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17643">
<title>Neural 3D Scene Reconstruction from Multi-view Images without 3D Supervision. (arXiv:2306.17643v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17643</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural scene reconstruction methods have achieved impressive performance in
reconstructing complex geometry and low-textured regions in large scenes.
However, these methods heavily rely on 3D supervised information which is
costly and time-consuming to obtain in the real world. In this paper, we
propose a novel neural reconstruction method that reconstructs scenes without
3D supervision. We perform differentiable volume rendering for scene
reconstruction by using accessible 2D images as supervision. We impose geometry
to improve the reconstruction quality of complex geometry regions in the
scenes, and impose plane constraints to improve the reconstruction quality of
low-textured regions in the scenes. Specifically, we introduce a signed
distance function (SDF) field, a color field, and a probability field to
represent the scene, and optimize the fields under the differentiable ray
marching to reconstruct the scene. Besides, we impose geometric constraints
that project 3D points on the surface to similar-looking regions with similar
features in different views. We also impose plane constraints to make large
planes keep parallel or vertical to the wall or floor. These two constraints
help to reconstruct accurate and smooth geometry structures of the scene.
Without 3D supervision information, our method achieves competitive
reconstruction compared with some existing methods that use 3D information as
supervision on the ScanNet dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Che Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yunde Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17651">
<title>Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-view. (arXiv:2306.17651v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17651</link>
<description rdf:parseType="Literal">&lt;p&gt;From an image of a person, we can easily infer the natural 3D pose and shape
of the person even if ambiguity exists. This is because we have a mental model
that allows us to imagine a person&apos;s appearance at different viewing directions
from a given image and utilize the consistency between them for inference.
However, existing human mesh recovery methods only consider the direction in
which the image was taken due to their structural limitations. Hence, we
propose &quot;Implicit 3D Human Mesh Recovery (ImpHMR)&quot; that can implicitly imagine
a person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR,
feature fields are generated by CNN-based image encoder for a given image.
Then, the 2D feature map is volume-rendered from the feature field for a given
viewing direction, and the pose and shape parameters are regressed from the
feature. To utilize consistency with pose and shape from unseen-view, if there
are 3D labels, the model predicts results including the silhouette from an
arbitrary direction and makes it equal to the rotated ground-truth. In the case
of only 2D labels, we perform self-supervised learning through the constraint
that the pose and shape parameters inferred from different directions should be
the same. Extensive evaluations show the efficacy of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hanbyel Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yooshin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1&quot;&gt;Jaesung Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17842">
<title>SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs. (arXiv:2306.17842v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling
frozen LLMs to perform both understanding and generation tasks involving
non-linguistic modalities such as images or videos. SPAE converts between raw
pixels and interpretable lexical tokens (or words) extracted from the LLM&apos;s
vocabulary. The resulting tokens capture both the semantic meaning and the
fine-grained details needed for visual reconstruction, effectively translating
the visual content into a language comprehensible to the LLM, and empowering it
to perform a wide array of multimodal tasks. Our approach is validated through
in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set
of image understanding and generation tasks. Our method marks the first
successful attempt to enable a frozen LLM to generate image content while
surpassing state-of-the-art performance in image understanding tasks, under the
same setting, by over 25%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lijun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiruo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vivek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1&quot;&gt;Wolfgang Macherey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanping Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1&quot;&gt;David A. Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1&quot;&gt;Irfan Essa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kevin Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Alexander G. Hauptmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09325">
<title>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2211.09325</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we imbue robots with the ability to efficiently manipulate unseen
objects and transfer relevant skills based on demonstrations? End-to-end
learning methods often fail to generalize to novel objects or unseen
configurations. Instead, we focus on the task-specific pose relationship
between relevant parts of interacting objects. We conjecture that this
relationship is a generalizable notion of a manipulation task that can transfer
to new objects in the same category; examples include the relationship between
the pose of a pan relative to an oven or the pose of a mug relative to a mug
rack. We call this task-specific pose relationship &quot;cross-pose&quot; and provide a
mathematical definition of this concept. We propose a vision-based system that
learns to estimate the cross-pose between two objects for a given manipulation
task using learned cross-object correspondences. The estimated cross-pose is
then used to guide a downstream motion planner to manipulate the objects into
the desired pose relationship (placing a pan into the oven or the mug onto the
mug rack). We demonstrate our method&apos;s capability to generalize to unseen
objects, in some cases after training on only 10 demonstrations in the real
world. Results show that our system achieves state-of-the-art performance in
both simulated and real-world experiments across a number of tasks.
Supplementary information and videos can be found at
https://sites.google.com/view/tax-pose/home.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chuer Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1&quot;&gt;Brian Okorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Harry Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1&quot;&gt;Ben Eisner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00068">
<title>Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.00068</link>
<description rdf:parseType="Literal">&lt;p&gt;During the COVID-19 pandemic, wearing a face mask has been known to be an
effective way to prevent the spread of COVID-19. In lots of monitoring tasks,
humans have been replaced with computers thanks to the outstanding performance
of the deep learning models. Monitoring the wearing of a face mask is another
task that can be done by deep learning models with acceptable accuracy. The
main challenge of this task is the limited amount of data because of the
quarantine. In this paper, we did an investigation on the capability of three
state-of-the-art object detection neural networks on face mask detection for
real-time applications. As mentioned, here are three models used, Single Shot
Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny,
and YOLOv4-tiny-3l from which the best was selected. In the proposed method,
according to the performance of different models, the best model that can be
suitable for use in real-world and mobile device applications in comparison to
other recent studies was the YOLOv4-tiny model, with 85.31% and 50.66 for mean
Average Precision (mAP) and Frames Per Second (FPS), respectively. These
acceptable values were achieved using two datasets with only 1531 images in
three separate classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoramdel_J/0/1/0/all/0/1&quot;&gt;Javad Khoramdel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatami_S/0/1/0/all/0/1&quot;&gt;Soheila Hatami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadedel_M/0/1/0/all/0/1&quot;&gt;Majid Sadedel&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>