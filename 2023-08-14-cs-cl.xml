<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.07306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05481" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2106.07306">
<title>Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.07306</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge in structured prediction is to represent the
interdependencies within output structures. When outputs are structured as
sequences, linear-chain conditional random fields (CRFs) are a widely used
model class which can learn \textit{local} dependencies in the output. However,
the CRF&apos;s Markov assumption makes it impossible for CRFs to represent
distributions with \textit{nonlocal} dependencies, and standard CRFs are unable
to respect nonlocal constraints of the data (such as global arity constraints
on output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show
empirically that it can be substantially better in practice. Additionally, we
demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF
into a deep neural model for semantic role labeling, exceeding state-of-the-art
results on a standard dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1&quot;&gt;Sean Papay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1&quot;&gt;Roman Klinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1&quot;&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15781">
<title>A Compact End-to-End Model with Local and Global Context for Spoken Language Identification. (arXiv:2210.15781v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15781</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce TitaNet-LID, a compact end-to-end neural network for Spoken
Language Identification (LID) that is based on the ContextNet architecture.
TitaNet-LID employs 1D depth-wise separable convolutions and
Squeeze-and-Excitation layers to effectively capture local and global context
within an utterance. Despite its small size, TitaNet-LID achieves performance
similar to state-of-the-art models on the VoxLingua107 dataset while being 10
times smaller. Furthermore, it can be easily adapted to new acoustic conditions
and unseen languages through simple fine-tuning, achieving a state-of-the-art
accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can
achieve a better trade-off between accuracy and speed. TitaNet-LID performs
well even on short utterances less than 5s in length, indicating its robustness
to input length.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fei Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Koluguri_N/0/1/0/all/0/1&quot;&gt;Nithin Rao Koluguri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balam_J/0/1/0/all/0/1&quot;&gt;Jagadeesh Balam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1&quot;&gt;Boris Ginsburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17316">
<title>There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17316</link>
<description rdf:parseType="Literal">&lt;p&gt;Whisper is a recent Automatic Speech Recognition (ASR) model displaying
impressive robustness to both out-of-distribution inputs and random noise. In
this work, we show that this robustness does not carry over to adversarial
noise. We show that we can degrade Whisper performance dramatically, or even
transcribe a target sentence of our choice, by generating very small input
perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling
the Whisper language detector we can very easily degrade the performance of
multilingual models. These vulnerabilities of a widely popular open-source
model have practical security implications and emphasize the need for
adversarially robust ASR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Olivier_R/0/1/0/all/0/1&quot;&gt;Raphael Olivier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00732">
<title>Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;Online encyclopedias, such as Wikipedia, have been well-developed and
researched in the last two decades. One can find any attributes or other
information of a wiki item on a wiki page edited by a community of volunteers.
However, the traditional text, images and tables can hardly express some
aspects of an wiki item. For example, when we talk about ``Shiba Inu&apos;&apos;, one may
care more about ``How to feed it&apos;&apos; or ``How to train it not to protect its
food&apos;&apos;. Currently, short-video platforms have become a hallmark in the online
world. Whether you&apos;re on TikTok, Instagram, Kuaishou, or YouTube Shorts,
short-video apps have changed how we consume and create content today. Except
for producing short videos for entertainment, we can find more and more authors
sharing insightful knowledge widely across all walks of life. These short
videos, which we call knowledge videos, can easily express any aspects (e.g.
hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and
they can be systematically analyzed and organized like an online encyclopedia.
In this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia
consisting of items, aspects, and short videos lined to them, which was
extracted from billions of videos of Kuaishou (Kwai), a well-known short-video
platform in China. We first collected items from multiple sources and mined
user-centered aspects from millions of users&apos; queries to build an item-aspect
tree. Then we propose a new task called ``multi-modal item-aspect linking&apos;&apos; as
an expansion of ``entity linking&apos;&apos; to link short videos into item-aspect pairs
and build the whole short-video encyclopedia. Intrinsic evaluations show that
our encyclopedia is of large scale and highly accurate. We also conduct
sufficient extrinsic experiments to show how Kuaipedia can help fundamental
applications such as entity typing and entity linking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Haojie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zepeng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruiji Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16878">
<title>Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16878</link>
<description rdf:parseType="Literal">&lt;p&gt;Short text classification is a crucial and challenging aspect of Natural
Language Processing. For this reason, there are numerous highly specialized
short text classifiers. However, in recent short text research, State of the
Art (SOTA) methods for traditional text classification, particularly the pure
use of Transformers, have been unexploited. In this work, we examine the
performance of a variety of short text classifiers as well as the top
performing traditional text classifier. We further investigate the effects on
two new real-world short text datasets in an effort to address the issue of
becoming overly dependent on benchmark datasets with a limited number of
characteristics. Our experiments unambiguously demonstrate that Transformers
achieve SOTA accuracy on short text classification tasks, raising the question
of whether specialized short text techniques are necessary.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karl_F/0/1/0/all/0/1&quot;&gt;Fabian Karl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1&quot;&gt;Ansgar Scherp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06817">
<title>RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06817</link>
<description rdf:parseType="Literal">&lt;p&gt;By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project&apos;s website and videos can be found at
robotics-transformer1.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1&quot;&gt;Anthony Brohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1&quot;&gt;Noah Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbajal_J/0/1/0/all/0/1&quot;&gt;Justice Carbajal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1&quot;&gt;Yevgen Chebotar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabis_J/0/1/0/all/0/1&quot;&gt;Joseph Dabis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzog_A/0/1/0/all/0/1&quot;&gt;Alex Herzog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1&quot;&gt;Jasmine Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1&quot;&gt;Julian Ibarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1&quot;&gt;Tomas Jackson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jesmonth_S/0/1/0/all/0/1&quot;&gt;Sally Jesmonth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nikhil J Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1&quot;&gt;Ryan Julian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1&quot;&gt;Dmitry Kalashnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1&quot;&gt;Yuheng Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1&quot;&gt;Isabel Leal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malla_U/0/1/0/all/0/1&quot;&gt;Utsav Malla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manjunath_D/0/1/0/all/0/1&quot;&gt;Deeksha Manjunath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parada_C/0/1/0/all/0/1&quot;&gt;Carolina Parada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peralta_J/0/1/0/all/0/1&quot;&gt;Jodilyn Peralta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Emily Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1&quot;&gt;Karl Pertsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiambao_J/0/1/0/all/0/1&quot;&gt;Jornell Quiambao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael Ryoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salazar_G/0/1/0/all/0/1&quot;&gt;Grecia Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1&quot;&gt;Pannag Sanketi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_K/0/1/0/all/0/1&quot;&gt;Kevin Sayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jaspiar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1&quot;&gt;Sumedh Sontakke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1&quot;&gt;Austin Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Clayton Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1&quot;&gt;Huong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1&quot;&gt;Vincent Vanhoucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vega_S/0/1/0/all/0/1&quot;&gt;Steve Vega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sichun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1&quot;&gt;Brianna Zitkovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08427">
<title>Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning. (arXiv:2301.08427v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08427</link>
<description rdf:parseType="Literal">&lt;p&gt;The Bidirectional Encoder Representations from Transformers (BERT) were
proposed in the natural language process (NLP) and shows promising results.
Recently researchers applied the BERT to source-code representation learning
and reported some good news on several downstream tasks. However, in this
paper, we illustrated that current methods cannot effectively understand the
logic of source codes. The representation of source code heavily relies on the
programmer-defined variable and function names. We design and implement a set
of experiments to demonstrate our conjecture and provide some insights for
future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07189">
<title>Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking. (arXiv:2302.07189v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07189</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also apply KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yinan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1&quot;&gt;Ian Horrocks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14057">
<title>Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14057</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic detection of multimodal fake news has gained a widespread attention
recently. Many existing approaches seek to fuse unimodal features to produce
multimodal news representations. However, the potential of powerful cross-modal
contrastive learning methods for fake news detection has not been well
exploited. Besides, how to aggregate features from different modalities to
boost the performance of the decision-making process is still an open question.
To address that, we propose COOLANT, a cross-modal contrastive learning
framework for multimodal fake news detection, aiming to achieve more accurate
image-text alignment. To further improve the alignment precision, we leverage
an auxiliary task to soften the loss term of negative samples during the
contrast process. A cross-modal fusion module is developed to learn the
cross-modality correlations. An attention mechanism with an attention guidance
module is implemented to help effectively and interpretably aggregate the
aligned unimodal representations and the cross-modality correlations. Finally,
we evaluate the COOLANT and conduct a comparative study on two widely used
datasets, Twitter and Weibo. The experimental results demonstrate that our
COOLANT outperforms previous approaches by a large margin and achieves new
state-of-the-art results on the two datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongxiu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08032">
<title>Verifying the Robustness of Automatic Credibility Assessment. (arXiv:2303.08032v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08032</link>
<description rdf:parseType="Literal">&lt;p&gt;Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we systematically test the robustness of popular text classifiers
against available attacking techniques and discover that, indeed, in some cases
insignificant changes in input text can mislead the models. We also introduce
BODEGA: a benchmark for testing both victim models and attack methods on four
misinformation detection tasks in an evaluation framework designed to simulate
real use-cases of content moderation. Finally, we manually analyse a subset
adversarial examples and check what kinds of modifications are used in
successful attacks. The BODEGA code and data is openly shared in hope of
enhancing the comparability and replicability of further research in this area
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Przybyla_P/0/1/0/all/0/1&quot;&gt;Piotr Przyby&amp;#x142;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shvets_A/0/1/0/all/0/1&quot;&gt;Alexander Shvets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saggion_H/0/1/0/all/0/1&quot;&gt;Horacio Saggion&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16618">
<title>Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16618</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models that are sensitive to external context can more effectively
capture the speaking patterns of individuals with specific characteristics or
in particular environments. However, obtaining and leveraging such annotations
can be challenging. In this work, we show how to leverage rich character and
film annotations to personalise language models in a scalable manner. Our best
model can reduce perplexity by up to 6.5% compared to a parameter-matched
language model. Our approach performs on par with speaker-specific fine-tuning
when the fine-tuning data (i.e. past dialogue) for individual speakers is
available. On top of that, it also generalises well to a scenario with no such
data, relying on combinations of demographic characteristics expressed via
metadata. Our findings are consistent across two corpora, one of which is also
a contribution of this paper: Cornell-rich contains rich manual annotations for
863 speaking characters from the Cornell Movie Dialog Corpus, including
features such as characteristic quotes and character descriptions, along with
six automatically extracted metadata features for over 95% of the featured
films. Finally, we also present a cost-benefit analysis highlighting which
annotations are most cost-effective in reducing perplexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1&quot;&gt;Sebastian Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumner_R/0/1/0/all/0/1&quot;&gt;Rowanne Sumner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowek_A/0/1/0/all/0/1&quot;&gt;Alice Dowek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charlotte Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preston_E/0/1/0/all/0/1&quot;&gt;Emily Preston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayliss_C/0/1/0/all/0/1&quot;&gt;Chris Bayliss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oakley_C/0/1/0/all/0/1&quot;&gt;Chris Oakley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1&quot;&gt;Carolina Scarton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10615">
<title>ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. (arXiv:2305.10615v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10615</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard
to benchmark the performance of Self-Supervised Learning (SSL) models on
various speech processing tasks. However, SUPERB largely considers English
speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),
covering 143 languages (ranging from high-resource to endangered), and
considering both automatic speech recognition and language identification.
Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and
employs a simple framework for multilingual tasks by learning a shallow
downstream model. Similar to the SUPERB benchmark, we find speech SSL models
can significantly improve performance compared to FBANK features. Furthermore,
we find that multilingual models do not always perform better than their
monolingual counterparts. We will release ML-SUPERB as a challenge with
organized datasets and reproducible training scripts for future multilingual
representation research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiatong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1&quot;&gt;Dan Berrebbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;William Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Ho-Lam Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1&quot;&gt;En-Pei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Ping Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xuankai Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09927">
<title>Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09927</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention-based neural networks such as transformers have demonstrated a
remarkable ability to exhibit in-context learning (ICL): Given a short prompt
sequence of tokens from an unseen task, they can formulate relevant per-token
and next-token predictions without any parameter updates. By embedding a
sequence of labeled training data and unlabeled test data as a prompt, this
allows for transformers to behave like supervised learning algorithms. Indeed,
recent work has shown that when training transformer architectures over random
instances of linear regression problems, these models&apos; predictions mimic those
of ordinary least squares.
&lt;/p&gt;
&lt;p&gt;Towards understanding the mechanisms underlying this phenomenon, we
investigate the dynamics of ICL in transformers with a single linear
self-attention layer trained by gradient flow on linear regression tasks. We
show that despite non-convexity, gradient flow with a suitable random
initialization finds a global minimum of the objective function. At this global
minimum, when given a test prompt of labeled examples from a new prediction
task, the transformer achieves prediction error competitive with the best
linear predictor over the test prompt distribution. We additionally
characterize the robustness of the trained transformer to a variety of
distribution shifts and show that although a number of shifts are tolerated,
shifts in the covariate distribution of the prompts are not. Motivated by this,
we consider a generalized ICL setting where the covariate distributions can
vary across prompts. We show that although gradient flow succeeds at finding a
global minimum in this setting, the trained transformer is still brittle under
mild covariate shifts. We complement this finding with experiments on large,
nonlinear transformer architectures which we show are more robust under
covariate shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frei_S/0/1/0/all/0/1&quot;&gt;Spencer Frei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14704">
<title>Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement. (arXiv:2306.14704v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14704</link>
<description rdf:parseType="Literal">&lt;p&gt;Mentions of new concepts appear regularly in texts and require automated
approaches to harvest and place them into Knowledge Bases (KB), e.g.,
ontologies and taxonomies. Existing datasets suffer from three issues, (i)
mostly assuming that a new concept is pre-discovered and cannot support
out-of-KB mention discovery; (ii) only using the concept label as the input
along with the KB and thus lacking the contexts of a concept label; and (iii)
mostly focusing on concept placement w.r.t a taxonomy of atomic concepts,
instead of complex concepts, i.e., with logical operators. To address these
issues, we propose a new benchmark, adapting MedMentions dataset (PubMed
abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases
sub-category and the broader categories of Clinical finding, Procedure, and
Pharmaceutical / biologic product. We provide usage on the evaluation with the
dataset for out-of-KB mention discovery and concept placement, adapting recent
Large Language Model based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1&quot;&gt;Ian Horrocks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03104">
<title>Efficient Domain Adaptation of Sentence Embeddings Using Adapters. (arXiv:2307.03104v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03104</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentence embeddings enable us to capture the semantic similarity of short
texts. Most sentence embedding models are trained for general semantic textual
similarity (STS) tasks. Therefore, to use sentence embeddings in a particular
domain, the model must be adapted to it in order to achieve good results.
Usually, this is done by fine-tuning the entire sentence embedding model for
the domain of interest. While this approach yields state-of-the-art results,
all of the model&apos;s weights are updated during fine-tuning, making this method
resource-intensive. Therefore, instead of fine-tuning entire sentence embedding
models for each target domain individually, we propose to train lightweight
adapters. These domain-specific adapters do not require fine-tuning all
underlying sentence embedding model parameters. Instead, we only train a small
number of additional parameters while keeping the weights of the underlying
sentence embedding model fixed. Training domain-specific adapters allows always
using the same base model and only exchanging the domain-specific adapters to
adapt sentence embeddings to a specific domain. We show that using adapters for
parameter-efficient domain adaptation of sentence embeddings yields competitive
performance within 1% of a domain-adapted, entirely fine-tuned sentence
embedding model while only training approximately 3.6% of the parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1&quot;&gt;Tim Schopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;Dennis N. Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1&quot;&gt;Florian Matthes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12267">
<title>Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12267</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent large language models (LLMs), e.g., ChatGPT, have been able to
generate human-like and fluent responses when provided with specific
instructions. While admitting the convenience brought by technological
advancement, educators also have concerns that students might leverage LLMs to
complete their writing assignments and pass them off as their original work.
Although many AI content detection studies have been conducted as a result of
such concerns, most of these prior studies modeled AI content detection as a
classification problem, assuming that a text is either entirely human-written
or entirely AI-generated. In this study, we investigated AI content detection
in a rarely explored yet realistic setting where the text to be detected is
collaboratively written by human and generative LLMs (i.e., hybrid text). We
first formalized the detection task as identifying the transition points
between human-written content and AI-generated content from a given hybrid text
(boundary detection). Then we proposed a two-step approach where we (1)
separated AI-generated content from human-written content during the encoder
training process; and (2) calculated the distances between every two adjacent
prototypes and assumed that the boundaries exist between the two adjacent
prototypes that have the furthest distance from each other. Through extensive
experiments, we observed the following main findings: (1) the proposed approach
consistently outperformed the baseline methods across different experiment
settings; (2) the encoder training process can significantly boost the
performance of the proposed approach; (3) when detecting boundaries for
single-boundary hybrid essays, the proposed approach could be enhanced by
adopting a relatively large prototype size, leading to a 22% improvement in the
In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zijie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1&quot;&gt;Lele Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1&quot;&gt;Dragan Ga&amp;#x161;evi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanliang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00158">
<title>Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Translation Quality Estimation (TQE) is an important step before deploying
the output translation into usage. TQE is also critical in assessing machine
translation (MT) and human translation (HT) quality without seeing the
reference translations. In this work, we examine if the state-of-the-art large
language models (LLMs) can be fine-tuned for the TQE task and their capability.
We take ChatGPT as one example and approach TQE as a binary classification
task. Using English to Italian, German, French, Japanese, Dutch, Portuguese,
Turkish, and Chinese training corpora, our experimental results show that
fine-tuned ChatGPT via its API can achieve a relatively high score on
predicting translation quality, i.e. if the translation needs to be edited, but
there is definitely much space to improve the accuracy. English-Italiano
bilingual Abstract is available in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1&quot;&gt;Serge Gladkoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1&quot;&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02080">
<title>Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02080</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media platforms, despite their value in promoting open discourse, are
often exploited to spread harmful content. Current deep learning and natural
language processing models used for detecting this harmful content overly rely
on domain-specific terms affecting their capabilities to adapt to generalizable
hate speech detection. This is because they tend to focus too narrowly on
particular linguistic signals or the use of certain categories of words.
Another significant challenge arises when platforms lack high-quality annotated
data for training, leading to a need for cross-platform models that can adapt
to different distribution shifts. Our research introduces a cross-platform hate
speech detection model capable of being trained on one platform&apos;s data and
generalizing to multiple unseen platforms. To achieve good generalizability
across platforms, one way is to disentangle the input representations into
invariant and platform-dependent features. We also argue that learning causal
relationships, which remain constant across diverse environments, can
significantly aid in understanding invariant representations in hate speech. By
disentangling input into platform-dependent features (useful for predicting
hate targets) and platform-independent features (used to predict the presence
of hate), we learn invariant representations resistant to distribution shifts.
These features are then used to predict hate speech across unseen platforms.
Our extensive experiments across four platforms highlight our model&apos;s enhanced
efficacy compared to existing state-of-the-art methods in detecting generalized
hate speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1&quot;&gt;Paras Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1&quot;&gt;Tharindu Kumarage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02463">
<title>Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02463</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM.We consider the construction of foundational models from
the perspectives of data, model design, and evaluation thoroughly. Our
contribution can be concluded as follows: (i), we construct a large-scale
Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.
To the best of our knowledge, this is the first multi-modal dataset containing
3D medical scans. (ii), We propose an architecture that enables visually
conditioned generative pre-training, allowing for the integration of text input
interleaved with 2D or 3D medical scans to generate response for diverse
radiologic tasks. The model was initially pre-trained on MedMD and subsequently
domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,
containing 3M radiologic visual-language pairs. (iii), we propose a new
evaluation benchmark that comprises five tasks, aiming to comprehensively
assess the capability of foundation models in handling practical clinical
problems. Our experimental results confirm that RadFM significantly outperforms
existing multi-modal foundation models. The codes, data, and model checkpoint
will all be made publicly available to promote further research and development
in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chaoyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03043">
<title>3D-EX : A Unified Dataset of Definitions and Dictionary Examples. (arXiv:2308.03043v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03043</link>
<description rdf:parseType="Literal">&lt;p&gt;Definitions are a fundamental building block in lexicography, linguistics and
computational semantics. In NLP, they have been used for retrofitting word
embeddings or augmenting contextual representations in language models.
However, lexical resources containing definitions exhibit a wide range of
properties, which has implications in the behaviour of models trained and
evaluated on them. In this paper, we introduce 3D- EX , a dataset that aims to
fill this gap by combining well-known English resources into one centralized
knowledge repository in the form of &amp;lt;term, definition, example&amp;gt; triples. 3D- EX
is a unified evaluation framework with carefully pre-computed
train/validation/test splits to prevent memorization. We report experimental
results that suggest that this dataset could be effectively leveraged in
downstream NLP tasks. Code and data are available at
https://github.com/F-Almeman/3D-EX .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeman_F/0/1/0/all/0/1&quot;&gt;Fatemah Almeman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikhi_H/0/1/0/all/0/1&quot;&gt;Hadi Sheikhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1&quot;&gt;Luis Espinosa-Anke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04255">
<title>CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04255</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of
the South Slavic languages, which is based on the Stanza natural language
processing pipeline. We describe the main improvements in CLASSLA-Stanza with
respect to Stanza, and give a detailed description of the model training
process for the latest 2.1 release of the pipeline. We also report performance
scores produced by the pipeline for different languages and varieties.
CLASSLA-Stanza exhibits consistently high performance across all the supported
languages and outperforms or expands its parent pipeline Stanza at all the
supported tasks. We also present the pipeline&apos;s new functionality enabling
efficient processing of web data and the reasons that led to its
implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tercon_L/0/1/0/all/0/1&quot;&gt;Luka Ter&amp;#x10d;on&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1&quot;&gt;Nikola Ljube&amp;#x161;i&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04823">
<title>Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04823</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents CG-Eval, the first comprehensive evaluation of the
generation capabilities of large Chinese language models across a wide range of
academic disciplines. The models&apos; performance was assessed based on their
ability to generate accurate and relevant responses to different types of
questions in six disciplines, namely, Science and Engineering, Humanities and
Social Sciences, Mathematical Calculations, Medical Practitioner Qualification
Examination, Judicial Examination, and Certified Public Accountant Examination.
This paper also presents Gscore, a composite index derived from the weighted
sum of multiple metrics to measure the quality of model&apos;s generation against a
reference. The test data and test results can be found at
&lt;a href=&quot;http://cgeval.besteasy.com/.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1&quot;&gt;Hui Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jingyuan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_M/0/1/0/all/0/1&quot;&gt;Meng Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_B/0/1/0/all/0/1&quot;&gt;Bin Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Na Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05476">
<title>Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis. (arXiv:2308.05476v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05476</link>
<description rdf:parseType="Literal">&lt;p&gt;Deceptive text classification is a critical task in natural language
processing that aims to identify deceptive o fraudulent content. This study
presents a comparative analysis of machine learning and transformer-based
approaches for deceptive text classification. We investigate the effectiveness
of traditional machine learning algorithms and state-of-the-art transformer
models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive
text. A labeled dataset consisting of deceptive and non-deceptive texts is used
for training and evaluation purposes. Through extensive experimentation, we
compare the performance metrics, including accuracy, precision, recall, and F1
score, of the different approaches. The results of this study shed light on the
strengths and limitations of machine learning and transformer-based methods for
deceptive text classification, enabling researchers and practitioners to make
informed decisions when dealing with deceptive content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1&quot;&gt;Anusuya Krishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05481">
<title>LLM As DBA. (arXiv:2308.05481v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05481</link>
<description rdf:parseType="Literal">&lt;p&gt;Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuanhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>