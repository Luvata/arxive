<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04414" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.01434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.00907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.10517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.03418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04212" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.04224">
<title>MELEP: A Novel Predictive Measure of Transferability in Multi-Label ECG Analysis. (arXiv:2311.04224v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04224</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MELEP, which stands for Muti-label Expected Log of Empirical
Predictions, a novel measure to estimate how effective it is to transfer
knowledge from a pre-trained model to a downstream task in a multi-label
settings. The measure is generic to work with new target data having a
different label set from source data. It is also computationally efficient,
only requires forward passing the downstream dataset through the pre-trained
model once. To the best of our knowledge, we are the first to develop such a
transferability metric for multi-label ECG classification problems. Our
experiments show that MELEP can predict the performance of pre-trained
convolutional and recurrent deep neural networks, on small and imbalanced ECG
data. Specifically, strong correlation coefficients, with absolute values
exceeding 0.6 in most cases, were observed between MELEP and the actual average
F1 scores of the fine-tuned models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cuong V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duong_H/0/1/0/all/0/1&quot;&gt;Hieu Minh Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Do_C/0/1/0/all/0/1&quot;&gt;Cuong D.Do&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04231">
<title>A Practical Large-Scale Roadside Multi-View Multi-Sensor Spatial Synchronization Framework for Intelligent Transportation Systems. (arXiv:2311.04231v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04231</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial synchronization in roadside scenarios is essential for integrating
data from multiple sensors at different locations. Current methods using
cascading spatial transformation (CST) often lead to cumulative errors in
large-scale deployments. Manual camera calibration is insufficient and requires
extensive manual work, and existing methods are limited to controlled or
single-view scenarios. To address these challenges, our research introduces a
parallel spatial transformation (PST)-based framework for large-scale,
multi-view, multi-sensor scenarios. PST parallelizes sensor coordinate system
transformation, reducing cumulative errors. We incorporate deep learning for
precise roadside monocular global localization, reducing manual work.
Additionally, we use geolocation cues and an optimization algorithm for
improved synchronization accuracy. Our framework has been tested in real-world
scenarios, outperforming CST-based methods. It significantly enhances
large-scale roadside multi-perspective, multi-sensor spatial synchronization,
reducing deployment costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Rui Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04234">
<title>Leveraging sinusoidal representation networks to predict fMRI signals from EEG. (arXiv:2311.04234v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.04234</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern neuroscience, functional magnetic resonance imaging (fMRI) has been
a crucial and irreplaceable tool that provides a non-invasive window into the
dynamics of whole-brain activity. Nevertheless, fMRI is limited by hemodynamic
blurring as well as high cost, immobility, and incompatibility with metal
implants. Electroencephalography (EEG) is complementary to fMRI and can
directly record the cortical electrical activity at high temporal resolution,
but has more limited spatial resolution and is unable to recover information
about deep subcortical brain structures. The ability to obtain fMRI information
from EEG would enable cost-effective, imaging across a wider set of brain
regions. Further, beyond augmenting the capabilities of EEG, cross-modality
models would facilitate the interpretation of fMRI signals. However, as both
EEG and fMRI are high-dimensional and prone to artifacts, it is currently
challenging to model fMRI from EEG. To address this challenge, we propose a
novel architecture that can predict fMRI signals directly from multi-channel
EEG without explicit feature engineering. Our model achieves this by
implementing a Sinusoidal Representation Network (SIREN) to learn frequency
information in brain dynamics from EEG, which serves as the input to a
subsequent encoder-decoder to effectively reconstruct the fMRI signal from a
specific brain region. We evaluate our model using a simultaneous EEG-fMRI
dataset with 8 subjects and investigate its potential for predicting
subcortical fMRI signals. The present results reveal that our model outperforms
a recent state-of-the-art model, and indicates the potential of leveraging
periodic activation functions in deep neural networks to model functional
neuroimaging data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yamin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lou_A/0/1/0/all/0/1&quot;&gt;Ange Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Catie Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04243">
<title>Toward Planet-Wide Traffic Camera Calibration. (arXiv:2311.04243v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04243</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the widespread deployment of outdoor cameras, their potential for
automated analysis remains largely untapped due, in part, to calibration
challenges. The absence of precise camera calibration data, including intrinsic
and extrinsic parameters, hinders accurate real-world distance measurements
from captured videos. To address this, we present a scalable framework that
utilizes street-level imagery to reconstruct a metric 3D model, facilitating
precise calibration of in-the-wild traffic cameras. Notably, our framework
achieves 3D scene reconstruction and accurate localization of over 100 global
traffic cameras and is scalable to any camera with sufficient street-level
imagery. For evaluation, we introduce a dataset of 20 fully calibrated traffic
cameras, demonstrating our method&apos;s significant enhancements over existing
automatic calibration techniques. Furthermore, we highlight our approach&apos;s
utility in traffic analysis by extracting insights via 3D vehicle
reconstruction and speed measurement, thereby opening up the potential of using
outdoor cameras for automated analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_K/0/1/0/all/0/1&quot;&gt;Khiem Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamburo_R/0/1/0/all/0/1&quot;&gt;Robert Tamburo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1&quot;&gt;Srinivasa G. Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04246">
<title>ADFactory: Automated Data Factory for Optical Flow Tasks. (arXiv:2311.04246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge faced by current optical flow methods is the difficulty in
generalizing them well into the real world, mainly due to the high production
cost of datasets, which currently do not have a large real-world optical flow
dataset. To address this challenge, we introduce a novel optical flow training
framework that can efficiently train optical flow networks on the target data
domain without manual annotation. Specifically, we use advanced Nerf technology
to reconstruct scenes from photo groups collected by monocular cameras, and
calculate the optical flow results between camera pose pairs from the rendered
results. On this basis, we screen the generated training data from various
aspects such as Nerf&apos;s reconstruction quality, visual consistency of optical
flow labels, reconstruction depth consistency, etc. The filtered training data
can be directly used for network supervision. Experimentally, the
generalization ability of our scheme on KITTI surpasses existing
self-supervised optical flow and monocular scene flow algorithms. Moreover, it
can always surpass most supervised methods in real-world zero-point
generalization evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Han Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04251">
<title>MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters. (arXiv:2311.04251v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04251</link>
<description rdf:parseType="Literal">&lt;p&gt;Most deep neural networks are trained under fixed network architectures and
require retraining when the architecture changes. If expanding the network&apos;s
size is needed, it is necessary to retrain from scratch, which is expensive. To
avoid this, one can grow from a small network by adding random weights over
time to gradually achieve the target network size. However, this naive approach
falls short in practice as it brings too much noise to the growing process.
Prior work tackled this issue by leveraging the already learned weights and
training data for generating new weights through conducting a computationally
expensive analysis step. In this paper, we introduce MixtureGrowth, a new
approach to growing networks that circumvents the initialization overhead in
prior work. Before growing, each layer in our model is generated with a linear
combination of parameter templates. Newly grown layer weights are generated by
using a new linear combination of existing templates for a layer. On one hand,
these templates are already trained for the task, providing a strong
initialization. On the other, the new coefficients provide flexibility for the
added layer weights to learn something new. We show that our approach boosts
top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet
datasets, while achieving comparable performance with fewer FLOPs to a larger
network trained from scratch. Code is available at
https://github.com/chaudatascience/mixturegrowth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1&quot;&gt;Chau Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teterwak_P/0/1/0/all/0/1&quot;&gt;Piotr Teterwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nelson_S/0/1/0/all/0/1&quot;&gt;Soren Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04257">
<title>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration. (arXiv:2311.04257v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.04257</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) have demonstrated impressive
instruction abilities across various open-ended tasks. However, previous
methods primarily focus on enhancing multi-modal capabilities. In this work, we
introduce a versatile multi-modal large language model, mPLUG-Owl2, which
effectively leverages modality collaboration to improve performance in both
text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design,
with the language decoder acting as a universal interface for managing
different modalities. Specifically, mPLUG-Owl2 incorporates shared functional
modules to facilitate modality collaboration and introduces a modality-adaptive
module that preserves modality-specific features. Extensive experiments reveal
that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal
tasks and achieving state-of-the-art performances with a single generic model.
Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality
collaboration phenomenon in both pure-text and multi-modal scenarios, setting a
pioneering path in the development of future multi-modal foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jiabo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haowei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1&quot;&gt;Qi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04260">
<title>Fully Automated Task Management for Generation, Execution, and Evaluation: A Framework for Fetch-and-Carry Tasks with Natural Language Instructions in Continuous Space. (arXiv:2311.04260v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.04260</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to develop a framework that enables a robot to execute tasks
based on visual information, in response to natural language instructions for
Fetch-and-Carry with Object Grounding (FCOG) tasks. Although there have been
many frameworks, they usually rely on manually given instruction sentences.
Therefore, evaluations have only been conducted with fixed tasks. Furthermore,
many multimodal language understanding models for the benchmarks only consider
discrete actions. To address the limitations, we propose a framework for the
full automation of the generation, execution, and evaluation of FCOG tasks. In
addition, we introduce an approach to solving the FCOG tasks by dividing them
into four distinct subtasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1&quot;&gt;Motonari Kambara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1&quot;&gt;Komei Sugiura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04261">
<title>Restoration of Analog Videos Using Swin-UNet. (arXiv:2311.04261v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04261</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a system to restore analog videos of historical
archives. These videos often contain severe visual degradation due to the
deterioration of their tape supports that require costly and slow manual
interventions to recover the original content. The proposed system uses a
multi-frame approach and is able to deal with severe tape mistracking, which
results in completely scrambled frames. Tests on real-world videos from a major
historical video archive show the effectiveness of our demo system. The code
and the pre-trained model are publicly available at
https://github.com/miccunifi/analog-video-restoration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnolucci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Agnolucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galteri_L/0/1/0/all/0/1&quot;&gt;Leonardo Galteri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04262">
<title>ETDPC: A Multimodality Framework for Classifying Pages in Electronic Theses and Dissertations. (arXiv:2311.04262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04262</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic theses and dissertations (ETDs) have been proposed, advocated, and
generated for more than 25 years. Although ETDs are hosted by commercial or
institutional digital library repositories, they are still an understudied type
of scholarly big data, partially because they are usually longer than
conference proceedings and journals. Segmenting ETDs will allow researchers to
study sectional content. Readers can navigate to particular pages of interest,
discover, and explore the content buried in these long documents. Most existing
frameworks on document page classification are designed for classifying general
documents and perform poorly on ETDs. In this paper, we propose ETDPC. Its
backbone is a two-stream multimodal model with a cross-attention network to
classify ETD pages into 13 categories. To overcome the challenge of imbalanced
labeled samples, we augmented data for minority categories and employed a
hierarchical classifier. ETDPC outperforms the state-of-the-art models in all
categories, achieving an F1 of 0.84 -- 0.96 for 9 out of 13 categories. We also
demonstrated its data efficiency. The code and data can be found on GitHub
(https://github.com/lamps-lab/ETDMiner/tree/master/etd_segmentation).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1&quot;&gt;Muntabir Hasan Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salsabil_L/0/1/0/all/0/1&quot;&gt;Lamia Salsabil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1&quot;&gt;William A. Ingram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Edward A. Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04263">
<title>Perceptual Quality Improvement in Videoconferencing using Keyframes-based GAN. (arXiv:2311.04263v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04263</link>
<description rdf:parseType="Literal">&lt;p&gt;In the latest years, videoconferencing has taken a fundamental role in
interpersonal relations, both for personal and business purposes. Lossy video
compression algorithms are the enabling technology for videoconferencing, as
they reduce the bandwidth required for real-time video streaming. However,
lossy video compression decreases the perceived visual quality. Thus, many
techniques for reducing compression artifacts and improving video visual
quality have been proposed in recent years. In this work, we propose a novel
GAN-based method for compression artifacts reduction in videoconferencing.
Given that, in this context, the speaker is typically in front of the camera
and remains the same for the entire duration of the transmission, we can
maintain a set of reference keyframes of the person from the higher-quality
I-frames that are transmitted within the video stream and exploit them to guide
the visual quality improvement; a novel aspect of this approach is the update
policy that maintains and updates a compact and effective set of reference
keyframes. First, we extract multi-scale features from the compressed and
reference frames. Then, our architecture combines these features in a
progressive manner according to facial landmarks. This allows the restoration
of the high-frequency details lost after the video compression. Experiments
show that the proposed approach improves visual quality and generates
photo-realistic results even with high compression rates. Code and pre-trained
networks are publicly available at
https://github.com/LorenzoAgnolucci/Keyframes-GAN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnolucci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Agnolucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galteri_L/0/1/0/all/0/1&quot;&gt;Leonardo Galteri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04287">
<title>Holistic Evaluation of Text-To-Image Models. (arXiv:2311.04287v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04287</link>
<description rdf:parseType="Literal">&lt;p&gt;The stunning qualitative improvement of recent text-to-image models has led
to their widespread attention and adoption. However, we lack a comprehensive
quantitative understanding of their capabilities and risks. To fill this gap,
we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models
(HEIM). Whereas previous evaluations focus mostly on text-image alignment and
image quality, we identify 12 aspects, including text-image alignment, image
quality, aesthetics, originality, reasoning, knowledge, bias, toxicity,
fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios
encompassing these aspects and evaluate 26 state-of-the-art text-to-image
models on this benchmark. Our results reveal that no single model excels in all
aspects, with different models demonstrating different strengths. We release
the generated images and human evaluation results for full transparency at
https://crfm.stanford.edu/heim/v1.1.0 and the code at
https://github.com/stanford-crfm/helm, which is integrated with the HELM
codebase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tony Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1&quot;&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1&quot;&gt;Chenlin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_Y/0/1/0/all/0/1&quot;&gt;Yifan Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Joon Sung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Agrim Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1&quot;&gt;Deepak Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Benita Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minguk Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taesung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04301">
<title>Class-Incremental Continual Learning for General Purpose Healthcare Models. (arXiv:2311.04301v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04301</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare clinics regularly encounter dynamic data that changes due to
variations in patient populations, treatment policies, medical devices, and
emerging disease patterns. Deep learning models can suffer from catastrophic
forgetting when fine-tuned in such scenarios, causing poor performance on
previously learned tasks. Continual learning allows learning on new tasks
without performance drop on previous tasks. In this work, we investigate the
performance of continual learning models on four different medical imaging
scenarios involving ten classification datasets from diverse modalities,
clinical specialties, and hospitals. We implement various continual learning
approaches and evaluate their performance in these scenarios. Our results
demonstrate that a single model can sequentially learn new tasks from different
specialties and achieve comparable performance to naive methods. These findings
indicate the feasibility of recycling or sharing models across the same or
different medical specialties, offering another step towards the development of
general-purpose medical imaging AI that can be shared across institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amritpal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurbuz_M/0/1/0/all/0/1&quot;&gt;Mustafa Burak Gurbuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gantha_S/0/1/0/all/0/1&quot;&gt;Shiva Souhith Gantha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jasti_P/0/1/0/all/0/1&quot;&gt;Prahlad Jasti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04315">
<title>A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization. (arXiv:2311.04315v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04315</link>
<description rdf:parseType="Literal">&lt;p&gt;Large text-to-image models have revolutionized the ability to generate
imagery using natural language. However, particularly unique or personal visual
concepts, such as your pet, an object in your house, etc., will not be captured
by the original model. This has led to interest in how to inject new visual
concepts, bound to a new text token, using as few as 4-6 examples. Despite
significant progress, this task remains a formidable challenge, particularly in
preserving the subject&apos;s identity. While most researchers attempt to to address
this issue by modifying model architectures, our approach takes a data-centric
perspective, advocating the modification of data rather than the model itself.
We introduce a novel regularization dataset generation strategy on both the
text and image level; demonstrating the importance of a rich and structured
regularization dataset (automatically generated) to prevent losing text
coherence and better identity preservation. The better quality is enabled by
allowing up to 5x more fine-tuning iterations without overfitting and
degeneration. The generated renditions of the desired subject preserve even
fine details such as text and logos; all while maintaining the ability to
generate diverse samples that follow the input text prompt. Since our method
focuses on data augmentation, rather than adjusting the model architecture, it
is complementary and can be combined with prior work. We show on established
benchmarks that our data-centric approach forms the new state of the art in
terms of image quality, with the best trade-off between identity preservation,
diversity, and text alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xingzhe He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1&quot;&gt;Nicholas Kolkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lantao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1&quot;&gt;Helge Rhodin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalarot_R/0/1/0/all/0/1&quot;&gt;Ratheesh Kalarot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04336">
<title>Efficient Semantic Matching with Hypercolumn Correlation. (arXiv:2311.04336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04336</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies show that leveraging the match-wise relationships within the
4D correlation map yields significant improvements in establishing semantic
correspondences - but at the cost of increased computation and latency. In this
work, we focus on the aspect that the performance improvements of recent
methods can also largely be attributed to the usage of multi-scale correlation
maps, which hold various information ranging from low-level geometric cues to
high-level semantic contexts. To this end, we propose HCCNet, an efficient yet
effective semantic matching method which exploits the full potential of
multi-scale correlation maps, while eschewing the reliance on expensive
match-wise relationship mining on the 4D correlation map. Specifically, HCCNet
performs feature slicing on the bottleneck features to yield a richer set of
intermediate features, which are used to construct a hypercolumn correlation.
HCCNet can consequently establish semantic correspondences in an effective
manner by reducing the volume of conventional high-dimensional convolution or
self-attention operations to efficient point-wise convolutions. HCCNet
demonstrates state-of-the-art or competitive performances on the standard
benchmarks of semantic matching, while incurring a notably lower latency and
computation overhead compared to the existing SoTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1&quot;&gt;Juhong Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsu Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04346">
<title>SaFL: Sybil-aware Federated Learning with Application to Face Recognition. (arXiv:2311.04346v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04346</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a machine learning paradigm to conduct
collaborative learning among clients on a joint model. The primary goal is to
share clients&apos; local training parameters with an integrating server while
preserving their privacy. This method permits to exploit the potential of
massive mobile users&apos; data for the benefit of machine learning models&apos;
performance while keeping sensitive data on local devices. On the downside, FL
raises security and privacy concerns that have just started to be studied. To
address some of the key threats in FL, researchers have proposed to use secure
aggregation methods (e.g. homomorphic encryption, secure multiparty
computation, etc.). These solutions improve some security and privacy metrics,
but at the same time bring about other serious threats such as poisoning
attacks, backdoor attacks, and free running attacks. This paper proposes a new
defense method against poisoning attacks in FL called SaFL (Sybil-aware
Federated Learning) that minimizes the effect of sybils with a novel
time-variant aggregation scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghafourian_M/0/1/0/all/0/1&quot;&gt;Mahdi Ghafourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1&quot;&gt;Julian Fierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1&quot;&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1&quot;&gt;Aythami Morales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04351">
<title>A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders. (arXiv:2311.04351v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04351</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research we propose a deep learning approach for detecting anomalies
in videos using convolutional autoencoder and decoder neural networks on the
UCSD dataset.Our method utilizes a convolutional autoencoder to learn the
spatiotemporal patterns of normal videos and then compares each frame of a test
video to this learned representation. We evaluated our approach on the UCSD
dataset and achieved an overall accuracy of 99.35% on the Ped1 dataset and
99.77% on the Ped2 dataset, demonstrating the effectiveness of our method for
detecting anomalies in surveillance videos. The results show that our method
outperforms other state-of-the-art methods, and it can be used in real-world
applications for video anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavuluri_G/0/1/0/all/0/1&quot;&gt;Gopikrishna Pavuluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Annem_G/0/1/0/all/0/1&quot;&gt;Gayathri Annem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04382">
<title>Basis restricted elastic shape analysis on the space of unregistered surfaces. (arXiv:2311.04382v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04382</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new mathematical and numerical framework for surface
analysis derived from the general setting of elastic Riemannian metrics on
shape spaces. Traditionally, those metrics are defined over the infinite
dimensional manifold of immersed surfaces and satisfy specific invariance
properties enabling the comparison of surfaces modulo shape preserving
transformations such as reparametrizations. The specificity of the approach we
develop is to restrict the space of allowable transformations to predefined
finite dimensional bases of deformation fields. These are estimated in a
data-driven way so as to emulate specific types of surface transformations
observed in a training set. The use of such bases allows to simplify the
representation of the corresponding shape space to a finite dimensional latent
space. However, in sharp contrast with methods involving e.g. mesh
autoencoders, the latent space is here equipped with a non-Euclidean Riemannian
metric precisely inherited from the family of aforementioned elastic metrics.
We demonstrate how this basis restricted model can be then effectively
implemented to perform a variety of tasks on surface meshes which, importantly,
does not assume these to be pre-registered (i.e. with given point
correspondences) or to even have a consistent mesh structure. We specifically
validate our approach on human body shape and pose data as well as human face
scans, and show how it generally outperforms state-of-the-art methods on
problems such as shape registration, interpolation, motion transfer or random
pose generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartman_E/0/1/0/all/0/1&quot;&gt;Emmanuel Hartman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1&quot;&gt;Emery Pierson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Martin Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1&quot;&gt;Mohamed Daoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charon_N/0/1/0/all/0/1&quot;&gt;Nicolas Charon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04391">
<title>3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features. (arXiv:2311.04391v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;We present 3DiffTection, a state-of-the-art method for 3D object detection
from single images, leveraging features from a 3D-aware diffusion model.
Annotating large-scale image data for 3D detection is resource-intensive and
time-consuming. Recently, pretrained large image diffusion models have become
prominent as effective feature extractors for 2D perception tasks. However,
these features are initially trained on paired text and image data, which are
not optimized for 3D tasks, and often exhibit a domain gap when applied to the
target data. Our approach bridges these gaps through two specialized tuning
strategies: geometric and semantic. For geometric tuning, we fine-tune a
diffusion model to perform novel view synthesis conditioned on a single image,
by introducing a novel epipolar warp operator. This task meets two essential
criteria: the necessity for 3D awareness and reliance solely on posed image
data, which are readily available (e.g., from videos) and does not require
manual annotation. For semantic refinement, we further train the model on
target data with detection supervision. Both tuning phases employ ControlNet to
preserve the integrity of the original feature capabilities. In the final step,
we harness these enhanced capabilities to conduct a test-time prediction
ensemble across multiple virtual viewpoints. Through our methodology, we obtain
3D-aware features that are tailored for 3D detection and excel in identifying
cross-view point correspondences. Consequently, our model emerges as a powerful
3D detector, substantially surpassing previous benchmarks, e.g., Cube-RCNN, a
precedent in single-view 3D detection by 9.43\% in AP3D on the
Omni3D-ARkitscene dataset. Furthermore, 3DiffTection showcases robust data
efficiency and generalization to cross-domain data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1&quot;&gt;Or Litany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04400">
<title>LRM: Large Reconstruction Model for Single Image to 3D. (arXiv:2311.04400v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04400</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the first Large Reconstruction Model (LRM) that predicts the 3D
model of an object from a single input image within just 5 seconds. In contrast
to many previous methods that are trained on small-scale datasets such as
ShapeNet in a category-specific fashion, LRM adopts a highly scalable
transformer-based architecture with 500 million learnable parameters to
directly predict a neural radiance field (NeRF) from the input image. We train
our model in an end-to-end manner on massive multi-view data containing around
1 million objects, including both synthetic renderings from Objaverse and real
captures from MVImgNet. This combination of a high-capacity model and
large-scale training data empowers our model to be highly generalizable and
produce high-quality 3D reconstructions from various testing inputs including
real-world in-the-wild captures and images from generative models. Video demos
and interactable 3D meshes can be found on this website:
https://yiconghong.me/LRM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yicong Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiuxiang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sai Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Difan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1&quot;&gt;Kalyan Sunkavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1&quot;&gt;Trung Bui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04414">
<title>Learning the What and How of Annotation in Video Object Segmentation. (arXiv:2311.04414v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04414</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Object Segmentation (VOS) is crucial for several applications, from
video editing to video data generation. Training a VOS model requires an
abundance of manually labeled training videos. The de-facto traditional way of
annotating objects requires humans to draw detailed segmentation masks on the
target objects at each video frame. This annotation process, however, is
tedious and time-consuming. To reduce this annotation cost, in this paper, we
propose EVA-VOS, a human-in-the-loop annotation framework for video object
segmentation. Unlike the traditional approach, we introduce an agent that
predicts iteratively both which frame (&quot;What&quot;) to annotate and which annotation
type (&quot;How&quot;) to use. Then, the annotator annotates only the selected frame that
is used to update a VOS module, leading to significant gains in annotation
time. We conduct experiments on the MOSE and the DAVIS datasets and we show
that: (a) EVA-VOS leads to masks with accuracy close to the human agreement
3.5x faster than the standard way of annotating videos; (b) our frame selection
achieves state-of-the-art performance; (c) EVA-VOS yields significant
performance gains in terms of annotation time compared to all other methods and
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delatolas_T/0/1/0/all/0/1&quot;&gt;Thanos Delatolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1&quot;&gt;Vicky Kalogeiton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_D/0/1/0/all/0/1&quot;&gt;Dim P. Papadopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04430">
<title>Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression. (arXiv:2311.04430v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04430</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing video compression (VC) methods primarily aim to reduce the spatial
and temporal redundancies between consecutive frames in a video while
preserving its quality. In this regard, previous works have achieved remarkable
results on videos acquired under specific settings such as instant (known)
exposure time and shutter speed which often result in sharp videos. However,
when these methods are evaluated on videos captured under different temporal
priors, which lead to degradations like motion blur and low frame rate, they
fail to maintain the quality of the contents. In this work, we tackle the VC
problem in a general scenario where a given video can be blurry due to
predefined camera settings or dynamics in the scene. By exploiting the natural
trade-off between visual enhancement and data compression, we formulate VC as a
min-max optimization problem and propose an effective framework and training
strategy to tackle the problem. Extensive experimental results on several
benchmark datasets confirm the effectiveness of our method compared to several
state-of-the-art VC approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Argaw_D/0/1/0/all/0/1&quot;&gt;Dawit Mureja Argaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junsik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kweon_I/0/1/0/all/0/1&quot;&gt;In So Kweon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04442">
<title>SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification. (arXiv:2311.04442v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04442</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked image modeling (MIM) is a highly popular and effective self-supervised
learning method for image understanding. Existing MIM-based methods mostly
focus on spatial feature modeling, neglecting spectral feature modeling.
Meanwhile, existing MIM-based methods use Transformer for feature extraction,
some local or high-frequency information may get lost. To this end, we propose
a spatial-spectral masked auto-encoder (SS-MAE) for HSI and LiDAR/SAR data
joint classification. Specifically, SS-MAE consists of a spatial-wise branch
and a spectral-wise branch. The spatial-wise branch masks random patches and
reconstructs missing pixels, while the spectral-wise branch masks random
spectral channels and reconstructs missing channels. Our SS-MAE fully exploits
the spatial and spectral representations of the input data. Furthermore, to
complement local features in the training stage, we add two lightweight CNNs
for feature extraction. Both global and local features are taken into account
for feature modeling. To demonstrate the effectiveness of the proposed SS-MAE,
we conduct extensive experiments on three publicly available datasets.
Extensive experiments on three multi-source datasets verify the superiority of
our SS-MAE compared with several state-of-the-art baselines. The source codes
are available at \url{https://github.com/summitgao/SS-MAE}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Junyan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junyu Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1&quot;&gt;Qian Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04458">
<title>Retargeting video with an end-to-end framework. (arXiv:2311.04458v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04458</link>
<description rdf:parseType="Literal">&lt;p&gt;Video holds significance in computer graphics applications. Because of the
heterogeneous of digital devices, retargeting videos becomes an essential
function to enhance user viewing experience in such applications. In the
research of video retargeting, preserving the relevant visual content in
videos, avoiding flicking, and processing time are the vital challenges.
Extending image retargeting techniques to the video domain is challenging due
to the high running time. Prior work of video retargeting mainly utilizes
time-consuming preprocessing to analyze frames. Plus, being tolerant of
different video content, avoiding important objects from shrinking, and the
ability to play with arbitrary ratios are the limitations that need to be
resolved in these systems requiring investigation. In this paper, we present an
end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We
eliminate the computational bottleneck in the conventional approaches by
designing RETVI with two modules, content feature analyzer (CFA) and adaptive
deforming estimator (ADE). The extensive experiments and evaluations show that
our system outperforms previous work in quality and running time. Visit our
project website for more results at
$\href{&lt;a href=&quot;http://graphics.csie.ncku.edu.tw/RETVI&quot;&gt;this http URL&lt;/a&gt;}{&lt;a href=&quot;http://graphics.csie.ncku.edu.tw/RETVI&quot;&gt;this http URL&lt;/a&gt;}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thi-Ngoc-Hanh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;HuiGuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Ru Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Tong-Yee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04464">
<title>Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning. (arXiv:2311.04464v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04464</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning generalized representations from limited training samples is crucial
for applying deep neural networks in low-resource scenarios. Recently, methods
based on Contrastive Language-Image Pre-training (CLIP) have exhibited
promising performance in few-shot adaptation tasks. To avoid catastrophic
forgetting and overfitting caused by few-shot fine-tuning, existing works
usually freeze the parameters of CLIP pre-trained on large-scale datasets,
overlooking the possibility that some parameters might not be suitable for
downstream tasks. To this end, we revisit CLIP&apos;s visual encoder with a specific
focus on its distinctive attention pooling layer, which performs a spatial
weighted-sum of the dense feature maps. Given that dense feature maps contain
meaningful semantic information, and different semantics hold varying
importance for diverse downstream tasks (such as prioritizing semantics like
ears and eyes in pet classification tasks rather than side mirrors), using the
same weighted-sum operation for dense features across different few-shot tasks
might not be appropriate. Hence, we propose fine-tuning the parameters of the
attention pooling layer during the training process to encourage the model to
focus on task-specific semantics. In the inference process, we perform residual
blending between the features pooled by the fine-tuned and the original
attention pooling layers to incorporate both the few-shot knowledge and the
pre-trained CLIP&apos;s prior knowledge. We term this method as Semantic-Aware
FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot
CLIP and is compatible with the existing adapter approach (termed SAFE-A).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhigang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+lu_W/0/1/0/all/0/1&quot;&gt;Wang lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04473">
<title>All-Optical Phase Conjugation Using Diffractive Wavefront Processing. (arXiv:2311.04473v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2311.04473</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical phase conjugation (OPC) is a nonlinear technique used for
counteracting wavefront distortions, with various applications ranging from
imaging to beam focusing. Here, we present the design of a diffractive
wavefront processor to approximate all-optical phase conjugation operation for
input fields with phase aberrations. Leveraging deep learning, a set of passive
diffractive layers was optimized to all-optically process an arbitrary
phase-aberrated coherent field from an input aperture, producing an output
field with a phase distribution that is the conjugate of the input wave. We
experimentally validated the efficacy of this wavefront processor by 3D
fabricating diffractive layers trained using deep learning and performing OPC
on phase distortions never seen by the diffractive processor during its
training. Employing terahertz radiation, our physical diffractive processor
successfully performed the OPC task through a shallow spatially-engineered
volume that axially spans tens of wavelengths. In addition to this transmissive
OPC configuration, we also created a diffractive phase-conjugate mirror by
combining deep learning-optimized diffractive layers with a standard mirror.
Given its compact, passive and scalable nature, our diffractive wavefront
processor can be used for diverse OPC-related applications, e.g., turbidity
suppression and aberration correction, and is also adaptable to different parts
of the electromagnetic spectrum, especially those where cost-effective
wavefront engineering solutions do not exist.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Che-Yung Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gan_T/0/1/0/all/0/1&quot;&gt;Tianyi Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jarrahi_M/0/1/0/all/0/1&quot;&gt;Mona Jarrahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1&quot;&gt;Aydogan Ozcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04480">
<title>CLearViD: Curriculum Learning for Video Description. (arXiv:2311.04480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04480</link>
<description rdf:parseType="Literal">&lt;p&gt;Video description entails automatically generating coherent natural language
sentences that narrate the content of a given video. We introduce CLearViD, a
transformer-based model for video description generation that leverages
curriculum learning to accomplish this task. In particular, we investigate two
curriculum strategies: (1) progressively exposing the model to more challenging
samples by gradually applying a Gaussian noise to the video data, and (2)
gradually reducing the capacity of the network through dropout during the
training process. These methods enable the model to learn more robust and
generalizable features. Moreover, CLearViD leverages the Mish activation
function, which provides non-linearity and non-monotonicity and helps alleviate
the issue of vanishing gradients. Our extensive experiments and ablation
studies demonstrate the effectiveness of the proposed model. The results on two
datasets, namely ActivityNet Captions and YouCook2, show that CLearViD
significantly outperforms existing state-of-the-art models in terms of both
accuracy and diversity metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yu Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazli_P/0/1/0/all/0/1&quot;&gt;Pooyan Fazli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04494">
<title>Non-Rigid Shape Registration via Deep Functional Maps Prior. (arXiv:2311.04494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04494</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a learning-based framework for non-rigid shape
registration without correspondence supervision. Traditional shape registration
techniques typically rely on correspondences induced by extrinsic proximity,
therefore can fail in the presence of large intrinsic deformations. Spectral
mapping methods overcome this challenge by embedding shapes into, geometric or
learned, high-dimensional spaces, where shapes are easier to align. However,
due to the dependency on abstract, non-linear embedding schemes, the latter can
be vulnerable with respect to perturbed or alien input. In light of this, our
framework takes the best of both worlds. Namely, we deform source mesh towards
the target point cloud, guided by correspondences induced by high-dimensional
embeddings learned from deep functional maps (DFM). In particular, the
correspondences are dynamically updated according to the intermediate
registrations and filtered by consistency prior, which prominently robustify
the overall pipeline. Moreover, in order to alleviate the requirement of
extrinsically aligned input, we train an orientation regressor on a set of
aligned synthetic shapes independent of the training shapes for DFM. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching, but also delivers high-quality
correspondences between unseen challenging shape pairs that undergo both
significant extrinsic and intrinsic deformations, in which case neither
traditional registration methods nor intrinsic methods work. The code is
available at https://github.com/rqhuang88/DFR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Puhua Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mingze Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruqi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04496">
<title>PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders. (arXiv:2311.04496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04496</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training is playing an increasingly important role in learning generic
feature representation for Person Re-identification (ReID). We argue that a
high-quality ReID representation should have three properties, namely,
multi-level awareness, occlusion robustness, and cross-region invariance. To
this end, we propose a simple yet effective pre-training framework, namely
PersonMAE, which involves two core designs into masked autoencoders to better
serve the task of Person Re-ID. 1) PersonMAE generates two regions from the
given image with RegionA as the input and \textit{RegionB} as the prediction
target. RegionA is corrupted with block-wise masking to mimic common occlusion
in ReID and its remaining visible parts are fed into the encoder. 2) Then
PersonMAE aims to predict the whole RegionB at both pixel level and semantic
feature level. It encourages its pre-trained feature representations with the
three properties mentioned above. These properties make PersonMAE compatible
with downstream Person ReID tasks, leading to state-of-the-art performance on
four downstream ReID tasks, i.e., supervised (holistic and occluded setting),
and unsupervised (UDA and USL setting). Notably, on the commonly adopted
supervised setting, PersonMAE with ViT-B backbone achieves 79.8% and 69.5% mAP
on the MSMT17 and OccDuke datasets, surpassing the previous state-of-the-art by
a large margin of +8.0 mAP, and +5.3 mAP, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hezhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1&quot;&gt;Jianmin Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04498">
<title>NExT-Chat: An LMM for Chat, Detection and Segmentation. (arXiv:2311.04498v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04498</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models (LLMs) has greatly advanced the
field of multimodal understanding, leading to the emergence of large multimodal
models (LMMs). In order to enhance the level of visual comprehension, recent
studies have equipped LMMs with region-level understanding capabilities by
representing object bounding box coordinates as a series of text sequences
(pixel2seq). In this paper, we introduce a novel paradigm for object location
modeling called pixel2emb method, where we ask the LMM to output the location
embeddings and then decoded by different decoders. This paradigm allows for
different location formats (such as bounding boxes and masks) to be used in
multimodal conversations Furthermore, this kind of embedding based location
modeling enables the utilization of existing practices in localization tasks,
such as detection and segmentation. In scenarios with limited resources, our
pixel2emb demonstrates superior performance compared to existing
state-of-the-art (SOTA) approaches in both the location input and output tasks
under fair comparison. Leveraging the proposed pixel2emb method, we train an
LMM named NExT-Chat and demonstrate its capability of handling multiple tasks
like visual grounding, region caption, and grounded reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chen-Wei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04501">
<title>PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds. (arXiv:2311.04501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04501</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training is crucial in 3D-related fields such as autonomous driving where
point cloud annotation is costly and challenging. Many recent studies on point
cloud pre-training, however, have overlooked the issue of incompleteness, where
only a fraction of the points are captured by LiDAR, leading to ambiguity
during the training phase. On the other hand, images offer more comprehensive
information and richer semantics that can bolster point cloud encoders in
addressing the incompleteness issue inherent in point clouds. Yet,
incorporating images into point cloud pre-training presents its own challenges
due to occlusions, potentially causing misalignments between points and pixels.
In this work, we propose PRED, a novel image-assisted pre-training framework
for outdoor point clouds in an occlusion-aware manner. The main ingredient of
our framework is a Birds-Eye-View (BEV) feature map conditioned semantic
rendering, leveraging the semantics of images for supervision through neural
rendering. We further enhance our model&apos;s performance by incorporating
point-wise masking with a high mask ratio (95%). Extensive experiments
demonstrate PRED&apos;s superiority over prior point cloud pre-training methods,
providing significant improvements on various large-scale datasets for 3D
perception tasks. Codes will be available at https://github.com/PRED4pc/PRED.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Di Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04505">
<title>NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction. (arXiv:2311.04505v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04505</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye contact is a crucial non-verbal interaction modality and plays an
important role in our everyday social life. While humans are very sensitive to
eye contact, the capabilities of machines to capture a person&apos;s gaze are still
mediocre. We tackle this challenge and present NITEC, a hand-annotated eye
contact dataset for ego-vision interaction. NITEC exceeds existing datasets for
ego-vision eye contact in size and variety of demographics, social contexts,
and lighting conditions, making it a valuable resource for advancing
ego-vision-based eye contact research. Our extensive evaluations on NITEC
demonstrate strong cross-dataset performance, emphasizing its effectiveness and
adaptability in various scenarios, that allows seamless utilization to the
fields of computer vision, human-computer interaction, and social robotics. We
make our NITEC dataset publicly available to foster reproducibility and further
exploration in the field of ego-vision interaction.
https://github.com/thohemp/nitec
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hempel_T/0/1/0/all/0/1&quot;&gt;Thorsten Hempel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Magnus Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelrahman_A/0/1/0/all/0/1&quot;&gt;Ahmed A. Abdelrahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hamadi_A/0/1/0/all/0/1&quot;&gt;Ayoub Al-Hamadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04509">
<title>Learning Discriminative Features for Crowd Counting. (arXiv:2311.04509v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04509</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowd counting models in highly congested areas confront two main challenges:
weak localization ability and difficulty in differentiating between foreground
and background, leading to inaccurate estimations. The reason is that objects
in highly congested areas are normally small and high-level features extracted
by convolutional neural networks are less discriminative to represent small
objects. To address these problems, we propose a learning discriminative
features framework for crowd counting, which is composed of a masked feature
prediction module (MPM) and a supervised pixel-level contrastive learning
module (CLM). The MPM randomly masks feature vectors in the feature map and
then reconstructs them, allowing the model to learn about what is present in
the masked regions and improving the model&apos;s ability to localize objects in
high-density regions. The CLM pulls targets close to each other and pushes them
far away from background in the feature space, enabling the model to
discriminate foreground objects from background. Additionally, the proposed
modules can be beneficial in various computer vision tasks, such as crowd
counting and object detection, where dense scenes or cluttered environments
pose challenges to accurate localization. The proposed two modules are
plug-and-play, incorporating the proposed modules into existing models can
potentially boost their performance in these scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuehai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04512">
<title>FFINet: Future Feedback Interaction Network for Motion Forecasting. (arXiv:2311.04512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04512</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion forecasting plays a crucial role in autonomous driving, with the aim
of predicting the future reasonable motions of traffic agents. Most existing
methods mainly model the historical interactions between agents and the
environment, and predict multi-modal trajectories in a feedforward process,
ignoring potential trajectory changes caused by future interactions between
agents. In this paper, we propose a novel Future Feedback Interaction Network
(FFINet) to aggregate features the current observations and potential future
interactions for trajectory prediction. Firstly, we employ different
spatial-temporal encoders to embed the decomposed position vectors and the
current position of each scene, providing rich features for the subsequent
cross-temporal aggregation. Secondly, the relative interaction and
cross-temporal aggregation strategies are sequentially adopted to integrate
features in the current fusion module, observation interaction module, future
feedback module and global fusion module, in which the future feedback module
can enable the understanding of pre-action by feeding the influence of preview
information to feedforward prediction. Thirdly, the comprehensive interaction
features are further fed into final predictor to generate the joint predicted
trajectories of multiple agents. Extensive experimental results show that our
FFINet achieves the state-of-the-art performance on Argoverse 1 and Argoverse 2
motion forecasting benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Miao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sanping Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1&quot;&gt;Ke Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jingjing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04521">
<title>Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images. (arXiv:2311.04521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04521</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an improved solution to the neural image-based rendering problem
in computer vision. Given a set of images taken from a freely moving camera at
train time, the proposed approach could synthesize a realistic image of the
scene from a novel viewpoint at test time. The key ideas presented in this
paper are (i) Recovering accurate camera parameters via a robust pipeline from
unposed day-to-day images is equally crucial in neural novel view synthesis
problem; (ii) It is rather more practical to model object&apos;s content at
different resolutions since dramatic camera motion is highly likely in
day-to-day unposed images. To incorporate the key ideas, we leverage the
fundamentals of scene rigidity, multi-scale neural scene representation, and
single-image depth prediction. Concretely, the proposed approach makes the
camera parameters as learnable in a neural fields-based modeling framework. By
assuming per view depth prediction is given up to scale, we constrain the
relative pose between successive frames. From the relative poses, absolute
camera pose estimation is modeled via a graph-neural network-based multiple
motion averaging within the multi-scale neural-fields network, leading to a
single loss function. Optimizing the introduced loss function provides camera
intrinsic, extrinsic, and image rendering from unposed images. We demonstrate,
with examples, that for a unified framework to accurately model multiscale
neural scene representation from day-to-day acquired unposed multi-view images,
it is equally essential to have precise camera-pose estimates within the scene
representation framework. Without considering robustness measures in the camera
pose estimation pipeline, modeling for multi-scale aliasing artifacts can be
counterproductive. We present extensive experiments on several benchmark
datasets to demonstrate the suitability of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1&quot;&gt;Nishant Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04552">
<title>A 3D generative model of pathological multi-modal MR images and segmentations. (arXiv:2311.04552v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04552</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative modelling and synthetic data can be a surrogate for real medical
imaging datasets, whose scarcity and difficulty to share can be a nuisance when
delivering accurate deep learning models for healthcare applications. In recent
years, there has been an increased interest in using these models for data
augmentation and synthetic data sharing, using architectures such as generative
adversarial networks (GANs) or diffusion models (DMs). Nonetheless, the
application of synthetic data to tasks such as 3D magnetic resonance imaging
(MRI) segmentation remains limited due to the lack of labels associated with
the generated images. Moreover, many of the proposed generative MRI models lack
the ability to generate arbitrary modalities due to the absence of explicit
contrast conditioning. These limitations prevent the user from adjusting the
contrast and content of the images and obtaining more generalisable data for
training task-specific models. In this work, we propose brainSPADE3D, a 3D
generative model for brain MRI and associated segmentations, where the user can
condition on specific pathological phenotypes and contrasts. The proposed joint
imaging-segmentation generative model is shown to generate high-fidelity
synthetic images and associated segmentations, with the ability to combine
pathologies. We demonstrate how the model can alleviate issues with
segmentation model performance when unexpected pathologies are present in the
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fernandez_V/0/1/0/all/0/1&quot;&gt;Virginia Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pinaya_W/0/1/0/all/0/1&quot;&gt;Walter Hugo Lopez Pinaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Borges_P/0/1/0/all/0/1&quot;&gt;Pedro Borges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Graham_M/0/1/0/all/0/1&quot;&gt;Mark S. Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1&quot;&gt;M. Jorge Cardoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04584">
<title>Weakly-supervised deepfake localization in diffusion-generated images. (arXiv:2311.04584v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04584</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable generative capabilities of denoising diffusion models have
raised new concerns regarding the authenticity of the images we see every day
on the Internet. However, the vast majority of existing deepfake detection
models are tested against previous generative approaches (e.g. GAN) and usually
provide only a &quot;fake&quot; or &quot;real&quot; label per image. We believe a more informative
output would be to augment the per-image label with a localization map
indicating which regions of the input have been manipulated. To this end, we
frame this task as a weakly-supervised localization problem and identify three
main categories of methods (based on either explanations, local scores or
attention), which we compare on an equal footing by using the Xception network
as the common backbone architecture. We provide a careful analysis of all the
main factors that parameterize the design space: choice of method, type of
supervision, dataset and generator used in the creation of manipulated images;
our study is enabled by constructing datasets in which only one of the
components is varied. Our results show that weakly-supervised localization is
attainable, with the best performing detection method (based on local scores)
being less sensitive to the looser supervision than to the mismatch in terms of
dataset or generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tantaru_D/0/1/0/all/0/1&quot;&gt;Dragos Tantaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oneata_E/0/1/0/all/0/1&quot;&gt;Elisabeta Oneata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oneata_D/0/1/0/all/0/1&quot;&gt;Dan Oneata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04588">
<title>Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection. (arXiv:2311.04588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04588</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) models become vulnerable to Model Stealing Attacks
(MSA) when they are deployed as a service. In such attacks, the deployed model
is queried repeatedly to build a labelled dataset. This dataset allows the
attacker to train a thief model that mimics the original model. To maximize
query efficiency, the attacker has to select the most informative subset of
data points from the pool of available data. Existing attack strategies utilize
approaches like Active Learning and Semi-Supervised learning to minimize costs.
However, in the black-box setting, these approaches may select sub-optimal
samples as they train only one thief model. Depending on the thief model&apos;s
capacity and the data it was pretrained on, the model might even select noisy
samples that harm the learning process. In this work, we explore the usage of
an ensemble of deep learning models as our thief model. We call our attack Army
of Thieves(AOT) as we train multiple models with varying complexities to
leverage the crowd&apos;s wisdom. Based on the ensemble&apos;s collective decision,
uncertain samples are selected for querying, while the most confident samples
are directly included in the training data. Our approach is the first one to
utilize an ensemble of thief models to perform model extraction. We outperform
the base approaches of existing state-of-the-art methods by at least 3% and
achieve a 21% higher adversarial sample transferability than previous work for
models trained on the CIFAR-10 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1&quot;&gt;Akshit Jindal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1&quot;&gt;Vikram Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1&quot;&gt;Saket Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1&quot;&gt;Chetan Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04591">
<title>Rethinking Event-based Human Pose Estimation with 3D Event Representations. (arXiv:2311.04591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04591</link>
<description rdf:parseType="Literal">&lt;p&gt;Human pose estimation is a critical component in autonomous driving and
parking, enhancing safety by predicting human actions. Traditional frame-based
cameras and videos are commonly applied, yet, they become less reliable in
scenarios under high dynamic range or heavy motion blur. In contrast, event
cameras offer a robust solution for navigating these challenging contexts.
Predominant methodologies incorporate event cameras into learning frameworks by
accumulating events into event frames. However, such methods tend to
marginalize the intrinsic asynchronous and high temporal resolution
characteristics of events. This disregard leads to a loss in essential temporal
dimension data, crucial for safety-critical tasks associated with dynamic human
activities. To address this issue and to unlock the 3D potential of event
information, we introduce two 3D event representations: the Rasterized Event
Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC collates
events within concise temporal slices at identical positions, preserving 3D
attributes with statistical cues and markedly mitigating memory and
computational demands. Meanwhile, the DEV representation discretizes events
into voxels and projects them across three orthogonal planes, utilizing
decoupled event attention to retrieve 3D cues from the 2D planes. Furthermore,
we develop and release EV-3DPW, a synthetic event-based dataset crafted to
facilitate training and quantitative analysis in outdoor scenes. On the public
real-world DHP19 dataset, our event point cloud technique excels in real-time
mobile predictions, while the decoupled event voxel method achieves the highest
accuracy. Experiments reveal our proposed 3D representation methods&apos; superior
generalization capacities against traditional RGB images and event frame
techniques. Our code and dataset are available at
https://github.com/MasterHow/EventPointPose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yaozu Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Huajian Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04592">
<title>On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology. (arXiv:2311.04592v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04592</link>
<description rdf:parseType="Literal">&lt;p&gt;We study how the topology of feature embedding space changes as it passes
through the layers of a well-trained deep neural network (DNN) through Betti
numbers. Motivated by existing studies using simplicial complexes on shallow
fully connected networks (FCN), we present an extended analysis using Cubical
homology instead, with a variety of popular deep architectures and real image
datasets. We demonstrate that as depth increases, a topologically complicated
dataset is transformed into a simple one, resulting in Betti numbers attaining
their lowest possible value. The rate of decay in topological complexity (as a
metric) helps quantify the impact of architectural choices on the
generalization ability. Interestingly from a representation learning
perspective, we highlight several invariances such as topological invariance of
(1) an architecture on similar datasets; (2) embedding space of a dataset for
architectures of variable depth; (3) embedding space to input resolution/size,
and (4) data sub-sampling. In order to further demonstrate the link between
expressivity \&amp;amp; the generalization capability of a network, we consider the
task of ranking pre-trained models for downstream classification task (transfer
learning). Compared to existing approaches, the proposed metric has a better
correlation to the actually achievable accuracy via fine-tuning the pre-trained
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1&quot;&gt;Suryaka Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_B/0/1/0/all/0/1&quot;&gt;Bishshoy Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrol_V/0/1/0/all/0/1&quot;&gt;Vinayak Abrol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sumantra Dutta Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04614">
<title>LuminanceL1Loss: A loss function which measures percieved brightness and colour differences. (arXiv:2311.04614v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04614</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce LuminanceL1Loss, a novel loss function designed to enhance the
performance of image restoration tasks. We demonstrate its superiority over MSE
when applied to the Retinexformer, BUIFD and DnCNN architectures. Our proposed
LuminanceL1Loss leverages a unique approach by transforming images into
grayscale and subsequently computing the MSE loss for both grayscale and color
channels. Experimental results demonstrate that this innovative loss function
consistently outperforms traditional methods, showcasing its potential in image
denoising and other related tasks in image reconstruction. It demonstrates
gains up to 4.7dB. The results presented in this study highlight the efficacy
of LuminanceL1Loss for various image restoration tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonge_D/0/1/0/all/0/1&quot;&gt;Dominic De Jonge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04617">
<title>Image Patch-Matching with Graph-Based Learning in Street Scenes. (arXiv:2311.04617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04617</link>
<description rdf:parseType="Literal">&lt;p&gt;Matching landmark patches from a real-time image captured by an on-vehicle
camera with landmark patches in an image database plays an important role in
various computer perception tasks for autonomous driving. Current methods focus
on local matching for regions of interest and do not take into account spatial
neighborhood relationships among the image patches, which typically correspond
to objects in the environment. In this paper, we construct a spatial graph with
the graph vertices corresponding to patches and edges capturing the spatial
neighborhood information. We propose a joint feature and metric learning model
with graph-based learning. We provide a theoretical basis for the graph-based
loss by showing that the information distance between the distributions
conditioned on matched and unmatched pairs is maximized under our framework. We
evaluate our model using several street-scene datasets and demonstrate that our
approach achieves state-of-the-art matching results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+She_R/0/1/0/all/0/1&quot;&gt;Rui She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qiyu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_W/0/1/0/all/0/1&quot;&gt;Wee Peng Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yong Liang Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navarro_D/0/1/0/all/0/1&quot;&gt;Diego Navarro Navarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartmannsgruber_A/0/1/0/all/0/1&quot;&gt;Andreas Hartmannsgruber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04633">
<title>General Framework to Evaluate Unlinkability in Biometric Template Protection Systems. (arXiv:2311.04633v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04633</link>
<description rdf:parseType="Literal">&lt;p&gt;The wide deployment of biometric recognition systems in the last two decades
has raised privacy concerns regarding the storage and use of biometric data. As
a consequence, the ISO/IEC 24745 international standard on biometric
information protection has established two main requirements for protecting
biometric templates: irreversibility and unlinkability. Numerous efforts have
been directed to the development and analysis of irreversible templates.
However, there is still no systematic quantitative manner to analyse the
unlinkability of such templates. In this paper we address this shortcoming by
proposing a new general framework for the evaluation of biometric templates&apos;
unlinkability. To illustrate the potential of the approach, it is applied to
assess the unlinkability of four state-of-the-art techniques for biometric
template protection: biometric salting, Bloom filters, Homomorphic Encryption
and block re-mapping. For the last technique, the proposed framework is
compared with other existing metrics to show its advantages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1&quot;&gt;Marta Gomez-Barrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1&quot;&gt;Javier Galbally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1&quot;&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04634">
<title>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering. (arXiv:2311.04634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04634</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last few years, deep neural networks opened the doors for big advances
in novel view synthesis. Many of these approaches are based on a (coarse) proxy
geometry obtained by structure from motion algorithms. Small deficiencies in
this proxy can be fixed by neural rendering, but larger holes or missing parts,
as they commonly appear for thin structures or for glossy regions, still lead
to distracting artifacts and temporal instability. In this paper, we present a
novel neural-rendering-based approach to detect and fix such deficiencies. As a
proxy, we use a point cloud, which allows us to easily remove outlier geometry
and to fill in missing geometry without complicated topological operations.
Keys to our approach are (i) a differentiable, blending point-based renderer
that can blend out redundant points, as well as (ii) the concept of Visual
Error Tomography (VET), which allows us to lift 2D error maps to identify
3D-regions lacking geometry and to spawn novel points accordingly. Furthermore,
(iii) by adding points as nested environment maps, our approach allows us to
generate high-quality renderings of the surroundings in the same pipeline. In
our results, we show that our approach can improve the quality of a point cloud
obtained by structure from motion and thus increase novel view synthesis
quality significantly. In contrast to point growing techniques, the approach
can also fix large-scale holes and missing thin structures effectively.
Rendering quality outperforms state-of-the-art methods and temporal stability
is significantly improved, while rendering is possible at real-time frame
rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franke_L/0/1/0/all/0/1&quot;&gt;Linus Franke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1&quot;&gt;Darius R&amp;#xfc;ckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_L/0/1/0/all/0/1&quot;&gt;Laura Fink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Innmann_M/0/1/0/all/0/1&quot;&gt;Matthias Innmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1&quot;&gt;Marc Stamminger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04640">
<title>Object-Centric Learning with Slot Mixture Module. (arXiv:2311.04640v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;Object-centric architectures usually apply a differentiable module to the
entire feature map to decompose it into sets of entity representations called
slots. Some of these methods structurally resemble clustering algorithms, where
the cluster&apos;s center in latent space serves as a slot representation. Slot
Attention is an example of such a method, acting as a learnable analog of the
soft k-means algorithm. Our work employs a learnable clustering method based on
the Gaussian Mixture Model. Unlike other approaches, we represent slots not
only as centers of clusters but also incorporate information about the distance
between clusters and assigned vectors, leading to more expressive slot
representations. Our experiments demonstrate that using this approach instead
of Slot Attention improves performance in object-centric scenarios, achieving
state-of-the-art results in the set property prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirilenko_D/0/1/0/all/0/1&quot;&gt;Daniil Kirilenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobyov_V/0/1/0/all/0/1&quot;&gt;Vitaliy Vorobyov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovalev_A/0/1/0/all/0/1&quot;&gt;Alexey K. Kovalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1&quot;&gt;Aleksandr I. Panov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04645">
<title>SKU-Patch: Towards Efficient Instance Segmentation for Unseen Objects in Auto-Store. (arXiv:2311.04645v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04645</link>
<description rdf:parseType="Literal">&lt;p&gt;In large-scale storehouses, precise instance masks are crucial for robotic
bin picking but are challenging to obtain. Existing instance segmentation
methods typically rely on a tedious process of scene collection, mask
annotation, and network fine-tuning for every single Stock Keeping Unit (SKU).
This paper presents SKU-Patch, a new patch-guided instance segmentation
solution, leveraging only a few image patches for each incoming new SKU to
predict accurate and robust masks, without tedious manual effort and model
re-training. Technical-wise, we design a novel transformer-based network with
(i) a patch-image correlation encoder to capture multi-level image features
calibrated by patch information and (ii) a patch-aware transformer decoder with
parallel task heads to generate instance masks. Extensive experiments on four
storehouse benchmarks manifest that SKU-Patch is able to achieve the best
performance over the state-of-the-art methods. Also, SKU-Patch yields an
average of nearly 100% grasping success rate on more than 50 unseen SKUs in a
robot-aided auto-store logistic pipeline, showing its effectiveness and
practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Biqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Weiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaojie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun-Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chi-Wing Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04678">
<title>Weakly supervised cross-model learning in high-content screening. (arXiv:2311.04678v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04678</link>
<description rdf:parseType="Literal">&lt;p&gt;With the surge in available data from various modalities, there is a growing
need to bridge the gap between different data types. In this work, we introduce
a novel approach to learn cross-modal representations between image data and
molecular representations for drug discovery. We propose EMM and IMM, two
innovative loss functions built on top of CLIP that leverage weak supervision
and cross sites replicates in High-Content Screening. Evaluating our model
against known baseline on cross-modal retrieval, we show that our proposed
approach allows to learn better representations and mitigate batch effect. In
addition, we also present a preprocessing method for the JUMP-CP dataset that
effectively reduce the required space from 85Tb to a mere usable 7Tb size,
still retaining all perturbations and most of the information content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_W/0/1/0/all/0/1&quot;&gt;Watkinson Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ethan_C/0/1/0/all/0/1&quot;&gt;Cohen Ethan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolas_B/0/1/0/all/0/1&quot;&gt;Bourriez Nicolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ihab_B/0/1/0/all/0/1&quot;&gt;Bendidi Ihab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillaume_B/0/1/0/all/0/1&quot;&gt;Bollot Guillaume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auguste_G/0/1/0/all/0/1&quot;&gt;Genovesio Auguste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04698">
<title>Challenging Common Assumptions in Multi-task Learning. (arXiv:2311.04698v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04698</link>
<description rdf:parseType="Literal">&lt;p&gt;While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge common assumptions in
MTL in the context of STL: First, the choice of optimizer has only been mildly
investigated in MTL. We show the pivotal role of common STL tools such as the
Adam optimizer in MTL. We deduce the effectiveness of Adam to its partial
loss-scale invariance. Second, the notion of gradient conflicts has often been
phrased as a specific problem in MTL. We delve into the role of gradient
conflicts in MTL and compare it to STL. For angular gradient alignment we find
no evidence that this is a unique problem in MTL. We emphasize differences in
gradient magnitude as the main distinguishing factor. Lastly, we compare the
transferability of features learned through MTL and STL on common image
corruptions, and find no conclusive evidence that MTL leads to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elich_C/0/1/0/all/0/1&quot;&gt;Cathrin Elich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchdorfer_L/0/1/0/all/0/1&quot;&gt;Lukas Kirchdorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jan M. K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1&quot;&gt;Lukas Schott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04699">
<title>3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud. (arXiv:2311.04699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04699</link>
<description rdf:parseType="Literal">&lt;p&gt;Greenhouse production of fruits and vegetables in developed countries is
challenged by labor 12 scarcity and high labor costs. Robots offer a good
solution for sustainable and cost-effective 13 production. Acquiring accurate
spatial information about relevant plant parts is vital for 14 successful robot
operation. Robot perception in greenhouses is challenging due to variations in
15 plant appearance, viewpoints, and illumination. This paper proposes a
keypoint-detection-based 16 method using data from an RGB-D camera to estimate
the 3D pose of peduncle nodes, which 17 provides essential information to
harvest the tomato bunches. 18 19 Specifically, this paper proposes a method
that detects four anatomical landmarks in the color 20 image and then
integrates 3D point-cloud information to determine the 3D pose. A 21
comprehensive evaluation was conducted in a commercial greenhouse to gain
insight into the 22 performance of different parts of the method. The results
showed: (1) high accuracy in object 23 detection, achieving an Average
Precision (AP) of AP@0.5=0.96; (2) an average Percentage of 24 Detected Joints
(PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation 25
accuracy with mean absolute errors (MAE) of 11.38o and 9.93o for the relative
upper and lower 26 angles between the peduncle and main stem, respectively.
Furthermore, the capability to handle 27 variations in viewpoint was
investigated, demonstrating the method was robust to view changes. 28 However,
canonical and higher views resulted in slightly higher performance compared to
other 29 views. Although tomato was selected as a use case, the proposed method
is also applicable to 30 other greenhouse crops like pepper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ci_J/0/1/0/all/0/1&quot;&gt;Jianchao Ci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapado_Rincon_D/0/1/0/all/0/1&quot;&gt;David Rapado-Rinc&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burusa_A/0/1/0/all/0/1&quot;&gt;Akshay K. Burusa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kootstra_G/0/1/0/all/0/1&quot;&gt;Gert Kootstra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04711">
<title>Training CLIP models on Data from Scientific Papers. (arXiv:2311.04711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04711</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pretraining (CLIP) models are able to capture the
semantic relationship of images and texts and have enabled a wide range of
applications, from image retrieval to classification. These models are trained
with datasets extracted from web crawls, which are of large quantity but
limited quality. This paper explores whether limited amounts higher quality
data in a specific domain improve the general performance of CLIP models. To
this purpose, we extract text-image data from scientific papers hosted in the
arXiv and PubMed Central repositories. Experiments on small-scale CLIP models
(ViT B/32) show that model performance increases on average, but only
moderately. This result indicates that using the data sources considered in the
paper to train large-scale CLIP models is a worthwile research direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzger_C/0/1/0/all/0/1&quot;&gt;Calvin Metzger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04726">
<title>Social Motion Prediction with Cognitive Hierarchies. (arXiv:2311.04726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04726</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans exhibit a remarkable capacity for anticipating the actions of others
and planning their own actions accordingly. In this study, we strive to
replicate this ability by addressing the social motion prediction problem. We
introduce a new benchmark, a novel formulation, and a cognition-inspired
framework. We present Wusi, a 3D multi-person motion dataset under the context
of team sports, which features intense and strategic human interactions and
diverse pose distributions. By reformulating the problem from a multi-agent
reinforcement learning perspective, we incorporate behavioral cloning and
generative adversarial imitation learning to boost learning efficiency and
generalization. Furthermore, we take into account the cognitive aspects of the
human social action planning process and develop a cognitive hierarchy
framework to predict strategic human social interactions. We conduct
comprehensive experiments to validate the effectiveness of our proposed dataset
and approach. Code and data are available at
https://walter0807.github.io/Social-CH/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jason Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1&quot;&gt;Yuke Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ci_H/0/1/0/all/0/1&quot;&gt;Hai Ci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04766">
<title>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation. (arXiv:2311.04766v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04766</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, audio-driven 3D facial animation has gained significant
attention, particularly in applications such as virtual reality, gaming, and
video conferencing. However, accurately modeling the intricate and subtle
dynamics of facial expressions remains a challenge. Most existing studies
approach the facial animation task as a single regression problem, which often
fail to capture the intrinsic inter-modal relationship between speech signals
and 3D facial animation and overlook their inherent consistency. Moreover, due
to the limited availability of 3D-audio-visual datasets, approaches learning
with small-size samples have poor generalizability that decreases the
performance. To address these issues, in this study, we propose a cross-modal
dual-learning framework, termed DualTalker, aiming at improving data usage
efficiency as well as relating cross-modal dependencies. The framework is
trained jointly with the primary task (audio-driven facial animation) and its
dual task (lip reading) and shares common audio/motion encoder components. Our
joint training framework facilitates more efficient data usage by leveraging
information from both tasks and explicitly capitalizing on the complementary
relationship between facial motion and audio to improve performance.
Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate
the potential over-smoothing underlying the cross-modal complementary
representations, enhancing the mapping of subtle facial expression dynamics.
Through extensive experiments and a perceptual user study conducted on the VOCA
and BIWI datasets, we demonstrate that our approach outperforms current
state-of-the-art methods both qualitatively and quantitatively. We have made
our code and video demonstrations available at
https://github.com/sabrina-su/iadf.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1&quot;&gt;Guinan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanwu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04769">
<title>An attention-based deep learning network for predicting Platinum resistance in ovarian cancer. (arXiv:2311.04769v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04769</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Ovarian cancer is among the three most frequent gynecologic
cancers globally. High-grade serous ovarian cancer (HGSOC) is the most common
and aggressive histological type. Guided treatment for HGSOC typically involves
platinum-based combination chemotherapy, necessitating an assessment of whether
the patient is platinum-resistant. The purpose of this study is to propose a
deep learning-based method to determine whether a patient is platinum-resistant
using multimodal positron emission tomography/computed tomography (PET/CT)
images. Methods: 289 patients with HGSOC were included in this study. An
end-to-end SE-SPP-DenseNet model was built by adding Squeeze-Excitation Block
(SE Block) and Spatial Pyramid Pooling Layer (SPPLayer) to Dense Convolutional
Network (DenseNet). Multimodal data from PET/CT images of the regions of
interest (ROI) were used to predict platinum resistance in patients. Results:
Through five-fold cross-validation, SE-SPP-DenseNet achieved a high accuracy
rate and an area under the curve (AUC) in predicting platinum resistance in
patients, which were 92.6% and 0.93, respectively. The importance of
incorporating SE Block and SPPLayer into the deep learning model, and
considering multimodal data was substantiated by carrying out ablation studies
and experiments with single modality data. Conclusions: The obtained
classification results indicate that our proposed deep learning framework
performs better in predicting platinum resistance in patients, which can help
gynecologists make better treatment decisions. Keywords: PET/CT, CNN, SE Block,
SPP Layer, Platinum resistance, Ovarian cancer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Haoming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Beibei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jingtong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Monkam_P/0/1/0/all/0/1&quot;&gt;Patrice Monkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Shouliang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_W/0/1/0/all/0/1&quot;&gt;Wei Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Dianning He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04772">
<title>GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration. (arXiv:2311.04772v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04772</link>
<description rdf:parseType="Literal">&lt;p&gt;Intracerebral Hemorrhage (ICH) is a severe condition resulting from damaged
brain blood vessel ruptures, often leading to complications and fatalities.
Timely and accurate prognosis and management are essential due to its high
mortality rate. However, conventional methods heavily rely on subjective
clinician expertise, which can lead to inaccurate diagnoses and delays in
treatment. Artificial intelligence (AI) models have been explored to assist
clinicians, but many prior studies focused on model modification without
considering domain knowledge. This paper introduces a novel deep learning
algorithm, GCS-ICHNet, which integrates multimodal brain CT image data and the
Glasgow Coma Scale (GCS) score to improve ICH prognosis. The algorithm utilizes
a transformer-based fusion module for assessment. GCS-ICHNet demonstrates high
sensitivity 81.03% and specificity 91.59%, outperforming average clinicians and
other state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shan_X/0/1/0/all/0/1&quot;&gt;Xuhao Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Ruiquan Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shibin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elazab_A/0/1/0/all/0/1&quot;&gt;Ahmed Elazab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jichao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lingyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_G/0/1/0/all/0/1&quot;&gt;Gangyong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_Q/0/1/0/all/0/1&quot;&gt;Qingying Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04777">
<title>Lidar Annotation Is All You Need. (arXiv:2311.04777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04777</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, computer vision has transformed fields such as medical
imaging, object recognition, and geospatial analytics. One of the fundamental
tasks in computer vision is semantic image segmentation, which is vital for
precise object delineation. Autonomous driving represents one of the key areas
where computer vision algorithms are applied. The task of road surface
segmentation is crucial in self-driving systems, but it requires a
labor-intensive annotation process in several data domains. The work described
in this paper aims to improve the efficiency of image segmentation using a
convolutional neural network in a multi-sensor setup. This approach leverages
lidar (Light Detection and Ranging) annotations to directly train image
segmentation models on RGB images. Lidar supplements the images by emitting
laser pulses and measuring reflections to provide depth information. However,
lidar&apos;s sparse point clouds often create difficulties for accurate object
segmentation. Segmentation of point clouds requires time-consuming preliminary
data preparation and a large amount of computational resources. The key
innovation of our approach is the masked loss, addressing sparse ground-truth
masks from point clouds. By calculating loss exclusively where lidar points
exist, the model learns road segmentation on images by using lidar points as
ground truth. This approach allows for blending of different ground-truth data
types during model training. Experimental validation of the approach on
benchmark datasets shows comparable performance to a high-quality image
segmentation model. Incorporating lidar reduces the load on annotations and
enables training of image-segmentation models without loss of segmentation
quality. The methodology is tested on diverse datasets, both publicly available
and proprietary. The strengths and weaknesses of the proposed method are also
discussed in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharafutdinov_D/0/1/0/all/0/1&quot;&gt;Dinar Sharafutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuskov_S/0/1/0/all/0/1&quot;&gt;Stanislav Kuskov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Protasov_S/0/1/0/all/0/1&quot;&gt;Saian Protasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voropaev_A/0/1/0/all/0/1&quot;&gt;Alexey Voropaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04783">
<title>VioLA: Aligning Videos to 2D LiDAR Scans. (arXiv:2311.04783v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04783</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of aligning a video that captures a local portion of an
environment to the 2D LiDAR scan of the entire environment. We introduce a
method (VioLA) that starts with building a semantic map of the local scene from
the image sequence, then extracts points at a fixed height for registering to
the LiDAR map. Due to reconstruction errors or partial coverage of the camera
scan, the reconstructed semantic map may not contain sufficient information for
registration. To address this problem, VioLA makes use of a pre-trained
text-to-image inpainting model paired with a depth completion model for filling
in the missing scene content in a geometrically consistent fashion to support
pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as
well as a self-captured dataset of a large office scene. Notably, our proposed
scene completion module improves the pose registration performance by up to
20%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_J/0/1/0/all/0/1&quot;&gt;Jun-Jee Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engin_S/0/1/0/all/0/1&quot;&gt;Selim Engin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavan_Dafle_N/0/1/0/all/0/1&quot;&gt;Nikhil Chavan-Dafle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Bhoram Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isler_V/0/1/0/all/0/1&quot;&gt;Volkan Isler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04811">
<title>Image-Based Virtual Try-On: A Survey. (arXiv:2311.04811v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04811</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-based virtual try-on aims to synthesize a naturally dressed person
image with a clothing image, which revolutionizes online shopping and inspires
related topics within image generation, showing both research significance and
commercial potentials. However, there is a great gap between current research
progress and commercial applications and an absence of comprehensive overview
towards this field to accelerate the development. In this survey, we provide a
comprehensive analysis of the state-of-the-art techniques and methodologies in
aspects of pipeline architecture, person representation and key modules such as
try-on indication, clothing warping and try-on stage. We propose a new semantic
criteria with CLIP, and evaluate representative methods with uniformly
implemented evaluation metrics on the same dataset. In addition to quantitative
and qualitative evaluation of current open-source methods, we also utilize
ControlNet to fine-tune a recent large image generation model (PBE) to show
future potentials of large-scale models on image-based virtual try-on task.
Finally, unresolved issues are revealed and future research directions are
prospected to identify key trends and inspire further exploration. The
uniformly implemented evaluation metrics, dataset and collected methods will be
made public available at
https://github.com/little-misfit/Survey-Of-Virtual-Try-On.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanpu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Juan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1&quot;&gt;Weizhi Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1&quot;&gt;Ruofeng Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;An-An Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04813">
<title>Be Careful When Evaluating Explanations Regarding Ground Truth. (arXiv:2311.04813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04813</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating explanations of image classifiers regarding ground truth, e.g.
segmentation masks defined by human perception, primarily evaluates the quality
of the models under consideration rather than the explanation methods
themselves. Driven by this observation, we propose a framework for
$\textit{jointly}$ evaluating the robustness of safety-critical systems that
$\textit{combine}$ a deep neural network with an explanation method. These are
increasingly used in real-world applications like medical image analysis or
robotics. We introduce a fine-tuning procedure to (mis)align
model$\unicode{x2013}$explanation pipelines with ground truth and use it to
quantify the potential discrepancy between worst and best-case scenarios of
human alignment. Experiments across various model architectures and post-hoc
local interpretation methods provide insights into the robustness of vision
transformers and the overall vulnerability of such AI systems to potential
adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baniecki_H/0/1/0/all/0/1&quot;&gt;Hubert Baniecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrabaszcz_M/0/1/0/all/0/1&quot;&gt;Maciej Chrabaszcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzinger_A/0/1/0/all/0/1&quot;&gt;Andreas Holzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeifer_B/0/1/0/all/0/1&quot;&gt;Bastian Pfeifer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saranti_A/0/1/0/all/0/1&quot;&gt;Anna Saranti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04815">
<title>Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning. (arXiv:2311.04815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04815</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based object detectors struggle generalizing to a new target
domain bearing significant variations in object and background. Most current
methods align domains by using image or instance-level adversarial feature
alignment. This often suffers due to unwanted background and lacks
class-specific alignment. A straightforward approach to promote class-level
alignment is to use high confidence predictions on unlabeled domain as
pseudo-labels. These predictions are often noisy since model is poorly
calibrated under domain shift. In this paper, we propose to leverage model&apos;s
predictive uncertainty to strike the right balance between adversarial feature
alignment and class-level alignment. We develop a technique to quantify
predictive uncertainty on class assignments and bounding-box predictions. Model
predictions with low uncertainty are used to generate pseudo-labels for
self-training, whereas the ones with higher uncertainty are used to generate
tiles for adversarial feature alignment. This synergy between tiling around
uncertain object regions and generating pseudo-labels from highly certain
object regions allows capturing both image and instance-level context during
the model adaptation. We report thorough ablation study to reveal the impact of
different components in our approach. Results on five diverse and challenging
adaptation scenarios show that our approach outperforms existing
state-of-the-art methods with noticeable margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1&quot;&gt;Muhammad Akhtar Munir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Haris Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1&quot;&gt;M. Saquib Sarfraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mohsen Ali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04818">
<title>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment. (arXiv:2311.04818v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04818</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from the collective knowledge of data dispersed across private
sources can provide neural networks with enhanced generalization capabilities.
Federated learning, a method for collaboratively training a machine learning
model across remote clients, achieves this by combining client models via the
orchestration of a central server. However, current approaches face two
critical limitations: i) they struggle to converge when client domains are
sufficiently different, and ii) current aggregation techniques produce an
identical global model for each client. In this work, we address these issues
by reformulating the typical federated learning setup: rather than learning a
single global model, we learn N models each optimized for a common objective.
To achieve this, we apply a weighted distance minimization to model parameters
shared in a peer-to-peer topology. The resulting framework, Iterative Parameter
Alignment, applies naturally to the cross-silo setting, and has the following
properties: (i) a unique solution for each participant, with the option to
globally converge each model in the federation, and (ii) an optional
early-stopping mechanism to elicit fairness among peers in collaborative
learning settings. These characteristics jointly provide a flexible new
framework for iteratively learning from peer models trained on disparate
datasets. We find that the technique achieves competitive results on a variety
of data partitions compared to state-of-the-art approaches. Further, we show
that the method is robust to divergent domains (i.e. disjoint classes across
peers) where existing approaches struggle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorbett_M/0/1/0/all/0/1&quot;&gt;Matt Gorbett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirazi_H/0/1/0/all/0/1&quot;&gt;Hossein Shirazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_I/0/1/0/all/0/1&quot;&gt;Indrakshi Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04828">
<title>SODAWideNet -- Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training. (arXiv:2311.04828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04828</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing a new Salient Object Detection (SOD) model involves selecting an
ImageNet pre-trained backbone and creating novel feature refinement modules to
use backbone features. However, adding new components to a pre-trained backbone
needs retraining the whole network on the ImageNet dataset, which requires
significant time. Hence, we explore developing a neural network from scratch
directly trained on SOD without ImageNet pre-training. Such a formulation
offers full autonomy to design task-specific components. To that end, we
propose SODAWideNet, an encoder-decoder-style network for Salient Object
Detection. We deviate from the commonly practiced paradigm of narrow and deep
convolutional models to a wide and shallow architecture, resulting in a
parameter-efficient deep neural network. To achieve a shallower network, we
increase the receptive field from the beginning of the network using a
combination of dilated convolutions and self-attention. Therefore, we propose
Multi Receptive Field Feature Aggregation Module (MRFFAM) that efficiently
obtains discriminative features from farther regions at higher resolutions
using dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which
creates a feature pyramid and efficiently computes attention across multiple
resolutions to extract global features from larger feature maps. Finally, we
propose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that
achieve competitive performance against state-of-the-art models on five
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dulam_R/0/1/0/all/0/1&quot;&gt;Rohit Venkata Sai Dulam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhamettu_C/0/1/0/all/0/1&quot;&gt;Chandra Kambhamettu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04833">
<title>Anonymizing medical case-based explanations through disentanglement. (arXiv:2311.04833v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04833</link>
<description rdf:parseType="Literal">&lt;p&gt;Case-based explanations are an intuitive method to gain insight into the
decision-making process of deep learning models in clinical contexts. However,
medical images cannot be shared as explanations due to privacy concerns. To
address this problem, we propose a novel method for disentangling identity and
medical characteristics of images and apply it to anonymize medical images. The
disentanglement mechanism replaces some feature vectors in an image while
ensuring that the remaining features are preserved, obtaining independent
feature vectors that encode the images&apos; identity and medical characteristics.
We also propose a model to manufacture synthetic privacy-preserving identities
to replace the original image&apos;s identity and achieve anonymization. The models
are applied to medical and biometric datasets, demonstrating their capacity to
generate realistic-looking anonymized images that preserve their original
medical content. Additionally, the experiments show the network&apos;s inherent
capacity to generate counterfactual images through the replacement of medical
features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montenegro_H/0/1/0/all/0/1&quot;&gt;Helena Montenegro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1&quot;&gt;Jaime S. Cardoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04834">
<title>Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction. (arXiv:2311.04834v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04834</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel self-supervised approach for representation learning,
particularly for the task of Visual Relationship Detection (VRD). Motivated by
the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding
Box Reconstruction (MBBR), a variation of MIM where a percentage of the
entities/objects within a scene are masked and subsequently reconstructed based
on the unmasked objects. The core idea is that, through object-level masked
modeling, the network learns context-aware representations that capture the
interaction of objects within a scene and thus are highly predictive of visual
object relationships. We extensively evaluate learned representations, both
qualitatively and quantitatively, in a few-shot setting and demonstrate the
efficacy of MBBR for learning robust visual representations, particularly
tailored for VRD. The proposed method is able to surpass state-of-the-art VRD
methods on the Predicate Detection (PredDet) evaluation setting, using only a
few annotated samples. We make our code available at
https://github.com/deeplab-ai/SelfSupervisedVRD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasakis_Z/0/1/0/all/0/1&quot;&gt;Zacharias Anastasakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Mallis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diomataris_M/0/1/0/all/0/1&quot;&gt;Markos Diomataris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandridis_G/0/1/0/all/0/1&quot;&gt;George Alexandridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollias_S/0/1/0/all/0/1&quot;&gt;Stefanos Kollias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitsikalis_V/0/1/0/all/0/1&quot;&gt;Vassilis Pitsikalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04847">
<title>Are foundation models efficient for medical image segmentation?. (arXiv:2311.04847v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.04847</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models are experiencing a surge in popularity. The Segment
Anything model (SAM) asserts an ability to segment a wide spectrum of objects
but required supervised training at unprecedented scale. We compared SAM&apos;s
performance (against clinical ground truth) and resources (labeling time,
compute) to a modality-specific, label-free self-supervised learning (SSL)
method on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly and
required significantly more labeling and computing resources, demonstrating
worse efficiency than SSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferreira_D/0/1/0/all/0/1&quot;&gt;Danielle Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arnaout_R/0/1/0/all/0/1&quot;&gt;Rima Arnaout&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04888">
<title>Towards Few-Annotation Learning in Computer Vision: Application to Image Classification and Object Detection tasks. (arXiv:2311.04888v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04888</link>
<description rdf:parseType="Literal">&lt;p&gt;In this thesis, we develop theoretical, algorithmic and experimental
contributions for Machine Learning with limited labels, and more specifically
for the tasks of Image Classification and Object Detection in Computer Vision.
In a first contribution, we are interested in bridging the gap between theory
and practice for popular Meta-Learning algorithms used in Few-Shot
Classification. We make connections to Multi-Task Representation Learning,
which benefits from solid theoretical foundations, to verify the best
conditions for a more efficient meta-learning. Then, to leverage unlabeled data
when training object detectors based on the Transformer architecture, we
propose both an unsupervised pretraining and a semi-supervised learning method
in two other separate contributions. For pretraining, we improve Contrastive
Learning for object detectors by introducing the localization information.
Finally, our semi-supervised method is the first tailored to transformer-based
detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouniot_Q/0/1/0/all/0/1&quot;&gt;Quentin Bouniot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04894">
<title>DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets. (arXiv:2311.04894v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04894</link>
<description rdf:parseType="Literal">&lt;p&gt;Construction of a universal detector poses a crucial question: How can we
most effectively train a model on a large mixture of datasets? The answer lies
in learning dataset-specific features and ensembling their knowledge but do all
this in a single model. Previous methods achieve this by having separate
detection heads on a common backbone but that results in a significant increase
in parameters. In this work, we present Mixture-of-Experts as a solution,
highlighting that MoEs are much more than a scalability tool. We propose
Dataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an
`expert&apos; of a dataset by learning to route each dataset tokens to its mapped
expert. Experiments on Universal Object-Detection Benchmark show that we
outperform the existing state-of-the-art by average +10.2 AP score and improve
over our non-MoE baseline by average +2.0 AP score. We also observe consistent
gains while mixing datasets with (1) limited availability, (2) disparate
domains and (3) divergent label sets. Further, we qualitatively show that DAMEX
is robust against expert representation collapse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_Y/0/1/0/all/0/1&quot;&gt;Yash Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behl_H/0/1/0/all/0/1&quot;&gt;Harkirat Behl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04898">
<title>Two Complementary Perspectives to Continual Learning: Ask Not Only What to Optimize, But Also How. (arXiv:2311.04898v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.04898</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen considerable progress in the continual training of
deep neural networks, predominantly thanks to approaches that add replay or
regularization terms to the loss function to approximate the joint loss over
all tasks so far. However, we show that even with a perfect approximation to
the joint loss, these approaches still suffer from temporary but substantial
forgetting when starting to train on a new task. Motivated by this &apos;stability
gap&apos;, we propose that continual learning strategies should focus not only on
the optimization objective, but also on the way this objective is optimized.
While there is some continual learning work that alters the optimization
trajectory (e.g., using gradient projection techniques), this line of research
is positioned as alternative to improving the optimization objective, while we
argue it should be complementary. To evaluate the merits of our proposition, we
plan to combine replay-approximated joint objectives with gradient
projection-based optimization routines to test whether the addition of the
latter provides benefits in terms of (1) alleviating the stability gap, (2)
increasing the learning efficiency and (3) improving the final learning
outcome.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1&quot;&gt;Timm Hess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1&quot;&gt;Gido M. van de Ven&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04901">
<title>GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs. (arXiv:2311.04901v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.04901</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have shown that Large Language Models (LLMs) could empower
traditional neuro-symbolic models via programming capabilities to translate
language into module descriptions, thus achieving strong visual reasoning
results while maintaining the model&apos;s transparency and efficiency. However,
these models usually exhaustively generate the entire code snippet given each
new instance of a task, which is extremely ineffective. We propose generative
neuro-symbolic visual reasoning by growing and reusing modules. Specifically,
our model consists of three unique stages, module initialization, module
generation, and module execution. First, given a vision-language task, we adopt
LLMs to examine whether we could reuse and grow over established modules to
handle this new task. If not, we initialize a new module needed by the task and
specify the inputs and outputs of this new module. After that, the new module
is created by querying LLMs to generate corresponding code snippets that match
the requirements. In order to get a better sense of the new module&apos;s ability,
we treat few-shot training examples as test cases to see if our new module
could pass these cases. If yes, the new module is added to the module library
for future reuse. Finally, we evaluate the performance of our model on the
testing set by executing the parsed programs with the newly made visual modules
to get the results. We find the proposed model possesses several advantages.
First, it performs competitively on standard tasks like visual question
answering and referring expression comprehension; Second, the modules learned
from one task can be seamlessly transferred to new tasks; Last but not least,
it is able to adapt to new visual reasoning tasks by observing a few training
examples and reusing modules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenfang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Rui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenjun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yining Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chuang Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.01434">
<title>Wavelet-Based Network For High Dynamic Range Imaging. (arXiv:2108.01434v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2108.01434</link>
<description rdf:parseType="Literal">&lt;p&gt;High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images
has been suffering from ghosting artifacts caused by scene and objects motion.
Existing methods, such as optical flow based and end-to-end deep learning based
solutions, are error-prone either in detail restoration or ghosting artifacts
removal. Comprehensive empirical evidence shows that ghosting artifacts caused
by large foreground motion are mainly low-frequency signals and the details are
mainly high-frequency signals. In this work, we propose a novel
frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion
in the frequency domain, and Discrete Wavelet Transform (DWT) is used to
decompose inputs into different frequency bands. The low-frequency signals are
used to avoid specific ghosting artifacts, while the high-frequency signals are
used for preserving details. Using a U-Net as the backbone, we propose two
novel modules: merging module and frequency-guided upsampling module. The
merging module applies the attention mechanism to the low-frequency components
to deal with the ghost caused by large foreground motion. The frequency-guided
upsampling module reconstructs details from multiple frequency-specific
components with rich details. In addition, a new RAW dataset is created for
training and evaluating multi-frame HDR imaging algorithms in the RAW domain.
Extensive experiments are conducted on public datasets and our RAW dataset,
showing that the proposed FHDRNet achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tianhong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xilei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xu Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1&quot;&gt;Ales Leonardis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shanxin Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.00907">
<title>Split Semantic Detection in Sandplay Images. (arXiv:2203.00907v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.00907</link>
<description rdf:parseType="Literal">&lt;p&gt;Sandplay image, as an important psychoanalysis carrier, is a visual scene
constructed by the client selecting and placing sand objects (e.g., sand,
river, human figures, animals, vegetation, buildings, etc.). As the projection
of the client&apos;s inner world, it contains high-level semantic information
reflecting the client&apos;s subjective psychological states, which is different
from the common natural image scene that only contains the objective basic
semantics (e.g., object&apos;s name, attribute, bounding box, etc.). In this work,
we take &quot;split&quot; which is a typical psychological semantics related to many
emotional and personality problems as the research goal, and we propose an
automatic detection model, which can replace the time-consuming and expensive
manual analysis process. To achieve that, we design a distribution map
generation method projecting the semantic judgment problem into a visual
problem, and a feature dimensionality reduction and extraction algorithm which
can provide a good representation of split semantics. Besides, we built a
sandplay datasets by collecting one sample from each client and inviting 5
therapists to label each sample, which has a large data cost. Experimental
results demonstrated the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xiaokun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaotang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jian Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.10517">
<title>Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations. (arXiv:2203.10517v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.10517</link>
<description rdf:parseType="Literal">&lt;p&gt;Patient-specific cardiac modeling combines geometries of the heart derived
from medical images and biophysical simulations to predict various aspects of
cardiac function. However, generating simulation-suitable models of the heart
from patient image data often requires complicated procedures and significant
human effort. We present a fast and automated deep-learning method to construct
simulation-suitable models of the heart from medical images. The approach
constructs meshes from 3D patient images by learning to deform a small set of
deformation handles on a whole heart template. For both 3D CT and MR data, this
method achieves promising accuracy for whole heart reconstruction, consistently
outperforming prior methods in constructing simulation-suitable meshes of the
heart. When evaluated on time-series CT data, this method produced more
anatomically and temporally consistent geometries than prior methods, and was
able to produce geometries that better satisfy modeling requirements for
cardiac flow simulations. Our source code and pretrained networks are available
at https://github.com/fkong7/HeartDeformNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fanwei Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shadden_S/0/1/0/all/0/1&quot;&gt;Shawn Shadden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13281">
<title>Movie Genre Classification by Language Augmentation and Shot Sampling. (arXiv:2203.13281v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13281</link>
<description rdf:parseType="Literal">&lt;p&gt;Video-based movie genre classification has garnered considerable attention
due to its various applications in recommendation systems. Prior work has
typically addressed this task by adapting models from traditional video
classification tasks, such as action recognition or event detection. However,
these models often neglect language elements (e.g., narrations or
conversations) present in videos, which can implicitly convey high-level
semantics of movie genres, like storylines or background context. Additionally,
existing approaches are primarily designed to encode the entire content of the
input video, leading to inefficiencies in predicting movie genres. Movie genre
prediction may require only a few shots to accurately determine the genres,
rendering a comprehensive understanding of the entire video unnecessary. To
address these challenges, we propose a Movie genre Classification method based
on Language augmentatIon and shot samPling (Movie-CLIP). Movie-CLIP mainly
consists of two parts: a language augmentation module to recognize language
elements from the input audio, and a shot sampling module to select
representative shots from the entire video. We evaluate our method on MovieNet
and Condensed Movies datasets, achieving approximate 6-9% improvement in mean
Average Precision (mAP) over the baselines. We also generalize Movie-CLIP to
the scene boundary detection task, achieving 1.1% improvement in Average
Precision (AP) over the state-of-the-art. We release our implementation at
github.com/Zhongping-Zhang/Movie-CLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yiwen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1&quot;&gt;Xin Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiayi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huayan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03860">
<title>CCMB: A Large-scale Chinese Cross-modal Benchmark. (arXiv:2205.03860v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03860</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language pre-training (VLP) on large-scale datasets has shown premier
performance on various downstream tasks. In contrast to plenty of available
benchmarks with English corpus, large-scale pre-training datasets and
downstream datasets with Chinese corpus remain largely unexplored. In this
work, we build a large-scale high-quality Chinese Cross-Modal Benchmark named
CCMB for the research community, which contains the currently largest public
pre-training dataset Zero and five human-annotated fine-tuning datasets for
downstream tasks. Zero contains 250 million images paired with 750 million text
descriptions, plus two of the five fine-tuning datasets are also currently the
largest ones for Chinese cross-modal downstream tasks. Along with the CCMB, we
also develop a VLP framework named R2D2, applying a pre-Ranking + Ranking
strategy to learn powerful vision-language representations and a two-way
distillation method (i.e., target-guided Distillation and feature-guided
Distillation) to further enhance the learning capability. With the Zero and the
R2D2 VLP framework, we achieve state-of-the-art performance on twelve
downstream datasets from five broad categories of tasks including image-text
retrieval, image-text matching, image caption, text-to-image generation, and
zero-shot image classification. The datasets, models, and codes are available
at https://github.com/yuxie11/R2D2
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chunyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Heng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fanjing Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jianfei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morimitsu_H/0/1/0/all/0/1&quot;&gt;Henrique Morimitsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangzheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_D/0/1/0/all/0/1&quot;&gt;Dawei Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baochang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yafeng Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10996">
<title>ProtoCLIP: Prototypical Contrastive Language Image Pretraining. (arXiv:2206.10996v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10996</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language Image Pretraining (CLIP) has received widespread
attention, since its learned representations can be transferred well to various
downstream tasks. During the training process of the CLIP model, the InfoNCE
objective aligns positive image-text pairs and separates negative ones. We show
an underlying representation grouping effect during this process: the InfoNCE
objective indirectly groups semantically similar representations together via
randomly emerged within-modal anchors. Based on this understanding, in this
paper, Prototypical Contrastive Language Image Pretraining (ProtoCLIP) is
introduced to enhance such grouping by boosting its efficiency and increasing
its robustness against the modality gap. Specifically, ProtoCLIP sets up
prototype-level discrimination between image and text spaces, which efficiently
transfers higher-level structural knowledge. Further, Prototypical Back
Translation (PBT) is proposed to decouple representation grouping from
representation alignment, resulting in effective learning of meaningful
representations under large modality gap. The PBT also enables us to introduce
additional external teachers with richer prior language knowledge. ProtoCLIP is
trained with an online episodic training strategy, which makes it can be scaled
up to unlimited amounts of data. We train our ProtoCLIP on Conceptual Captions
and achieved an +5.81% ImageNet linear probing improvement and an +2.01%
ImageNet zero-shot classification improvement. On the larger YFCC-15M dataset,
ProtoCLIP matches the performance of CLIP with 33% of training time. Codes are
available at https://github.com/megvii-research/protoclip.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Delong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zaiquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaxi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Ying Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Erjin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05629">
<title>Leveraging Large (Visual) Language Models for Robot 3D Scene Understanding. (arXiv:2209.05629v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05629</link>
<description rdf:parseType="Literal">&lt;p&gt;Abstract semantic 3D scene understanding is a problem of critical importance
in robotics. As robots still lack the common-sense knowledge about household
objects and locations of an average human, we investigate the use of
pre-trained language models to impart common sense for scene understanding. We
introduce and compare a wide range of scene classification paradigms that
leverage language only (zero-shot, embedding-based, and structured-language) or
vision and language (zero-shot and fine-tuned). We find that the best
approaches in both categories yield $\sim 70\%$ room classification accuracy,
exceeding the performance of pure-vision and graph classifiers. We also find
such methods demonstrate notable generalization and transfer capabilities
stemming from their use of language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;William Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Siyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1&quot;&gt;Rajat Talak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1&quot;&gt;Luca Carlone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05794">
<title>Designing Robust Transformers using Robust Kernel Density Estimation. (arXiv:2210.05794v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05794</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Transformer architectures have empowered their empirical
success in a variety of tasks across different domains. However, existing works
mainly focus on predictive accuracy and computational cost, without considering
other practical issues, such as robustness to contaminated samples. Recent work
by Nguyen et al., (2022) has shown that the self-attention mechanism, which is
the center of the Transformer architecture, can be viewed as a non-parametric
estimator based on kernel density estimation (KDE). This motivates us to
leverage a set of robust kernel density estimation methods for alleviating the
issue of data contamination. Specifically, we introduce a series of
self-attention mechanisms that can be incorporated into different Transformer
architectures and discuss the special properties of each method. We then
perform extensive empirical studies on language modeling and image
classification tasks. Our methods demonstrate robust performance in multiple
scenarios while maintaining competitive results on clean datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xing Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1&quot;&gt;Joydeep Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1&quot;&gt;Nhat Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.03418">
<title>A Quantum-Powered Photorealistic Rendering. (arXiv:2211.03418v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.03418</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving photorealistic rendering of real-world scenes poses a significant
challenge with diverse applications, including mixed reality and virtual
reality. Neural networks, extensively explored in solving differential
equations, have previously been introduced as implicit representations for
photorealistic rendering. However, achieving realism through traditional
computing methods is arduous due to the time-consuming optical ray tracing, as
it necessitates extensive numerical integration of color, transparency, and
opacity values for each sampling point during the rendering process. In this
paper, we introduce Quantum Radiance Fields (QRF), which incorporate quantum
circuits, quantum activation functions, and quantum volume rendering to
represent scenes implicitly. Our results demonstrate that QRF effectively
confronts the computational challenges associated with extensive numerical
integration by harnessing the parallelism capabilities of quantum computing.
Furthermore, current neural networks struggle with capturing fine signal
details and accurately modeling high-frequency information and higher-order
derivatives. Quantum computing&apos;s higher order of nonlinearity provides a
distinct advantage in this context. Consequently, QRF leverages two key
strengths of quantum computing: highly non-linear processing and extensive
parallelism, making it a potent tool for achieving photorealistic rendering of
real-world scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;YuanFu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Min Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06739">
<title>Partial Binarization of Neural Networks for Budget-Aware Efficient Learning. (arXiv:2211.06739v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06739</link>
<description rdf:parseType="Literal">&lt;p&gt;Binarization is a powerful compression technique for neural networks,
significantly reducing FLOPs, but often results in a significant drop in model
performance. To address this issue, partial binarization techniques have been
developed, but a systematic approach to mixing binary and full-precision
parameters in a single network is still lacking. In this paper, we propose a
controlled approach to partial binarization, creating a budgeted binary neural
network (B2NN) with our MixBin strategy. This method optimizes the mixing of
binary and full-precision components, allowing for explicit selection of the
fraction of the network to remain binary. Our experiments show that B2NNs
created using MixBin outperform those from random or iterative searches and
state-of-the-art layer selection methods by up to 3% on the ImageNet-1K
dataset. We also show that B2NNs outperform the structured pruning baseline by
approximately 23% at the extreme FLOP budget of 15%, and perform well in object
tracking, with up to a 12.4% relative improvement over other baselines.
Additionally, we demonstrate that B2NNs developed by MixBin can be transferred
across datasets, with some cases showing improved performance over directly
applying MixBin on the downstream data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamba_U/0/1/0/all/0/1&quot;&gt;Udbhav Bamba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anand_N/0/1/0/all/0/1&quot;&gt;Neeraj Anand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_S/0/1/0/all/0/1&quot;&gt;Saksham Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1&quot;&gt;Dilip K. Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak K. Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10611">
<title>Discriminator-free Unsupervised Domain Adaptation for Multi-label Image Classification. (arXiv:2301.10611v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10611</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a discriminator-free adversarial-based Unsupervised Domain
Adaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as
DDA-MLIC is proposed. Recently, some attempts have been made for introducing
adversarial-based UDA methods in the context of MLIC. However, these methods
which rely on an additional discriminator subnet present one major shortcoming.
The learning of domain-invariant features may harm their task-specific
discriminative power, since the classification and discrimination tasks are
decoupled. Herein, we propose to overcome this issue by introducing a novel
adversarial critic that is directly deduced from the task-specific classifier.
Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on the
source and target predictions in order to distinguish between two clusters.
This allows extracting a Gaussian distribution for each component. The
resulting Gaussian distributions are then used for formulating an adversarial
loss based on a Frechet distance. The proposed method is evaluated on several
multi-label image datasets covering three different types of domain shift. The
obtained results demonstrate that DDA-MLIC outperforms existing
state-of-the-art methods in terms of precision while requiring a lower number
of parameters. The code is publicly available at github.com/cvi2snt/DDA-MLIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Indel Pal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbel_E/0/1/0/all/0/1&quot;&gt;Enjie Ghorbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kacem_A/0/1/0/all/0/1&quot;&gt;Anis Kacem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathinam_A/0/1/0/all/0/1&quot;&gt;Arunkumar Rathinam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1&quot;&gt;Djamila Aouada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07025">
<title>Optimal Transport for Change Detection on LiDAR Point Clouds. (arXiv:2302.07025v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07025</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised change detection between airborne LiDAR data points, taken at
separate times over the same location, can be difficult due to unmatching
spatial support and noise from the acquisition system. Most current approaches
to detect changes in point clouds rely heavily on the computation of Digital
Elevation Models (DEM) images and supervised methods. Obtaining a DEM leads to
LiDAR informational loss due to pixelisation, and supervision requires large
amounts of labelled data often unavailable in real-world scenarios. We propose
an unsupervised approach based on the computation of the transport of 3D LiDAR
points over two temporal supports. The method is based on unbalanced optimal
transport and can be generalised to any change detection problem with LiDAR
data. We apply our approach to publicly available datasets for monitoring urban
sprawling in various noise and resolution configurations that mimic several
sensors used in practice. Our method allows for unsupervised multi-class
classification and outperforms the previous state-of-the-art unsupervised
approaches by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiorucci_M/0/1/0/all/0/1&quot;&gt;Marco Fiorucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naylor_P/0/1/0/all/0/1&quot;&gt;Peter Naylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1&quot;&gt;Makoto Yamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09043">
<title>Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences. (arXiv:2302.09043v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09043</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised feature learning enables perception systems to benefit from
the vast raw data recorded by vehicle fleets worldwide. While video-level
self-supervised learning approaches have shown strong generalizability on
classification tasks, the potential to learn dense representations from
sequential data has been relatively unexplored. In this work, we propose TempO,
a temporal ordering pretext task for pre-training region-level feature
representations for perception tasks. We embed each frame by an unordered set
of proposal feature vectors, a representation that is natural for object
detection or tracking systems, and formulate the sequential ordering by
predicting frame transition probabilities in a transformer-based multi-frame
architecture whose complexity scales less than quadratic with respect to the
sequence length. Extensive evaluations on the BDD100K, nuImages, and MOT17
datasets show that our TempO pre-training approach outperforms single-frame
self-supervised learning methods as well as supervised transfer learning
initialization strategies, achieving an improvement of +0.7% in mAP for object
detection and +2.0% in the HOTA score for multi-object tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1&quot;&gt;Christopher Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schillingmann_L/0/1/0/all/0/1&quot;&gt;Lars Schillingmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haug_K/0/1/0/all/0/1&quot;&gt;Karsten Haug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12231">
<title>DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models. (arXiv:2302.12231v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12231</link>
<description rdf:parseType="Literal">&lt;p&gt;Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive
results on novel view synthesis tasks. NeRFs learn a scene&apos;s color and density
fields by minimizing the photometric discrepancy between training views and
differentiable renderings of the scene. Once trained from a sufficient set of
views, NeRFs can generate novel views from arbitrary camera positions. However,
the scene geometry and color fields are severely under-constrained, which can
lead to artifacts, especially when trained with few input views.
&lt;/p&gt;
&lt;p&gt;To alleviate this problem we learn a prior over scene geometry and color,
using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of
the synthetic Hypersim dataset and can be used to predict the gradient of the
logarithm of a joint probability distribution of color and depth patches. We
show that, these gradients of logarithms of RGBD patch priors serve to
regularize geometry and color of a scene. During NeRF training, random RGBD
patches are rendered and the estimated gradient of the log-likelihood is
backpropagated to the color and density fields. Evaluations on LLFF, the most
relevant dataset, show that our learned prior achieves improved quality in the
reconstructed geometry and improved generalization to novel views. Evaluations
on DTU show improved reconstruction quality among NeRF methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wynn_J/0/1/0/all/0/1&quot;&gt;Jamie Wynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turmukhambetov_D/0/1/0/all/0/1&quot;&gt;Daniyar Turmukhambetov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10937">
<title>Boosting Weakly Supervised Object Detection using Fusion and Priors from Hallucinated Depth. (arXiv:2303.10937v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10937</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent attention and exploration of depth for various tasks, it is
still an unexplored modality for weakly-supervised object detection (WSOD). We
propose an amplifier method for enhancing the performance of WSOD by
integrating depth information. Our approach can be applied to any WSOD method
based on multiple-instance learning, without necessitating additional
annotations or inducing large computational expenses. Our proposed method
employs a monocular depth estimation technique to obtain hallucinated depth
information, which is then incorporated into a Siamese WSOD network using
contrastive loss and fusion. By analyzing the relationship between language
context and depth, we calculate depth priors to identify the bounding box
proposals that may contain an object of interest. These depth priors are then
utilized to update the list of pseudo ground-truth boxes, or adjust the
confidence of per-box predictions. Our proposed method is evaluated on six
datasets (COCO, PASCAL VOC, Conceptual Captions, Clipart1k, Watercolor2k, and
Comic2k) by implementing it on top of two state-of-the-art WSOD methods, and we
demonstrate a substantial enhancement in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gungor_C/0/1/0/all/0/1&quot;&gt;Cagri Gungor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1&quot;&gt;Adriana Kovashka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15413">
<title>Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation. (arXiv:2303.15413v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15413</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing score-distilling text-to-3D generation techniques, despite their
considerable promise, often encounter the view inconsistency problem. One of
the most notable issues is the Janus problem, where the most canonical view of
an object (\textit{e.g}., face or head) appears in other views. In this work,
we explore existing frameworks for score-distilling text-to-3D generation and
identify the main causes of the view inconsistency problem -- the embedded bias
of 2D diffusion models. Based on these findings, we propose two approaches to
debias the score-distillation frameworks for view-consistent text-to-3D
generation. Our first approach, called score debiasing, involves cutting off
the score estimated by 2D diffusion models and gradually increasing the
truncation value throughout the optimization process. Our second approach,
called prompt debiasing, identifies conflicting words between user prompts and
view prompts using a language model, and adjusts the discrepancy between view
prompts and the viewing direction of an object. Our experimental results show
that our methods improve the realism of the generated 3D objects by
significantly reducing artifacts and achieve a good trade-off between
faithfulness to the 2D diffusion models and 3D consistency with little
overhead. Our project page is available
at~\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Susung Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1&quot;&gt;Donghoon Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06373">
<title>3DoF Localization from a Single Image and an Object Map: the Flatlandia Problem and Dataset. (arXiv:2304.06373v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient visual localization is crucial to many applications, such as
large-scale deployment of autonomous agents and augmented reality. Traditional
visual localization, while achieving remarkable accuracy, relies on extensive
3D models of the scene or large collections of geolocalized images, which are
often inefficient to store and to scale to novel environments. In contrast,
humans orient themselves using very abstract 2D maps, using the location of
clearly identifiable landmarks. Drawing on this and on the success of recent
works that explored localization on 2D abstract maps, we propose Flatlandia, a
novel visual localization challenge. With Flatlandia, we investigate whether it
is possible to localize a visual query by comparing the layout of its common
objects detected against the known spatial layout of objects in the map. We
formalize the challenge as two tasks at different levels of accuracy to
investigate the problem and its possible limitations; for each, we propose
initial baseline models and compare them against state-of-the-art 6DoF and 3DoF
methods. Code and dataset are publicly available at
github.com/IIT-PAVIS/Flatlandia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toso_M/0/1/0/all/0/1&quot;&gt;Matteo Toso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taiana_M/0/1/0/all/0/1&quot;&gt;Matteo Taiana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Stuart James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1&quot;&gt;Alessio Del Bue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03807">
<title>Evading Watermark based Detection of AI-Generated Content. (arXiv:2305.03807v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03807</link>
<description rdf:parseType="Literal">&lt;p&gt;A generative AI model can generate extremely realistic-looking content,
posing growing challenges to the authenticity of information. To address the
challenges, watermark has been leveraged to detect AI-generated content.
Specifically, a watermark is embedded into an AI-generated content before it is
released. A content is detected as AI-generated if a similar watermark can be
decoded from it. In this work, we perform a systematic study on the robustness
of such watermark-based AI-generated content detection. We focus on
AI-generated images. Our work shows that an attacker can post-process a
watermarked image via adding a small, human-imperceptible perturbation to it,
such that the post-processed image evades detection while maintaining its
visual quality. We show the effectiveness of our attack both theoretically and
empirically. Moreover, to evade detection, our adversarial post-processing
method adds much smaller perturbations to AI-generated images and thus better
maintain their visual quality than existing popular post-processing methods
such as JPEG compression, Gaussian blur, and Brightness/Contrast. Our work
shows the insufficiency of existing watermark-based detection of AI-generated
content, highlighting the urgent needs of new methods. Our code is publicly
available: https://github.com/zhengyuan-jiang/WEvade.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinghuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04833">
<title>Revisiting Table Detection Datasets for Visually Rich Documents. (arXiv:2305.04833v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04833</link>
<description rdf:parseType="Literal">&lt;p&gt;Table Detection has become a fundamental task for visually rich document
understanding with the surging number of electronic documents. However, popular
public datasets widely used in related studies have inherent limitations,
including noisy and inconsistent samples, limited training samples, and limited
data sources. These limitations make these datasets unreliable to evaluate the
model performance and cannot reflect the actual capacity of models. Therefore,
this study revisits some open datasets with high-quality annotations,
identifies and cleans the noise, and aligns the annotation definitions of these
datasets to merge a larger dataset, termed Open-Tables. Moreover, to enrich the
data sources, we propose a new ICT-TD dataset using the PDF files of
Information and Communication Technologies (ICT) commodities, a different
domain containing unique samples that hardly appear in open datasets. To ensure
the label quality of the dataset, we annotated the dataset manually following
the guidance of a domain expert. The proposed dataset is challenging and can be
a sample of actual cases in the business context. We built strong baselines
using various state-of-the-art object detection models. Our experimental
results show that the domain differences among existing open datasets are minor
despite having different data sources. Our proposed Open-Tables and ICT-TD can
provide a more reliable evaluation for models because of their high quality and
consistent annotations. Besides, they are more suitable for cross-domain
settings. Our experimental results show that in the cross-domain setting,
benchmark models trained with cleaned Open-Tables dataset can achieve
0.6\%-2.6\% higher weighted average F1 than the corresponding ones trained with
the noisy version of Open-Tables, demonstrating the reliability of the proposed
datasets. The datasets are public available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1&quot;&gt;Bin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1&quot;&gt;Murat Simsek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1&quot;&gt;Burak Kantarci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkheir_A/0/1/0/all/0/1&quot;&gt;Ala Abu Alkheir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable model for classification that provides a novel
probabilistic interpretation of neural network softmax classifiers. We derive a
variational objective to train the model, analogous to the evidence lower bound
(ELBO) used to train variational auto-encoders, that generalises the
cross-entropy loss used to train classification models. Treating inputs to the
softmax layer as samples of a latent variable, our abstracted perspective
reveals a potential inconsistency between their anticipated distribution,
required for accurate label predictions to be output, and the empirical
distribution found in practice. We augment the variational objective to
mitigate such inconsistency and encourage a chosen latent distribution, instead
of the implicit assumption in off-the-shelf softmax classifiers. Overall, we
provide new theoretical insight into the inner workings of widely-used softmax
classification. Empirical evaluation on image and text classification datasets
demonstrates that our proposed approach, variational classification, maintains
classification accuracy while the reshaped latent space improves other
desirable properties of a classifier, such as calibration, adversarial
robustness, robustness to distribution shift and sample efficiency useful in
low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12011">
<title>Boosting Crop Classification by Hierarchically Fusing Satellite, Rotational, and Contextual Data. (arXiv:2305.12011v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12011</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate in-season crop type classification is crucial for the crop
production estimation and monitoring of agricultural parcels. However, the
complexity of the plant growth patterns and their spatio-temporal variability
present significant challenges. While current deep learning-based methods show
promise in crop type classification from single- and multi-modal time series,
most existing methods rely on a single modality, such as satellite optical
remote sensing data or crop rotation patterns. We propose a novel approach to
fuse multimodal information into a model for improved accuracy and robustness
across multiple years and countries. The approach relies on three modalities
used: remote sensing time series from Sentinel-2 and Landsat 8 observations,
parcel crop rotation and local crop distribution. To evaluate our approach, we
release a new annotated dataset of 7.4 million agricultural parcels in France
and Netherlands. We associate each parcel with time-series of surface
reflectance (Red and NIR) and biophysical variables (LAI, FAPAR). Additionally,
we propose a new approach to automatically aggregate crop types into a
hierarchical class structure for meaningful model evaluation and a novel
data-augmentation technique for early-season classification. Performance of the
multimodal approach was assessed at different aggregation level in the semantic
domain spanning from 151 to 8 crop types or groups. It resulted in accuracy
ranging from 91\% to 95\% for NL dataset and from 85\% to 89\% for FR dataset.
Pre-training on a dataset improves domain adaptation between countries,
allowing for cross-domain zero-shot learning, and robustness of the
performances in a few-shot setting from France to Netherlands. Our proposed
approach outperforms comparable methods by enabling learning methods to use the
often overlooked spatio-temporal context of parcels, resulting in increased
preci...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barriere_V/0/1/0/all/0/1&quot;&gt;Valentin Barriere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claverie_M/0/1/0/all/0/1&quot;&gt;Martin Claverie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_M/0/1/0/all/0/1&quot;&gt;Maja Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemoine_G/0/1/0/all/0/1&quot;&gt;Guido Lemoine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+dAndrimont_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l d&amp;#x27;Andrimont&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14979">
<title>Assessment of the Reliablity of a Model&apos;s Decision by Generalizing Attribution to the Wavelet Domain. (arXiv:2305.14979v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14979</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process&apos;s reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where and on what scales
the model focuses, thus enabling us to assess whether a decision is reliable.
Our code is accessible here:
\url{https://github.com/gabrielkasmi/spectral-attribution}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1&quot;&gt;Gabriel Kasmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1&quot;&gt;Laurent Dubus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drenan_Y/0/1/0/all/0/1&quot;&gt;Yves-Marie Saint Drenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1&quot;&gt;Philippe Blanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15296">
<title>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation. (arXiv:2305.15296v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15296</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent popularity of text-to-image diffusion models (DM) can largely be
attributed to the intuitive interface they provide to users. The intended
generation can be expressed in natural language, with the model producing
faithful interpretations of text prompts. However, expressing complex or
nuanced ideas in text alone can be difficult. To ease image generation, we
propose MultiFusion that allows one to express complex and nuanced concepts
with arbitrarily interleaved inputs of multiple modalities and languages.
MutliFusion leverages pre-trained models and aligns them for integration into a
cohesive system, thereby avoiding the need for extensive training from scratch.
Our experimental results demonstrate the efficient transfer of capabilities
from individual modules to the downstream model. Specifically, the fusion of
all independent components allows the image generation module to utilize
multilingual, interleaved multimodal inputs despite being trained solely on
monomodal data in a single language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Deiseroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1&quot;&gt;Constantin Eichenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldock_R/0/1/0/all/0/1&quot;&gt;Robert Baldock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_S/0/1/0/all/0/1&quot;&gt;Souradeep Nanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1&quot;&gt;Koen Oostermeijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_Salinas_A/0/1/0/all/0/1&quot;&gt;Andres Felipe Cruz-Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1&quot;&gt;Samuel Weinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15775">
<title>Concept-Centric Transformers: Enhancing Model Interpretability through Object-Centric Concept Learning within a Shared Global Workspace. (arXiv:2305.15775v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15775</link>
<description rdf:parseType="Literal">&lt;p&gt;Many interpretable AI approaches have been proposed to provide plausible
explanations for a model&apos;s decision-making. However, configuring an explainable
model that effectively communicates among computational modules has received
less attention. A recently proposed shared global workspace theory showed that
networks of distributed modules can benefit from sharing information with a
bottlenecked memory because the communication constraints encourage
specialization, compositionality, and synchronization among the modules.
Inspired by this, we propose Concept-Centric Transformers, a simple yet
effective configuration of the shared global workspace for interpretability,
consisting of: i) an object-centric-based memory module for extracting semantic
concepts from input features, ii) a cross-attention mechanism between the
learned concept and input embeddings, and iii) standard classification and
explanation losses to allow human analysts to directly assess an explanation
for the model&apos;s classification reasoning. We test our approach against other
existing concept-based methods on classification tasks for various datasets,
including CIFAR100, CUB-200-2011, and ImageNet, and we show that our model
achieves better classification accuracy than all baselines across all problems
but also generates more consistent concept-based explanations of classification
output.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jinyung Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1&quot;&gt;Keun Hee Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlic_T/0/1/0/all/0/1&quot;&gt;Theodore P. Pavlic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16681">
<title>CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning. (arXiv:2305.16681v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16681</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of Compositional Zero-Shot Learning
(CZSL), which is to recognize novel attribute-object combinations with
pre-existing concepts. Recent researchers focus on applying large-scale
Vision-Language Pre-trained (VLP) models like CLIP with strong generalization
ability. However, these methods treat the pre-trained model as a black box and
focus on pre- and post-CLIP operations, which do not inherently mine the
semantic concept between the layers inside CLIP. We propose to dive deep into
the architecture and insert adapters, a parameter-efficient technique proven to
be effective among large language models, into each CLIP encoder layer. We
further equip adapters with concept awareness so that concept-specific features
of &quot;object&quot;, &quot;attribute&quot;, and &quot;composition&quot; can be extracted. We assess our
method on four popular CZSL datasets, MIT-States, C-GQA, UT-Zappos, and
VAW-CZSL, which shows state-of-the-art performance compared to existing methods
on all of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhaoheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haidong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1&quot;&gt;Ram Nevatia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17489">
<title>Text-to-image Editing by Image Information Removal. (arXiv:2305.17489v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17489</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated impressive performance in text-guided
image generation. Current methods that leverage the knowledge of these models
for image editing either fine-tune them using the input image (e.g., Imagic) or
incorporate structure information as additional constraints (e.g., ControlNet).
However, fine-tuning large-scale diffusion models on a single image can lead to
severe overfitting issues and lengthy inference time. Information leakage from
pretrained models also make it challenging to preserve image content not
related to the text input. Additionally, methods that incorporate structural
guidance (e.g., edge maps, semantic maps, keypoints) find retaining attributes
like colors and textures difficult. Using the input image as a control could
mitigate these issues, but since these models are trained via reconstruction, a
model can simply hide information about the original image when encoding it to
perfectly reconstruct the image without learning the editing task. To address
these challenges, we propose a text-to-image editing model with an Image
Information Removal module (IIR) that selectively erases color-related and
texture-related information from the original image, allowing us to better
preserve the text-irrelevant content and avoid issues arising from information
hiding. Our experiments on CUB, Outdoor Scenes, and COCO reports our approach
achieves the best editability-fidelity trade-off results. In addition, a user
study on COCO shows that our edited images are preferred 35% more often than
prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jian Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jacob Zhiyuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07915">
<title>Image Captioners Are Scalable Vision Learners Too. (arXiv:2306.07915v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07915</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive pretraining on image-text pairs from the web is one of the most
popular large-scale pretraining strategies for vision backbones, especially in
the context of large multimodal models. At the same time, image captioning on
this type of data is commonly considered an inferior pretraining strategy. In
this paper, we perform a fair comparison of these two pretraining strategies,
carefully matching training data, compute, and model capacity. Using a standard
encoder-decoder transformer, we find that captioning alone is surprisingly
effective: on classification tasks, captioning produces vision encoders
competitive with contrastively pretrained encoders, while surpassing them on
vision &amp;amp; language tasks. We further analyze the effect of the model
architecture and scale, as well as the pretraining data on the representation
quality, and find that captioning exhibits the same or better scaling behavior
along these axes. Overall our results show that plain image captioning is a
more powerful pretraining strategy than was previously believed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1&quot;&gt;Michael Tschannen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Manoj Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1&quot;&gt;Andreas Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1&quot;&gt;Lucas Beyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08013">
<title>TopP&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a robust and reliable evaluation metric for generative models by
introducing topological and statistical treatments for rigorous support
estimation. Existing metrics, such as Inception Score (IS), Frechet Inception
Distance (FID), and the variants of Precision and Recall (P&amp;amp;R), heavily rely on
supports that are estimated from sample features. However, the reliability of
their estimation has not been seriously discussed (and overlooked) even though
the quality of the evaluation entirely depends on it. In this paper, we propose
Topological Precision and Recall (TopP&amp;amp;R, pronounced &apos;topper&apos;), which provides
a systematic approach to estimating supports, retaining only topologically and
statistically important features with a certain level of confidence. This not
only makes TopP&amp;amp;R strong for noisy features, but also provides statistical
consistency. Our theoretical and experimental results show that TopP&amp;amp;R is
robust to outliers and non-independent and identically distributed (Non-IID)
perturbations, while accurately capturing the true trend of change in samples.
To the best of our knowledge, this is the first evaluation metric focused on
the robust estimation of the support and provides its statistical consistency
under noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Pum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yoojin Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jisu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1&quot;&gt;Jaejun Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09244">
<title>Text Promptable Surgical Instrument Segmentation with Vision-Language Models. (arXiv:2306.09244v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09244</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel text promptable surgical instrument
segmentation approach to overcome challenges associated with diversity and
differentiation of surgical instruments in minimally invasive surgeries. We
redefine the task as text promptable, thereby enabling a more nuanced
comprehension of surgical instruments and adaptability to new instrument types.
Inspired by recent advancements in vision-language models, we leverage
pretrained image and text encoders as our model backbone and design a text
promptable mask decoder consisting of attention- and convolution-based
prompting schemes for surgical instrument segmentation prediction. Our model
leverages multiple text prompts for each surgical instrument through a new
mixture of prompts mechanism, resulting in enhanced segmentation performance.
Additionally, we introduce a hard instrument area reinforcement module to
improve image feature comprehension and segmentation precision. Extensive
experiments on several surgical instrument segmentation datasets demonstrate
our model&apos;s superior performance and promising generalization capability. To
our knowledge, this is the first implementation of a promptable approach to
surgical instrument segmentation, offering significant potential for practical
application in the field of robotic-assisted surgery. Code is available at
https://github.com/franciszzj/TP-SIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zijian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabi_O/0/1/0/all/0/1&quot;&gt;Oluwatosin Alabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Meng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1&quot;&gt;Tom Vercauteren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16635">
<title>Improving Fairness in Deepfake Detection. (arXiv:2306.16635v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16635</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the development of effective deepfake detectors in recent years,
recent studies have demonstrated that biases in the data used to train these
detectors can lead to disparities in detection accuracy across different races
and genders. This can result in different groups being unfairly targeted or
excluded from detection, allowing undetected deepfakes to manipulate public
opinion and erode trust in a deepfake detection model. While existing studies
have focused on evaluating fairness of deepfake detectors, to the best of our
knowledge, no method has been developed to encourage fairness in deepfake
detection at the algorithm level. In this work, we make the first attempt to
improve deepfake detection fairness by proposing novel loss functions that
handle both the setting where demographic information (eg, annotations of race
and gender) is available as well as the case where this information is absent.
Fundamentally, both approaches can be used to convert many existing deepfake
detectors into ones that encourages fairness. Extensive experiments on four
deepfake datasets and five deepfake detectors demonstrate the effectiveness and
flexibility of our approach in improving deepfake detection fairness. Our code
is available at https://github.com/littlejuyan/DF_Fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1&quot;&gt;Yan Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1&quot;&gt;Shan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;George H. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09933">
<title>Spuriosity Didn&apos;t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09933</link>
<description rdf:parseType="Literal">&lt;p&gt;To avoid failures on out-of-distribution data, recent works have sought to
extract features that have an invariant or stable relationship with the label
across domains, discarding &quot;spurious&quot; or unstable features whose relationship
with the label changes across domains. However, unstable features often carry
complementary information that could boost performance if used correctly in the
test domain. In this work, we show how this can be done without test-domain
labels. In particular, we prove that pseudo-labels based on stable features
provide sufficient guidance for doing so, provided that stable and unstable
features are conditionally independent given the label. Based on this
theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm
for: (i) learning a predictor that separates stable and
conditionally-independent unstable features; and (ii) using the stable-feature
predictions to adapt the unstable-feature predictions in the test domain.
Theoretically, we prove that SFB can learn an asymptotically-optimal predictor
without test-domain labels. Empirically, we demonstrate the effectiveness of
SFB on real and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1&quot;&gt;Cian Eastwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shashank Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Andrei Liviu Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1&quot;&gt;Marin Vlastelica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13639">
<title>Fake It Without Making It: Conditioned Face Generation for Accurate 3D Face Reconstruction. (arXiv:2307.13639v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13639</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D face reconstruction from 2D images is an enabling technology with
applications in healthcare, security, and creative industries. However, current
state-of-the-art methods either rely on supervised training with very limited
3D data or self-supervised training with 2D image data. To bridge this gap, we
present a method to generate a large-scale synthesised dataset of 250K
photorealistic images and their corresponding shape parameters and depth maps,
which we call SynthFace. Our synthesis method conditions Stable Diffusion on
depth maps sampled from the FLAME 3D Morphable Model (3DMM) of the human face,
allowing us to generate a diverse set of shape-consistent facial images that is
designed to be balanced in race and gender. We further propose ControlFace, a
deep neural network, trained on SynthFace, which achieves competitive
performance on the NoW benchmark, without requiring 3D supervision or manual 3D
asset creation. The complete SynthFace dataset will be made publicly available
upon publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowan_W/0/1/0/all/0/1&quot;&gt;Will Rowan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1&quot;&gt;Patrik Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pears_N/0/1/0/all/0/1&quot;&gt;Nick Pears&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keeling_A/0/1/0/all/0/1&quot;&gt;Andrew Keeling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00530">
<title>Tolerating Annotation Displacement in Dense Object Counting via Point Annotation Probability Map. (arXiv:2308.00530v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00530</link>
<description rdf:parseType="Literal">&lt;p&gt;Counting objects in crowded scenes remains a challenge to computer vision.
The current deep learning based approach often formulate it as a Gaussian
density regression problem. Such a brute-force regression, though effective,
may not consider the annotation displacement properly which arises from the
human annotation process and may lead to different distributions. We conjecture
that it would be beneficial to consider the annotation displacement in the
dense object counting task. To obtain strong robustness against annotation
displacement, generalized Gaussian distribution (GGD) function with a tunable
bandwidth and shape parameter is exploited to form the learning target point
annotation probability map, PAPM. Specifically, we first present a
hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD
to tolerate the annotation displacement. For end-to-end training, the
hand-designed PAPM may not be optimal for the particular network and dataset.
An adaptively learned PAPM method (AL-PAPM) is proposed. To improve the
robustness to annotation displacement, we design an effective transport cost
function based on GGD. The proposed PAPM is capable of integration with other
methods. We also combine PAPM with P2PNet through modifying the matching cost
matrix, forming P2P-PAPM. This could also improve the robustness to annotation
displacement of P2PNet. Extensive experiments show the superiority of our
proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuehai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Badong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1&quot;&gt;Hua Gang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Shaoyi Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06202">
<title>Exploring Predicate Visual Context in Detecting Human-Object Interactions. (arXiv:2308.06202v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06202</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the DETR framework has emerged as the dominant approach for
human--object interaction (HOI) research. In particular, two-stage
transformer-based HOI detectors are amongst the most performant and
training-efficient approaches. However, these often condition HOI
classification on object features that lack fine-grained contextual
information, eschewing pose and orientation information in favour of visual
cues about object identity and box extremities. This naturally hinders the
recognition of complex or ambiguous interactions. In this work, we study these
issues through visualisations and carefully designed experiments. Accordingly,
we investigate how best to re-introduce image features via cross-attention.
With an improved query design, extensive exploration of keys and values, and
box pair positional embeddings as spatial guidance, our model with enhanced
predicate visual context (PViC) outperforms state-of-the-art methods on the
HICO-DET and V-COCO benchmarks, while maintaining low training cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Frederic Z. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1&quot;&gt;Dylan Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhuoyao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1&quot;&gt;Stephen Gould&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06597">
<title>Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06597</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread adoption of commercial autonomous vehicles (AVs) and advanced
driver assistance systems (ADAS) may largely depend on their acceptance by
society, for which their perceived trustworthiness and interpretability to
riders are crucial. In general, this task is challenging because modern
autonomous systems software relies heavily on black-box artificial intelligence
models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a
multi-modal ego-centric dataset for Ranking the importance level and Telling
the reason for the importance. Using various close and open-ended visual
question answering, the dataset provides dense annotations of various semantic,
spatial, temporal, and relational attributes of various important objects in
complex traffic scenarios. The dense annotations and unique attributes of the
dataset make it a valuable resource for researchers working on visual scene
understanding and related fields. Furthermore, we introduce a joint model for
joint importance level ranking and natural language captions generation to
benchmark our dataset and demonstrate performance with quantitative
evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachdeva_E/0/1/0/all/0/1&quot;&gt;Enna Sachdeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1&quot;&gt;Nakul Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chundi_S/0/1/0/all/0/1&quot;&gt;Suhas Chundi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roelofs_S/0/1/0/all/0/1&quot;&gt;Sean Roelofs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel Kochenderfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1&quot;&gt;Chiho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dariush_B/0/1/0/all/0/1&quot;&gt;Behzad Dariush&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11119">
<title>BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Bird&apos;s Eye View Map Construction. (arXiv:2309.11119v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11119</link>
<description rdf:parseType="Literal">&lt;p&gt;A recent sensor fusion in a Bird&apos;s Eye View (BEV) space has shown its utility
in various tasks such as 3D detection, map segmentation, etc. However, the
approach struggles with inaccurate camera BEV estimation, and a perception of
distant areas due to the sparsity of LiDAR points. In this paper, we propose a
broad BEV fusion (BroadBEV) that addresses the problems with a spatial
synchronization approach of cross-modality. Our strategy aims to enhance camera
BEV estimation for a broad-sighted perception while simultaneously improving
the completion of LiDAR&apos;s sparsity in the entire BEV space. Toward that end, we
devise Point-scattering that scatters LiDAR BEV distribution to camera depth
distribution. The method boosts the learning of depth estimation of the camera
branch and induces accurate location of dense camera features in BEV space. For
an effective BEV fusion between the spatially synchronized features, we suggest
ColFusion that applies self-attention weights of LiDAR and camera BEV features
to each other. Our extensive experiments demonstrate that BroadBEV provides a
broad-sighted BEV perception with remarkable performance gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Giseop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1&quot;&gt;Kyong Hwan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sunwook Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12214">
<title>Can We Reliably Improve the Robustness to Image Acquisition of Remote Sensing of PV Systems?. (arXiv:2309.12214v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12214</link>
<description rdf:parseType="Literal">&lt;p&gt;Photovoltaic (PV) energy is crucial for the decarbonization of energy
systems. Due to the lack of centralized data, remote sensing of rooftop PV
installations is the best option to monitor the evolution of the rooftop PV
installed fleet at a regional scale. However, current techniques lack
reliability and are notably sensitive to shifts in the acquisition conditions.
To overcome this, we leverage the wavelet scale attribution method (WCAM),
which decomposes a model&apos;s prediction in the space-scale domain. The WCAM
enables us to assess on which scales the representation of a PV model rests and
provides insights to derive methods that improve the robustness to acquisition
conditions, thus increasing trust in deep learning systems to encourage their
use for the safe integration of clean energy in electric systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasmi_G/0/1/0/all/0/1&quot;&gt;Gabriel Kasmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubus_L/0/1/0/all/0/1&quot;&gt;Laurent Dubus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saint_Drenan_Y/0/1/0/all/0/1&quot;&gt;Yves-Marie Saint-Drenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanc_P/0/1/0/all/0/1&quot;&gt;Philippe Blanc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16536">
<title>Uncertainty Quantification for Eosinophil Segmentation. (arXiv:2309.16536v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16536</link>
<description rdf:parseType="Literal">&lt;p&gt;Eosinophilic Esophagitis (EoE) is an allergic condition increasing in
prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils
within a single high-power field (400X magnification). Determining whether or
not a patient has EoE can be an arduous process and any medical imaging
approaches used to assist diagnosis must consider both efficiency and
precision. We propose an improvement of Adorno et al&apos;s approach for quantifying
eosinphils using deep image segmentation. Our new approach leverages Monte
Carlo Dropout, a common approach in deep learning to reduce overfitting, to
provide uncertainty quantification on current deep learning models. The
uncertainty can be visualized in an output image to evaluate model performance,
provide insight to how deep learning algorithms function, and assist
pathologists in identifying eosinophils.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Donald Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Syed_S/0/1/0/all/0/1&quot;&gt;Sana Syed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Greene_A/0/1/0/all/0/1&quot;&gt;Adam Greene&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03661">
<title>Robustness-Guided Image Synthesis for Data-Free Quantization. (arXiv:2310.03661v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03661</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization has emerged as a promising direction for model compression.
Recently, data-free quantization has been widely studied as a promising method
to avoid privacy concerns, which synthesizes images as an alternative to real
training data. Existing methods use classification loss to ensure the
reliability of the synthesized images. Unfortunately, even if these images are
well-classified by the pre-trained model, they still suffer from low semantics
and homogenization issues. Intuitively, these low-semantic images are sensitive
to perturbations, and the pre-trained model tends to have inconsistent output
when the generator synthesizes an image with poor semantics. To this end, we
propose Robustness-Guided Image Synthesis (RIS), a simple but effective method
to enrich the semantics of synthetic images and improve image diversity,
further boosting the performance of downstream data-free compression tasks.
Concretely, we first introduce perturbations on input and model weight, then
define the inconsistency metrics at feature and prediction levels before and
after perturbations. On the basis of inconsistency on two levels, we design a
robustness optimization objective to enhance the semantics of synthetic images.
Moreover, we also make our approach diversity-aware by forcing the generator to
synthesize images with small correlations in the label space. With RIS, we
achieve state-of-the-art performance for various settings on data-free
quantization and can be extended to other data-free compression tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jianhong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1&quot;&gt;Huanpeng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruizhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_L/0/1/0/all/0/1&quot;&gt;Lianrui Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Chengfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoji Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04655">
<title>VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models. (arXiv:2310.04655v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04655</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language (VL) pre-trained models have shown their superiority on many
multimodal tasks. However, the adversarial robustness of such models has not
been fully explored. Existing approaches mainly focus on exploring the
adversarial robustness under the white-box setting, which is unrealistic. In
this paper, we aim to investigate a new yet practical task to craft image and
text perturbations using pre-trained VL models to attack black-box fine-tuned
models on different downstream tasks. Towards this end, we propose VLAttack to
generate adversarial samples by fusing perturbations of images and texts from
both single-modal and multimodal levels. At the single-modal level, we propose
a new block-wise similarity attack (BSA) strategy to learn image perturbations
for disrupting universal representations. Besides, we adopt an existing text
attack strategy to generate text perturbations independent of the image-modal
attack. At the multimodal level, we design a novel iterative cross-search
attack (ICSA) method to update adversarial image-text pairs periodically,
starting with the outputs from the single-modal level. We conduct extensive
experiments to attack three widely-used VL pretrained models for six tasks on
eight datasets. Experimental results show that the proposed VLAttack framework
achieves the highest attack success rates on all tasks compared with
state-of-the-art baselines, which reveals a significant blind spot in the
deployment of pre-trained VL models. Codes will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Muchao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianrong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1&quot;&gt;Tianyu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jinguo Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinghui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fenglong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09689">
<title>A Partially Supervised Reinforcement Learning Framework for Visual Active Search. (arXiv:2310.09689v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09689</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual active search (VAS) has been proposed as a modeling framework in which
visual cues are used to guide exploration, with the goal of identifying regions
of interest in a large geospatial area. Its potential applications include
identifying hot spots of rare wildlife poaching activity, search-and-rescue
scenarios, identifying illegal trafficking of weapons, drugs, or people, and
many others. State of the art approaches to VAS include applications of deep
reinforcement learning (DRL), which yield end-to-end search policies, and
traditional active search, which combines predictions with custom algorithmic
approaches. While the DRL framework has been shown to greatly outperform
traditional active search in such domains, its end-to-end nature does not make
full use of supervised information attained either during training, or during
actual search, a significant limitation if search tasks differ significantly
from those in the training distribution. We propose an approach that combines
the strength of both DRL and conventional active search by decomposing the
search policy into a prediction module, which produces a geospatial
distribution of regions of interest based on task embedding and search history,
and a search module, which takes the predictions and search history as input
and outputs the search distribution. We develop a novel meta-learning approach
for jointly learning the resulting combined policy that can make effective use
of supervised information obtained both at training and decision time. Our
extensive experiments demonstrate that the proposed representation and
meta-learning frameworks significantly outperform state of the art in visual
active search on several problem domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Anindya Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1&quot;&gt;Nathan Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1&quot;&gt;Yevgeniy Vorobeychik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16870">
<title>MACP: Efficient Model Adaptation for Cooperative Perception. (arXiv:2310.16870v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16870</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception
capabilities of connected and automated vehicles (CAVs) by enabling information
sharing to &quot;see through the occlusions&quot;, resulting in significant performance
improvements. However, developing and training complex multi-agent perception
models from scratch can be expensive and unnecessary when existing single-agent
models show remarkable generalization capabilities. In this paper, we propose a
new framework termed MACP, which equips a single-agent pre-trained model with
cooperation capabilities. We approach this objective by identifying the key
challenges of shifting from single-agent to cooperative settings, adapting the
model by freezing most of its parameters and adding a few lightweight modules.
We demonstrate in our experiments that the proposed framework can effectively
utilize cooperative observations and outperform other state-of-the-art
approaches in both simulated and real-world cooperative perception benchmarks
while requiring substantially fewer tunable parameters with reduced
communication costs. Our source code is available at
https://github.com/PurdueDigitalTwin/MACP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Juanwu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenqian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziran Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16872">
<title>SonoSAMTrack -- Segment and Track Anything on Ultrasound Images. (arXiv:2310.16872v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16872</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present SonoSAM - a promptable foundational model for
segmenting objects of interest on ultrasound images, followed by state of the
art tracking model to perform segmentations on 2D+t and 3D ultrasound datasets.
Fine-tuned exclusively on a rich, diverse set of objects from $\approx200$k
ultrasound image-mask pairs, SonoSAM demonstrates state-of-the-art performance
on $8$ unseen ultrasound data-sets, outperforming competing methods by a
significant margin on all metrics of interest. SonoSAM achieves average dice
similarity score of $&amp;gt;90\%$ on almost all test data-sets within 2-6 clicks on
an average, making it a valuable tool for annotating ultrasound images. We also
extend SonoSAM to 3-D (2-D +t) applications and demonstrate superior
performance making it a valuable tool for generating dense annotations from
ultrasound cine-loops. Further, to increase practical utility of SonoSAM, we
propose a two-step process of fine-tuning followed by knowledge distillation to
a smaller footprint model without comprising the performance. We present
detailed qualitative and quantitative comparisons of SonoSAM with
state-of-the-art methods showcasing efficacy of SonoSAM as one of the first
reliable, generic foundational model for ultrasound.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravishankar_H/0/1/0/all/0/1&quot;&gt;Hariharan Ravishankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Patil_R/0/1/0/all/0/1&quot;&gt;Rohan Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Melapudi_V/0/1/0/all/0/1&quot;&gt;Vikram Melapudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anzengruber_S/0/1/0/all/0/1&quot;&gt;Stephan Anzengruber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhatia_P/0/1/0/all/0/1&quot;&gt;Parminder Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taha_K/0/1/0/all/0/1&quot;&gt;Kass-Hout Taha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Annangi_P/0/1/0/all/0/1&quot;&gt;Pavan Annangi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18626">
<title>Benchmark Generation Framework with Customizable Distortions for Image Classifier Robustness. (arXiv:2310.18626v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18626</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for generating adversarial benchmarks to
evaluate the robustness of image classification models. Our framework allows
users to customize the types of distortions to be optimally applied to images,
which helps address the specific distortions relevant to their deployment. The
benchmark can generate datasets at various distortion levels to assess the
robustness of different image classifiers. Our results show that the
adversarial samples generated by our framework with any of the image
classification models, like ResNet-50, Inception-V3, and VGG-16, are effective
and transferable to other models causing them to fail. These failures happen
even when these models are adversarially retrained using state-of-the-art
techniques, demonstrating the generalizability of our adversarial samples. We
achieve competitive performance in terms of net $L_2$ distortion compared to
state-of-the-art benchmark techniques on CIFAR-10 and ImageNet; however, we
demonstrate our framework achieves such results with simple distortions like
Gaussian noise without introducing unnatural artifacts or color bleeds. This is
made possible by a model-based reinforcement learning (RL) agent and a
technique that reduces a deep tree search of the image for model sensitivity to
perturbations, to a one-level analysis and action. The flexibility of choosing
distortions and setting classification probability thresholds for multiple
classes makes our framework suitable for algorithmic audits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumyendu Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1&quot;&gt;Ashwin Ramesh Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Sajad Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmichael_Z/0/1/0/all/0/1&quot;&gt;Zachariah Carmichael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundecha_V/0/1/0/all/0/1&quot;&gt;Vineet Gundecha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbanpour_S/0/1/0/all/0/1&quot;&gt;Sahand Ghorbanpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luna_R/0/1/0/all/0/1&quot;&gt;Ricardo Luna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillen_G/0/1/0/all/0/1&quot;&gt;Gutierrez Antonio Guillen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naug_A/0/1/0/all/0/1&quot;&gt;Avisek Naug&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18660">
<title>Foundation Models for Generalist Geospatial Artificial Intelligence. (arXiv:2310.18660v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18660</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant progress in the development of highly adaptable and reusable
Artificial Intelligence (AI) models is expected to have a significant impact on
Earth science and remote sensing. Foundation models are pre-trained on large
unlabeled datasets through self-supervision, and then fine-tuned for various
downstream tasks with small labeled datasets. This paper introduces a
first-of-a-kind framework for the efficient pre-training and fine-tuning of
foundational models on extensive geospatial data. We have utilized this
framework to create Prithvi, a transformer-based geospatial foundational model
pre-trained on more than 1TB of multispectral satellite imagery from the
Harmonized Landsat-Sentinel 2 (HLS) dataset. Our study demonstrates the
efficacy of our framework in successfully fine-tuning Prithvi to a range of
Earth observation tasks that have not been tackled by previous work on
foundation models involving multi-temporal cloud gap imputation, flood mapping,
wildfire scar segmentation, and multi-temporal crop segmentation. Our
experiments show that the pre-trained model accelerates the fine-tuning process
compared to leveraging randomly initialized weights. In addition, pre-trained
Prithvi compares well against the state-of-the-art, e.g., outperforming a
conditional GAN model in multi-temporal cloud imputation by up to 5pp (or 5.7%)
in the structural similarity index. Finally, due to the limited availability of
labeled data in the field of Earth observation, we gradually reduce the
quantity of available labeled data for refining the model to evaluate data
efficiency and demonstrate that data can be decreased significantly without
affecting the model&apos;s accuracy. The pre-trained 100 million parameter model and
corresponding fine-tuning workflows have been released publicly as open source
contributions to the global Earth sciences community through Hugging Face.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubik_J/0/1/0/all/0/1&quot;&gt;Johannes Jakubik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sujit Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_C/0/1/0/all/0/1&quot;&gt;C. E. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraccaro_P/0/1/0/all/0/1&quot;&gt;Paolo Fraccaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godwin_D/0/1/0/all/0/1&quot;&gt;Denys Godwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1&quot;&gt;Bianca Zadrozny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szwarcman_D/0/1/0/all/0/1&quot;&gt;Daniela Szwarcman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1&quot;&gt;Carlos Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nyirjesy_G/0/1/0/all/0/1&quot;&gt;Gabby Nyirjesy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_B/0/1/0/all/0/1&quot;&gt;Blair Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kimura_D/0/1/0/all/0/1&quot;&gt;Daiki Kimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simumba_N/0/1/0/all/0/1&quot;&gt;Naomi Simumba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1&quot;&gt;Linsong Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukkavilli_S/0/1/0/all/0/1&quot;&gt;S. Karthik Mukkavilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambhate_D/0/1/0/all/0/1&quot;&gt;Devyani Lambhate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_K/0/1/0/all/0/1&quot;&gt;Kamal Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bangalore_R/0/1/0/all/0/1&quot;&gt;Ranjini Bangalore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1&quot;&gt;Dario Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muszynski_M/0/1/0/all/0/1&quot;&gt;Michal Muszynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ankur_K/0/1/0/all/0/1&quot;&gt;Kumar Ankur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasubramanian_M/0/1/0/all/0/1&quot;&gt;Muthukumaran Ramasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurung_I/0/1/0/all/0/1&quot;&gt;Iksha Gurung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khallaghi_S/0/1/0/all/0/1&quot;&gt;Sam Khallaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanxi/0/1/0/all/0/1&quot;&gt;Hanxi&lt;/a&gt; (Steve)Li, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecil_M/0/1/0/all/0/1&quot;&gt;Michael Cecil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_M/0/1/0/all/0/1&quot;&gt;Maryam Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kordi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Kordi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemohammad_H/0/1/0/all/0/1&quot;&gt;Hamed Alemohammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maskey_M/0/1/0/all/0/1&quot;&gt;Manil Maskey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganti_R/0/1/0/all/0/1&quot;&gt;Raghu Ganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weldemariam_K/0/1/0/all/0/1&quot;&gt;Kommy Weldemariam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1&quot;&gt;Rahul Ramachandran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18840">
<title>Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models. (arXiv:2310.18840v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18840</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized text-to-image (T2I) synthesis based on diffusion models has
attracted significant attention in recent research. However, existing methods
primarily concentrate on customizing subjects or styles, neglecting the
exploration of global geometry. In this study, we propose an approach that
focuses on the customization of 360-degree panoramas, which inherently possess
global geometric properties, using a T2I diffusion model. To achieve this, we
curate a paired image-text dataset specifically designed for the task and
subsequently employ it to fine-tune a pre-trained T2I diffusion model with
LoRA. Nevertheless, the fine-tuned model alone does not ensure the continuity
between the leftmost and rightmost sides of the synthesized images, a crucial
characteristic of 360-degree panoramas. To address this issue, we propose a
method called StitchDiffusion. Specifically, we perform pre-denoising
operations twice at each time step of the denoising process on the stitch block
consisting of the leftmost and rightmost image regions. Furthermore, a global
cropping is adopted to synthesize seamless 360-degree panoramas. Experimental
results demonstrate the effectiveness of our customized model combined with the
proposed StitchDiffusion in generating high-quality 360-degree panoramic
images. Moreover, our customized model exhibits exceptional generalization
ability in producing scenes unseen in the fine-tuning dataset. Code is
available at https://github.com/littlewhitesea/StitchDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jing-Hao Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20101">
<title>Medical Image Denosing via Explainable AI Feature Preserving Loss. (arXiv:2310.20101v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20101</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising algorithms play a crucial role in medical image processing and
analysis. However, classical denoising algorithms often ignore explanatory and
critical medical features preservation, which may lead to misdiagnosis and
legal liabilities. In this work, we propose a new denoising method for medical
images that not only efficiently removes various types of noise, but also
preserves key medical features throughout the process. To achieve this goal, we
utilize a gradient-based eXplainable Artificial Intelligence (XAI) approach to
design a feature preserving loss function. Our feature preserving loss function
is motivated by the characteristic that gradient-based XAI is sensitive to
noise. Through backpropagation, medical image features before and after
denoising can be kept consistent. We conducted extensive experiments on three
available medical image datasets, including synthesized 13 different types of
noise and artifacts. The experimental results demonstrate the superiority of
our method in terms of denoising performance, model explainability, and
generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_G/0/1/0/all/0/1&quot;&gt;Guanfang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Anup Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00332">
<title>SDF4CHD: Generative Modeling of Cardiac Anatomies with Congenital Heart Defects. (arXiv:2311.00332v2 [q-bio.TO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00332</link>
<description rdf:parseType="Literal">&lt;p&gt;Congenital heart disease (CHD) encompasses a spectrum of cardiovascular
structural abnormalities, often requiring customized treatment plans for
individual patients. Computational modeling and analysis of these unique
cardiac anatomies can improve diagnosis and treatment planning and may
ultimately lead to improved outcomes. Deep learning (DL) methods have
demonstrated the potential to enable efficient treatment planning by automating
cardiac segmentation and mesh construction for patients with normal cardiac
anatomies. However, CHDs are often rare, making it challenging to acquire
sufficiently large patient cohorts for training such DL models. Generative
modeling of cardiac anatomies has the potential to fill this gap via the
generation of virtual cohorts; however, prior approaches were largely designed
for normal anatomies and cannot readily capture the significant topological
variations seen in CHD patients. Therefore, we propose a type- and
shape-disentangled generative approach suitable to capture the wide spectrum of
cardiac anatomies observed in different CHD types and synthesize differently
shaped cardiac anatomies that preserve the unique topology for specific CHD
types. Our DL approach represents generic whole heart anatomies with CHD
type-specific abnormalities implicitly using signed distance fields (SDF) based
on CHD type diagnosis, which conveniently captures divergent anatomical
variations across different types and represents meaningful intermediate CHD
states. To capture the shape-specific variations, we then learn invertible
deformations to morph the learned CHD type-specific anatomies and reconstruct
patient-specific shapes. Our approach has the potential to augment the
image-segmentation pairs for rarer CHD types for cardiac segmentation and
generate cohorts of CHD cardiac meshes for computational simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fanwei Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Stocker_S/0/1/0/all/0/1&quot;&gt;Sascha Stocker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Choi_P/0/1/0/all/0/1&quot;&gt;Perry S. Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Michael Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ennis_D/0/1/0/all/0/1&quot;&gt;Daniel B. Ennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Marsden_A/0/1/0/all/0/1&quot;&gt;Alison Marsden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00660">
<title>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain. (arXiv:2311.00660v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00660</link>
<description rdf:parseType="Literal">&lt;p&gt;Rain generation algorithms have the potential to improve the generalization
of deraining methods and scene understanding in rainy conditions. However, in
practice, they produce artifacts and distortions and struggle to control the
amount of rain generated due to a lack of proper constraints. In this paper, we
propose an unpaired image-to-image translation framework for generating
realistic rainy images. We first introduce a Triangular Probability Similarity
(TPS) constraint to guide the generated images toward clear and rainy images in
the discriminator manifold, thereby minimizing artifacts and distortions during
rain generation. Unlike conventional contrastive learning approaches, which
indiscriminately push negative samples away from the anchors, we propose a
Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing
force of negative samples based on the semantic similarity between the clear
and the rainy images and the feature similarity between the anchor and the
negative samples. Experiments demonstrate realistic rain generation with
minimal artifacts and distortions, which benefits image deraining and object
detection in rain. Furthermore, the method can be used to generate realistic
snowy and night images, underscoring its potential for broader applicability.
Code is available at https://github.com/ShenZheng2000/TPSeNCE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Changjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1&quot;&gt;Srinivasa G. Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01813">
<title>FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation. (arXiv:2311.01813v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01813</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, open-domain text-to-video (T2V) generation models have made
remarkable progress. However, the promising results are mainly shown by the
qualitative cases of generated videos, while the quantitative evaluation of T2V
models still faces two critical problems. Firstly, existing studies lack
fine-grained evaluation of T2V models on different categories of text prompts.
Although some benchmarks have categorized the prompts, their categorization
either only focuses on a single aspect or fails to consider the temporal
information in video generation. Secondly, it is unclear whether the automatic
evaluation metrics are consistent with human standards. To address these
problems, we propose FETV, a benchmark for Fine-grained Evaluation of
Text-to-Video generation. FETV is multi-aspect, categorizing the prompts based
on three orthogonal aspects: the major content, the attributes to control and
the prompt complexity. FETV is also temporal-aware, which introduces several
temporal categories tailored for video generation. Based on FETV, we conduct
comprehensive manual evaluations of four representative T2V models, revealing
their pros and cons on different categories of prompts from different aspects.
We also extend FETV as a testbed to evaluate the reliability of automatic T2V
metrics. The multi-aspect categorization of FETV enables fine-grained analysis
of the metrics&apos; reliability in different scenarios. We find that existing
automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human
evaluation. To address this problem, we explore several solutions to improve
CLIPScore and FVD, and develop two automatic metrics that exhibit significant
higher correlation with humans than existing metrics. Benchmark page:
https://github.com/llyx97/FETV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rundong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sishuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lu Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02926">
<title>Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things. (arXiv:2311.02926v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02926</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of Artificial Intelligent Internet of Things
(AIoT), the image data from AIoT devices has been witnessing the explosive
increasing. In this paper, a novel deep image semantic communication model is
proposed for the efficient image communication in AIoT. Particularly, at the
transmitter side, a high-precision image semantic segmentation algorithm is
proposed to extract the semantic information of the image to achieve
significant compression of the image data. At the receiver side, a semantic
image restoration algorithm based on Generative Adversarial Network (GAN) is
proposed to convert the semantic image to a real scene image with detailed
information. Simulation results demonstrate that the proposed image semantic
communication model can improve the image compression ratio and recovery
accuracy by 71.93% and 25.07% on average in comparison with WebP and CycleGAN,
respectively. More importantly, our demo experiment shows that the proposed
model reduces the total delay by 95.26% in the image communication, when
comparing with the original image transmission.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Li Ping Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Sikai Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xuemin Sherman Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoniu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03725">
<title>DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries. (arXiv:2311.03725v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03725</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing Convolutional Neural Networks (CNNs), Recurrent Neural Networks
(RNNs), and Generative Adversarial Networks (GANs), our system introduces an
innovative approach to defect detection in manufacturing. This technology
excels in precisely identifying faults by extracting intricate details from
product photographs, utilizing RNNs to detect evolving errors and generating
synthetic defect data to bolster the model&apos;s robustness and adaptability across
various defect scenarios. The project leverages a deep learning framework to
automate real-time flaw detection in the manufacturing process. It harnesses
extensive datasets of annotated images to discern complex defect patterns. This
integrated system seamlessly fits into production workflows, thereby boosting
efficiency and elevating product quality. As a result, it reduces waste and
operational costs, ultimately enhancing market competitiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumbhar_A/0/1/0/all/0/1&quot;&gt;Arti Kumbhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chougule_A/0/1/0/all/0/1&quot;&gt;Amruta Chougule&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokhande_P/0/1/0/all/0/1&quot;&gt;Priya Lokhande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navaghane_S/0/1/0/all/0/1&quot;&gt;Saloni Navaghane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burud_A/0/1/0/all/0/1&quot;&gt;Aditi Burud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nimbalkar_S/0/1/0/all/0/1&quot;&gt;Saee Nimbalkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03784">
<title>UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields. (arXiv:2311.03784v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03784</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has enabled novel view synthesis with high
fidelity given images and camera poses. Subsequent works even succeeded in
eliminating the necessity of pose priors by jointly optimizing NeRF and camera
pose. However, these works are limited to relatively simple settings such as
photometrically consistent and occluder-free image collections or a sequence of
images from a video. So they have difficulty handling unconstrained images with
varying illumination and transient occluders. In this paper, we propose
$\textbf{UP-NeRF}$ ($\textbf{U}$nconstrained $\textbf{P}$ose-prior-free
$\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields) to optimize NeRF with
unconstrained image collections without camera pose prior. We tackle these
challenges with surrogate tasks that optimize color-insensitive feature fields
and a separate module for transient occluders to block their influence on pose
estimation. In addition, we introduce a candidate head to enable more robust
pose estimation and transient-aware depth supervision to minimize the effect of
incorrect prior. Our experiments verify the superior performance of our method
compared to the baselines including BARF and its variants in a challenging
internet photo collection, $\textit{Phototourism}$ dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Injae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minhyuk Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03959">
<title>Improving the Effectiveness of Deep Generative Data. (arXiv:2311.03959v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03959</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent deep generative models (DGMs) such as generative adversarial networks
(GANs) and diffusion probabilistic models (DPMs) have shown their impressive
ability in generating high-fidelity photorealistic images. Although looking
appealing to human eyes, training a model on purely synthetic images for
downstream image processing tasks like image classification often results in an
undesired performance drop compared to training on real data. Previous works
have demonstrated that enhancing a real dataset with synthetic images from DGMs
can be beneficial. However, the improvements were subjected to certain
circumstances and yet were not comparable to adding the same number of real
images. In this work, we propose a new taxonomy to describe factors
contributing to this commonly observed phenomenon and investigate it on the
popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a
large portion of the performance drop when using synthetic images from DGM and
propose strategies to better utilize them in downstream tasks. Extensive
experiments on multiple datasets showcase that our method outperforms baselines
on downstream classification tasks both in case of training on synthetic only
(Synthetic-to-Real) and training on a mix of real and synthetic data (Data
Augmentation), particularly in the data-scarce scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmedding_S/0/1/0/all/0/1&quot;&gt;Sabrina Schmedding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco F. Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04212">
<title>Video Instance Matting. (arXiv:2311.04212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04212</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional video matting outputs one alpha matte for all instances
appearing in a video frame so that individual instances are not distinguished.
While video instance segmentation provides time-consistent instance masks,
results are unsatisfactory for matting applications, especially due to applied
binarization. To remedy this deficiency, we propose Video Instance
Matting~(VIM), that is, estimating alpha mattes of each instance at each frame
of a video sequence. To tackle this challenging problem, we present MSG-VIM, a
Mask Sequence Guided Video Instance Matting neural network, as a novel baseline
model for VIM. MSG-VIM leverages a mixture of mask augmentations to make
predictions robust to inaccurate and inconsistent mask guidance. It
incorporates temporal mask and temporal feature guidance to improve the
temporal consistency of alpha matte predictions. Furthermore, we build a new
benchmark for VIM, called VIM50, which comprises 50 video clips with multiple
human instances as foreground objects. To evaluate performances on the VIM
task, we introduce a suitable metric called Video Instance-aware Matting
Quality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50
benchmark and outperforms existing methods by a large margin. The project is
open-sourced at https://github.com/SHI-Labs/VIM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henschel_R/0/1/0/all/0/1&quot;&gt;Roberto Henschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1&quot;&gt;Vidit Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohanyan_M/0/1/0/all/0/1&quot;&gt;Marianna Ohanyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navasardyan_S/0/1/0/all/0/1&quot;&gt;Shant Navasardyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>