<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.02057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.03719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.08790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.09920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08558" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.00664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07357" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.08706">
<title>Efficient Strongly Polynomial Algorithms for Quantile Regression. (arXiv:2307.08706v1 [cs.CG])</title>
<link>http://arxiv.org/abs/2307.08706</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear Regression is a seminal technique in statistics and machine learning,
where the objective is to build linear predictive models between a response
(i.e., dependent) variable and one or more predictor (i.e., independent)
variables. In this paper, we revisit the classical technique of Quantile
Regression (QR), which is statistically a more robust alternative to the other
classical technique of Ordinary Least Square Regression (OLS). However, while
there exist efficient algorithms for OLS, almost all of the known results for
QR are only weakly polynomial. Towards filling this gap, this paper proposes
several efficient strongly polynomial algorithms for QR for various settings.
For two dimensional QR, making a connection to the geometric concept of
$k$-set, we propose an algorithm with a deterministic worst-case time
complexity of $\mathcal{O}(n^{4/3} polylog(n))$ and an expected time complexity
of $\mathcal{O}(n^{4/3})$ for the randomized version. We also propose a
randomized divide-and-conquer algorithm -- RandomizedQR with an expected time
complexity of $\mathcal{O}(n\log^2{(n)})$ for two dimensional QR problem. For
the general case with more than two dimensions, our RandomizedQR algorithm has
an expected time complexity of $\mathcal{O}(n^{d-1}\log^2{(n)})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetiya_S/0/1/0/all/0/1&quot;&gt;Suraj Shetiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1&quot;&gt;Shohedul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asudeh_A/0/1/0/all/0/1&quot;&gt;Abolfazl Asudeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_G/0/1/0/all/0/1&quot;&gt;Gautam Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08712">
<title>Machine Learning Meets Mental Training -- A Proof of Concept Applied to Memory Sports. (arXiv:2307.08712v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08712</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to combine these two fields together by presenting a practical
implementation of machine learning to the particular form of mental training
that is the art of memory, taken in its competitive version called &quot;Memory
Sports&quot;. Such a fusion, on the one hand, strives to raise awareness about both
realms, while on the other it seeks to encourage research in this mixed field
as a way to, ultimately, drive forward the development of this seemingly
underestimated sport.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regnani_E/0/1/0/all/0/1&quot;&gt;Emanuele Regnani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08713">
<title>Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers. (arXiv:2307.08713v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08713</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of data classification, broad learning system (BLS) has proven
to be a potent tool that utilizes a layer-by-layer feed-forward neural network.
It consists of feature learning and enhancement segments, working together to
extract intricate features from input data. The traditional BLS treats all
samples as equally significant, which makes it less robust and less effective
for real-world datasets with noises and outliers. To address this issue, we
propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to
each training point to reduce the influence of noises and outliers. In
assigning the membership value, the F-BLS model solely considers the distance
from samples to the class center in the original feature space without
incorporating the extent of non-belongingness to a class. We further propose a
novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS
utilizes intuitionistic fuzzy numbers based on fuzzy membership and
non-membership values to assign scores to training points in the
high-dimensional feature space by using a kernel function. We evaluate the
performance of proposed F-BLS and IF-BLS models on 44 UCI benchmark datasets
across diverse domains. Furthermore, Gaussian noise is added to some UCI
datasets to assess the robustness of the proposed F-BLS and IF-BLS models.
Experimental results demonstrate superior generalization performance of the
proposed F-BLS and IF-BLS models compared to baseline models, both with and
without Gaussian noise. Additionally, we implement the proposed F-BLS and
IF-BLS models on the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset,
and promising results showcase the models effectiveness in real-world
applications. The proposed methods offer a promising solution to enhance the
BLS frameworks ability to handle noise and outliers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajid_M/0/1/0/all/0/1&quot;&gt;M. Sajid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1&quot;&gt;A.K. Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1&quot;&gt;M. Tanveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08722">
<title>Certifying the Fairness of KNN in the Presence of Dataset Bias. (arXiv:2307.08722v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08722</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for certifying the fairness of the classification result
of a widely used supervised learning algorithm, the k-nearest neighbors (KNN),
under the assumption that the training data may have historical bias caused by
systematic mislabeling of samples from a protected minority group. To the best
of our knowledge, this is the first certification method for KNN based on three
variants of the fairness definition: individual fairness, $\epsilon$-fairness,
and label-flipping fairness. We first define the fairness certification problem
for KNN and then propose sound approximations of the complex arithmetic
computations used in the state-of-the-art KNN algorithm. This is meant to lift
the computation results from the concrete domain to an abstract domain, to
reduce the computational cost. We show effectiveness of this abstract
interpretation based technique through experimental evaluation on six datasets
widely used in the fairness research literature. We also show that the method
is accurate enough to obtain fairness certifications for a large number of test
inputs, despite the presence of historical bias in the datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yannan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08753">
<title>A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology. (arXiv:2307.08753v1 [astro-ph.SR])</title>
<link>http://arxiv.org/abs/2307.08753</link>
<description rdf:parseType="Literal">&lt;p&gt;Stellar ages are critical building blocks of evolutionary models, but
challenging to measure for low mass main sequence stars. An unexplored solution
in this regime is the application of probabilistic machine learning methods to
gyrochronology, a stellar dating technique that is uniquely well suited for
these stars. While accurate analytical gyrochronological models have proven
challenging to develop, here we apply conditional normalizing flows to
photometric data from open star clusters, and demonstrate that a data-driven
approach can constrain gyrochronological ages with a precision comparable to
other standard techniques. We evaluate the flow results in the context of a
Bayesian framework, and show that our inferred ages recover literature values
well. This work demonstrates the potential of a probabilistic data-driven
solution to widen the applicability of gyrochronological stellar dating.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Van_Lane_P/0/1/0/all/0/1&quot;&gt;Phil Van-Lane&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Speagle_J/0/1/0/all/0/1&quot;&gt;Joshua S. Speagle&lt;/a&gt; (2 and 1 and 3 and 4), &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Douglas_S/0/1/0/all/0/1&quot;&gt;Stephanie Douglas&lt;/a&gt; (5) ((1) Department of Astronomy &amp;amp; Astrophysics, University of Toronto, Canada, (2) Department of Statistical Sciences, University of Toronto, Canada, (3) Dunlap Institute of Astronomy &amp;amp; Astrophysics, University of Toronto, Canada, (4) Data Sciences Institute, University of Toronto, Canada, (5) Department of Physics, Lafayette College, United States)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08766">
<title>Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices. (arXiv:2307.08766v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08766</link>
<description rdf:parseType="Literal">&lt;p&gt;Photoplethysmography (PPG) is a non-invasive technology that measures changes
in blood volume in the microvascular bed of tissue. It is commonly used in
medical devices such as pulse oximeters and wrist worn heart rate monitors to
monitor cardiovascular hemodynamics. PPG allows for the assessment of
parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that
can indicate conditions such as vasoconstriction or vasodilation, and provides
information about microvascular blood flow, making it a valuable tool for
monitoring cardiovascular health. However, PPG is subject to a number of
sources of variations that can impact its accuracy and reliability, especially
when using a wearable device for continuous monitoring, such as motion
artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27
statistical features from the PPG signal for training machine-learning models
based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF)
algorithms to assess quality of PPG signals that were labeled as good or poor
quality. We used the PPG time series from a publicly available dataset and
evaluated the algorithm s performance using Sensitivity (Se), Positive
Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV,
and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for
CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are
comparable to state-of-the-art reported in the literature but using a much
simpler model, indicating that ML models are promising for developing remote,
non-invasive, and continuous measurement devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dias_F/0/1/0/all/0/1&quot;&gt;Felipe M. Dias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toledo_M/0/1/0/all/0/1&quot;&gt;Marcelo A. F. Toledo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardenas_D/0/1/0/all/0/1&quot;&gt;Diego A. C. Cardenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1&quot;&gt;Douglas A. Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1&quot;&gt;Filipe A. C. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_E/0/1/0/all/0/1&quot;&gt;Estela Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krieger_J/0/1/0/all/0/1&quot;&gt;Jose E. Krieger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_M/0/1/0/all/0/1&quot;&gt;Marco A. Gutierrez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08767">
<title>A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08767</link>
<description rdf:parseType="Literal">&lt;p&gt;When to solve math problems, most language models take a sampling strategy to
predict next word according conditional probabilities. In the math reasoning
step, it may generate wrong answer. Considering math problems are
deterministic, we propose a mixed policy exploration approach to solve math
problems with reinforcement learning. In peculiar, we propose a two level token
exploration policy: the abstract level explores next token with probability and
the second level is deterministic. Specifically, the abstract level policy will
decide whether the token is operator or operand with probability sampling,
while the second level is deterministic to select next token with the highest
score in a greedy way. We test our method on GSM8K dataset with GPT-2 model,
and demonstrate more than $2\%$ performance gain. Our implementation is
available at https://github.com/vividitytech/math_lm_rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08782">
<title>Unsupervised Learning of Distributional Properties can Supplement Human Labeling and Increase Active Learning Efficiency in Anomaly Detection. (arXiv:2307.08782v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Exfiltration of data via email is a serious cybersecurity threat for many
organizations. Detecting data exfiltration (anomaly) patterns typically
requires labeling, most often done by a human annotator, to reduce the high
number of false alarms. Active Learning (AL) is a promising approach for
labeling data efficiently, but it needs to choose an efficient order in which
cases are to be labeled, and there are uncertainties as to what scoring
procedure should be used to prioritize cases for labeling, especially when
detecting rare cases of interest is crucial. We propose an adaptive AL sampling
strategy that leverages the underlying prior data distribution, as well as
model uncertainty, to produce batches of cases to be labeled that contain
instances of rare anomalies. We show that (1) the classifier benefits from a
batch of representative and informative instances of both normal and anomalous
examples, (2) unsupervised anomaly detection plays a useful role in building
the classifier in the early stages of training when relatively little labeling
has been done thus far. Our approach to AL for anomaly detection outperformed
existing AL approaches on three highly unbalanced UCI benchmarks and on one
real-world redacted email data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kongmanee_J/0/1/0/all/0/1&quot;&gt;Jaturong Kongmanee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chignell_M/0/1/0/all/0/1&quot;&gt;Mark Chignell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerath_K/0/1/0/all/0/1&quot;&gt;Khilan Jerath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_A/0/1/0/all/0/1&quot;&gt;Abhay Raman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08794">
<title>Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08794</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-timescale multi-agent reinforcement learning (MARL), agents interact
across different timescales. In general, policies for time-dependent behaviors,
such as those induced by multiple timescales, are non-stationary. Learning
non-stationary policies is challenging and typically requires sophisticated or
inefficient algorithms. Motivated by the prevalence of this control problem in
real-world complex systems, we introduce a simple framework for learning
non-stationary policies for multi-timescale MARL. Our approach uses available
information about agent timescales to define a periodic time encoding. In
detail, we theoretically demonstrate that the effects of non-stationarity
introduced by multiple timescales can be learned by a periodic multi-agent
policy. To learn such policies, we propose a policy gradient algorithm that
parameterizes the actor and critic with phase-functioned neural networks, which
provide an inductive bias for periodicity. The framework&apos;s ability to
effectively learn multi-timescale policies is validated on a gridworld and
building energy management environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emami_P/0/1/0/all/0/1&quot;&gt;Patrick Emami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biagioni_D/0/1/0/all/0/1&quot;&gt;David Biagioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1&quot;&gt;Ahmed S. Zamzam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08796">
<title>Classification with Incoherent Kernel Dictionary Learning. (arXiv:2307.08796v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08796</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a new classification method based on Dictionary
Learning (DL). The main contribution consists of a kernel version of incoherent
DL, derived from its standard linear counterpart. We also propose an
improvement of the AK-SVD algorithm concerning the representation update. Our
algorithms are tested on several popular databases of classification problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1&quot;&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Dumitrescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08798">
<title>Reduced Kernel Dictionary Learning. (arXiv:2307.08798v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.08798</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present new algorithms for training reduced-size nonlinear
representations in the Kernel Dictionary Learning (KDL) problem. Standard KDL
has the drawback of a large size of the kernel matrix when the data set is
large. There are several ways of reducing the kernel size, notably Nystr\&quot;om
sampling. We propose here a method more in the spirit of dictionary learning,
where the kernel vectors are obtained with a trained sparse representation of
the input signals. Moreover, we optimize directly the kernel vectors in the KDL
process, using gradient descent steps. We show with three data sets that our
algorithms are able to provide better representations, despite using a small
number of kernel vectors, and also decrease the execution time with respect to
KDL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1&quot;&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dumitrescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Dumitrescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08800">
<title>regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data. (arXiv:2307.08800v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2307.08800</link>
<description rdf:parseType="Literal">&lt;p&gt;The regulAS software package is a bioinformatics tool designed to support
computational biology researchers in investigating regulatory mechanisms of
splicing alterations through integrative analysis of large-scale RNA-Seq data
from cancer and healthy human donors, characterized by TCGA and GTEx projects.
This technical report provides a comprehensive overview of regulAS, focusing on
its core functionality, basic modules, experiment configuration, further
extensibility and customisation.
&lt;/p&gt;
&lt;p&gt;The core functionality of regulAS enables the automation of computational
experiments, efficient results storage and processing, and streamlined workflow
management. Integrated basic modules extend regulAS with features such as
RNA-Seq data retrieval from the public multi-omics UCSC Xena data repository,
predictive modeling and feature ranking capabilities using the scikit-learn
package, and flexible reporting generation for analysing gene expression
profiles and relevant modulations of alternative splicing aberrations across
tissues and cancer types. Experiment configuration is handled through YAML
files with the Hydra and OmegaConf libraries, offering a user-friendly
approach. Additionally, regulAS allows for the development and integration of
custom modules to handle specialized tasks.
&lt;/p&gt;
&lt;p&gt;In conclusion, regulAS provides an automated solution for alternative
splicing and cancer biology studies, enhancing efficiency, reproducibility, and
customization of experimental design, while the extensibility of the pipeline
enables researchers to further tailor the software package to their specific
needs. Source code is available under the MIT license at
https://github.com/slipnitskaya/regulAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lipnitskaya_S/0/1/0/all/0/1&quot;&gt;Sofya Lipnitskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08801">
<title>Towards Automated Design of Riboswitches. (arXiv:2307.08801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08801</link>
<description rdf:parseType="Literal">&lt;p&gt;Experimental screening and selection pipelines for the discovery of novel
riboswitches are expensive, time-consuming, and inefficient. Using
computational methods to reduce the number of candidates for the screen could
drastically decrease these costs. However, existing computational approaches do
not fully satisfy all requirements for the design of such initial screening
libraries. In this work, we present a new method, libLEARNA, capable of
providing RNA focus libraries of diverse variable-length qualified candidates.
Our novel structure-based design approach considers global properties as well
as desired sequence and structure features. We demonstrate the benefits of our
method by designing theophylline riboswitch libraries, following a previously
published protocol, and yielding 30% more unique high-quality candidates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runge_F/0/1/0/all/0/1&quot;&gt;Frederic Runge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franke_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg K. H. Franke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08807">
<title>Anomaly Detection with Selective Dictionary Learning. (arXiv:2307.08807v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08807</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present new methods of anomaly detection based on Dictionary
Learning (DL) and Kernel Dictionary Learning (KDL). The main contribution
consists in the adaption of known DL and KDL algorithms in the form of
unsupervised methods, used for outlier detection. We propose a reduced kernel
version (RKDL), which is useful for problems with large data sets, due to the
large kernel matrix. We also improve the DL and RKDL methods by the use of a
random selection of signals, which aims to eliminate the outliers from the
training procedure. All our algorithms are introduced in an anomaly detection
toolbox and are compared to standard benchmark results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1&quot;&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Dumitrescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08809">
<title>Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08809</link>
<description rdf:parseType="Literal">&lt;p&gt;Many existing FL methods assume clients with fully-labeled data, while in
realistic settings, clients have limited labels due to the expensive and
laborious process of labeling. Limited labeled local data of the clients often
leads to their local model having poor generalization abilities to their larger
unlabeled local data, such as having class-distribution mismatch with the
unlabeled data. As a result, clients may instead look to benefit from the
global model trained across clients to leverage their unlabeled data, but this
also becomes difficult due to data heterogeneity across clients. In our work,
we propose FedLabel where clients selectively choose the local or global model
to pseudo-label their unlabeled data depending on which is more of an expert of
the data. We further utilize both the local and global models&apos; knowledge via
global-local consistency regularization which minimizes the divergence between
the two models&apos; outputs when they have identical pseudo-labels for the
unlabeled data. Unlike other semi-supervised FL baselines, our method does not
require additional experts other than the local or global model, nor require
additional parameters to be communicated. We also do not assume any
server-labeled data or fully labeled clients. For both cross-device and
cross-silo settings, we show that FedLabel outperforms other semi-supervised FL
baselines by $8$-$24\%$, and even outperforms standard fully supervised FL
baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yae Jee Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1&quot;&gt;Gauri Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Dimitriadis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08810">
<title>Operator Guidance Informed by AI-Augmented Simulations. (arXiv:2307.08810v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.08810</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper will present a multi-fidelity, data-adaptive approach with a Long
Short-Term Memory (LSTM) neural network to estimate ship response statistics in
bimodal, bidirectional seas. The study will employ a fast low-fidelity,
volume-based tool SimpleCode and a higher-fidelity tool known as the Large
Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by
common bi-modal, bi-directional sea conditions in the North Atlantic as
training data. After training an LSTM network with LAMP ship motion response
data, a sample route was traversed and randomly sampled historical weather was
input into SimpleCode and the LSTM network, and compared against the higher
fidelity results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_S/0/1/0/all/0/1&quot;&gt;Samuel J. Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_M/0/1/0/all/0/1&quot;&gt;Michael Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08811">
<title>DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08811</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models are overparameterized to support generality and
avoid overfitting. Prior works have shown that these additional parameters can
be used for both malicious (e.g., hiding a model covertly within a trained
model) and beneficial purposes (e.g., watermarking a model). In this paper, we
propose a novel information theoretic perspective of the problem; we consider
the ML model as a storage channel with a capacity that increases with
overparameterization. Specifically, we consider a sender that embeds arbitrary
information in the model at training time, which can be extracted by a receiver
with a black-box access to the deployed model. We derive an upper bound on the
capacity of the channel based on the number of available parameters. We then
explore black-box write and read primitives that allow the attacker to: (i)
store data in an optimized way within the model by augmenting the training data
at the transmitter side, and (ii) to read it by querying the model after it is
deployed. We also analyze the detectability of the writing primitive and
consider a new version of the problem which takes information storage
covertness into account. Specifically, to obtain storage covertness, we
introduce a new constraint such that the data augmentation used for the write
primitives minimizes the distribution shift with the initial (baseline task)
distribution. This constraint introduces a level of &quot;interference&quot; with the
initial task, thereby limiting the channel&apos;s effective capacity. Therefore, we
develop optimizations to improve the capacity in this case, including a novel
ML-specific substitution based error correction protocol. We believe that the
proposed modeling of the problem offers new tools to better understand and
mitigate potential vulnerabilities of ML, especially in the context of
increasingly large models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1&quot;&gt;Md Abdullah Al Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_Q/0/1/0/all/0/1&quot;&gt;Quazi Mishkatul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaigani_E/0/1/0/all/0/1&quot;&gt;Erfan Shaigani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaree_P/0/1/0/all/0/1&quot;&gt;Pedram Zaree&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1&quot;&gt;Ihsen Alouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1&quot;&gt;Nael Abu-Ghazaleh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08813">
<title>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.08813</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding protein interactions and pathway knowledge is crucial for
unraveling the complexities of living systems and investigating the underlying
mechanisms of biological functions and complex diseases. While existing
databases provide curated biological data from literature and other sources,
they are often incomplete and their maintenance is labor-intensive,
necessitating alternative approaches. In this study, we propose to harness the
capabilities of large language models to address these issues by automatically
extracting such knowledge from the relevant scientific literature. Toward this
goal, in this work, we investigate the effectiveness of different large
language models in tasks that involve recognizing protein interactions,
pathways, and gene regulatory relations. We thoroughly evaluate the performance
of various models, highlight the significant findings, and discuss both the
future opportunities and the remaining challenges associated with this
approach. The code and data are available at:
https://github.com/boxorange/BioIE-LLM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Gilchan Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_B/0/1/0/all/0/1&quot;&gt;Byung-Jun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xihaier Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Marrero_V/0/1/0/all/0/1&quot;&gt;Vanessa L&amp;#xf3;pez-Marrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnstone_P/0/1/0/all/0/1&quot;&gt;Patrick Johnstone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_F/0/1/0/all/0/1&quot;&gt;Francis J. Alexander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08816">
<title>Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08816</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic optimization (SO) attempts to offer optimal decisions in the
presence of uncertainty. Often, the classical formulation of these problems
becomes intractable due to (a) the number of scenarios required to capture the
uncertainty and (b) the discrete nature of real-world planning problems. To
overcome these tractability issues, practitioners turn to decomposition methods
that divide the problem into smaller, more tractable sub-problems. The focal
decomposition method of this paper is Benders decomposition (BD), which
decomposes stochastic optimization problems on the basis of scenario
independence. In this paper we propose a method of accelerating BD with the aid
of a surrogate model in place of an NP-hard integer master problem. Through the
acceleration method we observe 30% faster average convergence when compared to
other accelerated BD implementations. We introduce a reinforcement learning
agent as a surrogate and demonstrate how it can be used to solve a stochastic
inventory management problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mak_S/0/1/0/all/0/1&quot;&gt;Stephen Mak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mana_K/0/1/0/all/0/1&quot;&gt;Kyle Mana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zehtabi_P/0/1/0/all/0/1&quot;&gt;Parisa Zehtabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cashmore_M/0/1/0/all/0/1&quot;&gt;Michael Cashmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1&quot;&gt;Daniele Magazzeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1&quot;&gt;Manuela Veloso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08822">
<title>A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access. (arXiv:2307.08822v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.08822</link>
<description rdf:parseType="Literal">&lt;p&gt;In this letter, we propose the use of a meta-learning based precoder
optimization framework to directly optimize the Rate-Splitting Multiple Access
(RSMA) precoders with partial Channel State Information at the Transmitter
(CSIT). By exploiting the overfitting of the compact neural network to maximize
the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need
for any other training data while minimizing the total running time. Numerical
results reveal that the meta-learning based solution achieves similar ASR
performance to conventional precoder optimization in medium-scale scenarios,
and significantly outperforms sub-optimal low complexity precoder algorithms in
the large-scale regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Loli_R/0/1/0/all/0/1&quot;&gt;Rafael Cerna Loli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clerckx_B/0/1/0/all/0/1&quot;&gt;Bruno Clerckx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08840">
<title>Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War. (arXiv:2307.08840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08840</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithmic and data-driven decisions and recommendations are commonly used
in high-stakes decision-making settings such as criminal justice, medicine, and
public policy. We investigate whether it would have been possible to improve a
security assessment algorithm employed during the Vietnam War, using outcomes
measured immediately after its introduction in late 1969. This empirical
application raises several methodological challenges that frequently arise in
high-stakes algorithmic decision-making. First, before implementing a new
algorithm, it is essential to characterize and control the risk of yielding
worse outcomes than the existing algorithm. Second, the existing algorithm is
deterministic, and learning a new algorithm requires transparent extrapolation.
Third, the existing algorithm involves discrete decision tables that are common
but difficult to optimize over.
&lt;/p&gt;
&lt;p&gt;To address these challenges, we introduce the Average Conditional Risk
(ACRisk), which first quantifies the risk that a new algorithmic policy leads
to worse outcomes for subgroups of individual units and then averages this over
the distribution of subgroups. We also propose a Bayesian policy learning
framework that maximizes the posterior expected value while controlling the
posterior expected ACRisk. This framework separates the estimation of
heterogeneous treatment effects from policy optimization, enabling flexible
estimation of effects and optimization over complex policy classes. We
characterize the resulting chance-constrained optimization problem as a
constrained linear programming problem. Our analysis shows that compared to the
actual algorithm used during the Vietnam War, the learned algorithm assesses
most regions as more secure and emphasizes economic and political factors over
military factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zeyang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Michael_E/0/1/0/all/0/1&quot;&gt;Eli Ben-Michael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imai_K/0/1/0/all/0/1&quot;&gt;Kosuke Imai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08847">
<title>Privacy-preserving patient clustering for personalized federated learning. (arXiv:2307.08847v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08847</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a machine learning framework that enables multiple
organizations to train a model without sharing their data with a central
server. However, it experiences significant performance degradation if the data
is non-identically independently distributed (non-IID). This is a problem in
medical settings, where variations in the patient population contribute
significantly to distribution differences across hospitals. Personalized FL
addresses this issue by accounting for site-specific distribution differences.
Clustered FL, a Personalized FL variant, was used to address this problem by
clustering patients into groups across hospitals and training separate models
on each group. However, privacy concerns remained as a challenge as the
clustering process requires exchange of patient-level information. This was
previously solved by forming clusters using aggregated data, which led to
inaccurate groups and performance degradation. In this study, we propose
Privacy-preserving Community-Based Federated machine Learning (PCBFL), a novel
Clustered FL framework that can cluster patients using patient-level data while
protecting privacy. PCBFL uses Secure Multiparty Computation, a cryptographic
technique, to securely calculate patient-level similarity scores across
hospitals. We then evaluate PCBFL by training a federated mortality prediction
model using 20 sites from the eICU dataset. We compare the performance gain
from PCBFL against traditional and existing Clustered FL frameworks. Our
results show that PCBFL successfully forms clinically meaningful cohorts of
low, medium, and high-risk patients. PCBFL outperforms traditional and existing
Clustered FL frameworks with an average AUC improvement of 4.3% and AUPRC
improvement of 7.8%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhussein_A/0/1/0/all/0/1&quot;&gt;Ahmed Elhussein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gursoy_G/0/1/0/all/0/1&quot;&gt;Gamze Gursoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08849">
<title>Autoregressive Diffusion Model for Graph Generation. (arXiv:2307.08849v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.08849</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion-based graph generative models have recently obtained promising
results for graph generation. However, existing diffusion-based graph
generative models are mostly one-shot generative models that apply Gaussian
diffusion in the dequantized adjacency matrix space. Such a strategy can suffer
from difficulty in model training, slow sampling speed, and incapability of
incorporating constraints. We propose an \emph{autoregressive diffusion} model
for graph generation. Unlike existing methods, we define a node-absorbing
diffusion process that operates directly in the discrete graph space. For
forward diffusion, we design a \emph{diffusion ordering network}, which learns
a data-dependent node absorbing ordering from graph topology. For reverse
generation, we design a \emph{denoising network} that uses the reverse node
ordering to efficiently reconstruct the graph by predicting the node type of
the new node and its edges with previously denoised nodes at a time. Based on
the permutation invariance of graph, we show that the two networks can be
jointly trained by optimizing a simple lower bound of data likelihood. Our
experiments on six diverse generic graph datasets and two molecule datasets
show that our model achieves better or comparable generation performance with
previous state-of-the-art, and meanwhile enjoys fast generation speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingkai Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haotian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_B/0/1/0/all/0/1&quot;&gt;B. Aditya Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08857">
<title>An Admissible Shift-Consistent Method for Recommender Systems. (arXiv:2307.08857v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.08857</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a new constraint, called shift-consistency, for
solving matrix/tensor completion problems in the context of recommender
systems. Our method provably guarantees several key mathematical properties:
(1) satisfies a recently established admissibility criterion for recommender
systems; (2) satisfies a definition of fairness that eliminates a specific
class of potential opportunities for users to maliciously influence system
recommendations; and (3) offers robustness by exploiting provable uniqueness of
missing-value imputation. We provide a rigorous mathematical description of the
method, including its generalization from matrix to tensor form to permit
representation and exploitation of complex structural relationships among sets
of user and product attributes. We argue that our analysis suggests a
structured means for defining latent-space projections that can permit provable
performance properties to be established for machine learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uhlmann_J/0/1/0/all/0/1&quot;&gt;Jeffrey Uhlmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08859">
<title>Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08859</link>
<description rdf:parseType="Literal">&lt;p&gt;A curriculum is a planned sequence of learning materials and an effective one
can make learning efficient and effective for both humans and machines. Recent
studies developed effective data-driven curriculum learning approaches for
training graph neural networks in language applications. However, existing
curriculum learning approaches often employ a single criterion of difficulty in
their training paradigms. In this paper, we propose a new perspective on
curriculum learning by introducing a novel approach that builds on graph
complexity formalisms (as difficulty criteria) and model competence during
training. The model consists of a scheduling scheme which derives effective
curricula by accounting for different views of sample difficulty and model
competence during training. The proposed solution advances existing research in
curriculum learning for graph neural networks with the ability to incorporate a
fine-grained spectrum of graph difficulty criteria in their training paradigms.
Experimental results on real-world link prediction and node classification
tasks illustrate the effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakil_N/0/1/0/all/0/1&quot;&gt;Nidhi Vakil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1&quot;&gt;Hadi Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08863">
<title>Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08863</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient-based learning in multi-agent systems is difficult because the
gradient derives from a first-order model which does not account for the
interaction between agents&apos; learning processes. LOLA (&lt;a href=&quot;/abs/1709.04326&quot;&gt;arXiv:1709.04326&lt;/a&gt;)
accounts for this by differentiating through one step of optimization. We
extend the ideas of LOLA and develop a fully-general value-based approach to
optimization. At the core is a function we call the meta-value, which at each
point in joint-policy space gives for each agent a discounted sum of its
objective over future optimization steps. We argue that the gradient of the
meta-value gives a more reliable improvement direction than the gradient of the
original objective, because the meta-value derives from empirical observations
of the effects of optimization. We show how the meta-value can be approximated
by training a neural network to minimize TD error along optimization
trajectories in which agents follow the gradient of the meta-value. We analyze
the behavior of our method on the Logistic Game and on the Iterated Prisoner&apos;s
Dilemma.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooijmans_T/0/1/0/all/0/1&quot;&gt;Tim Cooijmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghajohari_M/0/1/0/all/0/1&quot;&gt;Milad Aghajohari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08873">
<title>An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08873</link>
<description rdf:parseType="Literal">&lt;p&gt;Restricting the variance of a policy&apos;s return is a popular choice in
risk-averse Reinforcement Learning (RL) due to its clear mathematical
definition and easy interpretability. Traditional methods directly restrict the
total return variance. Recent methods restrict the per-step reward variance as
a proxy. We thoroughly examine the limitations of these variance-based methods,
such as sensitivity to numerical scale and hindering of policy learning, and
propose to use an alternative risk measure, Gini deviation, as a substitute. We
study various properties of this new risk measure and derive a policy gradient
algorithm to minimize it. Empirical evaluation in domains where risk-aversion
can be clearly defined, shows that our algorithm can mitigate the limitations
of variance-based risk measures and achieves high return with low risk in terms
of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yudong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guiliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1&quot;&gt;Pascal Poupart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yangchen Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08874">
<title>Latent Space Representations of Neural Algorithmic Reasoners. (arXiv:2307.08874v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08874</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Algorithmic Reasoning (NAR) is a research area focused on designing
neural architectures that can reliably capture classical computation, usually
by learning to execute algorithms. A typical approach is to rely on Graph
Neural Network (GNN) architectures, which encode inputs in high-dimensional
latent spaces that are repeatedly transformed during the execution of the
algorithm. In this work we perform a detailed analysis of the structure of the
latent space induced by the GNN when executing algorithms. We identify two
possible failure modes: (i) loss of resolution, making it hard to distinguish
similar values; (ii) inability to deal with values outside the range observed
during training. We propose to solve the first issue by relying on a softmax
aggregator, and propose to decay the latent space in order to deal with
out-of-range values. We show that these changes lead to improvements on the
majority of algorithms in the standard CLRS-30 benchmark when using the
state-of-the-art Triplet-GMPNN processor. Our code is available at
\href{https://github.com/mirjanic/nar-latent-spaces}{https://github.com/mirjanic/nar-latent-spaces}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirjanic_V/0/1/0/all/0/1&quot;&gt;Vladimir V. Mirjani&amp;#x107;&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt; (1 and 2) ((1) University of Cambridge, (2) Google DeepMind)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08875">
<title>Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation. (arXiv:2307.08875v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08875</link>
<description rdf:parseType="Literal">&lt;p&gt;We study robust reinforcement learning (RL) with the goal of determining a
well-performing policy that is robust against model mismatch between the
training simulator and the testing environment. Previous policy-based robust RL
algorithms mainly focus on the tabular setting under uncertainty sets that
facilitate robust policy evaluation, but are no longer tractable when the
number of states scales up. To this end, we propose two novel uncertainty set
formulations, one based on double sampling and the other on an integral
probability metric. Both make large-scale robust RL tractable even when one
only has access to a simulator. We propose a robust natural actor-critic (RNAC)
approach that incorporates the new uncertainty sets and employs function
approximation. We provide finite-time convergence guarantees for the proposed
RNAC algorithm to the optimal robust policy within the function approximation
error. Finally, we demonstrate the robust performance of the policy learned by
our proposed RNAC approach in multiple MuJoCo environments and a real-world
TurtleBot navigation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Ruida Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Min Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1&quot;&gt;Dileep Kalathil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1&quot;&gt;P. R. Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1&quot;&gt;Chao Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08877">
<title>Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction. (arXiv:2307.08877v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08877</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction is a crucial task in graph machine learning with diverse
applications. We explore the interplay between node attributes and graph
topology and demonstrate that incorporating pre-trained node attributes
improves the generalization power of link prediction models. Our proposed
method, UPNA (Unsupervised Pre-training of Node Attributes), solves the
inductive link prediction problem by learning a function that takes a pair of
node attributes and predicts the probability of an edge, as opposed to Graph
Neural Networks (GNN), which can be prone to topological shortcuts in graphs
with power-law degree distribution. In this manner, UPNA learns a significant
part of the latent graph generation mechanism since the learned function can be
used to add incoming nodes to a growing graph. By leveraging pre-trained node
attributes, we overcome observational bias and make meaningful predictions
about unobserved nodes, surpassing state-of-the-art performance (3X to 34X
improvement on benchmark datasets). UPNA can be applied to various pairwise
learning tasks and integrated with existing link prediction models to enhance
their generalizability and bolster graph generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1&quot;&gt;Ayan Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menichetti_G/0/1/0/all/0/1&quot;&gt;Giulia Menichetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eliassi_Rad_T/0/1/0/all/0/1&quot;&gt;Tina Eliassi-Rad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08880">
<title>Modular Neural Network Approaches for Surgical Image Recognition. (arXiv:2307.08880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08880</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based applications have seen a lot of success in recent years.
Text, audio, image, and video have all been explored with great success using
deep learning approaches. The use of convolutional neural networks (CNN) in
computer vision, in particular, has yielded reliable results. In order to
achieve these results, a large amount of data is required. However, the dataset
cannot always be accessible. Moreover, annotating data can be difficult and
time-consuming. Self-training is a semi-supervised approach that managed to
alleviate this problem and achieve state-of-the-art performances. Theoretical
analysis even proved that it may result in a better generalization than a
normal classifier. Another problem neural networks can face is the increasing
complexity of modern problems, requiring a high computational and storage cost.
One way to mitigate this issue, a strategy that has been inspired by human
cognition known as modular learning, can be employed. The principle of the
approach is to decompose a complex problem into simpler sub-tasks. This
approach has several advantages, including faster learning, better
generalization, and enables interpretability.
&lt;/p&gt;
&lt;p&gt;In the first part of this paper, we introduce and evaluate different
architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS)
instability classification. Our experiments have shown that modular learning
improves performances compared to non-modular systems. Moreover, we found that
weighted modular, that is to weight the output using the probabilities from the
gating module, achieved an almost perfect classification. In the second part,
we present our approach for data labeling and segmentation with self-training
applied on shoulder arthroscopy images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salem_N/0/1/0/all/0/1&quot;&gt;Nosseiba Ben Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennani_Y/0/1/0/all/0/1&quot;&gt;Younes Bennani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkazan_J/0/1/0/all/0/1&quot;&gt;Joseph Karkazan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbara_A/0/1/0/all/0/1&quot;&gt;Abir Barbara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dacheux_C/0/1/0/all/0/1&quot;&gt;Charles Dacheux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregory_T/0/1/0/all/0/1&quot;&gt;Thomas Gregory&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08881">
<title>Examining the Effects of Degree Distribution and Homophily in Graph Learning Models. (arXiv:2307.08881v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.08881</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite a surge in interest in GNN development, homogeneity in benchmarking
datasets still presents a fundamental issue to GNN research. GraphWorld is a
recent solution which uses the Stochastic Block Model (SBM) to generate diverse
populations of synthetic graphs for benchmarking any GNN task. Despite its
success, the SBM imposed fundamental limitations on the kinds of graph
structure GraphWorld could create.
&lt;/p&gt;
&lt;p&gt;In this work we examine how two additional synthetic graph generators can
improve GraphWorld&apos;s evaluation; LFR, a well-established model in the graph
clustering literature and CABAM, a recent adaptation of the Barabasi-Albert
model tailored for GNN benchmarking. By integrating these generators, we
significantly expand the coverage of graph space within the GraphWorld
framework while preserving key graph properties observed in real-world
networks. To demonstrate their effectiveness, we generate 300,000 graphs to
benchmark 11 GNN models on a node classification task. We find GNN performance
variations in response to homophily, degree distribution and feature signal.
Based on these findings, we classify models by their sensitivity to the new
generators under these properties. Additionally, we release the extensions made
to GraphWorld on the GitHub repository, offering further evaluation of GNN
performance on new graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasir_M/0/1/0/all/0/1&quot;&gt;Mustafa Yasir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palowitch_J/0/1/0/all/0/1&quot;&gt;John Palowitch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1&quot;&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_Thanh_L/0/1/0/all/0/1&quot;&gt;Long Tran-Thanh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1&quot;&gt;Bryan Perozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08890">
<title>The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free. (arXiv:2307.08890v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2307.08890</link>
<description rdf:parseType="Literal">&lt;p&gt;The main bottleneck in designing efficient dynamic algorithms is the unknown
nature of the update sequence. In particular, there are some problems, like
3-vertex connectivity, planar digraph all pairs shortest paths, and others,
where the separation in runtime between the best partially dynamic solutions
and the best fully dynamic solutions is polynomial, sometimes even exponential.
&lt;/p&gt;
&lt;p&gt;In this paper, we formulate the predicted-deletion dynamic model, motivated
by a recent line of empirical work about predicting edge updates in dynamic
graphs. In this model, edges are inserted and deleted online, and when an edge
is inserted, it is accompanied by a &quot;prediction&quot; of its deletion time. This
models real world settings where services may have access to historical data or
other information about an input and can subsequently use such information make
predictions about user behavior. The model is also of theoretical interest, as
it interpolates between the partially dynamic and fully dynamic settings, and
provides a natural extension of the algorithms with predictions paradigm to the
dynamic setting.
&lt;/p&gt;
&lt;p&gt;We give a novel framework for this model that &quot;lifts&quot; partially dynamic
algorithms into the fully dynamic setting with little overhead. We use our
framework to obtain improved efficiency bounds over the state-of-the-art
dynamic algorithms for a variety of problems. In particular, we design
algorithms that have amortized update time that scales with a partially dynamic
algorithm, with high probability, when the predictions are of high quality. On
the flip side, our algorithms do no worse than existing fully-dynamic
algorithms when the predictions are of low quality. Furthermore, our algorithms
exhibit a graceful trade-off between the two cases. Thus, we are able to take
advantage of ML predictions asymptotically &quot;for free.&apos;&apos;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Quanquan C. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_V/0/1/0/all/0/1&quot;&gt;Vaidehi Srinivas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08893">
<title>Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction. (arXiv:2307.08893v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08893</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional clinical data have become invaluable resources for genetic
studies, due to their accessibility in biobank-scale datasets and the
development of high performance modeling techniques especially using deep
learning. Recent work has shown that low dimensional embeddings of these
clinical data learned by variational autoencoders (VAE) can be used for
genome-wide association studies and polygenic risk prediction. In this work, we
consider multiple unsupervised learning methods for learning disentangled
representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the
context of genetic association studies. Using spirograms from UK Biobank as a
running example, we observed improvements in the number of genome-wide
significant loci, heritability, and performance of polygenic risk scores for
asthma and chronic obstructive pulmonary disease by using FactorVAE or
beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs
performed effectively across multiple values of the regularization
hyperparameter, while beta-VAEs were much more sensitive to the hyperparameter
values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1&quot;&gt;Taedong Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08897">
<title>Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology. (arXiv:2307.08897v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08897</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel multi-agent reinforcement learning (RL) approach
for personalized glucose control in individuals with type 1 diabetes (T1D). The
method employs a closed-loop system consisting of a blood glucose (BG)
metabolic model and a multi-agent soft actor-critic RL model acting as the
basal-bolus advisor. Performance evaluation is conducted in three scenarios,
comparing the RL agents to conventional therapy. Evaluation metrics include
glucose levels (minimum, maximum, and mean), time spent in different BG ranges,
and average daily bolus and basal insulin dosages. Results demonstrate that the
RL-based basal-bolus advisor significantly improves glucose control, reducing
glycemic variability and increasing time spent within the target range (70-180
mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia
events are reduced. The RL approach also leads to a statistically significant
reduction in average daily basal insulin dosage compared to conventional
therapy. These findings highlight the effectiveness of the multi-agent RL
approach in achieving better glucose control and mitigating the risk of severe
hyperglycemia in individuals with T1D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalolia_M/0/1/0/all/0/1&quot;&gt;Mehrad Jalolia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cescon_M/0/1/0/all/0/1&quot;&gt;Marzia Cescon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08910">
<title>Sharpness-Aware Graph Collaborative Filtering. (arXiv:2307.08910v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08910</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have achieved impressive performance in
collaborative filtering. However, GNNs tend to yield inferior performance when
the distributions of training and test data are not aligned well. Also,
training GNNs requires optimizing non-convex neural networks with an abundance
of local and global minima, which may differ widely in their performance at
test time. Thus, it is essential to choose the minima carefully. Here we
propose an effective training schema, called {gSAM}, under the principle that
the \textit{flatter} minima has a better generalization ability than the
\textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of
the weight loss landscape by forming a bi-level optimization: the outer problem
conducts the standard model training while the inner problem helps the model
jump out of the sharp minima. Experimental results show the superiority of our
gSAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1&quot;&gt;Vivian Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1&quot;&gt;Mahashweta Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08913">
<title>Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08913</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, self-supervised learning (SSL) has emerged as a promising
approach for extracting valuable representations from unlabeled data. One
successful SSL method is contrastive learning, which aims to bring positive
examples closer while pushing negative examples apart. Many current contrastive
learning approaches utilize a parameterized projection head. Through a
combination of empirical analysis and theoretical investigation, we provide
insights into the internal mechanisms of the projection head and its
relationship with the phenomenon of dimensional collapse. Our findings
demonstrate that the projection head enhances the quality of representations by
performing contrastive loss in a projected subspace. Therefore, we propose an
assumption that only a subset of features is necessary when minimizing the
contrastive loss of a mini-batch of data. Theoretical analysis further suggests
that a sparse projection head can enhance generalization, leading us to
introduce SparseHead - a regularization term that effectively constrains the
sparsity of the projection head, and can be seamlessly integrated with any
self-supervised learning (SSL) approaches. Our experimental results validate
the effectiveness of SparseHead, demonstrating its ability to improve the
performance of existing contrastive methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zeen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xingzhe Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08919">
<title>Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.08919</link>
<description rdf:parseType="Literal">&lt;p&gt;For many applications of classifiers to medical images, a trustworthy label
for each image can be difficult or expensive to obtain. In contrast, images
without labels are more readily available. Two major research directions both
promise that additional unlabeled data can improve classifier performance:
self-supervised learning pretrains useful representations on unlabeled data
only, then fine-tunes a classifier on these representations via the labeled
set; semi-supervised learning directly trains a classifier on labeled and
unlabeled data simultaneously. Recent methods from both directions have claimed
significant gains on non-medical tasks, but do not systematically assess
medical images and mostly compare only to methods in the same direction. This
study contributes a carefully-designed benchmark to help answer a
practitioner&apos;s key question: given a small labeled dataset and a limited budget
of hours to spend on training, what gains from additional unlabeled images are
possible and which methods best achieve them? Unlike previous benchmarks, ours
uses realistic-sized validation sets to select hyperparameters, assesses
runtime-performance tradeoffs, and bridges two research fields. By comparing 6
semi-supervised methods and 5 self-supervised methods to strong labeled-only
baselines on 3 medical datasets with 30-1000 labels per class, we offer
insights to resource-constrained, results-focused practitioners: MixMatch,
SimCLR, and BYOL represent strong choices that were not surpassed by more
recent methods. After much effort selecting hyperparameters on one dataset, we
publish settings that enable strong methods to perform well on new medical
tasks within a few hours, with further search over dozens of hours delivering
modest additional gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruijie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Michael C. Hughes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08920">
<title>Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees. (arXiv:2307.08920v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.08920</link>
<description rdf:parseType="Literal">&lt;p&gt;Continuous-time nonlinear optimal control problems hold great promise in
real-world applications. After decades of development, reinforcement learning
(RL) has achieved some of the greatest successes as a general nonlinear control
design method. However, a recent comprehensive analysis of state-of-the-art
continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming
(ADP)-based CT-RL algorithms, reveals they face significant design challenges
due to their complexity, numerical conditioning, and dimensional scaling
issues. Despite advanced theoretical results, existing ADP CT-RL synthesis
methods are inadequate in solving even small, academic problems. The goal of
this work is thus to introduce a suite of new CT-RL algorithms for control of
affine nonlinear systems. Our design approach relies on two important factors.
First, our methods are applicable to physical systems that can be partitioned
into smaller subproblems. This constructive consideration results in reduced
dimensionality and greatly improved intuitiveness of design. Second, we
introduce a new excitation framework to improve persistence of excitation (PE)
and numerical conditioning performance via classical input/output insights.
Such a design-centric approach is the first of its kind in the ADP CT-RL
community. In this paper, we progressively introduce a suite of (decentralized)
excitable integral reinforcement learning (EIRL) algorithms. We provide
convergence and closed-loop stability guarantees, and we demonstrate these
guarantees on a significant application problem of controlling an unstable,
nonminimum phase hypersonic vehicle (HSV).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wallace_B/0/1/0/all/0/1&quot;&gt;Brent A. Wallace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Si_J/0/1/0/all/0/1&quot;&gt;Jennie Si&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08921">
<title>Optimistic Estimate Uncovers the Potential of Nonlinear Models. (arXiv:2307.08921v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08921</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an optimistic estimate to evaluate the best possible fitting
performance of nonlinear models. It yields an optimistic sample size that
quantifies the smallest possible sample size to fit/recover a target function
using a nonlinear model. We estimate the optimistic sample sizes for matrix
factorization models, deep models, and deep neural networks (DNNs) with
fully-connected or convolutional architecture. For each nonlinear model, our
estimates predict a specific subset of targets that can be fitted at
overparameterization, which are confirmed by our experiments. Our optimistic
estimate reveals two special properties of the DNN models -- free
expressiveness in width and costly expressiveness in connection. These
properties suggest the following architecture design principles of DNNs: (i)
feel free to add neurons/kernels; (ii) restrain from connecting neurons.
Overall, our optimistic estimate theoretically unveils the vast potential of
nonlinear models in fitting at overparameterization. Based on this framework,
we anticipate gaining a deeper understanding of how and why numerous nonlinear
models such as DNNs can effectively realize their potential in practice in the
near future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qin John Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08924">
<title>Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08924</link>
<description rdf:parseType="Literal">&lt;p&gt;Through experiments on various meta-learning methods, task samplers, and
few-shot learning tasks, this paper arrives at three conclusions. Firstly,
there are no universal task sampling strategies to guarantee the performance of
meta-learning models. Secondly, task diversity can cause the models to either
underfit or overfit during training. Lastly, the generalization performance of
the models are influenced by task divergence, task entropy, and task
difficulty. In response to these findings, we propose a novel task sampler
called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes
task divergence, task entropy, and task difficulty to sample tasks. To optimize
ASr, we rethink and propose a simple and general meta-learning algorithm.
Finally, a large number of empirical experiments demonstrate the effectiveness
of the proposed ASr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zeen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xingzhe Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1&quot;&gt;Lingyu Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hongwei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08925">
<title>Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08925</link>
<description rdf:parseType="Literal">&lt;p&gt;Large scale language models (LLM) have received significant attention and
found diverse applications across various domains, but their development
encounters challenges in real-world scenarios. These challenges arise due to
the scarcity of public domain data availability and the need to maintain
privacy with respect to private domain data. To address these issues, federated
learning (FL) has emerged as a promising technology that enables collaborative
training of shared models while preserving decentralized data. We propose the
concept of federated LLM, which comprises three key components, i.e., federated
LLM pre-training, federated LLM fine-tuning, and federated LLM prompt
engineering. For each component, we discuss its advantage over traditional LLM
training methods and propose specific engineering strategies for
implementation. Furthermore, we explore the novel challenges introduced by the
integration of FL and LLM. We analyze existing solutions and identify potential
obstacles faced by these solutions within the context of federated LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xiaohua Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianwei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08929">
<title>On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2307.08929</link>
<description rdf:parseType="Literal">&lt;p&gt;The first-principles-based effective Hamiltonian is widely used to predict
and simulate the properties of ferroelectrics and relaxor ferroelectrics.
However, the parametrization method of the effective Hamiltonian is complicated
and hardly can resolve the systems with complex interactions and/or complex
components. Here, we developed an on-the-fly machine learning approach to
parametrize the effective Hamiltonian based on Bayesian linear regression. The
parametrization is completed in molecular dynamics simulations, with the
energy, forces and stress predicted at each step along with their
uncertainties. First-principles calculations are executed when the
uncertainties are large to retrain the parameters. This approach provides a
universal and automatic way to compute the effective Hamiltonian parameters for
any considered systems including complex systems which previous methods can not
handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness
of this approach comparing with conventional first-principles parametrization
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingyue Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bellaiche_L/0/1/0/all/0/1&quot;&gt;L. Bellaiche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yurong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08933">
<title>IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness. (arXiv:2307.08933v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.08933</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, advances in deep learning have resulted in a plethora of
successes in the use of reinforcement learning (RL) to solve complex sequential
decision tasks with high-dimensional inputs. However, existing systems lack the
necessary mechanisms to provide humans with a holistic view of their
competence, presenting an impediment to their adoption, particularly in
critical applications where the decisions an agent makes can have significant
consequences. Yet, existing RL-based systems are essentially competency-unaware
in that they lack the necessary interpretation mechanisms to allow human
operators to have an insightful, holistic view of their competency. Towards
more explainable Deep RL (xDRL), we propose a new framework based on analyses
of interestingness. Our tool provides various measures of RL agent competence
stemming from interestingness analysis and is applicable to a wide range of RL
algorithms, natively supporting the popular RLLib toolkit. We showcase the use
of our framework by applying the proposed pipeline in a set of scenarios of
varying complexity. We empirically assess the capability of the approach in
identifying agent behavior patterns and competency-controlling conditions, and
the task elements mostly responsible for an agent&apos;s competence, based on global
and local analyses of interestingness. Overall, we show that our framework can
provide agent designers with insights about RL agent competence, both their
capabilities and limitations, enabling more informed decisions about
interventions, additional training, and other interactions in collaborative
human-machine settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sequeira_P/0/1/0/all/0/1&quot;&gt;Pedro Sequeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervasio_M/0/1/0/all/0/1&quot;&gt;Melinda Gervasio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08934">
<title>Multi-stage Neural Networks: Function Approximator of Machine Precision. (arXiv:2307.08934v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08934</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques are increasingly applied to scientific problems,
where the precision of networks is crucial. Despite being deemed as universal
function approximators, neural networks, in practice, struggle to reduce the
prediction errors below $O(10^{-5})$ even with large network size and extended
training iterations. To address this issue, we developed the multi-stage neural
networks that divides the training process into different stages, with each
stage using a new network that is optimized to fit the residue from the
previous stage. Across successive stages, the residue magnitudes decreases
substantially and follows an inverse power-law relationship with the residue
frequencies. The multi-stage neural networks effectively mitigate the spectral
biases associated with regular neural networks, enabling them to capture the
high frequency feature of target functions. We demonstrate that the prediction
error from the multi-stage training for both regression problems and
physics-informed neural networks can nearly reach the machine-precision
$O(10^{-16})$ of double-floating point within a finite number of iterations.
Such levels of accuracy are rarely attainable using single neural networks
alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongji Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Ching-Yao Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08939">
<title>Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.08939</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive Cruise Control (ACC) is a widely used driver assistance feature for
maintaining desired speed and safe distance to the leading vehicles. This paper
evaluates the security of the deep neural network (DNN) based ACC systems under
stealthy perception attacks that strategically inject perturbations into camera
data to cause forward collisions. We present a combined
knowledge-and-data-driven approach to design a context-aware strategy for the
selection of the most critical times for triggering the attacks and a novel
optimization-based method for the adaptive generation of image perturbations at
run-time. We evaluate the effectiveness of the proposed attack using an actual
driving dataset and a realistic simulation platform with the control software
from a production ACC system and a physical-world driving simulator while
considering interventions by the driver and safety features such as Automatic
Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental
results show that the proposed attack achieves 142.9x higher success rate in
causing accidents than random attacks and is mitigated 89.6% less by the safety
features while being stealthy and robust to real-world factors and dynamic
changes in the environment. This study provides insights into the role of human
operators and basic safety interventions in preventing attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xugui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouzel_M/0/1/0/all/0/1&quot;&gt;Maxfield Kouzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Haotian Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarty_M/0/1/0/all/0/1&quot;&gt;Morgan McCarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1&quot;&gt;Cristina Nita-Rotaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1&quot;&gt;Homa Alemzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08941">
<title>NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08941</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-tuning a pre-trained language model (PLM) emerges as the predominant
strategy in many natural language processing applications. However, even
fine-tuning the PLMs and doing inference are expensive, especially on edge
devices with low computing power. Some general approaches (e.g. quantization
and distillation) have been widely studied to reduce the compute/memory of PLM
fine-tuning, while very few one-shot compression techniques are explored. In
this paper, we investigate the neural tangent kernel (NTK)--which reveals the
gradient descent dynamics of neural networks--of the multilayer perceptrons
(MLP) modules in a PLM and propose to coin a lightweight PLM through
NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a
bundle of sub-MLPs, and cluster them into a given number of centroids, which
can then be restored as a compressed MLP and surprisingly shown to well
approximate the NTK of the original PLM. Extensive experiments of PLM
fine-tuning on both natural language understanding (NLU) and generation (NLG)
tasks are provided to verify the effectiveness of the proposed method MLP
fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianxin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zeming Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingrui He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08944">
<title>Siamese Networks for Weakly Supervised Human Activity Recognition. (arXiv:2307.08944v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.08944</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has been successfully applied to human activity recognition.
However, training deep neural networks requires explicitly labeled data which
is difficult to acquire. In this paper, we present a model with multiple
siamese networks that are trained by using only the information about the
similarity between pairs of data samples without knowing the explicit labels.
The trained model maps the activity data samples into fixed size representation
vectors such that the distance between the vectors in the representation space
approximates the similarity of the data samples in the input space. Thus, the
trained model can work as a metric for a wide range of different clustering
algorithms. The training process minimizes a similarity loss function that
forces the distance metric to be small for pairs of samples from the same kind
of activity, and large for pairs of samples from different kinds of activities.
We evaluate the model on three datasets to verify its effectiveness in
segmentation and recognition of continuous human activity sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1&quot;&gt;Taoran Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Manfred Huber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08945">
<title>Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08945</link>
<description rdf:parseType="Literal">&lt;p&gt;Growing concerns regarding algorithmic fairness have led to a surge in
methodologies to mitigate algorithmic bias. However, such methodologies largely
assume that observed labels in training data are correct. This is problematic
because bias in labels is pervasive across important domains, including
healthcare, hiring, and content moderation. In particular, human-generated
labels are prone to encoding societal biases. While the presence of labeling
bias has been discussed conceptually, there is a lack of methodologies to
address this problem. We propose a pruning method -- Decoupled Confident
Learning (DeCoLe) -- specifically designed to mitigate label bias. After
illustrating its performance on a synthetic dataset, we apply DeCoLe in the
context of hate speech detection, where label bias has been recognized as an
important challenge, and show that it successfully identifies biased labels and
outperforms competing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1&quot;&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saar_Tsechansky_M/0/1/0/all/0/1&quot;&gt;Maytal Saar-Tsechansky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08949">
<title>Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud. (arXiv:2307.08949v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.08949</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-tenancy in public clouds may lead to co-location interference on shared
resources, which possibly results in performance degradation of cloud
applications. Cloud providers want to know when such events happen and how
serious the degradation is, to perform interference-aware migrations and
alleviate the problem. However, virtual machines (VM) in
Infrastructure-as-a-Service public clouds are black-boxes to providers, where
application-level performance information cannot be acquired. This makes
performance monitoring intensely challenging as cloud providers can only rely
on low-level metrics such as CPU usage and hardware counters.
&lt;/p&gt;
&lt;p&gt;We propose a novel machine learning framework, Alioth, to monitor the
performance degradation of cloud applications. To feed the data-hungry models,
we first elaborate interference generators and conduct comprehensive
co-location experiments on a testbed to build Alioth-dataset which reflects the
complexity and dynamicity in real-world scenarios. Then we construct Alioth by
(1) augmenting features via recovering low-level metrics under no interference
using denoising auto-encoders, (2) devising a transfer learning model based on
domain adaptation neural network to make models generalize on test cases unseen
in offline training, and (3) developing a SHAP explainer to automate feature
selection and enhance model interpretability. Experiments show that Alioth
achieves an average mean absolute error of 5.29% offline and 10.8% when testing
on applications unseen in the training stage, outperforming the baseline
methods. Alioth is also robust in signaling quality-of-service violation under
dynamicity. Finally, we demonstrate a possible application of Alioth&apos;s
interpretability, providing insights to benefit the decision-making of cloud
operators. The dataset and code of Alioth have been released on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Tianyao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yingxuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yunlong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08951">
<title>Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting. (arXiv:2307.08951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08951</link>
<description rdf:parseType="Literal">&lt;p&gt;Forecasting how landslides will evolve over time or whether they will fail is
a challenging task due to a variety of factors, both internal and external.
Despite their considerable potential to address these challenges, deep learning
techniques lack interpretability, undermining the credibility of the forecasts
they produce. The recent development of transformer-based deep learning offers
untapped possibilities for forecasting landslides with unprecedented
interpretability and nonlinear feature learning capabilities. Here, we present
a deep learning pipeline that is capable of predicting landslide behavior
holistically, which employs a transformer-based network called LFIT to learn
complex nonlinear relationships from prior knowledge and multiple source data,
identifying the most relevant variables, and demonstrating a comprehensive
understanding of landslide evolution and temporal patterns. By integrating
prior knowledge, we provide improvement in holistic landslide forecasting,
enabling us to capture diverse responses to various influencing factors in
different local landslide areas. Using deformation observations as proxies for
measuring the kinetics of landslides, we validate our approach by training
models to forecast reservoir landslides in the Three Gorges Reservoir and
creeping landslides on the Tibetan Plateau. When prior knowledge is
incorporated, we show that interpretable landslide forecasting effectively
identifies influential factors across various landslides. It further elucidates
how local areas respond to these factors, making landslide behavior and trends
more interpretable and predictable. The findings from this study will
contribute to understanding landslide behavior in a new way and make the
proposed approach applicable to other complex disasters influenced by internal
and external factors in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhengjing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1&quot;&gt;Gang Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08955">
<title>Discretization-based ensemble model for robust learning in IoT. (arXiv:2307.08955v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08955</link>
<description rdf:parseType="Literal">&lt;p&gt;IoT device identification is the process of recognizing and verifying
connected IoT devices to the network. This is an essential process for ensuring
that only authorized devices can access the network, and it is necessary for
network management and maintenance. In recent years, machine learning models
have been used widely for automating the process of identifying devices in the
network. However, these models are vulnerable to adversarial attacks that can
compromise their accuracy and effectiveness. To better secure device
identification models, discretization techniques enable reduction in the
sensitivity of machine learning models to adversarial attacks contributing to
the stability and reliability of the model. On the other hand, Ensemble methods
combine multiple heterogeneous models to reduce the impact of remaining noise
or errors in the model. Therefore, in this paper, we integrate discretization
techniques and ensemble methods and examine it on model robustness against
adversarial attacks. In other words, we propose a discretization-based ensemble
stacking technique to improve the security of our ML models. We evaluate the
performance of different ML-based IoT device identification models against
white box and black box attacks using a real-world dataset comprised of network
traffic from 28 IoT devices. We demonstrate that the proposed method enables
robustness to the models for IoT device identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namvar_A/0/1/0/all/0/1&quot;&gt;Anahita Namvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_C/0/1/0/all/0/1&quot;&gt;Chandra Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1&quot;&gt;Salil S. Kanhere&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08962">
<title>REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.08962</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an enhanced approach for Rapid Exploration and
eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have
inherent limitations, such as a heavy reliance on precise descriptions for
decision-making, and the lack of a systematic approach to leverage try-and-fail
procedures akin to traditional Reinforcement Learning (RL). REX introduces an
additional layer of rewards and integrates concepts similar to Upper Confidence
Bound (UCB) scores, leading to more robust and efficient AI agent performance.
This approach has the advantage of enabling the utilization of offline
behaviors from logs and allowing seamless integration with existing foundation
models while it does not require any model fine-tuning. Through comparative
analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA
Planning(RAP), REX-based methods demonstrate comparable performance and, in
certain cases, even surpass the results achieved by these existing techniques.
Notably, REX-based methods exhibit remarkable reductions in execution time,
enhancing their practical applicability across a diverse set of scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1&quot;&gt;Rithesh Murthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinecke_S/0/1/0/all/0/1&quot;&gt;Shelby Heinecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1&quot;&gt;Juan Carlos Niebles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Le Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Weiran Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yihao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1&quot;&gt;Akash Gokul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1&quot;&gt;Devansh Arpit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mui_P/0/1/0/all/0/1&quot;&gt;Phil Mui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08964">
<title>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08964</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works in learning-integrated optimization have shown promise in
settings where the optimization problem is only partially observed or where
general-purpose optimizers perform poorly without expert tuning. By learning an
optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the
objective, the optimization process can be substantially accelerated by
leveraging past experience. The optimizer can be trained with supervision from
known optimal solutions or implicitly by optimizing the compound function
$f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as
labels and is capable of handling problem uncertainty; however, it is slow to
train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both
training and testing. The training is further challenged by sparse gradients of
$\mathbf{g}$, especially for combinatorial solvers. To address these
challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as
a replacement for $f\circ \mathbf{g}$. This surrogate, learnable by neural
networks, can be computed faster than the solver $\mathbf{g}$, provides dense
and smooth gradients during training, can generalize to unseen optimization
problems, and is efficiently learned via alternating optimization. We test our
approach on both synthetic problems, including shortest path and
multidimensional knapsack, and real-world problems such as portfolio
optimization, achieving comparable or superior objective values compared to
state-of-the-art baselines while reducing the number of calls to $\mathbf{g}$.
Notably, our approach outperforms existing methods for computationally
expensive high-dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharmagambetov_A/0/1/0/all/0/1&quot;&gt;Arman Zharmagambetov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1&quot;&gt;Brandon Amos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1&quot;&gt;Aaron Ferber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Taoan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1&quot;&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08970">
<title>A Unifying Framework for Differentially Private Sums under Continual Observation. (arXiv:2307.08970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08970</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of maintaining a differentially private decaying sum
under continual observation. We give a unifying framework and an efficient
algorithm for this problem for \emph{any sufficiently smooth} function. Our
algorithm is the first differentially private algorithm that does not have a
multiplicative error for polynomially-decaying weights. Our algorithm improves
on all prior works on differentially private decaying sums under continual
observation and recovers exactly the additive error for the special case of
continual counting from Henzinger et al. (SODA 2023) as a corollary.
&lt;/p&gt;
&lt;p&gt;Our algorithm is a variant of the factorization mechanism whose error depends
on the $\gamma_2$ and $\gamma_F$ norm of the underlying matrix. We give a
constructive proof for an almost exact upper bound on the $\gamma_2$ and
$\gamma_F$ norm and an almost tight lower bound on the $\gamma_2$ norm for a
large class of lower-triangular matrices. This is the first non-trivial lower
bound for lower-triangular matrices whose non-zero entries are not all the
same. It includes matrices for all continual decaying sums problems, resulting
in an upper bound on the additive error of any differentially private decaying
sums algorithm under continual observation.
&lt;/p&gt;
&lt;p&gt;We also explore some implications of our result in discrepancy theory and
operator algebra. Given the importance of the $\gamma_2$ norm in computer
science and the extensive work in mathematics, we believe our result will have
further applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henzinger_M/0/1/0/all/0/1&quot;&gt;Monika Henzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_J/0/1/0/all/0/1&quot;&gt;Jalaj Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1&quot;&gt;Sarvagya Upadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08982">
<title>Neural Network Pruning as Spectrum Preserving Process. (arXiv:2307.08982v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08982</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have achieved remarkable performance in various application
domains. Nevertheless, a large number of weights in pre-trained deep neural
networks prohibit them from being deployed on smartphones and embedded systems.
It is highly desirable to obtain lightweight versions of neural networks for
inference in edge devices. Many cost-effective approaches were proposed to
prune dense and convolutional layers that are common in deep neural networks
and dominant in the parameter space. However, a unified theoretical foundation
for the problem mostly is missing. In this paper, we identify the close
connection between matrix spectrum learning and neural network training for
dense and convolutional layers and argue that weight pruning is essentially a
matrix sparsification process to preserve the spectrum. Based on the analysis,
we also propose a matrix sparsification algorithm tailored for neural network
pruning that yields better pruning result. We carefully design and conduct
experiments to support our arguments. Hence we provide a consolidated viewpoint
for neural network pruning and enhance the interpretability of deep neural
networks by identifying and preserving the critical neural weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shibo Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dantong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutis_I/0/1/0/all/0/1&quot;&gt;Ioannis Koutis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08989">
<title>GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. (arXiv:2307.08989v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08989</link>
<description rdf:parseType="Literal">&lt;p&gt;Drug-target binding affinity prediction plays an important role in the early
stages of drug discovery, which can infer the strength of interactions between
new drugs and new targets. However, the performance of previous computational
models is limited by the following drawbacks. The learning of drug
representation relies only on supervised data, without taking into account the
information contained in the molecular graph itself. Moreover, most previous
studies tended to design complicated representation learning module, while
uniformity, which is used to measure representation quality, is ignored. In
this study, we propose GraphCL-DTA, a graph contrastive learning with molecular
semantics for drug-target binding affinity prediction. In GraphCL-DTA, we
design a graph contrastive learning framework for molecular graphs to learn
drug representations, so that the semantics of molecular graphs are preserved.
Through this graph contrastive framework, a more essential and effective drug
representation can be learned without additional supervised data. Next, we
design a new loss function that can be directly used to smoothly adjust the
uniformity of drug and target representations. By directly optimizing the
uniformity of representations, the representation quality of drugs and targets
can be improved. The effectiveness of the above innovative elements is verified
on two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA
on the above datasets suggests its superiority to the state-of-the-art model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinxing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Genke Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1&quot;&gt;Jian Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08999">
<title>Oracle Efficient Online Multicalibration and Omniprediction. (arXiv:2307.08999v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08999</link>
<description rdf:parseType="Literal">&lt;p&gt;A recent line of work has shown a surprising connection between
multicalibration, a multi-group fairness notion, and omniprediction, a learning
paradigm that provides simultaneous loss minimization guarantees for a large
family of loss functions. Prior work studies omniprediction in the batch
setting. We initiate the study of omniprediction in the online adversarial
setting. Although there exist algorithms for obtaining notions of
multicalibration in the online adversarial setting, unlike batch algorithms,
they work only for small finite classes of benchmark functions $F$, because
they require enumerating every function $f \in F$ at every round. In contrast,
omniprediction is most interesting for learning theoretic hypothesis classes
$F$, which are generally continuously large.
&lt;/p&gt;
&lt;p&gt;We develop a new online multicalibration algorithm that is well defined for
infinite benchmark classes $F$, and is oracle efficient (i.e. for any class
$F$, the algorithm has the form of an efficient reduction to a no-regret
learning algorithm for $F$). The result is the first efficient online
omnipredictor -- an oracle efficient prediction algorithm that can be used to
simultaneously obtain no regret guarantees to all Lipschitz convex loss
functions. For the class $F$ of linear functions, we show how to make our
algorithm efficient in the worst case. Also, we show upper and lower bounds on
the extent to which our rates can be improved: our oracle efficient algorithm
actually promises a stronger guarantee called swap-omniprediction, and we prove
a lower bound showing that obtaining $O(\sqrt{T})$ bounds for
swap-omniprediction is impossible in the online setting. On the other hand, we
give a (non-oracle efficient) algorithm which can obtain the optimal
$O(\sqrt{T})$ omniprediction bounds without going through multicalibration,
giving an information theoretic separation between these two solution concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sumegha Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1&quot;&gt;Christopher Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1&quot;&gt;Omer Reingold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1&quot;&gt;Aaron Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09006">
<title>OxfordVGG Submission to the EGO4D AV Transcription Challenge. (arXiv:2307.09006v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.09006</link>
<description rdf:parseType="Literal">&lt;p&gt;This report presents the technical details of our submission on the EGO4D
Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the
OxfordVGG team. We present WhisperX, a system for efficient speech
transcription of long-form audio with word-level time alignment, along with two
text normalisers which are publicly available. Our final submission obtained
56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the
leaderboard. All baseline codes and models are available on
https://github.com/m-bain/whisperX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huh_J/0/1/0/all/0/1&quot;&gt;Jaesung Huh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bain_M/0/1/0/all/0/1&quot;&gt;Max Bain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09009">
<title>How is ChatGPT&apos;s behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09009</link>
<description rdf:parseType="Literal">&lt;p&gt;GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)
services. However, when and how these models are updated over time is opaque.
Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on
four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous
questions, 3) generating code and 4) visual reasoning. We find that the
performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.
For example, GPT-4 (March 2023) was very good at identifying prime numbers
(accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions
(accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5
(March 2023) in this task. GPT-4 was less willing to answer sensitive questions
in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes
in code generation in June than in March. Overall, our findings shows that the
behavior of the same LLM service can change substantially in a relatively short
amount of time, highlighting the need for continuous monitoring of LLM quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lingjiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1&quot;&gt;Matei Zaharia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09018">
<title>Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.09018</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation large language models (LLMs) have shown an impressive ability to
solve tasks across a wide range of fields including health. To effectively
solve personalized health tasks, LLMs need the ability to ingest a diversity of
data modalities that are relevant to an individual&apos;s health status. In this
paper, we take a step towards creating multimodal LLMs for health that are
grounded in individual-specific data by developing a framework (HeLM: Health
Large Language Model for Multimodal Understanding) that enables LLMs to use
high-dimensional clinical modalities to estimate underlying disease risk. HeLM
encodes complex data modalities by learning an encoder that maps them into the
LLM&apos;s token embedding space and for simple modalities like tabular data by
serializing the data into text. Using data from the UK Biobank, we show that
HeLM can effectively use demographic and clinical features in addition to
high-dimensional time-series data to estimate disease risk. For example, HeLM
achieves an AUROC of 0.75 for asthma prediction when combining tabular and
spirogram data modalities compared with 0.49 when only using tabular data.
Overall, we find that HeLM outperforms or performs at parity with classical
machine learning approaches across a selection of eight binary traits.
Furthermore, we investigate the downstream uses of this model such as its
generalizability to out-of-distribution traits and its ability to power
conversations around individual health and wellness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Belyaeva_A/0/1/0/all/0/1&quot;&gt;Anastasiya Belyaeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cosentino_J/0/1/0/all/0/1&quot;&gt;Justin Cosentino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hormozdiari_F/0/1/0/all/0/1&quot;&gt;Farhad Hormozdiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+McLean_C/0/1/0/all/0/1&quot;&gt;Cory Y. McLean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Furlotte_N/0/1/0/all/0/1&quot;&gt;Nicholas A. Furlotte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09019">
<title>U-shaped Transformer: Retain High Frequency Context in Time Series Analysis. (arXiv:2307.09019v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09019</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series prediction plays a crucial role in various industrial fields. In
recent years, neural networks with a transformer backbone have achieved
remarkable success in many domains, including computer vision and NLP. In time
series analysis domain, some studies have suggested that even the simplest MLP
networks outperform advanced transformer-based networks on time series forecast
tasks. However, we believe these findings indicate there to be low-rank
properties in time series sequences. In this paper, we consider the low-pass
characteristics of transformers and try to incorporate the advantages of MLP.
We adopt skip-layer connections inspired by Unet into traditional transformer
backbone, thus preserving high-frequency context from input to output, namely
U-shaped Transformer. We introduce patch merge and split operation to extract
features with different scales and use larger datasets to fully make use of the
transformer backbone. Our experiments demonstrate that the model performs at an
advanced level across multiple datasets with relatively low cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingkui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiqin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09025">
<title>qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers. (arXiv:2307.09025v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.09025</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a general framework for decoding quantum error-correcting codes
with generative modeling. The model utilizes autoregressive neural networks,
specifically Transformers, to learn the joint probability of logical operators
and syndromes. This training is in an unsupervised way, without the need for
labeled training data, and is thus referred to as pre-training. After the
pre-training, the model can efficiently compute the likelihood of logical
operators for any given syndrome, using maximum likelihood decoding. It can
directly generate the most-likely logical operators with computational
complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is
significantly better than the conventional maximum likelihood decoding
algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained
model, we further propose refinement to achieve more accurately the likelihood
of logical operators for a given syndrome by directly sampling the stabilizer
operators. We perform numerical experiments on stabilizer codes with small code
distances, using both depolarizing error models and error models with
correlated noise. The results show that our approach provides significantly
better decoding accuracy than the minimum weight perfect matching and
belief-propagation-based algorithms. Our framework is general and can be
applied to any error model and quantum codes with different topologies such as
surface codes and quantum LDPC codes. Furthermore, it leverages the
parallelization capabilities of GPUs, enabling simultaneous decoding of a large
number of syndromes. Our approach sheds light on the efficient and accurate
decoding of quantum error-correcting codes using generative artificial
intelligence and modern computational power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hanyan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pan_F/0/1/0/all/0/1&quot;&gt;Feng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09055">
<title>Outlier-Robust Tensor Low-Rank Representation for Data Clustering. (arXiv:2307.09055v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.09055</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank tensor analysis has received widespread attention with many
practical applications. However, the tensor data are often contaminated by
outliers or sample-specific corruptions. How to recover the tensor data that
are corrupted by outliers and perform data clustering remains a challenging
problem. This paper develops an outlier-robust tensor low-rank representation
(OR-TLRR) method for simultaneous outlier detection and tensor data clustering
based on the tensor singular value decomposition (t-SVD) algebraic framework.
It is motivated by the recently proposed tensor-tensor product induced by
invertible linear transforms that satisfy certain conditions. For tensor
observations with arbitrary outlier corruptions, OR-TLRR has provable
performance guarantee for exactly recovering the row space of clean data and
detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is
also proposed to handle the case when parts of the data are missing. Finally,
extensive experimental results on both synthetic and real data demonstrate the
effectiveness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09057">
<title>Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces. (arXiv:2307.09057v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.09057</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a framework for computing the Gromov-Wasserstein problem
between two sets of points in low dimensional spaces, where the discrepancy is
the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization
of the optimal transport problem that finds the assignment between two sets
preserving pairwise distances as much as possible. This can be used to quantify
the similarity between two formations or shapes, a common problem in AI and
machine learning. The problem can be formulated as a Quadratic Assignment
Problem (QAP), which is in general computationally intractable even for small
problems. Our framework addresses this challenge by reformulating the QAP as an
optimization problem with a low-dimensional domain, leveraging the fact that
the problem can be expressed as a concave quadratic optimization problem with
low rank. The method scales well with the number of points, and it can be used
to find the global solution for large-scale problems with thousands of points.
We compare the computational complexity of our approach with state-of-the-art
methods on synthetic problems and apply it to a near-symmetrical problem which
is of particular interest in computational biology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ryner_M/0/1/0/all/0/1&quot;&gt;Martin Ryner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kronqvist_J/0/1/0/all/0/1&quot;&gt;Jan Kronqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Karlsson_J/0/1/0/all/0/1&quot;&gt;Johan Karlsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09060">
<title>Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning. (arXiv:2307.09060v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2307.09060</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a data-driven emulator, stochastic weather generator (SWG),
suitable for estimating probabilities of prolonged heatwaves in France and
Scandinavia. This emulator is based on the method of analogs of circulation to
which we add temperature and soil moisture as predictor fields. We train the
emulator on an intermediate complexity climate model run and show that it is
capable of predicting conditional probabilities (forecasting) of heatwaves out
of sample. Special attention is payed that this prediction is evaluated using
proper score appropriate for rare events. To accelerate the computation of
analogs dimensionality reduction techniques are applied and the performance is
evaluated. The probabilistic prediction achieved with SWG is compared with the
one achieved with
&lt;/p&gt;
&lt;p&gt;Convolutional Neural Network (CNN). With the availability of hundreds of
years of training data CNNs perform better at the task of probabilistic
prediction. In addition, we show that the SWG emulator trained on 80 years of
data is capable of estimating extreme return times of order of thousands of
years for heatwaves longer than several days more precisely than the fit based
on generalised extreme value distribution. Finally, the quality of its
synthetic extreme teleconnection patterns obtained with stochastic weather
generator is studied. We showcase two examples of such synthetic teleconnection
patterns for heatwaves in France and Scandinavia that compare favorably to the
very long climate model control run.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Miloshevich_G/0/1/0/all/0/1&quot;&gt;George Miloshevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lucente_D/0/1/0/all/0/1&quot;&gt;Dario Lucente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yiou_P/0/1/0/all/0/1&quot;&gt;Pascal Yiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bouchet_F/0/1/0/all/0/1&quot;&gt;Freddy Bouchet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09065">
<title>Learning Adaptive Neighborhoods for Graph Neural Networks. (arXiv:2307.09065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09065</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCNs) enable end-to-end learning on graph
structured data. However, many works assume a given graph structure. When the
input graph is noisy or unavailable, one approach is to construct or learn a
latent graph structure. These methods typically fix the choice of node degree
for the entire graph, which is suboptimal. Instead, we propose a novel
end-to-end differentiable graph generator which builds graph topologies where
each node selects both its neighborhood and its size. Our module can be readily
integrated into existing pipelines involving graph convolution operations,
replacing the predetermined or existing adjacency matrix with one that is
learned, and optimized, as part of the general objective. As such it is
applicable to any GCN. We integrate our module into trajectory prediction,
point cloud classification and node classification pipelines resulting in
improved accuracy over other structure-learning methods across a wide range of
datasets and GCN backbones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Avishkar Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1&quot;&gt;Oscar Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1&quot;&gt;Chris Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1&quot;&gt;Richard Bowden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09067">
<title>Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09067</link>
<description rdf:parseType="Literal">&lt;p&gt;Fetal head segmentation is a crucial step in measuring the fetal head
circumference (HC) during gestation, an important biometric in obstetrics for
monitoring fetal growth. However, manual biometry generation is time-consuming
and results in inconsistent accuracy. To address this issue, convolutional
neural network (CNN) models have been utilized to improve the efficiency of
medical biometry. But training a CNN network from scratch is a challenging
task, we proposed a Transfer Learning (TL) method. Our approach involves
fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to
perform segmentation on a set of fetal head ultrasound (US) images with limited
effort. This method addresses the challenges associated with training a CNN
network from scratch. It suggests that our proposed FT strategy yields
segmentation performance that is comparable when trained with a reduced number
of parameters by 85.8%. And our proposed FT strategy outperforms other
strategies with smaller trainable parameter sizes below 4.4 million. Thus, we
contend that it can serve as a dependable FT approach for reducing the size of
models in medical image analysis. Our key findings highlight the importance of
the balance between model performance and size in developing Artificial
Intelligence (AI) applications by TL methods. Code is available at
https://github.&lt;a href=&quot;/abs/com/1320494&quot;&gt;com/1320494&lt;/a&gt;2/FT_Methods_for_Fetal_Head_Segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fangyijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Silvestre_G/0/1/0/all/0/1&quot;&gt;Gu&amp;#xe9;nol&amp;#xe9; Silvestre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09072">
<title>DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09072</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving partial differential equations (PDEs) using a data-driven approach
has become increasingly common. The recent development of the operator learning
paradigm has enabled the solution of a broader range of PDE-related problems.
We propose an operator learning method to solve time-dependent PDEs
continuously in time without needing any temporal discretization. The proposed
approach, named DiTTO, is inspired by latent diffusion models. While diffusion
models are usually used in generative artificial intelligence tasks, their
time-conditioning mechanism is extremely useful for PDEs. The
diffusion-inspired framework is combined with elements from the Transformer
architecture to improve its capabilities.
&lt;/p&gt;
&lt;p&gt;We demonstrate the effectiveness of the new approach on a wide variety of
PDEs in multiple dimensions, namely the 1-D Burgers&apos; equation, 2-D
Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO
achieves state-of-the-art results in terms of accuracy for these problems. We
also present a method to improve the performance of DiTTO by using fast
sampling concepts from diffusion models. Finally, we show that DiTTO can
accurately perform zero-shot super-resolution in time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovadia_O/0/1/0/all/0/1&quot;&gt;Oded Ovadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turkel_E/0/1/0/all/0/1&quot;&gt;Eli Turkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1&quot;&gt;Adar Kahana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09080">
<title>A Federated learning model for Electric Energy management using Blockchain Technology. (arXiv:2307.09080v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09080</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy shortfall and electricity load shedding are the main problems for
developing countries. The main causes are lack of management in the energy
sector and the use of non-renewable energy sources. The improved energy
management and use of renewable sources can be significant to resolve energy
crisis. It is necessary to increase the use of renewable energy sources (RESs)
to meet the increasing energy demand due to high prices of fossil-fuel based
energy. Federated learning (FL) is the most emerging technique in the field of
artificial intelligence. Federated learning helps to generate global model at
server side by ensemble locally trained models at remote edges sites while
preserving data privacy. The global model used to predict energy demand to
satisfy the needs of consumers. In this article, we have proposed Blockchain
based safe distributed ledger technology for transaction of data between
prosumer and consumer to ensure their transparency, traceability and security.
Furthermore, we have also proposed a Federated learning model to forecast the
energy requirements of consumer and prosumer. Moreover, Blockchain has been
used to store excess energy data from prosumer for better management of energy
between prosumer and grid. Lastly, the experiment results revealed that
renewable energy sources have produced better and comparable results to other
non-renewable energy resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Shoaib Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayat_A/0/1/0/all/0/1&quot;&gt;Azeen Ahmed Hayat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09093">
<title>Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.09093v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09093</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential decision-making under uncertainty is often associated with long
feedback delays. Such delays degrade the performance of the learning agent in
identifying a subset of arms with the optimal collective reward in the long
run. This problem becomes significantly challenging in a non-stationary
environment with structural dependencies amongst the reward distributions
associated with the arms. Therefore, besides adapting to delays and
environmental changes, learning the causal relations alleviates the adverse
effects of feedback delay on the decision-making process. We formalize the
described setting as a non-stationary and delayed combinatorial semi-bandit
problem with causally related rewards. We model the causal relations by a
directed graph in a stationary structural equation model. The agent maximizes
the long-term average payoff, defined as a linear function of the base arms&apos;
rewards. We develop a policy that learns the structural dependencies from
delayed feedback and utilizes that to optimize the decision-making while
adapting to drifts. We prove a regret bound for the performance of the proposed
algorithm. Besides, we evaluate our method via numerical analysis using
synthetic and real-world datasets to detect the regions that contribute the
most to the spread of Covid-19 in Italy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghoorchian_S/0/1/0/all/0/1&quot;&gt;Saeed Ghoorchian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1&quot;&gt;Setareh Maghsudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09109">
<title>Mining of Single-Class by Active Learning for Semantic Segmentation. (arXiv:2307.09109v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09109</link>
<description rdf:parseType="Literal">&lt;p&gt;Several Active Learning (AL) policies require retraining a target model
several times in order to identify the most informative samples and rarely
offer the option to focus on the acquisition of samples from underrepresented
classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm
is introduced where an AL policy is constructed through deep reinforcement
learning and exploits quantity-accuracy correlations to build datasets on which
high-performance models can be trained with regards to specific classes.
MiSiCAL is especially helpful in the case of very large batch sizes since it
does not require repeated model training sessions as is common in other AL
methods. This is thanks to its ability to exploit fixed representations of the
candidate data points. We find that MiSiCAL is able to outperform a random
policy on 150 out of 171 COCO10k classes, while the strongest baseline only
outperforms random on 101 classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_H/0/1/0/all/0/1&quot;&gt;Hugues Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1&quot;&gt;Emma Slade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09142">
<title>Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs. (arXiv:2307.09142v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/2307.09142</link>
<description rdf:parseType="Literal">&lt;p&gt;The molten sand, a mixture of calcia, magnesia, alumina, and silicate, known
as CMAS, is characterized by its high viscosity, density, and surface tension.
The unique properties of CMAS make it a challenging material to deal with in
high-temperature applications, requiring innovative solutions and materials to
prevent its buildup and damage to critical equipment. Here, we use multiphase
many-body dissipative particle dynamics (mDPD) simulations to study the wetting
dynamics of highly viscous molten CMAS droplets. The simulations are performed
in three dimensions, with varying initial droplet sizes and equilibrium contact
angles. We propose a coarse parametric ordinary differential equation (ODE)
that captures the spreading radius behavior of the CMAS droplets. The ODE
parameters are then identified based on the Physics-Informed Neural Network
(PINN) framework. Subsequently, the closed form dependency of parameter values
found by PINN on the initial radii and contact angles are given using symbolic
regression. Finally, we employ Bayesian PINNs (B-PINNs) to assess and quantify
the uncertainty associated with the discovered parameters. In brief, this study
provides insight into spreading dynamics of CMAS droplets by fusing simple
parametric ODE modeling and state-of-the-art machine learning techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kiyani_E/0/1/0/all/0/1&quot;&gt;Elham Kiyani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kooshkbaghi_M/0/1/0/all/0/1&quot;&gt;Mahdi Kooshkbaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shukla_K/0/1/0/all/0/1&quot;&gt;Khemraj Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Koneru_R/0/1/0/all/0/1&quot;&gt;Rahul Babu Koneru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bravo_L/0/1/0/all/0/1&quot;&gt;Luis Bravo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ghoshal_A/0/1/0/all/0/1&quot;&gt;Anindya Ghoshal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Karttunen_M/0/1/0/all/0/1&quot;&gt;Mikko Karttunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09143">
<title>MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results. (arXiv:2307.09143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09143</link>
<description rdf:parseType="Literal">&lt;p&gt;Small Object Detection (SOD) is an important machine vision topic because (i)
a variety of real-world applications require object detection for distant
objects and (ii) SOD is a challenging task due to the noisy, blurred, and
less-informative image appearances of small objects. This paper proposes a new
SOD dataset consisting of 39,070 images including 137,121 bird instances, which
is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The
detail of the challenge with the SOD4SB dataset is introduced in this paper. In
total, 223 participants joined this challenge. This paper briefly introduces
the award-winning methods. The dataset, the baseline code, and the website for
evaluation on the public testset are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_Y/0/1/0/all/0/1&quot;&gt;Yuki Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1&quot;&gt;Norimichi Ukita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1&quot;&gt;Takayuki Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1&quot;&gt;Hao-Yu Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1&quot;&gt;Mu-Yi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chia-Chi Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1&quot;&gt;En-Ming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chien-Yao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1&quot;&gt;Da Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kastner_M/0/1/0/all/0/1&quot;&gt;Marc A. Kastner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tingwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1&quot;&gt;Yasutomo Kawanishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirayama_T/0/1/0/all/0/1&quot;&gt;Takatsugu Hirayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komamizu_T/0/1/0/all/0/1&quot;&gt;Takahiro Komamizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ide_I/0/1/0/all/0/1&quot;&gt;Ichiro Ide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinya_Y/0/1/0/all/0/1&quot;&gt;Yosuke Shinya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1&quot;&gt;Guang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1&quot;&gt;Syusuke Yasui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09165">
<title>Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09165</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiency and trustworthiness are two eternal pursuits when applying deep
learning in real-world applications. With regard to efficiency, dataset
distillation (DD) endeavors to reduce training costs by distilling the large
dataset into a tiny synthetic dataset. However, existing methods merely
concentrate on in-distribution (InD) classification in a closed-world setting,
disregarding out-of-distribution (OOD) samples. On the other hand, OOD
detection aims to enhance models&apos; trustworthiness, which is always
inefficiently achieved in full-data settings. For the first time, we
simultaneously consider both issues and propose a novel paradigm called
Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and
outliers, the condensed datasets are capable to train models competent in both
InD classification and OOD detection. To alleviate the requirement of real
outlier data and make OOD detection more practical, we further propose to
corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier
Exposure (POE). Comprehensive experiments on various settings demonstrate the
effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art
method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more
trustworthy and applicable to real open-world scenarios. Our code will be
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shijie Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu-Yao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09169">
<title>Efficient Prediction of Peptide Self-assembly through Sequential and Graphical Encoding. (arXiv:2307.09169v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2307.09169</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been an explosion of research on the application
of deep learning to the prediction of various peptide properties, due to the
significant development and market potential of peptides. Molecular dynamics
has enabled the efficient collection of large peptide datasets, providing
reliable training data for deep learning. However, the lack of systematic
analysis of the peptide encoding, which is essential for AI-assisted
peptide-related tasks, makes it an urgent problem to be solved for the
improvement of prediction accuracy. To address this issue, we first collect a
high-quality, colossal simulation dataset of peptide self-assembly containing
over 62,000 samples generated by coarse-grained molecular dynamics (CGMD).
Then, we systematically investigate the effect of peptide encoding of amino
acids into sequences and molecular graphs using state-of-the-art sequential
(i.e., RNN, LSTM, and Transformer) and structural deep learning models (i.e.,
GCN, GAT, and GraphSAGE), on the accuracy of peptide self-assembly prediction,
an essential physiochemical process prior to any peptide-related applications.
Extensive benchmarking studies have proven Transformer to be the most powerful
sequence-encoding-based deep learning model, pushing the limit of peptide
self-assembly prediction to decapeptides. In summary, this work provides a
comprehensive benchmark analysis of peptide encoding with advanced deep
learning models, serving as a guide for a wide range of peptide-related
predictions such as isoelectric points, hydration free energy, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shuang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09182">
<title>Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey. (arXiv:2307.09182v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09182</link>
<description rdf:parseType="Literal">&lt;p&gt;With an increasing number of smart devices like internet of things (IoT)
devices deployed in the field, offloadingtraining of neural networks (NNs) to a
central server becomes more and more infeasible. Recent efforts toimprove
users&apos; privacy have led to on-device learning emerging as an alternative.
However, a model trainedonly on a single device, using only local data, is
unlikely to reach a high accuracy. Federated learning (FL)has been introduced
as a solution, offering a privacy-preserving trade-off between communication
overheadand model accuracy by sharing knowledge between devices but disclosing
the devices&apos; private data. Theapplicability and the benefit of applying
baseline FL are, however, limited in many relevant use cases dueto the
heterogeneity present in such environments. In this survey, we outline the
heterogeneity challengesFL has to overcome to be widely applicable in
real-world applications. We especially focus on the aspect ofcomputation
heterogeneity among the participating devices and provide a comprehensive
overview of recentworks on heterogeneity-aware FL. We discuss two groups: works
that adapt the NN architecture and worksthat approach heterogeneity on a system
level, covering Federated Averaging (FedAvg), distillation, and
splitlearning-based approaches, as well as synchronous and asynchronous
aggregation schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfeiffer_K/0/1/0/all/0/1&quot;&gt;Kilian Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapp_M/0/1/0/all/0/1&quot;&gt;Martin Rapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalili_R/0/1/0/all/0/1&quot;&gt;Ramin Khalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henkel_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Henkel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09191">
<title>A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09191</link>
<description rdf:parseType="Literal">&lt;p&gt;Categorical encoders transform categorical features into numerical
representations that are indispensable for a wide range of machine learning
models. Existing encoder benchmark studies lack generalizability because of
their limited choice of (1) encoders, (2) experimental factors, and (3)
datasets. Additionally, inconsistencies arise from the adoption of varying
aggregation strategies. This paper is the most comprehensive benchmark of
categorical encoders to date, including an extensive evaluation of 32
configurations of encoders from diverse families, with 36 combinations of
experimental factors, and on 50 datasets. The study shows the profound
influence of dataset selection, experimental factors, and aggregation
strategies on the benchmark&apos;s conclusions -- aspects disregarded in previous
encoder benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matteucci_F/0/1/0/all/0/1&quot;&gt;Federico Matteucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arzamasov_V/0/1/0/all/0/1&quot;&gt;Vadim Arzamasov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boehm_K/0/1/0/all/0/1&quot;&gt;Klemens Boehm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09205">
<title>Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning. (arXiv:2307.09205v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09205</link>
<description rdf:parseType="Literal">&lt;p&gt;In many reinforcement learning tasks, the agent has to learn to interact with
many objects of different types and generalize to unseen combinations and
numbers of objects. Often a task is a composition of previously learned tasks
(e.g. block stacking). These are examples of compositional generalization, in
which we compose object-centric representations to solve complex tasks. Recent
works have shown the benefits of object-factored representations and
hierarchical abstractions for improving sample efficiency in these settings. On
the other hand, these methods do not fully exploit the benefits of
factorization in terms of object attributes. In this paper, we address this
opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL)
framework. In DAFT-RL, we leverage object-centric representation learning to
extract objects from visual inputs. We learn to classify them in classes and
infer their latent parameters. For each class of object, we learn a class
template graph that describes how the dynamics and reward of an object of this
class factorize according to its attributes. We also learn an interaction
pattern graph that describes how objects of different classes interact with
each other at the attribute level. Through these graphs and a dynamic
interaction graph that models the interactions between objects, we can learn a
policy that can then be directly applied in a new environment by just
estimating the interactions and latent parameters. We evaluate DAFT-RL in three
benchmark datasets and show our framework outperforms the state-of-the-art in
generalizing across unseen objects with varying attributes and latent
parameters, as well as in the composition of previously learned tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1&quot;&gt;Sara Magliacane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09206">
<title>Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.09206</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous navigation settings, several quantities can be subject to
variations. Terrain properties such as friction coefficients may vary over time
depending on the location of the robot. Also, the dynamics of the robot may
change due to, e.g., different payloads, changing the system&apos;s mass, or wear
and tear, changing actuator gains or joint friction. An autonomous agent should
thus be able to adapt to such variations. In this paper, we develop a novel
probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN,
which is able to adapt to the above-mentioned variations. It builds on recent
advances in meta-learning forward dynamics models based on Neural Processes. We
evaluate our method in a simulated 2D navigation setting with a unicycle-like
robot and different terrain layouts with spatially varying friction
coefficients. In our experiments, the proposed model exhibits lower prediction
error for the task of long-horizon trajectory prediction, compared to
non-adaptive ablation models. We also evaluate our model on the downstream task
of navigation planning, which demonstrates improved performance in planning
control-efficient paths by taking robot and terrain properties into account.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1&quot;&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achterhold_J/0/1/0/all/0/1&quot;&gt;Jan Achterhold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haolong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1&quot;&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1&quot;&gt;Joerg Stueckler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09207">
<title>Joint Microseismic Event Detection and Location with a Detection Transformer. (arXiv:2307.09207v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2307.09207</link>
<description rdf:parseType="Literal">&lt;p&gt;Microseismic event detection and location are two primary components in
microseismic monitoring, which offers us invaluable insights into the
subsurface during reservoir stimulation and evolution. Conventional approaches
for event detection and location often suffer from manual intervention and/or
heavy computation, while current machine learning-assisted approaches typically
address detection and location separately; such limitations hinder the
potential for real-time microseismic monitoring. We propose an approach to
unify event detection and source location into a single framework by adapting a
Convolutional Neural Network backbone and an encoder-decoder Transformer with a
set-based Hungarian loss, which is applied directly to recorded waveforms. The
proposed network is trained on synthetic data simulating multiple microseismic
events corresponding to random source locations in the area of suspected
microseismic activities. A synthetic test on a 2D profile of the SEAM Time
Lapse model illustrates the capability of the proposed method in detecting the
events properly and locating them in the subsurface accurately; while, a field
test using the Arkoma Basin data further proves its practicability, efficiency,
and its potential in paving the way for real-time monitoring of microseismic
events.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Birnie_C/0/1/0/all/0/1&quot;&gt;Claire Birnie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Alkhalifah_T/0/1/0/all/0/1&quot;&gt;Tariq Alkhalifah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09209">
<title>Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09209</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze sentiment analysis and toxicity detection models to detect the
presence of explicit bias against people with disability (PWD). We employ the
bias identification framework of Perturbation Sensitivity Analysis to examine
conversations related to PWD on social media platforms, specifically Twitter
and Reddit, in order to gain insight into how disability bias is disseminated
in real-world social settings. We then create the \textit{Bias Identification
Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any
sentiment analysis and toxicity detection models. Our study utilizes BITS to
uncover significant biases in four open AIaaS (AI as a Service) sentiment
analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,
DistilBERT and two toxicity detection models, namely two versions of
Toxic-BERT. Our findings indicate that all of these models exhibit
statistically significant explicit bias against PWD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkit_P/0/1/0/all/0/1&quot;&gt;Pranav Narayanan Venkit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinath_M/0/1/0/all/0/1&quot;&gt;Mukund Srinath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1&quot;&gt;Shomir Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09212">
<title>How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09212</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the size of a neural network needed to approximate the maximum
function over $d$ inputs, in the most basic setting of approximating with
respect to the $L_2$ norm, for continuous distributions, for a network that
uses ReLU activations. We provide new lower and upper bounds on the width
required for approximation across various depths. Our results establish new
depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as
providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$
construction which approximates the maximum function, significantly improving
upon the depth requirements of the best previously known bounds for networks
with linearly-bounded width. Our depth separation results are facilitated by a
new lower bound for depth 2 networks approximating the maximum function over
the uniform distribution, assuming an exponential upper bound on the size of
the weights. Furthermore, we are able to use this depth 2 lower bound to
provide tight bounds on the number of neurons needed to approximate the maximum
by a depth 3 network. Our lower bounds are of potentially broad interest as
they apply to the widely studied and used \emph{max} function, in contrast to
many previous results that base their bounds on specially constructed or
pathological functions and distributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1&quot;&gt;Itay Safran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichman_D/0/1/0/all/0/1&quot;&gt;Daniel Reichman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_P/0/1/0/all/0/1&quot;&gt;Paul Valiant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09218">
<title>A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09218</link>
<description rdf:parseType="Literal">&lt;p&gt;Forgetting refers to the loss or deterioration of previously acquired
information or knowledge. While the existing surveys on forgetting have
primarily focused on continual learning, forgetting is a prevalent phenomenon
observed in various other research domains within deep learning. Forgetting
manifests in research fields such as generative models due to generator shifts,
and federated learning due to heterogeneous data distributions across clients.
Addressing forgetting encompasses several challenges, including balancing the
retention of old task knowledge with fast learning of new tasks, managing task
interference with conflicting goals, and preventing privacy leakage, etc.
Moreover, most existing surveys on continual learning implicitly assume that
forgetting is always harmful. In contrast, our survey argues that forgetting is
a double-edged sword and can be beneficial and desirable in certain cases, such
as privacy-preserving scenarios. By exploring forgetting in a broader context,
we aim to present a more nuanced understanding of this phenomenon and highlight
its potential advantages. Through this comprehensive survey, we aspire to
uncover potential solutions by drawing upon ideas and approaches from various
fields that have dealt with forgetting. By examining forgetting beyond its
conventional boundaries, in future work, we hope to encourage the development
of novel strategies for mitigating, harnessing, or even embracing forgetting in
real applications. A comprehensive list of papers about forgetting in various
research fields is available at
\url{https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;Enneng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09230">
<title>Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review. (arXiv:2307.09230v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09230</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we perform a scoping review of the current literature on the
detection of throat cancer from speech recordings using machine learning and
artificial intelligence. We find 22 papers within this area and discuss their
methods and results. We split these papers into two groups - nine performing
binary classification, and 13 performing multi-class classification. The papers
present a range of methods with neural networks being most commonly
implemented. Many features are also extracted from the audio before
classification, with the most common bring mel-frequency cepstral coefficients.
None of the papers found in this search have associated code repositories and
as such are not reproducible. Therefore, we create a publicly available code
repository of our own classifiers. We use transfer learning on a multi-class
problem, classifying three pathologies and healthy controls. Using this
technique we achieve an unweighted average recall of 53.54%, sensitivity of
83.14%, and specificity of 64.00%. We compare our classifiers with the results
obtained on the same dataset and find similar results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paterson_M/0/1/0/all/0/1&quot;&gt;Mary Paterson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moor_J/0/1/0/all/0/1&quot;&gt;James Moor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutillo_L/0/1/0/all/0/1&quot;&gt;Luisa Cutillo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09238">
<title>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly. (arXiv:2307.09238v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09238</link>
<description rdf:parseType="Literal">&lt;p&gt;As collaborative robots (cobots) continue to gain popularity in industrial
manufacturing, effective human-robot collaboration becomes crucial. Cobots
should be able to recognize human actions to assist with assembly tasks and act
autonomously. To achieve this, skeleton-based approaches are often used due to
their ability to generalize across various people and environments. Although
body skeleton approaches are widely used for action recognition, they may not
be accurate enough for assembly actions where the worker&apos;s fingers and hands
play a significant role. To address this limitation, we propose a method in
which less detailed body skeletons are combined with highly detailed hand
skeletons. We investigate CNNs and transformers, the latter of which are
particularly adept at extracting and combining important information from both
skeleton types using attention. This paper demonstrates the effectiveness of
our proposed approach in enhancing action recognition in assembly scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aganian_D/0/1/0/all/0/1&quot;&gt;Dustin Aganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_M/0/1/0/all/0/1&quot;&gt;Mona K&amp;#xf6;hler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephan_B/0/1/0/all/0/1&quot;&gt;Benedict Stephan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenbach_M/0/1/0/all/0/1&quot;&gt;Markus Eisenbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1&quot;&gt;Horst-Michael Gross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09244">
<title>Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09244</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-intrusive load monitoring (NILM) is the process of obtaining
appliance-level data from a single metering point, measuring total electricity
consumption of a household or a business. Appliance-level data can be directly
used for demand response applications and energy management systems as well as
for awareness raising and motivation for improvements in energy efficiency and
reduction in the carbon footprint. Recently, classical machine learning and
deep learning (DL) techniques became very popular and proved as highly
effective for NILM classification, but with the growing complexity these
methods are faced with significant computational and energy demands during both
their training and operation. In this paper, we introduce a novel DL model
aimed at enhanced multi-label classification of NILM with improved computation
and energy efficiency. We also propose a testing methodology for comparison of
different models using data synthesized from the measurement datasets so as to
better represent real-world scenarios. Compared to the state-of-the-art, the
proposed model has its carbon footprint reduced by more than 23% while
providing on average approximately 8 percentage points in performance
improvement when testing on data derived from REFIT and UK-DALE datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pirnat_A/0/1/0/all/0/1&quot;&gt;An&amp;#x17e;e Pirnat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertalanic_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; Bertalani&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerar_G/0/1/0/all/0/1&quot;&gt;Gregor Cerar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohorcic_M/0/1/0/all/0/1&quot;&gt;Mihael Mohor&amp;#x10d;i&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuna_C/0/1/0/all/0/1&quot;&gt;Carolina Fortuna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09248">
<title>Application of BERT in Wind Power Forecasting-Teletraan&apos;s Solution in Baidu KDD Cup 2022. (arXiv:2307.09248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09248</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, wind energy has drawn increasing attention as its important role in
carbon neutrality and sustainable development. When wind power is integrated
into the power grid, precise forecasting is necessary for the sustainability
and security of the system. However, the unpredictable nature and long sequence
prediction make it especially challenging. In this technical report, we
introduce the BERT model applied for Baidu KDD Cup 2022, and the daily
fluctuation is added by post-processing to make the predicted results in line
with daily periodicity. Our solution achieves 3rd place of 2490 teams. The code
is released athttps://github.com/LongxingTan/KDD2022-Baidu
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1&quot;&gt;Longxing Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1&quot;&gt;Hongying Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09249">
<title>UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09249</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Natural Language Processing (NLP) have witnessed the
groundbreaking impact of pretrained models, yielding impressive outcomes across
various tasks. This study seeks to extend the power of pretraining
methodologies to tabular data, a domain traditionally overlooked, yet
inherently challenging due to the plethora of table schemas intrinsic to
different tasks. The primary research questions underpinning this work revolve
around the adaptation to heterogeneous table structures, the establishment of a
universal pretraining protocol for tabular data, the generalizability and
transferability of learned knowledge across tasks, the adaptation to diverse
downstream applications, and the incorporation of incremental columns over
time. In response to these challenges, we introduce UniTabE, a pioneering
method designed to process tables in a uniform manner, devoid of constraints
imposed by specific table structures. UniTabE&apos;s core concept relies on
representing each basic table element with a module, termed TabUnit. This is
subsequently followed by a Transformer encoder to refine the representation.
Moreover, our model is designed to facilitate pretraining and finetuning
through the utilization of free-form prompts. In order to implement the
pretraining phase, we curated an expansive tabular dataset comprising
approximately 13 billion samples, meticulously gathered from the Kaggle
platform. Rigorous experimental testing and analyses were performed under a
myriad of scenarios to validate the effectiveness of our methodology. The
experimental results demonstrate UniTabE&apos;s superior performance against several
baseline models across a multitude of benchmark datasets. This, therefore,
underscores UniTabE&apos;s potential to significantly enhance the semantic
representation of tabular data, thereby marking a significant stride in the
field of tabular data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yazheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Ledell Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09254">
<title>PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09254</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty learning and quantification of models are crucial tasks to
enhance the trustworthiness of the models. Importantly, the recent surge of
generative language models (GLMs) emphasizes the need for reliable uncertainty
quantification due to the concerns on generating hallucinated facts. In this
paper, we propose to learn neural prediction set models that comes with the
probably approximately correct (PAC) guarantee for quantifying the uncertainty
of GLMs. Unlike existing prediction set models, which are parameterized by a
scalar value, we propose to parameterize prediction sets via neural networks,
which achieves more precise uncertainty quantification but still satisfies the
PAC guarantee. We demonstrate the efficacy of our method on four types of
language datasets and six types of models by showing that our method improves
the quantified uncertainty by $63\%$ on average, compared to a standard
baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangdon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesoo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09259">
<title>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09259</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning for point clouds has been attracting much attention, with
many applications in various fields, such as shape recognition and material
science. To enhance the accuracy of such machine learning methods, it is known
to be effective to incorporate global topological features, which are typically
extracted by persistent homology. In the calculation of persistent homology for
a point cloud, we need to choose a filtration for the point clouds, an
increasing sequence of spaces. Because the performance of machine learning
methods combined with persistent homology is highly affected by the choice of a
filtration, we need to tune it depending on data and tasks. In this paper, we
propose a framework that learns a filtration adaptively with the use of neural
networks. In order to make the resulting persistent homology
isometry-invariant, we develop a neural network architecture with such
invariance. Additionally, we theoretically show a finite-dimensional
approximation result that justifies our architecture. Experimental results
demonstrated the efficacy of our framework in several classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishikawa_N/0/1/0/all/0/1&quot;&gt;Naoki Nishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ike_Y/0/1/0/all/0/1&quot;&gt;Yuichi Ike&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1&quot;&gt;Kenji Yamanishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09263">
<title>Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning. (arXiv:2307.09263v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.09263</link>
<description rdf:parseType="Literal">&lt;p&gt;As an efficient distributed machine learning approach, Federated learning
(FL) can obtain a shared model by iterative local model training at the user
side and global model aggregating at the central server side, thereby
protecting privacy of users. Mobile users in FL systems typically communicate
with base stations (BSs) via wireless channels, where training performance
could be degraded due to unreliable access caused by user mobility. However,
existing work only investigates a static scenario or random initialization of
user locations, which fail to capture mobility in real-world networks. To
tackle this issue, we propose a practical model for user mobility in FL across
multiple BSs, and develop a user scheduling and resource allocation method to
minimize the training delay with constrained communication resources.
Specifically, we first formulate an optimization problem with user mobility
that jointly considers user selection, BS assignment to users, and bandwidth
allocation to minimize the latency in each communication round. This
optimization problem turned out to be NP-hard and we proposed a delay-aware
greedy search algorithm (DAGSA) to solve it. Simulation results show that the
proposed algorithm achieves better performance than the state-of-the-art
baselines and a certain level of user mobility could improve training
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1&quot;&gt;Kecheng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiumei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xuefeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09269">
<title>End-to-End Neural Network Training for Hyperbox-Based Classification. (arXiv:2307.09269v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09269</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbox-based classification has been seen as a promising technique in which
decisions on the data are represented as a series of orthogonal,
multidimensional boxes (i.e., hyperboxes) that are often interpretable and
human-readable. However, existing methods are no longer capable of efficiently
handling the increasing volume of data many application domains face nowadays.
We address this gap by proposing a novel, fully differentiable framework for
hyperbox-based classification via neural networks. In contrast to previous
work, our hyperbox models can be efficiently trained in an end-to-end fashion,
which leads to significantly reduced training times and superior classification
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_D/0/1/0/all/0/1&quot;&gt;Denis Mayr Lima Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lulf_C/0/1/0/all/0/1&quot;&gt;Christian L&amp;#xfc;lf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gieseke_F/0/1/0/all/0/1&quot;&gt;Fabian Gieseke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09286">
<title>FlexiAST: Flexibility is What AST Needs. (arXiv:2307.09286v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.09286</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this work is to give patch-size flexibility to Audio
Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior
performance in various audio-based tasks. However, the performance of standard
ASTs degrades drastically when evaluated using different patch sizes from that
used during training. As a result, AST models are typically re-trained to
accommodate changes in patch sizes. To overcome this limitation, this paper
proposes a training procedure to provide flexibility to standard AST models
without architectural changes, allowing them to work with various patch sizes
at the inference stage - FlexiAST. This proposed training approach simply
utilizes random patch size selection and resizing of patch and positional
embedding weights. Our experiments show that FlexiAST gives similar performance
to standard AST models while maintaining its evaluation ability at various
patch sizes on different datasets for audio classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erol_M/0/1/0/all/0/1&quot;&gt;Mehmet Hamza Erol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Joon Son Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senocak_A/0/1/0/all/0/1&quot;&gt;Arda Senocak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09295">
<title>Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09295</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of best-item identification from choice-based feedback.
In this problem, a company sequentially and adaptively shows display sets to a
population of customers and collects their choices. The objective is to
identify the most preferred item with the least number of samples and at a high
confidence level. We propose an elimination-based algorithm, namely Nested
Elimination (NE), which is inspired by the nested structure implied by the
information-theoretic lower bound. NE is simple in structure, easy to
implement, and has a strong theoretical guarantee for sample complexity.
Specifically, NE utilizes an innovative elimination criterion and circumvents
the need to solve any complex combinatorial optimization problem. We provide an
instance-specific and non-asymptotic bound on the expected sample complexity of
NE. We also show NE achieves high-order worst-case asymptotic optimality.
Finally, numerical experiments from both synthetic and real data corroborate
our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junwen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yifan Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09302">
<title>Conformal prediction under ambiguous ground truth. (arXiv:2307.09302v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09302</link>
<description rdf:parseType="Literal">&lt;p&gt;In safety-critical classification tasks, conformal prediction allows to
perform rigorous uncertainty quantification by providing confidence sets
including the true class with a user-specified probability. This generally
assumes the availability of a held-out calibration set with access to ground
truth labels. Unfortunately, in many domains, such labels are difficult to
obtain and usually approximated by aggregating expert opinions. In fact, this
holds true for almost all datasets, including well-known ones such as CIFAR and
ImageNet. Applying conformal prediction using such labels underestimates
uncertainty. Indeed, when expert opinions are not resolvable, there is inherent
ambiguity present in the labels. That is, we do not have ``crisp&apos;&apos;, definitive
ground truth labels and this uncertainty should be taken into account during
calibration. In this paper, we develop a conformal prediction framework for
such ambiguous ground truth settings which relies on an approximation of the
underlying posterior distribution of labels given inputs. We demonstrate our
methodology on synthetic and real datasets, including a case study of skin
condition classification in dermatology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1&quot;&gt;David Stutz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Abhijit Guha Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matejovicova_T/0/1/0/all/0/1&quot;&gt;Tatiana Matejovicova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strachan_P/0/1/0/all/0/1&quot;&gt;Patricia Strachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1&quot;&gt;Ali Taylan Cemgil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1&quot;&gt;Arnaud Doucet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09306">
<title>EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting. (arXiv:2307.09306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09306</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing high-dimensional social interactions and feasible futures is
essential for predicting trajectories. To address this complex nature, several
attempts have been devoted to reducing the dimensionality of the output
variables via parametric curve fitting such as the B\&apos;ezier curve and B-spline
function. However, these functions, which originate in computer graphics
fields, are not suitable to account for socially acceptable human dynamics. In
this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction
approach that uses a novel trajectory descriptor to form a compact space, known
here as $\mathbb{ET}$ space, in place of Euclidean space, for representing
pedestrian movements. We first reduce the complexity of the trajectory
descriptor via a low-rank approximation. We transform the pedestrians&apos; history
paths into our $\mathbb{ET}$ space represented by spatio-temporal principle
components, and feed them into off-the-shelf trajectory forecasting models. The
inputs and outputs of the models as well as social interactions are all
gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we
propose a trajectory anchor-based refinement method to cover all possible
futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate
that our EigenTrajectory predictor can significantly improve both the
prediction accuracy and reliability of existing trajectory forecasting models
on public benchmarks, indicating that the proposed descriptor is suited to
represent pedestrian behaviors. Code is publicly available at
https://github.com/inhwanbae/EigenTrajectory .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1&quot;&gt;Inhwan Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jean Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1&quot;&gt;Hae-Gon Jeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09311">
<title>Automatic Differentiation for Inverse Problems with Applications in Quantum Transport. (arXiv:2307.09311v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09311</link>
<description rdf:parseType="Literal">&lt;p&gt;A neural solver and differentiable simulation of the quantum transmitting
boundary model is presented for the inverse quantum transport problem. The
neural solver is used to engineer continuous transmission properties and the
differentiable simulation is used to engineer current-voltage characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_I/0/1/0/all/0/1&quot;&gt;Ivan Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polizzi_E/0/1/0/all/0/1&quot;&gt;Eric Polizzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09312">
<title>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.09312</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal
graph-based transformer model for detecting hate speech in online social
networks. In contrast to traditional text-only methods, our approach to
labelling a comment as hate speech centers around the holistic analysis of text
and images. This is done by leveraging graph transformers to capture the
contextual relationships in the entire discussion that surrounds a comment,
with interwoven fusion layers to combine text and image embeddings instead of
processing different modalities separately. We compare the performance of our
model to baselines that only process text; we also conduct extensive ablation
studies. We conclude with future work for multimodal solutions to deliver
social value in online contexts, arguing that capturing a holistic view of a
conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1&quot;&gt;Liam Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1&quot;&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivas_N/0/1/0/all/0/1&quot;&gt;Nanda Kishore Sreenivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1&quot;&gt;Lukasz Golab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Robin Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09320">
<title>Biomaker CA: a Biome Maker project using Cellular Automata. (arXiv:2307.09320v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.09320</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA).
In Biomaker CA, morphogenesis is a first class citizen and small seeds need to
grow into plant-like organisms to survive in a nutrient starved environment and
eventually reproduce with variation so that a biome survives for long
timelines. We simulate complex biomes by means of CA rules in 2D grids and
parallelize all of its computation on GPUs through the Python JAX framework. We
show how this project allows for several different kinds of environments and
laws of &apos;physics&apos;, alongside different model architectures and mutation
strategies. We further analyze some configurations to show how plant agents can
grow, survive, reproduce, and evolve, forming stable and unstable biomes. We
then demonstrate how one can meta-evolve models to survive in a harsh
environment either through end-to-end meta-evolution or by a more surgical and
efficient approach, called Petri dish meta-evolution. Finally, we show how to
perform interactive evolution, where the user decides how to evolve a plant
model interactively and then deploys it in a larger environment. We open source
Biomaker CA at: https://tinyurl.com/2x8yu34s .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1&quot;&gt;Ettore Randazzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1&quot;&gt;Alexander Mordvintsev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09321">
<title>Exploiting Field Dependencies for Learning on Categorical Data. (arXiv:2307.09321v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09321</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional approaches for learning on categorical data underexploit the
dependencies between columns (\aka fields) in a dataset because they rely on
the embedding of data points driven alone by the classification/regression
loss. In contrast, we propose a novel method for learning on categorical data
with the goal of exploiting dependencies between fields. Instead of modelling
statistics of features globally (i.e., by the covariance matrix of features),
we learn a global field dependency matrix that captures dependencies between
fields and then we refine the global field dependency matrix at the
instance-wise level with different weights (so-called local dependency
modelling) w.r.t. each field to improve the modelling of the field
dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the
dependency matrices are refined in the inner loop of the meta-learning
algorithm without the use of labels, whereas the outer loop intertwines the
updates of the embedding matrix (the matrix performing projection) and global
dependency matrix in a supervised fashion (with the use of labels). Our method
is simple yet it outperforms several state-of-the-art methods on six popular
dataset benchmarks. Detailed ablation studies provide additional insights into
our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pagendam_D/0/1/0/all/0/1&quot;&gt;Daniel Edward Pagendam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09337">
<title>Variational Monte Carlo on a Budget -- Fine-tuning pre-trained Neural Wavefunctions. (arXiv:2307.09337v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/2307.09337</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining accurate solutions to the Schr\&quot;odinger equation is the key
challenge in computational quantum chemistry. Deep-learning-based Variational
Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms
of accuracy, but only at large computational cost. Whereas in many domains
models are trained once and subsequently applied for inference, accurate DL-VMC
so far requires a full optimization for every new problem instance, consuming
thousands of GPUhs even for small molecules. We instead propose a DL-VMC model
which has been pre-trained using self-supervised wavefunction optimization on a
large and chemically diverse set of molecules. Applying this model to new
molecules without any optimization, yields wavefunctions and absolute energies
that outperform established methods such as CCSD(T)-2Z. To obtain accurate
relative energies, only few fine-tuning steps of this base model are required.
We accomplish this with a fully end-to-end machine-learned model, consisting of
an improved geometry embedding architecture and an existing SE(3)-equivariant
model to represent molecular orbitals. Combining this architecture with
continuous sampling of geometries, we improve zero-shot accuracy by two orders
of magnitude compared to the state of the art. We extensively evaluate the
accuracy, scalability and limitations of our base model on a wide variety of
test systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Scherbela_M/0/1/0/all/0/1&quot;&gt;Michael Scherbela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gerard_L/0/1/0/all/0/1&quot;&gt;Leon Gerard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Grohs_P/0/1/0/all/0/1&quot;&gt;Philipp Grohs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09342">
<title>Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.09342</link>
<description rdf:parseType="Literal">&lt;p&gt;Many constraint satisfaction and optimisation problems can be solved
effectively by encoding them as instances of the Boolean Satisfiability problem
(SAT). However, even the simplest types of constraints have many encodings in
the literature with widely varying performance, and the problem of selecting
suitable encodings for a given problem instance is not trivial. We explore the
problem of selecting encodings for pseudo-Boolean and linear constraints using
a supervised machine learning approach. We show that it is possible to select
encodings effectively using a standard set of features for constraint problems;
however we obtain better performance with a new set of features specifically
designed for the pseudo-Boolean and linear constraints. In fact, we achieve
good results when selecting encodings for unseen problem classes. Our results
compare favourably to AutoFolio when using the same feature set. We discuss the
relative importance of instance features to the task of selecting the best
encodings, and compare several variations of the machine learning method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_Oltean_F/0/1/0/all/0/1&quot;&gt;Felix Ulrich-Oltean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nightingale_P/0/1/0/all/0/1&quot;&gt;Peter Nightingale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;James Alfred Walker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09357">
<title>Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference. (arXiv:2307.09357v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2307.09357</link>
<description rdf:parseType="Literal">&lt;p&gt;Analog In-Memory Computing (AIMC) is a promising approach to reduce the
latency and energy consumption of Deep Neural Network (DNN) inference and
training. However, the noisy and non-linear device characteristics, and the
non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be
deployed on such hardware to achieve equivalent accuracy to digital computing.
In this tutorial, we provide a deep dive into how such adaptations can be
achieved and evaluated using the recently released IBM Analog Hardware
Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit.
The AIHWKit is a Python library that simulates inference and training of DNNs
using AIMC. We present an in-depth description of the AIHWKit design,
functionality, and best practices to properly perform inference and training.
We also present an overview of the Analog AI Cloud Composer, that provides the
benefits of using the AIHWKit simulation platform in a fully managed cloud
setting. Finally, we show examples on how users can expand and customize
AIHWKit for their own needs. This tutorial is accompanied by comprehensive
Jupyter Notebook code examples that can be run using AIHWKit, which can be
downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallo_M/0/1/0/all/0/1&quot;&gt;Manuel Le Gallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lammie_C/0/1/0/all/0/1&quot;&gt;Corey Lammie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buechel_J/0/1/0/all/0/1&quot;&gt;Julian Buechel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carta_F/0/1/0/all/0/1&quot;&gt;Fabio Carta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fagbohungbe_O/0/1/0/all/0/1&quot;&gt;Omobayode Fagbohungbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackin_C/0/1/0/all/0/1&quot;&gt;Charles Mackin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_H/0/1/0/all/0/1&quot;&gt;Hsinyu Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1&quot;&gt;Vijay Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1&quot;&gt;Abu Sebastian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maghraoui_K/0/1/0/all/0/1&quot;&gt;Kaoutar El Maghraoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasch_M/0/1/0/all/0/1&quot;&gt;Malte J. Rasch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09361">
<title>MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09361</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning can be used for mitigating the greedy needs of
Vision Transformer networks for very large fully-annotated datasets. Different
classes of self-supervised learning offer representations with either good
contextual reasoning properties, e.g., using masked image modeling strategies,
or invariance to image perturbations, e.g., with contrastive methods. In this
work, we propose a single-stage and standalone method, MOCA, which unifies both
desired properties using novel mask-and-predict objectives defined with
high-level features (instead of pixel-level details). Moreover, we show how to
effectively employ both learning paradigms in a synergistic and
computation-efficient way. Doing so, we achieve new state-of-the-art results on
low-shot settings and strong experimental results in various evaluation
protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1&quot;&gt;Spyros Gidaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1&quot;&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeoni_O/0/1/0/all/0/1&quot;&gt;Oriane Simeoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vobecky_A/0/1/0/all/0/1&quot;&gt;Antonin Vobecky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1&quot;&gt;Nikos Komodakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1&quot;&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09365">
<title>An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness. (arXiv:2307.09365v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09365</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-cost proxies are nowadays frequently studied and used to search for
neural architectures. They show an impressive ability to predict the
performance of architectures by making use of their untrained weights. These
techniques allow for immense search speed-ups. So far the joint search for
well-performing and robust architectures has received much less attention in
the field of NAS. Therefore, the main focus of zero-cost proxies is the clean
accuracy of architectures, whereas the model robustness should play an evenly
important part. In this paper, we analyze the ability of common zero-cost
proxies to serve as performance predictors for robustness in the popular
NAS-Bench-201 search space. We are interested in the single prediction task for
robustness and the joint multi-objective of clean and robust accuracy. We
further analyze the feature importance of the proxies and show that predicting
the robustness makes the prediction task from existing zero-cost proxies more
challenging. As a result, the joint consideration of several proxies becomes
necessary to predict a model&apos;s robustness while the clean accuracy can be
regressed from a single such feature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1&quot;&gt;Jovita Lukasik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1&quot;&gt;Michael Moeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09366">
<title>Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives. (arXiv:2307.09366v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09366</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of learning a sparse graph underlying an undirected
Gaussian graphical model, a key problem in statistical machine learning. Given
$n$ samples from a multivariate Gaussian distribution with $p$ variables, the
goal is to estimate the $p \times p$ inverse covariance matrix (aka precision
matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose
GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the
pseudolikelihood function, while most earlier approaches are based on the
$\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer
program (MIP) which can be difficult to compute at scale using off-the-shelf
commercial solvers. To solve the MIP, we propose a custom nonlinear
branch-and-bound (BnB) framework that solves node relaxations with tailored
first-order methods. As a by-product of our BnB framework, we propose
large-scale solvers for obtaining good primal solutions that are of independent
interest. We derive novel statistical guarantees (estimation and variable
selection) for our estimator and discuss how our approach improves upon
existing estimators. Our numerical experiments on real/synthetic datasets
suggest that our method can solve, to near-optimality, problem instances with
$p = 10^4$ -- corresponding to a symmetric matrix of size $p \times p$ with
$p^2/2$ binary variables. We demonstrate the usefulness of GraphL0BnB versus
various state-of-the-art approaches on a range of datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behdin_K/0/1/0/all/0/1&quot;&gt;Kayhan Behdin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_R/0/1/0/all/0/1&quot;&gt;Rahul Mazumder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09372">
<title>Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation. (arXiv:2307.09372v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09372</link>
<description rdf:parseType="Literal">&lt;p&gt;Support Vector Machines (SVM) have gathered significant acclaim as
classifiers due to their successful implementation of Statistical Learning
Theory. However, in the context of multiclass and multilabel settings, the
reliance on vector-based formulations in existing SVM-based models poses
limitations regarding flexibility and ease of incorporating additional terms to
handle specific challenges. To overcome these limitations, our research paper
focuses on introducing a matrix formulation for SVM that effectively addresses
these constraints. By employing the Accelerated Gradient Descent method in the
dual, we notably enhance the efficiency of solving the Matrix-SVM problem.
Experimental evaluations on multilabel and multiclass datasets demonstrate that
Matrix SVM achieves superior time efficacy while delivering similar results to
Binary Relevance SVM.
&lt;/p&gt;
&lt;p&gt;Moreover, our matrix formulation unveils crucial insights and advantages that
may not be readily apparent in traditional vector-based notations. We emphasize
that numerous multilabel models can be viewed as extensions of SVM, with
customised modifications to meet specific requirements. The matrix formulation
presented in this paper establishes a solid foundation for developing more
sophisticated models capable of effectively addressing the distinctive
challenges encountered in multilabel learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1&quot;&gt;Sambhav Jain Reshma Rastogi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09377">
<title>Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading. (arXiv:2307.09377v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09377</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of machine learning in algorithmic trading systems is increasingly
common. In a typical set-up, supervised learning is used to predict the future
prices of assets, and those predictions drive a simple trading and execution
strategy. This is quite effective when the predictions have sufficient signal,
markets are liquid, and transaction costs are low. However, those conditions
often do not hold in thinly traded financial markets and markets for
differentiated assets such as real estate or vehicles. In these markets, the
trading strategy must consider the long-term effects of taking positions that
are relatively more difficult to change. In this work, we propose a
Reinforcement Learning (RL) algorithm that trades based on signals from a
learned predictive model and addresses these challenges. We test our algorithm
on 20+ years of equity data from Bursa Malaysia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvvur_V/0/1/0/all/0/1&quot;&gt;Vikram Duvvur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Aashay Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1&quot;&gt;Edward Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1&quot;&gt;Ken Yew Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09379">
<title>Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.09379</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the generalization properties of batched predictors, i.e., models
tasked with predicting the mean label of a small set (or batch) of examples.
The batched prediction paradigm is particularly relevant for models deployed to
determine the quality of a group of compounds in preparation for offline
testing. By utilizing a suitable generalization of the Rademacher complexity,
we prove that batched predictors come with exponentially stronger
generalization guarantees as compared to the standard per-sample approach.
Surprisingly, the proposed bound holds independently of overparametrization.
Our theoretical insights are validated experimentally for various tasks,
architectures, and applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loukas_A/0/1/0/all/0/1&quot;&gt;Andreas Loukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kessel_P/0/1/0/all/0/1&quot;&gt;Pan Kessel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09388">
<title>Online Learning with Costly Features in Non-stationary Environments. (arXiv:2307.09388v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09388</link>
<description rdf:parseType="Literal">&lt;p&gt;Maximizing long-term rewards is the primary goal in sequential
decision-making problems. The majority of existing methods assume that side
information is freely available, enabling the learning agent to observe all
features&apos; states before making a decision. In real-world problems, however,
collecting beneficial information is often costly. That implies that, besides
individual arms&apos; reward, learning the observations of the features&apos; states is
essential to improve the decision-making strategy. The problem is aggravated in
a non-stationary environment where reward and cost distributions undergo abrupt
changes over time. To address the aforementioned dual learning problem, we
extend the contextual bandit setting and allow the agent to observe subsets of
features&apos; states. The objective is to maximize the long-term average gain,
which is the difference between the accumulated rewards and the paid costs on
average. Therefore, the agent faces a trade-off between minimizing the cost of
information acquisition and possibly improving the decision-making process
using the obtained information. To this end, we develop an algorithm that
guarantees a sublinear regret in time. Numerical results demonstrate the
superiority of our proposed policy in a real-world scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghoorchian_S/0/1/0/all/0/1&quot;&gt;Saeed Ghoorchian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortukov_E/0/1/0/all/0/1&quot;&gt;Evgenii Kortukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1&quot;&gt;Setareh Maghsudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09423">
<title>Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09423</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation Learning (IL) is one of the most widely used methods in machine
learning. Yet, while powerful, many works find it is often not able to fully
recover the underlying expert behavior. However, none of these works deeply
investigate the role of scaling up the model and data size. Inspired by recent
work in Natural Language Processing (NLP) where &quot;scaling up&quot; has resulted in
increasingly more capable LLMs, we investigate whether carefully scaling up
model and data size can bring similar improvements in the imitation learning
setting. To demonstrate our findings, we focus on the game of NetHack, a
challenging environment featuring procedural generation, stochasticity,
long-term dependencies, and partial observability. We find IL loss and mean
return scale smoothly with the compute budget and are strongly correlated,
resulting in power laws for training compute-optimal IL agents with respect to
model size and number of samples. We forecast and train several NetHack agents
with IL and find they outperform prior state-of-the-art by at least 2x in all
settings. Our work both demonstrates the scaling behavior of imitation learning
in a challenging domain, as well as the viability of scaling up current
approaches for increasingly capable agents in NetHack, a game that remains
elusively hard for current AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuyls_J/0/1/0/all/0/1&quot;&gt;Jens Tuyls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1&quot;&gt;Dhruv Madeka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torkkola_K/0/1/0/all/0/1&quot;&gt;Kari Torkkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dean Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1&quot;&gt;Karthik Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09437">
<title>Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09437</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting object-level representations for downstream reasoning tasks is an
emerging area in AI. Learning object-centric representations in an unsupervised
setting presents multiple challenges, a key one being binding an arbitrary
number of object instances to a specialized object slot. Recent object-centric
representation methods like Slot Attention utilize iterative attention to learn
composable representations with dynamic inference level binding but fail to
achieve specialized slot level binding. To address this, in this paper we
propose Unsupervised Conditional Slot Attention using a novel Probabilistic
Slot Dictionary (PSD). We define PSD with (i) abstract object-level property
vectors as key and (ii) parametric Gaussian distribution as its corresponding
value. We demonstrate the benefits of the learnt specific object-level
conditioning distributions in multiple downstream tasks, namely object
discovery, compositional scene generation, and compositional visual reasoning.
We show that our method provides scene composition capabilities and a
significant boost in a few shot adaptability tasks of compositional visual
reasoning, while performing similarly or better than slot attention in object
discovery tasks
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1&quot;&gt;Avinash Kori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09441">
<title>Convergent regularization in inverse problems and linear plug-and-play denoisers. (arXiv:2307.09441v1 [math.NA])</title>
<link>http://arxiv.org/abs/2307.09441</link>
<description rdf:parseType="Literal">&lt;p&gt;Plug-and-play (PnP) denoising is a popular iterative framework for solving
imaging inverse problems using off-the-shelf image denoisers. Their empirical
success has motivated a line of research that seeks to understand the
convergence of PnP iterates under various assumptions on the denoiser. While a
significant amount of research has gone into establishing the convergence of
the PnP iteration for different regularity conditions on the denoisers, not
much is known about the asymptotic properties of the converged solution as the
noise level in the measurement tends to zero, i.e., whether PnP methods are
provably convergent regularization schemes under reasonable assumptions on the
denoiser. This paper serves two purposes: first, we provide an overview of the
classical regularization theory in inverse problems and survey a few notable
recent data-driven methods that are provably convergent regularization schemes.
We then continue to discuss PnP algorithms and their established convergence
guarantees. Subsequently, we consider PnP algorithms with linear denoisers and
propose a novel spectral filtering technique to control the strength of
regularization arising from the denoiser. Further, by relating the implicit
regularization of the denoiser to an explicit regularization functional, we
rigorously show that PnP with linear denoisers leads to a convergent
regularization scheme. More specifically, we prove that in the limit as the
noise vanishes, the PnP reconstruction converges to the minimizer of a
regularization potential subject to the solution satisfying the noiseless
operator equation. The theoretical analysis is corroborated by numerical
experiments for the classical inverse problem of tomographic image
reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hauptmann_A/0/1/0/all/0/1&quot;&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Subhadip Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sherry_F/0/1/0/all/0/1&quot;&gt;Ferdia Sherry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09457">
<title>Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection. (arXiv:2307.09457v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09457</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple Instance Learning (MIL) has been widely applied to medical imaging
diagnosis, where bag labels are known and instance labels inside bags are
unknown. Traditional MIL assumes that instances in each bag are independent
samples from a given distribution. However, instances are often spatially or
sequentially ordered, and one would expect similar diagnostic importance for
neighboring instances. To address this, in this study, we propose a smooth
attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction
of first and second order constraints on the latent function encoding the
attention paid to each instance in a bag. The method is applied to the
detection of intracranial hemorrhage (ICH) on head CT scans. The results show
that this novel SA-DMIL: (a) achieves better performance than the non-smooth
attention MIL at both scan (bag) and slice (instance) levels; (b) learns
spatial dependencies between slices; and (c) outperforms current
state-of-the-art MIL methods on the same ICH test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yunan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Castro_Macias_F/0/1/0/all/0/1&quot;&gt;Francisco M. Castro-Mac&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morales_Alvarez_P/0/1/0/all/0/1&quot;&gt;Pablo Morales-&amp;#xc1;lvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Molina_R/0/1/0/all/0/1&quot;&gt;Rafael Molina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Katsaggelos_A/0/1/0/all/0/1&quot;&gt;Aggelos K. Katsaggelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09458">
<title>Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09458</link>
<description rdf:parseType="Literal">&lt;p&gt;\emph{Circuit analysis} is a promising technique for understanding the
internal mechanisms of language models. However, existing analyses are done in
small models far from the state of the art. To address this, we present a case
study of circuit analysis in the 70B Chinchilla model, aiming to test the
scalability of circuit analysis. In particular, we study multiple-choice
question answering, and investigate Chinchilla&apos;s capability to identify the
correct answer \emph{label} given knowledge of the correct answer \emph{text}.
We find that the existing techniques of logit attribution, attention pattern
visualization, and activation patching naturally scale to Chinchilla, allowing
us to identify and categorize a small set of `output nodes&apos; (attention heads
and MLPs).
&lt;/p&gt;
&lt;p&gt;We further study the `correct letter&apos; category of attention heads aiming to
understand the semantics of their features, with mixed results. For normal
multiple-choice question answers, we significantly compress the query, key and
value subspaces of the head without loss of performance when operating on the
answer labels for multiple-choice questions, and we show that the query and key
subspaces represent an `Nth item in an enumeration&apos; feature to at least some
extent. However, when we attempt to use this explanation to understand the
heads&apos; behaviour on a more general distribution including randomized answer
labels, we find that it is only a partial explanation, suggesting there is more
to learn about the operation of `correct letter&apos; heads on multiple choice
question answering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lieberum_T/0/1/0/all/0/1&quot;&gt;Tom Lieberum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahtz_M/0/1/0/all/0/1&quot;&gt;Matthew Rahtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kramar_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe1;nos Kram&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1&quot;&gt;Geoffrey Irving&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rohin Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1&quot;&gt;Vladimir Mikulik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09463">
<title>A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction. (arXiv:2307.09463v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.09463</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural decoders for quantum error correction (QEC) rely on neural networks to
classify syndromes extracted from error correction codes and find appropriate
recovery operators to protect logical information against errors. Despite the
good performance of neural decoders, important practical requirements remain to
be achieved, such as minimizing the decoding time to meet typical rates of
syndrome generation in repeated error correction schemes, and ensuring the
scalability of the decoding approach as the code distance increases. Designing
a dedicated integrated circuit to perform the decoding task in co-integration
with a quantum processor appears necessary to reach these decoding time and
scalability requirements, as routing signals in and out of a cryogenic
environment to be processed externally leads to unnecessary delays and an
eventual wiring bottleneck. In this work, we report the design and performance
analysis of a neural decoder inference accelerator based on an in-memory
computing (IMC) architecture, where crossbar arrays of resistive memory devices
are employed to both store the synaptic weights of the decoder neural network
and perform analog matrix-vector multiplications during inference. In
proof-of-concept numerical experiments supported by experimental measurements,
we investigate the impact of TiO$_\textrm{x}$-based memristive devices&apos;
non-idealities on decoding accuracy. Hardware-aware training methods are
developed to mitigate the loss in accuracy, allowing the memristive neural
decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the
distance-three surface code, whereas the equivalent digital neural decoder
achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a
pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated
QEC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Marcotte_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Marcotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Mouny_P/0/1/0/all/0/1&quot;&gt;Pierre-Antoine Mouny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yon_V/0/1/0/all/0/1&quot;&gt;Victor Yon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dagnew_G/0/1/0/all/0/1&quot;&gt;Gebremedhin A. Dagnew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kulchytskyy_B/0/1/0/all/0/1&quot;&gt;Bohdan Kulchytskyy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Rochette_S/0/1/0/all/0/1&quot;&gt;Sophie Rochette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Beilliard_Y/0/1/0/all/0/1&quot;&gt;Yann Beilliard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Drouin_D/0/1/0/all/0/1&quot;&gt;Dominique Drouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ronagh_P/0/1/0/all/0/1&quot;&gt;Pooya Ronagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09469">
<title>Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications. (arXiv:2307.09469v1 [physics.plasm-ph])</title>
<link>http://arxiv.org/abs/2307.09469</link>
<description rdf:parseType="Literal">&lt;p&gt;Topological analysis of the magnetic field in simulated plasmas allows the
study of various physical phenomena in a wide range of settings. One such
application is magnetic reconnection, a phenomenon related to the dynamics of
the magnetic field topology, which is difficult to detect and characterize in
three dimensions. We propose a scalable pipeline for topological data analysis
and spatiotemporal graph representation of three-dimensional magnetic vector
fields. We demonstrate our methods on simulations of the Earth&apos;s magnetosphere
produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for
near-Earth space. The purpose of this work is to challenge the machine learning
community to explore graph-based machine learning approaches to address a
largely open scientific problem with wide-ranging potential impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bouri_I/0/1/0/all/0/1&quot;&gt;Ioanna Bouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Franssila_F/0/1/0/all/0/1&quot;&gt;Fanni Franssila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Alho_M/0/1/0/all/0/1&quot;&gt;Markku Alho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cozzani_G/0/1/0/all/0/1&quot;&gt;Giulia Cozzani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zaitsev_I/0/1/0/all/0/1&quot;&gt;Ivan Zaitsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Palmroth_M/0/1/0/all/0/1&quot;&gt;Minna Palmroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Roos_T/0/1/0/all/0/1&quot;&gt;Teemu Roos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09470">
<title>Multi-Player Zero-Sum Markov Games with Networked Separable Interactions. (arXiv:2307.09470v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.09470</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a new class of Markov games (MGs), \textit{Multi-player Zero-sum
Markov Games} with {\it Networked separable interactions} (MZNMGs), to model
the local interaction structure in non-cooperative multi-agent sequential
decision-making. We define an MZNMG as a model where {the payoffs of the
auxiliary games associated with each state are zero-sum and} have some
separable (i.e., polymatrix) structure across the neighbors over some
interaction network. We first identify the necessary and sufficient conditions
under which an MG can be presented as an MZNMG, and show that the set of Markov
coarse correlated equilibrium (CCE) collapses to the set of Markov Nash
equilibrium (NE) in these games, in that the {product of} per-state
marginalization of the former for all players yields the latter. Furthermore,
we show that finding approximate Markov \emph{stationary} CCE in
infinite-horizon discounted MZNMGs is \texttt{PPAD}-hard, unless the underlying
network has a ``star topology&apos;&apos;. Then, we propose fictitious-play-type
dynamics, the classical learning dynamics in normal-form games, for MZNMGs, and
establish convergence guarantees to Markov stationary NE under a star-shaped
network structure. Finally, in light of the hardness result, we focus on
computing a Markov \emph{non-stationary} NE and provide finite-iteration
guarantees for a series of value-iteration-based algorithms. We also provide
numerical experiments to corroborate our theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1&quot;&gt;Asuman Ozdaglar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09476">
<title>Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09476</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern language models can imitate complex patterns through few-shot
learning, enabling them to complete challenging tasks without fine-tuning.
However, imitation can also lead models to reproduce inaccuracies or harmful
content if present in the context. We study harmful imitation through the lens
of a model&apos;s internal representations, and identify two related phenomena:
overthinking and false induction heads. The first phenomenon, overthinking,
appears when we decode predictions from intermediate layers, given correct vs.
incorrect few-shot demonstrations. At early layers, both demonstrations induce
similar model behavior, but the behavior diverges sharply at some &quot;critical
layer&quot;, after which the accuracy given incorrect demonstrations progressively
decreases. The second phenomenon, false induction heads, are a possible
mechanistic cause of overthinking: these are heads in late layers that attend
to and copy false information from previous demonstrations, and whose ablation
reduces overthinking. Beyond scientific understanding, our results suggest that
studying intermediate model computations could be a promising avenue for
understanding and guarding against harmful model behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halawi_D/0/1/0/all/0/1&quot;&gt;Danny Halawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1&quot;&gt;Jean-Stanislas Denain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09477">
<title>Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.09477</link>
<description rdf:parseType="Literal">&lt;p&gt;Order is one of the main instruments to measure the relationship between
objects in (empirical) data. However, compared to methods that use numerical
properties of objects, the amount of ordinal methods developed is rather small.
One reason for this is the limited availability of computational resources in
the last century that would have been required for ordinal computations.
Another reason -- particularly important for this line of research -- is that
order-based methods are often seen as too mathematically rigorous for applying
them to real-world data. In this paper, we will therefore discuss different
means for measuring and &apos;calculating&apos; with ordinal structures -- a specific
class of directed graphs -- and show how to infer knowledge from them. Our aim
is to establish Ordinal Data Science as a fundamentally new research agenda.
Besides cross-fertilization with other cornerstone machine learning and
knowledge representation methods, a broad range of disciplines will benefit
from this endeavor, including, psychology, sociology, economics, web science,
knowledge engineering, scientometrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1&quot;&gt;Gerd Stumme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durrschnabel_D/0/1/0/all/0/1&quot;&gt;Dominik D&amp;#xfc;rrschnabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1&quot;&gt;Tom Hanika&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09478">
<title>The Role of Transparency in Repeated First-Price Auctions with Unknown Valuations. (arXiv:2307.09478v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.09478</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of regret minimization for a single bidder in a sequence
of first-price auctions where the bidder knows the item&apos;s value only if the
auction is won. Our main contribution is a complete characterization, up to
logarithmic factors, of the minimax regret in terms of the auction&apos;s
transparency, which regulates the amount of information on competing bids
disclosed by the auctioneer at the end of each auction. Our results hold under
different assumptions (stochastic, adversarial, and their smoothed variants) on
the environment generating the bidder&apos;s valuations and competing bids. These
minimax rates reveal how the interplay between transparency and the nature of
the environment affects how fast one can learn to bid optimally in first-price
auctions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1&quot;&gt;Tommaso Cesari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colomboni_R/0/1/0/all/0/1&quot;&gt;Roberto Colomboni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Federico Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1&quot;&gt;Stefano Leonardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09483">
<title>Forecasting the steam mass flow in a powerplant using the parallel hybrid network. (arXiv:2307.09483v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09483</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient and sustainable power generation is a crucial concern in the energy
sector. In particular, thermal power plants grapple with accurately predicting
steam mass flow, which is crucial for operational efficiency and cost
reduction. In this study, we use a parallel hybrid neural network architecture
that combines a parametrized quantum circuit and a conventional feed-forward
neural network specifically designed for time-series prediction in industrial
settings to enhance predictions of steam mass flow 15 minutes into the future.
Our results show that the parallel hybrid model outperforms standalone
classical and quantum models, achieving more than 5.7 and 4.9 times lower mean
squared error (MSE) loss on the test set after training compared to pure
classical and pure quantum networks, respectively. Furthermore, the hybrid
model demonstrates smaller relative errors between the ground truth and the
model predictions on the test set, up to 2 times better than the pure classical
model. These findings contribute to the broader scientific understanding of how
integrating quantum and classical machine learning techniques can be applied to
real-world challenges faced by the energy sector, ultimately leading to
optimized power plant operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurkin_A/0/1/0/all/0/1&quot;&gt;Andrii Kurkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegemann_J/0/1/0/all/0/1&quot;&gt;Jonas Hegemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kordzanganeh_M/0/1/0/all/0/1&quot;&gt;Mo Kordzanganeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnikov_A/0/1/0/all/0/1&quot;&gt;Alexey Melnikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.02057">
<title>Online Observer-Based Inverse Reinforcement Learning. (arXiv:2011.02057v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2011.02057</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a novel approach to the output-feedback inverse reinforcement
learning (IRL) problem is developed by casting the IRL problem, for linear
systems with quadratic cost functions, as a state estimation problem. Two
observer-based techniques for IRL are developed, including a novel observer
method that re-uses previous state estimates via history stacks. Theoretical
guarantees for convergence and robustness are established under appropriate
excitation conditions. Simulations demonstrate the performance of the developed
observers and filters under noisy and noise-free measurements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Self_R/0/1/0/all/0/1&quot;&gt;Ryan Self&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Coleman_K/0/1/0/all/0/1&quot;&gt;Kevin Coleman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;He Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1&quot;&gt;Rushikesh Kamalapurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.03719">
<title>A survey on learning from imbalanced data streams: taxonomy, challenges, empirical study, and reproducible experimental framework. (arXiv:2204.03719v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.03719</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance poses new challenges when it comes to classifying data
streams. Many algorithms recently proposed in the literature tackle this
problem using a variety of data-level, algorithm-level, and ensemble
approaches. However, there is a lack of standardized and agreed-upon procedures
and benchmarks on how to evaluate these algorithms. This work proposes a
standardized, exhaustive, and comprehensive experimental framework to evaluate
algorithms in a collection of diverse and challenging imbalanced data stream
scenarios. The experimental study evaluates 24 state-of-the-art data streams
algorithms on 515 imbalanced data streams that combine static and dynamic class
imbalance ratios, instance-level difficulties, concept drift, real-world and
semi-synthetic datasets in binary and multi-class scenarios. This leads to a
large-scale experimental study comparing state-of-the-art classifiers in the
data stream mining domain. We discuss the advantages and disadvantages of
state-of-the-art classifiers in each of these scenarios and we provide general
recommendations to end-users for selecting the best algorithms for imbalanced
data streams. Additionally, we formulate open challenges and future directions
for this domain. Our experimental framework is fully reproducible and easy to
extend with new methods. This way, we propose a standardized approach to
conducting experiments in imbalanced data streams that can be used by other
researchers to create complete, trustworthy, and fair evaluation of newly
proposed methods. Our experimental framework can be downloaded from
https://github.com/canoalberto/imbalanced-streams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aguiar_G/0/1/0/all/0/1&quot;&gt;Gabriel Aguiar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krawczyk_B/0/1/0/all/0/1&quot;&gt;Bartosz Krawczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cano_A/0/1/0/all/0/1&quot;&gt;Alberto Cano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.08790">
<title>On-device modeling of user&apos;s social context and familiar places from smartphone-embedded sensor data. (arXiv:2205.08790v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.08790</link>
<description rdf:parseType="Literal">&lt;p&gt;Context modeling and recognition represent complex tasks that allow mobile
and ubiquitous computing applications to adapt to the user&apos;s situation. Current
solutions mainly focus on limited context information generally processed on
centralized architectures, potentially exposing users&apos; personal data to privacy
leakage, and missing personalization features. For these reasons on-device
context modeling and recognition represent the current research trend in this
area. Among the different information characterizing the user&apos;s context in
mobile environments, social interactions and visited locations remarkably
contribute to the characterization of daily life scenarios. In this paper we
propose a novel, unsupervised and lightweight approach to model the user&apos;s
social context and her locations based on ego networks directly on the user
mobile device. Relying on this model, the system is able to extract high-level
and semantic-rich context features from smartphone-embedded sensors data.
Specifically, for the social context it exploits data related to both physical
and cyber social interactions among users and their devices. As far as location
context is concerned, we assume that it is more relevant to model the
familiarity degree of a specific location for the user&apos;s context than the raw
location data, both in terms of GPS coordinates and proximity devices. By using
5 real-world datasets, we assess the structure of the social and location ego
networks, we provide a semantic evaluation of the proposed models and a
complexity evaluation in terms of mobile computing performance. Finally, we
demonstrate the relevance of the extracted features by showing the performance
of 3 machine learning algorithms to recognize daily-life situations, obtaining
an improvement of 3% of AUROC, 9% of Precision, and 5% in terms of Recall with
respect to use only features related to physical context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campana_M/0/1/0/all/0/1&quot;&gt;Mattia Giovanni Campana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmastro_F/0/1/0/all/0/1&quot;&gt;Franca Delmastro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14568">
<title>Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14568</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty quantification is crucial for assessing the predictive ability of
AI algorithms. Much research has been devoted to describing the predictive
distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$
given complex input features $\mathbf{x} \in \mathcal{X}$. However,
off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks)
often lack conditional calibration with the probability of occurrence of an
event given input $\mathbf{x}$ being significantly different from the predicted
probability. Current calibration methods do not fully assess and enforce
conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that
addresses both PD diagnostics and recalibration by learning a single
probability-probability map from calibration data. The key idea is to regress
probability integral transform scores against $\mathbf{x}$. The estimated
regression provides interpretable diagnostics of conditional coverage across
the feature space. The same regression function morphs the misspecified PD to a
re-calibrated PD for all $\mathbf{x}$. We benchmark our corrected prediction
bands (a by-product of corrected PDs) against oracle bands and state-of-the-art
predictive inference algorithms for synthetic data. We also provide results for
two applications: (i) probabilistic nowcasting given sequences of satellite
images, and (ii) conditional density estimation of galaxy distances given
imaging data (so-called photometric redshift estimation). Our code is available
as a Python package https://github.com/lee-group-cmu/Cal-PIT .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dey_B/0/1/0/all/0/1&quot;&gt;Biprateep Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;David Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Newman_J/0/1/0/all/0/1&quot;&gt;Jeffrey A. Newman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Andrews_B/0/1/0/all/0/1&quot;&gt;Brett H. Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Izbicki_R/0/1/0/all/0/1&quot;&gt;Rafael Izbicki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Ann B. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10381">
<title>TabText: A Flexible and Contextual Approach to Tabular Data Representation. (arXiv:2206.10381v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10381</link>
<description rdf:parseType="Literal">&lt;p&gt;Tabular data is essential for applying machine learning tasks across various
industries. However, traditional data processing methods do not fully utilize
all the information available in the tables, ignoring important contextual
information such as column header descriptions. In addition, pre-processing
data into a tabular format can remain a labor-intensive bottleneck in model
development. This work introduces TabText, a processing and feature extraction
framework that extracts contextual information from tabular data structures.
TabText addresses processing difficulties by converting the content into
language and utilizing pre-trained large language models (LLMs). We evaluate
our framework on nine healthcare prediction tasks ranging from patient
discharge, ICU admission, and mortality. We show that 1) applying our TabText
framework enables the generation of high-performing and simple machine learning
baseline models with minimal data pre-processing, and 2) augmenting
pre-processed tabular data with TabText representations improves the average
and worst-case AUC performance of standard machine learning models by as much
as 6%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carballo_K/0/1/0/all/0/1&quot;&gt;Kimberly Villalobos Carballo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_L/0/1/0/all/0/1&quot;&gt;Liangyuan Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boussioux_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;onard Boussioux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1&quot;&gt;Cynthia Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soenksen_L/0/1/0/all/0/1&quot;&gt;Luis R. Soenksen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1&quot;&gt;Dimitris Bertsimas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02149">
<title>Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths. (arXiv:2207.02149v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02149</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of sampling transition paths between two given
metastable states of a molecular system, e.g. a folded and unfolded protein or
products and reactants of a chemical reaction. Due to the existence of high
energy barriers separating the states, these transition paths are unlikely to
be sampled with standard Molecular Dynamics (MD) simulation. Traditional
methods to augment MD with a bias potential to increase the probability of the
transition rely on a dimensionality reduction step based on Collective
Variables (CVs). Unfortunately, selecting appropriate CVs requires chemical
intuition and traditional methods are therefore not always applicable to larger
systems. Additionally, when incorrect CVs are used, the bias potential might
not be minimal and bias the system along dimensions irrelevant to the
transition. Showing a formal relation between the problem of sampling molecular
transition paths, the Schr\&quot;odinger bridge problem and stochastic optimal
control with neural network policies, we propose a machine learning method for
sampling said transitions. Unlike previous non-machine learning approaches our
method, named PIPS, does not depend on CVs. We show that our method successful
generates low energy transitions for Alanine Dipeptide as well as the larger
Polyproline and Chignolin proteins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Holdijk_L/0/1/0/all/0/1&quot;&gt;Lars Holdijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuanqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hooft_F/0/1/0/all/0/1&quot;&gt;Ferry Hooft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jaini_P/0/1/0/all/0/1&quot;&gt;Priyank Jaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ensing_B/0/1/0/all/0/1&quot;&gt;Bernd Ensing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.09920">
<title>DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.09920</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal Inference has wide applications in various areas such as E-commerce
and precision medicine, and its performance heavily relies on the accurate
estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is
predicted by modeling the treated and control response functions separately in
their individual sample spaces. However, such an approach usually encounters
two issues in practice, i.e. divergent distribution between treated and control
groups due to treatment bias, and significant sample imbalance of their
population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN)
to model treatment effects from an end-to-end perspective. DESCN captures the
integrated information of the treatment propensity, the response, and the
hidden treatment effect through a cross network in a multi-task learning
manner. Our method jointly learns the treatment and response functions in the
entire sample space to avoid treatment bias and employs an intermediate pseudo
treatment effect prediction network to relieve sample imbalance. Extensive
experiments are conducted on a synthetic dataset and a large-scaled production
dataset from the E-commerce voucher distribution business. The results indicate
that DESCN can successfully enhance the accuracy of ITE estimation and improve
the uplift ranking performance. A sample of the production dataset and the
source code are released to facilitate future research in the community, which
is, to the best of our knowledge, the first large-scale public biased treatment
dataset for causal inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1&quot;&gt;Kailiang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1&quot;&gt;Fengtong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yaorong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenqing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cen_L/0/1/0/all/0/1&quot;&gt;Ling Cen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06868">
<title>Frouros: A Python library for drift detection in machine learning systems. (arXiv:2208.06868v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06868</link>
<description rdf:parseType="Literal">&lt;p&gt;Frouros is an open-source Python library capable of detecting drift in
machine learning systems. It provides a combination of classical and more
recent algorithms for drift detection: both concept and data drift. We have
designed it with the objective of making it compatible with any machine
learning framework and easily adaptable to real-world use cases. The library is
developed following a set of best development and continuous integration
practices to ensure ease of maintenance and extensibility. The source code is
available at https://github.com/IFCA/frouros.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cespedes_Sisniega_J/0/1/0/all/0/1&quot;&gt;Jaime C&amp;#xe9;spedes-Sisniega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Garcia_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro L&amp;#xf3;pez-Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01426">
<title>Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01426</link>
<description rdf:parseType="Literal">&lt;p&gt;In many complex sequential decision-making tasks, online planning is crucial
for high performance. For efficient online planning, Monte Carlo Tree Search
(MCTS) employs a principled mechanism for trading off exploration for
exploitation. MCTS outperforms comparison methods in many discrete
decision-making domains such as Go, Chess, and Shogi. Following, extensions of
MCTS to continuous domains have been proposed. However, the inherent high
branching factor and the resulting explosion of search tree size are limiting
existing methods. To address this problem, we propose Continuous Monte Carlo
Graph Search (CMCGS), a novel extension of MCTS to online planning in
environments with continuous state and action spaces. CMCGS takes advantage of
the insight that, during planning, sharing the same action policy between
several states can yield high performance. To implement this idea, at each time
step, CMCGS clusters similar states into a limited number of stochastic action
bandit nodes, which produce a layered directed graph instead of an MCTS search
tree. Experimental evaluation shows that CMCGS outperforms comparable planning
methods in several complex continuous DeepMind Control Suite benchmarks and a
2D navigation task with limited sample budgets. Furthermore, CMCGS can be
parallelized to scale up and it outperforms the Cross-Entropy Method (CEM) in
continuous control with learned dynamics models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujanpaa_K/0/1/0/all/0/1&quot;&gt;Kalle Kujanp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babadi_A/0/1/0/all/0/1&quot;&gt;Amin Babadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1&quot;&gt;Juho Kannala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1&quot;&gt;Alexander Ilin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12765">
<title>Multi-Objective GFlowNets. (arXiv:2210.12765v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12765</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of generating diverse candidates in the context of
Multi-Objective Optimization. In many applications of machine learning such as
drug discovery and material design, the goal is to generate candidates which
simultaneously optimize a set of potentially conflicting objectives. Moreover,
these objectives are often imperfect evaluations of some underlying property of
interest, making it important to generate diverse candidates to have multiple
options for expensive downstream evaluations. We propose Multi-Objective
GFlowNets (MOGFNs), a novel method for generating diverse Pareto optimal
solutions, based on GFlowNets. We introduce two variants of MOGFNs: MOGFN-PC,
which models a family of independent sub-problems defined by a scalarization
function, with reward-conditional GFlowNets, and MOGFN-AL, which solves a
sequence of sub-problems defined by an acquisition function in an active
learning loop. Our experiments on wide variety of synthetic and benchmark tasks
demonstrate advantages of the proposed methods in terms of the Pareto
performance and importantly, improved candidate diversity, which is the main
contribution of this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1&quot;&gt;Moksh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raparthy_S/0/1/0/all/0/1&quot;&gt;Sharath Chandra Raparthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1&quot;&gt;Alex Hernandez-Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rector_Brooks_J/0/1/0/all/0/1&quot;&gt;Jarrid Rector-Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miret_S/0/1/0/all/0/1&quot;&gt;Santiago Miret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_E/0/1/0/all/0/1&quot;&gt;Emmanuel Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13108">
<title>Heat Demand Forecasting with Multi-Resolutional Representation of Heterogeneous Temporal Ensemble. (arXiv:2210.13108v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13108</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the primal challenges faced by utility companies is ensuring efficient
supply with minimal greenhouse gas emissions. The advent of smart meters and
smart grids provide an unprecedented advantage in realizing an optimised supply
of thermal energies through proactive techniques such as load forecasting. In
this paper, we propose a forecasting framework for heat demand based on neural
networks where the time series are encoded as scalograms equipped with the
capacity of embedding exogenous variables such as weather, and
holiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load
multi-step ahead. Finally, the proposed framework is compared with other
state-of-the-art methods, such as SARIMAX and LSTM. The quantitative results
from retrospective experiments show that the proposed framework consistently
outperforms the state-of-the-art baseline method with real-world data acquired
from Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is
achieved with the proposed framework in comparison to all other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1&quot;&gt;Adithya Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Satyaki Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayer_S/0/1/0/all/0/1&quot;&gt;Siming Bayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flensmark_T/0/1/0/all/0/1&quot;&gt;Thorkil Flensmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16299">
<title>Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16299</link>
<description rdf:parseType="Literal">&lt;p&gt;A key challenge in solving the deterministic inverse reinforcement learning
(IRL) problem online and in real-time is the existence of multiple solutions.
Nonuniqueness necessitates the study of the notion of equivalent solutions,
i.e., solutions that result in a different cost functional but same feedback
matrix, and convergence to such solutions. While offline algorithms that result
in convergence to equivalent solutions have been developed in the literature,
online, real-time techniques that address nonuniqueness are not available. In
this paper, a regularized history stack observer that converges to
approximately equivalent solutions of the IRL problem is developed. Novel
data-richness conditions are developed to facilitate the analysis and
simulation results are provided to demonstrate the effectiveness of the
developed technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Town_J/0/1/0/all/0/1&quot;&gt;Jared Town&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morrison_Z/0/1/0/all/0/1&quot;&gt;Zachary Morrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1&quot;&gt;Rushikesh Kamalapurkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04965">
<title>Resource frugal optimizer for quantum machine learning. (arXiv:2211.04965v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum-enhanced data science, also known as quantum machine learning (QML),
is of growing interest as an application of near-term quantum computers.
Variational QML algorithms have the potential to solve practical problems on
real hardware, particularly when involving quantum data. However, training
these algorithms can be challenging and calls for tailored optimization
procedures. Specifically, QML applications can require a large shot-count
overhead due to the large datasets involved. In this work, we advocate for
simultaneous random sampling over both the dataset as well as the measurement
operators that define the loss function. We consider a highly general loss
function that encompasses many QML applications, and we show how to construct
an unbiased estimator of its gradient. This allows us to propose a shot-frugal
gradient descent optimizer called Refoqus (REsource Frugal Optimizer for
QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can
save several orders of magnitude in shot cost, even relative to optimizers that
sample over measurement operators alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Moussa_C/0/1/0/all/0/1&quot;&gt;Charles Moussa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1&quot;&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Baczyk_M/0/1/0/all/0/1&quot;&gt;Michal Baczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1&quot;&gt;M. Cerezo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1&quot;&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1&quot;&gt;Patrick J. Coles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16596">
<title>Towards Dynamic Causal Discovery with Rare Events: A Nonparametric Conditional Independence Test. (arXiv:2211.16596v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16596</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal phenomena associated with rare events occur across a wide range of
engineering problems, such as risk-sensitive safety analysis, accident analysis
and prevention, and extreme value theory. However, current methods for causal
discovery are often unable to uncover causal links, between random variables in
a dynamic setting, that manifest only when the variables first experience
low-probability realizations. To address this issue, we introduce a novel
statistical independence test on data collected from time-invariant dynamical
systems in which rare but consequential events occur. In particular, we exploit
the time-invariance of the underlying data to construct a superimposed dataset
of the system state before rare events happen at different timesteps. We then
design a conditional independence test on the reorganized data. We provide
non-asymptotic sample complexity bounds for the consistency of our method, and
validate its performance across various simulated and real-world datasets,
including incident data collected from the Caltrans Performance Measurement
System (PeMS). Code containing the datasets and experiments is publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Chih-Yuan Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kulkarni_K/0/1/0/all/0/1&quot;&gt;Kshitij Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sastry_S/0/1/0/all/0/1&quot;&gt;Shankar Sastry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03181">
<title>Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03181</link>
<description rdf:parseType="Literal">&lt;p&gt;Signal Temporal Logic (STL) is a powerful framework for describing the
complex temporal and logical behaviour of the dynamical system. Numerous
studies have attempted to employ reinforcement learning to learn a controller
that enforces STL specifications; however, they have been unable to effectively
tackle the challenges of ensuring robust satisfaction in continuous state space
and maintaining tractability. In this paper, leveraging the concept of funnel
functions, we propose a tractable reinforcement learning algorithm to learn a
time-dependent policy for robust satisfaction of STL specification in
continuous state space. We demonstrate the utility of our approach on several
STL tasks using different environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saxena_N/0/1/0/all/0/1&quot;&gt;Naman Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sandeep_G/0/1/0/all/0/1&quot;&gt;Gorantla Sandeep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jagtap_P/0/1/0/all/0/1&quot;&gt;Pushpak Jagtap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10426">
<title>Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10426</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art performance in electroencephalography (EEG) decoding tasks
is currently often achieved with either Deep-Learning (DL) or
Riemannian-Geometry-based decoders (RBDs). Recently, there is growing interest
in Deep Riemannian Networks (DRNs) possibly combining the advantages of both
previous classes of methods. However, there are still a range of topics where
additional insight is needed to pave the way for a more widespread application
of DRNs in EEG. These include architecture design questions such as network
size and end-to-end ability.How these factors affect model performance has not
been explored. Additionally, it is not clear how the data within these networks
is transformed, and whether this would correlate with traditional EEG decoding.
Our study aims to lay the groundwork in the area of these topics through the
analysis of DRNs for EEG with a wide range of hyperparameters. Networks were
tested on two public EEG datasets and compared with state-of-the-art ConvNets.
Here we propose end-to-end EEG SPDNet (EE(G)-SPDNet), and we show that this
wide, end-to-end DRN can outperform the ConvNets, and in doing so use
physiologically plausible frequency regions. We also show that the end-to-end
approach learns more complex filters than traditional band-pass filters
targeting the classical alpha, beta, and gamma frequency bands of the EEG, and
that performance can benefit from channel specific filtering approaches.
Additionally, architectural analysis revealed areas for further improvement due
to the possible loss of Riemannian specific information throughout the network.
Our study thus shows how to design and train DRNs to infer task-related
information from the raw EEG without the need of handcrafted filterbanks and
highlights the potential of end-to-end DRNs such as EE(G)-SPDNet for
high-performance EEG decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1&quot;&gt;Daniel Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1&quot;&gt;Robin Tibor Schirrmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemein_L/0/1/0/all/0/1&quot;&gt;Lukas Alexander Wilhelm Gemein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1&quot;&gt;Tonio Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13556">
<title>Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13556</link>
<description rdf:parseType="Literal">&lt;p&gt;To date, no &quot;information-theoretic&quot; frameworks for reasoning about
generalization error have been shown to establish minimax rates for gradient
descent in the setting of stochastic convex optimization. In this work, we
consider the prospect of establishing such rates via several existing
information-theoretic frameworks: input-output mutual information bounds,
conditional mutual information bounds and variants, PAC-Bayes bounds, and
recent conditional variants thereof. We prove that none of these bounds are
able to establish minimax rates. We then consider a common tactic employed in
studying gradient methods, whereby the final iterate is corrupted by Gaussian
noise, producing a noisy &quot;surrogate&quot; algorithm. We prove that minimax rates
cannot be established via the analysis of such surrogates. Our results suggest
that new ideas are required to analyze gradient descent using
information-theoretic techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghifam_M/0/1/0/all/0/1&quot;&gt;Mahdi Haghifam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Galvez_B/0/1/0/all/0/1&quot;&gt;Borja Rodr&amp;#xed;guez-G&amp;#xe1;lvez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thobaben_R/0/1/0/all/0/1&quot;&gt;Ragnar Thobaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1&quot;&gt;Mikael Skoglund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1&quot;&gt;Daniel M. Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1&quot;&gt;Gintare Karolina Dziugaite&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02877">
<title>Deep Learning for Mean Field Games with non-separable Hamiltonians. (arXiv:2301.02877v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new method based on Deep Galerkin Methods (DGMs) for
solving high-dimensional stochastic Mean Field Games (MFGs). We achieve this by
using two neural networks to approximate the unknown solutions of the MFG
system and forward-backward conditions. Our method is efficient, even with a
small number of iterations, and is capable of handling up to 300 dimensions
with a single layer, which makes it faster than other approaches. In contrast,
methods based on Generative Adversarial Networks (GANs) cannot solve MFGs with
non-separable Hamiltonians. We demonstrate the effectiveness of our approach by
applying it to a traffic flow problem, which was previously solved using the
Newton iteration method only in the deterministic case. We compare the results
of our method to analytical solutions and previous approaches, showing its
efficiency. We also prove the convergence of our neural network approximation
with a single hidden layer using the universal approximation theorem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assouli_M/0/1/0/all/0/1&quot;&gt;Mouhcine Assouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Missaoui_B/0/1/0/all/0/1&quot;&gt;Badr Missaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13816">
<title>Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13816</link>
<description rdf:parseType="Literal">&lt;p&gt;The utilization of programming language (PL) models, pre-trained on
large-scale code corpora, as a means of automating software engineering
processes has demonstrated considerable potential in streamlining various code
generation tasks such as code completion, code translation, and program
synthesis. However, current approaches mainly rely on supervised fine-tuning
objectives borrowed from text generation, neglecting unique sequence-level
characteristics of code, including but not limited to compilability as well as
syntactic and functional correctness. To address this limitation, we propose
PPOCoder, a new framework for code generation that synergistically combines
pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely
used deep reinforcement learning technique. By utilizing non-differentiable
feedback from code execution and structure alignment, PPOCoder seamlessly
integrates external code-specific knowledge into the model optimization
process. It&apos;s important to note that PPOCoder is a task-agnostic and
model-agnostic framework that can be used across different code generation
tasks and PLs. Extensive experiments on three code generation tasks demonstrate
the effectiveness of our proposed approach compared to SOTA methods, achieving
significant improvements in compilation success rates and functional
correctness across different PLs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shojaee_P/0/1/0/all/0/1&quot;&gt;Parshin Shojaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aneesh Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1&quot;&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00270">
<title>Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00270</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a class of reinforcement learning problems where the reward signals
for policy learning are generated by a discriminator that is dependent on and
jointly optimized with the policy. This interdependence between the policy and
the discriminator leads to an unstable learning process because reward signals
from an immature discriminator are noisy and impede policy learning, and
conversely, an under-optimized policy impedes discriminator learning. We call
this learning setting \textit{Internally Rewarded Reinforcement Learning}
(IRRL) as the reward is not provided directly by the environment but
\textit{internally} by the discriminator. In this paper, we formally formulate
IRRL and present a class of problems that belong to IRRL. We theoretically
derive and empirically analyze the effect of the reward function in IRRL and
based on these analyses propose the clipped linear reward function.
Experimental results show that the proposed reward function can consistently
stabilize the training process by reducing the impact of reward noise, which
leads to faster convergence and higher performance compared with baselines in
diverse tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengdi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xufeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae Hee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1&quot;&gt;Cornelius Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00422">
<title>Robust online active learning. (arXiv:2302.00422v6 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00422</link>
<description rdf:parseType="Literal">&lt;p&gt;In many industrial applications, obtaining labeled observations is not
straightforward as it often requires the intervention of human experts or the
use of expensive testing equipment. In these circumstances, active learning can
be highly beneficial in suggesting the most informative data points to be used
when fitting a model. Reducing the number of observations needed for model
development alleviates both the computational burden required for training and
the operational expenses related to labeling. Online active learning, in
particular, is useful in high-volume production processes where the decision
about the acquisition of the label for a data point needs to be taken within an
extremely short time frame. However, despite the recent efforts to develop
online active learning strategies, the behavior of these methods in the
presence of outliers has not been thoroughly examined. In this work, we
investigate the performance of online active linear regression in contaminated
data streams. Our study shows that the currently available query strategies are
prone to sample outliers, whose inclusion in the training set eventually
degrades the predictive performance of the models. To address this issue, we
propose a solution that bounds the search area of a conditional D-optimal
algorithm and uses a robust estimator. Our approach strikes a balance between
exploring unseen regions of the input space and protecting against outliers.
Through numerical simulations, we show that the proposed method is effective in
improving the performance of online active learning in the presence of
outliers, thus expanding the potential applications of this powerful tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cacciarelli_D/0/1/0/all/0/1&quot;&gt;Davide Cacciarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kulahci_M/0/1/0/all/0/1&quot;&gt;Murat Kulahci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tyssedal_J/0/1/0/all/0/1&quot;&gt;John S&amp;#xf8;lve Tyssedal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00999">
<title>High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance. (arXiv:2302.00999v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00999</link>
<description rdf:parseType="Literal">&lt;p&gt;During recent years the interest of optimization and machine learning
communities in high-probability convergence of stochastic optimization methods
has been growing. One of the main reasons for this is that high-probability
complexity bounds are more accurate and less studied than in-expectation ones.
However, SOTA high-probability non-asymptotic convergence results are derived
under strong assumptions such as the boundedness of the gradient noise variance
or of the objective&apos;s gradient itself. In this paper, we propose several
algorithms with high-probability convergence results under less restrictive
assumptions. In particular, we derive new high-probability convergence results
under the assumption that the gradient/operator noise has bounded central
$\alpha$-th moment for $\alpha \in (1,2]$ in the following setups: (i) smooth
non-convex / Polyak-Lojasiewicz / convex / strongly convex / quasi-strongly
convex minimization problems, (ii) Lipschitz / star-cocoercive and monotone /
quasi-strongly monotone variational inequalities. These results justify the
usage of the considered methods for solving problems that do not fit standard
functional classes studied in stochastic optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sadiev_A/0/1/0/all/0/1&quot;&gt;Abdurakhmon Sadiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Danilova_M/0/1/0/all/0/1&quot;&gt;Marina Danilova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gorbunov_E/0/1/0/all/0/1&quot;&gt;Eduard Gorbunov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Horvath_S/0/1/0/all/0/1&quot;&gt;Samuel Horv&amp;#xe1;th&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gidel_G/0/1/0/all/0/1&quot;&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1&quot;&gt;Pavel Dvurechensky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gasnikov_A/0/1/0/all/0/1&quot;&gt;Alexander Gasnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richtarik_P/0/1/0/all/0/1&quot;&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00046">
<title>Edit at your own risk: evaluating the robustness of edited models to distribution shifts. (arXiv:2303.00046v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00046</link>
<description rdf:parseType="Literal">&lt;p&gt;The current trend toward ever-larger models makes standard retraining
procedures an ever-more expensive burden. For this reason, there is growing
interest in model editing, which enables computationally inexpensive,
interpretable, post-hoc model modifications. While many model editing
techniques are promising, research on the properties of edited models is
largely limited to evaluation of validation accuracy. The robustness of edited
models is an important and yet mostly unexplored topic. In this paper, we
employ recently developed techniques from the field of deep learning robustness
to investigate both how model editing affects the general robustness of a
model, as well as the robustness of the specific behavior targeted by the edit.
We find that edits tend to reduce general robustness, but that the degree of
degradation depends on the editing algorithm and layers chosen. Motivated by
these observations we introduce a new model editing algorithm, 1-layer
interpolation (1-LI), which uses weight-space interpolation to navigate the
trade-off between editing task accuracy and general robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Davis Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1&quot;&gt;Charles Godfrey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nizinski_C/0/1/0/all/0/1&quot;&gt;Cody Nizinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1&quot;&gt;Jonathan Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1&quot;&gt;Henry Kvinge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05118">
<title>SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05118</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Guoliang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06289">
<title>Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06289</link>
<description rdf:parseType="Literal">&lt;p&gt;While the acquisition of time series has become more straightforward,
developing dynamical models from time series is still a challenging and
evolving problem domain. Within the last several years, to address this
problem, there has been a merging of machine learning tools with what is called
the dynamic mode decomposition (DMD). This general approach has been shown to
be an especially promising avenue for accurate model development. Building on
this prior body of work, we develop a deep learning DMD based method which
makes use of the fundamental insight of Takens&apos; Embedding Theorem to build an
adaptive learning scheme that better approximates higher dimensional and
chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We
likewise explore how our method learns mappings which tend, after successful
training, to significantly change the mutual information between dimensions in
the dynamics. This appears to be a key feature in enhancing the DMD overall,
and it should help provide further insight for developing other deep learning
methods for time series analysis and model generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1&quot;&gt;Christopher W. Curtis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1&quot;&gt;D. Jay Alford-Lago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bollt_E/0/1/0/all/0/1&quot;&gt;Erik Bollt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuma_A/0/1/0/all/0/1&quot;&gt;Andrew Tuma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06825">
<title>Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm. (arXiv:2303.06825v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06825</link>
<description rdf:parseType="Literal">&lt;p&gt;The linear bandit problem has been studied for many years in both stochastic
and adversarial settings. Designing an algorithm that can optimize the
environment without knowing the loss type attracts lots of interest.
\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and
then switches between different algorithms specially designed for specific
settings. However, such an approach requires meticulous designs to perform well
in all environments. Follow-the-regularized-leader (FTRL) is another type of
popular algorithm that can adapt to different environments. This algorithm is
of simple design and the regret bounds are shown to be optimal in traditional
multi-armed bandit problems compared with the detect-switch type. Designing an
FTRL-type algorithm for linear bandits is an important question that has been
open for a long time. In this paper, we prove that the FTRL algorithm with a
negative entropy regularizer can achieve the best-of-three-world results for
the linear bandit problem. Our regret bounds achieve the same or nearly the
same order as the previous detect-switch type algorithm but with a much simpler
algorithmic design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Canzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13024">
<title>Identifying TBI Physiological States by Clustering Multivariate Clinical Time-Series Data. (arXiv:2303.13024v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13024</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining clinically relevant physiological states from multivariate time
series data with missing values is essential for providing appropriate
treatment for acute conditions such as Traumatic Brain Injury (TBI),
respiratory failure, and heart failure. Utilizing non-temporal clustering or
data imputation and aggregation techniques may lead to loss of valuable
information and biased analyses. In our study, we apply the SLAC-Time
algorithm, an innovative self-supervision-based approach that maintains data
integrity by avoiding imputation or aggregation, offering a more useful
representation of acute patient states. By using SLAC-Time to cluster data in a
large research dataset, we identified three distinct TBI physiological states
and their specific feature profiles. We employed various clustering evaluation
metrics and incorporated input from a clinical domain expert to validate and
interpret the identified physiological states. Further, we discovered how
specific clinical events and interventions can influence patient states and
state transitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghaderi_H/0/1/0/all/0/1&quot;&gt;Hamid Ghaderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foreman_B/0/1/0/all/0/1&quot;&gt;Brandon Foreman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayebi_A/0/1/0/all/0/1&quot;&gt;Amin Nayebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tipirneni_S/0/1/0/all/0/1&quot;&gt;Sindhu Tipirneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1&quot;&gt;Chandan K. Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subbian_V/0/1/0/all/0/1&quot;&gt;Vignesh Subbian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02011">
<title>FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02011</link>
<description rdf:parseType="Literal">&lt;p&gt;Particle localization and -classification constitute two of the most
fundamental problems in computational microscopy. In recent years, deep
learning based approaches have been introduced for these tasks with great
success. A key shortcoming of these supervised learning methods is their need
for large training data sets, typically generated from particle models in
conjunction with complex numerical forward models simulating the physics of
transmission electron microscopes. Computer implementations of such forward
models are computationally extremely demanding and limit the scope of their
applicability. In this paper we propose a method for simulating the forward
operator of an electron microscope based on additive noise and Neural Style
Transfer techniques. We evaluate the method on localization and classification
tasks using one of the established state-of-the-art architectures showing
performance on par with the benchmark. In contrast to previous approaches, our
method accelerates the data generation process by a factor of 750 while using
33 times less memory and scales well to typical transmission electron
microscope detector sizes. It utilizes GPU acceleration and parallel
processing. It can be used to adapt a synthetic training data set according to
reference data from any transmission electron microscope. The source code is
available at https://gitlab.com/deepet/faket.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harar_P/0/1/0/all/0/1&quot;&gt;Pavol Harar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_L/0/1/0/all/0/1&quot;&gt;Lukas Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grohs_P/0/1/0/all/0/1&quot;&gt;Philipp Grohs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haselbach_D/0/1/0/all/0/1&quot;&gt;David Haselbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02689">
<title>ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02689</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical data often exhibits long-tail distributions with heavy class
imbalance, which naturally leads to difficulty in classifying the minority
classes (i.e., boundary regions or rare objects). Recent work has significantly
improved semi-supervised medical image segmentation in long-tailed scenarios by
equipping them with unsupervised contrastive criteria. However, it remains
unclear how well they will perform in the labeled portion of data where class
distribution is also highly imbalanced. In this work, we present ACTION++, an
improved contrastive learning framework with adaptive anatomical contrast for
semi-supervised medical segmentation. Specifically, we propose an adaptive
supervised contrastive loss, where we first compute the optimal locations of
class centers uniformly distributed on the embedding space (i.e., off-line),
and then perform online contrastive matching training by encouraging different
class features to adaptively match these distinct and uniformly distributed
class centers. Moreover, we argue that blindly adopting a constant temperature
$\tau$ in the contrastive loss on long-tailed medical data is not optimal, and
propose to use a dynamic $\tau$ via a simple cosine schedule to yield better
separation between majority and minority classes. Empirically, we evaluate
ACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-art
across two semi-supervised settings. Theoretically, we analyze the performance
of adaptive anatomical contrast and confirm its superiority in label
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weicheng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1&quot;&gt;Yifei Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1&quot;&gt;Lawrence Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekhon_J/0/1/0/all/0/1&quot;&gt;Jasjeet S. Sekhon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03209">
<title>Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03209</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating high-level semantically correlated contents and low-level
anatomical features is of central importance in medical image segmentation.
Towards this end, recent deep learning-based medical segmentation methods have
shown great promise in better modeling such information. However, convolution
operators for medical segmentation typically operate on regular grids, which
inherently blur the high-frequency regions, i.e., boundary regions. In this
work, we propose MORSE, a generic implicit neural rendering framework designed
at an anatomical level to assist learning in medical image segmentation. Our
method is motivated by the fact that implicit neural representation has been
shown to be more effective in fitting complex signals and solving computer
graphics problems than discrete grid-based representation. The core of our
approach is to formulate medical image segmentation as a rendering problem in
an end-to-end manner. Specifically, we continuously align the coarse
segmentation prediction with the ambiguous coordinate-based point
representations and aggregate these features to adaptively refine the boundary
region. To parallelly optimize multi-scale pixel-level features, we leverage
the idea from Mixture-of-Expert (MoE) to design and train our MORSE with a
stochastic gating mechanism. Our experiments demonstrate that MORSE can work
well with different medical segmentation backbones, consistently achieving
competitive performance improvements in both 2D and 3D supervised medical
segmentation methods. We also theoretically analyze the superiority of MORSE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1&quot;&gt;Chenyu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Weicheng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1&quot;&gt;Yifei Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1&quot;&gt;Lawrence Staib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12906">
<title>The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12906</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit generative modeling (IGM) aims to produce samples of synthetic data
matching the characteristics of a target data distribution. Recent work (e.g.
score-matching networks, diffusion models) has approached the IGM problem from
the perspective of pushing synthetic source data toward the target distribution
via dynamical perturbations or flows in the ambient space. In this direction,
we present the score difference (SD) between arbitrary target and source
distributions as a flow that optimally reduces the Kullback-Leibler divergence
between them while also solving the Schroedinger bridge problem. We apply the
SD flow to convenient proxy distributions, which are aligned if and only if the
original distributions are aligned. We demonstrate the formal equivalence of
this formulation to denoising diffusion models under certain conditions. We
also show that the training of generative adversarial networks includes a
hidden data-optimization sub-problem, which induces the SD flow under certain
choices of loss function when the discriminator is optimal. As a result, the SD
flow provides a theoretical link between model classes that individually
address the three challenges of the &quot;generative modeling trilemma&quot; -- high
sample quality, mode coverage, and fast sampling -- thereby setting the stage
for a unified approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_R/0/1/0/all/0/1&quot;&gt;Romann M. Weber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03829">
<title>Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03829</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-based precision medicine aims to personalize treatment decisions based
on an individual&apos;s unique imaging features so as to improve their clinical
outcome. Machine learning frameworks that integrate uncertainty estimation as
part of their treatment recommendations would be safer and more reliable.
However, little work has been done in adapting uncertainty estimation
techniques and validation metrics for precision medicine. In this paper, we use
Bayesian deep learning for estimating the posterior distribution over factual
and counterfactual outcomes on several treatments. This allows for estimating
the uncertainty for each treatment option and for the individual treatment
effects (ITE) between any two treatments. We train and evaluate this model to
predict future new and enlarging T2 lesion counts on a large, multi-center
dataset of MR brain images of patients with multiple sclerosis, exposed to
several treatments during randomized controlled trials. We evaluate the
correlation of the uncertainty estimate with the factual error, and, given the
lack of ground truth counterfactual outcomes, demonstrate how uncertainty for
the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate
how knowledge of uncertainty could modify clinical decision-making to improve
individual patient and clinical trial outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durso_Finley_J/0/1/0/all/0/1&quot;&gt;Joshua Durso-Finley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falet_J/0/1/0/all/0/1&quot;&gt;Jean-Pierre Falet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1&quot;&gt;Raghav Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnold_D/0/1/0/all/0/1&quot;&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1&quot;&gt;Nick Pawlowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1&quot;&gt;Tal Arbel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04422">
<title>Performance Gaps of Artificial Intelligence Models Screening Mammography -- Towards Fair and Interpretable Models. (arXiv:2305.04422v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04422</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though deep learning models for abnormality classification can perform
well in screening mammography, the demographic and imaging characteristics
associated with increased risk of failure for abnormality classification in
screening mammograms remain unclear. This retrospective study used data from
the Emory BrEast Imaging Dataset (EMBED) including mammograms from 115,931
patients imaged at Emory University Healthcare between 2013 to 2020. Clinical
and imaging data includes Breast Imaging Reporting and Data System (BI-RADS)
assessment, region of interest coordinates for abnormalities, imaging features,
pathologic outcomes, and patient demographics. Deep learning models including
InceptionV3, VGG16, ResNet50V2, and ResNet152V2 were developed to distinguish
between patches of abnormal tissue and randomly selected patches of normal
tissue from the screening mammograms. The distributions of the training,
validation and test sets are 29,144 (55.6%) patches of 10,678 (54.2%) patients,
9,910 (18.9%) patches of 3,609 (18.3%) patients, and 13,390 (25.5%) patches of
5,404 (27.5%) patients. We assessed model performance overall and within
subgroups defined by age, race, pathologic outcome, and imaging characteristics
to evaluate reasons for misclassifications. On the test set, a ResNet152V2
model trained to classify normal versus abnormal tissue patches achieved an
accuracy of 92.6% (95%CI=92.0-93.2%), and area under the receiver operative
characteristics curve 0.975 (95%CI=0.972-0.978). Imaging characteristics
associated with higher misclassifications of images include higher tissue
densities (risk ratio [RR]=1.649; p=.010, BI-RADS density C and RR=2.026;
p=.003, BI-RADS density D), and presence of architectural distortion (RR=1.026;
p&amp;lt;.001). Small but statistically significant differences in performance were
observed by age, race, pathologic outcome, and other imaging features (p&amp;lt;.001).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linglin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brown_Mulry_B/0/1/0/all/0/1&quot;&gt;Beatrice Brown-Mulry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nalla_V/0/1/0/all/0/1&quot;&gt;Vineela Nalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hwang_I/0/1/0/all/0/1&quot;&gt;InChan Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gastounioti_A/0/1/0/all/0/1&quot;&gt;Aimilia Gastounioti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1&quot;&gt;Imon Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1&quot;&gt;Laleh Seyyed-Kalantari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Woo_M/0/1/0/all/0/1&quot;&gt;MinJae Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trivedi_H/0/1/0/all/0/1&quot;&gt;Hari Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05097">
<title>Self-Repellent Random Walks on General Graphs -- Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v2 [math.PR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05097</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider random walks on discrete state spaces, such as general undirected
graphs, where the random walkers are designed to approximate a target quantity
over the network topology via sampling and neighborhood exploration in the form
of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain
corresponding to a target probability distribution, we design a self-repellent
random walk (SRRW) which is less likely to transition to nodes that were highly
visited in the past, and more likely to transition to seldom visited nodes. For
a class of SRRWs parameterized by a positive real {\alpha}, we prove that the
empirical distribution of the process converges almost surely to the the target
(stationary) distribution of the underlying Markov chain kernel. We then
provide a central limit theorem and derive the exact form of the arising
asymptotic co-variance matrix, which allows us to show that the SRRW with a
stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
covariance, in the sense of Loewner ordering of co-variance matrices.
Especially for SRRW-driven MCMC algorithms, we show that the decrease in the
asymptotic sampling variance is of the order O(1/{\alpha}), eventually going
down to zero. Finally, we provide numerical simulations complimentary to our
theoretical results, also empirically testing a version of SRRW with {\alpha}
increasing in time to combine the benefits of smaller asymptotic variance due
to large {\alpha}, with empirically observed faster mixing properties of SRRW
with smaller {\alpha}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doshi_V/0/1/0/all/0/1&quot;&gt;Vishwaraj Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Eun_D/0/1/0/all/0/1&quot;&gt;Do Young Eun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07617">
<title>Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07617</link>
<description rdf:parseType="Literal">&lt;p&gt;In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs. In this
paper, we introduce a scalable neural architecture and loss function dedicated
to learning the constraints and criteria of NP-hard reasoning problems
expressed as discrete Graphical Models. Our loss function solves one of the
main limitations of Besag&apos;s pseudo-loglikelihood, enabling learning of high
energies. We empirically show it is able to efficiently learn how to solve
NP-hard reasoning problems from natural inputs as the symbolic, visual or
many-solutions Sudoku problems as well as the energy optimization formulation
of the protein design problem, providing data efficiency, interpretability, and
\textit{a posteriori} control over predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defresne_M/0/1/0/all/0/1&quot;&gt;Marianne Defresne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbe_S/0/1/0/all/0/1&quot;&gt;Sophie Barbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiex_T/0/1/0/all/0/1&quot;&gt;Thomas Schiex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07848">
<title>Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07848</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, polyp segmentation has gained significant importance, and
many methods have been developed using CNN, Vision Transformer, and Transformer
techniques to achieve competitive results. However, these methods often face
difficulties when dealing with out-of-distribution datasets, missing
boundaries, and small polyps. In 2022, Meta-Former was introduced as a new
baseline for vision, which not only improved the performance of multi-task
computer vision but also addressed the limitations of the Vision Transformer
and CNN family backbones. To further enhance segmentation, we propose a fusion
of Meta-Former with UNet, along with the introduction of a Multi-scale
Upsampling block with a level-up combination in the decoder stage to enhance
the texture, also we propose the Convformer block base on the idea of the
Meta-former to enhance the crucial information of the local feature. These
blocks enable the combination of global information, such as the overall shape
of the polyp, with local information and boundary information, which is crucial
for the decision of the medical segmentation. Our proposed approach achieved
competitive performance and obtained the top result in the State of the Art on
the CVC-300 dataset, Kvasir, and CVC-ColonDB dataset. Apart from Kvasir-SEG,
others are out-of-distribution datasets. The implementation can be found at:
https://github.com/huyquoctrinh/MetaPolyp-CBMS2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Trinh_Q/0/1/0/all/0/1&quot;&gt;Quoc-Huy Trinh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09211">
<title>CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09211</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers, due to their ability to learn long range dependencies, have
overcome the shortcomings of convolutional neural networks (CNNs) for global
perspective learning. Therefore, they have gained the focus of researchers for
several vision related tasks including medical diagnosis. However, their
multi-head attention module only captures global level feature representations,
which is insufficient for medical images. To address this issue, we propose a
Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning
to generate boosted channels and employs both transformers and CNNs to analyse
lymphocytes in histopathological images. The proposed CB HVT comprises five
modules, including a channel generation module, channel exploitation module,
channel merging module, region-aware module, and a detection and segmentation
head, which work together to effectively identify lymphocytes. The channel
generation module uses the idea of channel boosting through transfer learning
to extract diverse channels from different auxiliary learners. In the CB HVT,
these boosted channels are first concatenated and ranked using an attention
mechanism in the channel exploitation module. A fusion block is then utilized
in the channel merging module for a gradual and systematic merging of the
diverse boosted channels to improve the network&apos;s learning representations. The
CB HVT also employs a proposal network in its region aware module and a head to
effectively identify objects, even in overlapping regions and with artifacts.
We evaluated the proposed CB HVT on two publicly available datasets for
lymphocyte assessment in histopathological images. The results show that CB HVT
outperformed other state of the art detection models, and has good
generalization ability, demonstrating its value as a tool for pathologists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Momina Liaqat Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1&quot;&gt;Zunaira Rauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1&quot;&gt;Anabia Sohail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ullah_R/0/1/0/all/0/1&quot;&gt;Rafi Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1&quot;&gt;Jeonghwan Gwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11997">
<title>Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11997</link>
<description rdf:parseType="Literal">&lt;p&gt;There is an emerging interest in generating robust counterfactual
explanations that would remain valid if the model is updated or changed even
slightly. Towards finding robust counterfactuals, existing literature often
assumes that the original model $m$ and the new model $M$ are bounded in the
parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&amp;lt;}\Delta$.
However, models can often change significantly in the parameter space with
little to no change in their predictions or accuracy on the given dataset. In
this work, we introduce a mathematical abstraction termed
\emph{naturally-occurring} model change, which allows for arbitrary changes in
the parameter space such that the change in predictions on points that lie on
the data manifold is limited. Next, we propose a measure -- that we call
\emph{Stability} -- to quantify the robustness of counterfactuals to potential
model changes for differentiable models, e.g., neural networks. Our main
contribution is to show that counterfactuals with sufficiently high value of
\emph{Stability} as defined by our measure will remain valid after potential
``naturally-occurring&apos;&apos; model changes with high probability (leveraging
concentration bounds for Lipschitz function of independent Gaussians). Since
our quantification depends on the local Lipschitz constant around a data point
which is not always available, we also examine practical relaxations of our
proposed measure and demonstrate experimentally how they can be incorporated to
find robust counterfactuals for neural networks that are close, realistic, and
remain valid after potential model changes. This work also has interesting
connections with model multiplicity, also known as, the Rashomon effect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hamman_F/0/1/0/all/0/1&quot;&gt;Faisal Hamman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Noorani_E/0/1/0/all/0/1&quot;&gt;Erfaun Noorani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Saumitra Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Magazzeni_D/0/1/0/all/0/1&quot;&gt;Daniele Magazzeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sanghamitra Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13115">
<title>Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13115</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the great potential of attention mechanism in
graph representation learning. However, while variants of attention-based GNNs
are setting new benchmarks for numerous real-world datasets, recent works have
pointed out that their induced attentions are less robust and generalizable
against noisy graphs due to lack of direct supervision. In this paper, we
present a new framework which utilizes the tool of causality to provide a
powerful supervision signal for the learning process of attention functions.
Specifically, we estimate the direct causal effect of attention to the final
prediction, and then maximize such effect to guide attention attending to more
meaningful neighbors. Our method can serve as a plug-and-play module for any
canonical attention-based GNNs in an end-to-end fashion. Extensive experiments
on a wide range of benchmark datasets illustrated that, by directly supervising
attention functions, the model is able to converge faster with a clearer
decision boundary, and thus yields better performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Lun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13399">
<title>Efficient Large-Scale Visual Representation Learning And Evaluation. (arXiv:2305.13399v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we present our approach to single-modality visual
representation learning. Understanding visual representations of items is vital
for fashion recommendations in e-commerce. We detail and contrast techniques
used to finetune large-scale visual representation learning models in an
efficient manner under low-resource settings, including several pretrained
backbone architectures, both in the convolutional neural network as well as the
vision transformer family. We describe the challenges for e-commerce
applications at-scale and highlight the efforts to more efficiently train,
evaluate, and serve visual representations. We present ablation studies
evaluating the representation offline performance for several downstream tasks,
including visually similar ad recommendations on mobile devices. To this end,
we present a novel multilingual text-to-image generative offline evaluation
method for visually similar recommendation systems. Finally, we include online
results from deployed machine learning systems in production at Etsy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1&quot;&gt;Eden Dolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1&quot;&gt;Alaa Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1&quot;&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1&quot;&gt;Marcin Mejran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1&quot;&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mahir Yavuz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16044">
<title>Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v5 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16044</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks of spiking neurons underpin the extraordinary information-processing
capabilities of the brain and have become pillar models in neuromorphic
artificial intelligence. Despite extensive research on spiking neural networks
(SNNs), most studies are established on deterministic models, overlooking the
inherent non-deterministic, noisy nature of neural computations. This study
introduces the noisy spiking neural network (NSNN) and the noise-driven
learning rule (NDL) by incorporating noisy neuronal dynamics to exploit the
computational advantages of noisy neural processing. NSNN provides a
theoretical framework that yields scalable, flexible, and reliable computation.
We demonstrate that NSNN leads to spiking neural models with competitive
performance, improved robustness against challenging perturbations than
deterministic SNNs, and better reproducing probabilistic neural computation in
neural coding. This study offers a powerful and easy-to-use tool for machine
learning, neuromorphic intelligence practitioners, and computational
neuroscience researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gehua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huajin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16189">
<title>Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16189</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised source separation involves unraveling an unknown set of source
signals recorded through a mixing operator, with limited prior knowledge about
the sources, and only access to a dataset of signal mixtures. This problem is
inherently ill-posed and is further challenged by the variety of time-scales
exhibited by sources in time series data. Existing methods typically rely on a
preselected window size that limits their capacity to handle multi-scale
sources. To address this issue, instead of operating in the time domain, we
propose an unsupervised multi-scale clustering and source separation framework
by leveraging wavelet scattering covariances that provide a low-dimensional
representation of stochastic processes, capable of distinguishing between
different non-Gaussian stochastic processes. Nested within this representation
space, we develop a factorial Gaussian-mixture variational autoencoder that is
trained to (1) probabilistically cluster sources at different time-scales and
(2) independently sample scattering covariance representations associated with
each cluster. Using samples from each cluster as prior information, we
formulate source separation as an optimization problem in the wavelet
scattering covariance representation space, resulting in separated sources in
the time domain. When applied to seismic data recorded during the NASA InSight
mission on Mars, our multi-scale nested approach proves to be a powerful tool
for discriminating between sources varying greatly in time-scale, e.g.,
minute-long transient one-sided pulses (known as ``glitches&apos;&apos;) and structured
ambient noises resulting from atmospheric activities that typically last for
tens of minutes. These results provide an opportunity to conduct further
investigations into the isolated sources related to atmospheric-surface
interactions, thermal relaxations, and other complex phenomena.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siahkoohi_A/0/1/0/all/0/1&quot;&gt;Ali Siahkoohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morel_R/0/1/0/all/0/1&quot;&gt;Rudy Morel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allys_E/0/1/0/all/0/1&quot;&gt;Erwan Allys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sainton_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;gory Sainton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamura_T/0/1/0/all/0/1&quot;&gt;Taichi Kawamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoop_M/0/1/0/all/0/1&quot;&gt;Maarten V. de Hoop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16562">
<title>Unsupervised Embedding Quality Evaluation. (arXiv:2305.16562v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16562</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning has recently significantly gained in popularity,
especially with deep learning-based approaches. Despite numerous successes and
approaching supervised-level performance on a variety of academic benchmarks,
it is still hard to train and evaluate SSL models in practice due to the
unsupervised nature of the problem. Even with networks trained in a supervised
fashion, it is often unclear whether they will perform well when transferred to
another domain.
&lt;/p&gt;
&lt;p&gt;Past works are generally limited to assessing the amount of information
contained in embeddings, which is most relevant for self-supervised learning of
deep neural networks. This works chooses to follow a different approach: can we
quantify how easy it is to linearly separate the data in a stable way? We
survey the literature and uncover three methods that could be potentially used
for evaluating quality of representations. We also introduce one novel method
based on recent advances in understanding the high-dimensional geometric
structure of self-supervised learning.
&lt;/p&gt;
&lt;p&gt;We conduct extensive experiments and study the properties of these metrics
and ones introduced in the previous work. Our results suggest that while there
is no free lunch, there are metrics that can robustly estimate embedding
quality in an unsupervised way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1&quot;&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munkhoeva_M/0/1/0/all/0/1&quot;&gt;Marina Munkhoeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1&quot;&gt;Bryan Perozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05439">
<title>Contrastive Representation Disentanglement for Clustering. (arXiv:2306.05439v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05439</link>
<description rdf:parseType="Literal">&lt;p&gt;Clustering continues to be a significant and challenging task. Recent studies
have demonstrated impressive results by applying clustering to feature
representations acquired through self-supervised learning, particularly on
small datasets. However, when dealing with datasets containing a large number
of clusters, such as ImageNet, current methods struggle to achieve satisfactory
clustering performance. In this paper, we introduce a novel method called
Contrastive representation Disentanglement for Clustering (CDC) that leverages
contrastive learning to directly disentangle the feature representation for
clustering. In CDC, we decompose the representation into two distinct
components: one component encodes categorical information under an
equipartition constraint, and the other component captures instance-specific
factors. To train our model, we propose a contrastive loss that effectively
utilizes both components of the representation. We conduct a theoretical
analysis of the proposed loss and highlight how it assigns different weights to
negative samples during the process of disentangling the feature
representation. Further analysis of the gradients reveals that larger weights
emphasize a stronger focus on hard negative samples. As a result, the proposed
loss exhibits strong expressiveness, enabling efficient disentanglement of
categorical information. Through experimental evaluation on various benchmark
datasets, our method demonstrates either state-of-the-art or highly competitive
clustering performance. Notably, on the complete ImageNet dataset, we achieve
an accuracy of 53.4%, surpassing existing methods by a substantial margin of
+10.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1&quot;&gt;Venkat Krovi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Feng Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06534">
<title>K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06534</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel self-consistency clustering algorithm
($K$-Tensors) designed for {partitioning a distribution of}
positive-semidefinite matrices based on their eigenstructures. As positive
semi-definite matrices can be represented as ellipsoids in $\mathbb R^p$, $p
\ge 2$, it is critical to maintain their structural information to perform
effective clustering. However, traditional clustering algorithms {applied to
matrices} often {involve vectorization of} the matrices, resulting in a loss of
essential structural information. To address this issue, we propose a distance
metric {for clustering} that is specifically based on the structural
information of positive semi-definite matrices. This distance metric enables
the clustering algorithm to consider the differences between positive
semi-definite matrices and their projections onto {a} common space spanned by
\thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite
matrices. This innovative approach to clustering positive semi-definite
matrices has broad applications in several domains including financial and
biomedical research, such as analyzing functional connectivity data. By
maintaining the structural information of positive semi-definite matrices, our
proposed algorithm promises to cluster the positive semi-definite matrices in a
more meaningful way, thereby facilitating deeper insights into the underlying
data in various applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarpey_T/0/1/0/all/0/1&quot;&gt;Thaddeus Tarpey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06849">
<title>Mitigating Transformer Overconfidence via Lipschitz Regularization. (arXiv:2306.06849v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06849</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Transformers have achieved promising results in many computer vision
tasks, they tend to be over-confident in predictions, as the standard Dot
Product Self-Attention (DPSA) can barely preserve distance for the unbounded
input domain. In this work, we fill this gap by proposing a novel Lipschitz
Regularized Transformer (LRFormer). Specifically, we present a new similarity
function with the distance within Banach Space to ensure the Lipschitzness and
also regularize the term by a contractive Lipschitz Bound. The proposed method
is analyzed with a theoretical guarantee, providing a rigorous basis for its
effectiveness and reliability. Extensive experiments conducted on standard
vision benchmarks demonstrate that our method outperforms the state-of-the-art
single forward pass approaches in prediction, calibration, and uncertainty
estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenqian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kun Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07528">
<title>Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective. (arXiv:2306.07528v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07528</link>
<description rdf:parseType="Literal">&lt;p&gt;Off-policy Learning to Rank (LTR) aims to optimize a ranker from data
collected by a deployed logging policy. However, existing off-policy learning
to rank methods often make strong assumptions about how users generate the
click data, i.e., the click model, and hence need to tailor their methods
specifically under different click models. In this paper, we unified the
ranking process under general stochastic click models as a Markov Decision
Process (MDP), and the optimal ranking could be learned with offline
reinforcement learning (RL) directly. Building upon this, we leverage offline
RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified
Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a
wide range of click models. Through a dedicated formulation of the MDP, we show
that offline RL algorithms can adapt to various click models without complex
debiasing techniques and prior knowledge of the model. Results on various
large-scale datasets demonstrate that CUOLR consistently outperforms the
state-of-the-art off-policy learning to rank algorithms while maintaining
consistency and robustness under different click models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yiran Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_R/0/1/0/all/0/1&quot;&gt;Rishab Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huazheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09662">
<title>Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09662</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing traffic signal control systems rely on oversimplified rule-based
methods, and even RL-based methods are often suboptimal and unstable. To
address this, we propose a cooperative multi-objective architecture called
Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG),
which estimates multiple reward terms for traffic signal control optimization
using age-decaying weights. Our approach involves two types of agents: one
focuses on optimizing local traffic at each intersection, while the other aims
to optimize global traffic throughput. We evaluate our method using real-world
traffic data collected from an Asian country&apos;s traffic cameras. Despite the
inclusion of a global agent, our solution remains decentralized as this agent
is no longer necessary during the inference stage. Our results demonstrate the
effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all
performance metrics. Additionally, our proposed system minimizes both waiting
time and carbon emissions. Notably, this paper is the first to link carbon
emissions and global agents in traffic signal control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Cheng Ruei Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1&quot;&gt;Jun Wei Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1&quot;&gt;Shin You Teng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15548">
<title>Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15548</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for
non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound
Localization Microscopy (ULM) has enabled a revolutionary breakthrough by
offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers
are used to render ULM frames, ultimately determining the image resolution
capability. To take full advantage of ULM, this study questions whether
beamforming is the most effective processing step for ULM, suggesting an
alternative approach that relies solely on Time-Difference-of-Arrival (TDoA)
information. To this end, a novel geometric framework for micro bubble
localization via ellipse intersections is proposed to overcome existing
beamforming limitations. We present a benchmark comparison based on a public
dataset for which our geometric ULM outperforms existing baseline methods in
terms of accuracy and robustness while only utilizing a portion of the
available transducer data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahne_C/0/1/0/all/0/1&quot;&gt;Christopher Hahne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1&quot;&gt;Raphael Sznitman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15656">
<title>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design. (arXiv:2306.15656v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15656</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer&apos;s robustness and efficacy. Crucially, SparseOptimizer&apos;s plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be publicly
available upon paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fu-Ming Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15764">
<title>High Fidelity Image Counterfactuals with Probabilistic Causal Models. (arXiv:2306.15764v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15764</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a general causal generative modelling framework for accurate
estimation of high fidelity image counterfactuals with deep structural causal
models. Estimation of interventional and counterfactual queries for
high-dimensional structured variables, such as images, remains a challenging
task. We leverage ideas from causal mediation analysis and advances in
generative modelling to design new deep causal mechanisms for structured
variables in causal models. Our experiments demonstrate that our proposed
mechanisms are capable of accurate abduction and estimation of direct, indirect
and total effects as measured by axiomatic soundness of counterfactuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tian Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1&quot;&gt;Miguel Monteiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1&quot;&gt;Nick Pawlowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16844">
<title>Macro Placement by Wire-Mask-Guided Black-Box Optimization. (arXiv:2306.16844v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16844</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of very large-scale integration (VLSI) technology has posed
new challenges for electronic design automation (EDA) techniques in chip
floorplanning. During this process, macro placement is an important subproblem,
which tries to determine the positions of all macros with the aim of minimizing
half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods
include packing-based, analytical and reinforcement learning methods. In this
paper, we propose a new black-box optimization (BBO) framework (called
WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure
for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO
empirically achieves significant improvements over previous methods, i.e.,
achieves significantly shorter HPWL by using much less time. Furthermore, it
can fine-tune existing placements by treating them as initial solutions, which
can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to
significantly improve the quality and efficiency of chip floorplanning, which
makes it appealing to researchers and practitioners in EDA and will also
promote the application of BBO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yunqi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1&quot;&gt;Ke Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Lei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00497">
<title>Don&apos;t Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00497</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are prone to forgetting information learned in the past
when trained on new data. This problem becomes even more pronounced in the
context of federated learning (FL), where data is decentralized and subject to
independent changes for each user. Continual Learning (CL) studies this
so-called \textit{catastrophic forgetting} phenomenon primarily in centralized
settings, where the learner has direct access to the complete training dataset.
However, applying CL techniques to FL is not straightforward due to privacy
concerns and resource limitations. This paper presents a framework for
federated class incremental learning that utilizes a generative model to
synthesize samples from past distributions instead of storing part of past
data. Then, clients can leverage the generative model to mitigate catastrophic
forgetting locally. The generative model is trained on the server using
data-free methods at the end of each task without requesting data from clients.
Therefore, it reduces the risk of data leakage as opposed to training it on the
client&apos;s private data. We demonstrate significant improvements for the
CIFAR-100 dataset compared to existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babakniya_S/0/1/0/all/0/1&quot;&gt;Sara Babakniya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1&quot;&gt;Zalan Fabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chaoyang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1&quot;&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1&quot;&gt;Salman Avestimehr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04550">
<title>Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04550</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent regulation on right-to-be-forgotten emerges tons of interest in
unlearning pre-trained machine learning models. While approximating a
straightforward yet expensive approach of retrain-from-scratch, recent machine
unlearning methods unlearn a sample by updating weights to remove its influence
on the weight parameters. In this paper, we introduce a simple yet effective
approach to remove a data influence on the deep generative model. Inspired by
works in multi-task learning, we propose to manipulate gradients to regularize
the interplay of influence among samples by projecting gradients onto the
normal plane of the gradients to be retained. Our work is agnostic to
statistics of the removal samples, outperforming existing baselines while
providing theoretical analysis for the first time in unlearning a generative
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seohui Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seoyoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1&quot;&gt;Hyemin Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_W/0/1/0/all/0/1&quot;&gt;Woohyung Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04778">
<title>Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case. (arXiv:2307.04778v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04778</link>
<description rdf:parseType="Literal">&lt;p&gt;Business statistics play a crucial role in implementing a data-driven
strategic plan at the enterprise level to employ various analytics where the
outcomes of such a plan enable an enterprise to enhance the decision-making
process or to mitigate risks to the organization. In this work, a strategic
plan informed by the statistical analysis is introduced for a financial company
called LendingClub, where the plan is comprised of exploring the possibility of
onboarding a big data platform along with advanced feature selection
capacities. The main objectives of such a plan are to increase the company&apos;s
revenue while reducing the risks of granting loans to borrowers who cannot
return their loans. In this study, different hypotheses formulated to address
the company&apos;s concerns are studied, where the results reveal that the amount of
loans profoundly impacts the number of borrowers charging off their loans.
Also, the proposed strategic plan includes onboarding advanced analytics such
as machine learning technologies that allow the company to build better
generalized data-driven predictive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarraf_S/0/1/0/all/0/1&quot;&gt;Saman Sarraf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04964">
<title>Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04964</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes, aiming to make modest
contributions to the advancement of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yuan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binghai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Senjie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Limao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1&quot;&gt;Wenbin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1&quot;&gt;Rongxiang Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wensen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05520">
<title>Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05520</link>
<description rdf:parseType="Literal">&lt;p&gt;Current research in the computer vision field mainly focuses on improving
Deep Learning (DL) correctness and inference time performance. However, there
is still little work on the huge carbon footprint that has training DL models.
This study aims to analyze the impact of the model architecture and training
environment when training greener computer vision models. We divide this goal
into two research questions. First, we analyze the effects of model
architecture on achieving greener models while keeping correctness at optimal
levels. Second, we study the influence of the training environment on producing
greener models. To investigate these relationships, we collect multiple metrics
related to energy efficiency and model correctness during the models&apos; training.
Then, we outline the trade-offs between the measured energy efficiency and the
models&apos; correctness regarding model architecture, and their relationship with
the training environment. We conduct this research in the context of a computer
vision system for image classification. In conclusion, we show that selecting
the proper model architecture and training environment can reduce energy
consumption dramatically (up to 98.83%) at the cost of negligible decreases in
correctness. Also, we find evidence that GPUs should scale with the models&apos;
computational complexity for better energy efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rey_S/0/1/0/all/0/1&quot;&gt;Santiago del Rey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1&quot;&gt;Silverio Mart&amp;#xed;nez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1&quot;&gt;Xavier Franch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06887">
<title>Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06887</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature learning, i.e. extracting meaningful representations of data, is
quintessential to the practical success of neural networks trained with
gradient descent, yet it is notoriously difficult to explain how and why it
occurs. Recent theoretical studies have shown that shallow neural networks
optimized on a single task with gradient-based methods can learn meaningful
features, extending our understanding beyond the neural tangent kernel or
random feature regime in which negligible feature learning occurs. But in
practice, neural networks are increasingly often trained on {\em many} tasks
simultaneously with differing loss functions, and these prior analyses do not
generalize to such settings. In the multi-task learning setting, a variety of
studies have shown effective feature learning by simple linear models. However,
multi-task learning via {\em nonlinear} models, arguably the most common
learning paradigm in practice, remains largely mysterious. In this work, we
present the first results proving feature learning occurs in a multi-task
setting with a nonlinear model. We show that when the tasks are binary
classification problems with labels depending on only $r$ directions within the
ambient $d\gg r$-dimensional input space, executing a simple gradient-based
multitask learning algorithm on a two-layer ReLU neural network learns the
ground-truth $r$ directions. In particular, any downstream task on the $r$
ground-truth coordinates can be solved by learning a linear classifier with
sample and neuron complexity independent of the ambient dimension $d$, while a
random feature model requires exponential complexity in $d$ for such a
guarantee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collins_L/0/1/0/all/0/1&quot;&gt;Liam Collins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1&quot;&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1&quot;&gt;Sanjay Shakkottai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06915">
<title>Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06915</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Descent (SGD) is one of the simplest and most popular
algorithms in modern statistical and machine learning due to its computational
and memory efficiency. Various averaging schemes have been proposed to
accelerate the convergence of SGD in different settings. In this paper, we
explore a general averaging scheme for SGD. Specifically, we establish the
asymptotic normality of a broad range of weighted averaged SGD solutions and
provide asymptotically valid online inference approaches. Furthermore, we
propose an adaptive averaging scheme that exhibits both optimal statistical
rate and favorable non-asymptotic convergence, drawing insights from the
optimal weight for the linear model in terms of non-asymptotic mean squared
error (MSE).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Ziyang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Biao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07250">
<title>Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byung-Kwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07674">
<title>An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07674</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) algorithms aim to learn an optimal policy by
iteratively sampling actions to learn how to maximize the total expected
return, $R(x)$. GFlowNets are a special class of algorithms designed to
generate diverse candidates, $x$, from a discrete set, by learning a policy
that approximates the proportional sampling of $R(x)$. GFlowNets exhibit
improved mode discovery compared to conventional RL algorithms, which is very
useful for applications such as drug discovery and combinatorial search.
However, since GFlowNets are a relatively recent class of algorithms, many
techniques which are useful in RL have not yet been associated with them. In
this paper, we study the utilization of a replay buffer for GFlowNets. We
explore empirically various replay buffer sampling techniques and assess the
impact on the speed of mode discovery and the quality of the modes discovered.
Our experimental results in the Hypergrid toy domain and a molecule synthesis
environment demonstrate significant improvements in mode discovery when
training with a replay buffer, compared to training only with trajectories
generated on-policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemgal_N/0/1/0/all/0/1&quot;&gt;Nikhil Vemgal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1&quot;&gt;Elaine Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07916">
<title>On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07916</link>
<description rdf:parseType="Literal">&lt;p&gt;Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08433">
<title>From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08433</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world datasets have an underlying dynamic graph structure, where
entities and their interactions evolve over time. Machine learning models
should consider these dynamics in order to harness their full potential in
downstream tasks. Previous approaches for graph representation learning have
focused on either sampling k-hop neighborhoods, akin to breadth-first search,
or random walks, akin to depth-first search. However, these methods are
computationally expensive and unsuitable for real-time, low-latency inference
on dynamic graphs. To overcome these limitations, we propose graph-sprints a
general purpose feature extraction framework for continuous-time-dynamic-graphs
(CTDGs) that has low latency and is competitive with state-of-the-art, higher
latency models. To achieve this, a streaming, low latency approximation to the
random-walk based features is proposed. In our framework, time-aware node
embeddings summarizing multi-hop information are computed using only single-hop
operations on the incoming edges. We evaluate our proposed approach on three
open-source datasets and two in-house datasets, and compare with three
state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our
graph-sprints features, combined with a machine learning classifier, achieve
competitive performance (outperforming all baselines for the node
classification tasks in five datasets). Simultaneously, graph-sprints
significantly reduce inference latencies, achieving close to an order of
magnitude speed-up in our experimental setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eddin_A/0/1/0/all/0/1&quot;&gt;Ahmad Naser Eddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_J/0/1/0/all/0/1&quot;&gt;Jacopo Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aparicio_D/0/1/0/all/0/1&quot;&gt;David Apar&amp;#xed;cio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_H/0/1/0/all/0/1&quot;&gt;Hugo Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ascensao_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Ascens&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_P/0/1/0/all/0/1&quot;&gt;Pedro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1&quot;&gt;Pedro Bizarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08466">
<title>Generalizable Classification of UHF Partial Discharge Signals in Gas-Insulated HVDC Systems Using Neural Networks. (arXiv:2307.08466v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08466</link>
<description rdf:parseType="Literal">&lt;p&gt;Undetected partial discharges (PDs) are a safety critical issue in high
voltage (HV) gas insulated systems (GIS). While the diagnosis of PDs under AC
voltage is well-established, the analysis of PDs under DC voltage remains an
active research field. A key focus of these investigations is the
classification of different PD sources to enable subsequent sophisticated
analysis.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose and analyze a neural network-based approach for
classifying PD signals caused by metallic protrusions and conductive particles
on the insulator of HVDC GIS, without relying on pulse sequence analysis
features. In contrast to previous approaches, our proposed model can
discriminate the studied PD signals obtained at negative and positive
potentials, while also generalizing to unseen operating voltage multiples.
Additionally, we compare the performance of time- and frequency-domain input
signals and explore the impact of different normalization schemes to mitigate
the influence of free-space path loss between the sensor and defect location.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1&quot;&gt;Steffen Seitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gotz_T/0/1/0/all/0/1&quot;&gt;Thomas G&amp;#xf6;tz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindenberg_C/0/1/0/all/0/1&quot;&gt;Christopher Lindenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tetzlaff_R/0/1/0/all/0/1&quot;&gt;Ronald Tetzlaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlegel_S/0/1/0/all/0/1&quot;&gt;Stephan Schlegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08533">
<title>Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08533</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have achieved remarkable breakthroughs by leveraging
multiple layers of data processing to extract hidden representations, albeit at
the cost of large electronic computing power. To enhance energy efficiency and
speed, the optical implementation of neural networks aims to harness the
advantages of optical bandwidth and the energy efficiency of optical
interconnections. In the absence of low-power optical nonlinearities, the
challenge in the implementation of multilayer optical networks lies in
realizing multiple optical layers without resorting to electronic components.
In this study, we present a novel framework that uses multiple scattering that
is capable of synthesizing programmable linear and nonlinear transformations
concurrently at low optical power by leveraging the nonlinear relationship
between the scattering potential, represented by data, and the scattered field.
Theoretical and experimental investigations show that repeating the data by
multiple scattering enables non-linear optical computing at low power
continuous wave light.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yildirim_M/0/1/0/all/0/1&quot;&gt;Mustafa Yildirim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dinc_N/0/1/0/all/0/1&quot;&gt;Niyazi Ulas Dinc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ilker Oguz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Psaltis_D/0/1/0/all/0/1&quot;&gt;Demetri Psaltis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Moser_C/0/1/0/all/0/1&quot;&gt;Christophe Moser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08535">
<title>Multi-class point cloud completion networks for 3D cardiac anatomy reconstruction from cine magnetic resonance images. (arXiv:2307.08535v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08535</link>
<description rdf:parseType="Literal">&lt;p&gt;Cine magnetic resonance imaging (MRI) is the current gold standard for the
assessment of cardiac anatomy and function. However, it typically only acquires
a set of two-dimensional (2D) slices of the underlying three-dimensional (3D)
anatomy of the heart, thus limiting the understanding and analysis of both
healthy and pathological cardiac morphology and physiology. In this paper, we
propose a novel fully automatic surface reconstruction pipeline capable of
reconstructing multi-class 3D cardiac anatomy meshes from raw cine MRI
acquisitions. Its key component is a multi-class point cloud completion network
(PCCN) capable of correcting both the sparsity and misalignment issues of the
3D reconstruction task in a unified model. We first evaluate the PCCN on a
large synthetic dataset of biventricular anatomies and observe Chamfer
distances between reconstructed and gold standard anatomies below or similar to
the underlying image resolution for multiple levels of slice misalignment.
Furthermore, we find a reduction in reconstruction error compared to a
benchmark 3D U-Net by 32% and 24% in terms of Hausdorff distance and mean
surface distance, respectively. We then apply the PCCN as part of our automated
reconstruction pipeline to 1000 subjects from the UK Biobank study in a
cross-domain transfer setting and demonstrate its ability to reconstruct
accurate and topologically plausible biventricular heart meshes with clinical
metrics comparable to the previous literature. Finally, we investigate the
robustness of our proposed approach and observe its capacity to successfully
handle multiple common outlier conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beetz_M/0/1/0/all/0/1&quot;&gt;Marcel Beetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Abhirup Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ossenberg_Engels_J/0/1/0/all/0/1&quot;&gt;Julius Ossenberg-Engels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08558">
<title>Deep Learning with Passive Optical Nonlinear Mapping. (arXiv:2307.08558v2 [physics.optics] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08558</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has fundamentally transformed artificial intelligence, but the
ever-increasing complexity in deep learning models calls for specialized
hardware accelerators. Optical accelerators can potentially offer enhanced
performance, scalability, and energy efficiency. However, achieving nonlinear
mapping, a critical component of neural networks, remains challenging
optically. Here, we introduce a design that leverages multiple scattering in a
reverberating cavity to passively induce optical nonlinear random mapping,
without the need for additional laser power. A key advantage emerging from our
work is that we show we can perform optical data compression, facilitated by
multiple scattering in the cavity, to efficiently compress and retain vital
information while also decreasing data dimensionality. This allows rapid
optical information processing and generation of low dimensional mixtures of
highly nonlinear features. These are particularly useful for applications
demanding high-speed analysis and responses such as in edge computing devices.
Utilizing rapid optical information processing capabilities, our optical
platforms could potentially offer more efficient and real-time processing
solutions for a broad range of applications. We demonstrate the efficacy of our
design in improving computational performance across tasks, including
classification, image reconstruction, key-point detection, and object
detection, all achieved through optical data compression combined with a
digital decoder. Notably, we observed high performance, at an extreme
compression ratio, for real-time pedestrian detection. Our findings pave the
way for novel algorithms and architectural designs for optical computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kyungduk Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Eliezer_Y/0/1/0/all/0/1&quot;&gt;Yaniv Eliezer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shaughnessy_L/0/1/0/all/0/1&quot;&gt;Liam Shaughnessy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gigan_S/0/1/0/all/0/1&quot;&gt;Sylvain Gigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hui Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08572">
<title>Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08572</link>
<description rdf:parseType="Literal">&lt;p&gt;Coping with distributional shifts is an important part of transfer learning
methods in order to perform well in real-life tasks. However, most of the
existing approaches in this area either focus on an ideal scenario in which the
data does not contain noises or employ a complicated training paradigm or model
design to deal with distributional shifts. In this paper, we revisit the
robustness of the minimum error entropy (MEE) criterion, a widely used
objective in statistical signal processing to deal with non-Gaussian noises,
and investigate its feasibility and usefulness in real-life transfer learning
regression tasks, where distributional shifts are common. Specifically, we put
forward a new theoretical result showing the robustness of MEE against
covariate shift. We also show that by simply replacing the mean squared error
(MSE) loss with the MEE on basic transfer learning algorithms such as
fine-tuning and linear probing, we can achieve competitive performance with
respect to state-of-the-art transfer learning algorithms. We justify our
arguments on both synthetic data and 5 real-world time-series data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silvestrin_L/0/1/0/all/0/1&quot;&gt;Luis Pedro Silvestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1&quot;&gt;Mark Hoogendoorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08674">
<title>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08674</link>
<description rdf:parseType="Literal">&lt;p&gt;Tables are prevalent in real-world databases, requiring significant time and
effort for humans to analyze and manipulate. The advancements in large language
models (LLMs) have made it possible to interact with tables using natural
language input, bringing this capability closer to reality. In this paper, we
present TableGPT, a unified fine-tuned framework that enables LLMs to
understand and operate on tables using external functional commands. It
introduces the capability to seamlessly interact with tables, enabling a wide
range of functionalities such as question answering, data manipulation (e.g.,
insert, delete, query, and modify operations), data visualization, analysis
report generation, and automated prediction. TableGPT aims to provide
convenience and accessibility to users by empowering them to effortlessly
leverage tabular data. At the core of TableGPT lies the novel concept of global
tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces.
Moreover, it supports efficient data process flow, query rejection (when
appropriate) and private deployment, enabling faster domain data fine-tuning
and ensuring data privacy, which enhances the framework&apos;s adaptability to
specific use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1&quot;&gt;Liangyu Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junlin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Saisai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jing Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1&quot;&gt;Changbao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1&quot;&gt;Aofeng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_K/0/1/0/all/0/1&quot;&gt;Kaizhe Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wufang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guoshan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yali Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wentao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xinglong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08686">
<title>An R package for parametric estimation of causal effects. (arXiv:2307.08686v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08686</link>
<description rdf:parseType="Literal">&lt;p&gt;This article explains the usage of R package CausalModels, which is publicly
available on the Comprehensive R Archive Network. While packages are available
for sufficiently estimating causal effects, there lacks a package that provides
a collection of structural models using the conventional statistical approach
developed by Hernan and Robins (2020). CausalModels addresses this deficiency
of software in R concerning causal inference by offering tools for methods that
account for biases in observational data without requiring extensive
statistical knowledge. These methods should not be ignored and may be more
appropriate or efficient in solving particular problems. While implementations
of these statistical models are distributed among a number of causal packages,
CausalModels introduces a simple and accessible framework for a consistent
modeling pipeline among a variety of statistical methods for estimating causal
effects in a single R package. It consists of common methods including
standardization, IP weighting, G-estimation, outcome regression, instrumental
variables and propensity matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Anderson_J/0/1/0/all/0/1&quot;&gt;Joshua Wolff Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rakovski_C/0/1/0/all/0/1&quot;&gt;Cyril Rakovski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13697">
<title>FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2205.13697</link>
<description rdf:parseType="Literal">&lt;p&gt;A core issue in multi-agent federated reinforcement learning is defining how
to aggregate insights from multiple agents. This is commonly done by taking the
average of each participating agent&apos;s model weights into one common model
(FedAvg). We instead propose FedFormer, a novel federation strategy that
utilizes Transformer Attention to contextually aggregate embeddings from models
originating from different learner agents. In so doing, we attentively weigh
the contributions of other agents with respect to the current agent&apos;s
environment and learned relationships, thus providing a more effective and
efficient federation. We evaluate our methods on the Meta-World environment and
find that our approach yields significant improvements over FedAvg and
non-federated Soft Actor-Critic single-agent methods. Our results compared to
Soft Actor-Critic show that FedFormer achieves higher episodic return while
still abiding by the privacy constraints of federated learning. Finally, we
also demonstrate improvements in effectiveness with increased agent pools
across all methods in certain tasks. This is contrasted by FedAvg, which fails
to make noticeable improvements when scaled.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1&quot;&gt;Liam Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1&quot;&gt;Lukasz Golab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1&quot;&gt;Pascal Poupart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Robin Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.00664">
<title>\nu-Flows: Conditional Neutrino Regression. (arXiv:2207.00664v7 [hep-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2207.00664</link>
<description rdf:parseType="Literal">&lt;p&gt;We present $\nu$-Flows, a novel method for restricting the likelihood space
of neutrino kinematics in high energy collider experiments using conditional
normalizing flows and deep invertible neural networks. This method allows the
recovery of the full neutrino momentum which is usually left as a free
parameter and permits one to sample neutrino values under a learned conditional
likelihood given event observations. We demonstrate the success of $\nu$-Flows
in a case study by applying it to simulated semileptonic $t\bar{t}$ events and
show that it can lead to more accurate momentum reconstruction, particularly of
the longitudinal coordinate. We also show that this has direct benefits in a
downstream task of jet association, leading to an improvement of up to a factor
of 1.41 compared to conventional methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Leigh_M/0/1/0/all/0/1&quot;&gt;Matthew Leigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Raine_J/0/1/0/all/0/1&quot;&gt;John Andrew Raine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Zoch_K/0/1/0/all/0/1&quot;&gt;Knut Zoch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Golling_T/0/1/0/all/0/1&quot;&gt;Tobias Golling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07357">
<title>Inverse Optimization for Routing Problems. (arXiv:2307.07357v1 [math.OC] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.07357</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for learning decision-makers&apos; behavior in routing
problems using Inverse Optimization (IO). The IO framework falls into the
supervised learning category and builds on the premise that the target behavior
is an optimizer of an unknown cost function. This cost function is to be
learned through historical data, and in the context of routing problems, can be
interpreted as the routing preferences of the decision-makers. In this view,
the main contributions of this study are to propose an IO methodology with a
hypothesis function, loss function, and stochastic first-order algorithm
tailored to routing problems. We further test our IO approach in the Amazon
Last Mile Routing Research Challenge, where the goal is to learn models that
replicate the routing preferences of human drivers, using thousands of
real-world routing examples. Our final IO-learned routing model achieves a
score that ranks 2nd compared with the 48 models that qualified for the final
round of the challenge. Our results showcase the flexibility and real-world
potential of the proposed IO methodology to learn from decision-makers&apos;
decisions in routing problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Scroccaro_P/0/1/0/all/0/1&quot;&gt;Pedro Zattoni Scroccaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Beek_P/0/1/0/all/0/1&quot;&gt;Piet van Beek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Esfahani_P/0/1/0/all/0/1&quot;&gt;Peyman Mohajerin Esfahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Atasoy_B/0/1/0/all/0/1&quot;&gt;Bilge Atasoy&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>