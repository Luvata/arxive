<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05502" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.09035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05468" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.05502">
<title>Estimating See and Be Seen Performance with an Airborne Visual Acquisition Model. (arXiv:2307.05502v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2307.05502</link>
<description rdf:parseType="Literal">&lt;p&gt;Separation provision and collision avoidance to avoid other air traffic are
fundamental components of the layered conflict management system to ensure safe
and efficient operations. Pilots have visual-based separation responsibilities
to see and be seen to maintain separation between aircraft. To safely integrate
into the airspace, drones should be required to have a minimum level of
performance based on the safety achieved as baselined by crewed aircraft seen
and be seen interactions. Drone interactions with crewed aircraft should not be
more hazardous than interactions between traditional aviation aircraft.
Accordingly, there is need for a methodology to design and evaluate detect and
avoid systems, to be equipped by drones to mitigate the risk of a midair
collision, where the methodology explicitly addresses, both semantically and
mathematically, the appropriate operating rules associated with see and be
seen. In response, we simulated how onboard pilots safely operate through see
and be seen interactions using an updated visual acquisition model that was
originally developed by J.W. Andrews decades ago. Monte Carlo simulations were
representative two aircraft flying under visual flight rules and results were
analyzed with respect to drone detect and avoid performance standards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Underhill_N/0/1/0/all/0/1&quot;&gt;Ngaire Underhill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maki_E/0/1/0/all/0/1&quot;&gt;Evan Maki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gill_B/0/1/0/all/0/1&quot;&gt;Bilal Gill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinert_A/0/1/0/all/0/1&quot;&gt;Andrew Weinert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05508">
<title>Human in the AI loop via xAI and Active Learning for Visual Inspection. (arXiv:2307.05508v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.05508</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial revolutions have historically disrupted manufacturing by
introducing automation into production. Increasing automation reshapes the role
of the human worker. Advances in robotics and artificial intelligence open new
frontiers of human-machine collaboration. In this chapter, we first describe
Industry 5.0, human-machine collaboration, and state-of-the-art regarding
quality inspection, emphasizing visual inspection. We then provide our
perspective on how human-machine collaboration could be realized and enhanced
in visual inspection. Finally, we share some of the results obtained in the EU
H2020 STAR project regarding visual inspection, considering artificial
intelligence, human digital twins, and cybersecurity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozanec_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#x17e;e M. Ro&amp;#x17e;anec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montini_E/0/1/0/all/0/1&quot;&gt;Elias Montini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutrona_V/0/1/0/all/0/1&quot;&gt;Vincenzo Cutrona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papamartzivanos_D/0/1/0/all/0/1&quot;&gt;Dimitrios Papamartzivanos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemencic_T/0/1/0/all/0/1&quot;&gt;Timotej Klemen&amp;#x10d;i&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuna_B/0/1/0/all/0/1&quot;&gt;Bla&amp;#x17e; Fortuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mladenic_D/0/1/0/all/0/1&quot;&gt;Dunja Mladeni&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veliou_E/0/1/0/all/0/1&quot;&gt;Entso Veliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannetsos_T/0/1/0/all/0/1&quot;&gt;Thanassis Giannetsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emmanouilidis_C/0/1/0/all/0/1&quot;&gt;Christos Emmanouilidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05519">
<title>Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis. (arXiv:2307.05519v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.05519</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential of artificial intelligence (AI) in digital pathology is limited
by technical inconsistencies in the production of whole slide images (WSIs),
leading to degraded AI performance and posing a challenge for widespread
clinical application as fine-tuning algorithms for each new site is
impractical. Changes in the imaging workflow can also lead to compromised
diagnoses and patient safety risks. We evaluated whether physical color
calibration of scanners can standardize WSI appearance and enable robust AI
performance. We employed a color calibration slide in four different
laboratories and evaluated its impact on the performance of an AI system for
prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in
consistently improved AI model calibration and significant improvements in
Gleason grading performance. The study demonstrates that physical color
calibration provides a potential solution to the variation introduced by
different scanners, making AI-based cancer diagnostics more reliable and
applicable in clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiaoyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Salmon_R/0/1/0/all/0/1&quot;&gt;Richard Salmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mulliqi_N/0/1/0/all/0/1&quot;&gt;Nita Mulliqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Khan_U/0/1/0/all/0/1&quot;&gt;Umair Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Blilie_A/0/1/0/all/0/1&quot;&gt;Anders Blilie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Olsson_H/0/1/0/all/0/1&quot;&gt;Henrik Olsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pedersen_B/0/1/0/all/0/1&quot;&gt;Bodil Ginnerup Pedersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sorensen_K/0/1/0/all/0/1&quot;&gt;Karina Dalsgaard S&amp;#xf8;rensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ulhoi_B/0/1/0/all/0/1&quot;&gt;Benedicte Parm Ulh&amp;#xf8;i&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kjosavik_S/0/1/0/all/0/1&quot;&gt;Svein R Kjosavik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Janssen_E/0/1/0/all/0/1&quot;&gt;Emilius AM Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rantalainen_M/0/1/0/all/0/1&quot;&gt;Mattias Rantalainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Egevad_L/0/1/0/all/0/1&quot;&gt;Lars Egevad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ruusuvuori_P/0/1/0/all/0/1&quot;&gt;Pekka Ruusuvuori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Eklund_M/0/1/0/all/0/1&quot;&gt;Martin Eklund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kartasalo_K/0/1/0/all/0/1&quot;&gt;Kimmo Kartasalo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05534">
<title>Face Image Quality Enhancement Study for Face Recognition. (arXiv:2307.05534v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05534</link>
<description rdf:parseType="Literal">&lt;p&gt;Unconstrained face recognition is an active research area among computer
vision and biometric researchers for many years now. Still the problem of face
recognition in low quality photos has not been well-studied so far. In this
paper, we explore the face recognition performance on low quality photos, and
we try to improve the accuracy in dealing with low quality face images. We
assemble a large database with low quality photos, and examine the performance
of face recognition algorithms for three different quality sets. Using
state-of-the-art facial image enhancement approaches, we explore the face
recognition performance for the enhanced face images. To perform this without
experimental bias, we have developed a new protocol for recognition with low
quality face photos and validate the performance experimentally. Our designed
protocol for face recognition with low quality face images can be useful to
other researchers. Moreover, experiment results show some of the challenging
aspects of this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nouyed_I/0/1/0/all/0/1&quot;&gt;Iqbal Nouyed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Na Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05541">
<title>High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition. (arXiv:2307.05541v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05541</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive performance obtained by recent single-image hand
modeling techniques, they lack the capability to capture sufficient details of
the 3D hand mesh. This deficiency greatly limits their applications when
high-fidelity hand modeling is required, e.g., personalized hand modeling. To
address this problem, we design a frequency split network to generate 3D hand
mesh using different frequency bands in a coarse-to-fine manner. To capture
high-frequency personalized details, we transform the 3D mesh into the
frequency domain, and propose a novel frequency decomposition loss to supervise
each frequency component. By leveraging such a coarse-to-fine scheme, hand
details that correspond to the higher frequency domain can be preserved. In
addition, the proposed network is scalable, and can stop the inference at any
resolution level to accommodate different hardware with varying computational
powers. To quantitatively evaluate the performance of our method in terms of
recovering personalized shape details, we introduce a new evaluation metric
named Mean Signal-to-Noise Ratio (MSNR) to measure the signal-to-noise ratio of
each mesh frequency component. Extensive experiments demonstrate that our
approach generates fine-grained details for high-fidelity 3D hand
reconstruction, and our evaluation metric is more effective for measuring mesh
details compared with traditional metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_T/0/1/0/all/0/1&quot;&gt;Tianyu Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1&quot;&gt;Jingjing Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Junsong Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05561">
<title>TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement. (arXiv:2307.05561v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05561</link>
<description rdf:parseType="Literal">&lt;p&gt;As demand for robotics manipulation application increases, accurate
vision-based 6D pose estimation becomes essential for autonomous operations.
Convolutional Neural Networks (CNNs) based approaches for pose estimation have
been previously introduced. However, the quest for better performance still
persists especially for accurate robotics manipulation. This quest extends to
the Agri-robotics domain. In this paper, we propose TransPose, an improved
Transformer-based 6D pose estimation with a depth refinement module. The
architecture takes in only an RGB image as input with no additional
supplementing modalities such as depth or thermal images. The architecture
encompasses an innovative lighter depth estimation network that estimates depth
from an RGB image using feature pyramid with an up-sampling method. A
transformer-based detection network with additional prediction heads is
proposed to directly regress the object&apos;s centre and predict the 6D pose of the
target. A novel depth refinement module is then used alongside the predicted
centers, 6D poses and depth patches to refine the accuracy of the estimated 6D
pose. We extensively compared our results with other state-of-the-art methods
and analysed our results for fruit-picking applications. The results we
achieved show that our proposed technique outperforms the other methods
available in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulsalam_M/0/1/0/all/0/1&quot;&gt;Mahmoud Abdulsalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1&quot;&gt;Nabil Aouf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05563">
<title>RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05563</link>
<description rdf:parseType="Literal">&lt;p&gt;Contactless fingerprint matching using smartphone cameras can alleviate major
challenges of traditional fingerprint systems including hygienic acquisition,
portability and presentation attacks. However, development of practical and
robust contactless fingerprint matching techniques is constrained by the
limited availability of large scale real-world datasets. To motivate further
advances in contactless fingerprint matching across sensors, we introduce the
RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless
and contact-based fingerprint image pairs acquired from 88 individuals under
different background and lighting conditions using two smartphone cameras and
one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to
promote research under different matching scenarios that include Single Finger
Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL)
and contact-to-contactless (C2CL) verification and identification. Furthermore,
due to the high intra-sample variance in contactless fingerprints belonging to
the same finger, we propose a set-based matching protocol inspired by the
advances in facial recognition datasets. This protocol is specifically designed
for pragmatic contactless fingerprint matching that can account for variances
in focus, polarity and finger-angles. We report qualitative and quantitative
baseline results for different protocols using a COTS fingerprint matcher
(Verifinger) and a Deep CNN based approach on the RidgeBase dataset. The
dataset can be downloaded here:
https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1&quot;&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratha_N/0/1/0/all/0/1&quot;&gt;Nalini Ratha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05587">
<title>Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05587</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms have pushed the boundaries of computer vision
research and have depicted commendable performance in a variety of
applications. However, training a robust deep neural network necessitates a
large amount of labeled training data, acquiring which involves significant
time and human effort. This problem is even more serious for an application
like video classification, where a human annotator has to watch an entire video
end-to-end to furnish a label. Active learning algorithms automatically
identify the most informative samples from large amounts of unlabeled data;
this tremendously reduces the human annotation effort in inducing a machine
learning model, as only the few samples that are identified by the algorithm,
need to be labeled manually. In this paper, we propose a novel active learning
framework for video classification, with the goal of further reducing the
labeling onus on the human annotators. Our framework identifies a batch of
exemplar videos, together with a set of informative frames for each video; the
human annotator needs to merely review the frames and provide a label for each
video. This involves much less manual work than watching the complete video to
come up with a label. We formulate a criterion based on uncertainty and
diversity to identify the informative videos and exploit representative
sampling techniques to extract a set of exemplar frames from each video. To the
best of our knowledge, this is the first research effort to develop an active
learning framework for video classification, where the annotators need to
inspect only a few frames to produce a label, rather than watching the
end-to-end video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1&quot;&gt;Debanjan Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Shayok Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05591">
<title>SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05591</link>
<description rdf:parseType="Literal">&lt;p&gt;Textual and semantic comprehension of images is essential for generating
proper captions. The comprehension requires detection of objects, modeling of
relations between them, an assessment of the semantics of the scene and,
finally, representing the extracted knowledge in a language space. To achieve
rich language capabilities while ensuring good image-language mappings,
pretrained language models (LMs) were conditioned on pretrained multi-modal
(image-text) models that allow for image inputs. This requires an alignment of
the image representation of the multi-modal model with the language
representations of a generative LM. However, it is not clear how to best
transfer semantics detected by the vision encoder of the multi-modal model to
the LM. We introduce two novel ways of constructing a linear mapping that
successfully transfers semantics between the embedding spaces of the two
pretrained models. The first aligns the embedding space of the multi-modal
language encoder with the embedding space of the pretrained LM via token
correspondences. The latter leverages additional data that consists of
image-text pairs to construct the mapping directly from vision to language
space. Using our semantic mappings, we unlock image captioning for LMs without
access to gradient information. By using different sources of data we achieve
strong captioning performance on MS-COCO and Flickr30k datasets. Even in the
face of limited data, our method partly exceeds the performance of other
zero-shot and even finetuned competitors. Our ablation studies show that even
LMs at a scale of merely 250M parameters can generate decent captions employing
our semantic mappings. Our approach makes image captioning more accessible for
institutions with restricted computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1&quot;&gt;Fabian Paischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adler_T/0/1/0/all/0/1&quot;&gt;Thomas Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmarcher_M/0/1/0/all/0/1&quot;&gt;Markus Hofmarcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05601">
<title>Unsupervised Domain Adaptation with Deep Neural-Network. (arXiv:2307.05601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05601</link>
<description rdf:parseType="Literal">&lt;p&gt;This report contributes to the field of unsupervised domain adaptation by
providing an analysis of existing methods, introducing a new approach, and
demonstrating the potential for improving visual recognition tasks across
different domains. The results of this study open up opportunities for further
study and development of advanced methods in the field of domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bituitskii_A/0/1/0/all/0/1&quot;&gt;Artem Bituitskii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05610">
<title>Substance or Style: What Does Your Image Embedding Know?. (arXiv:2307.05610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05610</link>
<description rdf:parseType="Literal">&lt;p&gt;Probes are small networks that predict properties of underlying data from
embeddings, and they provide a targeted, effective way to illuminate the
information contained in embeddings. While analysis through the use of probes
has become standard in NLP, there has been much less exploration in vision.
Image foundation models have primarily been evaluated for semantic content.
Better understanding the non-semantic information in popular embeddings (e.g.,
MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and
on the uses for these foundation models. We design a systematic transformation
prediction task and measure the visual content of embeddings along many axes,
including image style, quality, and a range of natural and artificial
transformations. Surprisingly, six embeddings (including SimCLR) encode enough
non-semantic information to identify dozens of transformations. We also
consider a generalization task, where we group similar transformations and hold
out several for testing. We find that image-text models (CLIP and ALIGN) are
better at recognizing new examples of style transfer than masking-based models
(CAN and MAE). Overall, our results suggest that the choice of pre-training
algorithm impacts the types of information in the embedding, and certain models
are better than others for non-semantic downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1&quot;&gt;Cyrus Rashtchian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1&quot;&gt;Charles Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferng_C/0/1/0/all/0/1&quot;&gt;Chun-Sung Ferng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1&quot;&gt;Dilip Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Deqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomkins_A/0/1/0/all/0/1&quot;&gt;Andrew Tomkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05616">
<title>Image Reconstruction using Enhanced Vision Transformer. (arXiv:2307.05616v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05616</link>
<description rdf:parseType="Literal">&lt;p&gt;Removing noise from images is a challenging and fundamental problem in the
field of computer vision. Images captured by modern cameras are inevitably
degraded by noise which limits the accuracy of any quantitative measurements on
those images. In this project, we propose a novel image reconstruction
framework which can be used for tasks such as image denoising, deblurring or
inpainting. The model proposed in this project is based on Vision Transformer
(ViT) that takes 2D images as input and outputs embeddings which can be used
for reconstructing denoised images. We incorporate four additional optimization
techniques in the framework to improve the model reconstruction capability,
namely Locality Sensitive Attention (LSA), Shifted Patch Tokenization (SPT),
Rotary Position Embeddings (RoPE) and adversarial loss function inspired from
Generative Adversarial Networks (GANs). LSA, SPT and RoPE enable the
transformer to learn from the dataset more efficiently, while the adversarial
loss function enhances the resolution of the reconstructed images. Based on our
experiments, the proposed architecture outperforms the benchmark U-Net model by
more than 3.5\% structural similarity (SSIM) for the reconstruction tasks of
image denoising and inpainting. The proposed enhancements further show an
improvement of \textasciitilde5\% SSIM over the benchmark for both tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1&quot;&gt;Nikhil Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaur_D/0/1/0/all/0/1&quot;&gt;Deepkamal Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1&quot;&gt;Lydia Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05617">
<title>$\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model. (arXiv:2307.05617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05617</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large vision model, Segment Anything Model (SAM), has
revolutionized the computer vision field, especially for image segmentation.
SAM presented a new promptable segmentation paradigm that exhibit its
remarkable zero-shot generalization ability. An extensive researches have
explore the potential and limits of SAM in various downstream tasks. In this
study, we presents $\mathrm{SAM^{Med}}$, an enhanced framework for medical
image annotation that leverages the capabilities of SAM. $\mathrm{SAM^{Med}}$
framework consisted of two submodules, namely $\mathrm{SAM^{assist}}$ and
$\mathrm{SAM^{auto}}$. The $\mathrm{SAM^{assist}}$ demonstrates the
generalization ability of SAM to the downstream medical segmentation task using
the prompt-learning approach. Results show a significant improvement in
segmentation accuracy with only approximately 5 input points. The
$\mathrm{SAM^{auto}}$ model aims to accelerate the annotation process by
automatically generating input prompts. The proposed SAP-Net model achieves
superior segmentation performance with only five annotated slices, achieving an
average Dice coefficient of 0.80 and 0.82 for kidney and liver segmentation,
respectively. Overall, $\mathrm{SAM^{Med}}$ demonstrates promising results in
medical image annotation. These findings highlight the potential of leveraging
large-scale vision models in medical image annotation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenglong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dexuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sucheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengxiu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05634">
<title>Hyperspherical Embedding for Point Cloud Completion. (arXiv:2307.05634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05634</link>
<description rdf:parseType="Literal">&lt;p&gt;Most real-world 3D measurements from depth sensors are incomplete, and to
address this issue the point cloud completion task aims to predict the complete
shapes of objects from partial observations. Previous works often adapt an
encoder-decoder architecture, where the encoder is trained to extract
embeddings that are used as inputs to generate predictions from the decoder.
However, the learned embeddings have sparse distribution in the feature space,
which leads to worse generalization results during testing. To address these
problems, this paper proposes a hyperspherical module, which transforms and
normalizes embeddings from the encoder to be on a unit hypersphere. With the
proposed module, the magnitude and direction of the output hyperspherical
embedding are decoupled and only the directional information is optimized. We
theoretically analyze the hyperspherical embedding and show that it enables
more stable training with a wider range of learning rates and more compact
embedding distributions. Experiment results show consistent improvement of
point cloud completion in both single-task and multi-task learning, which
demonstrates the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haomeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasudevan_R/0/1/0/all/0/1&quot;&gt;Ram Vasudevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson-Roberson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05650">
<title>Evidence-based Hand Hygiene. Can You Trust the Fluorescent-based Assessment Methods?. (arXiv:2307.05650v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.05650</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare-Associated Infections present a major threat to patient safety
globally. According to studies, more than 50% of HAI could be prevented by
proper hand hygiene. Effectiveness of hand hygiene is regularly evaluated with
the fluorescent method: performing hand hygiene with a handrub containing an
ultra violet (UV) fluorescent marker. Typically, human experts evaluate the
hands under UV-A light, and decide whether the applied handrub covered the
whole hand surface. The aim of this study was to investigate how different
experts judge the same UV-pattern, and compare that to microbiology for
objective validation. Hands of volunteer participants were contaminated with
high concentration of a Staphylococcus epidermidis suspension. Hands were
incompletely disinfected with UV-labeled handrub. Four different UV-box type
devices were used to take CCD pictures of the hands under UV light. Size of
inadequately disinfected areas on the hands were determined in two different
ways. First, based on microbiology; the areas where colonies were grown were
measured. Second, four independent senior infection control specialists were
asked to mark the missed areas on printed image, captured under UV light. 8
hands of healthy volunteers were examined. Expert evaluations were highly
uncorrelated (regarding interrater reliability) and inconsistent. Microbiology
results weakly correlated with the expert evaluations. In half of the cases,
there were more than 10% difference in the size of properly disinfected area,
as measured by microbiology versus human experts. Considering the result of the
expert evaluations, variability was disconcertingly high. Evaluating the
fluorescent method is challenging, even for highly experienced professionals. A
patient safety quality assurance system cannot be built on these data quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansaghi_S/0/1/0/all/0/1&quot;&gt;Sz&amp;#xe1;va B&amp;#xe1;ns&amp;#xe1;ghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sari_V/0/1/0/all/0/1&quot;&gt;Viola S&amp;#xe1;ri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szeremy_P/0/1/0/all/0/1&quot;&gt;P&amp;#xe9;ter Szer&amp;#xe9;my&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehotsky_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;kos Lehotsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takacs_B/0/1/0/all/0/1&quot;&gt;Bence Tak&amp;#xe1;cs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toth_B/0/1/0/all/0/1&quot;&gt;Brigitta K. T&amp;#xf3;th&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haidegger_T/0/1/0/all/0/1&quot;&gt;Tam&amp;#xe1;s Haidegger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05663">
<title>Objaverse-XL: A Universe of 10M+ 3D Objects. (arXiv:2307.05663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05663</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language processing and 2D vision models have attained remarkable
proficiency on many tasks primarily by escalating the scale of training data.
However, 3D vision tasks have not seen the same progress, in part due to the
challenges of acquiring high-quality 3D data. In this work, we present
Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises
deduplicated 3D objects from a diverse set of sources, including manually
designed objects, photogrammetry scans of landmarks and everyday items, and
professional scans of historic and antique artifacts. Representing the largest
scale and diversity in the realm of 3D datasets, Objaverse-XL enables
significant new possibilities for 3D vision. Our experiments demonstrate the
improvements enabled with the scale provided by Objaverse-XL. We show that by
training Zero123 on novel view synthesis, utilizing over 100 million multi-view
rendered images, we achieve strong zero-shot generalization abilities. We hope
that releasing Objaverse-XL will enable further innovations in the field of 3D
vision at scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deitke_M/0/1/0/all/0/1&quot;&gt;Matt Deitke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruoshi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1&quot;&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1&quot;&gt;Huong Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michel_O/0/1/0/all/0/1&quot;&gt;Oscar Michel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1&quot;&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1&quot;&gt;Alan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laforte_C/0/1/0/all/0/1&quot;&gt;Christian Laforte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1&quot;&gt;Vikram Voleti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+VanderBilt_E/0/1/0/all/0/1&quot;&gt;Eli VanderBilt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1&quot;&gt;Aniruddha Kembhavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1&quot;&gt;Carl Vondrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gkioxari_G/0/1/0/all/0/1&quot;&gt;Georgia Gkioxari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1&quot;&gt;Kiana Ehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05694">
<title>A Survey on Figure Classification Techniques in Scientific Documents. (arXiv:2307.05694v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.05694</link>
<description rdf:parseType="Literal">&lt;p&gt;Figures visually represent an essential piece of information and provide an
effective means to communicate scientific facts. Recently there have been many
efforts toward extracting data directly from figures, specifically from tables,
diagrams, and plots, using different Artificial Intelligence and Machine
Learning techniques. This is because removing information from figures could
lead to deeper insights into the concepts highlighted in the scientific
documents. In this survey paper, we systematically categorize figures into five
classes - tables, photos, diagrams, maps, and plots, and subsequently present a
critical review of the existing methodologies and data sets that address the
problem of figure classification. Finally, we identify the current research
gaps and provide possible directions for further research on figure
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhote_A/0/1/0/all/0/1&quot;&gt;Anurag Dhote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David S Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05700">
<title>SepHRNet: Generating High-Resolution Crop Maps from Remote Sensing imagery using HRNet with Separable Convolution. (arXiv:2307.05700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05700</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate mapping of crop production is crucial for ensuring food
security, effective resource management, and sustainable agricultural
practices. One way to achieve this is by analyzing high-resolution satellite
imagery. Deep Learning has been successful in analyzing images, including
remote sensing imagery. However, capturing intricate crop patterns is
challenging due to their complexity and variability. In this paper, we propose
a novel Deep learning approach that integrates HRNet with Separable
Convolutional layers to capture spatial patterns and Self-attention to capture
temporal patterns of the data. The HRNet model acts as a backbone and extracts
high-resolution features from crop images. Spatially separable convolution in
the shallow layers of the HRNet model captures intricate crop patterns more
effectively while reducing the computational cost. The multi-head attention
mechanism captures long-term temporal dependencies from the encoded vector
representation of the images. Finally, a CNN decoder generates a crop map from
the aggregated representation. Adaboost is used on top of this to further
improve accuracy. The proposed algorithm achieves a high classification
accuracy of 97.5\% and IoU of 55.2\% in generating crop maps. We evaluate the
performance of our pipeline on the Zuericrop dataset and demonstrate that our
results outperform state-of-the-art models such as U-Net++, ResNet50, VGG19,
InceptionV3, DenseNet, and EfficientNet. This research showcases the potential
of Deep Learning for Earth Observation Systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Priyanka Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patnaik_S/0/1/0/all/0/1&quot;&gt;Sohan Patnaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1&quot;&gt;Adway Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_M/0/1/0/all/0/1&quot;&gt;Manjira Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05704">
<title>A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05704</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised representation learning with variational inference relies
heavily on independence assumptions over latent variables. Causal
representation learning (CRL), however, argues that factors of variation in a
dataset are, in fact, causally related. Allowing latent variables to be
correlated, as a consequence of causal relationships, is more realistic and
generalisable. So far, provably identifiable methods rely on: auxiliary
information, weak labels, and interventional or even counterfactual data.
Inspired by causal discovery with functional causal models, we propose a fully
unsupervised representation learning method that considers a data generation
process with a latent additive noise model (ANM). We encourage the latent space
to follow a causal ordering via loss function based on the Hessian of the
latent distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1&quot;&gt;Avinash Kori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilouras_K/0/1/0/all/0/1&quot;&gt;Konstantinos Vilouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1&quot;&gt;Sotirios A. Tsaftaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05707">
<title>MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning. (arXiv:2307.05707v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05707</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent progress in incremental learning, addressing catastrophic
forgetting under distributional drift is still an open and important problem.
Indeed, while state-of-the-art domain incremental learning (DIL) methods
perform satisfactorily within known domains, their performance largely degrades
in the presence of novel domains. This limitation hampers their
generalizability, and restricts their scalability to more realistic settings
where train and test data are drawn from different distributions. To address
these limitations, we present a novel DIL approach based on a mixture of
prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of
S-Prompting to handle both in-distribution and out-of-distribution data at
inference. In particular, at the training stage we model the features
distribution of every class in each domain, learning individual text and visual
prompts to adapt to a given domain. At inference, the learned distributions
allow us to identify whether a given test sample belongs to a known domain,
selecting the correct prompt for the classification task, or from an unseen
domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical
evaluation reveals the poor performance of existing DIL methods under domain
shift, and suggests that the proposed MoP-CLIP performs competitively in the
standard DIL settings while outperforming state-of-the-art methods in OOD
scenarios. These results demonstrate the superiority of MoP-CLIP, offering a
robust and general solution to the problem of domain incremental learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolas_J/0/1/0/all/0/1&quot;&gt;Julien Nicolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiaroni_F/0/1/0/all/0/1&quot;&gt;Florent Chiaroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziko_I/0/1/0/all/0/1&quot;&gt;Imtiaz Ziko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_O/0/1/0/all/0/1&quot;&gt;Ola Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1&quot;&gt;Christian Desrosiers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05721">
<title>HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding. (arXiv:2307.05721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05721</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding comprehensive assembly knowledge from videos is critical for
futuristic ultra-intelligent industry. To enable technological breakthrough, we
present HA-ViD - the first human assembly video dataset that features
representative industrial assembly scenarios, natural procedural knowledge
acquisition process, and consistent human-robot shared annotations.
Specifically, HA-ViD captures diverse collaboration patterns of real-world
assembly, natural human behaviors and learning progression during assembly, and
granulate action annotations to subject, action verb, manipulated object,
target object, and tool. We provide 3222 multi-view, multi-modality videos
(each video contains one assembly task), 1.5M frames, 96K temporal labels and
2M spatial labels. We benchmark four foundational video understanding tasks:
action recognition, action segmentation, object detection and multi-object
tracking. Importantly, we analyze their performance for comprehending knowledge
in assembly progress, process efficiency, task collaboration, skill parameters
and human intention. Details of HA-ViD is available at:
https://iai-hrc.github.io/ha-vid.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1&quot;&gt;Regina Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuqian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05747">
<title>Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05747</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans engage in learning and reviewing processes with curricula when
acquiring new skills or knowledge. This human learning behavior has inspired
the integration of curricula with replay methods in continual learning agents.
The goal is to emulate the human learning process, thereby improving knowledge
retention and facilitating learning transfer. Existing replay methods in
continual learning agents involve the random selection and ordering of data
from previous tasks, which has shown to be effective. However, limited research
has explored the integration of different curricula with replay methods to
enhance continual learning. Our study takes initial steps in examining the
impact of integrating curricula with replay methods on continual learning in
three specific aspects: the interleaved frequency of replayed exemplars with
training data, the sequence in which exemplars are replayed, and the strategy
for selecting exemplars into the replay buffer. These aspects of curricula
design align with cognitive psychology principles and leverage the benefits of
interleaved practice during replays, easy-to-hard rehearsal, and exemplar
selection strategy involving exemplars from a uniform distribution of
difficulties. Based on our results, these three curricula effectively mitigated
catastrophic forgetting and enhanced positive knowledge transfer, demonstrating
the potential of curricula in advancing continual learning methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tee_R/0/1/0/all/0/1&quot;&gt;Ren Jie Tee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05760">
<title>Line Art Colorization of Fakemon using Generative Adversarial Neural Networks. (arXiv:2307.05760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05760</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a complete methodology to colorize images of Fakemon,
anime-style monster-like creatures. In addition, we propose algorithms to
extract the line art from colorized images as well as to extract color hints.
Our work is the first in the literature to use automatic color hint extraction,
to train the networks specifically with anime-styled creatures and to combine
the Pix2Pix and CycleGAN approaches, two different generative adversarial
networks that create a single final result. Visual results of the colorizations
are feasible but there is still room for improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_E/0/1/0/all/0/1&quot;&gt;Erick Oliveira Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clua_E/0/1/0/all/0/1&quot;&gt;Esteban Clua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vitor_G/0/1/0/all/0/1&quot;&gt;Giovani Bernardes Vitor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05780">
<title>Automated Artifact Detection in Ultra-widefield Fundus Photography of Patients with Sickle Cell Disease. (arXiv:2307.05780v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05780</link>
<description rdf:parseType="Literal">&lt;p&gt;Importance: Ultra-widefield fundus photography (UWF-FP) has shown utility in
sickle cell retinopathy screening; however, image artifact may diminish quality
and gradeability of images. Objective: To create an automated algorithm for
UWF-FP artifact classification. Design: A neural network based automated
artifact detection algorithm was designed to identify commonly encountered
UWF-FP artifacts in a cross section of patient UWF-FP. A pre-trained ResNet-50
neural network was trained on a subset of the images and the classification
accuracy, sensitivity, and specificity were quantified on the hold out test
set. Setting: The study is based on patients from a tertiary care hospital
site. Participants: There were 243 UWF-FP acquired from patients with sickle
cell disease (SCD), and artifact labelling in the following categories was
performed: Eyelash Present, Lower Eyelid Obstructing, Upper Eyelid Obstructing,
Image Too Dark, Dark Artifact, and Image Not Centered. Results: Overall, the
accuracy for each class was Eyelash Present at 83.7%, Lower Eyelid Obstructing
at 83.7%, Upper Eyelid Obstructing at 98.0%, Image Too Dark at 77.6%, Dark
Artifact at 93.9%, and Image Not Centered at 91.8%. Conclusions and Relevance:
This automated algorithm shows promise in identifying common imaging artifacts
on a subset of Optos UWF-FP in SCD patients. Further refinement is ongoing with
the goal of improving efficiency of tele-retinal screening in sickle cell
retinopathy (SCR) by providing a photographer real-time feedback as to the
types of artifacts present, and the need for image re-acquisition. This
algorithm also may have potential future applicability in other retinal
diseases by improving quality and efficiency of image acquisition of UWF-FP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1&quot;&gt;Anqi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1&quot;&gt;Dimitri Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reilly_G/0/1/0/all/0/1&quot;&gt;Grace R. Reilly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thangamathesvaran_L/0/1/0/all/0/1&quot;&gt;Loka Thangamathesvaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nampomba_A/0/1/0/all/0/1&quot;&gt;Ann Nampomba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1&quot;&gt;Mathias Unberath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_A/0/1/0/all/0/1&quot;&gt;Adrienne W. Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1&quot;&gt;Craig Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05784">
<title>EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video. (arXiv:2307.05784v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05784</link>
<description rdf:parseType="Literal">&lt;p&gt;In egocentric action recognition a single population model is typically
trained and subsequently embodied on a head-mounted device, such as an
augmented reality headset. While this model remains static for new users and
environments, we introduce an adaptive paradigm of two phases, where after
pretraining a population model, the model adapts on-device and online to the
user&apos;s experience. This setting is highly challenging due to the change from
population to user domain and the distribution shifts in the user&apos;s data
stream. Coping with the latter in-stream distribution shifts is the focus of
continual learning, where progress has been rooted in controlled benchmarks but
challenges faced in real-world applications often remain unaddressed. We
introduce EgoAdapt, a benchmark for real-world egocentric action recognition
that facilitates our two-phased adaptive paradigm, and real-world challenges
naturally occur in the egocentric video streams from Ego4d, such as long-tailed
action distributions and large-scale classification over 2740 actions. We
introduce an evaluation framework that directly exploits the user&apos;s data stream
with new metrics to measure the adaptation gain over the population model,
online generalization, and hindsight performance. In contrast to single-stream
evaluation in existing works, our framework proposes a meta-evaluation that
aggregates the results from 50 independent user streams. We provide an
extensive empirical study for finetuning and experience replay.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_M/0/1/0/all/0/1&quot;&gt;Matthias De Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eghbalzadeh_H/0/1/0/all/0/1&quot;&gt;Hamid Eghbalzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Reuben Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1&quot;&gt;Michael Iuzzolino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1&quot;&gt;Franziska Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ridgeway_K/0/1/0/all/0/1&quot;&gt;Karl Ridgeway&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05786">
<title>Merging multiple input descriptors and supervisors in a deep neural network for tractogram filtering. (arXiv:2307.05786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05786</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the main issues of the current tractography methods is their high
false-positive rate. Tractogram filtering is an option to remove false-positive
streamlines from tractography data in a post-processing step. In this paper, we
train a deep neural network for filtering tractography data in which every
streamline of a tractogram is classified as {\em plausible, implausible}, or
{\em inconclusive}. For this, we use four different tractogram filtering
strategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an
anatomy-inspired filter. Their outputs are combined to obtain the
classification labels for the streamlines. We assessed the importance of
different types of information along the streamlines for performing this
classification task, including the coordinates of the streamlines, diffusion
data, landmarks, T1-weighted information, and a brain parcellation. We found
that the streamline coordinates are the most relevant followed by the diffusion
data in this particular classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jorgens_D/0/1/0/all/0/1&quot;&gt;Daniel J&amp;#xf6;rgens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1&quot;&gt;Pierre-Marc Jodoin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Descoteaux_M/0/1/0/all/0/1&quot;&gt;Maxime Descoteaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1&quot;&gt;Rodrigo Moreno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05799">
<title>3D Medical Image Segmentation based on multi-scale MPU-Net. (arXiv:2307.05799v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05799</link>
<description rdf:parseType="Literal">&lt;p&gt;The high cure rate of cancer is inextricably linked to physicians&apos; accuracy
in diagnosis and treatment, therefore a model that can accomplish
high-precision tumor segmentation has become a necessity in many applications
of the medical industry. It can effectively lower the rate of misdiagnosis
while considerably lessening the burden on clinicians. However, fully automated
target organ segmentation is problematic due to the irregular stereo structure
of 3D volume organs. As a basic model for this class of real applications,
U-Net excels. It can learn certain global and local features, but still lacks
the capacity to grasp spatial long-range relationships and contextual
information at multiple scales. This paper proposes a tumor segmentation model
MPU-Net for patient volume CT images, which is inspired by Transformer with a
global attention mechanism. By combining image serialization with the Position
Attention Module, the model attempts to comprehend deeper contextual
dependencies and accomplish precise positioning. Each layer of the decoder is
also equipped with a multi-scale module and a cross-attention mechanism. The
capability of feature extraction and integration at different levels has been
enhanced, and the hybrid loss function developed in this study can better
exploit high-resolution characteristic information. Moreover, the suggested
architecture is tested and evaluated on the Liver Tumor Segmentation Challenge
2017 (LiTS 2017) dataset. Compared with the benchmark model U-Net, MPU-Net
shows excellent segmentation results. The dice, accuracy, precision,
specificity, IOU, and MCC metrics for the best model segmentation results are
92.17%, 99.08%, 91.91%, 99.52%, 85.91%, and 91.74%, respectively. Outstanding
indicators in various aspects illustrate the exceptional performance of this
framework in automatic medical image segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zeqiu.Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shuo.Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05800">
<title>A Hierarchical Transformer Encoder to Improve Entire Neoplasm Segmentation on Whole Slide Image of Hepatocellular Carcinoma. (arXiv:2307.05800v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05800</link>
<description rdf:parseType="Literal">&lt;p&gt;In digital histopathology, entire neoplasm segmentation on Whole Slide Image
(WSI) of Hepatocellular Carcinoma (HCC) plays an important role, especially as
a preprocessing filter to automatically exclude healthy tissue, in histological
molecular correlations mining and other downstream histopathological tasks. The
segmentation task remains challenging due to HCC&apos;s inherent high-heterogeneity
and the lack of dependency learning in large field of view. In this article, we
propose a novel deep learning architecture with a hierarchical Transformer
encoder, HiTrans, to learn the global dependencies within expanded
4096$\times$4096 WSI patches. HiTrans is designed to encode and decode the
patches with larger reception fields and the learned global dependencies,
compared to the state-of-the-art Fully Convolutional Neural networks (FCNN).
Empirical evaluations verified that HiTrans leads to better segmentation
performance by taking into account regional and global dependency information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhuxian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Palpanas_T/0/1/0/all/0/1&quot;&gt;Themis Palpanas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lomenie_N/0/1/0/all/0/1&quot;&gt;Nicolas Lom&amp;#xe9;nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurtz_C/0/1/0/all/0/1&quot;&gt;Camille Kurtz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05801">
<title>Differentiable Forward Projector for X-ray Computed Tomography. (arXiv:2307.05801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05801</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven deep learning has been successfully applied to various computed
tomographic reconstruction problems. The deep inference models may outperform
existing analytical and iterative algorithms, especially in ill-posed CT
reconstruction. However, those methods often predict images that do not agree
with the measured projection data. This paper presents an accurate
differentiable forward and back projection software library to ensure the
consistency between the predicted images and the original measurements. The
software library efficiently supports various projection geometry types while
minimizing the GPU memory footprint requirement, which facilitates seamless
integration with existing deep learning training and inference pipelines. The
proposed software is available as open source: https://github.com/LLNL/LEAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyojin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Champley_K/0/1/0/all/0/1&quot;&gt;Kyle Champley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05804">
<title>Improving Segmentation and Detection of Lesions in CT Scans Using Intensity Distribution Supervision. (arXiv:2307.05804v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05804</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to incorporate the intensity information of a target
lesion on CT scans in training segmentation and detection networks. We first
build an intensity-based lesion probability (ILP) function from an intensity
histogram of the target lesion. It is used to compute the probability of being
the lesion for each voxel based on its intensity. Finally, the computed ILP map
of each input CT scan is provided as additional supervision for network
training, which aims to inform the network about possible lesion locations in
terms of intensity values at no additional labeling cost. The method was
applied to improve the segmentation of three different lesion types, namely,
small bowel carcinoid tumor, kidney tumor, and lung nodule. The effectiveness
of the proposed method on a detection task was also investigated. We observed
improvements of 41.3% -&amp;gt; 47.8%, 74.2% -&amp;gt; 76.0%, and 26.4% -&amp;gt; 32.7% in
segmenting small bowel carcinoid tumor, kidney tumor, and lung nodule,
respectively, in terms of per case Dice scores. An improvement of 64.6% -&amp;gt;
75.5% was achieved in detecting kidney tumors in terms of average precision.
The results of different usages of the ILP map and the effect of varied amount
of training data are also presented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Seung Yeon Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_T/0/1/0/all/0/1&quot;&gt;Thomas C. Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1&quot;&gt;Ronald M. Summers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05832">
<title>Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05832</link>
<description rdf:parseType="Literal">&lt;p&gt;UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has been experiencing an increasing surge of interest due to
the recent advancements in image processing and deep learning-based techniques.
View planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish view planning for active
reconstruction; the former approach poses a challenge for complex or newly
identified targets while the latter is computationally expensive. In this work,
we present Bag-of-Views (BoV), a fully appearance-based model used to assign
utility to the captured views for both offline dataset refinement and online
next-best-view (NBV) planning applications targeting the task of 3D
reconstruction. With this contribution, we also developed the View Planning
Toolbox (VPT), a lightweight package for training and testing machine
learning-based view planning frameworks, custom view dataset generation of
arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a
BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of
our model in reducing the number of required views for high-quality
reconstructions in dataset refinement and NBV planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gazani_S/0/1/0/all/0/1&quot;&gt;Sara Hatami Gazani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucsok_M/0/1/0/all/0/1&quot;&gt;Matthew Tucsok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mantegh_I/0/1/0/all/0/1&quot;&gt;Iraj Mantegh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05845">
<title>PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PIGEON, a multi-task end-to-end system for planet-scale image
geolocalization that achieves state-of-the-art performance on both external
benchmarks and in human evaluation. Our work incorporates semantic geocell
creation with label smoothing, conducts pretraining of a vision transformer on
images with geographic information, and refines location predictions with
ProtoNets across a candidate set of geocells. The contributions of PIGEON are
three-fold: first, we design a semantic geocells creation and splitting
algorithm based on open-source data which can be adapted to any geospatial
dataset. Second, we show the effectiveness of intra-geocell refinement and the
applicability of unsupervised clustering and ProtNets to the task. Finally, we
make our pre-trained CLIP transformer model, StreetCLIP, publicly available for
use in adjacent domains with applications to fighting climate change and urban
and rural scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1&quot;&gt;Lukas Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1&quot;&gt;Michal Skreta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05853">
<title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human. (arXiv:2307.05853v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05853</link>
<description rdf:parseType="Literal">&lt;p&gt;3D human pose estimation has been researched for decades with promising
fruits. 3D human pose lifting is one of the promising research directions
toward the task where both estimated pose and ground truth pose data are used
for training. Existing pose lifting works mainly focus on improving the
performance of estimated pose, but they usually underperform when testing on
the ground truth pose data. We observe that the performance of the estimated
pose can be easily improved by preparing good quality 2D pose, such as
fine-tuning the 2D pose or using advanced 2D pose detectors. As such, we
concentrate on improving the 3D human pose lifting via ground truth data for
the future improvement of more quality estimated pose data. Towards this goal,
a simple yet effective model called Global-local Adaptive Graph Convolutional
Network (GLA-GCN) is proposed in this work. Our GLA-GCN globally models the
spatiotemporal structure via a graph representation and backtraces local joint
features for 3D human pose estimation via individually connected layers. To
validate our model design, we conduct extensive experiments on three benchmark
datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results show
that our GLA-GCN implemented with ground truth 2D poses significantly
outperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% error
reductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bruce X.B. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Sheng-hua Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chang Wen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05873">
<title>OG: Equip vision occupancy with instance segmentation and visual grounding. (arXiv:2307.05873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05873</link>
<description rdf:parseType="Literal">&lt;p&gt;Occupancy prediction tasks focus on the inference of both geometry and
semantic labels for each voxel, which is an important perception mission.
However, it is still a semantic segmentation task without distinguishing
various instances. Further, although some existing works, such as
Open-Vocabulary Occupancy (OVO), have already solved the problem of open
vocabulary detection, visual grounding in occupancy has not been solved to the
best of our knowledge. To tackle the above two limitations, this paper proposes
Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance
segmentation ability and could operate visual grounding in a voxel manner with
the help of grounded-SAM. Keys to our approach are (1) affinity field
prediction for instance clustering and (2) association strategy for aligning 2D
instance masks and 3D occupancy instances. Extensive experiments have been
conducted whose visualization results and analysis are shown below. Our code
will be publicly released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zichao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Hang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weikun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xufeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junbo Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05874">
<title>Multi-Object Tracking as Attention Mechanism. (arXiv:2307.05874v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05874</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a conceptually simple and thus fast multi-object tracking (MOT)
model that does not require any attached modules, such as the Kalman filter,
Hungarian algorithm, transformer blocks, or graph networks. Conventional MOT
models are built upon the multi-step modules listed above, and thus the
computational cost is high. Our proposed end-to-end MOT model,
\textit{TicrossNet}, is composed of a base detector and a cross-attention
module only. As a result, the overhead of tracking does not increase
significantly even when the number of instances ($N_t$) increases. We show that
TicrossNet runs \textit{in real-time}; specifically, it achieves 32.6 FPS on
MOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $&amp;gt;$100
instances per frame. We also demonstrate that TicrossNet is robust to $N_t$;
thus, it does not have to change the size of the base detector, depending on
$N_t$, as is often done by other models for real-time processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukui_H/0/1/0/all/0/1&quot;&gt;Hiroshi Fukui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miyagawa_T/0/1/0/all/0/1&quot;&gt;Taiki Miyagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morishita_Y/0/1/0/all/0/1&quot;&gt;Yusuke Morishita&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05889">
<title>Rethinking Mitosis Detection: Towards Diverse Data and Feature Representation. (arXiv:2307.05889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05889</link>
<description rdf:parseType="Literal">&lt;p&gt;Mitosis detection is one of the fundamental tasks in computational pathology,
which is extremely challenging due to the heterogeneity of mitotic cell. Most
of the current studies solve the heterogeneity in the technical aspect by
increasing the model complexity. However, lacking consideration of the
biological knowledge and the complex model design may lead to the overfitting
problem while limited the generalizability of the detection model. In this
paper, we systematically study the morphological appearances in different
mitotic phases as well as the ambiguous non-mitotic cells and identify that
balancing the data and feature diversity can achieve better generalizability.
Based on this observation, we propose a novel generalizable framework (MitDet)
for mitosis detection. The data diversity is considered by the proposed
diversity-guided sample balancing (DGSB). And the feature diversity is
preserved by inter- and intra- class feature diversity-preserved module
(InCDP). Stain enhancement (SE) module is introduced to enhance the
domain-relevant diversity of both data and features simultaneously. Extensive
experiments have demonstrated that our proposed model outperforms all the SOTA
approaches in several popular mitosis detection datasets in both internal and
external test sets using minimal annotation efforts with point annotations
only. Comprehensive ablation studies have also proven the effectiveness of the
rethinking of data and feature diversity balancing. By analyzing the results
quantitatively and qualitatively, we believe that our proposed model not only
achieves SOTA performance but also might inspire the future studies in new
perspectives. Source code is at https://github.com/Onehour0108/MitDet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiatai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingchao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xipeng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huadeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingbing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Changhong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1&quot;&gt;Guoqiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Li Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05890">
<title>FreeSeed: Frequency-band-aware and Self-guided Network for Sparse-view CT Reconstruction. (arXiv:2307.05890v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05890</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse-view computed tomography (CT) is a promising solution for expediting
the scanning process and mitigating radiation exposure to patients, the
reconstructed images, however, contain severe streak artifacts, compromising
subsequent screening and diagnosis. Recently, deep learning-based image
post-processing methods along with their dual-domain counterparts have shown
promising results. However, existing methods usually produce over-smoothed
images with loss of details due to (1) the difficulty in accurately modeling
the artifact patterns in the image domain, and (2) the equal treatment of each
pixel in the loss function. To address these issues, we concentrate on the
image post-processing and propose a simple yet effective FREquency-band-awarE
and SElf-guidED network, termed FreeSeed, which can effectively remove artifact
and recover missing detail from the contaminated sparse-view CT images.
Specifically, we first propose a frequency-band-aware artifact modeling network
(FreeNet), which learns artifact-related frequency-band attention in Fourier
domain for better modeling the globally distributed streak artifact on the
sparse-view CT images. We then introduce a self-guided artifact refinement
network (SeedNet), which leverages the predicted artifact to assist FreeNet in
continuing to refine the severely corrupted details. Extensive experiments
demonstrate the superior performance of FreeSeed and its dual-domain
counterpart over the state-of-the-art sparse-view CT reconstruction methods.
Source code is made available at https://github.com/Masaaki-75/freeseed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chenglong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zilong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05892">
<title>SC-NeuS: Consistent Neural Surface Reconstruction from Sparse and Noisy Views. (arXiv:2307.05892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05892</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent neural surface reconstruction by volume rendering approaches have
made much progress by achieving impressive surface reconstruction quality, but
are still limited to dense and highly accurate posed views. To overcome such
drawbacks, this paper pays special attention on the consistent surface
reconstruction from sparse views with noisy camera poses. Unlike previous
approaches, the key difference of this paper is to exploit the multi-view
constraints directly from the explicit geometry of the neural surface, which
can be used as effective regularization to jointly learn the neural surface and
refine the camera poses. To build effective multi-view constraints, we
introduce a fast differentiable on-surface intersection to generate on-surface
points, and propose view-consistent losses based on such differentiable points
to regularize the neural surface learning. Based on this point, we propose a
jointly learning strategy for neural surface and camera poses, named SC-NeuS,
to perform geometry-consistent surface reconstruction in an end-to-end manner.
With extensive evaluation on public datasets, our SC-NeuS can achieve
consistently better surface reconstruction results with fine-grained details
than previous state-of-the-art neural surface reconstruction approaches,
especially from sparse and noisy camera views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shi-Sheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zi-Xin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi-Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05896">
<title>Deep learning-based estimation of whole-body kinematics from multi-view images. (arXiv:2307.05896v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05896</link>
<description rdf:parseType="Literal">&lt;p&gt;It is necessary to analyze the whole-body kinematics (including joint
locations and joint angles) to assess risks of fatal and musculoskeletal
injuries in occupational tasks. Human pose estimation has gotten more attention
in recent years as a method to minimize the errors in determining joint
locations. However, the joint angles are not often estimated, nor is the
quality of joint angle estimation assessed. In this paper, we presented an
end-to-end approach on direct joint angle estimation from multi-view images.
Our method leveraged the volumetric pose representation and mapped the rotation
representation to a continuous space where each rotation was uniquely
represented. We also presented a new kinematic dataset in the domain of
residential roofing with a data processing pipeline to generate necessary
annotations for the supervised training procedure on direct joint angle
estimation. We achieved a mean angle error of $7.19^\circ$ on the new Roofing
dataset and $8.41^\circ$ on the Human3.6M dataset, paving the way for
employment of on-site kinematic analysis using multi-view images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Kien X. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Liying Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawke_A/0/1/0/all/0/1&quot;&gt;Ashley L. Hawke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carey_R/0/1/0/all/0/1&quot;&gt;Robert E. Carey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breloff_S/0/1/0/all/0/1&quot;&gt;Scott P. Breloff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05898">
<title>Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation. (arXiv:2307.05898v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05898</link>
<description rdf:parseType="Literal">&lt;p&gt;Noisy label problems are inevitably in existence within medical image
segmentation causing severe performance degradation. Previous segmentation
methods for noisy label problems only utilize a single image while the
potential of leveraging the correlation between images has been overlooked.
Especially for video segmentation, adjacent frames contain rich contextual
information beneficial in cognizing noisy labels. Based on two insights, we
propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to
resolve noisy-labeled medical video segmentation issues. First, we argue the
sequential prior of videos is an effective reference, i.e., pixel-level
features from adjacent frames are close in distance for the same class and far
in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is
devised to indicate possible noisy labels by evaluating the affinity between
pixels in two adjacent frames. We also notice that the noise distribution
exhibits considerable variations across video, image, and pixel levels. In this
way, we introduce Multi-Scale Supervision (MSS) to supervise the network from
three different perspectives by re-weighting and refining the samples. This
design enables the network to concentrate on clean samples in a coarse-to-fine
manner. Experiments with both synthetic and real-world label noise demonstrate
that our method outperforms recent state-of-the-art robust segmentation
approaches. Code is available at https://github.com/BeileiCui/MS-TFAL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Beilei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengya Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;An Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05899">
<title>DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation. (arXiv:2307.05899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05899</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion probabilistic models (DPMs) have shown remarkable results on
various image synthesis tasks such as text-to-image generation and image
inpainting. However, compared to other generative methods like VAEs and GANs,
DPMs lack a low-dimensional, interpretable, and well-decoupled latent code.
Recently, diffusion autoencoders (Diff-AE) were proposed to explore the
potential of DPMs for representation learning via autoencoding. Diff-AE
provides an accessible latent space that exhibits remarkable interpretability,
allowing us to manipulate image attributes based on latent codes from the
space. However, previous works are not generic as they only operated on a few
limited attributes. To further explore the latent space of Diff-AE and achieve
a generic editing pipeline, we proposed a module called Group-supervised
AutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the
latent code. Our proposed GAE has trained via an attribute-swap strategy to
acquire the latent codes for multi-attribute image manipulation based on
examples. We empirically demonstrate that our method enables
multiple-attributes manipulation and achieves convincing sample quality and
attribute alignments, while significantly reducing computational requirements
compared to pixel-based approaches for representational decoupling. Code will
be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1&quot;&gt;Yipeng Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiangjuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05901">
<title>Single Domain Generalization via Normalised Cross-correlation Based Convolutions. (arXiv:2307.05901v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05901</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning techniques often perform poorly in the presence of domain
shift, where the test data follows a different distribution than the training
data. The most practically desirable approach to address this issue is Single
Domain Generalization (S-DG), which aims to train robust models using data from
a single source. Prior work on S-DG has primarily focused on using data
augmentation techniques to generate diverse training data. In this paper, we
explore an alternative approach by investigating the robustness of linear
operators, such as convolution and dense layers commonly used in deep learning.
We propose a novel operator called XCNorm that computes the normalized
cross-correlation between weights and an input feature patch. This approach is
invariant to both affine shifts and changes in energy within a local feature
patch and eliminates the need for commonly used non-linear activation
functions. We show that deep neural networks composed of this operator are
robust to common semantic distribution shifts. Furthermore, our empirical
results on single-domain generalization benchmarks demonstrate that our
proposed technique performs comparably to the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1&quot;&gt;WeiQin Chuah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1&quot;&gt;Ruwan Tennakoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoseinnezhad_R/0/1/0/all/0/1&quot;&gt;Reza Hoseinnezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1&quot;&gt;David Suter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1&quot;&gt;Alireza Bab-Hadiashar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05913">
<title>Close-up View synthesis by Interpolating Optical Flow. (arXiv:2307.05913v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05913</link>
<description rdf:parseType="Literal">&lt;p&gt;The virtual viewpoint is perceived as a new technique in virtual navigation,
as yet not supported due to the lack of depth information and obscure camera
parameters. In this paper, a method for achieving close-up virtual view is
proposed and it only uses optical flow to build parallax effects to realize
pseudo 3D projection without using depth sensor. We develop a bidirectional
optical flow method to obtain any virtual viewpoint by proportional
interpolation of optical flow. Moreover, with the ingenious application of the
optical-flow-value, we achieve clear and visual-fidelity magnified results
through lens stretching in any corner, which overcomes the visual distortion
and image blur through viewpoint magnification and transition in Google Street
View system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xinyi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hong Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05916">
<title>SwiFT: Swin 4D fMRI Transformer. (arXiv:2307.05916v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05916</link>
<description rdf:parseType="Literal">&lt;p&gt;The modeling of spatiotemporal brain dynamics from high-dimensional data,
such as 4D functional MRI, is a formidable task in neuroscience. To address
this challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer
architecture that can learn brain dynamics directly from 4D functional brain
MRI data in a memory and computation-efficient manner. SwiFT achieves this by
implementing a 4D window multi-head self-attention mechanism and absolute
positional embeddings. We evaluate SwiFT using multiple largest-scale human
functional brain imaging datasets in tasks such as predicting sex, age, and
cognitive intelligence. Our experimental outcomes reveal that SwiFT
consistently outperforms recent state-of-the-art models. To the best of our
knowledge, SwiFT is the first Swin Transformer architecture that can process
dimensional spatiotemporal brain functional data in an end-to-end fashion.
Furthermore, due to the end-to-end learning capability, we also show that
contrastive loss-based self-supervised pre-training of SwiFT is also feasible
for achieving improved performance on a downstream task. We believe that our
work holds substantial potential in facilitating scalable learning of
functional brain imaging in neuroscience research by reducing the hurdles
associated with applying Transformer models to high-dimensional fMRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Peter Yongho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1&quot;&gt;Junbeom Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joo_S/0/1/0/all/0/1&quot;&gt;Sunghwan Joo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Sangyoon Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donggyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1&quot;&gt;Yoonho Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1&quot;&gt;Jiook Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1&quot;&gt;Taesup Moon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05920">
<title>Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt. (arXiv:2307.05920v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05920</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive language-image Pre-training (CLIP) [13] can leverage large
datasets of unlabeled Image-Text pairs, which have demonstrated impressive
performance in various downstream tasks. Given that annotating medical data is
time-consuming and laborious, Image-Text Pre-training has promising
applications in exploiting large-scale medical image and radiology report
datasets. However, medical Image-Text Pre-training faces several challenges, as
follows: (1) Due to privacy concerns, the amount of available medical data is
relatively small compared to natural data, leading to weaker generalization
ability of the model. (2) Medical images are highly similar with only
fine-grained differences in subtleties, resulting in a large number of
false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt
usually differs from the natural medical image report, Subtle changes in
wording can lead to significant differences in performance. In this paper, we
propose a unified Image-Text-Label contrastive learning framework based on
continuous prompts, with three main contributions. First, We unified the data
of images, text, and labels, which greatly expanded the training data that the
model could utilize. Second, we address the issue of data diversity and the
impact of hand-crafted prompts on model performance by introducing continuous
implicit prompts. Lastly, we propose a ImageText-Label contrastive Training to
mitigate the problem of too many false-negative samples. We demonstrate through
sufficient experiments that the Unified Medical Contrastive Learning (UMCL)
framework exhibits excellent performance on several downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05921">
<title>Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05921</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05929">
<title>A New Dataset and Comparative Study for Aphid Cluster Detection. (arXiv:2307.05929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05929</link>
<description rdf:parseType="Literal">&lt;p&gt;Aphids are one of the main threats to crops, rural families, and global food
security. Chemical pest control is a necessary component of crop production for
maximizing yields, however, it is unnecessary to apply the chemical approaches
to the entire fields in consideration of the environmental pollution and the
cost. Thus, accurately localizing the aphid and estimating the infestation
level is crucial to the precise local application of pesticides. Aphid
detection is very challenging as each individual aphid is really small and all
aphids are crowded together as clusters. In this paper, we propose to estimate
the infection level by detecting aphid clusters. We have taken millions of
images in the sorghum fields, manually selected 5,447 images that contain
aphids, and annotated each aphid cluster in the image. To use these images for
machine learning models, we crop the images into patches and created a labeled
dataset with over 151,000 image patches. Then, we implement and compare the
performance of four state-of-the-art object detection models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kaidong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1&quot;&gt;Cuncong Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teran_I/0/1/0/all/0/1&quot;&gt;Ivan Grijalva Teran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCornack_B/0/1/0/all/0/1&quot;&gt;Brian McCornack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flippo_D/0/1/0/all/0/1&quot;&gt;Daniel Flippo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharda_A/0/1/0/all/0/1&quot;&gt;Ajay Sharda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanghui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05934">
<title>Sem-CS: Semantic CLIPStyler for Text-Based Image Style Transfer. (arXiv:2307.05934v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05934</link>
<description rdf:parseType="Literal">&lt;p&gt;CLIPStyler demonstrated image style transfer with realistic textures using
only a style text description (instead of requiring a reference style image).
However, the ground semantics of objects in the style transfer output is lost
due to style spill-over on salient and background objects (content mismatch) or
over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS), that
performs semantic style transfer. Sem-CS first segments the content image into
salient and non-salient objects and then transfers artistic style based on a
given style text description. The semantic style transfer is achieved using
global foreground loss (for salient objects) and global background loss (for
non-salient objects). Our empirical results, including DISTS, NIMA and user
study scores, show that our proposed framework yields superior qualitative and
quantitative performance. Our code is available at
github.com/chandagrover/sem-cs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamra_C/0/1/0/all/0/1&quot;&gt;Chanda Grover Kamra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mastan_I/0/1/0/all/0/1&quot;&gt;Indra Deep Mastan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Debayan Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05942">
<title>Prototypical Contrastive Transfer Learning for Multimodal Language Understanding. (arXiv:2307.05942v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.05942</link>
<description rdf:parseType="Literal">&lt;p&gt;Although domestic service robots are expected to assist individuals who
require support, they cannot currently interact smoothly with people through
natural language. For example, given the instruction &quot;Bring me a bottle from
the kitchen,&quot; it is difficult for such robots to specify the bottle in an
indoor environment. Most conventional models have been trained on real-world
datasets that are labor-intensive to collect, and they have not fully leveraged
simulation data through a transfer learning framework. In this study, we
propose a novel transfer learning approach for multimodal language
understanding called Prototypical Contrastive Transfer Learning (PCTL), which
uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task
of identifying target objects in domestic environments according to free-form
natural language instructions. To validate PCTL, we built new real-world and
simulation datasets. Our experiment demonstrated that PCTL outperformed
existing methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas
simple fine-tuning achieved an accuracy of 73.4%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otsuki_S/0/1/0/all/0/1&quot;&gt;Seitaro Otsuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1&quot;&gt;Shintaro Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1&quot;&gt;Komei Sugiura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05945">
<title>YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention. (arXiv:2307.05945v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05945</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce YOGA, a deep learning based yet lightweight object detection
model that can operate on low-end edge devices while still achieving
competitive accuracy. The YOGA architecture consists of a two-phase feature
learning pipeline with a cheap linear transformation, which learns feature maps
using only half of the convolution filters required by conventional
convolutional neural networks. In addition, it performs multi-scale feature
fusion in its neck using an attention mechanism instead of the naive
concatenation used by conventional detectors. YOGA is a flexible model that can
be easily scaled up or down by several orders of magnitude to fit a broad range
of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets
with other over 10 state-of-the-art object detectors. The results show that
YOGA strikes the best trade-off between model size and accuracy (up to 22%
increase of AP and 23-34% reduction of parameters and FLOPs), making it an
ideal choice for deployment in the wild on low-end edge devices. This is
further affirmed by our hardware implementation and evaluation on NVIDIA Jetson
Nano.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkara_R/0/1/0/all/0/1&quot;&gt;Raja Sunkara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05963">
<title>GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation. (arXiv:2307.05963v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.05963</link>
<description rdf:parseType="Literal">&lt;p&gt;Language-Guided Robotic Manipulation (LGRM) is a challenging task as it
requires a robot to understand human instructions to manipulate everyday
objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG)
models to detect objects without adapting to manipulation environments. This
results in a performance drop due to a substantial domain gap between the
pre-training and real-world data. A straightforward solution is to collect
additional training data, but the cost of human-annotation is extortionate. In
this paper, we propose Grounding Vision to Ceaselessly Created Instructions
(GVCCI), a lifelong learning framework for LGRM, which continuously learns VG
without human supervision. GVCCI iteratively generates synthetic instruction
via object detection and trains the VG model with the generated data. We
validate our framework in offline and online settings across diverse
environments on different VG models. Experimental results show that
accumulating synthetic data from GVCCI leads to a steady improvement in VG by
up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the
qualitative analysis shows that the unadapted VG model often fails to find
correct objects due to a strong bias learned from the pre-training data.
Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k
triplets of image-object-instruction from diverse manipulation environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Gi-Cheon Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaein Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Suyeon Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05973">
<title>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.05973</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunzhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05977">
<title>Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05977</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Seohyeon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Balhae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Moonseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05979">
<title>Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05979</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1&quot;&gt;Pranav Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Aamer Abdul Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+St_Charles_P/0/1/0/all/0/1&quot;&gt;Pierre-Luc St-Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prince_S/0/1/0/all/0/1&quot;&gt;Simon J.D. Prince&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1&quot;&gt;Samira Ebrahimi Kahou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05999">
<title>Flexible and Fully Quantized Ultra-Lightweight TinyissimoYOLO for Ultra-Low-Power Edge Systems. (arXiv:2307.05999v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05999</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper deploys and explores variants of TinyissimoYOLO, a highly flexible
and fully quantized ultra-lightweight object detection network designed for
edge systems with a power envelope of a few milliwatts. With experimental
measurements, we present a comprehensive characterization of the network&apos;s
detection performance, exploring the impact of various parameters, including
input resolution, number of object classes, and hidden layer adjustments. We
deploy variants of TinyissimoYOLO on state-of-the-art ultra-low-power extreme
edge platforms, presenting an in-depth a comparison on latency, energy
efficiency, and their ability to efficiently parallelize the workload. In
particular, the paper presents a comparison between a novel parallel RISC-V
processor (GAP9 from Greenwaves) with and without use of its on-chip hardware
accelerator, an ARM Cortex-M7 core (STM32H7 from ST Microelectronics), two ARM
Cortex-M4 cores (STM32L4 from STM and Apollo4b from Ambiq), and a multi-core
platform with a CNN hardware accelerator (Analog Devices MAX78000).
Experimental results show that the GAP9&apos;s hardware accelerator achieves the
lowest inference latency and energy at 2.12ms and 150uJ respectively, which is
around 2x faster and 20% more efficient than the next best platform, the
MAX78000. The hardware accelerator of GAP9 can even run an increased resolution
version of TinyissimoYOLO with 112x112 pixels and 10 detection classes within
3.2ms, consuming 245uJ. To showcase the competitiveness of a versatile
general-purpose system we also deployed and profiled a multi-core
implementation on GAP9 at different operating points, achieving 11.3ms with the
lowest-latency and 490uJ with the most energy-efficient configuration. With
this paper, we demonstrate the suitability and flexibility of TinyissimoYOLO on
state-of-the-art detection datasets for real-time ultra-low-power edge
inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_H/0/1/0/all/0/1&quot;&gt;Hanna Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmerman_N/0/1/0/all/0/1&quot;&gt;Nicky Zimmerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutishauser_G/0/1/0/all/0/1&quot;&gt;Georg Rutishauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06003">
<title>Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera. (arXiv:2307.06003v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06003</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently selecting an appropriate spike stream data length to extract
precise information is the key to the spike vision tasks. To address this
issue, we propose a dynamic timing representation for spike streams. Based on
multi-layers architecture, it applies dilated convolutions on temporal
dimension to extract features on multi-temporal scales with few parameters. And
we design layer attention to dynamically fuse these features. Moreover, we
propose an unsupervised learning method for optical flow estimation in a
spike-based manner to break the dependence on labeled data. In addition, to
verify the robustness, we also build a spike-based synthetic validation dataset
for extreme scenarios in autonomous driving, denoted as SSES dataset. It
consists of various corner cases. Experiments show that our method can predict
optical flow from spike streams in different high-speed scenes, including real
scenes. For instance, our method gets $15\%$ and $19\%$ error reduction from
the best spike-based work, SCFlow, in $\Delta t=10$ and $\Delta t=20$
respectively which are the same settings as the previous works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lujie Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Ziluo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Ruiqin Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06006">
<title>What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation. (arXiv:2307.06006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06006</link>
<description rdf:parseType="Literal">&lt;p&gt;The pretrain-finetune paradigm usually improves downstream performance over
training a model from scratch on the same task, becoming commonplace across
many areas of machine learning. While pretraining is empirically observed to be
beneficial for a range of tasks, there is not a clear understanding yet of the
reasons for this effect. In this work, we examine the relationship between
pretrained vision transformers and the corresponding finetuned versions on
several benchmark datasets and tasks. We present new metrics that specifically
investigate the degree to which invariances learned by a pretrained model are
retained or forgotten during finetuning. Using these metrics, we present a
suite of empirical findings, including that pretraining induces transferable
invariances in shallow layers and that invariances from deeper pretrained
layers are compressed towards shallower layers during finetuning. Together,
these findings contribute to understanding some of the reasons for the
successes of pretrained models and the changes that a pretrained model
undergoes when finetuned on a downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merlin_G/0/1/0/all/0/1&quot;&gt;Gabriele Merlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_V/0/1/0/all/0/1&quot;&gt;Vedant Nanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1&quot;&gt;Ruchit Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1&quot;&gt;Mariya Toneva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06026">
<title>Learning from Exemplary Explanations. (arXiv:2307.06026v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06026</link>
<description rdf:parseType="Literal">&lt;p&gt;eXplanation Based Learning (XBL) is a form of Interactive Machine Learning
(IML) that provides a model refining approach via user feedback collected on
model explanations. Although the interactivity of XBL promotes model
transparency, XBL requires a huge amount of user interaction and can become
expensive as feedback is in the form of detailed annotation rather than simple
category labelling which is more common in IML. This expense is exacerbated in
high stakes domains such as medical image classification. To reduce the effort
and expense of XBL we introduce a new approach that uses two input instances
and their corresponding Gradient Weighted Class Activation Mapping (GradCAM)
model explanations as exemplary explanations to implement XBL. Using a medical
image classification task, we demonstrate that, using minimal human input, our
approach produces improved explanations (+0.02, +3%) and achieves reduced
classification performance (-0.04, -4%) when compared against a model trained
without interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06038">
<title>Pyramid Deep Fusion Network for Two-Hand Reconstruction from RGB-D Images. (arXiv:2307.06038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06038</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately recovering the dense 3D mesh of both hands from monocular images
poses considerable challenges due to occlusions and projection ambiguity. Most
of the existing methods extract features from color images to estimate the
root-aligned hand meshes, which neglect the crucial depth and scale information
in the real world. Given the noisy sensor measurements with limited resolution,
depth-based methods predict 3D keypoints rather than a dense mesh. These
limitations motivate us to take advantage of these two complementary inputs to
acquire dense hand meshes on a real-world scale. In this work, we propose an
end-to-end framework for recovering dense meshes for both hands, which employ
single-view RGB-D image pairs as input. The primary challenge lies in
effectively utilizing two different input modalities to mitigate the blurring
effects in RGB images and noises in depth images. Instead of directly treating
depth maps as additional channels for RGB images, we encode the depth
information into the unordered point cloud to preserve more geometric details.
Specifically, our framework employs ResNet50 and PointNet++ to derive features
from RGB and point cloud, respectively. Additionally, we introduce a novel
pyramid deep fusion network (PDFNet) to aggregate features at different scales,
which demonstrates superior efficacy compared to previous fusion strategies.
Furthermore, we employ a GCN-based decoder to process the fused features and
recover the corresponding 3D pose and dense mesh. Through comprehensive
ablation experiments, we have not only demonstrated the effectiveness of our
proposed fusion algorithm but also outperformed the state-of-the-art approaches
on publicly available datasets. To reproduce the results, we will make our
source code and models publicly available at
{\url{https://github.com/zijinxuxu/PDFNet}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jinwei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jianke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06052">
<title>Visualization for Multivariate Gaussian Anomaly Detection in Images. (arXiv:2307.06052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06052</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly
Detection through Instance Modeling) method for anomaly detection in images,
fitting a single multivariate Gaussian (MVG) distribution to the feature
vectors extracted from a backbone convolutional neural network (CNN) and using
their Mahalanobis distance as the anomaly score. We introduce an intermediate
step in this framework by applying a whitening transformation to the feature
vectors, which enables the generation of heatmaps capable of visually
explaining the features learned by the MVG. The proposed technique is evaluated
on the MVTec-AD dataset, and the results show the importance of visual model
validation, providing insights into issues in this framework that were
otherwise invisible. The visualizations generated for this paper are publicly
available at https://doi.org/10.5281/zenodo.7937978.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Joao P C Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arrustico_D/0/1/0/all/0/1&quot;&gt;David Arrustico&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06065">
<title>Operational Support Estimator Networks. (arXiv:2307.06065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06065</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel approach called Operational Support
Estimator Networks (OSENs) for the support estimation task. Support Estimation
(SE) is defined as finding the locations of non-zero elements in a sparse
signal. By its very nature, the mapping between the measurement and sparse
signal is a non-linear operation. Traditional support estimators rely on
computationally expensive iterative signal recovery techniques to achieve such
non-linearity. Contrary to the convolution layers, the proposed OSEN approach
consists of operational layers that can learn such complex non-linearities
without the need for deep networks. In this way, the performance of the
non-iterative support estimation is greatly improved. Moreover, the operational
layers comprise so-called generative \textit{super neurons} with non-local
kernels. The kernel location for each neuron/feature map is optimized jointly
for the SE task during the training. We evaluate the OSENs in three different
applications: i. support estimation from Compressive Sensing (CS) measurements,
ii. representation-based classification, and iii. learning-aided CS
reconstruction where the output of OSENs is used as prior knowledge to the CS
algorithm for an enhanced reconstruction. Experimental results show that the
proposed approach achieves computational efficiency and outperforms competing
methods, especially at low measurement rates by a significant margin. The
software implementation is publicly shared at
https://github.com/meteahishali/OSEN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1&quot;&gt;Mete Ahishali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1&quot;&gt;Mehmet Yamac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06082">
<title>VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06082</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schumann_R/0/1/0/all/0/1&quot;&gt;Raphael Schumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Weixi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tsu-Jui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1&quot;&gt;Stefan Riezler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06091">
<title>AICT: An Adaptive Image Compression Transformer. (arXiv:2307.06091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06091</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the efficiency investigation of the Tranformer-based transform
coding framework, namely SwinT-ChARM, we propose to enhance the latter, as
first, with a more straightforward yet effective Tranformer-based channel-wise
auto-regressive prior model, resulting in an absolute image compression
transformer (ICT). Current methods that still rely on ConvNet-based entropy
coding are limited in long-range modeling dependencies due to their local
connectivity and an increasing number of architectural biases and priors. On
the contrary, the proposed ICT can capture both global and local contexts from
the latent representations and better parameterize the distribution of the
quantized latents. Further, we leverage a learnable scaling module with a
sandwich ConvNeXt-based pre/post-processor to accurately extract more compact
latent representation while reconstructing higher-quality images. Extensive
experimental results on benchmark datasets showed that the proposed adaptive
image compression transformer (AICT) framework significantly improves the
trade-off between coding efficiency and decoder complexity over the versatile
video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbel_A/0/1/0/all/0/1&quot;&gt;Ahmed Ghorbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1&quot;&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1&quot;&gt;Luce Morin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06099">
<title>RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation. (arXiv:2307.06099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06099</link>
<description rdf:parseType="Literal">&lt;p&gt;Glass-like objects are widespread in daily life but remain intractable to be
segmented for most existing methods. The transparent property makes it
difficult to be distinguished from background, while the tiny separation
boundary further impedes the acquisition of their exact contour. In this paper,
by revealing the key co-evolution demand of semantic and boundary learning, we
propose a Selective Mutual Evolution (SME) module to enable the reciprocal
feature learning between them. Then to exploit the global shape context, we
propose a Structurally Attentive Refinement (SAR) module to conduct a
fine-grained feature refinement for those ambiguous points around the boundary.
Finally, to further utilize the multi-scale representation, we integrate the
above two modules into a cascaded structure and then introduce a Reciprocal
Feature Evolution Network (RFENet) for effective glass-like object
segmentation. Extensive experiments demonstrate that our RFENet achieves
state-of-the-art performance on three popular public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1&quot;&gt;Ke Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yabiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06118">
<title>TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image. (arXiv:2307.06118v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06118</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic tree density estimation and counting using single aerial and
satellite images is a challenging task in photogrammetry and remote sensing,
yet has an important role in forest management. In this paper, we propose the
first semisupervised transformer-based framework for tree counting which
reduces the expensive tree annotations for remote sensing images. Our method,
termed as TreeFormer, first develops a pyramid tree representation module based
on transformer blocks to extract multi-scale features during the encoding
stage. Contextual attention-based feature fusion and tree density regressor
modules are further designed to utilize the robust features from the encoder to
estimate tree density maps in the decoder. Moreover, we propose a pyramid
learning strategy that includes local tree density consistency and local tree
count ranking losses to utilize unlabeled images into the training process.
Finally, the tree counter token is introduced to regulate the network by
computing the global tree counts for both labeled and unlabeled images. Our
model was evaluated on two benchmark tree counting datasets, Jiangsu, and
Yosemite, as well as a new dataset, KCL-London, created by ourselves. Our
TreeFormer outperforms the state of the art semi-supervised methods under the
same setting and exceeds the fully-supervised methods using the same number of
labeled images. The codes and datasets are available at
https://github.com/HAAClassic/TreeFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amirkolaee_H/0/1/0/all/0/1&quot;&gt;Hamed Amini Amirkolaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mulligan_M/0/1/0/all/0/1&quot;&gt;Mark Mulligan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06120">
<title>Recognizing student identification numbers from the matrix templates using a modified U-net architecture. (arXiv:2307.06120v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06120</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an innovative approach to student identification during
exams and knowledge tests, which overcomes the limitations of the traditional
personal information entry method. The proposed method employs a matrix
template on the designated section of the exam, where squares containing
numbers are selectively blackened. The methodology involves the development of
a neural network specifically designed for recognizing students&apos; personal
identification numbers. The neural network utilizes a specially adapted U-Net
architecture, trained on an extensive dataset comprising images of blackened
tables. The network demonstrates proficiency in recognizing the patterns and
arrangement of blackened squares, accurately interpreting the information
inscribed within them. Additionally, the model exhibits high accuracy in
correctly identifying entered student personal numbers and effectively
detecting erroneous entries within the table. This approach offers multiple
advantages. Firstly, it significantly accelerates the exam marking process by
automatically extracting identifying information from the blackened tables,
eliminating the need for manual entry and minimizing the potential for errors.
Secondly, the method automates the identification process, thereby reducing
administrative effort and expediting data processing. The introduction of this
innovative identification system represents a notable advancement in the field
of exams and knowledge tests, replacing the conventional manual entry of
personal data with a streamlined, efficient, and accurate identification
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavicic_F/0/1/0/all/0/1&quot;&gt;Filip Pavi&amp;#x10d;i&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06143">
<title>Learning Kernel-Modulated Neural Representation for Efficient Light Field Compression. (arXiv:2307.06143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06143</link>
<description rdf:parseType="Literal">&lt;p&gt;Light field is a type of image data that captures the 3D scene information by
recording light rays emitted from a scene at various orientations. It offers a
more immersive perception than classic 2D images but at the cost of huge data
volume. In this paper, we draw inspiration from the visual characteristics of
Sub-Aperture Images (SAIs) of light field and design a compact neural network
representation for the light field compression task. The network backbone takes
randomly initialized noise as input and is supervised on the SAIs of the target
light field. It is composed of two types of complementary kernels: descriptive
kernels (descriptors) that store scene description information learned during
training, and modulatory kernels (modulators) that control the rendering of
different SAIs from the queried perspectives. To further enhance compactness of
the network meanwhile retain high quality of the decoded light field, we
accordingly introduce modulator allocation and kernel tensor decomposition
mechanisms, followed by non-uniform quantization and lossless entropy coding
techniques, to finally form an efficient compression pipeline. Extensive
experiments demonstrate that our method outperforms other state-of-the-art
(SOTA) methods by a significant margin in the light field compression task.
Moreover, after aligning descriptors, the modulators learned from one light
field can be transferred to new light fields for rendering dense views,
indicating a potential solution for view synthesis task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jinglei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yihong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guillemot_C/0/1/0/all/0/1&quot;&gt;Christine Guillemot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06165">
<title>The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and Context Dataset. (arXiv:2307.06165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06165</link>
<description rdf:parseType="Literal">&lt;p&gt;Inner-city intersections are among the most critical traffic areas for injury
and fatal accidents. Automated vehicles struggle with the complex and hectic
everyday life within those areas. Sensor-equipped smart infrastructures, which
can cooperate with vehicles, can benefit automated traffic by extending the
perception capabilities of drivers and vehicle perception systems.
Additionally, they offer the opportunity to gather reproducible and precise
data of a holistic scene understanding, including context information as a
basis for training algorithms for various applications in automated traffic.
Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context
Dataset (IMPTC). We use an intelligent public inner-city intersection in
Germany with visual sensor technology. A multi-view camera and LiDAR system
perceives traffic situations and road users&apos; behavior. Additional sensors
monitor contextual information like weather, lighting, and traffic light signal
status. The data acquisition system focuses on Vulnerable Road Users (VRUs) and
multi-agent interaction. The resulting dataset consists of eight hours of
measurement data. It contains over 2,500 VRU trajectories, including
pedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, and
over 20,000 vehicle trajectories at different day times, weather conditions,
and seasons. In addition, to enable the entire stack of research capabilities,
the dataset includes all data, starting from the sensor-, calibration- and
detection data until trajectory and context data. The dataset is continuously
expanded and is available online for non-commercial research at
https://github.com/kav-institute/imptc-dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hetzel_M/0/1/0/all/0/1&quot;&gt;Manuel Hetzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichert_H/0/1/0/all/0/1&quot;&gt;Hannes Reichert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reitberger_G/0/1/0/all/0/1&quot;&gt;G&amp;#xfc;nther Reitberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_E/0/1/0/all/0/1&quot;&gt;Erich Fuchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06166">
<title>Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06166</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Models (VLMs) are expected to be capable of reasoning with
commonsense knowledge as human beings. One example is that humans can reason
where and when an image is taken based on their knowledge. This makes us wonder
if, based on visual cues, Vision-Language Models that are pre-trained with
large-scale image-text resources can achieve and even outperform human&apos;s
capability in reasoning times and location. To address this question, we
propose a two-stage \recognition\space and \reasoning\space probing task,
applied to discriminative and generative VLMs to uncover whether VLMs can
recognize times and location-relevant features and further reason about it. To
facilitate the investigation, we introduce WikiTiLo, a well-curated image
dataset compromising images with rich socio-cultural cues. In the extensive
experimental studies, we find that although VLMs can effectively retain
relevant features in visual encoders, they still fail to make perfect
reasoning. We will release our dataset and codes to facilitate future studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yurui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kerui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06177">
<title>Smart Infrastructure: A Research Junction. (arXiv:2307.06177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06177</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex inner-city junctions are among the most critical traffic areas for
injury and fatal accidents. The development of highly automated driving (HAD)
systems struggles with the complex and hectic everyday life within those areas.
Sensor-equipped smart infrastructures, which can communicate and cooperate with
vehicles, are essential to enable a holistic scene understanding to resolve
occlusions drivers and vehicle perception systems for themselves can not cover.
We introduce an intelligent research infrastructure equipped with visual sensor
technology, located at a public inner-city junction in Aschaffenburg, Germany.
A multiple-view camera system monitors the traffic situation to perceive road
users&apos; behavior. Both motorized and non-motorized traffic is considered. The
system is used for research in data generation, evaluating new HAD sensors
systems, algorithms, and Artificial Intelligence (AI) training strategies using
real-, synthetic- and augmented data. In addition, the junction features a
highly accurate digital twin. Real-world data can be taken into the digital
twin for simulation purposes and synthetic data generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hetzel_M/0/1/0/all/0/1&quot;&gt;Manuel Hetzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichert_H/0/1/0/all/0/1&quot;&gt;Hannes Reichert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1&quot;&gt;Konrad Doll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06179">
<title>Large Class Separation is not what you need for Relational Reasoning-based OOD Detection. (arXiv:2307.06179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06179</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard recognition approaches are unable to deal with novel categories at
test time. Their overconfidence on the known classes makes the predictions
unreliable for safety-critical applications such as healthcare or autonomous
driving. Out-Of-Distribution (OOD) detection methods provide a solution by
identifying semantic novelty. Most of these methods leverage a learning stage
on the known data, which means training (or fine-tuning) a model to capture the
concept of normality. This process is clearly sensitive to the amount of
available samples and might be computationally expensive for on-board systems.
A viable alternative is that of evaluating similarities in the embedding space
produced by large pre-trained models without any further learning effort. We
focus exactly on such a fine-tuning-free OOD detection setting. This works
presents an in-depth analysis of the recently introduced relational reasoning
pre-training and investigates the properties of the learned embedding,
highlighting the existence of a correlation between the inter-class feature
distance and the OOD detection accuracy. As the class separation depends on the
chosen pre-training objective, we propose an alternative loss function to
control the inter-class margin, and we show its advantage with thorough
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lorenzo Li Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAscenzi_G/0/1/0/all/0/1&quot;&gt;Giulia D&amp;#x27;Ascenzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1&quot;&gt;Francesco Cappio Borlino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1&quot;&gt;Tatiana Tommasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06182">
<title>CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification. (arXiv:2307.06182v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06182</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic examination of thin-prep cytologic test (TCT) slides can assist
pathologists in finding cervical abnormality for accurate and efficient cancer
screening. Current solutions mostly need to localize suspicious cells and
classify abnormality based on local patches, concerning the fact that whole
slide images of TCT are extremely large. It thus requires many annotations of
normal and abnormal cervical cells, to supervise the training of the
patch-level classifier for promising performance. In this paper, we propose
CellGAN to synthesize cytopathological images of various cervical cell types
for augmenting patch-level cell classification. Built upon a lightweight
backbone, CellGAN is equipped with a non-linear class mapping network to
effectively incorporate cell type information into image generation. We also
propose the Skip-layer Global Context module to model the complex spatial
relationship of the cells, and attain high fidelity of the synthesized images
through adversarial learning. Our experiments demonstrate that CellGAN can
produce visually plausible TCT cytopathological images for different cell
types. We also validate the effectiveness of using CellGAN to greatly augment
patch-level cell classification performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhenrong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Maosong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06206">
<title>SepVAE: a contrastive VAE to separate pathological patterns from healthy ones. (arXiv:2307.06206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06206</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders
(VAEs) that aims at separating the common factors of variation between a
background dataset (BG) (i.e., healthy subjects) and a target dataset (TG)
(i.e., patients) from the ones that only exist in the target dataset. To do so,
these methods separate the latent space into a set of salient features (i.e.,
proper to the target dataset) and a set of common features (i.e., exist in both
datasets). Currently, all models fail to prevent the sharing of information
between latent spaces effectively and to capture all salient factors of
variation. To this end, we introduce two crucial regularization losses: a
disentangling term between common and salient representations and a
classification term between background and target samples in the salient space.
We show a better performance than previous CA-VAEs methods on three medical
applications and a natural images dataset (CelebA). Code and datasets are
available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louiset_R/0/1/0/all/0/1&quot;&gt;Robin Louiset&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1&quot;&gt;Edouard Duchesnay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grigis_A/0/1/0/all/0/1&quot;&gt;Antoine Grigis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1&quot;&gt;Benoit Dufumier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1&quot;&gt;Pietro Gori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06233">
<title>On the Importance of Denoising when Learning to Compress Images. (arXiv:2307.06233v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06233</link>
<description rdf:parseType="Literal">&lt;p&gt;Image noise is ubiquitous in photography. However, image noise is not
compressible nor desirable, thus attempting to convey the noise in compressed
image bitstreams yields sub-par results in both rate and distortion. We propose
to explicitly learn the image denoising task when training a codec. Therefore,
we leverage the Natural Image Noise Dataset, which offers a wide variety of
scenes captured with various ISO numbers, leading to different noise levels,
including insignificant ones. Given this training set, we supervise the codec
with noisy-clean image pairs, and show that a single model trained based on a
mixture of images with variable noise levels appears to yield best-in-class
results with both noisy and clean images, achieving better rate-distortion than
a compression-only model or even than a pair of denoising-then-compression
models with almost one order of magnitude fewer GMac operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brummer_B/0/1/0/all/0/1&quot;&gt;Benoit Brummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vleeschouwer_C/0/1/0/all/0/1&quot;&gt;Christophe De Vleeschouwer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06260">
<title>UGCANet: A Unified Global Context-Aware Transformer-based Network with Feature Alignment for Endoscopic Image Analysis. (arXiv:2307.06260v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06260</link>
<description rdf:parseType="Literal">&lt;p&gt;Gastrointestinal endoscopy is a medical procedure that utilizes a flexible
tube equipped with a camera and other instruments to examine the digestive
tract. This minimally invasive technique allows for diagnosing and managing
various gastrointestinal conditions, including inflammatory bowel disease,
gastrointestinal bleeding, and colon cancer. The early detection and
identification of lesions in the upper gastrointestinal tract and the
identification of malignant polyps that may pose a risk of cancer development
are critical components of gastrointestinal endoscopy&apos;s diagnostic and
therapeutic applications. Therefore, enhancing the detection rates of
gastrointestinal disorders can significantly improve a patient&apos;s prognosis by
increasing the likelihood of timely medical intervention, which may prolong the
patient&apos;s lifespan and improve overall health outcomes. This paper presents a
novel Transformer-based deep neural network designed to perform multiple tasks
simultaneously, thereby enabling accurate identification of both upper
gastrointestinal tract lesions and colon polyps. Our approach proposes a unique
global context-aware module and leverages the powerful MiT backbone, along with
a feature alignment block, to enhance the network&apos;s representation capability.
This novel design leads to a significant improvement in performance across
various endoscopic diagnosis tasks. Extensive experiments demonstrate the
superior performance of our method compared to other state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_P/0/1/0/all/0/1&quot;&gt;Pham Vu Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manh_N/0/1/0/all/0/1&quot;&gt;Nguyen Duy Manh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oanh_N/0/1/0/all/0/1&quot;&gt;Nguyen Thi Oanh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1&quot;&gt;Nguyen Thi Thuy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_D/0/1/0/all/0/1&quot;&gt;Dinh Viet Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06272">
<title>Exposing the Fake: Effective Diffusion-Generated Images Detection. (arXiv:2307.06272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06272</link>
<description rdf:parseType="Literal">&lt;p&gt;Image synthesis has seen significant advancements with the advent of
diffusion-based generative models like Denoising Diffusion Probabilistic Models
(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a
dearth of research dedicated to detecting diffusion-generated images, which
could pose potential security and privacy risks. This paper addresses this gap
by proposing a novel detection method called Stepwise Error for
Diffusion-generated Image Detection (SeDID). Comprising statistical-based
$\text{SeDID}_{\text{Stat}}$ and neural network-based
$\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion
models, namely deterministic reverse and deterministic denoising computation
errors. Our evaluations demonstrate SeDID&apos;s superior performance over existing
methods when applied to diffusion models. Thus, our work makes a pivotal
contribution to distinguishing diffusion model-generated images, marking a
significant step in the domain of artificial intelligence security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruipeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fei Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaoshuang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06277">
<title>Stochastic Light Field Holography. (arXiv:2307.06277v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06277</link>
<description rdf:parseType="Literal">&lt;p&gt;The Visual Turing Test is the ultimate goal to evaluate the realism of
holographic displays. Previous studies have focused on addressing challenges
such as limited \&apos;etendue and image quality over a large focal volume, but they
have not investigated the effect of pupil sampling on the viewing experience in
full 3D holograms. In this work, we tackle this problem with a novel hologram
generation algorithm motivated by matching the projection operators of
incoherent Light Field and coherent Wigner Function light transport. To this
end, we supervise hologram computation using synthesized photographs, which are
rendered on-the-fly using Light Field refocusing from stochastically sampled
pupil states during optimization. The proposed method produces holograms with
correct parallax and focus cues, which are important for passing the Visual
Turing Test. We validate that our approach compares favorably to
state-of-the-art CGH algorithms that use Light Field and Focal Stack
supervision. Our experiments demonstrate that our algorithm significantly
improves the realism of the viewing experience for a variety of different pupil
states.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiffers_F/0/1/0/all/0/1&quot;&gt;Florian Schiffers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakravarthula_P/0/1/0/all/0/1&quot;&gt;Praneeth Chakravarthula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1&quot;&gt;Nathan Matsuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_G/0/1/0/all/0/1&quot;&gt;Grace Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_E/0/1/0/all/0/1&quot;&gt;Ethan Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanman_D/0/1/0/all/0/1&quot;&gt;Douglas Lanman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1&quot;&gt;Felix Heide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1&quot;&gt;Oliver Cossairt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06281">
<title>MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06281</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model&apos;s abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model&apos;s predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haodong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wangbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yike Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06298">
<title>Improved Real-time Image Smoothing with Weak Structures Preserved and High-contrast Details Removed. (arXiv:2307.06298v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06298</link>
<description rdf:parseType="Literal">&lt;p&gt;Image smoothing is by reducing pixel-wise gradients to smooth out details. As
existing methods always rely on gradients to determine smoothing manners, it is
difficult to distinguish structures and details to handle distinctively due to
the overlapped ranges of gradients for structures and details. Thus, it is
still challenging to achieve high-quality results, especially on preserving
weak structures and removing high-contrast details. In this paper, we address
this challenge by improving the real-time optimization-based method via
iterative least squares (called ILS). We observe that 1) ILS uses gradients as
the independent variable in its penalty function for determining smoothing
manners, and 2) the framework of ILS can still work for image smoothing when we
use some values instead of gradients in the penalty function. Thus,
corresponding to the properties of pixels on structures or not, we compute some
values to use in the penalty function to determine smoothing manners, and so we
can handle structures and details distinctively, no matter whether their
gradients are high or low. As a result, we can conveniently remove
high-contrast details while preserving weak structures. Moreover, such values
can be adjusted to accelerate optimization computation, so that we can use
fewer iterations than the original ILS method for efficiency. This also reduces
the changes onto structures to help structure preservation. Experimental
results show our advantages over existing methods on efficiency and quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengchun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wencheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1&quot;&gt;Fei Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06304">
<title>Patch n&apos; Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06304</link>
<description rdf:parseType="Literal">&lt;p&gt;The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1&quot;&gt;Josip Djolonga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heek_J/0/1/0/all/0/1&quot;&gt;Jonathan Heek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1&quot;&gt;Matthias Minderer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1&quot;&gt;Mathilde Caron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1&quot;&gt;Andreas Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1&quot;&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1&quot;&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliver_A/0/1/0/all/0/1&quot;&gt;Avital Oliver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padlewski_P/0/1/0/all/0/1&quot;&gt;Piotr Padlewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1&quot;&gt;Alexey Gritsenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1&quot;&gt;Mario Lu&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06307">
<title>Facial Reenactment Through a Personalized Generator. (arXiv:2307.06307v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06307</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the role of image generative models in facial reenactment
has been steadily increasing. Such models are usually subject-agnostic and
trained on domain-wide datasets. The appearance of the reenacted individual is
learned from a single image, and hence, the entire breadth of the individual&apos;s
appearance is not entirely captured, leading these methods to resort to
unfaithful hallucination. Thanks to recent advancements, it is now possible to
train a personalized generative model tailored specifically to a given
individual. In this paper, we propose a novel method for facial reenactment
using a personalized generator. We train the generator using frames from a
short, yet varied, self-scan video captured using a simple commodity camera.
Images synthesized by the personalized generator are guaranteed to preserve
identity. The premise of our work is that the task of reenactment is thus
reduced to accurately mimicking head poses and expressions. To this end, we
locate the desired frames in the latent space of the personalized generator
using carefully designed latent optimization. Through extensive evaluation, we
demonstrate state-of-the-art performance for facial reenactment. Furthermore,
we show that since our reenactment takes place in a semantic latent space, it
can be semantically edited and stylized in post-processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazary_A/0/1/0/all/0/1&quot;&gt;Ariel Elazary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1&quot;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06312">
<title>Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation. (arXiv:2307.06312v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06312</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning has become increasingly popular in medical image
segmentation due to its ability to leverage large amounts of unlabeled data to
extract additional information. However, most existing semi-supervised
segmentation methods only focus on extracting information from unlabeled data,
disregarding the potential of labeled data to further improve the performance
of the model. In this paper, we propose a novel Correlation Aware Mutual
Learning (CAML) framework that leverages labeled data to guide the extraction
of information from unlabeled data. Our approach is based on a mutual learning
strategy that incorporates two modules: the Cross-sample Mutual Attention
Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module
establishes dense cross-sample correlations among a group of samples, enabling
the transfer of label prior knowledge to unlabeled data. The OCC module
constructs omni-correlations between the unlabeled and labeled datasets and
regularizes dual models by constraining the omni-correlation matrix of each
sub-model to be consistent. Experiments on the Atrial Segmentation Challenge
dataset demonstrate that our proposed approach outperforms state-of-the-art
methods, highlighting the effectiveness of our framework in medical image
segmentation tasks. The codes, pre-trained weights, and data are publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shengbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiechao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06322">
<title>Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of &quot;Never Enough Training Data&quot;. (arXiv:2307.06322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06322</link>
<description rdf:parseType="Literal">&lt;p&gt;Crystalline defects, such as line-like dislocations, play an important role
for the performance and reliability of many metallic devices. Their interaction
and evolution still poses a multitude of open questions to materials science
and materials physics. In-situ TEM experiments can provide important insights
into how dislocations behave and move. During such experiments, the dislocation
microstructure is captured in form of videos. The analysis of individual video
frames can provide useful insights but is limited by the capabilities of
automated identification, digitization, and quantitative extraction of the
dislocations as curved objects. The vast amount of data also makes manual
annotation very time consuming, thereby limiting the use of Deep
Learning-based, automated image analysis and segmentation of the dislocation
microstructure. In this work, a parametric model for generating synthetic
training data for segmentation of dislocations is developed. Even though domain
scientists might dismiss synthetic training images sometimes as too artificial,
our findings show that they can result in superior performance, particularly
regarding the generalizing of the Deep Learning models with respect to
different microstructures and imaging conditions. Additionally, we propose an
enhanced deep learning method optimized for segmenting overlapping or
intersecting dislocation lines. Upon testing this framework on four distinct
real datasets, we find that our synthetic training data are able to yield
high-quality results also on real images-even more so if fine-tune on a few
real images was done.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govind_K/0/1/0/all/0/1&quot;&gt;Kishan Govind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveros_D/0/1/0/all/0/1&quot;&gt;Daniela Oliveros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dlouhy_A/0/1/0/all/0/1&quot;&gt;Antonin Dlouhy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legros_M/0/1/0/all/0/1&quot;&gt;Marc Legros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandfeld_S/0/1/0/all/0/1&quot;&gt;Stefan Sandfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.09035">
<title>Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.09035</link>
<description rdf:parseType="Literal">&lt;p&gt;A regularized version of Mixture Models is proposed to learn a principal
graph from a distribution of $D$-dimensional data points. In the particular
case of manifold learning for ridge detection, we assume that the underlying
manifold can be modeled as a graph structure acting like a topological prior
for the Gaussian clusters turning the problem into a maximum a posteriori
estimation. Parameters of the model are iteratively estimated through an
Expectation-Maximization procedure making the learning of the structure
computationally efficient with guaranteed convergence for any graph prior in a
polynomial time. We also embed in the formalism a natural way to make the
algorithm robust to outliers of the pattern and heteroscedasticity of the
manifold sampling coherently with the graph structure. The method uses a graph
prior given by the minimum spanning tree that we extend using random
sub-samplings of the dataset to take into account cycles that can be observed
in the spatial distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnaire_T/0/1/0/all/0/1&quot;&gt;Tony Bonnaire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghanim_N/0/1/0/all/0/1&quot;&gt;Nabila Aghanim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04104">
<title>Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models. (arXiv:2207.04104v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04104</link>
<description rdf:parseType="Literal">&lt;p&gt;A growing body of work studies Blindspot Discovery Methods (&quot;BDM&quot;s): methods
that use an image embedding to find semantically meaningful (i.e., united by a
human-understandable concept) subsets of the data where an image classifier
performs significantly worse. Motivated by observed gaps in prior work, we
introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic
image datasets to train models with known blindspots and a new BDM, PlaneSpot,
that uses a 2D image representation. We use SpotCheck to run controlled
experiments that identify factors that influence BDM performance (e.g., the
number of blindspots in a model, or features used to define the blindspot) and
show that PlaneSpot is competitive with and in many cases outperforms existing
BDMs. Importantly, we validate these findings by designing additional
experiments that use real image data from MS-COCO, a large image benchmark
dataset. Our findings suggest several promising directions for future work on
BDM design and evaluation. Overall, we hope that the methodology and analyses
presented in this work will help facilitate a more rigorous science of
blindspot discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1&quot;&gt;Gregory Plumb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_N/0/1/0/all/0/1&quot;&gt;Nari Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel Alexander Cabrera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1&quot;&gt;Ameet Talwalkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01346">
<title>ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions. (arXiv:2210.01346v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01346</link>
<description rdf:parseType="Literal">&lt;p&gt;3D human reconstruction from RGB images achieves decent results in good
weather conditions but degrades dramatically in rough weather. Complementary,
mmWave radars have been employed to reconstruct 3D human joints and meshes in
rough weather. However, combining RGB and mmWave signals for robust all-weather
3D human reconstruction is still an open challenge, given the sparse nature of
mmWave and the vulnerability of RGB images. In this paper, we present
ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies
in all weather conditions robustly. Specifically, our ImmFusion consists of
image and point backbones for token feature extraction and a Transformer module
for token fusion. The image and point backbones refine global and local
features from original data, and the Fusion Transformer Module aims for
effective information fusion of two modalities by dynamically selecting
informative tokens. Extensive experiments on a large-scale dataset, mmBody,
captured in various environments demonstrate that ImmFusion can efficiently
utilize the information of two modalities to achieve a robust 3D human body
reconstruction in all weather conditions. In addition, our method&apos;s accuracy is
significantly superior to that of state-of-the-art Transformer-based
LiDAR-camera fusion methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1&quot;&gt;Kun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shaohao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1&quot;&gt;Bin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yuchi Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qi Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01361">
<title>Uncertainty-Aware Lidar Place Recognition in Novel Environments. (arXiv:2210.01361v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01361</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art lidar place recognition models exhibit unreliable
performance when tested on environments different from their training dataset,
which limits their use in complex and evolving environments. To address this
issue, we investigate the task of uncertainty-aware lidar place recognition,
where each predicted place must have an associated uncertainty that can be used
to identify and reject incorrect predictions. We introduce a novel evaluation
protocol and present the first comprehensive benchmark for this task, testing
across five uncertainty estimation techniques and three large-scale datasets.
Our results show that an Ensembles approach is the highest performing
technique, consistently improving the performance of lidar place recognition
and uncertainty estimation in novel environments, though it incurs a
computational cost. Code is publicly available at
https://github.com/csiro-robotics/Uncertainty-LPR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mason_K/0/1/0/all/0/1&quot;&gt;Keita Mason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1&quot;&gt;Joshua Knights&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1&quot;&gt;Milad Ramezani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1&quot;&gt;Dimity Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09452">
<title>Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning. (arXiv:2210.09452v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09452</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning representations for individual instances when only bag-level labels
are available is a fundamental challenge in multiple instance learning (MIL).
Recent works have shown promising results using contrastive self-supervised
learning (CSSL), which learns to push apart representations corresponding to
two different randomly-selected instances. Unfortunately, in real-world
applications such as medical image classification, there is often class
imbalance, so randomly-selected instances mostly belong to the same majority
class, which precludes CSSL from learning inter-class differences. To address
this issue, we propose a novel framework, Iterative Self-paced Supervised
Contrastive Learning for MIL Representations (ItS2CLR), which improves the
learned representation by exploiting instance-level pseudo labels derived from
the bag-level labels. The framework employs a novel self-paced sampling
strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three
medical datasets, showing that it improves the quality of instance-level pseudo
labels and representations, and outperforms existing MIL methods in terms of
both bag and instance level accuracy. Code is available at
https://github.com/Kangningthu/ItS2CLR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Weicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1&quot;&gt;Narges Razavian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1&quot;&gt;Krzysztof J. Geras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1&quot;&gt;Carlos Fernandez-Granda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02760">
<title>Development and evaluation of automated localisation and reconstruction of all fruits on tomato plants in a greenhouse based on multi-view perception and 3D multi-object tracking. (arXiv:2211.02760v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02760</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to accurately represent and localise relevant objects is
essential for robots to carry out tasks effectively. Traditional approaches,
where robots simply capture an image, process that image to take an action, and
then forget the information, have proven to struggle in the presence of
occlusions. Methods using multi-view perception, which have the potential to
address some of these problems, require a world model that guides the
collection, integration and extraction of information from multiple viewpoints.
Furthermore, constructing a generic representation that can be applied in
various environments and tasks is a difficult challenge. In this paper, a novel
approach for building generic representations in occluded agro-food
environments using multi-view perception and 3D multi-object tracking is
introduced. The method is based on a detection algorithm that generates partial
point clouds for each detected object, followed by a 3D multi-object tracking
algorithm that updates the representation over time. The accuracy of the
representation was evaluated in a real-world environment, where successful
representation and localisation of tomatoes in tomato plants were achieved,
despite high levels of occlusion, with the total count of tomatoes estimated
with a maximum error of 5.08% and the tomatoes tracked with an accuracy up to
71.47%. Novel tracking metrics were introduced, demonstrating that valuable
insight into the errors in localising and representing the fruits can be
provided by their use. This approach presents a novel solution for building
representations in occluded agro-food environments, demonstrating potential to
enable robots to perform tasks effectively in these challenging environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rincon_D/0/1/0/all/0/1&quot;&gt;David Rapado Rincon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henten_E/0/1/0/all/0/1&quot;&gt;Eldert J. van Henten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kootstra_G/0/1/0/all/0/1&quot;&gt;Gert Kootstra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07625">
<title>What Images are More Memorable to Machines?. (arXiv:2211.07625v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07625</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of measuring and predicting how memorable an
image is to pattern recognition machines, as a path to explore machine
intelligence. Firstly, we propose a self-supervised machine memory
quantification pipeline, dubbed ``MachineMem measurer&apos;&apos;, to collect machine
memorability scores of images. Similar to humans, machines also tend to
memorize certain kinds of images, whereas the types of images that machines and
humans memorize are different. Through in-depth analysis and comprehensive
visualizations, we gradually unveil that``complex&quot; images are usually more
memorable to machines. We further conduct extensive experiments across 11
different machines (from linear classifiers to modern ViTs) and 9 pre-training
methods to analyze and understand machine memory. This work proposes the
concept of machine memorability and opens a new research direction at the
interface between machine memory and visual data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Junlin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Huangying Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jie Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1&quot;&gt;Pengfei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11220">
<title>STGlow: A Flow-based Generative Framework with Dual Graphormer for Pedestrian Trajectory Prediction. (arXiv:2211.11220v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11220</link>
<description rdf:parseType="Literal">&lt;p&gt;The pedestrian trajectory prediction task is an essential component of
intelligent systems. Its applications include but are not limited to autonomous
driving, robot navigation, and anomaly detection of monitoring systems. Due to
the diversity of motion behaviors and the complex social interactions among
pedestrians, accurately forecasting their future trajectory is challenging.
Existing approaches commonly adopt GANs or CVAEs to generate diverse
trajectories. However, GAN-based methods do not directly model data in a latent
space, which may make them fail to have full support over the underlying data
distribution; CVAE-based methods optimize a lower bound on the log-likelihood
of observations, which may cause the learned distribution to deviate from the
underlying distribution. The above limitations make existing approaches often
generate highly biased or inaccurate trajectories. In this paper, we propose a
novel generative flow based framework with dual graphormer for pedestrian
trajectory prediction (STGlow). Different from previous approaches, our method
can more precisely model the underlying data distribution by optimizing the
exact log-likelihood of motion behaviors. Besides, our method has clear
physical meanings for simulating the evolution of human motion behaviors. The
forward process of the flow gradually degrades complex motion behavior into
simple behavior, while its reverse process represents the evolution of simple
behavior into complex motion behavior. Further, we introduce a dual graphormer
combining with the graph structure to more adequately model the temporal
dependencies and the mutual spatial interactions. Experimental results on
several benchmarks demonstrate that our method achieves much better performance
compared to previous state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Rongqin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanman Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13466">
<title>Hierarchical Consistent Contrastive Learning for Skeleton-Based Action Recognition with Growing Augmentations. (arXiv:2211.13466v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13466</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has been proven beneficial for self-supervised
skeleton-based action recognition. Most contrastive learning methods utilize
carefully designed augmentations to generate different movement patterns of
skeletons for the same semantics. However, it is still a pending issue to apply
strong augmentations, which distort the images/skeletons&apos; structures and cause
semantic loss, due to their resulting unstable training. In this paper, we
investigate the potential of adopting strong augmentations and propose a
general hierarchical consistent contrastive learning framework (HiCLR) for
skeleton-based action recognition. Specifically, we first design a gradual
growing augmentation policy to generate multiple ordered positive pairs, which
guide to achieve the consistency of the learned representation from different
views. Then, an asymmetric loss is proposed to enforce the hierarchical
consistency via a directional clustering operation in the feature space,
pulling the representations from strongly augmented views closer to those from
weakly augmented views for better generalizability. Meanwhile, we propose and
evaluate three kinds of strong augmentations for 3D skeletons to demonstrate
the effectiveness of our method. Extensive experiments show that HiCLR
outperforms the state-of-the-art methods notably on three large-scale datasets,
i.e., NTU60, NTU120, and PKUMMD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lilang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02991">
<title>Proximal methods for point source localisation. (arXiv:2212.02991v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02991</link>
<description rdf:parseType="Literal">&lt;p&gt;Point source localisation is generally modelled as a Lasso-type problem on
measures. However, optimisation methods in non-Hilbert spaces, such as the
space of Radon measures, are much less developed than in Hilbert spaces. Most
numerical algorithms for point source localisation are based on the Frank-Wolfe
conditional gradient method, for which ad hoc convergence theory is developed.
We develop extensions of proximal-type methods to spaces of measures. This
includes forward-backward splitting, its inertial version, and primal-dual
proximal splitting. Their convergence proofs follow standard patterns. We
demonstrate their numerical efficacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Valkonen_T/0/1/0/all/0/1&quot;&gt;Tuomo Valkonen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03198">
<title>The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes. (arXiv:2301.03198v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03198</link>
<description rdf:parseType="Literal">&lt;p&gt;The sciences of biological and artificial intelligence are ever more
intertwined. Neural computational principles inspire new intelligent machines,
which are in turn used to advance theoretical understanding of the brain. To
promote further exchange of ideas and collaboration between biological and
artificial intelligence researchers, we introduce the 2023 installment of the
Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes
(&lt;a href=&quot;http://algonauts.csail.mit.edu&quot;&gt;this http URL&lt;/a&gt;). This installment prompts the fields of
artificial and biological intelligence to come together towards building
computational models of the visual brain using the largest and richest dataset
of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD
provides high-quality fMRI responses to ~73,000 different naturalistic colored
scenes, making it the ideal candidate for data-driven model building approaches
promoted by the 2023 challenge. The challenge is open to all and makes results
directly comparable and transparent through a public leaderboard automatically
updated after each submission, thus allowing for rapid model development. We
believe that the 2023 installment will spark symbiotic collaborations between
biological and artificial intelligence scientists, leading to a deeper
understanding of the brain through cutting-edge computational models and to
novel ways of engineering artificial intelligent agents through inductive
biases from biological systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gifford_A/0/1/0/all/0/1&quot;&gt;A. T. Gifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahner_B/0/1/0/all/0/1&quot;&gt;B. Lahner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_Sadiya_S/0/1/0/all/0/1&quot;&gt;S. Saba-Sadiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilas_M/0/1/0/all/0/1&quot;&gt;M. G. Vilas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lascelles_A/0/1/0/all/0/1&quot;&gt;A. Lascelles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1&quot;&gt;A. Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kay_K/0/1/0/all/0/1&quot;&gt;K. Kay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1&quot;&gt;G. Roig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cichy_R/0/1/0/all/0/1&quot;&gt;R. M. Cichy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10473">
<title>Aircraft Skin Inspections: Towards a New Model for Dent Evaluation. (arXiv:2301.10473v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10473</link>
<description rdf:parseType="Literal">&lt;p&gt;Aircraft maintenance, repair and overhaul industry is gradually switching to
3D scanning for dent inspection. High-accuracy devices allow quick and
repeatable measurements, which translate into efficient reporting and more
objective damage evaluations. However, the potential of 3D scanners is far from
being exploited. This is due to the traditional way in which the structural
repair manual deals with dents, that is, considering length, width and depth as
the only relevant measures. Being equivalent to describing a dent similarly to
a box, the current approach discards any information about the actual shape.
This causes high degrees of ambiguity, with very different shapes (and
corresponding fatigue life) being classified as the same, and nullifies the
effort of acquiring such great amount of information from high-accuracy 3D
scanners. In this paper a 7-parameter model is proposed to describe the actual
dent shape, thus enabling the exploitation of the high fidelity data produced
by 3D scanners. The compact set of values can then be compared against
historical data and structural evaluations based on the same model. The
proposed approach has been evaluated in both simulations and point cloud data
generated by 8tree&apos;s dentCHECK tool, suggesting increased capability to
evaluate damage, enabling more targeted interventions and, ultimately, saving
costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafiosca_P/0/1/0/all/0/1&quot;&gt;Pasquale Lafiosca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_I/0/1/0/all/0/1&quot;&gt;Ip-Shing Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avdelidis_N/0/1/0/all/0/1&quot;&gt;Nicolas P. Avdelidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10873">
<title>Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction. (arXiv:2302.10873v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10873</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time, accurate prediction of human steering behaviors has wide
applications, from developing intelligent traffic systems to deploying
autonomous driving systems in both real and simulated worlds. In this paper, we
present ContextVAE, a context-aware approach for multi-modal vehicle trajectory
prediction. Built upon the backbone architecture of a timewise variational
autoencoder, ContextVAE observation encoding employs a dual attention mechanism
that accounts for the environmental context and the dynamic agents&apos; states, in
a unified way. By utilizing features extracted from semantic maps during agent
state encoding, our approach takes into account both the social features
exhibited by agents on the scene and the physical environment constraints to
generate map-compliant and socially-aware trajectories. We perform extensive
testing on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo
Open Motion Dataset to show the effectiveness of our approach and its
state-of-the-art performance. In all tested datasets, ContextVAE models are
fast to train and provide high-quality multi-modal predictions in real-time.
Our code is available at: https://github.com/xupei0610/ContextVAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayet_J/0/1/0/all/0/1&quot;&gt;Jean-Bernard Hayet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1&quot;&gt;Ioannis Karamouzas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10703">
<title>CCTV-Gun: Benchmarking Handgun Detection in CCTV Images. (arXiv:2303.10703v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10703</link>
<description rdf:parseType="Literal">&lt;p&gt;Gun violence is a critical security problem, and it is imperative for the
computer vision community to develop effective gun detection algorithms for
real-world scenarios, particularly in Closed Circuit Television (CCTV)
surveillance data. Despite significant progress in visual object detection,
detecting guns in real-world CCTV images remains a challenging and
under-explored task. Firearms, especially handguns, are typically very small in
size, non-salient in appearance, and often severely occluded or
indistinguishable from other small objects. Additionally, the lack of
principled benchmarks and difficulty collecting relevant datasets further
hinder algorithmic development. In this paper, we present a meticulously
crafted and annotated benchmark, called \textbf{CCTV-Gun}, which addresses the
challenges of detecting handguns in real-world CCTV images. Our contribution is
three-fold. Firstly, we carefully select and analyze real-world CCTV images
from three datasets, manually annotate handguns and their holders, and assign
each image with relevant challenge factors such as blur and occlusion.
Secondly, we propose a new cross-dataset evaluation protocol in addition to the
standard intra-dataset protocol, which is vital for gun detection in practical
settings. Finally, we comprehensively evaluate both classical and
state-of-the-art object detection algorithms, providing an in-depth analysis of
their generalizing abilities. The benchmark will facilitate further research
and development on this topic and ultimately enhance security. Code,
annotations, and trained models are available at
https://github.com/srikarym/CCTV-Gun.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yellapragada_S/0/1/0/all/0/1&quot;&gt;Srikar Yellapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenghong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_K/0/1/0/all/0/1&quot;&gt;Kevin Bhadresh Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mhasakar_P/0/1/0/all/0/1&quot;&gt;Purva Makarand Mhasakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jie Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blasch_E/0/1/0/all/0/1&quot;&gt;Erik Blasch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haibin Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11615">
<title>Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer. (arXiv:2303.11615v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11615</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new table structure recognition (TSR) approach, called
TSRFormer, to robustly recognizing the structures of complex tables with
geometrical distortions from various table images. Unlike previous methods, we
formulate table separation line prediction as a line regression problem instead
of an image segmentation problem and propose a new two-stage dynamic queries
enhanced DETR based separation line regression approach, named DQ-DETR, to
predict separation lines from table images directly. Compared to Vallina DETR,
we propose three improvements in DQ-DETR to make the two-stage DETR framework
work efficiently and effectively for the separation line prediction task: 1) A
new query design, named Dynamic Query, to decouple single line query into
separable point queries which could intuitively improve the localization
accuracy for regression tasks; 2) A dynamic queries based progressive line
regression approach to progressively regressing points on the line which
further enhances localization accuracy for distorted tables; 3) A
prior-enhanced matching strategy to solve the slow convergence issue of DETR.
After separation line prediction, a simple relation network based cell merging
module is used to recover spanning cells. With these new techniques, our
TSRFormer achieves state-of-the-art performance on several benchmark datasets,
including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated
the robustness and high localization accuracy of our approach to tables with
complex structures, borderless cells, large blank spaces, empty or spanning
cells as well as distorted or even curved shapes on a more challenging
real-world in-house dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiawei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weihong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chixiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingze Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Q/0/1/0/all/0/1&quot;&gt;Qiang Huo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14618">
<title>BoxVIS: Video Instance Segmentation with Box Annotations. (arXiv:2303.14618v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14618</link>
<description rdf:parseType="Literal">&lt;p&gt;It is expensive and labour-extensive to label the pixel-wise object masks in
a video. As a result, the amount of pixel-wise annotations in existing video
instance segmentation (VIS) datasets is small, limiting the generalization
capability of trained VIS models. An alternative but much cheaper solution is
to use bounding boxes to label instances in videos. Inspired by the recent
success of box-supervised image instance segmentation, we adapt the
state-of-the-art pixel-supervised VIS models to a box-supervised VIS (BoxVIS)
baseline, and observe slight performance degradation. We consequently propose
to improve the BoxVIS performance from two aspects. First, we propose a
box-center guided spatial-temporal pairwise affinity (STPA) loss to predict
instance masks for better spatial and temporal consistency. Second, we collect
a larger scale box-annotated VIS dataset (BVISD) by consolidating the videos
from current VIS benchmarks and converting images from the COCO dataset to
short pseudo video clips. With the proposed BVISD and the STPA loss, our
trained BoxVIS model achieves 43.2\% and 29.0\% mask AP on the YouTube-VIS 2021
and OVIS valid sets, respectively. It exhibits comparable instance mask
prediction performance and better generalization ability than state-of-the-art
pixel-supervised VIS models by using only 16\% of their annotation time and
cost. Codes and data can be found at \url{https://github.com/MinghanLi/BoxVIS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04319">
<title>On the dice loss gradient and the ways to mimic it. (arXiv:2304.04319v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04319</link>
<description rdf:parseType="Literal">&lt;p&gt;In the past few years, in the context of fully-supervised semantic
segmentation, several losses -- such as cross-entropy and dice -- have emerged
as de facto standards to supervise neural networks. The Dice loss is an
interesting case, as it comes from the relaxation of the popular Dice
coefficient; one of the main evaluation metric in medical imaging applications.
In this paper, we first study theoretically the gradient of the dice loss,
showing that concretely it is a weighted negative of the ground truth, with a
very small dynamic range. This enables us, in the second part of this paper, to
mimic the supervision of the dice loss, through a simple element-wise
multiplication of the network output with a negative of the ground truth. This
rather surprising result sheds light on the practical supervision performed by
the dice loss during gradient descent. This can help the practitioner to
understand and interpret results while guiding researchers when designing new
losses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1&quot;&gt;Hoel Kervadec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1&quot;&gt;Marleen de Bruijne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06718">
<title>Segment Everything Everywhere All at Once. (arXiv:2304.06718v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06718</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present SEEM, a promptable and interactive model for
segmenting everything everywhere all at once in an image, as shown in Fig.1. In
SEEM, we propose a novel decoding mechanism that enables diverse prompting for
all types of segmentation tasks, aiming at a universal segmentation interface
that behaves like large language models (LLMs). More specifically, SEEM is
designed with four desiderata: i) Versatility. We introduce a new visual prompt
to unify different spatial queries including points, boxes, scribbles and
masks, which can further generalize to a different referring image; ii)
Compositionality. We learn a joint visual-semantic space between text and
visual prompts, which facilitates the dynamic composition of two prompt types
required for various segmentation tasks; iii) Interactivity. We further
incorporate learnable memory prompts into the decoder to retain segmentation
history through mask-guided cross-attention from decoder to image features; and
iv) Semantic-awareness. We use a text encoder to encode text queries and mask
labels into the same semantic space for open-vocabulary segmentation. We
conduct a comprehensive empirical study to validate the effectiveness of SEEM
across diverse segmentation tasks. Notably, our single SEEM model achieves
competitive performance across interactive segmentation, generic segmentation,
referring segmentation, and video object segmentation on 9 datasets with
minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity
for generalization to novel prompts or their combinations, rendering it a
readily universal image segmentation interface.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07139">
<title>Neuromorphic Optical Flow and Real-time Implementation with Event Cameras. (arXiv:2304.07139v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07139</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow provides information on relative motion that is an important
component in many computer vision pipelines. Neural networks provide high
accuracy optical flow, yet their complexity is often prohibitive for
application at the edge or in robots, where efficiency and latency play crucial
role. To address this challenge, we build on the latest developments in
event-based vision and spiking neural networks. We propose a new network
architecture, inspired by Timelens, that improves the state-of-the-art
self-supervised optical flow accuracy when operated both in spiking and
non-spiking mode. To implement a real-time pipeline with a physical event
camera, we propose a methodology for principled model simplification based on
activity and latency analysis. We demonstrate high speed optical flow
prediction with almost two orders of magnitude reduced complexity while
maintaining the accuracy, opening the path for real-time deployments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnider_Y/0/1/0/all/0/1&quot;&gt;Yannick Schnider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1&quot;&gt;Stanislaw Wozniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1&quot;&gt;Mathias Gehrig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecomte_J/0/1/0/all/0/1&quot;&gt;Jules Lecomte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnim_A/0/1/0/all/0/1&quot;&gt;Axel von Arnim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1&quot;&gt;Davide Scaramuzza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pantazi_A/0/1/0/all/0/1&quot;&gt;Angeliki Pantazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10701">
<title>Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10701</link>
<description rdf:parseType="Literal">&lt;p&gt;Data valuation is critical in machine learning, as it helps enhance model
transparency and protect data properties. Existing data valuation methods have
primarily focused on discriminative models, neglecting deep generative models
that have recently gained considerable attention. Similar to discriminative
models, there is an urgent need to assess data contributions in deep generative
models as well. However, previous data valuation approaches mainly relied on
discriminative model performance metrics and required model retraining.
Consequently, they cannot be applied directly and efficiently to recent deep
generative models, such as generative adversarial networks and diffusion
models, in practice. To bridge this gap, we formulate the data valuation
problem in generative models from a similarity-matching perspective.
Specifically, we introduce Generative Model Valuator (GMValuator), the first
model-agnostic approach for any generative models, designed to provide data
valuation for generation tasks. We have conducted extensive experiments to
demonstrate the effectiveness of the proposed method. To the best of their
knowledge, GMValuator is the first work that offers a training-free, post-hoc
data valuation strategy for deep generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wenglong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Benlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yangsibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11549">
<title>Spectral Sensitivity Estimation Without a Camera. (arXiv:2304.11549v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11549</link>
<description rdf:parseType="Literal">&lt;p&gt;A number of problems in computer vision and related fields would be mitigated
if camera spectral sensitivities were known. As consumer cameras are not
designed for high-precision visual tasks, manufacturers do not disclose
spectral sensitivities. Their estimation requires a costly optical setup, which
triggered researchers to come up with numerous indirect methods that aim to
lower cost and complexity by using color targets. However, the use of color
targets gives rise to new complications that make the estimation more
difficult, and consequently, there currently exists no simple, low-cost, robust
go-to method for spectral sensitivity estimation. Furthermore, even if not
limited by hardware or cost, researchers frequently work with imagery from
multiple cameras that they do not have in their possession. To provide a
practical solution to this problem, we propose a framework for spectral
sensitivity estimation that not only does not require any hardware, but also
does not require physical access to the camera itself. Similar to other work,
we formulate an optimization problem that minimizes a two-term objective
function: a camera-specific term from a system of equations, and a universal
term that bounds the solution space. Different than other work, we use publicly
available high-quality calibration data to construct both terms. We use the
colorimetric mapping matrices provided by the Adobe DNG Converter to formulate
the camera-specific system of equations, and constrain the solutions using an
autoencoder trained on a database of ground-truth curves. On average, we
achieve reconstruction errors as low as those that can arise due to
manufacturing imperfections between two copies of the same camera. We provide
our code and predicted sensitivities for 1,000+ cameras, and discuss which
tasks can become trivial when camera responses are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Solomatov_G/0/1/0/all/0/1&quot;&gt;Grigory Solomatov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akkaynak_D/0/1/0/all/0/1&quot;&gt;Derya Akkaynak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14108">
<title>DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14108</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal datasets are a critical component in recent breakthroughs such as
Stable Diffusion and GPT-4, yet their design does not receive the same research
attention as model architectures or training algorithms. To address this
shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset
experiments centered around a new candidate pool of 12.8 billion image-text
pairs from Common Crawl. Participants in our benchmark design new filtering
techniques or curate new data sources and then evaluate their new dataset by
running our standardized CLIP training code and testing the resulting model on
38 downstream test sets. Our benchmark consists of multiple compute scales
spanning four orders of magnitude, which enables the study of scaling trends
and makes the benchmark accessible to researchers with varying resources. Our
baseline experiments show that the DataComp workflow leads to better training
sets. In particular, our best baseline, DataComp-1B, enables training a CLIP
ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming
OpenAI&apos;s CLIP ViT-L/14 by 3.7 percentage points while using the same training
procedure and compute. We release DataComp and all accompanying code at
www.datacomp.ai.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1&quot;&gt;Alex Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayase_J/0/1/0/all/0/1&quot;&gt;Jonathan Hayase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1&quot;&gt;Georgios Smyrnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1&quot;&gt;Ryan Marten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1&quot;&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1&quot;&gt;Dhruba Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orgad_E/0/1/0/all/0/1&quot;&gt;Eyal Orgad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1&quot;&gt;Rahim Entezari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1&quot;&gt;Giannis Daras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1&quot;&gt;Sarah Pratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1&quot;&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1&quot;&gt;Kalyani Marathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1&quot;&gt;Richard Vencu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1&quot;&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1&quot;&gt;Olga Saukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1&quot;&gt;Alexander Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuran Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1&quot;&gt;Romain Beaumont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1&quot;&gt;Alex Dimakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1&quot;&gt;Jenia Jitsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1&quot;&gt;Yair Carmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11474">
<title>RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11474</link>
<description rdf:parseType="Literal">&lt;p&gt;Although many recent works have made advancements in the image restoration
(IR) field, they often suffer from an excessive number of parameters. Another
issue is that most Transformer-based IR methods focus only on either local or
global features, leading to limited receptive fields or deficient parameter
issues. To address these problems, we propose a lightweight IR network,
Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed
dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which
compute bi-dimensional (spatial and channel) self-attentions in parallel with
different numbers of multi-heads. The bi-dimensional attentions help each other
to complement their counterpart&apos;s drawbacks and are then mixed. Additionally,
we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that
compensates for pixel-level information losses and utilizes semantic
information while maintaining an efficient hierarchical structure. Furthermore,
we revisit and modify MobileNet V1 and V2 to attach efficient convolutions to
our proposed components. The experimental results demonstrate that RAMiT
achieves state-of-the-art performance on multiple lightweight IR tasks,
including super-resolution, color denoising, grayscale denoising, low-light
enhancement, and deraining. Codes are available at
https://github.com/rami0205/RAMiT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Haram Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_C/0/1/0/all/0/1&quot;&gt;Cheolwoong Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jihyeon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinseop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_S/0/1/0/all/0/1&quot;&gt;Subeen Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jeongmin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jihoon Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12286">
<title>Low-Earth Satellite Orbit Determination Using Deep Convolutional Networks with Satellite Imagery. (arXiv:2305.12286v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12286</link>
<description rdf:parseType="Literal">&lt;p&gt;It is increasingly common for satellites to lose connection with the ground
stations on Earth with which they communicate, due to signal interruptions from
the Earth&apos;s ionosphere and magnetosphere. Given the important roles that
satellites play in national defense, public safety, and worldwide
communications, finding ways to determine satellite trajectories in such
situations is a crucially important task. In this paper, we demonstrate the
efficacy of a novel computer vision based approach, which relies on earth
imagery taken by the satellite itself, to determine the orbit of a satellite
that has lost contact with its ground stations. We empirically observe
significant improvements by more than an order of magnitude, over the present
state of the art approach, namely, the Gibbs method for an initial orbit
estimate with the Kalman filter for differential error correction. We further
investigate the performance of the approach by comparing various neural
networks, namely, ResNet50, ResNet101, VGG19, VGG16, AlexNet, and CoAtNet4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorana_R/0/1/0/all/0/1&quot;&gt;Rohit Khorana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12529">
<title>DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12529</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DreamWaltz, a novel framework for generating and animating complex
3D avatars given text guidance and parametric human body prior. While recent
methods have shown encouraging results for text-to-3D generation of common
objects, creating high-quality and animatable 3D avatars remains challenging.
To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent
occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural
representations with canonical poses. It provides view-aligned supervision via
3D-aware skeleton conditioning which enables complex avatar generation without
artifacts and multiple faces. For animation, our method learns an animatable
and generalizable avatar representation which could map arbitrary poses to the
canonical pose representation. Extensive evaluations demonstrate that
DreamWaltz is an effective and robust approach for creating 3D avatars that can
take on complex shapes and appearances as well as novel poses for animation.
The proposed framework further enables the creation of complex scenes with
diverse compositions, including avatar-avatar, avatar-object and avatar-scene
interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and
animation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yukun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;He Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xianbiao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19767">
<title>Analytical reconstructions of full-scan multiple source-translation computed tomography under large field of views. (arXiv:2305.19767v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19767</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is to investigate the high-quality analytical reconstructions of
multiple source-translation computed tomography (mSTCT) under an extended field
of view (FOV). Under the larger FOVs, the previously proposed backprojection
filtration (BPF) algorithms for mSTCT, including D-BPF and S-BPF (their
differences are different derivate directions along the detector and source,
respectively), make some errors and artifacts in the reconstructed images due
to a backprojection weighting factor and the half-scan mode, which deviates
from the intention of mSTCT imaging. In this paper, to achieve reconstruction
with as little error as possible under the extremely extended FOV, we combine
the full-scan mSTCT (F-mSTCT) geometry with the previous BPF algorithms to
study the performance and derive a suitable redundancy-weighted function for
F-mSTCT. The experimental results indicate FS-BPF can get high-quality, stable
images under the extremely extended FOV of imaging a large object, though it
requires more projections than FD-BPF. Finally, for different practical
requirements in extending FOV imaging, we give suggestions on algorithm
selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shunli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1&quot;&gt;Xingyuan Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Junning Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00001">
<title>TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers. (arXiv:2306.00001v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00001</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a highly flexible, quantized, memory-efficient, and
ultra-lightweight object detection network, called TinyissimoYOLO. It aims to
enable object detection on microcontrollers in the power domain of milliwatts,
with less than 0.5MB memory available for storing convolutional neural network
(CNN) weights. The proposed quantized network architecture with 422k
parameters, enables real-time object detection on embedded microcontrollers,
and it has been evaluated to exploit CNN accelerators. In particular, the
proposed network has been deployed on the MAX78000 microcontroller achieving
high frame-rate of up to 180fps and an ultra-low energy consumption of only
196{\mu}J per inference with an inference efficiency of more than 106
MAC/Cycle. TinyissimoYOLO can be trained for any multi-object detection.
However, considering the small network size, adding object detection classes
will increase the size and memory consumption of the network, thus object
detection with up to 3 classes is demonstrated. Furthermore, the network is
trained using quantization-aware training and deployed with 8-bit quantization
on different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and on
the MAX78000&apos;s CNN accelerator. Performance evaluations are presented in this
paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordano_M/0/1/0/all/0/1&quot;&gt;Marco Giordano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_C/0/1/0/all/0/1&quot;&gt;Christian Vogt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04670">
<title>Object Detection with Transformers: A Review. (arXiv:2306.04670v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04670</link>
<description rdf:parseType="Literal">&lt;p&gt;The astounding performance of transformers in natural language processing
(NLP) has motivated researchers to explore their applications in computer
vision tasks. DEtection TRansformer (DETR) introduces transformers to object
detection tasks by reframing detection as a set prediction problem.
Consequently, eliminating the need for proposal generation and post-processing
steps. Initially, despite competitive performance, DETR suffered from slow
training convergence and ineffective detection of smaller objects. However,
numerous improvements are proposed to address these issues, leading to
substantial improvements in DETR and enabling it to exhibit state-of-the-art
performance. To our knowledge, this is the first paper to provide a
comprehensive review of 21 recently proposed advancements in the original DETR
model. We dive into both the foundational modules of DETR and its recent
enhancements, such as modifications to the backbone structure, query design
strategies, and refinements to attention mechanisms. Moreover, we conduct a
comparative analysis across various detection transformers, evaluating their
performance and network architectures. We hope that this study will ignite
further interest among researchers in addressing the existing challenges and
exploring the application of transformers in the object detection domain.
Readers interested in the ongoing developments in detection transformers can
refer to our website at:
https://github.com/mindgarage-shan/trans_object_detection_survey
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehzadi_T/0/1/0/all/0/1&quot;&gt;Tahira Shehzadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashmi_K/0/1/0/all/0/1&quot;&gt;Khurram Azeem Hashmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1&quot;&gt;Didier Stricker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afzal_M/0/1/0/all/0/1&quot;&gt;Muhammad Zeshan Afzal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04934">
<title>On the Effectiveness of Out-of-Distribution Data in Self-Supervised Long-Tail Learning. (arXiv:2306.04934v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Self-supervised learning (SSL) has been widely studied as a promising
technique for representation learning, it doesn&apos;t generalize well on
long-tailed datasets due to the majority classes dominating the feature space.
Recent work shows that the long-tailed learning performance could be boosted by
sampling extra in-domain (ID) data for self-supervised training, however,
large-scale ID data which can rebalance the minority classes are expensive to
collect. In this paper, we propose an alternative but easy-to-use and effective
solution, Contrastive with Out-of-distribution (OOD) data for Long-Tail
learning (COLT), which can effectively exploit OOD data to dynamically
re-balance the feature space. We empirically identify the counter-intuitive
usefulness of OOD samples in SSL long-tailed learning and principally design a
novel SSL method. Concretely, we first localize the `head&apos; and `tail&apos; samples
by assigning a tailness score to each OOD sample based on its neighborhoods in
the feature space. Then, we propose an online OOD sampling strategy to
dynamically re-balance the feature space. Finally, we enforce the model to be
capable of distinguishing ID and OOD samples by a distribution-level supervised
contrastive loss. Extensive experiments are conducted on various datasets and
several state-of-the-art SSL frameworks to verify the effectiveness of the
proposed method. The results show that our method significantly improves the
performance of SSL on long-tailed datasets by a large margin, and even
outperforms previous work which uses external ID data. Our code is available at
https://github.com/JianhongBai/COLT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jianhong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1&quot;&gt;Huanpeng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoji Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07591">
<title>I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models. (arXiv:2306.07591v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07591</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern image-to-text systems typically adopt the encoder-decoder framework,
which comprises two main components: an image encoder, responsible for
extracting image features, and a transformer-based decoder, used for generating
captions. Taking inspiration from the analysis of neural networks&apos; robustness
against adversarial perturbations, we propose a novel gray-box algorithm for
creating adversarial examples in image-to-text models. Unlike image
classification tasks that have a finite set of class labels, finding visually
similar adversarial examples in an image-to-text task poses greater challenges
because the captioning system allows for a virtually infinite space of possible
captions. In this paper, we present a gray-box adversarial attack on
image-to-text, both untargeted and targeted. We formulate the process of
discovering adversarial perturbations as an optimization problem that uses only
the image-encoder component, meaning the proposed attack is language-model
agnostic. Through experiments conducted on the ViT-GPT2 model, which is the
most-used image-to-text model in Hugging Face, and the Flickr30k dataset, we
demonstrate that our proposed attack successfully generates visually similar
adversarial examples, both with untargeted and targeted captions. Notably, our
attack operates in a gray-box manner, requiring no knowledge about the decoder
module. We also show that our attacks fool the popular open-source platform
Hugging Face.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapid_R/0/1/0/all/0/1&quot;&gt;Raz Lapid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1&quot;&gt;Moshe Sipper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07615">
<title>UOD: Universal One-shot Detection of Anatomical Landmarks. (arXiv:2306.07615v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07615</link>
<description rdf:parseType="Literal">&lt;p&gt;One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Heqin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Quan Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02273">
<title>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression. (arXiv:2307.02273v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02273</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the performance of neural image compression (NIC) has steadily
improved thanks to the last line of study, reaching or outperforming
state-of-the-art conventional codecs. Despite significant progress, current NIC
methods still rely on ConvNet-based entropy coding, limited in modeling
long-range dependencies due to their local connectivity and the increasing
number of architectural biases and priors, resulting in complex underperforming
models with high decoding latency. Motivated by the efficiency investigation of
the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose
to enhance the latter, as first, with a more straightforward yet effective
Tranformer-based channel-wise auto-regressive prior model, resulting in an
absolute image compression transformer (ICT). Through the proposed ICT, we can
capture both global and local contexts from the latent representations and
better parameterize the distribution of the quantized latents. Further, we
leverage a learnable scaling module with a sandwich ConvNeXt-based
pre-/post-processor to accurately extract more compact latent codes while
reconstructing higher-quality images. Extensive experimental results on
benchmark datasets showed that the proposed framework significantly improves
the trade-off between coding efficiency and decoder complexity over the
versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM. Moreover, we provide model scaling studies to verify the
computational efficiency of our approach and conduct several objective and
subjective analyses to bring to the fore the performance gap between the
adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbel_A/0/1/0/all/0/1&quot;&gt;Ahmed Ghorbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1&quot;&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1&quot;&gt;Luce Morin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02682">
<title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02682</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense video captioning, a task of localizing meaningful moments and
generating relevant captions for videos, often requires a large, expensive
corpus of annotated video segments paired with text. In an effort to minimize
the annotation cost, we propose ZeroTA, a novel method for dense video
captioning in a zero-shot manner. Our method does not require any videos or
annotations for training; instead, it localizes and describes events within
each input video at test time by optimizing solely on the input. This is
accomplished by introducing a soft moment mask that represents a temporal
segment in the video and jointly optimizing it with the prefix parameters of a
language model. This joint optimization aligns a frozen language generation
model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,
CLIP) by maximizing the matching score between the generated text and a moment
within the video. We also introduce a pairwise temporal IoU loss to let a set
of soft moment masks capture multiple distinct events within the video. Our
method effectively discovers diverse significant events within the video, with
the resulting captions appropriately describing these events. The empirical
results demonstrate that ZeroTA surpasses zero-shot baselines and even
outperforms the state-of-the-art few-shot method on the widely-used benchmark
ActivityNet Captions. Moreover, our method shows greater robustness compared to
supervised methods when evaluated in out-of-domain scenarios. This research
provides insight into the potential of aligning widely-used models, such as
language generation models and vision-language models, to unlock a new
capability: understanding temporal aspects of videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yongrae Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seongyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Aiden SJ Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunji Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1&quot;&gt;Hanseok Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minjoon Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03008">
<title>Self-supervised learning via inter-modal reconstruction and feature projection networks for label-efficient 3D-to-2D segmentation. (arXiv:2307.03008v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03008</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has become a valuable tool for the automation of certain
medical image segmentation tasks, significantly relieving the workload of
medical specialists. Some of these tasks require segmentation to be performed
on a subset of the input dimensions, the most common case being 3D-to-2D.
However, the performance of existing methods is strongly conditioned by the
amount of labeled data available, as there is currently no data efficient
method, e.g. transfer learning, that has been validated on these tasks. In this
work, we propose a novel convolutional neural network (CNN) and self-supervised
learning (SSL) method for label-efficient 3D-to-2D segmentation. The CNN is
composed of a 3D encoder and a 2D decoder connected by novel 3D-to-2D blocks.
The SSL method consists of reconstructing image pairs of modalities with
different dimensionality. The approach has been validated in two tasks with
clinical relevance: the en-face segmentation of geographic atrophy and
reticular pseudodrusen in optical coherence tomography. Results on different
datasets demonstrate that the proposed CNN significantly improves the state of
the art in scenarios with limited labeled data by up to 8% in Dice score.
Moreover, the proposed SSL method allows further improvement of this
performance by up to 23%, and we show that the SSL is beneficial regardless of
the network architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Morano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aresta_G/0/1/0/all/0/1&quot;&gt;Guilherme Aresta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lachinov_D/0/1/0/all/0/1&quot;&gt;Dmitrii Lachinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1&quot;&gt;Julia Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1&quot;&gt;Ursula Schmidt-Erfurth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03190">
<title>Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Text2Cinemagraph, a fully automated method for creating
cinemagraphs from text descriptions - an especially challenging task when
prompts feature imaginary elements and artistic styles, given the complexity of
interpreting the semantics and motions of these images. Existing single-image
animation methods fall short on artistic inputs, and recent text-based video
methods frequently introduce temporal inconsistencies, struggling to keep
certain regions static. To address these challenges, we propose an idea of
synthesizing image twins from a single text prompt - a pair of an artistic
image and its pixel-aligned corresponding natural-looking twin. While the
artistic image depicts the style and appearance detailed in our text prompt,
the realistic counterpart greatly simplifies layout and motion analysis.
Leveraging existing natural image and video datasets, we can accurately segment
the realistic image and predict plausible motion given the semantic
information. The predicted motion can then be transferred to the artistic image
to create the final cinemagraph. Our method outperforms existing approaches in
creating cinemagraphs for natural landscapes as well as artistic and
other-worldly scenes, as validated by automated metrics and user studies.
Finally, we demonstrate two extensions: animating existing paintings and
controlling motion directions using text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1&quot;&gt;Aniruddha Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1&quot;&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1&quot;&gt;Sergey Tulyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04106">
<title>Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird&apos;s Eye View. (arXiv:2307.04106v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04106</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision-only perception models for autonomous driving achieved
promising results by encoding multi-view image features into Bird&apos;s-Eye-View
(BEV) space. A critical step and the main bottleneck of these methods is
transforming image features into the BEV coordinate frame. This paper focuses
on leveraging geometry information, such as depth, to model such feature
transformation. Existing works rely on non-parametric depth distribution
modeling leading to significant memory consumption, or ignore the geometry
information to address this problem. In contrast, we propose to use parametric
depth distribution modeling for feature transformation. We first lift the 2D
image features to the 3D space defined for the ego vehicle via a predicted
parametric depth distribution for each pixel in each view. Then, we aggregate
the 3D feature volume based on the 3D space occupancy derived from depth to the
BEV frame. Finally, we use the transformed features for downstream tasks such
as object detection and semantic segmentation. Existing semantic segmentation
methods do also suffer from an hallucination problem as they do not take
visibility information into account. This hallucination can be particularly
problematic for subsequent modules such as control and planning. To mitigate
the issue, our method provides depth uncertainty and reliable visibility-aware
estimations. We further leverage our parametric depth modeling to present a
novel visibility-aware evaluation metric that, when taken into account, can
mitigate the hallucination problem. Extensive experiments on object detection
and semantic segmentation on the nuScenes datasets demonstrate that our method
outperforms existing methods on both tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miaomiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04132">
<title>Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04132</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip&apos;s corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshadri_A/0/1/0/all/0/1&quot;&gt;Amrit Diggavi Seshadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1&quot;&gt;Alessandra Russo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04149">
<title>Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04149</link>
<description rdf:parseType="Literal">&lt;p&gt;Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ayush Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhambhu_Y/0/1/0/all/0/1&quot;&gt;Yash Bhambhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckchash_H/0/1/0/all/0/1&quot;&gt;Himanshu Buckchash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1&quot;&gt;Dilip K. Prasad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04617">
<title>Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04617</link>
<description rdf:parseType="Literal">&lt;p&gt;Large medical imaging datasets can be cheaply and quickly annotated with
low-confidence, weak labels (e.g., radiological scores). Access to
high-confidence labels, such as histology-based diagnoses, is rare and costly.
Pretraining strategies, like contrastive learning (CL) methods, can leverage
unlabeled or weakly-annotated datasets. These methods typically require large
batch sizes, which poses a difficulty in the case of large 3D images at full
resolution, due to limited GPU memory. Nevertheless, volumetric positional
information about the spatial context of each 2D slice can be very important
for some medical applications. In this work, we propose an efficient
weakly-supervised positional (WSP) contrastive learning strategy where we
integrate both the spatial context of each 2D slice and a weak label via a
generic kernel-based loss function. We illustrate our method on cirrhosis
prediction using a large volume of weakly-labeled images, namely radiological
low-confidence annotations, and small strongly-labeled (i.e., high-confidence)
datasets. The proposed model improves the classification AUC by 5% with respect
to a baseline model on our internal dataset, and by 26% on the public LIHC
dataset from the Cancer Genome Atlas. The code is available at:
https://github.com/Guerbet-AI/wsp-contrastive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarfati_E/0/1/0/all/0/1&quot;&gt;Emma Sarfati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bone_A/0/1/0/all/0/1&quot;&gt;Alexandre B&amp;#xf4;ne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohe_M/0/1/0/all/0/1&quot;&gt;Marc-Michel Roh&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1&quot;&gt;Pietro Gori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1&quot;&gt;Isabelle Bloch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05014">
<title>Test-Time Training on Video Streams. (arXiv:2307.05014v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1&quot;&gt;Yossi Gandelsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05136">
<title>Unveiling the Invisible: Enhanced Detection and Analysis of Deteriorated Areas in Solar PV Modules Using Unsupervised Sensing Algorithms and 3D Augmented Reality. (arXiv:2307.05136v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05136</link>
<description rdf:parseType="Literal">&lt;p&gt;Solar Photovoltaic (PV) is increasingly being used to address the global
concern of energy security. However, hot spot and snail trails in PV modules
caused mostly by crakes reduce their efficiency and power capacity. This
article presents a groundbreaking methodology for automatically identifying and
analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)
modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality
(AR) visualization. By transforming the traditional methods of diagnosis and
repair, our approach not only enhances efficiency but also substantially cuts
down the cost of PV system maintenance. Validated through computer simulations
and real-world image datasets, the proposed framework accurately identifies
dirty regions, emphasizing the critical role of regular maintenance in
optimizing the power capacity of solar PV modules. Our immediate objective is
to leverage drone technology for real-time, automatic solar panel detection,
significantly boosting the efficacy of PV maintenance. The proposed methodology
could revolutionize solar PV maintenance, enabling swift, precise anomaly
detection without human intervention. This could result in significant cost
savings, heightened energy production, and improved overall performance of
solar PV systems. Moreover, the novel combination of unsupervised sensing
algorithms with 3D AR visualization heralds new opportunities for further
research and development in solar PV maintenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oulefki_A/0/1/0/all/0/1&quot;&gt;Adel Oulefki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trongtiraku_T/0/1/0/all/0/1&quot;&gt;Thaweesak Trongtiraku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1&quot;&gt;Kahina Amara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1&quot;&gt;Sos Agaian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benbelkacem_S/0/1/0/all/0/1&quot;&gt;Samir Benbelkacem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerroudji_M/0/1/0/all/0/1&quot;&gt;Mohamed Amine Guerroudji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemmouri_M/0/1/0/all/0/1&quot;&gt;Mohamed Zemmouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferhat_S/0/1/0/all/0/1&quot;&gt;Sahla Ferhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenati_N/0/1/0/all/0/1&quot;&gt;Nadia Zenati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atalla_S/0/1/0/all/0/1&quot;&gt;Shadi Atalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_W/0/1/0/all/0/1&quot;&gt;Wathiq Mansoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05288">
<title>Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle Safety. (arXiv:2307.05288v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05288</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles require accurate and reliable short-term trajectory
predictions for safe and efficient driving. While most commercial automated
vehicles currently use state machine-based algorithms for trajectory
forecasting, recent efforts have focused on end-to-end data-driven systems.
Often, the design of these models is limited by the availability of datasets,
which are typically restricted to generic scenarios. To address this
limitation, we have developed a synthetic dataset for short-term trajectory
prediction tasks using the CARLA simulator. This dataset is extensive and
incorporates what is considered complex scenarios - pedestrians crossing the
road, vehicles overtaking - and comprises 6000 perspective view images with
corresponding IMU and odometry information for each frame. Furthermore, an
end-to-end short-term trajectory prediction model using convolutional neural
networks (CNN) and long short-term memory (LSTM) networks has also been
developed. This model can handle corner cases, such as slowing down near zebra
crossings and stopping when pedestrians cross the road, without the need for
explicit encoding of the surrounding environment. In an effort to accelerate
this research and assist others, we are releasing our dataset and model to the
research community. Our datasets are publicly available on
https://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Sushil Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahiaoui_L/0/1/0/all/0/1&quot;&gt;Lucie Yahiaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arindam Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halton_M/0/1/0/all/0/1&quot;&gt;Mark Halton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05350">
<title>Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2307.05350v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05350</link>
<description rdf:parseType="Literal">&lt;p&gt;ML model design either starts with an interpretable model or a Blackbox and
explains it post hoc. Blackbox models are flexible but difficult to explain,
while interpretable models are inherently explainable. Yet, interpretable
models require extensive ML knowledge and tend to be less flexible and
underperforming than their Blackbox variants. This paper aims to blur the
distinction between a post hoc explanation of a Blackbox and constructing
interpretable models. Beginning with a Blackbox, we iteratively carve out a
mixture of interpretable experts (MoIE) and a residual network. Each
interpretable model specializes in a subset of samples and explains them using
First Order Logic (FOL), providing basic reasoning on concepts from the
Blackbox. We route the remaining samples through a flexible residual. We repeat
the method on the residual network until all the interpretable models explain
the desired proportion of data. Our extensive experiments show that our route,
interpret, and repeat approach (1) identifies a diverse set of
instance-specific concepts with high concept completeness via MoIE without
compromising in performance, (2) identifies the relatively ``harder&apos;&apos; samples
to explain via residuals, (3) outperforms the interpretable by-design models by
significant margins during test-time interventions, and (4) fixes the shortcut
learned by the original Blackbox. The code for MoIE is publicly available at:
\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05468">
<title>My3DGen: Building Lightweight Personalized 3D Generative Model. (arXiv:2307.05468v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05468</link>
<description rdf:parseType="Literal">&lt;p&gt;Our paper presents My3DGen, a practical system for creating a personalized
and lightweight 3D generative prior using as few as 10 images. My3DGen can
reconstruct multi-view consistent images from an input test image, and generate
novel appearances by interpolating between any two images of the same
individual. While recent studies have demonstrated the effectiveness of
personalized generative priors in producing high-quality 2D portrait
reconstructions and syntheses, to the best of our knowledge, we are the first
to develop a personalized 3D generative prior. Instead of fine-tuning a large
pre-trained generative model with millions of parameters to achieve
personalization, we propose a parameter-efficient approach. Our method involves
utilizing a pre-trained model with fixed weights as a generic prior, while
training a separate personalized prior through low-rank decomposition of the
weights in each convolution and fully connected layer. However,
parameter-efficient few-shot fine-tuning on its own often leads to overfitting.
To address this, we introduce a regularization technique based on symmetry of
human faces. This regularization enforces that novel view renderings of a
training sample, rendered from symmetric poses, exhibit the same identity. By
incorporating this symmetry prior, we enhance the quality of reconstruction and
synthesis, particularly for non-frontal (profile) faces. Our final system
combines low-rank fine-tuning with symmetry regularization and significantly
surpasses the performance of pre-trained models, e.g. EG3D. It introduces only
approximately 0.6 million additional parameters per identity compared to 31
million for full finetuning of the original model. As a result, our system
achieves a 50-fold reduction in model size without sacrificing the quality of
the generated 3D faces. Code will be available at our project page:
https://luchaoqi.github.io/my3dgen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Luchao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaye Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sengupta&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>