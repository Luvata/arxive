<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-15T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.01251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.02474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07955" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.08417">
<title>Image complexity based fMRI-BOLD visual network categorization across visual datasets using topological descriptors and deep-hybrid learning. (arXiv:2311.08417v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08417</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes a new approach that investigates differences in
topological characteristics of visual networks, which are constructed using
fMRI BOLD time-series corresponding to visual datasets of COCO, ImageNet, and
SUN. A publicly available BOLD5000 dataset is utilized that contains fMRI scans
while viewing 5254 images of diverse complexities. The objective of this study
is to examine how network topology differs in response to distinct visual
stimuli from these visual datasets. To achieve this, 0- and 1-dimensional
persistence diagrams are computed for each visual network representing COCO,
ImageNet, and SUN. For extracting suitable features from topological
persistence diagrams, K-means clustering is executed. The extracted K-means
cluster features are fed to a novel deep-hybrid model that yields accuracy in
the range of 90%-95% in classifying these visual networks. To understand
vision, this type of visual network categorization across visual datasets is
important as it captures differences in BOLD signals while perceiving images
with different contexts and complexities. Furthermore, distinctive topological
patterns of visual network associated with each dataset, as revealed from this
study, could potentially lead to the development of future neuroimaging
biomarkers for diagnosing visual processing disorders like visual agnosia or
prosopagnosia, and tracking changes in visual cognition over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Debanjali Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinha_N/0/1/0/all/0/1&quot;&gt;Neelam Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+R%2E_Y/0/1/0/all/0/1&quot;&gt;Yashwanth R.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chattopadhyay_A/0/1/0/all/0/1&quot;&gt;Amit Chattopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08438">
<title>LocaliseBot: Multi-view 3D object localisation with differentiable rendering for robot grasping. (arXiv:2311.08438v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08438</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot grasp typically follows five stages: object detection, object
localisation, object pose estimation, grasp pose estimation, and grasp
planning. We focus on object pose estimation. Our approach relies on three
pieces of information: multiple views of the object, the camera&apos;s extrinsic
parameters at those viewpoints, and 3D CAD models of objects. The first step
involves a standard deep learning backbone (FCN ResNet) to estimate the object
label, semantic segmentation, and a coarse estimate of the object pose with
respect to the camera. Our novelty is using a refinement module that starts
from the coarse pose estimate and refines it by optimisation through
differentiable rendering. This is a purely vision-based approach that avoids
the need for other information such as point cloud or depth images. We evaluate
our object pose estimation approach on the ShapeNet dataset and show
improvements over the state of the art. We also show that the estimated object
pose results in 99.65% grasp accuracy with the ground truth grasp candidates on
the Object Clutter Indoor Dataset (OCID) Grasp dataset, as computed using
standard practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayaraghavan_S/0/1/0/all/0/1&quot;&gt;Sujal Vijayaraghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alqasemi_R/0/1/0/all/0/1&quot;&gt;Redwan Alqasemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_R/0/1/0/all/0/1&quot;&gt;Rajiv Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Sudeep Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08439">
<title>A Unified Approach for Comprehensive Analysis of Various Spectral and Tissue Doppler Echocardiography. (arXiv:2311.08439v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08439</link>
<description rdf:parseType="Literal">&lt;p&gt;Doppler echocardiography offers critical insights into cardiac function and
phases by quantifying blood flow velocities and evaluating myocardial motion.
However, previous methods for automating Doppler analysis, ranging from initial
signal processing techniques to advanced deep learning approaches, have been
constrained by their reliance on electrocardiogram (ECG) data and their
inability to process Doppler views collectively. We introduce a novel unified
framework using a convolutional neural network for comprehensive analysis of
spectral and tissue Doppler echocardiography images that combines automatic
measurements and end-diastole (ED) detection into a singular method. The
network automatically recognizes key features across various Doppler views,
with novel Doppler shape embedding and anti-aliasing modules enhancing
interpretation and ensuring consistent analysis. Empirical results indicate a
consistent outperformance in performance metrics, including dice similarity
coefficients (DSC) and intersection over union (IoU). The proposed framework
demonstrates strong agreement with clinicians in Doppler automatic measurements
and competitive performance in ED detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1&quot;&gt;Jaeik Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yeonggul Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1&quot;&gt;Yeonyee E. Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;Dawun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Youngtaek Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung-Ah Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyuk-Jae Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08479">
<title>Leveraging Foundation Models to Improve Lightweight Clients in Federated Learning. (arXiv:2311.08479v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08479</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a distributed training paradigm that enables
clients scattered across the world to cooperatively learn a global model
without divulging confidential data. However, FL faces a significant challenge
in the form of heterogeneous data distributions among clients, which leads to a
reduction in performance and robustness. A recent approach to mitigating the
impact of heterogeneous data distributions is through the use of foundation
models, which offer better performance at the cost of larger computational
overheads and slower inference speeds. We introduce foundation model
distillation to assist in the federated training of lightweight client models
and increase their performance under heterogeneous data settings while keeping
inference costs low. Our results show improvement in the global model
performance on a balanced testing set, which contains rarely observed samples,
even under extreme non-IID client data distributions. We conduct a thorough
evaluation of our framework with different foundation model backbones on
CIFAR10, with varying degrees of heterogeneous data distributions ranging from
class-specific data partitions across clients to dirichlet data sampling,
parameterized by values between 0.01 and 1.0.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xidong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wan-Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willmott_D/0/1/0/all/0/1&quot;&gt;Devin Willmott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Condessa_F/0/1/0/all/0/1&quot;&gt;Filipe Condessa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_M/0/1/0/all/0/1&quot;&gt;Madan Ravi Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08488">
<title>MUDD: A New Re-Identification Dataset with Efficient Annotation for Off-Road Racers in Extreme Conditions. (arXiv:2311.08488v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08488</link>
<description rdf:parseType="Literal">&lt;p&gt;Re-identifying individuals in unconstrained environments remains an open
challenge in computer vision. We introduce the Muddy Racer re-IDentification
Dataset (MUDD), the first large-scale benchmark for matching identities of
motorcycle racers during off-road competitions. MUDD exhibits heavy mud
occlusion, motion blurring, complex poses, and extreme lighting conditions
previously unseen in existing re-id datasets. We present an annotation
methodology incorporating auxiliary information that reduced labeling time by
over 65%. We establish benchmark performance using state-of-the-art re-id
models including OSNet and ResNet-50. Without fine-tuning, the best models
achieve only 33% Rank-1 accuracy. Fine-tuning on MUDD boosts results to 79%
Rank-1, but significant room for improvement remains. We analyze the impact of
real-world factors including mud, pose, lighting, and more. Our work exposes
open problems in re-identifying individuals under extreme conditions. We hope
MUDD serves as a diverse and challenging benchmark to spur progress in robust
re-id, especially for computer vision applications in emerging sports
analytics. All code and data can be found at https://github.com/JacobTyo/MUDD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyo_J/0/1/0/all/0/1&quot;&gt;Jacob Tyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olarinre_M/0/1/0/all/0/1&quot;&gt;Motolani Olarinre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1&quot;&gt;Youngseog Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08493">
<title>Performance of Machine Learning Classification in Mammography Images using BI-RADS. (arXiv:2311.08493v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08493</link>
<description rdf:parseType="Literal">&lt;p&gt;This research aims to investigate the classification accuracy of various
state-of-the-art image classification models across different categories of
breast ultrasound images, as defined by the Breast Imaging Reporting and Data
System (BI-RADS). To achieve this, we have utilized a comprehensively assembled
dataset of 2,945 mammographic images sourced from 1,540 patients. In order to
conduct a thorough analysis, we employed six advanced classification
architectures, including VGG19 \cite{simonyan2014very}, ResNet50
\cite{he2016deep}, GoogleNet \cite{szegedy2015going}, ConvNext
\cite{liu2022convnet}, EfficientNet \cite{tan2019efficientnet}, and Vision
Transformers (ViT) \cite{dosovitskiy2020image}, instead of traditional machine
learning models. We evaluate models in three different settings: full
fine-tuning, linear evaluation and training from scratch. Our findings
demonstrate the effectiveness and capability of our Computer-Aided Diagnosis
(CAD) system, with a remarkable accuracy of 76.39\% and an F1 score of 67.94\%
in the full fine-tuning setting. Our findings indicate the potential for
enhanced diagnostic accuracy in the field of breast imaging, providing a solid
foundation for future endeavors aiming to improve the precision and reliability
of CAD systems in medical imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gunawardhana_M/0/1/0/all/0/1&quot;&gt;Malitha Gunawardhana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zolek_N/0/1/0/all/0/1&quot;&gt;Norbert Zolek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08503">
<title>MADG: Margin-based Adversarial Learning for Domain Generalization. (arXiv:2311.08503v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08503</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain Generalization (DG) techniques have emerged as a popular approach to
address the challenges of domain shift in Deep Learning (DL), with the goal of
generalizing well to the target domain unseen during the training. In recent
years, numerous methods have been proposed to address the DG setting, among
which one popular approach is the adversarial learning-based methodology. The
main idea behind adversarial DG methods is to learn domain-invariant features
by minimizing a discrepancy metric. However, most adversarial DG methods use
0-1 loss based $\mathcal{H}\Delta\mathcal{H}$ divergence metric. In contrast,
the margin loss-based discrepancy metric has the following advantages: more
informative, tighter, practical, and efficiently optimizable. To mitigate this
gap, this work proposes a novel adversarial learning DG algorithm, MADG,
motivated by a margin loss-based discrepancy metric. The proposed MADG model
learns domain-invariant features across all source domains and uses adversarial
training to generalize well to the unseen target domain. We also provide a
theoretical analysis of the proposed MADG model based on the unseen target
error bound. Specifically, we construct the link between the source and unseen
domains in the real-valued hypothesis space and derive the generalization bound
using margin loss and Rademacher complexity. We extensively experiment with the
MADG model on popular real-world DG datasets, VLCS, PACS, OfficeHome,
DomainNet, and TerraIncognita. We evaluate the proposed algorithm on
DomainBed&apos;s benchmark and observe consistent performance across all the
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayal_A/0/1/0/all/0/1&quot;&gt;Aveen Dayal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+B%2E_V/0/1/0/all/0/1&quot;&gt;Vimal K. B.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cenkeramaddi_L/0/1/0/all/0/1&quot;&gt;Linga Reddy Cenkeramaddi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1&quot;&gt;C. Krishna Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhinav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08524">
<title>Cross-dataset domain adaptation for the classification COVID-19 using chest computed tomography images. (arXiv:2311.08524v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08524</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting COVID-19 patients using Computed Tomography (CT) images of the
lungs is an active area of research. Datasets of CT images from COVID-19
patients are becoming available. Deep learning (DL) solutions and in particular
Convolutional Neural Networks (CNN) have achieved impressive results for the
classification of COVID-19 CT images, but only when the training and testing
take place within the same dataset. Work on the cross-dataset problem is still
limited and the achieved results are low. Our work tackles the cross-dataset
problem through a Domain Adaptation (DA) technique with deep learning. Our
proposed solution, COVID19-DANet, is based on pre-trained CNN backbone for
feature extraction. For this task, we select the pre-trained Efficientnet-B3
CNN because it has achieved impressive classification accuracy in previous
work. The backbone CNN is followed by a prototypical layer which is a concept
borrowed from prototypical networks in few-shot learning (FSL). It computes a
cosine distance between given samples and the class prototypes and then
converts them to class probabilities using the Softmax function. To train the
COVID19-DANet model, we propose a combined loss function that is composed of
the standard cross-entropy loss for class discrimination and another entropy
loss computed over the unlabelled target set only. This so-called unlabelled
target entropy loss is minimized and maximized in an alternative fashion, to
reach the two objectives of class discrimination and domain invariance.
COVID19-DANet is tested under four cross-dataset scenarios using the
SARS-CoV-2-CT and COVID19-CT datasets and has achieved encouraging results
compared to recent work in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ouni_R/0/1/0/all/0/1&quot;&gt;Ridha Ouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alhichri_H/0/1/0/all/0/1&quot;&gt;Haikel Alhichri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08525">
<title>Efficient Rotation Invariance in Deep Neural Networks through Artificial Mental Rotation. (arXiv:2311.08525v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08525</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans and animals recognize objects irrespective of the beholder&apos;s point of
view, which may drastically change their appearances. Artificial pattern
recognizers also strive to achieve this, e.g., through translational invariance
in convolutional neural networks (CNNs). However, both CNNs and vision
transformers (ViTs) perform very poorly on rotated inputs. Here we present
artificial mental rotation (AMR), a novel deep learning paradigm for dealing
with in-plane rotations inspired by the neuro-psychological concept of mental
rotation. Our simple AMR implementation works with all common CNN and ViT
architectures. We test it on ImageNet, Stanford Cars, and Oxford Pet. With a
top-1 error (averaged across datasets and architectures) of $0.743$, AMR
outperforms the current state of the art (rotational data augmentation, average
top-1 error of $0.626$) by $19\%$. We also easily transfer a trained AMR module
to a downstream task to improve the performance of a pre-trained semantic
segmentation model on rotated CoCo from $32.7$ to $55.2$ IoU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1&quot;&gt;Lukas Tuggener&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1&quot;&gt;Thilo Stadelmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Schmidhuber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08530">
<title>SceneScore: Learning a Cost Function for Object Arrangement. (arXiv:2311.08530v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.08530</link>
<description rdf:parseType="Literal">&lt;p&gt;Arranging objects correctly is a key capability for robots which unlocks a
wide range of useful tasks. A prerequisite for creating successful arrangements
is the ability to evaluate the desirability of a given arrangement. Our method
&quot;SceneScore&quot; learns a cost function for arrangements, such that desirable,
human-like arrangements have a low cost. We learn the distribution of training
arrangements offline using an energy-based model, solely from example images
without requiring environment interaction or human supervision. Our model is
represented by a graph neural network which learns object-object relations,
using graphs constructed from images. Experiments demonstrate that the learned
cost function can be used to predict poses for missing objects, generalise to
novel objects using semantic features, and can be composed with other cost
functions to satisfy constraints at inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapelyukh_I/0/1/0/all/0/1&quot;&gt;Ivan Kapelyukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1&quot;&gt;Edward Johns&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08539">
<title>Physical Adversarial Examples for Multi-Camera Systems. (arXiv:2311.08539v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08539</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks build the foundation of several intelligent systems, which,
however, are known to be easily fooled by adversarial examples. Recent advances
made these attacks possible even in air-gapped scenarios, where the autonomous
system observes its surroundings by, e.g., a camera. We extend these ideas in
our research and evaluate the robustness of multi-camera setups against such
physical adversarial examples. This scenario becomes ever more important with
the rise in popularity of autonomous vehicles, which fuse the information of
several cameras for their driving decision. While we find that multi-camera
setups provide some robustness towards past attack methods, we see that this
advantage reduces when optimizing on multiple perspectives at once. We propose
a novel attack method that we call Transcender-MC, where we incorporate online
3D renderings and perspective projections in the training process. Moreover, we
motivate that certain data augmentation techniques can facilitate the
generation of successful adversarial examples even further. Transcender-MC is
11% more effective in successfully attacking multi-camera setups than
state-of-the-art methods. Our findings offer valuable insights regarding the
resilience of object detection in a setup with multiple cameras and motivate
the need of developing adequate defense mechanisms against them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radutoiu_A/0/1/0/all/0/1&quot;&gt;Ana R&amp;#x103;du&amp;#x163;oiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulze_J/0/1/0/all/0/1&quot;&gt;Jan-Philipp Schulze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperl_P/0/1/0/all/0/1&quot;&gt;Philip Sperl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1&quot;&gt;Konstantin B&amp;#xf6;ttinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08544">
<title>JOSA: Joint surface-based registration and atlas construction of brain geometry and function. (arXiv:2311.08544v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2311.08544</link>
<description rdf:parseType="Literal">&lt;p&gt;Surface-based cortical registration is an important topic in medical image
analysis and facilitates many downstream applications. Current approaches for
cortical registration are mainly driven by geometric features, such as sulcal
depth and curvature, and often assume that registration of folding patterns
leads to alignment of brain function. However, functional variability of
anatomically corresponding areas across subjects has been widely reported,
particularly in higher-order cognitive areas. In this work, we present JOSA, a
novel cortical registration framework that jointly models the mismatch between
geometry and function while simultaneously learning an unbiased
population-specific atlas. Using a semi-supervised training strategy, JOSA
achieves superior registration performance in both geometry and function to the
state-of-the-art methods but without requiring functional data at inference.
This learning framework can be extended to any auxiliary data to guide
spherical registration that is available during training but is difficult or
impossible to obtain during inference, such as parcellations, architectonic
identity, transcriptomic information, and molecular profiles. By recognizing
the mismatch between geometry and function, JOSA provides new insights into the
future development of registration methods using joint analysis of the brain
structure and function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tuckute_G/0/1/0/all/0/1&quot;&gt;Greta Tuckute&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Edlow_B/0/1/0/all/0/1&quot;&gt;Brian L. Edlow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fischl_B/0/1/0/all/0/1&quot;&gt;Bruce Fischl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08548">
<title>Topology of Surface Electromyogram Signals: Hand Gesture Decoding on Riemannian Manifolds. (arXiv:2311.08548v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2311.08548</link>
<description rdf:parseType="Literal">&lt;p&gt;Decoding gestures from the upper limb using noninvasive surface
electromyogram (sEMG) signals is of keen interest for the rehabilitation of
amputees, artificial supernumerary limb augmentation, gestural control of
computers, and virtual/augmented realities. We show that sEMG signals recorded
across an array of sensor electrodes in multiple spatial locations around the
forearm evince a rich geometric pattern of global motor unit (MU) activity that
can be leveraged to distinguish different hand gestures. We demonstrate a
simple technique to analyze spatial patterns of muscle MU activity within a
temporal window and show that distinct gestures can be classified in both
supervised and unsupervised manners. Specifically, we construct symmetric
positive definite (SPD) covariance matrices to represent the spatial
distribution of MU activity in a time window of interest, calculated as
pairwise covariance of electrical signals measured across different electrodes.
This allows us to understand and manipulate multivariate sEMG timeseries on a
more natural subspace -the Riemannian manifold. Furthermore, it directly
addresses signal variability across individuals and sessions, which remains a
major challenge in the field. sEMG signals measured at a single electrode lack
contextual information such as how various anatomical and physiological factors
influence the signals and how their combined effect alters the evident
interaction among neighboring muscles. As we show here, analyzing spatial
patterns using covariance matrices on Riemannian manifolds allows us to
robustly model complex interactions across spatially distributed MUs and
provides a flexible and transparent framework to quantify differences in sEMG
signals across individuals. The proposed method is novel in the study of sEMG
signals and its performance exceeds the current benchmarks while maintaining
exceptional computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gowda_H/0/1/0/all/0/1&quot;&gt;Harshavardhana T. Gowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miller_L/0/1/0/all/0/1&quot;&gt;Lee M. Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08557">
<title>Low-light Pedestrian Detection in Visible and Infrared Image Feeds: Issues and Challenges. (arXiv:2311.08557v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08557</link>
<description rdf:parseType="Literal">&lt;p&gt;Pedestrian detection has become a cornerstone for several high-level tasks,
including autonomous driving, intelligent transportation, and traffic
surveillance. There are several works focussed on pedestrian detection using
visible images, mainly in the daytime. However, this task is very intriguing
when the environmental conditions change to poor lighting or nighttime.
Recently, new ideas have been spurred to use alternative sources, such as Far
InfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light
conditions. This study comprehensively reviews recent developments in low-light
pedestrian detection approaches. It systematically categorizes and analyses
various algorithms from region-based to non-region-based and graph-based
learning methodologies by highlighting their methodologies, implementation
issues, and challenges. It also outlines the key benchmark datasets that can be
used for research and development of advanced pedestrian detection algorithms,
particularly in low-light situations
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vachhani_H/0/1/0/all/0/1&quot;&gt;Hrishikesh Vachhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akilan_T/0/1/0/all/0/1&quot;&gt;Thangarajah Akilan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devmurari_Y/0/1/0/all/0/1&quot;&gt;Yash Devmurari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaik_N/0/1/0/all/0/1&quot;&gt;Nisharaff Shaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Dhruvisha Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08577">
<title>Finding AI-Generated Faces in the Wild. (arXiv:2311.08577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porcile_G/0/1/0/all/0/1&quot;&gt;Gonzalo J. Aniano Porcile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gindi_J/0/1/0/all/0/1&quot;&gt;Jack Gindi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundra_S/0/1/0/all/0/1&quot;&gt;Shivansh Mundra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbus_J/0/1/0/all/0/1&quot;&gt;James R. Verbus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1&quot;&gt;Hany Farid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08581">
<title>Drivable 3D Gaussian Avatars. (arXiv:2311.08581v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08581</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable
model for human bodies rendered with Gaussian splats. Current photorealistic
drivable avatars require either accurate 3D registrations during training,
dense input images during testing, or both. The ones based on neural radiance
fields also tend to be prohibitively slow for telepresence applications. This
work uses the recently presented 3D Gaussian Splatting (3DGS) technique to
render realistic humans at real-time framerates, using dense calibrated
multi-view videos as input. To deform those primitives, we depart from the
commonly used point deformation method of linear blend skinning (LBS) and use a
classic volumetric deformation method: cage deformations. Given their smaller
size, we drive these deformations with joint angles and keypoints, which are
more suitable for communication applications. Our experiments on nine subjects
with varied body shapes, clothes, and motions obtain higher-quality results
than state-of-the-art methods when using the same training and test data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zielonka_W/0/1/0/all/0/1&quot;&gt;Wojciech Zielonka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1&quot;&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1&quot;&gt;Shunsuke Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1&quot;&gt;Michael Zollh&amp;#xf6;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1&quot;&gt;Justus Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1&quot;&gt;Javier Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08585">
<title>Unsupervised segmentation of irradiation$\unicode{x2010}$induced order$\unicode{x2010}$disorder phase transitions in electron microscopy. (arXiv:2311.08585v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2311.08585</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for the unsupervised segmentation of electron microscopy
images, which are powerful descriptors of materials and chemical systems.
Images are oversegmented into overlapping chips, and similarity graphs are
generated from embeddings extracted from a domain$\unicode{x2010}$pretrained
convolutional neural network (CNN). The Louvain method for community detection
is then applied to perform segmentation. The graph representation provides an
intuitive way of presenting the relationship between chips and communities. We
demonstrate our method to track irradiation$\unicode{x2010}$induced amorphous
fronts in thin films used for catalysis and electronics. This method has
potential for &quot;on$\unicode{x2010}$the$\unicode{x2010}$fly&quot; segmentation to
guide emerging automated electron microscopes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ter_Petrosyan_A/0/1/0/all/0/1&quot;&gt;Arman H Ter-Petrosyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bilbrey_J/0/1/0/all/0/1&quot;&gt;Jenna A Bilbrey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Doty_C/0/1/0/all/0/1&quot;&gt;Christina M Doty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Matthews_B/0/1/0/all/0/1&quot;&gt;Bethany E Matthews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Le Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yingge Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lang_E/0/1/0/all/0/1&quot;&gt;Eric Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hattar_K/0/1/0/all/0/1&quot;&gt;Khalid Hattar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Spurgeon_S/0/1/0/all/0/1&quot;&gt;Steven R Spurgeon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08622">
<title>Multiple-Question Multiple-Answer Text-VQA. (arXiv:2311.08622v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08622</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do
text-VQA in encoder-decoder transformer models. The text-VQA task requires a
model to answer a question by understanding multi-modal content: text
(typically from OCR) and an associated image. To the best of our knowledge,
almost all previous approaches for text-VQA process a single question and its
associated content to predict a single answer. In order to answer multiple
questions from the same image, each question and content are fed into the model
multiple times. In contrast, our proposed MQMA approach takes multiple
questions and content as input at the encoder and predicts multiple answers at
the decoder in an auto-regressive manner at the same time. We make several
novel architectural modifications to standard encoder-decoder transformers to
support MQMA. We also propose a novel MQMA denoising pre-training task which is
designed to teach the model to align and delineate multiple questions and
content with associated answers. MQMA pre-trained model achieves
state-of-the-art results on multiple text-VQA datasets, each with strong
baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),
DocVQA (+1.1%) absolute improvements over the previous state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Peng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1&quot;&gt;Srikar Appalaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1&quot;&gt;R. Manmatha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yusheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1&quot;&gt;Vijay Mahadevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08623">
<title>DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models. (arXiv:2311.08623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08623</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoder-decoder transformer models have achieved great success on various
vision-language (VL) tasks, but they suffer from high inference latency.
Typically, the decoder takes up most of the latency because of the
auto-regressive decoding. To accelerate the inference, we propose an approach
of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit
encoder-decoder transformer model which is trained with deep supervision so
that each of its decoder layers is capable of generating plausible predictions.
In addition, we leverage simple yet practical techniques, including shared
generation head and adaptation modules, to keep accuracy when exiting at
shallow decoder layers. Based on the multi-exit model, we perform step-level
dynamic early exit during inference, where the model may decide to use fewer
decoder layers based on its confidence of the current layer at each individual
decoding step. Considering different number of decoder layers may be used at
different decoding steps, we compute deeper-layer decoder features of previous
decoding steps just-in-time, which ensures the features from different decoding
steps are semantically aligned. We evaluate our approach with two
state-of-the-art encoder-decoder transformer models on various VL tasks. We
show our approach can reduce overall inference latency by 30%-60% with
comparable or even higher accuracy compared to baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1&quot;&gt;Peng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengkai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1&quot;&gt;Srikar Appalaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahadevan_V/0/1/0/all/0/1&quot;&gt;Vijay Mahadevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1&quot;&gt;R. Manmatha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08646">
<title>Painterly Image Harmonization via Adversarial Residual Learning. (arXiv:2311.08646v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08646</link>
<description rdf:parseType="Literal">&lt;p&gt;Image compositing plays a vital role in photo editing. After inserting a
foreground object into another background image, the composite image may look
unnatural and inharmonious. When the foreground is photorealistic and the
background is an artistic painting, painterly image harmonization aims to
transfer the style of background painting to the foreground object, which is a
challenging task due to the large domain gap between foreground and background.
In this work, we employ adversarial learning to bridge the domain gap between
foreground feature map and background feature map. Specifically, we design a
dual-encoder generator, in which the residual encoder produces the residual
features added to the foreground feature map from main encoder. Then, a
pixel-wise discriminator plays against the generator, encouraging the refined
foreground feature map to be indistinguishable from background feature map.
Extensive experiments demonstrate that our method could achieve more harmonious
and visually appealing results than previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xudong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Li Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Junyan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08652">
<title>Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing. (arXiv:2311.08652v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.08652</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception contracts provide a method for evaluating safety of control
systems that use machine learning for perception. A perception contract is a
specification for testing the ML components, and it gives a method for proving
end-to-end system-level safety requirements. The feasibility of contract-based
testing and assurance was established earlier in the context of straight lane
keeping: a 3-dimensional system with relatively simple dynamics. This paper
presents the analysis of two 6 and 12-dimensional flight control systems that
use multi-stage, heterogeneous, ML-enabled perception. The paper advances
methodology by introducing an algorithm for constructing data and requirement
guided refinement of perception contracts (DaRePC). The resulting analysis
provides testable contracts which establish the state and environment
conditions under which an aircraft can safety touchdown on the runway and a
drone can safely pass through a sequence of gates. It can also discover
conditions (e.g., low-horizon sun) that can possibly violate the safety of the
vision-based control system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Benjamin C Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yixuan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1&quot;&gt;Daniel Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Sayan Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08655">
<title>Review of AlexNet for Medical Image Classification. (arXiv:2311.08655v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the rapid development of deep learning has led to a wide
range of applications in the field of medical image classification. The
variants of neural network models with ever-increasing performance share some
commonalities: to try to mitigate overfitting, improve generalization, avoid
gradient vanishing and exploding, etc. AlexNet first utilizes the dropout
technique to mitigate overfitting and the ReLU activation function to avoid
gradient vanishing. Therefore, we focus our discussion on AlexNet, which has
contributed greatly to the development of CNNs in 2012. After reviewing over 40
papers, including journal papers and conference papers, we give a narrative on
the technical details, advantages, and application areas of AlexNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wenhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Junding Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuihua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yudong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08657">
<title>ConeQuest: A Benchmark for Cone Segmentation on Mars. (arXiv:2311.08657v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08657</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the years, space scientists have collected terabytes of Mars data from
satellites and rovers. One important set of features identified in Mars orbital
images is pitted cones, which are interpreted to be mud volcanoes believed to
form in regions that were once saturated in water (i.e., a lake or ocean).
Identifying pitted cones globally on Mars would be of great importance, but
expert geologists are unable to sort through the massive orbital image archives
to identify all examples. However, this task is well suited for computer
vision. Although several computer vision datasets exist for various
Mars-related tasks, there is currently no open-source dataset available for
cone detection/segmentation. Furthermore, previous studies trained models using
data from a single region, which limits their applicability for global
detection and mapping. Motivated by this, we introduce ConeQuest, the first
expert-annotated public dataset to identify cones on Mars. ConeQuest consists
of &amp;gt;13k samples from 3 different regions of Mars. We propose two benchmark
tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size
Generalization. We finetune and evaluate widely-used segmentation models on
both benchmark tasks. Results indicate that cone segmentation is a challenging
open problem not solved by existing segmentation models, which achieve an
average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and
(ii), respectively. We believe this new benchmark dataset will facilitate the
development of more accurate and robust models for cone segmentation. Data and
code are available at https://github.com/kerner-lab/ConeQuest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purohit_M/0/1/0/all/0/1&quot;&gt;Mirali Purohit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adler_J/0/1/0/all/0/1&quot;&gt;Jacob Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerner_H/0/1/0/all/0/1&quot;&gt;Hannah Kerner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08661">
<title>Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data. (arXiv:2311.08661v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2311.08661</link>
<description rdf:parseType="Literal">&lt;p&gt;As is true of many complex tasks, the work of discovering, describing, and
understanding the diversity of life on Earth (viz., biological systematics and
taxonomy) requires many tools. Some of this work can be accomplished as it has
been done in the past, but some aspects present us with challenges which
traditional knowledge and tools cannot adequately resolve. One such challenge
is presented by species complexes in which the morphological similarities among
the group members make it difficult to reliably identify known species and
detect new ones. We address this challenge by developing new tools using the
principles of machine learning to resolve two specific questions related to
species complexes. The first question is formulated as a classification problem
in statistics and machine learning and the second question is an
out-of-distribution (OOD) detection problem. We apply these tools to a species
complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex)
and employ a morphological character (hind limb skin texture) traditionally
treated qualitatively in a quantitative and objective manner. We demonstrate
that deep neural networks can successfully automate the classification of an
image into a known species group for which it has been trained. We further
demonstrate that the algorithm can successfully classify an image into a new
class if the image does not belong to the existing classes. Additionally, we
use the larger MNIST dataset to test the performance of our OOD detection
algorithm. We finish our paper with some concluding remarks regarding the
application of these methods to species complexes and our efforts to document
true biodiversity. This paper has online supplementary materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yili Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_E/0/1/0/all/0/1&quot;&gt;Eric P. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McLeod_D/0/1/0/all/0/1&quot;&gt;David S. McLeod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xinwei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Freeman_L/0/1/0/all/0/1&quot;&gt;Laura J. Freeman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08673">
<title>CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding. (arXiv:2311.08673v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08673</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a talking face generation method named &quot;CP-EB&quot; that takes
an audio signal as input and a person image as reference, to synthesize a
photo-realistic people talking video with head poses controlled by a short
video clip and proper eye blinking embedding. It&apos;s noted that not only the head
pose but also eye blinking are both important aspects for deep fake detection.
The implicit control of poses by video has already achieved by the state-of-art
work. According to recent research, eye blinking has weak correlation with
input audio which means eye blinks extraction from audio and generation are
possible. Hence, we propose a GAN-based architecture to extract eye blink
feature from input audio and reference video respectively and employ
contrastive training between them, then embed it into the concatenated features
of identity and poses to generate talking face images. Experimental results
show that the proposed method can generate photo-realistic talking face with
synchronous lips motions, natural head poses and blinking eyes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianzong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yimin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xulong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Ning Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08695">
<title>Attribute Diversity Determines the Systematicity Gap in VQA. (arXiv:2311.08695v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08695</link>
<description rdf:parseType="Literal">&lt;p&gt;The degree to which neural networks can generalize to new combinations of
familiar concepts, and the conditions under which they are able to do so, has
long been an open question. In this work, we study the systematicity gap in
visual question answering: the performance difference between reasoning on
previously seen and unseen combinations of object attributes. To test, we
introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased
quantity of training data does not reduce the systematicity gap, increased
training data diversity of the attributes in the unseen combination does. In
all, our experiments suggest that the more distinct attribute type combinations
are seen during training, the more systematic we can expect the resulting model
to be.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berlot_Attwell_I/0/1/0/all/0/1&quot;&gt;Ian Berlot-Attwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrell_A/0/1/0/all/0/1&quot;&gt;A. Michael Carrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1&quot;&gt;Kumar Krishna Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1&quot;&gt;Yash Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1&quot;&gt;Naomi Saphra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08716">
<title>Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories. (arXiv:2311.08716v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08716</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a privacy-preserving training method which consists of
training from a plurality of clients but without sharing their confidential
data. However, previous work on federated learning do not explore suitable
neural network architectures for clients with different input images sizes and
different numbers of output categories. In this paper, we propose an effective
federated learning method named ScalableFL, where the depths and widths of the
local models for each client are adjusted according to the clients&apos; input image
size and the numbers of output categories. In addition, we provide a new bound
for the generalization gap of federated learning. In particular, this bound
helps to explain the effectiveness of our scalable neural network approach. We
demonstrate the effectiveness of ScalableFL in several heterogeneous client
settings for both image classification and object detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitta_S/0/1/0/all/0/1&quot;&gt;Shuhei Nitta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1&quot;&gt;Taiji Suzuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mulet_A/0/1/0/all/0/1&quot;&gt;Albert Rodr&amp;#xed;guez Mulet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaguchi_A/0/1/0/all/0/1&quot;&gt;Atsushi Yaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirai_R/0/1/0/all/0/1&quot;&gt;Ryusuke Hirai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08746">
<title>A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video. (arXiv:2311.08746v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08746</link>
<description rdf:parseType="Literal">&lt;p&gt;Video post-processing methods can improve the quality of compressed videos at
the decoder side. Most of the existing methods need to train corresponding
models for compressed videos with different quantization parameters to improve
the quality of compressed videos. However, in most cases, the quantization
parameters of the decoded video are unknown. This makes existing methods have
their limitations in improving video quality. To tackle this problem, this work
proposes a diffusion model based post-processing method for compressed videos.
The proposed method first estimates the feature vectors of the compressed video
and then uses the estimated feature vectors as the prior information for the
quality enhancement model to adaptively enhance the quality of compressed video
with different quantization parameters. Experimental results show that the
quality enhancement results of our proposed method on mixed datasets are
superior to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Honggang Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08747">
<title>Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection. (arXiv:2311.08747v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08747</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared small target detection based on deep learning offers unique
advantages in separating small targets from complex and dynamic backgrounds.
However, the features of infrared small targets gradually weaken as the depth
of convolutional neural network (CNN) increases. To address this issue, we
propose a novel method for detecting infrared small targets called improved
dense nested attention network (IDNANet), which is based on the transformer
architecture. We preserve the dense nested structure of dense nested attention
network (DNANet) and introduce the Swin-transformer during feature extraction
stage to enhance the continuity of features. Furthermore, we integrate the
ACmix attention structure into the dense nested structure to enhance the
features of intermediate layers. Additionally, we design a weighted dice binary
cross-entropy (WD-BCE) loss function to mitigate the negative impact of
foreground-background imbalance in the samples. Moreover, we develop a dataset
specifically for infrared small targets, called BIT-SIRST. The dataset
comprises a significant amount of real-world targets and manually annotated
labels, as well as synthetic data and corresponding labels. We have evaluated
the effectiveness of our method through experiments conducted on public
datasets. In comparison to other state-of-the-art methods, our approach
outperforms in terms of probability of detection (P_d), false-alarm rate (F_a),
and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the
NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1&quot;&gt;Chun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yaqian Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianhua Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zechen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1&quot;&gt;Qun Hao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08759">
<title>4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters. (arXiv:2311.08759v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08759</link>
<description rdf:parseType="Literal">&lt;p&gt;The illumination of improperly exposed photographs has been widely corrected
using deep convolutional neural networks or Transformers. Despite with
promising performance, these methods usually suffer from large parameter
amounts and heavy computational FLOPs on high-resolution photographs. In this
paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale
Linear Transformation (MSLT) networks under the multi-layer perception
architecture, which can process 4K-resolution sRGB images at 125
Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT
networks first decompose an input image into high and low frequency layers by
Laplacian pyramid techniques, and then sequentially correct different layers by
pixel-adaptive linear transformation, which is implemented by efficient
bilateral grid learning or 1x1 convolutions. Experiments on two benchmark
datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts
on photo exposure correction. Extensive ablation studies validate the
effectiveness of our contributions. The code is available at
https://github.com/Zhou-Yijie/MSLTNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yijie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08764">
<title>Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning. (arXiv:2311.08764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08764</link>
<description rdf:parseType="Literal">&lt;p&gt;Class Incremental Learning (CIL) aims to handle the scenario where data of
novel classes occur continuously and sequentially. The model should recognize
the sequential novel classes while alleviating the catastrophic forgetting. In
the self-supervised manner, it becomes more challenging to avoid the conflict
between the feature embedding spaces of novel classes and old ones without any
class labels. To address the problem, we propose a self-supervised CIL
framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF
consists of a prototype clustering module (PC), an embedding space reserving
module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the
ESR modules reserve embedding space for subsequent phases at the prototype
level and the feature level respectively to prepare for knowledge learned in
the future. 2) The MTD module maintains the representations of the current
phase without the interference of past knowledge. One of the teacher networks
retains the representations of the past phases, and the other teacher network
distills relation information of the current phase to the student network.
Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our
proposed method boosts the performance of self-supervised class incremental
learning. We will release code in the near future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoshuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongtao Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08774">
<title>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation. (arXiv:2311.08774v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08774</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-assisted nuclei segmentation in histopathological images is a crucial task
in the diagnosis and treatment of cancer diseases. It decreases the time
required to manually screen microscopic tissue images and can resolve the
conflict between pathologists during diagnosis. Deep Learning has proven useful
in such a task. However, lack of labeled data is a significant barrier for deep
learning-based approaches. In this study, we propose a novel approach to nuclei
segmentation that leverages the available labelled and unlabelled data. The
proposed method combines the strengths of both transductive and inductive
learning, which have been previously attempted separately, into a single
framework. Inductive learning aims at approximating the general function and
generalizing to unseen test data, while transductive learning has the potential
of leveraging the unlabelled test data to improve the classification. To the
best of our knowledge, this is the first study to propose such a hybrid
approach for medical image segmentation. Moreover, we propose a novel two-stage
transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to
demonstrate the efficacy and potential of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1&quot;&gt;Hesham Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tondji_I/0/1/0/all/0/1&quot;&gt;Idriss Tondji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Mennatullah Siam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08782">
<title>Language Semantic Graph Guided Data-Efficient Learning. (arXiv:2311.08782v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing generalizable models that can effectively learn from limited data
and with minimal reliance on human supervision is a significant objective
within the machine learning community, particularly in the era of deep neural
networks. Therefore, to achieve data-efficient learning, researchers typically
explore approaches that can leverage more related or unlabeled data without
necessitating additional manual labeling efforts, such as Semi-Supervised
Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL
leverages unlabeled data in the training process, while TL enables the transfer
of expertise from related data distributions. DA broadens the dataset by
synthesizing new data from existing examples. However, the significance of
additional knowledge contained within labels has been largely overlooked in
research. In this paper, we propose a novel perspective on data efficiency that
involves exploiting the semantic information contained in the labels of the
available data. Specifically, we introduce a Language Semantic Graph (LSG)
which is constructed from labels manifest as natural language descriptions.
Upon this graph, an auxiliary graph neural network is trained to extract
high-level semantic relations and then used to guide the training of the
primary model, enabling more adequate utilization of label knowledge. Across
image, video, and audio modalities, we utilize the LSG method in both TL and
SSL scenarios and illustrate its versatility in significantly enhancing
performance compared to other data-efficient learning approaches. Additionally,
our in-depth analysis shows that the LSG method also expedites the training
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wenxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Lincan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jingxuan Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08786">
<title>HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection. (arXiv:2311.08786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08786</link>
<description rdf:parseType="Literal">&lt;p&gt;With the popularity of smart devices and the development of computer vision
technology, concerns about face privacy protection are growing. The face
de-identification technique is a practical way to solve the identity protection
problem. The existing facial de-identification methods have revealed several
problems, including the impact on the realism of anonymized results when faced
with occlusions and the inability to maintain identity-irrelevant details in
anonymized results. We present a High-Fidelity and Occlusion-Robust
De-identification (HFORD) method to deal with these issues. This approach can
disentangle identities and attributes while preserving image-specific details
such as background, facial features (e.g., wrinkles), and lighting, even in
occluded scenes. To disentangle the latent codes in the GAN inversion space, we
introduce an Identity Disentanglement Module (IDM). This module selects the
latent codes that are closely related to the identity. It further separates the
latent codes into identity-related codes and attribute-related codes, enabling
the network to preserve attributes while only modifying the identity. To ensure
the preservation of image details and enhance the network&apos;s robustness to
occlusions, we propose an Attribute Retention Module (ARM). This module
adaptively preserves identity-irrelevant details and facial occlusions and
blends them into the generated results in a modulated manner. Extensive
experiments show that our method has higher quality, better detail fidelity,
and stronger occlusion robustness than other face de-identification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingrui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08799">
<title>EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery. (arXiv:2311.08799v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.08799</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic ophthalmic surgery is an emerging technology to facilitate
high-precision interventions such as retina penetration in subretinal injection
and removal of floating tissues in retinal detachment depending on the input
imaging modalities such as microscopy and intraoperative OCT (iOCT). Although
iOCT is explored to locate the needle tip within its range-limited ROI, it is
still difficult to coordinate iOCT&apos;s motion with the needle, especially at the
initial target-approaching stage. Meanwhile, due to 2D perspective projection
and thus the loss of depth information, current image-based methods cannot
effectively estimate the needle tip&apos;s trajectory towards both retinal and
floating targets. To address this limitation, we propose to use the shadow
positions of the target and the instrument tip to estimate their relative depth
position and accordingly optimize the instrument tip&apos;s insertion trajectory
until the tip approaches targets within iOCT&apos;s scanning area. Our method
succeeds target approaching on a retina model, and achieves an average depth
error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively
in the surgical simulator without damaging the retina.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Siyuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zapp_D/0/1/0/all/0/1&quot;&gt;Daniel Zapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_M/0/1/0/all/0/1&quot;&gt;Mathias Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasseri_M/0/1/0/all/0/1&quot;&gt;M. Ali Nasseri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08806">
<title>SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer. (arXiv:2311.08806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08806</link>
<description rdf:parseType="Literal">&lt;p&gt;As the third-generation neural network, the Spiking Neural Network (SNN) has
the advantages of low power consumption and high energy efficiency, making it
suitable for implementation on edge devices. More recently, the most advanced
SNN, Spikformer, combines the self-attention module from Transformer with SNN
to achieve remarkable performance. However, it adopts larger channel dimensions
in MLP layers, leading to an increased number of redundant model parameters. To
effectively decrease the computational complexity and weight parameters of the
model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very
sparse ($\ge$90%) subnetwork that achieves comparable performance to the
original network. Furthermore, we also design a lightweight token selector
module, which can remove unimportant background information from images based
on the average spike firing rate of neurons, selecting only essential
foreground image tokens to participate in attention calculation. Based on that,
we present SparseSpikformer, a co-design framework aimed at achieving sparsity
in Spikformer through token and weight pruning techniques. Experimental results
demonstrate that our framework can significantly reduce 90% model parameters
and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining
the accuracy of the original model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shanlin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08811">
<title>Correlation-aware active learning for surgery video segmentation. (arXiv:2311.08811v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08811</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a complex task that relies heavily on large amounts
of annotated image data. However, annotating such data can be time-consuming
and resource-intensive, especially in the medical domain. Active Learning (AL)
is a popular approach that can help to reduce this burden by iteratively
selecting images for annotation to improve the model performance. In the case
of video data, it is important to consider the model uncertainty and the
temporal nature of the sequences when selecting images for annotation. This
work proposes a novel AL strategy for surgery video segmentation, \COALSamp{},
COrrelation-aWare Active Learning. Our approach involves projecting images into
a latent space that has been fine-tuned using contrastive learning and then
selecting a fixed number of representative images from local clusters of video
frames. We demonstrate the effectiveness of this approach on two video datasets
of surgical instruments and three real-world video datasets. The datasets and
code will be made publicly available upon receiving necessary approvals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marquez_Neila_P/0/1/0/all/0/1&quot;&gt;Pablo Marquez-Neila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Mingyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafii_Tari_H/0/1/0/all/0/1&quot;&gt;Hedyeh Rafii-Tari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1&quot;&gt;Raphael Sznitman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08815">
<title>Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations. (arXiv:2311.08815v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised representation learning often uses data augmentations to
induce some invariance to &quot;style&quot; attributes of the data. However, with
downstream tasks generally unknown at training time, it is difficult to deduce
a priori which attributes of the data are indeed &quot;style&quot; and can be safely
discarded. To address this, we introduce a more principled approach that seeks
to disentangle style features rather than discard them. The key idea is to add
multiple style embedding spaces where: (i) each is invariant to all-but-one
augmentation; and (ii) joint entropy is maximized. We formalize our structured
data-augmentation procedure from a causal latent-variable-model perspective,
and prove identifiability of both content and (multiple blocks of) style
variables. We empirically demonstrate the benefits of our approach on synthetic
datasets and then present promising but limited results on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1&quot;&gt;Cian Eastwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ericsson_L/0/1/0/all/0/1&quot;&gt;Linus Ericsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchacourt_D/0/1/0/all/0/1&quot;&gt;Diane Bouchacourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1&quot;&gt;Pascal Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Mark Ibrahim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08816">
<title>Target-oriented Domain Adaptation for Infrared Image Super-Resolution. (arXiv:2311.08816v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08816</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent efforts have explored leveraging visible light images to enrich
texture details in infrared (IR) super-resolution. However, this direct
adaptation approach often becomes a double-edged sword, as it improves texture
at the cost of introducing noise and blurring artifacts. To address these
challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN),
an innovative framework specifically engineered for robust IR super-resolution
model adaptation. DASRGAN operates on the synergy of two key components: 1)
Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and
2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer.
Specifically, TOA uniquely integrates a specialized discriminator,
incorporating a prior extraction branch, and employs a Sobel-guided adversarial
loss to align texture distributions effectively. Concurrently, NOA utilizes a
noise adversarial loss to distinctly separate the generative and Gaussian noise
pattern distributions during adversarial training. Our extensive experiments
confirm DASRGAN&apos;s superiority. Comparative analyses against leading methods
across multiple benchmarks and upsampling factors reveal that DASRGAN sets new
state-of-the-art performance standards. Code are available at
\url{https://github.com/yongsongH/DASRGAN}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miyazaki_T/0/1/0/all/0/1&quot;&gt;Tomo Miyazaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yafei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Omachi_S/0/1/0/all/0/1&quot;&gt;Shinichiro Omachi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08819">
<title>Frequency Domain-based Dataset Distillation. (arXiv:2311.08819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08819</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents FreD, a novel parameterization method for dataset
distillation, which utilizes the frequency domain to distill a small-sized
synthetic dataset from a large-sized original dataset. Unlike conventional
approaches that focus on the spatial domain, FreD employs frequency-based
transforms to optimize the frequency representations of each data instance. By
leveraging the concentration of spatial domain information on specific
frequency components, FreD intelligently selects a subset of frequency
dimensions for optimization, leading to a significant reduction in the required
budget for synthesizing an instance. Through the selection of frequency
dimensions based on the explained variance, FreD demonstrates both theoretical
and empirical evidence of its ability to operate efficiently within a limited
budget, while better preserving the information of the original dataset
compared to conventional parameterization methods. Furthermore, based on the
orthogonal compatibility of FreD with existing methods, we confirm that FreD
consistently improves the performances of existing distillation methods over
the evaluation scenarios with different benchmark datasets. We release the code
at https://github.com/sdh0818/FreD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1&quot;&gt;Donghyeok Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Seungjae Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1&quot;&gt;Il-Chul Moon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08835">
<title>Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding. (arXiv:2311.08835v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08835</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent endeavors in video temporal grounding enforce strong cross-modal
interactions through attention mechanisms to overcome the modality gap between
video and text query. However, previous works treat all video clips equally
regardless of their semantic relevance with the text query in attention
modules. In this paper, our goal is to provide clues for query-associated video
clips within the crossmodal encoding process. With our Correlation-Guided
Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of
cross-modal interactions and how to exploit such degrees for prediction. First,
we design an adaptive cross-attention layer with dummy tokens. Dummy tokens
conditioned by text query take a portion of the attention weights, preventing
irrelevant video clips from being represented by the text query. Yet, not all
word tokens equally inherit the text query&apos;s correlation to video clips. Thus,
we further guide the cross-attention map by inferring the fine-grained
correlation between video clips and words. We enable this by learning a joint
embedding space for high-level concepts, i.e., moment and sentence level, and
inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency
detector to exploit each video clip&apos;s degrees of text engagement. We validate
the superiority of CG-DETR with the state-of-the-art results on various
benchmarks for both moment retrieval and highlight detection. Codes are
available at https://github.com/wjun0830/CGDETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_W/0/1/0/all/0/1&quot;&gt;WonJun Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_S/0/1/0/all/0/1&quot;&gt;Sangeek Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;SuBeen Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Jae-Pil Heo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08843">
<title>Personalized Video Relighting Using Casual Light Stage. (arXiv:2311.08843v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08843</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a personalized video relighting algorithm that
produces high-quality and temporally consistent relit video under any pose,
expression, and lighting conditions in real-time. Existing relighting
algorithms typically rely either on publicly available synthetic data, which
yields poor relighting results, or instead on Light Stage data which is
inaccessible and is not publicly available. We show that by casually capturing
video of a user watching YouTube videos on a monitor we can train a
personalized algorithm capable of producing high-quality relighting under any
condition. Our key contribution is a novel neural relighting architecture that
effectively separates the intrinsic appearance features, geometry and
reflectance, from the source lighting and then combines it with the target
lighting to generate a relit image. This neural architecture enables smoothing
of intrinsic appearance features leading to temporally stable video relighting.
Both qualitative and quantitative evaluations show that our relighting
architecture improves portrait image relighting quality and temporal
consistency over state-of-the-art approaches on both casually captured Light
Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time
(OLAT) datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jun Myeong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christman_M/0/1/0/all/0/1&quot;&gt;Max Christman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_R/0/1/0/all/0/1&quot;&gt;Roni Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08844">
<title>Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder. (arXiv:2311.08844v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08844</link>
<description rdf:parseType="Literal">&lt;p&gt;Although image captioning has a vast array of applications, it has not
reached its full potential in languages other than English. Arabic, for
instance, although the native language of more than 400 million people, remains
largely underrepresented in this area. This is due to the lack of labeled data
and powerful Arabic generative models. We alleviate this issue by presenting a
novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our
model is based on a vision encoder and a Gemini text decoder that maintains
generation fluency while allowing fusion between the vision and language
components. To train our model, we introduce a new method for automatically
acquiring data from available English datasets. We also manually prepare a new
dataset for evaluation. \textit{Violet} performs sizeably better than our
baselines on all of our evaluation datasets. For example, it reaches a CIDEr
score of $61.2$ on our manually annotated dataset and achieves an improvement
of $13$ points on Flickr8k.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alwajih_F/0/1/0/all/0/1&quot;&gt;Fakhraddin Alwajih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1&quot;&gt;El Moatez Billah Nagoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inciarte_A/0/1/0/all/0/1&quot;&gt;Alcides Alcoba Inciarte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdul-Mageed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08850">
<title>Controlling the Output of a Generative Model by Latent Feature Vector Shifting. (arXiv:2311.08850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08850</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias})
often generate photorealistic images based on vectors sampled from their latent
space. However, the ability to control the output is limited. Here we present
our novel method for latent vector shifting for controlled output image
modification utilizing semantic features of the generated images. In our
approach we use a pre-trained model of StyleGAN3 that generates images of
realistic human faces in relatively high resolution. We complement the
generative model with a convolutional neural network classifier, namely
ResNet34, trained to classify the generated images with binary facial features
from the CelebA dataset. Our latent feature shifter is a neural network model
with a task to shift the latent vectors of a generative model into a specified
feature direction. We have trained latent feature shifter for multiple facial
features, and outperformed our baseline method in the number of generated
images with the desired feature. To train our latent feature shifter neural
network, we have designed a dataset of pairs of latent vectors with and without
a certain feature. Based on the evaluation, we conclude that our latent feature
shifter approach was successful in the controlled generation of the StyleGAN3
generator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belanec_R/0/1/0/all/0/1&quot;&gt;R&amp;#xf3;bert Belanec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacko_P/0/1/0/all/0/1&quot;&gt;Peter Lacko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinovska_K/0/1/0/all/0/1&quot;&gt;Krist&amp;#xed;na Malinovsk&amp;#xe1;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08851">
<title>Data Augmentations in Deep Weight Spaces. (arXiv:2311.08851v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08851</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning in weight spaces, where neural networks process the weights of other
deep neural networks, has emerged as a promising research direction with
applications in various fields, from analyzing and editing neural fields and
implicit neural representations, to network pruning and quantization. Recent
works designed architectures for effective learning in that space, which takes
into account its unique, permutation-equivariant, structure. Unfortunately, so
far these architectures suffer from severe overfitting and were shown to
benefit from large datasets. This poses a significant challenge because
generating data for this learning setup is laborious and time-consuming since
each data sample is a full set of network weights that has to be trained. In
this paper, we address this difficulty by investigating data augmentations for
weight spaces, a set of techniques that enable generating new data examples on
the fly without having to train additional input weight space elements. We
first review several recently proposed data augmentation schemes %that were
proposed recently and divide them into categories. We then introduce a novel
augmentation scheme based on the Mixup method. We evaluate the performance of
these techniques on existing benchmarks as well as new benchmarks we generate,
which can be valuable for future studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1&quot;&gt;Aviv Shamsian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David W. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1&quot;&gt;Aviv Navon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kofinas_M/0/1/0/all/0/1&quot;&gt;Miltiadis Kofinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1&quot;&gt;Idan Achituve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valperga_R/0/1/0/all/0/1&quot;&gt;Riccardo Valperga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1&quot;&gt;Gertjan J. Burghouts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1&quot;&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1&quot;&gt;Ethan Fetaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1&quot;&gt;Haggai Maron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08863">
<title>Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques. (arXiv:2311.08863v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08863</link>
<description rdf:parseType="Literal">&lt;p&gt;Airborne hyperspectral images can be used to map the land cover in large
urban areas, thanks to their very high spatial and spectral resolutions on a
wide spectral domain. While the spectral dimension of hyperspectral images is
highly informative of the chemical composition of the land surface, the use of
state-of-the-art machine learning algorithms to map the land cover has been
dramatically limited by the availability of training data. To cope with the
scarcity of annotations, semi-supervised and self-supervised techniques have
lately raised a lot of interest in the community. Yet, the publicly available
hyperspectral data sets commonly used to benchmark machine learning models are
not totally suited to evaluate their generalization performances due to one or
several of the following properties: a limited geographical coverage (which
does not reflect the spectral diversity in metropolitan areas), a small number
of land cover classes and a lack of appropriate standard train / test splits
for semi-supervised and self-supervised learning. Therefore, we release in this
paper the Toulouse Hyperspectral Data Set that stands out from other data sets
in the above-mentioned respects in order to meet key issues in spectral
representation learning and classification over large-scale hyperspectral
images with very few labeled pixels. Besides, we discuss and experiment the
self-supervised task of Masked Autoencoders and establish a baseline for
pixel-wise classification based on a conventional autoencoder combined with a
Random Forest classifier achieving 82% overall accuracy and 74% F1 score. The
Toulouse Hyperspectral Data Set and our code are publicly available at
https://www.toulouse-hyperspectral-data-set.com and
https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoreau_R/0/1/0/all/0/1&quot;&gt;Romain Thoreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risser_L/0/1/0/all/0/1&quot;&gt;Laurent Risser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achard_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe9;ronique Achard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berthelot_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe9;atrice Berthelot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briottet_X/0/1/0/all/0/1&quot;&gt;Xavier Briottet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08870">
<title>One-Shot Federated Learning with Classifier-Guided Diffusion Models. (arXiv:2311.08870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08870</link>
<description rdf:parseType="Literal">&lt;p&gt;One-shot federated learning (OSFL) has gained attention in recent years due
to its low communication cost. However, most of the existing methods require
auxiliary datasets or training generators, which hinders their practicality in
real-world scenarios. In this paper, we explore the novel opportunities that
diffusion models bring to OSFL and propose FedCADO, utilizing guidance from
client classifiers to generate data that complies with clients&apos; distributions
and subsequently training the aggregated model on the server. Specifically, our
method involves targeted optimizations in two aspects. On one hand, we
conditionally edit the randomly sampled initial noises, embedding them with
specified semantics and distributions, resulting in a significant improvement
in both the quality and stability of generation. On the other hand, we employ
the BN statistics from the classifiers to provide detailed guidance during
generation. These tailored optimizations enable us to limitlessly generate
datasets, which closely resemble the distribution and quality of the original
client dataset. Our method effectively handles the heterogeneous client models
and the problems of non-IID features or labels. In terms of privacy protection,
our method avoids training any generator or transferring any auxiliary
information on clients, eliminating any additional privacy leakage risks.
Leveraging the extensive knowledge stored in the pre-trained diffusion model,
the synthetic datasets can assist us in surpassing the knowledge limitations of
the client samples, resulting in aggregation models that even outperform the
performance ceiling of centralized training in some cases, which is
convincingly demonstrated in the sufficient quantification and visualization
experiments conducted on three large-scale multi-domain image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingzhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shangchao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08891">
<title>AdapterShadow: Adapting Segment Anything Model for Shadow Detection. (arXiv:2311.08891v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08891</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment anything model (SAM) has shown its spectacular performance in
segmenting universal objects, especially when elaborate prompts are provided.
However, the drawback of SAM is twofold. On the first hand, it fails to segment
specific targets, e.g., shadow images or lesions in medical images. On the
other hand, manually specifying prompts is extremely time-consuming. To
overcome the problems, we propose AdapterShadow, which adapts SAM model for
shadow detection. To adapt SAM for shadow images, trainable adapters are
inserted into the frozen image encoder of SAM, since the training of the full
SAM model is both time and memory consuming. Moreover, we introduce a novel
grid sampling method to generate dense point prompts, which helps to
automatically segment shadows without any manual interventions. Extensive
experiments are conducted on four widely used benchmark datasets to demonstrate
the superior performance of our proposed method. Codes will are publicly
available at https://github.com/LeipingJie/AdapterShadow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_L/0/1/0/all/0/1&quot;&gt;Leiping Jie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08908">
<title>Robust Brain MRI Image Classification with SIBOW-SVM. (arXiv:2311.08908v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2311.08908</link>
<description rdf:parseType="Literal">&lt;p&gt;The majority of primary Central Nervous System (CNS) tumors in the brain are
among the most aggressive diseases affecting humans. Early detection of brain
tumor types, whether benign or malignant, glial or non-glial, is critical for
cancer prevention and treatment, ultimately improving human life expectancy.
Magnetic Resonance Imaging (MRI) stands as the most effective technique to
detect brain tumors by generating comprehensive brain images through scans.
However, human examination can be error-prone and inefficient due to the
complexity, size, and location variability of brain tumors. Recently, automated
classification techniques using machine learning (ML) methods, such as
Convolutional Neural Network (CNN), have demonstrated significantly higher
accuracy than manual screening, while maintaining low computational costs.
Nonetheless, deep learning-based image classification methods, including CNN,
face challenges in estimating class probabilities without proper model
calibration. In this paper, we propose a novel brain tumor image classification
method, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with
SIFT feature extraction and weighted Support Vector Machines (wSVMs). This new
approach effectively captures hidden image features, enabling the
differentiation of various tumor types and accurate label predictions.
Additionally, the SIBOW-SVM is able to estimate the probabilities of images
belonging to each class, thereby providing high-confidence classification
decisions. We have also developed scalable and parallelable algorithms to
facilitate the practical implementation of SIBOW-SVM for massive images. As a
benchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI
images containing four classes: glioma, meningioma, pituitary, and normal. Our
results show that the new method outperforms state-of-the-art methods,
including CNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Liyun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Helen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08909">
<title>DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack. (arXiv:2311.08909v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08909</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) are extremely computationally demanding, which
presents a large barrier to their deployment on resource-constrained devices.
Since such devices are where many emerging deep learning applications lie
(e.g., drones, vision-based medical technology), significant bodies of work
from both the machine learning and systems communities have attempted to
provide optimizations to accelerate DNNs. To help unify these two perspectives,
in this paper we combine machine learning and systems techniques within the
Deep Learning Acceleration Stack (DLAS), and demonstrate how these layers can
be tightly dependent on each other with an across-stack perturbation study. We
evaluate the impact on accuracy and inference time when varying different
parameters of DLAS across two datasets, seven popular DNN architectures, four
DNN compression techniques, three algorithmic primitives with sparse and dense
variants, untuned and auto-scheduled code generation, and four hardware
platforms. Our evaluation highlights how perturbations across DLAS parameters
can cause significant variation and across-stack interactions. The highest
level observation from our evaluation is that the model size, accuracy, and
inference time are not guaranteed to be correlated. Overall we make 13 key
observations, including that speedups provided by compression techniques are
very hardware dependent, and that compiler auto-tuning can significantly alter
what the best algorithm to use for a given configuration is. With DLAS, we aim
to provide a reference framework to aid machine learning and systems
practitioners in reasoning about the context in which their respective DNN
acceleration solutions exist in. With our evaluation strongly motivating the
need for co-design, we believe that DLAS can be a valuable concept for
exploring the next generation of co-designed accelerated deep learning
solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_P/0/1/0/all/0/1&quot;&gt;Perry Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cano_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1&quot;&gt;Elliot J. Crowley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1&quot;&gt;Amos Storkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OBoyle_M/0/1/0/all/0/1&quot;&gt;Michael O&amp;#x27;Boyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08910">
<title>Progressive Feedback-Enhanced Transformer for Image Forgery Localization. (arXiv:2311.08910v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08910</link>
<description rdf:parseType="Literal">&lt;p&gt;Blind detection of the forged regions in digital images is an effective
authentication means to counter the malicious use of local image editing
techniques. Existing encoder-decoder forensic networks overlook the fact that
detecting complex and subtle tampered regions typically requires more feedback
information. In this paper, we propose a Progressive FeedbACk-enhanced
Transformer (ProFact) network to achieve coarse-to-fine image forgery
localization. Specifically, the coarse localization map generated by an initial
branch network is adaptively fed back to the early transformer encoder layers
for enhancing the representation of positive features while suppressing
interference factors. The cascaded transformer network, combined with a
contextual spatial pyramid module, is designed to refine discriminative
forensic features for improving the forgery localization accuracy and
reliability. Furthermore, we present an effective strategy to automatically
generate large-scale forged image samples close to real-world forensic
scenarios, especially in realistic and coherent processing. Leveraging on such
samples, a progressive and cost-effective two-stage training protocol is
applied to the ProFact network. The extensive experimental results on nine
public forensic datasets show that our proposed localizer greatly outperforms
the state-of-the-art on the generalization ability and robustness of image
forgery localization. Code will be publicly available at
https://github.com/multimediaFor/ProFact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Haochen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Gang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xianglin Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08923">
<title>Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery. (arXiv:2311.08923v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08923</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural protected areas are vital for biodiversity, climate change
mitigation, and supporting ecological processes. Despite their significance,
comprehensive mapping is hindered by a lack of understanding of their
characteristics and a missing land cover class definition. This paper aims to
advance the explanation of the designating patterns forming protected and wild
areas. To this end, we propose a novel framework that uses activation
maximization and a generative adversarial model. With this, we aim to generate
satellite images that, in combination with domain knowledge, are capable of
offering complete and valid explanations for the spatial and spectral patterns
that define the natural authenticity of these regions. Our proposed framework
produces more precise attribution maps pinpointing the designating patterns
forming the natural authenticity of protected areas. Our approach fosters our
understanding of the ecological integrity of the protected natural areas and
may contribute to future monitoring and preservation efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emam_A/0/1/0/all/0/1&quot;&gt;Ahmed Emam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stomberg_T/0/1/0/all/0/1&quot;&gt;Timo T. Stomberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1&quot;&gt;Ribana Roscher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08931">
<title>Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation. (arXiv:2311.08931v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08931</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores uncertainty quantification (UQ) as an indicator of the
trustworthiness of automated deep-learning (DL) tools in the context of white
matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of
multiple sclerosis (MS) patients. Our study focuses on two principal aspects of
uncertainty in structured output segmentation tasks. Firstly, we postulate that
a good uncertainty measure should indicate predictions likely to be incorrect
with high uncertainty values. Second, we investigate the merit of quantifying
uncertainty at different anatomical scales (voxel, lesion, or patient). We
hypothesize that uncertainty at each scale is related to specific types of
errors. Our study aims to confirm this relationship by conducting separate
analyses for in-domain and out-of-domain settings. Our primary methodological
contributions are (i) the development of novel measures for quantifying
uncertainty at lesion and patient scales, derived from structural prediction
discrepancies, and (ii) the extension of an error retention curve analysis
framework to facilitate the evaluation of UQ performance at both lesion and
patient scales. The results from a multi-centric MRI dataset of 172 patients
demonstrate that our proposed measures more effectively capture model errors at
the lesion and patient scales compared to measures that average voxel-scale
uncertainty values. We provide the UQ protocols code at
https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molchanova_N/0/1/0/all/0/1&quot;&gt;Nataliia Molchanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1&quot;&gt;Vatsal Raina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1&quot;&gt;Andrey Malinin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_F/0/1/0/all/0/1&quot;&gt;Francesco La Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Depeursinge_A/0/1/0/all/0/1&quot;&gt;Adrien Depeursinge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1&quot;&gt;Mark Gales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granziera_C/0/1/0/all/0/1&quot;&gt;Cristina Granziera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziani_M/0/1/0/all/0/1&quot;&gt;Mara Graziani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuadra_M/0/1/0/all/0/1&quot;&gt;Meritxell Bach Cuadra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08936">
<title>Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness. (arXiv:2311.08936v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.08936</link>
<description rdf:parseType="Literal">&lt;p&gt;Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emam_A/0/1/0/all/0/1&quot;&gt;Ahmed Emam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farag_M/0/1/0/all/0/1&quot;&gt;Mohamed Farag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1&quot;&gt;Ribana Roscher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08949">
<title>Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard. (arXiv:2311.08949v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.08949</link>
<description rdf:parseType="Literal">&lt;p&gt;The volume-corrected mitotic index (M/V-Index) was shown to provide
prognostic value in invasive breast carcinomas. However, despite its prognostic
significance, it is not established as the standard method for assessing
aggressive biological behaviour, due to the high additional workload associated
with determining the epithelial proportion. In this work, we show that using a
deep learning pipeline solely trained with an annotation-free,
immunohistochemistry-based approach, provides accurate estimations of
epithelial segmentation in canine breast carcinomas. We compare our automatic
framework with the manually annotated M/V-Index in a study with three
board-certified pathologists. Our results indicate that the deep learning-based
pipeline shows expert-level performance, while providing time efficiency and
reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ammeling_J/0/1/0/all/0/1&quot;&gt;Jonas Ammeling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hecker_M/0/1/0/all/0/1&quot;&gt;Moritz Hecker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ganz_J/0/1/0/all/0/1&quot;&gt;Jonathan Ganz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Donovan_T/0/1/0/all/0/1&quot;&gt;Taryn A. Donovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1&quot;&gt;Christof A. Bertram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1&quot;&gt;Katharina Breininger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1&quot;&gt;Marc Aubreville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08955">
<title>A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution. (arXiv:2311.08955v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08955</link>
<description rdf:parseType="Literal">&lt;p&gt;Fusion-based hyperspectral image (HSI) super-resolution aims to produce a
high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a
high-spatial-resolution multispectral image. Such a HSI super-resolution
process can be modeled as an inverse problem, where the prior knowledge is
essential for obtaining the desired solution. Motivated by the success of
diffusion models, we propose a novel spectral diffusion prior for fusion-based
HSI super-resolution. Specifically, we first investigate the spectrum
generation problem and design a spectral diffusion model to model the spectral
data distribution. Then, in the framework of maximum a posteriori, we keep the
transition information between every two neighboring states during the reverse
generative process, and thereby embed the knowledge of trained spectral
diffusion model into the fusion problem in the form of a regularization term.
At last, we treat each generation step of the final optimization problem as its
subproblem, and employ the Adam to solve these subproblems in a reverse
sequence. Experimental results conducted on both synthetic and real datasets
demonstrate the effectiveness of the proposed approach. The code of the
proposed approach will be available on https://github.com/liuofficial/SDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianjun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zebin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Liang Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08972">
<title>Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging. (arXiv:2311.08972v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08972</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised deep learning approaches have recently become one of the crucial
research areas in imaging owing to their ability to learn expressive and
powerful reconstruction operators even when paired high-quality training data
is scarcely available. In this chapter, we review theoretically principled
unsupervised learning schemes for solving imaging inverse problems, with a
particular focus on methods rooted in optimal transport and convex analysis. We
begin by reviewing the optimal transport-based unsupervised approaches such as
the cycle-consistency-based models and learned adversarial regularization
methods, which have clear probabilistic interpretations. Subsequently, we give
an overview of a recent line of works on provably convergent learned
optimization algorithms applied to accelerate the solution of imaging inverse
problems, alongside their dedicated unsupervised training schemes. We also
survey a number of provably convergent plug-and-play algorithms (based on
gradient-step deep denoisers), which are among the most important and widely
applied unsupervised approaches for imaging problems. At the end of this
survey, we provide an overview of a few related unsupervised learning
frameworks that complement our focused schemes. Together with a detailed
survey, we provide an overview of the key mathematical results that underlie
the methods reviewed in the chapter to keep our discussion self-contained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1&quot;&gt;Marcello Carioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Subhadip Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hong Ye Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Junqi Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08995">
<title>Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images. (arXiv:2311.08995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.08995</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality labeled datasets are essential for deep learning. Traditional
manual annotation methods are not only costly and inefficient but also pose
challenges in specialized domains where expert knowledge is needed.
Self-supervised methods, despite leveraging unlabeled data for feature
extraction, still require hundreds or thousands of labeled instances to guide
the model for effective specialized image classification. Current unsupervised
learning methods offer automatic classification without prior annotation but
often compromise on accuracy. As a result, efficiently procuring high-quality
labeled datasets remains a pressing challenge for specialized domain images
devoid of annotated data. Addressing this, an unsupervised classification
method with three key ideas is introduced: 1) dual-step feature dimensionality
reduction using a pre-trained model and manifold learning, 2) a voting
mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior
manual annotation. This approach outperforms supervised methods in
classification accuracy, as demonstrated with fungal image data, achieving
94.1% and 96.7% on public and private datasets respectively. The proposed
unsupervised classification method reduces dependency on pre-annotated
datasets, enabling a closed-loop for data classification. The simplicity and
ease of use of this method will also bring convenience to researchers in
various fields in building datasets, promoting AI applications for images in
specialized domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+liu_Z/0/1/0/all/0/1&quot;&gt;Zhaocong liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fa Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Huanxi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chichun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09004">
<title>Incremental Object-Based Novelty Detection with Feedback Loop. (arXiv:2311.09004v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09004</link>
<description rdf:parseType="Literal">&lt;p&gt;Object-based Novelty Detection (ND) aims to identify unknown objects that do
not belong to classes seen during training by an object detection model. The
task is particularly crucial in real-world applications, as it allows to avoid
potentially harmful behaviours, e.g. as in the case of object detection models
adopted in a self-driving car or in an autonomous robot. Traditional approaches
to ND focus on one time offline post processing of the pretrained object
detection output, leaving no possibility to improve the model robustness after
training and discarding the abundant amount of out-of-distribution data
encountered during deployment.
&lt;/p&gt;
&lt;p&gt;In this work, we propose a novel framework for object-based ND, assuming that
human feedback can be requested on the predicted output and later incorporated
to refine the ND model without negatively affecting the main object detection
performance. This refinement operation is repeated whenever new feedback is
available. To tackle this new formulation of the problem for object detection,
we propose a lightweight ND module attached on top of a pre-trained object
detection model, which is incrementally updated through a feedback loop. We
also propose a new benchmark to evaluate methods on this new setting and test
extensively our ND approach against baselines, showing increased robustness and
a successful incorporation of the received feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caldarella_S/0/1/0/all/0/1&quot;&gt;Simone Caldarella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1&quot;&gt;Rahaf Aljundi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09024">
<title>Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing. (arXiv:2311.09024v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09024</link>
<description rdf:parseType="Literal">&lt;p&gt;A key benefit of deep vision-language models such as CLIP is that they enable
zero-shot open vocabulary classification; the user has the ability to define
novel class labels via natural language prompts at inference time. However,
while CLIP-based zero-shot classifiers have demonstrated competitive
performance across a range of domain shifts, they remain highly vulnerable to
adversarial attacks. Therefore, ensuring the robustness of such models is
crucial for their reliable deployment in the wild.
&lt;/p&gt;
&lt;p&gt;In this work, we introduce Open Vocabulary Certification (OVC), a fast
certification method designed for open-vocabulary models like CLIP via
randomized smoothing techniques. Given a base &quot;training&quot; set of prompts and
their corresponding certified CLIP classifiers, OVC relies on the observation
that a classifier with a novel prompt can be viewed as a perturbed version of
nearby classifiers in the base training set. Therefore, OVC can rapidly certify
the novel classifier using a variation of incremental randomized smoothing. By
using a caching trick, we achieve approximately two orders of magnitude
acceleration in the certification process for novel prompts. To achieve further
(heuristic) speedups, OVC approximates the embedding space at a given input
using a multivariate normal distribution bypassing the need for sampling via
forward passes through the vision backbone. We demonstrate the effectiveness of
OVC on through experimental evaluation using multiple vision-language backbones
on the CIFAR-10 and ImageNet test datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nirala_A/0/1/0/all/0/1&quot;&gt;A K Nirala&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;A Joshi&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;C Hegde&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;S Sarkar&lt;/a&gt; (1) ((1) Iowa State University, (2) New York University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09029">
<title>Self-Annotated 3D Geometric Learning for Smeared Points Removal. (arXiv:2311.09029v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09029</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been significant progress in improving the accuracy and quality of
consumer-level dense depth sensors. Nevertheless, there remains a common depth
pixel artifact which we call smeared points. These are points not on any 3D
surface and typically occur as interpolations between foreground and background
objects. As they cause fictitious surfaces, these points have the potential to
harm applications dependent on the depth maps. Statistical outlier removal
methods fare poorly in removing these points as they tend also to remove actual
surface points. Trained network-based point removal faces difficulty in
obtaining sufficient annotated data. To address this, we propose a fully
self-annotated method to train a smeared point removal classifier. Our approach
relies on gathering 3D geometric evidence from multiple perspectives to
automatically detect and annotate smeared points and valid points. To validate
the effectiveness of our method, we present a new benchmark dataset: the Real
Azure-Kinect dataset. Experimental results and ablation studies show that our
method outperforms traditional filters and other self-annotated methods. Our
work is publicly available at
https://github.com/wangmiaowei/wacv2024_smearedremover.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1&quot;&gt;Daniel Morris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09050">
<title>Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts. (arXiv:2311.09050v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09050</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot Visual Question Answering (VQA) is a prominent vision-language task
that examines both the visual and textual understanding capability of systems
in the absence of training data. Recently, by converting the images into
captions, information across multi-modalities is bridged and Large Language
Models (LLMs) can apply their strong zero-shot generalization capability to
unseen questions. To design ideal prompts for solving VQA via LLMs, several
studies have explored different strategies to select or generate
question-answer pairs as the exemplar prompts, which guide LLMs to answer the
current questions effectively. However, they totally ignore the role of
question prompts. The original questions in VQA tasks usually encounter
ellipses and ambiguity which require intermediate reasoning. To this end, we
present Reasoning Question Prompts for VQA tasks, which can further activate
the potential of LLMs in zero-shot scenarios. Specifically, for each question,
we first generate self-contained questions as reasoning question prompts via an
unsupervised question edition module considering sentence fluency, semantic
integrity and syntactic invariance. Each reasoning question prompt clearly
indicates the intent of the original question. This results in a set of
candidate answers. Then, the candidate answers associated with their confidence
scores acting as answer heuristics are fed into LLMs and produce the final
answer. We evaluate reasoning question prompts on three VQA challenges,
experimental results demonstrate that they can significantly improve the
results of LLMs on zero-shot setting and outperform existing state-of-the-art
zero-shot methods on three out of four data sets. Our source code is publicly
released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yunshi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1&quot;&gt;Wei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1&quot;&gt;Weining Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09064">
<title>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models. (arXiv:2311.09064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09064</link>
<description rdf:parseType="Literal">&lt;p&gt;Systematic compositionality, or the ability to adapt to novel situations by
creating a mental model of the world using reusable pieces of knowledge,
remains a significant challenge in machine learning. While there has been
considerable progress in the language domain, efforts towards systematic visual
imagination, or envisioning the dynamical implications of a visual observation,
are in their infancy. We introduce the Systematic Visual Imagination Benchmark
(SVIB), the first benchmark designed to address this problem head-on. SVIB
offers a novel framework for a minimal world modeling problem, where models are
evaluated based on their ability to generate one-step image-to-image
transformations under a latent world dynamics. The framework provides benefits
such as the possibility to jointly optimize for systematic perception and
imagination, a range of difficulty levels, and the ability to control the
fraction of possible factor combinations used during training. We provide a
comprehensive evaluation of various baseline models on SVIB, offering insight
into the current state-of-the-art in systematic visual imagination. We hope
that this benchmark will help advance visual systematic compositionality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeongbin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gautam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junyeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1&quot;&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungjin Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09077">
<title>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation. (arXiv:2311.09077v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09077</link>
<description rdf:parseType="Literal">&lt;p&gt;A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of spiking neuron
and the theoretical accuracy of geometry, Based on this, we propose a bounded
spiking neuron to build the discontinuous density field. Our results achieve
SOTA performance. Our code and data will be released to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Zhanfeng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qian Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1&quot;&gt;Gang Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09084">
<title>Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search. (arXiv:2311.09084v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09084</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a descriptive text query, text-based person search (TBPS) aims to
retrieve the best-matched target person from an image gallery. Such a
cross-modal retrieval task is quite challenging due to significant modality
gap, fine-grained differences and insufficiency of annotated data. To better
align the two modalities, most existing works focus on introducing
sophisticated network structures and auxiliary tasks, which are complex and
hard to implement. In this paper, we propose a simple yet effective dual
Transformer model for text-based person search. By exploiting a hardness-aware
contrastive learning strategy, our model achieves state-of-the-art performance
without any special design for local feature alignment or side information.
Moreover, we propose a proximity data generation (PDG) module to automatically
produce more diverse data for cross-modal training. The PDG module first
introduces an automatic generation algorithm based on a text-to-image diffusion
model, which generates new text-image pair samples in the proximity space of
original ones. Then it combines approximate text generation and feature-level
mixup during training to further strengthen the data diversity. The PDG module
can largely guarantee the reasonability of the generated samples that are
directly used for training without any human inspection for noise rejection. It
improves the performance of our model significantly, providing a feasible
solution to the data insufficiency problem faced by such fine-grained
visual-linguistic tasks. Extensive experiments on two popular datasets of the
TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach
outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%,
4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be
available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hefeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhibin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianshui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09093">
<title>Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions. (arXiv:2311.09093v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09093</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicle refers to a vehicle capable of perceiving its surrounding
environment and driving with little or no human driver input. The perception
system is a fundamental component which enables the autonomous vehicle to
collect data and extract relevant information from the environment to drive
safely. Benefit from the recent advances in computer vision, the perception
task can be achieved by using sensors, such as camera, LiDAR, radar, and
ultrasonic sensor. This paper reviews publications on computer vision and
autonomous driving that are published during the last ten years. In particular,
we first investigate the development of autonomous driving systems and
summarize these systems that are developed by the major automotive
manufacturers from different countries. Second, we investigate the sensors and
benchmark data sets that are commonly utilized for autonomous driving. Then, a
comprehensive overview of computer vision applications for autonomous driving
such as depth estimation, object detection, lane detection, and traffic sign
recognition are discussed. Additionally, we review public opinions and concerns
on autonomous vehicles. Based on the discussion, we analyze the current
technological challenges that autonomous vehicles meet with. Finally, we
present our insights and point out some promising directions for future
research. This paper will help the reader to understand autonomous vehicles
from the perspectives of academia and industry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xingshuai Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cappuccio_M/0/1/0/all/0/1&quot;&gt;Massimiliano L. Cappuccio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09103">
<title>Guided Scale Space Radon Transform for linear structures detection. (arXiv:2311.09103v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09103</link>
<description rdf:parseType="Literal">&lt;p&gt;Using integral transforms to the end of lines detection in images with
complex background, makes the detection a hard task needing additional
processing to manage the detection. As an integral transform, the Scale Space
Radon Transform (SSRT) suffers from such drawbacks, even with its great
abilities for thick lines detection. In this work, we propose a method to
address this issue for automatic detection of thick linear structures in gray
scale and binary images using the SSRT, whatever the image background content.
This method involves the calculated Hessian orientations of the investigated
image while computing its SSRT, in such a way that linear structures are
emphasized in the SSRT space. As a consequence, the subsequent maxima detection
in the SSRT space is done on a modified transform space freed from unwanted
parts and, consequently, from irrelevant peaks that usually drown the peaks
representing lines. Besides, highlighting the linear structure in the SSRT
space permitting, thus, to efficiently detect lines of different thickness in
synthetic and real images, the experiments show also the method robustness
against noise and complex background.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goumeidane_A/0/1/0/all/0/1&quot;&gt;Aicha Baya Goumeidane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziou_D/0/1/0/all/0/1&quot;&gt;Djemel Ziou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nacereddine_N/0/1/0/all/0/1&quot;&gt;Nafaa Nacereddine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09104">
<title>Cross-view and Cross-pose Completion for 3D Human Understanding. (arXiv:2311.09104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09104</link>
<description rdf:parseType="Literal">&lt;p&gt;Human perception and understanding is a major domain of computer vision
which, like many other vision subdomains recently, stands to gain from the use
of large models pre-trained on large datasets. We hypothesize that the most
common pre-training strategy of relying on general purpose, object-centric
image datasets such as ImageNet, is limited by an important domain shift. On
the other hand, collecting domain specific ground truth such as 2D or 3D labels
does not scale well. Therefore, we propose a pre-training approach based on
self-supervised learning that works on human-centric data using only images.
Our method uses pairs of images of humans: the first is partially masked and
the model is trained to reconstruct the masked parts given the visible ones and
a second image. It relies on both stereoscopic (cross-view) pairs, and temporal
(cross-pose) pairs taken from videos, in order to learn priors about 3D as well
as human motion. We pre-train a model for body-centric tasks and one for
hand-centric tasks. With a generic transformer architecture, these models
outperform existing self-supervised pre-training methods on a wide set of
human-centric downstream tasks, and obtain state-of-the-art performance for
instance when fine-tuning for model-based and model-free human mesh recovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armando_M/0/1/0/all/0/1&quot;&gt;Matthieu Armando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galaaoui_S/0/1/0/all/0/1&quot;&gt;Salma Galaaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baradel_F/0/1/0/all/0/1&quot;&gt;Fabien Baradel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_T/0/1/0/all/0/1&quot;&gt;Thomas Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leroy_V/0/1/0/all/0/1&quot;&gt;Vincent Leroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bregier_R/0/1/0/all/0/1&quot;&gt;Romain Br&amp;#xe9;gier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1&quot;&gt;Philippe Weinzaepfel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;gory Rogez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09118">
<title>WildlifeDatasets: An open-source toolkit for animal re-identification. (arXiv:2311.09118v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09118</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present WildlifeDatasets
(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source
toolkit intended primarily for ecologists and computer-vision /
machine-learning researchers. The WildlifeDatasets is written in Python, allows
straightforward access to publicly available wildlife datasets, and provides a
wide variety of methods for dataset pre-processing, performance analysis, and
model fine-tuning. We showcase the toolkit in various scenarios and baseline
experiments, including, to the best of our knowledge, the most comprehensive
experimental comparison of datasets and methods for wildlife re-identification,
including both local descriptors and deep learning approaches. Furthermore, we
provide the first-ever foundation model for individual re-identification within
a wide range of species - MegaDescriptor - that provides state-of-the-art
performance on animal re-identification datasets and outperforms other
pre-trained models such as CLIP and DINOv2 by a significant margin. To make the
model available to the general public and to allow easy integration with any
existing wildlife monitoring applications, we provide multiple MegaDescriptor
flavors (i.e., Small, Medium, and Large) through the HuggingFace hub
(https://huggingface.co/BVRA).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cermak_V/0/1/0/all/0/1&quot;&gt;Vojt&amp;#x11b;ch &amp;#x10c;erm&amp;#xe1;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_L/0/1/0/all/0/1&quot;&gt;Lukas Picek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1&quot;&gt;Luk&amp;#xe1;&amp;#x161; Adam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papafitsoros_K/0/1/0/all/0/1&quot;&gt;Kostas Papafitsoros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09178">
<title>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution. (arXiv:2311.09178v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09178</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, video super resolution (VSR) has become a very impactful task in
the area of Computer Vision due to its various applications. In this paper, we
propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for
VSR in an attempt to generate temporally coherent solutions while preserving
spatial details. RBPGAN integrates two state-of-the-art models to get the best
in both worlds without compromising the accuracy of produced video. The
generator of the model is inspired by RBPN system, while the discriminator is
inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal
consistency over time. Our contribution together results in a model that
outperforms earlier work in terms of temporally consistent details, as we will
demonstrate qualitatively and quantitatively using different datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussein_D/0/1/0/all/0/1&quot;&gt;Dareen Hussein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1&quot;&gt;Hesham Eraqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fahmy_I/0/1/0/all/0/1&quot;&gt;Israa Fahmy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1&quot;&gt;Marwah Sulaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barakat_M/0/1/0/all/0/1&quot;&gt;Mohammed Barakat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Naggar_M/0/1/0/all/0/1&quot;&gt;Mohammed El-Naggar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youssef_M/0/1/0/all/0/1&quot;&gt;Moustafa Youssef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehabeldin_Z/0/1/0/all/0/1&quot;&gt;Zahraa Shehabeldin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09190">
<title>On the Computation of the Gaussian Rate-Distortion-Perception Function. (arXiv:2311.09190v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2311.09190</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the computation of the rate-distortion-perception
function (RDPF) for a multivariate Gaussian source under mean squared error
(MSE) distortion and, respectively, Kullback-Leibler divergence, geometric
Jensen-Shannon divergence, squared Hellinger distance, and squared
Wasserstein-2 distance perception metrics. To this end, we first characterize
the analytical bounds of the scalar Gaussian RDPF for the aforementioned
divergence functions, also providing the RDPF-achieving forward &quot;test-channel&quot;
realization. Focusing on the multivariate case, we establish that, for
tensorizable distortion and perception metrics, the optimal solution resides on
the vector space spanned by the eigenvector of the source covariance matrix.
Consequently, the multivariate optimization problem can be expressed as a
function of the scalar Gaussian RDPFs of the source marginals, constrained by
global distortion and perception levels. Leveraging this characterization, we
design an alternating minimization scheme based on the block nonlinear
Gauss-Seidel method, which optimally solves the problem while identifying the
Gaussian RDPF-achieving realization. Furthermore, the associated algorithmic
embodiment is provided, as well as the convergence and the rate of convergence
characterization. Lastly, for the &quot;perfect realism&quot; regime, the analytical
solution for the multivariate Gaussian RDPF is obtained. We corroborate our
results with numerical simulations and draw connections to existing results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1&quot;&gt;Giuseppe Serra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stavrou_P/0/1/0/all/0/1&quot;&gt;Photios A. Stavrou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kountouris_M/0/1/0/all/0/1&quot;&gt;Marios Kountouris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09191">
<title>Domain Aligned CLIP for Few-shot Classification. (arXiv:2311.09191v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.09191</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language representation learning models like CLIP have
demonstrated impressive performance for zero-shot transfer to downstream tasks
while largely benefiting from inter-modal (image-text) alignment via
contrastive objectives. This downstream performance can further be enhanced by
full-scale fine-tuning which is often compute intensive, requires large
labelled data, and can reduce out-of-distribution (OOD) robustness.
Furthermore, sole reliance on inter-modal alignment might overlook the rich
information embedded within each individual modality. In this work, we
introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain
Aligned CLIP (DAC), which improves both intra-modal (image-image) and
inter-modal alignment on target distributions without fine-tuning the main
model. For intra-modal alignment, we introduce a lightweight adapter that is
specifically trained with an intra-modal contrastive objective. To improve
inter-modal alignment, we introduce a simple framework to modulate the
precomputed class text embeddings. The proposed few-shot fine-tuning framework
is computationally efficient, robust to distribution shifts, and does not alter
CLIP&apos;s parameters. We study the effectiveness of DAC by benchmarking on 11
widely used image classification tasks with consistent improvements in 16-shot
classification upon strong baselines by about 2.3% and demonstrate competitive
performance on 4 OOD robustness benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gondal_M/0/1/0/all/0/1&quot;&gt;Muhammad Waleed Gondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gast_J/0/1/0/all/0/1&quot;&gt;Jochen Gast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_I/0/1/0/all/0/1&quot;&gt;Inigo Alonso Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Droste_R/0/1/0/all/0/1&quot;&gt;Richard Droste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macri_T/0/1/0/all/0/1&quot;&gt;Tommaso Macri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Suren Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staudigl_L/0/1/0/all/0/1&quot;&gt;Luitpold Staudigl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09193">
<title>The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task. (arXiv:2311.09193v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.09193</link>
<description rdf:parseType="Literal">&lt;p&gt;The study explores the effectiveness of the Chain-of-Thought approach, known
for its proficiency in language tasks by breaking them down into sub-tasks and
intermediate steps, in improving vision-language tasks that demand
sophisticated perception and reasoning. We present the &quot;Description then
Decision&quot; strategy, which is inspired by how humans process signals. This
strategy significantly improves probing task performance by 50%, establishing
the groundwork for future research on reasoning paradigms in complex
vision-language tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1&quot;&gt;Barlas Oguz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James C. Gee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yixin Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.01251">
<title>Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods. (arXiv:2206.01251v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.01251</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of evaluating the quality of self-supervised learning
(SSL) models without access to supervised labels, while being agnostic to the
architecture, learning algorithm or data manipulation used during training. We
argue that representations can be evaluated through the lens of expressiveness
and learnability. We propose to use the Intrinsic Dimension (ID) to assess
expressiveness and introduce Cluster Learnability (CL) to assess learnability.
CL is measured in terms of the performance of a KNN classifier trained to
predict labels obtained by clustering the representations with K-means. We thus
combine CL and ID into a single predictor -- CLID. Through a large-scale
empirical study with a diverse family of SSL algorithms, we find that CLID
better correlates with in-distribution model performance than other competing
recent evaluation schemes. We also benchmark CLID on out-of-domain
generalization, where CLID serves as a predictor of the transfer performance of
SSL models on several visual classification tasks, yielding improvements with
respect to the competing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuchen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baratin_A/0/1/0/all/0/1&quot;&gt;Aristide Baratin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1&quot;&gt;Aaron Courville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1&quot;&gt;Alessandro Sordoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.02474">
<title>CFARnet: deep learning for target detection with constant false alarm rate. (arXiv:2208.02474v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.02474</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of target detection with a constant false alarm rate
(CFAR). This constraint is crucial in many practical applications and is a
standard requirement in classical composite hypothesis testing. In settings
where classical approaches are computationally expensive or where only data
samples are given, machine learning methodologies are advantageous. CFAR is
less understood in these settings. To close this gap, we introduce a framework
of CFAR constrained detectors. Theoretically, we prove that a CFAR constrained
Bayes optimal detector is asymptotically equivalent to the classical
generalized likelihood ratio test (GLRT). Practically, we develop a deep
learning framework for fitting neural networks that approximate it. Experiments
of target detection in different setting demonstrate that the proposed CFARnet
allows a flexible tradeoff between CFAR and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diskin_T/0/1/0/all/0/1&quot;&gt;Tzvi Diskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beer_Y/0/1/0/all/0/1&quot;&gt;Yiftach Beer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okun_U/0/1/0/all/0/1&quot;&gt;Uri Okun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiesel_A/0/1/0/all/0/1&quot;&gt;Ami Wiesel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07853">
<title>Estimating Appearance Models for Image Segmentation via Tensor Factorization. (arXiv:2208.07853v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07853</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Segmentation is one of the core tasks in Computer Vision and solving it
often depends on modeling the image appearance data via the color distributions
of each it its constituent regions. Whereas many segmentation algorithms handle
the appearance models dependence using alternation or implicit methods, we
propose here a new approach to directly estimate them from the image without
prior information on the underlying segmentation. Our method uses local high
order color statistics from the image as an input to tensor factorization-based
estimator for latent variable models. This approach is able to estimate models
in multiregion images and automatically output the regions proportions without
prior user interaction, overcoming the drawbacks from a prior attempt to this
problem. We also demonstrate the performance of our proposed method in many
challenging synthetic and real imaging scenarios and show that it leads to an
efficient segmentation algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1&quot;&gt;Jeova Farias Sales Rocha Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02998">
<title>ThoraX-PriorNet: A Novel Attention-Based Architecture Using Anatomical Prior Probability Maps for Thoracic Disease Classification. (arXiv:2210.02998v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02998</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Computer-aided disease diagnosis and prognosis based on medical
images is a rapidly emerging field. Many Convolutional Neural Network (CNN)
architectures have been developed by researchers for disease classification and
localization from chest X-ray images. It is known that different thoracic
disease lesions are more likely to occur in specific anatomical regions
compared to others. This article aims to incorporate this disease and
region-dependent prior probability distribution within a deep learning
framework. Methods: We present the ThoraX-PriorNet, a novel attention-based CNN
model for thoracic disease classification. We first estimate a
disease-dependent spatial probability, i.e., an anatomical prior, that
indicates the probability of occurrence of a disease in a specific region in a
chest X-ray image. Next, we develop a novel attention-based classification
model that combines information from the estimated anatomical prior and
automatically extracted chest region of interest (ROI) masks to provide
attention to the feature maps generated from a deep convolution network. Unlike
previous works that utilize various self-attention mechanisms, the proposed
method leverages the extracted chest ROI masks along with the probabilistic
anatomical prior information, which selects the region of interest for
different diseases to provide attention. Results: The proposed method shows
superior performance in disease classification on the NIH ChestX-ray14 dataset
compared to existing state-of-the-art methods while reaching an area under the
ROC curve (%AUC) of 84.67. Regarding disease localization, the anatomy prior
attention method shows competitive performance compared to state-of-the-art
methods, achieving an accuracy of 0.80, 0.63, 0.49, 0.33, 0.28, 0.21, and 0.04
with an Intersection over Union (IoU) threshold of 0.1, 0.2, 0.3, 0.4, 0.5,
0.6, and 0.7, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Md. Iqbal Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zunaed_M/0/1/0/all/0/1&quot;&gt;Mohammad Zunaed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Md. Kawsar Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1&quot;&gt;S. M. Jawwad Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Anwarul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hasan_T/0/1/0/all/0/1&quot;&gt;Taufiq Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06891">
<title>Residual Degradation Learning Unfolding Framework with Mixing Priors across Spectral and Spatial for Compressive Spectral Imaging. (arXiv:2211.06891v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06891</link>
<description rdf:parseType="Literal">&lt;p&gt;To acquire a snapshot spectral image, coded aperture snapshot spectral
imaging (CASSI) is proposed. A core problem of the CASSI system is to recover
the reliable and fine underlying 3D spectral cube from the 2D measurement. By
alternately solving a data subproblem and a prior subproblem, deep unfolding
methods achieve good performance. However, in the data subproblem, the used
sensing matrix is ill-suited for the real degradation process due to the device
errors caused by phase aberration, distortion; in the prior subproblem, it is
important to design a suitable model to jointly exploit both spatial and
spectral priors. In this paper, we propose a Residual Degradation Learning
Unfolding Framework (RDLUF), which bridges the gap between the sensing matrix
and the degradation process. Moreover, a Mix$S^2$ Transformer is designed via
mixing priors across spectral and spatial to strengthen the spectral-spatial
representation capability. Finally, plugging the Mix$S^2$ Transformer into the
RDLUF leads to an end-to-end trainable neural network RDLUF-Mix$S^2$.
Experimental results establish the superior performance of the proposed method
over existing ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yubo Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Dahua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Minxi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Guangming Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10368">
<title>Masked Event Modeling: Self-Supervised Pretraining for Event Cameras. (arXiv:2212.10368v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10368</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras offer the capacity to asynchronously capture brightness changes
with low latency, high temporal resolution, and high dynamic range. Deploying
deep learning methods for classification or other tasks to these sensors
typically requires large labeled datasets. Since the amount of labeled event
data is tiny compared to the bulk of labeled RGB imagery, the progress of
event-based vision has remained limited. To reduce the dependency on labeled
event data, we introduce Masked Event Modeling (MEM), a self-supervised
pretraining framework for events. Our method pretrains a neural network on
unlabeled events, which can originate from any event camera recording.
Subsequently, the pretrained model is finetuned on a downstream task leading to
an overall better performance while requiring fewer labels. Our method
outperforms the state-of-the-art on N-ImageNet, N-Cars, and N-Caltech101,
increasing the object classification accuracy on N-ImageNet by 7.96%. We
demonstrate that Masked Event Modeling is superior to RGB-based pretraining on
a real world dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1&quot;&gt;Simon Klenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonello_D/0/1/0/all/0/1&quot;&gt;David Bonello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1&quot;&gt;Lukas Koestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araslanov_N/0/1/0/all/0/1&quot;&gt;Nikita Araslanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12322">
<title>Infrared Image Super-Resolution: Systematic Review, and Future Trends. (arXiv:2212.12322v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12322</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Super-Resolution (SR) is essential for a wide range of computer vision
and image processing tasks. Investigating infrared (IR) image (or thermal
images) super-resolution is a continuing concern within the development of deep
learning. This survey aims to provide a comprehensive perspective of IR image
super-resolution, including its applications, hardware imaging system dilemmas,
and taxonomy of image processing methodologies. In addition, the datasets and
evaluation metrics in IR image super-resolution tasks are also discussed.
Furthermore, the deficiencies in current technologies and possible promising
directions for the community to explore are highlighted. To cope with the rapid
development in this field, we intend to regularly update the relevant excellent
work at \url{https://github.com/yongsongH/Infrared_Image_SR_Survey
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miyazaki_T/0/1/0/all/0/1&quot;&gt;Tomo Miyazaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Omachi_S/0/1/0/all/0/1&quot;&gt;Shinichiro Omachi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12554">
<title>Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12554</link>
<description rdf:parseType="Literal">&lt;p&gt;While prior research has proposed a plethora of methods that build neural
classifiers robust against adversarial robustness, practitioners are still
reluctant to adopt them due to their unacceptably severe clean accuracy
penalties. This paper significantly alleviates this accuracy-robustness
trade-off by mixing the output probabilities of a standard classifier and a
robust classifier, where the standard network is optimized for clean accuracy
and is not robust in general. We show that the robust base classifier&apos;s
confidence difference for correct and incorrect examples is the key to this
improvement. In addition to providing intuitions and empirical evidence, we
theoretically certify the robustness of the mixed classifier under realistic
assumptions. Furthermore, we adapt an adversarial input detector into a mixing
network that adaptively adjusts the mixture of the two base models, further
reducing the accuracy penalty of achieving robustness. The proposed flexible
method, termed &quot;adaptive smoothing&quot;, can work in conjunction with existing or
even future methods that improve clean accuracy, robustness, or adversary
detection. Our empirical evaluation considers strong attack methods, including
AutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves
an 85.21% clean accuracy while maintaining a 38.72% $\ell_\infty$-AutoAttacked
($\epsilon = 8/255$) accuracy, becoming the second most robust method on the
RobustBench CIFAR-100 benchmark as of submission, while improving the clean
accuracy by ten percentage points compared with all listed models. The code
that implements our method is available at
https://github.com/Bai-YT/AdaptiveSmoothing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yatong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1&quot;&gt;Brendon G. Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Aerin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14516">
<title>OVeNet: Offset Vector Network for Semantic Segmentation. (arXiv:2303.14516v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14516</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a fundamental task in visual scene understanding. We
focus on the supervised setting, where ground-truth semantic annotations are
available. Based on knowledge about the high regularity of real-world scenes,
we propose a method for improving class predictions by learning to selectively
exploit information from neighboring pixels. In particular, our method is based
on the prior that for each pixel, there is a seed pixel in its close
neighborhood sharing the same prediction with the former. Motivated by this
prior, we design a novel two-head network, named Offset Vector Network
(OVeNet), which generates both standard semantic predictions and a dense 2D
offset vector field indicating the offset from each pixel to the respective
seed pixel, which is used to compute an alternative, seed-based semantic
prediction. The two predictions are adaptively fused at each pixel using a
learnt dense confidence map for the predicted offset vector field. We supervise
offset vectors indirectly via optimizing the seed-based prediction and via a
novel loss on the confidence map. Compared to the baseline state-of-the-art
architectures HRNet and HRNet+OCR on which OVeNet is built, the latter achieves
significant performance gains on three prominent benchmarks for semantic
segmentation, namely Cityscapes, ACDC and ADE20K. Code is available at
https://github.com/stamatisalex/OVeNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandropoulos_S/0/1/0/all/0/1&quot;&gt;Stamatis Alexandropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1&quot;&gt;Christos Sakaridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1&quot;&gt;Petros Maragos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10722">
<title>Discriminative Diffusion Models as Few-shot Vision and Language Learners. (arXiv:2305.10722v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10722</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models, such as Stable Diffusion, have shown incredible performance
on text-to-image generation. Since text-to-image generation often requires
models to generate visual concepts with fine-grained details and attributes
specified in text prompts, can we leverage the powerful representations learned
by pre-trained diffusion models for discriminative tasks such as image-text
matching? To answer this question, we propose a novel approach, Discriminative
Stable Diffusion (DSD), which turns pre-trained text-to-image diffusion models
into few-shot discriminative learners. Our approach mainly uses the
cross-attention score of a Stable Diffusion model to capture the mutual
influence between visual and textual information and fine-tune the model via
efficient attention-based prompt learning to perform image-text matching. By
comparing DSD with state-of-the-art methods on several benchmark datasets, we
demonstrate the potential of using pre-trained diffusion models for
discriminative tasks with superior results on few-shot image-text matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuehai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Weixi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tsu-Jui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1&quot;&gt;Arjun Akula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayana_P/0/1/0/all/0/1&quot;&gt;Pradyumna Narayana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Sugato Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Eric Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12437">
<title>PLAR: Prompt Learning for Action Recognition. (arXiv:2305.12437v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12437</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new general learning approach, Prompt Learning for Action
Recognition (PLAR), which leverages the strengths of prompt learning to guide
the learning process. Our approach is designed to predict the action label by
helping the models focus on the descriptions or instructions associated with
actions in the input videos. Our formulation uses various prompts, including
learnable prompts, auxiliary visual information, and large vision models to
improve the recognition performance. In particular, we design a learnable
prompt method that learns to dynamically generate prompts from a pool of prompt
experts under different inputs. By sharing the same objective with the task,
our proposed PLAR can optimize prompts that guide the model&apos;s predictions while
explicitly learning input-invariant (prompt experts pool) and input-specific
(data-dependent) prompt knowledge. We evaluate our approach on datasets
consisting of both ground camera videos and aerial videos, and scenes with
single-agent and multi-agent actions. In practice, we observe a 3.17-10.2%
accuracy improvement on the aerial multi-agent dataset Okutamam and a 1.0-3.6%
improvement on the ground camera single-agent dataset Something Something V2.
We plan to release our code on the WWW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_R/0/1/0/all/0/1&quot;&gt;Ruiqi Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1&quot;&gt;Tianrui Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12704">
<title>Rotation-Constrained Cross-View Feature Fusion for Multi-View Appearance-based Gaze Estimation. (arXiv:2305.12704v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12704</link>
<description rdf:parseType="Literal">&lt;p&gt;Appearance-based gaze estimation has been actively studied in recent years.
However, its generalization performance for unseen head poses is still a
significant limitation for existing methods. This work proposes a generalizable
multi-view gaze estimation task and a cross-view feature fusion method to
address this issue. In addition to paired images, our method takes the relative
rotation matrix between two cameras as additional input. The proposed network
learns to extract rotatable feature representation by using relative rotation
as a constraint and adaptively fuses the rotatable features via stacked fusion
modules. This simple yet efficient approach significantly improves
generalization performance under unseen head poses without significantly
increasing computational cost. The model can be trained with random
combinations of cameras without fixing the positioning and can generalize to
unseen camera pairs during inference. Through experiments using multiple
datasets, we demonstrate the advantage of the proposed method over baseline
methods, including state-of-the-art domain generalization approaches. The code
will be available at https://github.com/ut-vision/Rot-MVGaze.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hisadome_Y/0/1/0/all/0/1&quot;&gt;Yoichiro Hisadome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jiawei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1&quot;&gt;Yusuke Sugano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09012">
<title>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization. (arXiv:2306.09012v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09012</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale visual localization systems continue to rely on 3D point clouds
built from image collections using structure-from-motion. While the 3D points
in these models are represented using local image features, directly matching a
query image&apos;s local features against the point cloud is challenging due to the
scale of the nearest-neighbor search problem. Many recent approaches to visual
localization have thus proposed a hybrid method, where first a global (per
image) embedding is used to retrieve a small subset of database images, and
local features of the query are matched only against those. It seems to have
become common belief that global embeddings are critical for said
image-retrieval in visual localization, despite the significant downside of
having to compute two feature types for each query image. In this paper, we
take a step back from this assumption and propose Constrained Approximate
Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both
the geometry and appearance space using only local features. We first derive
the theoretical foundation for k-nearest-neighbor retrieval across multiple
metrics and then showcase how CANN improves visual localization. Our
experiments on public localization benchmarks demonstrate that our method
significantly outperforms both state-of-the-art global feature-based retrieval
and approaches using local feature aggregation schemes. Moreover, it is an
order of magnitude faster in both index and query time than feature aggregation
schemes for these datasets. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aiger_D/0/1/0/all/0/1&quot;&gt;Dror Aiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lynen_S/0/1/0/all/0/1&quot;&gt;Simon Lynen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06507">
<title>Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models. (arXiv:2307.06507v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06507</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating deep learning with clinical expertise holds great potential for
addressing healthcare challenges and empowering medical professionals with
improved diagnostic tools. However, the need for annotated medical images is
often an obstacle to leveraging the full power of machine learning models. Our
research demonstrates that by combining synthetic images, generated using
diffusion models, with real images, we can enhance nonalcoholic fatty liver
disease (NAFLD) classification performance even in low-data regime settings. We
evaluate the quality of the synthetic images by comparing two metrics:
Inception Score (IS) and Fr\&apos;{e}chet Inception Distance (FID), computed on
diffusion- and generative adversarial network (GAN)-generated images. Our
results show superior performance for the diffusion-generated images, with a
maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score
of $69.45$ compared to $100.05$ for GANs. Utilizing a partially frozen CNN
backbone (EfficientNet v1), our synthetic augmentation method achieves a
maximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardy_R/0/1/0/all/0/1&quot;&gt;Romain Hardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klepich_J/0/1/0/all/0/1&quot;&gt;Joe Klepich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1&quot;&gt;Ryan Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1&quot;&gt;Steve Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villareal_J/0/1/0/all/0/1&quot;&gt;Jericho Villareal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_C/0/1/0/all/0/1&quot;&gt;Cornelia Ilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10894">
<title>Human Motion Generation: A Survey. (arXiv:2307.10894v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10894</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion generation aims to generate natural human pose sequences and
shows immense potential for real-world applications. Substantial progress has
been made recently in motion data collection technologies and generation
methods, laying the foundation for increasing interest in human motion
generation. Most research within this field focuses on generating human motions
based on conditional signals, such as text, audio, and scene contexts. While
significant advancements have been made in recent years, the task continues to
pose challenges due to the intricate nature of human motion and its implicit
relationship with conditional signals. In this survey, we present a
comprehensive literature review of human motion generation, which, to the best
of our knowledge, is the first of its kind in this field. We begin by
introducing the background of human motion and generative models, followed by
an examination of representative methods for three mainstream sub-tasks:
text-conditioned, audio-conditioned, and scene-conditioned human motion
generation. Additionally, we provide an overview of common datasets and
evaluation metrics. Lastly, we discuss open problems and outline potential
future research directions. We hope that this survey could provide the
community with a comprehensive glimpse of this rapidly evolving field and
inspire novel ideas that address the outstanding challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_D/0/1/0/all/0/1&quot;&gt;Dongwoo Ro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ci_H/0/1/0/all/0/1&quot;&gt;Hai Ci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinlu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00755">
<title>The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00755</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias amplification is a phenomenon in which models exacerbate biases or
stereotypes present in the training data. In this paper, we study bias
amplification in the text-to-image domain using Stable Diffusion by comparing
gender ratios in training vs. generated images. We find that the model appears
to amplify gender-occupation biases found in the training data (LAION)
considerably. However, we discover that amplification can be largely attributed
to discrepancies between training captions and model prompts. For example, an
inherent difference is that captions from the training data often contain
explicit gender information while our prompts do not, which leads to a
distribution shift and consequently inflates bias measures. Once we account for
distributional differences between texts used for training and generation when
evaluating amplification, we observe that amplification decreases drastically.
Our findings illustrate the challenges of comparing biases in models and their
training data, and highlight confounding factors that impact analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1&quot;&gt;Preethi Seshadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02487">
<title>Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP. (arXiv:2308.02487v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02487</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary segmentation is a challenging task requiring segmenting and
recognizing objects from an open set of categories. One way to address this
challenge is to leverage multi-modal models, such as CLIP, to provide image and
text features in a shared embedding space, which bridges the gap between
closed-vocabulary and open-vocabulary recognition. Hence, existing methods
often adopt a two-stage framework to tackle the problem, where the inputs first
go through a mask generator and then through the CLIP model along with the
predicted masks. This process involves extracting features from images multiple
times, which can be ineffective and inefficient. By contrast, we propose to
build everything into a single-stage framework using a shared Frozen
Convolutional CLIP backbone, which not only significantly simplifies the
current two-stage pipeline, but also remarkably yields a better accuracy-cost
trade-off. The proposed FC-CLIP, benefits from the following observations: the
frozen CLIP backbone maintains the ability of open-vocabulary classification
and can also serve as a strong mask generator, and the convolutional CLIP
generalizes well to a larger input resolution than the one used during
contrastive image-text pretraining. When training on COCO panoptic data only
and testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1
mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2
mIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoU
on ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,
respectively. Additionally, the training and testing time of FC-CLIP is 7.5x
and 6.6x significantly faster than the same prior art, while using 5.9x fewer
parameters. FC-CLIP also sets a new state-of-the-art performance across various
open-vocabulary semantic segmentation datasets. Code at
https://github.com/bytedance/fc-clip
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qihang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Ju He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xueqing Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang-Chieh Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10103">
<title>ASPIRE: Language-Guided Augmentation for Robust Image Classification. (arXiv:2308.10103v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10103</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural image classifiers can often learn to make predictions by overly
relying on non-predictive features that are spuriously correlated with the
class labels in the training data. This leads to poor performance in real-world
atypical scenarios where such features are absent. Supplementing the training
dataset with images without such spurious features can aid robust learning
against spurious correlations via better generalization. This paper presents
ASPIRE (Language-guided data Augmentation for SPurIous correlation REmoval), a
simple yet effective solution for expanding the training dataset with synthetic
images without spurious features. ASPIRE, guided by language, generates these
images without requiring any form of additional supervision or existing
examples. Precisely, we employ LLMs to first extract foreground and background
features from textual descriptions of an image, followed by advanced
language-guided image editing to discover the features that are spuriously
correlated with the class label. Finally, we personalize a text-to-image
generation model to generate diverse in-domain images without spurious
features. We demonstrate the effectiveness of ASPIRE on 4 datasets, including
the very challenging Hard ImageNet dataset, and 9 baselines and show that
ASPIRE improves the classification accuracy of prior methods by 1% - 38%. Code
soon at: https://github.com/Sreyan88/ASPIRE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evuru_C/0/1/0/all/0/1&quot;&gt;Chandra Kiran Reddy Evuru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sonal Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_U/0/1/0/all/0/1&quot;&gt;Utkarsh Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sakshi Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Sanjoy Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14119">
<title>Semi-Supervised Learning in the Few-Shot Zero-Shot Scenario. (arXiv:2308.14119v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14119</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-Supervised Learning (SSL) is a framework that utilizes both labeled and
unlabeled data to enhance model performance. Conventional SSL methods operate
under the assumption that labeled and unlabeled data share the same label
space. However, in practical real-world scenarios, especially when the labeled
training dataset is limited in size, some classes may be totally absent from
the labeled set. To address this broader context, we propose a general approach
to augment existing SSL methods, enabling them to effectively handle situations
where certain classes are missing. This is achieved by introducing an
additional term into their objective function, which penalizes the
KL-divergence between the probability vectors of the true class frequencies and
the inferred class frequencies. Our experimental results reveal significant
improvements in accuracy when compared to state-of-the-art SSL, open-set SSL,
and open-world SSL methods. We conducted these experiments on two benchmark
image classification datasets, CIFAR-100 and STL-10, with the most remarkable
improvements observed when the labeled data is severely limited, with only a
few labeled examples per class
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fluss_N/0/1/0/all/0/1&quot;&gt;Noam Fluss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hacohen_G/0/1/0/all/0/1&quot;&gt;Guy Hacohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1&quot;&gt;Daphna Weinshall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07944">
<title>Text-to-Image Models for Counterfactual Explanations: a Black-Box Approach. (arXiv:2309.07944v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the challenge of generating Counterfactual Explanations
(CEs), involving the identification and modification of the fewest necessary
features to alter a classifier&apos;s prediction for a given image. Our proposed
method, Text-to-Image Models for Counterfactual Explanations (TIME), is a
black-box counterfactual technique based on distillation. Unlike previous
methods, this approach requires solely the image and its prediction, omitting
the need for the classifier&apos;s structure, parameters, or gradients. Before
generating the counterfactuals, TIME introduces two distinct biases into Stable
Diffusion in the form of textual embeddings: the context bias, associated with
the image&apos;s structure, and the class bias, linked to class-specific features
learned by the target classifier. After learning these biases, we find the
optimal latent code applying the classifier&apos;s predicted class token and
regenerate the image using the target embedding as conditioning, producing the
counterfactual explanation. Extensive empirical studies validate that TIME can
generate explanations of comparable effectiveness even when operating within a
black-box setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeanneret_G/0/1/0/all/0/1&quot;&gt;Guillaume Jeanneret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;ric Jurie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14136">
<title>Masked Image Residual Learning for Scaling Deeper Vision Transformers. (arXiv:2309.14136v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14136</link>
<description rdf:parseType="Literal">&lt;p&gt;Deeper Vision Transformers (ViTs) are more challenging to train. We expose a
degradation problem in deeper layers of ViT when using masked image modeling
(MIM) for pre-training. To ease the training of deeper ViTs, we introduce a
self-supervised learning framework called Masked Image Residual Learning
(MIRL), which significantly alleviates the degradation problem, making scaling
ViT along depth a promising direction for performance upgrade. We reformulate
the pre-training objective for deeper layers of ViT as learning to recover the
residual of the masked image. We provide extensive empirical evidence showing
that deeper ViTs can be effectively optimized using MIRL and easily gain
accuracy from increased depth. With the same level of computational complexity
as ViT-Base and ViT-Large, we instantiate 4.5$\times$ and 2$\times$ deeper
ViTs, dubbed ViT-S-54 and ViT-B-48. The deeper ViT-S-54, costing 3$\times$ less
than ViT-Large, achieves performance on par with ViT-Large. ViT-B-48 achieves
86.2% top-1 accuracy on ImageNet. On one hand, deeper ViTs pre-trained with
MIRL exhibit excellent generalization capabilities on downstream tasks, such as
object detection and semantic segmentation. On the other hand, MIRL
demonstrates high pre-training efficiency. With less pre-training time, MIRL
yields competitive performance compared to other approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guoxi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hongtao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1&quot;&gt;Adrian G. Bors&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01258">
<title>MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device. (arXiv:2310.01258v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01258</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural video codecs have recently become competitive with standard codecs
such as HEVC in the low-delay setting. However, most neural codecs are large
floating-point networks that use pixel-dense warping operations for temporal
modeling, making them too computationally expensive for deployment on mobile
devices. Recent work has demonstrated that running a neural decoder in real
time on mobile is feasible, but shows this only for 720p RGB video. This work
presents the first neural video codec that decodes 1080p YUV420 video in real
time on a mobile device. Our codec relies on two major contributions. First, we
design an efficient codec that uses a block-based motion compensation algorithm
available on the warping core of the mobile accelerator, and we show how to
quantize this model to integer precision. Second, we implement a fast decoder
pipeline that concurrently runs neural network components on the neural signal
processor, parallel entropy coding on the mobile GPU, and warping on the
warping core. Our codec outperforms the previous on-device codec by a large
margin with up to 48% BD-rate savings, while reducing the MAC count on the
receiver side by $10 \times$. We perform a careful ablation to demonstrate the
effect of the introduced motion compensation scheme, and ablate the effect of
model quantization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rozendaal_T/0/1/0/all/0/1&quot;&gt;Ties van Rozendaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singhal_T/0/1/0/all/0/1&quot;&gt;Tushar Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_H/0/1/0/all/0/1&quot;&gt;Hoang Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sautiere_G/0/1/0/all/0/1&quot;&gt;Guillaume Sautiere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Said_A/0/1/0/all/0/1&quot;&gt;Amir Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Buska_K/0/1/0/all/0/1&quot;&gt;Krishna Buska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raha_A/0/1/0/all/0/1&quot;&gt;Anjuman Raha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kalatzis_D/0/1/0/all/0/1&quot;&gt;Dimitris Kalatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_H/0/1/0/all/0/1&quot;&gt;Hitarth Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mayer_F/0/1/0/all/0/1&quot;&gt;Frank Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nagel_M/0/1/0/all/0/1&quot;&gt;Markus Nagel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiggers_A/0/1/0/all/0/1&quot;&gt;Auke Wiggers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07250">
<title>Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v3 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic
resonance imaging (MRI) plays a significant role in the diagnosis, treatment
planning, and follow-up of glioblastoma patients due to its non-invasive and
radiation-free nature. The International Brain Tumor Segmentation (BraTS)
challenge has contributed to generating numerous AI algorithms to accurately
and efficiently segment glioblastoma sub-compartments using four structural
(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not
always be available. To address this issue, Generative Adversarial Networks
(GANs) can be used to synthesize the missing MRI sequences. In this paper, we
implement and utilize an open-source GAN approach that takes any three MRI
sequences as input to generate the missing fourth structural sequence. Our
proposed approach is contributed to the community-driven generally nuanced deep
learning framework (GaNDLF) and demonstrates promising results in synthesizing
high-quality and realistic MRI sequences, enabling clinicians to improve their
diagnostic capabilities and support the application of AI methods to brain
tumor MRI quantification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hamamci_I/0/1/0/all/0/1&quot;&gt;Ibrahim Ethem Hamamci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17842">
<title>What You See Is What You Detect: Towards better Object Densification in 3D detection. (arXiv:2310.17842v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have demonstrated the importance of object completion in 3D
Perception from Lidar signal. Several methods have been proposed in which
modules were used to densify the point clouds produced by laser scanners,
leading to better recall and more accurate results. Pursuing in that direction,
we present, in this work, a counter-intuitive perspective: the widely-used
full-shape completion approach actually leads to a higher error-upper bound
especially for far away objects and small objects like pedestrians. Based on
this observation, we introduce a visible part completion method that requires
only 11.3\% of the prediction points that previous methods generate. To recover
the dense representation, we propose a mesh-deformation-based method to augment
the point set associated with visible foreground objects. Considering that our
approach focuses only on the visible part of the foreground objects to achieve
accurate 3D detection, we named our method What You See Is What You Detect
(WYSIWYD). Our proposed method is thus a detector-independent model that
consists of 2 parts: an Intra-Frustum Segmentation Transformer (IFST) and a
Mesh Depth Completion Network(MDCNet) that predicts the foreground depth from
mesh deformation. This way, our model does not require the time-consuming
full-depth completion task used by most pseudo-lidar-based methods. Our
experimental evaluation shows that our approach can provide up to 12.2\%
performance improvements over most of the public baseline models on the KITTI
and NuScenes dataset bringing the state-of-the-art to a new level. The codes
will be available at
\textcolor[RGB]{0,0,255}{\url{{https://github.com/Orbis36/WYSIWYD}}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianran Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasandi_M/0/1/0/all/0/1&quot;&gt;Morteza Mousa Pasandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laganiere_R/0/1/0/all/0/1&quot;&gt;Robert Laganiere&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00735">
<title>PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00735</link>
<description rdf:parseType="Literal">&lt;p&gt;Positron emission tomography (PET) serves as an essential tool for diagnosis
of encephalopathy and brain science research. However, it suffers from the
limited choice of tracers. Nowadays, with the wide application of PET imaging
in neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine
(DOPA) has been found to be more effective than 18F-labeled
fluorine-2-deoxyglucose (FDG) in the field. Nevertheless, due to the complexity
of its preparation and other limitations, DOPA is far less widely used than
FDG. To address this issue, a tracer conversion invertible neural network
(TC-INN) for image projection is developed to map FDG images to DOPA images
through deep learning. More diagnostic information is obtained by generating
PET images from FDG to DOPA. Specifically, the proposed TC-INN consists of two
separate phases, one for training traceable data, the other for rebuilding new
data. The reference DOPA PET image is used as a learning target for the
corresponding network during the training process of tracer conversion.
Meanwhile, the invertible network iteratively estimates the resultant DOPA PET
data and compares it to the reference DOPA PET data. Notably, the reversible
model employs variable enhancement technique to achieve better power
generation. Moreover, image registration needs to be performed before training
due to the angular deviation of the acquired FDG and DOPA data information.
Experimental results exhibited excellent generation capability in mapping
between FDG and DOPA, suggesting that PET tracer conversion has great potential
in the case of limited tracer applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bohui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xubiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Pengfei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shirui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xinchong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangsong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiegen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02762">
<title>Fast Sparse 3D Convolution Network with VDB. (arXiv:2311.02762v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02762</link>
<description rdf:parseType="Literal">&lt;p&gt;We proposed a new Convolution Neural Network implementation optimized for
sparse 3D data inference. This implementation uses NanoVDB as the data
structure to store the sparse tensor. It leaves a relatively small memory
footprint while maintaining high performance. We demonstrate that this
architecture is around 20 times faster than the state-of-the-art dense CNN
model on a high-resolution 3D object classification network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fangjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1&quot;&gt;Anyong Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1&quot;&gt;Eftychios Sifakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03217">
<title>Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data. (arXiv:2311.03217v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03217</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer screening, primarily conducted through mammography, is often
supplemented with ultrasound for women with dense breast tissue. However,
existing deep learning models analyze each modality independently, missing
opportunities to integrate information across imaging modalities and time. In
this study, we present Multi-modal Transformer (MMT), a neural network that
utilizes mammography and ultrasound synergistically, to identify patients who
currently have cancer and estimate the risk of future cancer for patients who
are currently cancer-free. MMT aggregates multi-modal data through
self-attention and tracks temporal tissue changes by comparing current exams to
prior imaging. Trained on 1.3 million exams, MMT achieves an AUROC of 0.943 in
detecting existing cancers, surpassing strong uni-modal baselines. For 5-year
risk prediction, MMT attains an AUROC of 0.826, outperforming prior
mammography-based risk models. Our research highlights the value of multi-modal
and longitudinal imaging in cancer diagnosis and risk stratification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jungkyu Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yeung_F/0/1/0/all/0/1&quot;&gt;Frank Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goldberg_E/0/1/0/all/0/1&quot;&gt;Eliana Goldberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heacock_L/0/1/0/all/0/1&quot;&gt;Laura Heacock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shamout_F/0/1/0/all/0/1&quot;&gt;Farah Shamout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Geras_K/0/1/0/all/0/1&quot;&gt;Krzysztof J. Geras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04246">
<title>ADFactory: An Effective Framework for Generalizing Optical Flow with Nerf. (arXiv:2311.04246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant challenge facing current optical flow methods is the difficulty
in generalizing them well to the real world. This is mainly due to the high
cost of hand-crafted datasets, and existing self-supervised methods are limited
by indirect loss and occlusions, resulting in fuzzy outcomes. To address this
challenge, we introduce a novel optical flow training framework: automatic data
factory (ADF). ADF only requires RGB images as input to effectively train the
optical flow network on the target data domain. Specifically, we use advanced
Nerf technology to reconstruct scenes from photo groups collected by a
monocular camera, and then calculate optical flow labels between camera pose
pairs based on the rendering results. To eliminate erroneous labels caused by
defects in the scene reconstructed by Nerf, we screened the generated labels
from multiple aspects, such as optical flow matching accuracy, radiation field
confidence, and depth consistency. The filtered labels can be directly used for
network supervision. Experimentally, the generalization ability of ADF on KITTI
surpasses existing self-supervised optical flow and monocular scene flow
algorithms. In addition, ADF achieves impressive results in real-world
zero-point generalization evaluations and surpasses most supervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Han Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04818">
<title>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment. (arXiv:2311.04818v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04818</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from the collective knowledge of data dispersed across private
sources can provide neural networks with enhanced generalization capabilities.
Federated learning, a method for collaboratively training a machine learning
model across remote clients, achieves this by combining client models via the
orchestration of a central server. However, current approaches face two
critical limitations: i) they struggle to converge when client domains are
sufficiently different, and ii) current aggregation techniques produce an
identical global model for each client. In this work, we address these issues
by reformulating the typical federated learning setup: rather than learning a
single global model, we learn N models each optimized for a common objective.
To achieve this, we apply a weighted distance minimization to model parameters
shared in a peer-to-peer topology. The resulting framework, Iterative Parameter
Alignment, applies naturally to the cross-silo setting, and has the following
properties: (i) a unique solution for each participant, with the option to
globally converge each model in the federation, and (ii) an optional
early-stopping mechanism to elicit fairness among peers in collaborative
learning settings. These characteristics jointly provide a flexible new
framework for iteratively learning from peer models trained on disparate
datasets. We find that the technique achieves competitive results on a variety
of data partitions compared to state-of-the-art approaches. Further, we show
that the method is robust to divergent domains (i.e. disjoint classes across
peers) where existing approaches struggle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorbett_M/0/1/0/all/0/1&quot;&gt;Matt Gorbett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirazi_H/0/1/0/all/0/1&quot;&gt;Hossein Shirazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_I/0/1/0/all/0/1&quot;&gt;Indrakshi Ray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05836">
<title>Uncertainty-aware Single View Volumetric Rendering for Medical Neural Radiance Fields. (arXiv:2311.05836v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05836</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qinrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07042">
<title>Open-Vocabulary Video Anomaly Detection. (arXiv:2311.07042v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07042</link>
<description rdf:parseType="Literal">&lt;p&gt;Video anomaly detection (VAD) with weak supervision has achieved remarkable
performance in utilizing video-level labels to discriminate whether a video
frame is normal or abnormal. However, current approaches are inherently limited
to a closed-set setting and may struggle in open-world applications where there
can be anomaly categories in the test data unseen during training. A few recent
studies attempt to tackle a more realistic setting, open-set VAD, which aims to
detect unseen anomalies given seen anomalies and normal videos. However, such a
setting focuses on predicting frame anomaly scores, having no ability to
recognize the specific categories of anomalies, despite the fact that this
ability is essential for building more informed video surveillance systems.
This paper takes a step further and explores open-vocabulary video anomaly
detection (OVVAD), in which we aim to leverage pre-trained large models to
detect and categorize seen and unseen anomalies. To this end, we propose a
model that decouples OVVAD into two mutually complementary tasks --
class-agnostic detection and class-specific classification -- and jointly
optimizes both tasks. Particularly, we devise a semantic knowledge injection
module to introduce semantic knowledge from large language models for the
detection task, and design a novel anomaly synthesis module to generate pseudo
unseen anomaly videos with the help of large vision generation models for the
classification task. These semantic knowledge and synthesis anomalies
substantially extend our model&apos;s capability in detecting and categorizing a
variety of seen and unseen anomalies. Extensive experiments on three
widely-used benchmarks demonstrate our model achieves state-of-the-art
performance on OVVAD task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Peng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuerong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yujia Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07955">
<title>Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle Imagery: Review and Experimental Comparisons. (arXiv:2311.07955v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07955</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of maritime unmanned aerial vehicles (UAVs) and deep
learning technologies, the application of UAV-based object detection has become
increasingly significant in the fields of maritime industry and ocean
engineering. Endowed with intelligent sensing capabilities, the maritime UAVs
enable effective and efficient maritime surveillance. To further promote the
development of maritime UAV-based object detection, this paper provides a
comprehensive review of challenges, relative methods, and UAV aerial datasets.
Specifically, in this work, we first briefly summarize four challenges for
object detection on maritime UAVs, i.e., object feature diversity, device
limitation, maritime environment variability, and dataset scarcity. We then
focus on computational methods to improve maritime UAV-based object detection
performance in terms of scale-aware, small object detection, view-aware,
rotated object detection, lightweight methods, and others. Next, we review the
UAV aerial image/video datasets and propose a maritime UAV aerial dataset named
MS2ship for ship detection. Furthermore, we conduct a series of experiments to
present the performance evaluation and robustness analysis of object detection
methods on maritime datasets. Eventually, we give the discussion and outlook on
future works for maritime UAV-based object detection. The MS2ship dataset is
available at
\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ryan Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Jingxiang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruobin Gao&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>